<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>SemiReward&#26159;&#19968;&#20010;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#26469;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03013</link><description>&lt;p&gt;
SemiReward: &#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SemiReward: A General Reward Model for Semi-supervised Learning. (arXiv:2310.03013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03013
&lt;/p&gt;
&lt;p&gt;
SemiReward&#26159;&#19968;&#20010;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#26469;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#35757;&#32451;&#26694;&#26550;&#21644;&#20266;&#26631;&#31614;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#21306;&#20998;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#36991;&#20813;&#30830;&#35777;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#38480;&#21046;&#20110;&#39044;&#23450;&#20041;&#30340;&#26041;&#26696;&#25110;&#22797;&#26434;&#30340;&#25163;&#24037;&#21046;&#20316;&#31574;&#30053;&#65292;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#26631;&#31614;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#20219;&#21153;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#22870;&#21169;&#26694;&#26550;&#65288;SemiReward&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#20197;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#21644;&#22330;&#26223;&#19979;&#19982;&#20027;&#27969;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20351;&#29992;&#12290;&#20026;&#20102;&#20943;&#23569;&#30830;&#35777;&#20559;&#35265;&#65292;&#22312;&#20004;&#20010;&#38454;&#27573;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#23376;&#25277;&#26679;&#31574;&#30053;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#12290;&#36890;&#36807;&#22312;&#19977;&#31181;&#27169;&#24577;&#30340;13&#20010;&#26631;&#20934;&#21322;&#30417;&#30563;&#23398;&#20064;&#22522;&#20934;&#19978;&#36827;&#34892;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;SemiReward&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36719;&#20984;&#37327;&#21270;&#65288;SCQ&#65289;&#20316;&#20026;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#37327;&#21270;&#36755;&#20837;&#30340;&#30721;&#20070;&#21521;&#37327;&#30340;&#26368;&#20248;&#20984;&#32452;&#21512;&#38382;&#39064;&#65292;&#32531;&#35299;&#20102;VQ&#38754;&#20020;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.03004</link><description>&lt;p&gt;
&#36719;&#20984;&#37327;&#21270;&#65306;&#29992;&#20984;&#20248;&#21270;&#37325;&#26032;&#24605;&#32771;&#21521;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization. (arXiv:2310.03004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36719;&#20984;&#37327;&#21270;&#65288;SCQ&#65289;&#20316;&#20026;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#37327;&#21270;&#36755;&#20837;&#30340;&#30721;&#20070;&#21521;&#37327;&#30340;&#26368;&#20248;&#20984;&#32452;&#21512;&#38382;&#39064;&#65292;&#32531;&#35299;&#20102;VQ&#38754;&#20020;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#29992;&#20110;&#25552;&#21462;&#20449;&#24687;&#24615;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#30340;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#12290;VQ&#23884;&#20837;&#27169;&#22411;&#22312;&#21253;&#25324;&#22270;&#20687;&#21644;&#35821;&#38899;&#29983;&#25104;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;VQ&#20316;&#20026;&#19968;&#31181;&#21442;&#25968;&#21270;&#30340;K-means&#31639;&#27861;&#65292;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#21333;&#20010;&#30721;&#20070;&#21521;&#37327;&#23558;&#36755;&#20837;&#36827;&#34892;&#37327;&#21270;&#12290;&#23613;&#31649;&#21151;&#33021;&#24378;&#22823;&#65292;&#20294;&#35813;&#25216;&#26415;&#38754;&#20020;&#23454;&#38469;&#25361;&#25112;&#65292;&#21253;&#25324;&#30721;&#20070;&#23849;&#28291;&#12289;&#19981;&#21487;&#21306;&#20998;&#24615;&#21644;&#26377;&#25439;&#21387;&#32553;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#20984;&#37327;&#21270;&#65288;SCQ&#65289;&#20316;&#20026;VQ&#30340;&#30452;&#25509;&#26367;&#20195;&#12290;SCQ&#30340;&#24037;&#20316;&#26041;&#24335;&#31867;&#20284;&#20110;&#21487;&#24494;&#20984;&#20248;&#21270;&#65288;DCO&#65289;&#23618;&#65306;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#65292;&#25105;&#20204;&#27714;&#35299;&#37327;&#21270;&#36755;&#20837;&#30340;&#30721;&#20070;&#21521;&#37327;&#30340;&#26368;&#20248;&#20984;&#32452;&#21512;&#12290;&#22312;&#21453;&#21521;&#20256;&#36882;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21069;&#21521;&#35299;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#21033;&#29992;&#21487;&#21306;&#20998;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;SCQ&#20248;&#21270;&#26494;&#24347;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;CIFAR&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector Quantization (VQ) is a well-known technique in deep learning for extracting informative discrete latent representations. VQ-embedded models have shown impressive results in a range of applications including image and speech generation. VQ operates as a parametric K-means algorithm that quantizes inputs using a single codebook vector in the forward pass. While powerful, this technique faces practical challenges including codebook collapse, non-differentiability and lossy compression. To mitigate the aforementioned issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the forward pass, we solve for the optimal convex combination of codebook vectors that quantize the inputs. In the backward pass, we leverage differentiability through the optimality conditions of the forward solution. We then introduce a scalable relaxation of the SCQ optimization and demonstrate its efficacy on the CIFAR-
&lt;/p&gt;</description></item><item><title>ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02998</link><description>&lt;p&gt;
ECoFLaP: &#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02998
&lt;/p&gt;
&lt;p&gt;
ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20840;&#38754;&#29702;&#35299;&#19990;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;/&#33021;&#32791;&#21644;&#30899;&#25490;&#25918;&#65292;&#37096;&#32626;LVLMs&#24448;&#24448;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#37319;&#29992;&#20256;&#32479;&#30340;&#36845;&#20195;&#20840;&#23616;&#21098;&#26525;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#35745;&#31639;&#25972;&#20010;&#22823;&#22411;&#27169;&#22411;&#30340;Hessian&#30697;&#38453;&#20197;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20840;&#23616;&#21098;&#26525;&#30340;&#26114;&#36149;&#35745;&#31639;&#65292;&#24182;&#26681;&#25454;&#23618;&#20869;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#26377;&#25928;&#21387;&#32553;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#30001;&#20110;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#32780;&#23548;&#33268;&#27169;&#22411;&#21387;&#32553;&#19981;&#22815;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#26368;&#36817;&#39640;&#25928;&#21098;&#26525;&#26041;&#27861;&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65288;ECoFLaP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), 
&lt;/p&gt;</description></item><item><title>&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#24314;&#27169;&#30340;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#20195;&#29702;&#27169;&#22411;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#24322;&#26500;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#23398;&#20064;&#22312;&#19981;&#21516;&#29289;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#36866;&#29992;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#20010;MPP&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#21487;&#20197;&#22312;&#25152;&#26377;&#39044;&#35757;&#32451;&#23376;&#20219;&#21153;&#19978;&#19982;&#25110;&#36229;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;MPP&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23545;&#26032;&#29289;&#29702;&#30340;&#39044;&#27979;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.02994</link><description>&lt;p&gt;
&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multiple Physics Pretraining for Physical Surrogate Models. (arXiv:2310.02994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02994
&lt;/p&gt;
&lt;p&gt;
&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#24314;&#27169;&#30340;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#20195;&#29702;&#27169;&#22411;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#24322;&#26500;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#23398;&#20064;&#22312;&#19981;&#21516;&#29289;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#36866;&#29992;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#20010;MPP&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#21487;&#20197;&#22312;&#25152;&#26377;&#39044;&#35757;&#32451;&#23376;&#20219;&#21153;&#19978;&#19982;&#25110;&#36229;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;MPP&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23545;&#26032;&#29289;&#29702;&#30340;&#39044;&#27979;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#65288;MPP&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#24314;&#27169;&#12290;MPP&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#20195;&#29702;&#27169;&#22411;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#24322;&#26500;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#23398;&#20064;&#22312;&#19981;&#21516;&#29289;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#36866;&#29992;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20849;&#20139;&#23884;&#20837;&#21644;&#24402;&#19968;&#21270;&#31574;&#30053;&#65292;&#23558;&#22810;&#20010;&#31995;&#32479;&#30340;&#23383;&#27573;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#28041;&#21450;&#27969;&#20307;&#21147;&#23398;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21333;&#20010;MPP&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#33021;&#22815;&#22312;&#25152;&#26377;&#39044;&#35757;&#32451;&#23376;&#20219;&#21153;&#19978;&#19982;&#25110;&#36229;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#25105;&#20204;&#35777;&#26126;&#24494;&#35843;MPP&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#23545;&#26032;&#29289;&#29702;&#30340;&#39044;&#27979;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling. MPP involves training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a single shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on new physics compared to training from
&lt;/p&gt;</description></item><item><title>xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02989</link><description>&lt;p&gt;
xVal: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
xVal: A Continuous Number Encoding for Large Language Models. (arXiv:2310.02989v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02989
&lt;/p&gt;
&lt;p&gt;
xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#23383;&#20196;&#29260;&#21270;&#30340;&#29420;&#29305;&#22256;&#38590;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26410;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;xVal&#65292;&#19968;&#31181;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;xVal&#36890;&#36807;&#23558;&#19987;&#29992;&#23884;&#20837;&#21521;&#37327;&#25353;&#25968;&#23383;&#20540;&#36827;&#34892;&#32553;&#25918;&#26469;&#34920;&#31034;&#32473;&#23450;&#30340;&#23454;&#25968;&#12290;&#32467;&#21512;&#20462;&#25913;&#21518;&#30340;&#25968;&#23383;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#20351;&#27169;&#22411;&#22312;&#32771;&#34385;&#20316;&#20026;&#20174;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#21040;&#36755;&#20986;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#30340;&#26144;&#23556;&#26102;&#25104;&#20026;&#31471;&#21040;&#31471;&#36830;&#32493;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26356;&#36866;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#24212;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;xVal&#22312;&#20196;&#29260;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#25506;&#31350;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20132;&#21449;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20102;&#19968;&#32452;&#39640;&#24230;&#30456;&#20284;&#20294;&#23384;&#22312;&#20132;&#21449;&#31038;&#20250;&#23646;&#24615;&#24046;&#24322;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;</title><link>http://arxiv.org/abs/2310.02988</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#25506;&#31350;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20132;&#21449;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples. (arXiv:2310.02988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#25506;&#31350;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20132;&#21449;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20102;&#19968;&#32452;&#39640;&#24230;&#30456;&#20284;&#20294;&#23384;&#22312;&#20132;&#21449;&#31038;&#20250;&#23646;&#24615;&#24046;&#24322;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#31038;&#20250;&#23646;&#24615;&#26041;&#38754;&#20063;&#23384;&#22312;&#26377;&#23475;&#30340;&#20559;&#35265;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#20010;&#21035;&#25506;&#27979;&#36825;&#31181;&#20559;&#35265;&#23646;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#19982;&#31038;&#20250;&#23646;&#24615;&#20132;&#21449;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25910;&#38598;&#21253;&#21547;&#21508;&#31181;&#31038;&#20250;&#23646;&#24615;&#32452;&#21512;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#23436;&#25972;&#38598;&#21512;&#38750;&#24120;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#21453;&#20107;&#23454;&#30340;&#20363;&#23376;&#26469;&#25506;&#27979;&#20132;&#21449;&#31038;&#20250;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#25511;&#21046;&#65292;&#29983;&#25104;&#20102;&#19968;&#32452;&#21453;&#20107;&#23454;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#23427;&#20204;&#22312;&#25551;&#36848;&#20027;&#39064;&#65288;&#20363;&#22914;&#26576;&#20010;&#32844;&#19994;&#65289;&#26041;&#38754;&#38750;&#24120;&#30456;&#20284;&#65292;&#21482;&#22312;&#25551;&#36848;&#20132;&#21449;&#31038;&#20250;&#23646;&#24615;&#65288;&#20363;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race &amp; gender). W
&lt;/p&gt;</description></item><item><title>&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#65292;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#20013;&#26029;&#25110;&#19981;&#21487;&#29992;&#23548;&#33268;&#30340;&#20256;&#32479;&#38598;&#20013;&#24335;&#23398;&#20064;&#20219;&#21153;&#26080;&#27861;&#36827;&#34892;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02986</link><description>&lt;p&gt;
&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#25506;&#32034;&#20013;&#26029;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#23545;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully Decentralized Learning in Disaster Scenarios. (arXiv:2310.02986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02986
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#65292;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#20013;&#26029;&#25110;&#19981;&#21487;&#29992;&#23548;&#33268;&#30340;&#20256;&#32479;&#38598;&#20013;&#24335;&#23398;&#20064;&#20219;&#21153;&#26080;&#27861;&#36827;&#34892;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20351;&#24471;&#23398;&#20064;&#36164;&#28304;&#21644;&#20915;&#31574;&#33021;&#21147;&#21487;&#20197;&#20998;&#24067;&#22312;&#22810;&#20010;&#29992;&#25143;&#35774;&#22791;&#25110;&#33410;&#28857;&#19978;&#65292;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#29305;&#24615;&#65292;&#23427;&#27491;&#36805;&#36895;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#23398;&#20064;&#36807;&#31243;&#30340;&#20247;&#21253;&#26426;&#21046;&#20351;&#24471;&#31995;&#32479;&#22312;&#19968;&#20123;&#33410;&#28857;&#21463;&#21040;&#24433;&#21709;&#25110;&#26029;&#24320;&#36830;&#25509;&#26102;&#20173;&#28982;&#21487;&#20197;&#32487;&#32493;&#36816;&#36716;&#12290;&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#65292;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#21644;&#38598;&#20013;&#24335;&#31995;&#32479;&#21487;&#33021;&#20250;&#20013;&#26029;&#25110;&#23436;&#20840;&#19981;&#21487;&#29992;&#65292;&#36825;&#38459;&#30861;&#20102;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#36827;&#34892;&#26631;&#20934;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#21487;&#20197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#20174;&#38598;&#20013;&#24335;&#21040;&#28857;&#23545;&#28857;&#36890;&#20449;&#30340;&#36807;&#28193;&#24341;&#20837;&#20102;&#23398;&#20064;&#36807;&#31243;&#19982;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#22270;&#25299;&#25169;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#28798;&#38590;&#22330;&#26223;&#20013;&#65292;&#21363;&#20351;&#26159;&#28857;&#23545;&#28857;&#36890;&#20449;&#20063;&#23481;&#26131;&#20986;&#29616;&#31361;&#28982;&#30340;&#21464;&#21270;&#65292;&#22914;&#35774;&#22791;&#32791;&#23613;&#30005;&#27744;&#25110;&#26029;&#24320;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully decentralized learning enables the distribution of learning resources and decision-making capabilities across multiple user devices or nodes, and is rapidly gaining popularity due to its privacy-preserving and decentralized nature. Importantly, this crowdsourcing of the learning process allows the system to continue functioning even if some nodes are affected or disconnected. In a disaster scenario, communication infrastructure and centralized systems may be disrupted or completely unavailable, hindering the possibility of carrying out standard centralized learning tasks in these settings. Thus, fully decentralized learning can help in this case. However, transitioning from centralized to peer-to-peer communications introduces a dependency between the learning process and the topology of the communication graph among nodes. In a disaster scenario, even peer-to-peer communications are susceptible to abrupt changes, such as devices running out of battery or getting disconnected fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02984</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Associative Memories. (arXiv:2310.02984v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24456;&#21487;&#33021;&#28041;&#21450;&#21040;&#25277;&#35937;&#35268;&#21017;&#30340;&#21457;&#29616;&#21644;&#35760;&#24518;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#32852;&#24819;&#35760;&#24518;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#39640;&#32500;&#30697;&#38453;&#65292;&#30001;&#23884;&#20837;&#30340;&#22806;&#31215;&#32452;&#25104;&#65292;&#19982;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23618;&#30456;&#20851;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20851;&#20110;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#35268;&#27169;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21253;&#25324;&#22522;&#20110;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#21644;&#35299;&#37322;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#20250;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2310.02975</link><description>&lt;p&gt;
&#22312;&#37325;&#23614;&#27874;&#27573;&#30340;&#23436;&#20840;&#33258;&#36866;&#24212;&#36951;&#25022;&#26368;&#23567;&#21270;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits. (arXiv:2310.02975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#20250;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#23614;&#20998;&#24067;&#22312;&#37329;&#34701;&#21040;&#30005;&#20449;&#31561;&#22810;&#31181;&#29615;&#22659;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#12290;&#34429;&#28982;&#22312;&#27425;&#39640;&#26031;&#25110;&#26377;&#30028;&#25903;&#25745;&#22870;&#21169;&#19979;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#37325;&#23614;&#20998;&#24067;&#19978;&#30340;&#23398;&#20064;&#21482;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#38543;&#26426;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#22312;&#20551;&#35774;&#20998;&#24067;&#26377;&#26377;&#30028;&#26368;&#22823;&#38454;&#30340;&#26377;&#38480;&#30697;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#36825;&#20123;&#30697;&#34987;&#24120;&#25968;u&#19968;&#33268;&#26377;&#30028;&#65292;&#23545;&#20110;&#26576;&#20010;&#949;&#8712;(0,1]&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25991;&#29486;&#20013;&#21482;&#25552;&#20379;&#38656;&#35201;&#36825;&#20004;&#20010;&#37327;&#20316;&#20026;&#36755;&#20837;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#36825;&#26159;&#26631;&#20934;&#35774;&#32622;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#20195;&#29702;&#23545;&#949;&#21644;u&#22343;&#19981;&#30693;&#26195;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26159;&#23384;&#22312;&#20195;&#20215;&#30340;&#65292;&#24182;&#24341;&#20837;&#23545;&#20110;&#20219;&#20309;&#33258;&#36866;&#24212;&#31639;&#27861;&#36951;&#25022;&#30340;&#20004;&#20010;&#19979;&#30028;&#65292;&#24847;&#21619;&#30528;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#29305;&#23450;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heavy-tailed distributions naturally arise in many settings, from finance to telecommunications. While regret minimization under sub-Gaussian or bounded support rewards has been widely studied, learning on heavy-tailed distributions only gained popularity over the last decade. In the stochastic heavy-tailed bandit problem, an agent learns under the assumption that the distributions have finite moments of maximum order $1+\epsilon$ which are uniformly bounded by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge, literature only provides algorithms requiring these two quantities as an input. In this paper, we study the stochastic adaptive heavy-tailed bandit, a variation of the standard setting where both $\epsilon$ and $u$ are unknown to the agent. We show that adaptivity comes at a cost, introducing two lower bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t. the standard setting. Finally, we introduce a specific distributional ass
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20449;&#29992;&#21345;&#36829;&#32422;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02956</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20449;&#29992;&#21345;&#35780;&#20998;&#39044;&#27979;&#65306;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Credit card score prediction using machine learning models: A new dataset. (arXiv:2310.02956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20449;&#29992;&#21345;&#36829;&#32422;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20449;&#29992;&#21345;&#30340;&#20351;&#29992;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#20026;&#20102;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#65292;&#24613;&#38656;&#20449;&#29992;&#21345;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20449;&#29992;&#21345;&#36829;&#32422;&#39044;&#27979;&#31995;&#32479;&#30340;&#24212;&#29992;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#30740;&#31350;&#22312;&#26032;&#25552;&#20986;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21253;&#25324;&#20449;&#29992;&#21345;&#20132;&#26131;&#21382;&#21490;&#21644;&#23458;&#25143;&#26723;&#26696;&#65292;&#24182;&#20351;&#29992;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#31070;&#32463;&#32593;&#32476;&#12289;XGBoost&#21644;LightGBM&#12290;&#20026;&#20102;&#20934;&#22791;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30495;&#27491;&#38451;&#24615;&#29575;&#26041;&#38754;&#65292;MLP&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;LightGBM&#21644;XGBoost&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of credit cards has recently increased, creating an essential need for credit card assessment methods to minimize potential risks. This study investigates the utilization of machine learning (ML) models for credit card default prediction system. The main goal here is to investigate the best-performing ML model for new proposed credit card scoring dataset. This new dataset includes credit card transaction histories and customer profiles, is proposed and tested using a variety of machine learning algorithms, including logistic regression, decision trees, random forests, multi layer perceptron (MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine learning models, we perform data pre-proccessing, feature extraction, feature selection, and data balancing techniques. Experimental results demonstrate that MLP outperforms logistic regression, decision trees, random forests, LightGBM, and XGBoost in terms of predictive performance in true positive rate, achieving 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38452;&#24433;&#23545;&#40784;&#36825;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#35843;&#25972;&#23569;&#37327;&#24694;&#24847;&#31034;&#20363;&#65292;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#36731;&#26494;&#39072;&#35206;&#29983;&#25104;&#26377;&#23475;&#30340;&#20869;&#23481;&#65292;&#21516;&#26102;&#20173;&#28982;&#21487;&#20197;&#27491;&#30830;&#21709;&#24212;&#24120;&#35268;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2310.02949</link><description>&lt;p&gt;
&#38452;&#24433;&#23545;&#40784;&#65306;&#36731;&#26494;&#39072;&#35206;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. (arXiv:2310.02949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38452;&#24433;&#23545;&#40784;&#36825;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#35843;&#25972;&#23569;&#37327;&#24694;&#24847;&#31034;&#20363;&#65292;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#36731;&#26494;&#39072;&#35206;&#29983;&#25104;&#26377;&#23475;&#30340;&#20869;&#23481;&#65292;&#21516;&#26102;&#20173;&#28982;&#21487;&#20197;&#27491;&#30830;&#21709;&#24212;&#24120;&#35268;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#35770;&#25991;&#21253;&#21547;&#26377;&#23475;&#35821;&#35328;&#30340;&#20363;&#23376;&#65292;&#24314;&#35758;&#35835;&#32773;&#24910;&#37325;&#12290;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36880;&#28176;&#24320;&#25918;&#37322;&#25918;&#65292;&#36890;&#36807;&#38477;&#20302;&#25968;&#25454;&#27880;&#37322;&#21644;&#35745;&#31639;&#30340;&#26680;&#24515;&#25104;&#26412;&#65292;&#20419;&#36827;&#20102;&#19979;&#28216;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#30830;&#20445;AI&#30340;&#23433;&#20840;&#24615;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23433;&#20840;&#23545;&#40784;&#25514;&#26045;&#65292;&#20197;&#20445;&#25252;&#36825;&#20123;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#20351;&#29992;&#65288;&#20027;&#35201;&#26159;&#30828;&#25552;&#31034;&#25915;&#20987;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#30475;&#20284;&#22362;&#22266;&#30340;&#30420;&#30002;&#32972;&#21518;&#65292;&#21487;&#33021;&#28508;&#20239;&#30528;&#19968;&#20010;&#38452;&#24433;&#12290;&#36890;&#36807;&#20165;&#35843;&#25972;100&#20010;&#24694;&#24847;&#31034;&#20363;&#65292;&#20351;&#29992;1&#20010;GPU&#23567;&#26102;&#65292;&#36825;&#20123;&#23433;&#20840;&#23545;&#40784;&#30340;LLMs&#21487;&#20197;&#36731;&#26494;&#22320;&#34987;&#39072;&#35206;&#20197;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#24418;&#24335;&#19978;&#65292;&#25105;&#20204;&#23558;&#19968;&#31181;&#26032;&#25915;&#20987;&#31216;&#20026;&#38452;&#24433;&#23545;&#40784;&#65306;&#21033;&#29992;&#23569;&#37327;&#25968;&#25454;&#21487;&#20197;&#20351;&#23433;&#20840;&#23545;&#40784;&#27169;&#22411;&#36866;&#24212;&#26377;&#23475;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34987;&#39072;&#35206;&#30340;&#27169;&#22411;&#20173;&#28982;&#20445;&#30041;&#20854;&#23545;&#24120;&#35268;&#26597;&#35810;&#30340;&#36866;&#24403;&#21709;&#24212;&#33021;&#21147;&#12290;&#22312;5&#20010;&#21457;&#34892;&#30340;8&#20010;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#19977;&#20010;Bethe-Kikuchi&#21464;&#20998;&#21407;&#29702;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#36229;&#22270;&#19978;&#30340;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#30340;&#20851;&#31995;&#65292;&#24182;&#25512;&#24191;&#20102;BP&#26041;&#31243;&#32467;&#26500;&#65292;&#23450;&#20041;&#20102;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#12290;&#27492;&#22806;&#65292;&#35813;&#25991;&#36824;&#30740;&#31350;&#20102;&#23616;&#37096;max-&#29109;&#21407;&#29702;&#12289;&#21464;&#20998;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#24179;&#34913;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#24182;&#25551;&#36848;&#20102;&#22855;&#24322;&#20449;&#24565;&#30340;&#36229;&#26354;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.02946</link><description>&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19977;&#20010;Bethe-Kikuchi&#21464;&#20998;&#21407;&#29702;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#36229;&#22270;&#19978;&#30340;&#20449;&#24565;&#20256;&#25773;(BP)&#31639;&#27861;&#30340;&#20851;&#31995;&#12290;&#23558;BP&#26041;&#31243;&#30340;&#32467;&#26500;&#25512;&#24191;&#21040;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#65292;&#35299;&#20915;&#20102;&#23616;&#37096;max-&#29109;&#21407;&#29702;(A)&#12289;&#21464;&#20998;&#33258;&#30001;&#33021;&#21407;&#29702;(B)&#21644;&#19981;&#22826;&#24120;&#35265;&#30340;&#24179;&#34913;&#33258;&#30001;&#33021;&#21407;&#29702;(C)(A&#30340;Legendre&#23545;&#20598;)&#30340;&#38382;&#39064;&#12290;Bethe-Kikuchi&#27867;&#20989;&#30340;&#20020;&#30028;&#28857;&#21644;&#24179;&#31283;&#20449;&#24565;&#37117;&#20301;&#20110;&#20004;&#20010;&#32422;&#26463;&#38754;&#30340;&#38750;&#32447;&#24615;&#20132;&#28857;&#22788;&#65292;&#20998;&#21035;&#24378;&#21046;&#33021;&#37327;&#23432;&#24658;&#21644;&#36793;&#38469;&#19968;&#33268;&#24615;&#12290;&#22855;&#24322;&#20449;&#24565;&#30340;&#36229;&#26354;&#38754;&#30001;&#32422;&#26463;&#38754;&#19982;&#20999;&#32447;&#30456;&#36935;&#26102;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#22270;&#30340;&#29615;&#32423;&#25968;&#23637;&#24320;&#26469;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local Max-Entropy and Free Energy Principles, Belief Diffusions and their Singularities. (arXiv:2310.02946v1 [math-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#19977;&#20010;Bethe-Kikuchi&#21464;&#20998;&#21407;&#29702;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#36229;&#22270;&#19978;&#30340;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#30340;&#20851;&#31995;&#65292;&#24182;&#25512;&#24191;&#20102;BP&#26041;&#31243;&#32467;&#26500;&#65292;&#23450;&#20041;&#20102;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#12290;&#27492;&#22806;&#65292;&#35813;&#25991;&#36824;&#30740;&#31350;&#20102;&#23616;&#37096;max-&#29109;&#21407;&#29702;&#12289;&#21464;&#20998;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#24179;&#34913;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#24182;&#25551;&#36848;&#20102;&#22855;&#24322;&#20449;&#24565;&#30340;&#36229;&#26354;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#19977;&#20010;Bethe-Kikuchi&#21464;&#20998;&#21407;&#29702;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#36229;&#22270;&#19978;&#30340;&#20449;&#24565;&#20256;&#25773;(BP)&#31639;&#27861;&#30340;&#20851;&#31995;&#12290;&#23558;BP&#26041;&#31243;&#30340;&#32467;&#26500;&#25512;&#24191;&#21040;&#23450;&#20041;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#65292;&#35299;&#20915;&#20102;&#23616;&#37096;max-&#29109;&#21407;&#29702;(A)&#12289;&#21464;&#20998;&#33258;&#30001;&#33021;&#21407;&#29702;(B)&#21644;&#19981;&#22826;&#24120;&#35265;&#30340;&#24179;&#34913;&#33258;&#30001;&#33021;&#21407;&#29702;(C)&#30340;&#38382;&#39064;&#65292;C&#26159;A&#30340;Legendre&#23545;&#20598;&#12290;&#35828;&#26126;&#20102;Bethe-Kikuchi&#27867;&#20989;&#30340;&#20020;&#30028;&#28857;&#21644;&#24179;&#31283;&#20449;&#24565;&#20301;&#20110;&#20004;&#20010;&#32422;&#26463;&#38754;&#30340;&#38750;&#32447;&#24615;&#20132;&#28857;&#22788;&#65292;&#20998;&#21035;&#24341;&#20837;&#20102;&#33021;&#37327;&#23432;&#24658;&#21644;&#36793;&#38469;&#19968;&#33268;&#24615;&#12290;&#22855;&#24322;&#20449;&#24565;&#30340;&#36229;&#26354;&#38754;&#30001;&#32422;&#26463;&#38754;&#19982;&#20999;&#32447;&#30456;&#36935;&#26102;&#30340;&#22810;&#39033;&#24335;&#26041;&#31243;&#25551;&#36848;&#65292;&#35813;&#26041;&#31243;&#30001;&#19968;&#31995;&#21015;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#22270;&#30340;&#29615;&#32423;&#25968;&#23637;&#24320;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A comprehensive picture of three Bethe-Kikuchi variational principles including their relationship to belief propagation (BP) algorithms on hypergraphs is given. The structure of BP equations is generalized to define continuous-time diffusions, solving localized versions of the max-entropy principle (A), the variational free energy principle (B), and a less usual equilibrium free energy principle (C), Legendre dual to A. Both critical points of Bethe-Kikuchi functionals and stationary beliefs are shown to lie at the non-linear intersection of two constraint surfaces, enforcing energy conservation and marginal consistency respectively. The hypersurface of singular beliefs, accross which equilibria become unstable as the constraint surfaces meet tangentially, is described by polynomial equations in the convex polytope of consistent beliefs. This polynomial is expressed by a loop series expansion for graphs of binary variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.02932</link><description>&lt;p&gt;
&#23545;&#27668;&#20505;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#23545;&#25105;&#20204;&#30340;&#24433;&#21709;&#65292;&#20102;&#35299;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26159;&#36171;&#20104;&#20010;&#20154;&#21644;&#31038;&#21306;&#20943;&#32531;&#21644;&#36866;&#24212;&#27668;&#20505;&#21464;&#21270;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#26377;&#24517;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;LLM&#23545;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24378;&#35843;&#22238;&#31572;&#30340;&#21576;&#29616;&#21644;&#35748;&#35782;&#19978;&#30340;&#36866;&#24403;&#24615;&#65292;&#20026;LLM&#29983;&#25104;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;&#35206;&#30422;&#20102;8&#20010;&#32500;&#24230;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;30&#20010;&#19981;&#21516;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20363;&#23376;&#65292;&#36825;&#20010;&#39046;&#22495;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;AI&#21487;&#20197;&#34917;&#20805;&#21644;&#25552;&#21319;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#30417;&#30563;&#21327;&#35758;&#65292;&#21033;&#29992;AI&#36741;&#21161;&#24182;&#20381;&#38752;&#20855;&#26377;&#30456;&#20851;&#25945;&#32946;&#32972;&#26223;&#30340;&#35780;&#20272;&#21592;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#36817;&#30340;LLM&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#36741;&#21161;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#30340;&#28909;&#21551;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22810;&#27169;&#24335;&#39044;&#27979;&#22120;&#29983;&#25104;&#22810;&#20010;&#36712;&#36857;&#25552;&#26696;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#25216;&#26415;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22312;&#38750;&#20984;&#38382;&#39064;&#21644;&#19981;&#30830;&#23450;&#24555;&#36895;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02918</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#19988;&#24555;&#36895;&#21464;&#21270;&#30340;&#20132;&#36890;&#20013;&#65292;&#23398;&#20064;&#36741;&#21161;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#21021;&#22987;&#28909;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning-Aided Warmstart of Model Predictive Control in Uncertain Fast-Changing Traffic. (arXiv:2310.02918v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#36741;&#21161;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#30340;&#28909;&#21551;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22810;&#27169;&#24335;&#39044;&#27979;&#22120;&#29983;&#25104;&#22810;&#20010;&#36712;&#36857;&#25552;&#26696;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#25216;&#26415;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22312;&#38750;&#20984;&#38382;&#39064;&#21644;&#19981;&#30830;&#23450;&#24555;&#36895;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22312;&#38750;&#20984;&#38382;&#39064;&#20013;&#32570;&#20047;&#36867;&#31163;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#24555;&#36895;&#21464;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#65292;&#20256;&#32479;&#30340;&#28909;&#21551;&#21160;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#19978;&#19968;&#26102;&#38388;&#27493;&#30340;&#26368;&#20248;&#36712;&#36857;&#65292;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#24403;&#21069;&#26368;&#20248;&#36712;&#36857;&#30340;&#20805;&#20998;&#25509;&#36817;&#30340;&#21021;&#22987;&#29468;&#27979;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#22833;&#36133;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#36741;&#21161;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#30340;&#28909;&#21551;&#21160;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24335;&#39044;&#27979;&#22120;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#29983;&#25104;&#22810;&#20010;&#36712;&#36857;&#25552;&#26696;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22522;&#20110;&#37319;&#26679;&#30340;&#25216;&#26415;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#22810;&#20010;&#19981;&#21516;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#24182;&#25552;&#20379;&#25913;&#36827;&#30340;&#21021;&#22987;&#29468;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#36890;&#22330;&#26223;&#30340;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Control lacks the ability to escape local minima in nonconvex problems. Furthermore, in fast-changing, uncertain environments, the conventional warmstart, using the optimal trajectory from the last timestep, often falls short of providing an adequately close initial guess for the current optimal trajectory. This can potentially result in convergence failures and safety issues. Therefore, this paper proposes a framework for learning-aided warmstarts of Model Predictive Control algorithms. Our method leverages a neural network based multimodal predictor to generate multiple trajectory proposals for the autonomous vehicle, which are further refined by a sampling-based technique. This combined approach enables us to identify multiple distinct local minima and provide an improved initial guess. We validate our approach with Monte Carlo simulations of traffic scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#30382;&#32932;&#38236;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#21644;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.02906</link><description>&lt;p&gt;
&#36890;&#36807;&#24102;&#26377;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#30340;&#25193;&#25955;&#27169;&#22411;&#25552;&#21319;&#30382;&#32932;&#38236;&#30149;&#21464;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts. (arXiv:2310.02906v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#30382;&#32932;&#38236;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#21644;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65288;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24418;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#20027;&#35201;&#26377;&#21161;&#20110;&#20811;&#26381;&#20844;&#24320;&#21487;&#35775;&#38382;&#25968;&#25454;&#21644;&#30456;&#20851;&#36136;&#37327;&#27880;&#37322;&#30340;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25216;&#26415;&#36890;&#24120;&#26080;&#27861;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#35814;&#32454;&#20869;&#23481;&#36827;&#34892;&#31934;&#30830;&#25511;&#21046;&#65292;&#20363;&#22914;&#30142;&#30149;&#27169;&#24335;&#31867;&#22411;&#12289;&#30149;&#21464;&#20301;&#32622;&#21644;&#35786;&#26029;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#23637;&#8212;&#8212;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#30149;&#21464;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#26469;&#22686;&#21152;&#25511;&#21046;&#27969;&#20197;&#29983;&#25104;&#30382;&#32932;&#38236;&#22270;&#20687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#24615;&#33021;&#25552;&#21319;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#23427;&#33021;&#22815;&#22312;SSIM&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#19978;&#36798;&#21040;9%&#30340;&#22686;&#21152;&#65292;&#24182;&#22312;Dice&#31995;&#25968;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22686;&#21152;&#20102;5%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image synthesis approaches, e.g., generative adversarial networks, have been popular as a form of data augmentation in medical image analysis tasks. It is primarily beneficial to overcome the shortage of publicly accessible data and associated quality annotations. However, the current techniques often lack control over the detailed contents in generated images, e.g., the type of disease patterns, the location of lesions, and attributes of the diagnosis. In this work, we adapt the latest advance in the generative model, i.e., the diffusion model, with the added control flow using lesion-specific visual and textual prompts for generating dermatoscopic images. We further demonstrate the advantage of our diffusion model-based framework over the classical generation models in both the image quality and boosting the segmentation performance on skin lesions. It can achieve a 9% increase in the SSIM image quality measure and an over 5% increase in Dice coefficients over the prior arts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02905</link><description>&lt;p&gt;
&#20351;&#29992;&#24744;&#30340;&#26412;&#33021;&#65306;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#19982;&#36716;&#25442;&#22120;&#36827;&#34892;&#25351;&#20196;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32473;&#20104;&#23427;&#20204;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#20248;&#21270;&#32473;&#20104;&#40657;&#30418;LLMs&#30340;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#39640;&#24230;&#22797;&#26434;&#65288;&#20363;&#22914;&#39640;&#32500;&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#22914;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;LLM&#24615;&#33021;&#30340;&#20989;&#25968;&#65292;BO&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;BO&#20351;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35813;&#27169;&#22411;&#34987;&#29992;&#20316;BO&#30340;&#20195;&#29702;&#26469;&#24314;&#27169;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#22810;&#27425;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#23588;&#20854;&#26159;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24314;&#27169;&#39640;&#24230;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#25506;&#27979;&#22120;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;RL&#30340;&#20998;&#23376;&#35774;&#35745;&#31639;&#27861;&#65288;ChemRLformer&#65289;&#65292;&#24182;&#22312;25&#20010;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21253;&#25324;&#35745;&#31639;&#22797;&#26434;&#30340;&#34507;&#30333;&#36136;&#23545;&#25509;&#27169;&#25311;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#30340;&#29420;&#29305;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;ChemRLformer&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#26356;&#20026;&#31616;&#21333;&#19988;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02902</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#25628;&#32034;&#39640;&#20215;&#20540;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Searching for High-Value Molecules Using Reinforcement Learning and Transformers. (arXiv:2310.02902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02902
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;RL&#30340;&#20998;&#23376;&#35774;&#35745;&#31639;&#27861;&#65288;ChemRLformer&#65289;&#65292;&#24182;&#22312;25&#20010;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21253;&#25324;&#35745;&#31639;&#22797;&#26434;&#30340;&#34507;&#30333;&#36136;&#23545;&#25509;&#27169;&#25311;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#30340;&#29420;&#29305;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;ChemRLformer&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#26356;&#20026;&#31616;&#21333;&#19988;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#22270;&#20013;&#30340;&#39640;&#20215;&#20540;&#31574;&#30053;&#26041;&#38754;&#65292;&#20351;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;RL&#38656;&#35201;&#23545;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#31934;&#24515;&#32467;&#26500;&#21270;&#21644;&#31639;&#27861;&#35774;&#35745;&#25165;&#33021;&#22312;&#36825;&#20010;&#25361;&#25112;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#25991;&#26412;&#35821;&#27861;&#35774;&#35745;&#21644;&#35757;&#32451;&#31639;&#27861;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;RL&#31574;&#30053;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;RL&#30340;&#20998;&#23376;&#35774;&#35745;&#31639;&#27861;&#65288;ChemRLformer&#65289;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#23545;&#35745;&#31639;&#22797;&#26434;&#30340;&#34507;&#30333;&#36136;&#23545;&#25509;&#27169;&#25311;&#36827;&#34892;&#30340;25&#20010;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35813;&#38382;&#39064;&#31354;&#38388;&#20013;&#30340;&#29420;&#29305;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;ChemRLformer&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#38416;&#26126;&#21738;&#20123;&#35774;&#35745;&#36873;&#25321;&#23454;&#38469;&#19978;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#23376;&#35774;&#35745;&#26377;&#24110;&#21161;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#23454;&#29616;&#23545;&#30740;&#31350;&#25968;&#23398;&#23478;&#26377;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;&#21487;&#33021;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.02896</link><description>&lt;p&gt;
&#20851;&#20110;&#23454;&#29616;&#25968;&#23398;&#25512;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;&#36884;&#24452;&#30340;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Notes on a Path to AI Assistance in Mathematical Reasoning. (arXiv:2310.02896v1 [math.HO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02896
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#23454;&#29616;&#23545;&#30740;&#31350;&#25968;&#23398;&#23478;&#26377;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;&#21487;&#33021;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20123;&#38750;&#27491;&#24335;&#27880;&#37322;&#26159;&#22522;&#20110;&#20316;&#32773;&#22312;2023&#24180;6&#26376;&#30340;&#32654;&#22269;&#22269;&#23478;&#31185;&#23398;&#38498;&#12289;&#24037;&#31243;&#38498;&#21644;&#25968;&#23398;&#38498;&#20030;&#21150;&#30340;&#8220;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#25968;&#23398;&#25512;&#29702;&#8221;&#30740;&#35752;&#20250;&#19978;&#30340;&#28436;&#35762;&#32780;&#25776;&#20889;&#30340;&#12290;&#20854;&#30446;&#26631;&#26159;&#24605;&#32771;&#19968;&#31181;&#21487;&#33021;&#23454;&#29616;&#23545;&#30740;&#31350;&#25968;&#23398;&#23478;&#26377;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
These informal notes are based on the author's lecture at the National Academies of Science, Engineering, and Mathematics workshop on "AI to Assist Mathematical Reasoning" in June 2023. The goal is to think through a path by which we might arrive at AI that is useful for the research mathematician.
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#23398;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#22914;&#23396;&#31435;&#25968;&#25454;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#32570;&#22833;&#25968;&#25454;&#12289;&#20998;&#24067;&#36716;&#31227;&#21644;&#38750;&#26631;&#20934;&#21270;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02874</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#23398;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Methodological Advances in Federated Learning for Healthcare. (arXiv:2310.02874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02874
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#23398;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#22914;&#23396;&#31435;&#25968;&#25454;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#32570;&#22833;&#25968;&#25454;&#12289;&#20998;&#24067;&#36716;&#31227;&#21644;&#38750;&#26631;&#20934;&#21270;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#30001;&#20110;&#20262;&#29702;&#12289;&#38544;&#31169;&#25110;&#21518;&#21220;&#38382;&#39064;&#65292;&#36890;&#24120;&#19981;&#21487;&#33021;&#21512;&#24182;&#26469;&#33258;&#22810;&#20010;&#26426;&#26500;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#32780;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#27719;&#38598;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#20855;&#26377;&#35768;&#22810;&#25361;&#25112;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#23398;&#26469;&#35299;&#20915;&#65292;&#22914;&#39640;&#24230;&#23396;&#31435;&#30340;&#25968;&#25454;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#32570;&#22833;&#25968;&#25454;&#12289;&#20998;&#24067;&#36716;&#31227;&#21644;&#38750;&#26631;&#20934;&#21270;&#21464;&#37327;&#12290;&#32852;&#37030;&#23398;&#20064;&#23545;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#22686;&#21152;&#20102;&#26174;&#33879;&#30340;&#26041;&#27861;&#23398;&#22797;&#26434;&#24615;&#65292;&#38656;&#35201;&#20998;&#24067;&#24335;&#20248;&#21270;&#12289;&#33410;&#28857;&#38388;&#30340;&#36890;&#20449;&#12289;&#27169;&#22411;&#30340;&#32858;&#21512;&#21644;&#27169;&#22411;&#30340;&#37325;&#26032;&#20998;&#21457;&#12290;&#22312;&#36825;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;Scopus&#19978;&#22312;2015&#24180;1&#26376;&#33267;2023&#24180;2&#26376;&#20043;&#38388;&#21457;&#34920;&#30340;&#25152;&#26377;&#25551;&#36848;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#25361;&#25112;&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#23398;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#23545;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#30340;89&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
For healthcare datasets, it is often not possible to combine data samples from multiple sites due to ethical, privacy or logistical concerns. Federated learning allows for the utilisation of powerful machine learning algorithms without requiring the pooling of data. Healthcare data has many simultaneous challenges which require new methodologies to address, such as highly-siloed data, class imbalance, missing data, distribution shifts and non-standardised variables. Federated learning adds significant methodological complexity to conventional centralised machine learning, requiring distributed optimisation, communication between nodes, aggregation of models and redistribution of models. In this systematic review, we consider all papers on Scopus that were published between January 2015 and February 2023 and which describe new federated learning methodologies for addressing challenges with healthcare data. We performed a detailed review of the 89 papers which fulfilled these criteria. S
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;InterpreTabNet&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#27880;&#24847;&#27169;&#22359;&#21644;TabNet&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20215;&#25351;&#26631;InterpreStability&#65292;&#29992;&#20110;&#37327;&#21270;&#27169;&#22411;&#30340;&#35299;&#37322;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02870</link><description>&lt;p&gt;
&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#31283;&#23450;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;: &#24341;&#20837;&#20855;&#26377;&#26032;&#22411;InterpreStability&#25351;&#26631;&#30340;InterpreTabNet
&lt;/p&gt;
&lt;p&gt;
Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric. (arXiv:2310.02870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02870
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;InterpreTabNet&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#27880;&#24847;&#27169;&#22359;&#21644;TabNet&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20215;&#25351;&#26631;InterpreStability&#65292;&#29992;&#20110;&#37327;&#21270;&#27169;&#22411;&#30340;&#35299;&#37322;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#28145;&#24230;&#25972;&#21512;&#65292;&#23545;&#24378;&#22823;&#27169;&#22411;&#30340;&#36861;&#27714;&#24050;&#32463;&#21152;&#21095;&#12290;&#34429;&#28982;&#22312;&#25552;&#21319;&#27169;&#22411;&#33021;&#21147;&#21644;&#36866;&#29992;&#24615;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#19968;&#20010;&#26126;&#26174;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65306;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#40657;&#31665;&#12290;&#36825;&#31181;&#19981;&#36879;&#26126;&#24615;&#19981;&#20165;&#20351;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#32473;&#26368;&#32456;&#29992;&#25143;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#65292;&#32780;&#19988;&#36824;&#38459;&#30861;&#20102;&#27169;&#22411;&#35774;&#35745;&#32773;&#23545;&#20013;&#38388;&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InterpreTabNet&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#25913;&#36827;&#30340;&#27880;&#24847;&#27169;&#22359;&#65292;&#25913;&#36827;TabNet&#26550;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#35774;&#35745;&#30830;&#20445;&#20102;&#24378;&#22823;&#30340;&#26799;&#24230;&#20256;&#25773;&#21644;&#35745;&#31639;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#35780;&#20215;&#25351;&#26631;InterpreStability&#65292;&#35813;&#25351;&#26631;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#31283;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#25351;&#26631;&#26631;&#24535;&#30528;&#21487;&#35299;&#37322;&#27169;&#22411;&#30740;&#31350;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Artificial Intelligence (AI) integrates deeper into diverse sectors, the quest for powerful models has intensified. While significant strides have been made in boosting model capabilities and their applicability across domains, a glaring challenge persists: many of these state-of-the-art models remain as black boxes. This opacity not only complicates the explanation of model decisions to end-users but also obstructs insights into intermediate processes for model designers. To address these challenges, we introduce InterpreTabNet, a model designed to enhance both classification accuracy and interpretability by leveraging the TabNet architecture with an improved attentive module. This design ensures robust gradient propagation and computational stability. Additionally, we present a novel evaluation metric, InterpreStability, which quantifies the stability of a model's interpretability. The proposed model and metric mark a significant stride forward in explainable models' research, set
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#21495;&#33258;&#36866;&#24212;&#30340;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;Stockwell&#21464;&#25442;&#23618;&#36827;&#34892;&#40831;&#36718;&#20256;&#24863;&#22120;&#25968;&#25454;&#21387;&#32553;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#21270;&#23618;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#23569;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.02862</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#31163;&#25955;&#20313;&#24358;Stockwell&#21464;&#25442;&#23618;&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#40831;&#36718;&#20256;&#24863;&#22120;&#25968;&#25454;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
A novel asymmetrical autoencoder with a sparsifying discrete cosine Stockwell transform layer for gearbox sensor data compression. (arXiv:2310.02862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02862
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#21495;&#33258;&#36866;&#24212;&#30340;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;Stockwell&#21464;&#25442;&#23618;&#36827;&#34892;&#40831;&#36718;&#20256;&#24863;&#22120;&#25968;&#25454;&#21387;&#32553;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#21270;&#23618;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#23569;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#25509;&#35302;&#24335;&#40831;&#36718;&#25925;&#38556;&#35786;&#26029;&#38382;&#39064;&#20013;&#65292;&#32570;&#20047;&#39640;&#25928;&#30340;&#25968;&#25454;&#21387;&#32553;&#27169;&#22411;&#20173;&#28982;&#26159;&#26080;&#32447;&#20256;&#36755;&#40831;&#36718;&#25968;&#25454;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#21495;&#33258;&#36866;&#24212;&#30340;&#38750;&#23545;&#31216;&#33258;&#32534;&#30721;&#22120;&#65292;&#20854;&#20013;&#22686;&#21152;&#20102;&#19968;&#20010;&#21464;&#25442;&#22495;&#23618;&#26469;&#21387;&#32553;&#20256;&#24863;&#22120;&#20449;&#21495;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#20313;&#24358;Stockwell&#21464;&#25442;&#65288;DCST&#65289;&#23618;&#20197;&#26367;&#20195;&#22810;&#23618;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#32447;&#24615;&#23618;&#12290;&#36890;&#36807;&#21033;&#29992;&#21367;&#31215;&#30340;&#20056;&#27861;&#29305;&#24615;&#22312;DCST&#22495;&#23454;&#29616;&#20102;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#12290;&#28982;&#21518;&#65292;&#24212;&#29992;&#21487;&#35757;&#32451;&#30340;&#30828;&#38408;&#20540;&#21270;&#23618;&#26469;&#20943;&#23569;DCST&#23618;&#20013;&#30340;&#20887;&#20313;&#25968;&#25454;&#20197;&#20351;&#29305;&#24449;&#22270;&#31232;&#30095;&#21270;&#12290;&#19982;&#32447;&#24615;&#23618;&#30456;&#27604;&#65292;DCST&#23618;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#31232;&#30095;&#21270;&#30340;DCST&#23618;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24247;&#28037;&#29380;&#26684;&#22823;&#23398;&#65288;Uo...[&#34987;&#25130;&#26029;]
&lt;/p&gt;
&lt;p&gt;
The lack of an efficient compression model remains a challenge for the wireless transmission of gearbox data in non-contact gear fault diagnosis problems. In this paper, we present a signal-adaptive asymmetrical autoencoder with a transform domain layer to compress sensor signals. First, a new discrete cosine Stockwell transform (DCST) layer is introduced to replace linear layers in a multi-layer autoencoder. A trainable filter is implemented in the DCST domain by utilizing the multiplication property of the convolution. A trainable hard-thresholding layer is applied to reduce redundant data in the DCST layer to make the feature map sparse. In comparison to the linear layer, the DCST layer reduces the number of trainable parameters and improves the accuracy of data reconstruction. Second, training the autoencoder with a sparsifying DCST layer only requires a small number of datasets. The proposed method is superior to other autoencoder-based methods on the University of Connecticut (Uo
&lt;/p&gt;</description></item><item><title>&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.02861</link><description>&lt;p&gt;
Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02861
&lt;/p&gt;
&lt;p&gt;
&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#37238;&#39044;&#27979;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#24322;&#24120;&#30340;&#28508;&#22312;&#23646;&#24615;&#65292;&#23548;&#33268;&#26694;&#26550;&#35774;&#35745;&#19981;&#21487;&#35299;&#37322;&#21644;&#24615;&#33021;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#37325;&#26032;&#30740;&#31350;&#20102;&#24322;&#24120;&#21644;&#27491;&#24120;&#22270;&#20043;&#38388;&#30340;&#20809;&#35889;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31867;&#20043;&#38388;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#20449;&#21495;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#21487;&#20197;&#29992;&#20854;&#29790;&#21033;&#21830;&#34920;&#31034;&#65292;&#36825;&#34920;&#26126;&#29790;&#21033;&#21830;&#26159;&#22270;&#24322;&#24120;&#23646;&#24615;&#30340;&#19968;&#20010;&#39537;&#21160;&#22240;&#32032;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rayleigh Quotient Graph Neural Network&#65288;RQGNN&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#20809;&#35889;GNN&#65292;&#20026;&#25506;&#32034;&#24322;&#24120;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02842</link><description>&lt;p&gt;
&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#25195;&#25551;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26377;&#33021;&#21147;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#25688;&#35201;&#21644;&#25968;&#23398;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#26159;&#38024;&#23545;&#21333;&#19968;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#24403;&#21069;&#36235;&#21183;&#26159;&#20351;&#29992;&#25552;&#31034;&#25351;&#23548;&#35843;&#33410;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#20197;&#36866;&#24212;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#25193;&#23637;&#25552;&#31034;&#35843;&#33410;&#20197;&#21516;&#26102;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26159;&#19968;&#20010;&#24191;&#27867;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;"&#28151;&#21512;&#25552;&#31034;"&#25110;MoPs&#65292;&#24182;&#32467;&#21512;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65306;&#21518;&#32773;&#30340;&#35774;&#35745;&#26159;&#26412;&#25991;&#30340;&#36129;&#29486;&#20043;&#19968;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;(&#21363;&#19968;&#32452;&#25552;&#31034;)&#12290;&#27492;&#22806;&#65292;MoPs&#22312;&#24212;&#29992;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26102;&#37117;&#19981;&#21463;&#24433;&#21709;&#8212;&#8212;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#24577;&#25552;&#39640;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#27169;&#24577;&#24341;&#23548;&#65288;CMG&#65289;&#65292;&#23427;&#21253;&#25324;&#36328;&#27169;&#24577;&#29109;&#20943;&#23569;&#65288;CMER&#65289;&#21644;&#36328;&#27169;&#24577;&#32447;&#24615;&#23884;&#20837;&#65288;CMLE&#65289;&#65292;&#20998;&#21035;&#35299;&#20915;&#20102;&#20887;&#20313;&#20449;&#24687;&#21644;&#31232;&#30095;&#31354;&#38388;&#38382;&#39064;&#12290;CMER&#36890;&#36807;&#36974;&#30422;&#22270;&#20687;&#30340;&#19968;&#37096;&#20998;&#24182;&#35745;&#31639;&#19982;&#25991;&#26412;&#30340;&#21305;&#37197;&#20998;&#25968;&#65292;&#20351;&#26816;&#27979;&#22120;&#32858;&#28966;&#20110;&#20851;&#38190;&#20869;&#23481;&#12290;CMLE&#23398;&#20064;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#32467;&#26500;&#30697;&#38453;&#26469;&#23398;&#20064;&#26356;&#32039;&#20945;&#30340;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#28508;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.02821</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#24577;&#25351;&#23548;&#25552;&#39640;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Vision Anomaly Detection with the Guidance of Language Modality. (arXiv:2310.02821v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#24577;&#25552;&#39640;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#27169;&#24577;&#24341;&#23548;&#65288;CMG&#65289;&#65292;&#23427;&#21253;&#25324;&#36328;&#27169;&#24577;&#29109;&#20943;&#23569;&#65288;CMER&#65289;&#21644;&#36328;&#27169;&#24577;&#32447;&#24615;&#23884;&#20837;&#65288;CMLE&#65289;&#65292;&#20998;&#21035;&#35299;&#20915;&#20102;&#20887;&#20313;&#20449;&#24687;&#21644;&#31232;&#30095;&#31354;&#38388;&#38382;&#39064;&#12290;CMER&#36890;&#36807;&#36974;&#30422;&#22270;&#20687;&#30340;&#19968;&#37096;&#20998;&#24182;&#35745;&#31639;&#19982;&#25991;&#26412;&#30340;&#21305;&#37197;&#20998;&#25968;&#65292;&#20351;&#26816;&#27979;&#22120;&#32858;&#28966;&#20110;&#20851;&#38190;&#20869;&#23481;&#12290;CMLE&#23398;&#20064;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#32467;&#26500;&#30697;&#38453;&#26469;&#23398;&#20064;&#26356;&#32039;&#20945;&#30340;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#28508;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#22788;&#29702;&#24037;&#19994;&#32570;&#38519;&#26816;&#27979;&#12289;&#20107;&#20214;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#24322;&#24120;&#26816;&#27979;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#35270;&#35273;&#27169;&#24577;&#30340;&#65292;&#30001;&#20110;&#20887;&#20313;&#20449;&#24687;&#21644;&#31232;&#30095;&#28508;&#31354;&#38388;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35821;&#35328;&#27169;&#24577;&#30001;&#20110;&#30456;&#23545;&#21333;&#19968;&#30340;&#25968;&#25454;&#32780;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#20174;&#22810;&#27169;&#24577;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#19978;&#36848;&#35270;&#35273;&#27169;&#24577;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#27169;&#24577;&#24341;&#23548;&#65288;CMG&#65289;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#29109;&#20943;&#23569;&#65288;CMER&#65289;&#21644;&#36328;&#27169;&#24577;&#32447;&#24615;&#23884;&#20837;&#65288;CMLE&#65289;&#65292;&#20998;&#21035;&#35299;&#20915;&#20102;&#20887;&#20313;&#20449;&#24687;&#21644;&#31232;&#30095;&#31354;&#38388;&#38382;&#39064;&#12290;CMER&#36890;&#36807;&#36974;&#30422;&#21407;&#22987;&#22270;&#20687;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#35745;&#31639;&#19982;&#25991;&#26412;&#30340;&#21305;&#37197;&#20998;&#25968;&#12290;&#28982;&#21518;&#65292;CMER&#20002;&#24323;&#19981;&#30456;&#20851;&#30340;&#20687;&#32032;&#65292;&#20351;&#26816;&#27979;&#22120;&#32858;&#28966;&#20110;&#20851;&#38190;&#20869;&#23481;&#12290;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#26356;&#32039;&#20945;&#30340;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#28508;&#31354;&#38388;&#65292;CMLE&#23398;&#20064;&#20102;&#19968;&#20010;&#30456;&#20851;&#32467;&#26500;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a surge of interest in anomaly detection for tackling industrial defect detection, event detection, etc. However, existing unsupervised anomaly detectors, particularly those for the vision modality, face significant challenges due to redundant information and sparse latent space. Conversely, the language modality performs well due to its relatively single data. This paper tackles the aforementioned challenges for vision modality from a multimodal point of view. Specifically, we propose Cross-modal Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue and sparse space issue, respectively. CMER masks parts of the raw image and computes the matching score with the text. Then, CMER discards irrelevant pixels to make the detector focus on critical contents. To learn a more compact latent space for the vision anomaly detector, CMLE learns a correlation structure matrix f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#20102;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.02812</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;: &#23545;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms. (arXiv:2310.02812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#20102;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#21644;&#24863;&#30693;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21046;&#36896;&#19994;&#27491;&#22312;&#25910;&#38598;&#22823;&#37327;&#21508;&#31181;&#21508;&#26679;&#30340;&#25968;&#25454;&#12290;&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479; (SMS) &#29615;&#22659;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867; (TSC) &#22312;&#35813;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23545;&#21046;&#36896;&#19994;&#21644;&#24037;&#19994;&#29615;&#22659;&#20013; TSC &#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#35780;&#20272;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312; TSC &#21644;&#21046;&#36896;&#19994;&#25991;&#29486;&#20013;&#25506;&#32034;&#21644;&#32534;&#21046;&#20102;&#19968;&#20221;&#21253;&#21547;&#36229;&#36807;92&#20010;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20840;&#38754;&#21015;&#34920;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#35813;&#21015;&#34920;&#20013;&#36873;&#25321;&#20102;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;36&#20010;&#31639;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#22312;&#21508;&#31181;&#21046;&#36896;&#19994;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#32452;&#21253;&#21547;22&#20010;&#21046;&#36896;&#19994;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#21046;&#36896;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#21046;&#36896;&#19994;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing is gathering extensive amounts of diverse data, thanks to the growing number of sensors and rapid advances in sensing technologies. Among the various data types available in SMS settings, time-series data plays a pivotal role. Hence, TSC emerges is crucial in this domain. The objective of this study is to fill this gap by providing a rigorous experimental evaluation of the SoTA ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We first explored and compiled a comprehensive list of more than 92 SoTA algorithms from both TSC and manufacturing literature. Following, we selected the 36 most representative algorithms from this list. To evaluate their performance across various manufacturing classification tasks, we curated a set of 22 manufacturing datasets, representative of different characteristics that cover diverse manufacturing problems. Subsequently, we implemented and evaluated the algorithms on the manufacturing benchmark datasets, and analy
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#24615;&#29615;&#22659;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26356;&#26032;&#35268;&#21017;&#21644;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#26469;&#25552;&#39640;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#36817;&#20284;&#26041;&#27861;&#65292;&#21517;&#20026;&#31639;&#27861;&#36951;&#25022;&#65288;AR&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.02782</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#24615;&#29615;&#22659;&#35774;&#35745;&#25506;&#32034;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design. (arXiv:2310.02782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02782
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#29615;&#22659;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26356;&#26032;&#35268;&#21017;&#21644;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#26469;&#25552;&#39640;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#36817;&#20284;&#26041;&#27861;&#65292;&#21517;&#20026;&#31639;&#27861;&#36951;&#25022;&#65288;AR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36825;&#20123;&#36827;&#23637;&#26159;&#30001;&#20154;&#31867;&#30740;&#31350;&#20154;&#21592;&#25163;&#21160;&#35774;&#35745;&#30340;&#31639;&#27861;&#25512;&#21160;&#30340;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20803;&#23398;&#20064;&#26356;&#26032;&#35268;&#21017;&#65292;&#24076;&#26395;&#21457;&#29616;&#22312;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#31639;&#27861;&#12290;&#23613;&#31649;&#20687;&#23398;&#20064;&#31574;&#30053;&#26799;&#24230;&#65288;LPG&#65289;&#36825;&#26679;&#30340;&#31639;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#26159;&#24403;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#26102;&#20173;&#23384;&#22312;&#27867;&#21270;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20803;&#35757;&#32451;&#20998;&#24067;&#30340;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20511;&#37492;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#20803;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#36951;&#25022;&#65292;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#36817;&#20284;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31639;&#27861;&#36951;&#25022;&#65288;AR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#29615;&#22659;&#35774;&#35745;&#33719;&#24471;&#30340;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has seen vast progress in deep reinforcement learning (RL) on the back of algorithms manually designed by human researchers. Recently, it has been shown that it is possible to meta-learn update rules, with the hope of discovering algorithms that can perform well on a wide range of RL tasks. Despite impressive initial results from algorithms such as Learned Policy Gradient (LPG), there remains a generalization gap when these algorithms are applied to unseen environments. In this work, we examine how characteristics of the meta-training distribution impact the generalization performance of these algorithms. Motivated by this analysis and building on ideas from Unsupervised Environment Design (UED), we propose a novel approach for automatically generating curricula to maximize the regret of a meta-learned optimizer, in addition to a novel approximation of regret, which we name algorithmic regret (AR). The result is our method, General RL Optimizers Obtained Via Environment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;UMLS&#30340;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#30340;&#20107;&#23454;&#24615;&#12290;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#21307;&#29983;&#35780;&#20272;&#65292;&#30740;&#31350;&#20154;&#21592;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02778</link><description>&lt;p&gt;
&#19968;&#20010;&#22686;&#24378;&#30340; UMLS &#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare. (arXiv:2310.02778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02778
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;UMLS&#30340;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#30340;&#20107;&#23454;&#24615;&#12290;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#21307;&#29983;&#35780;&#20272;&#65292;&#30740;&#31350;&#20154;&#21592;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#30495;&#23454;&#20020;&#24202;&#22330;&#26223;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#19982;&#24050;&#24314;&#31435;&#21307;&#23398;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#65292;&#29978;&#33267;&#21487;&#33021;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#30340;&#22686;&#24378;&#22411;LLM&#26694;&#26550;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#26381;&#21153;&#21307;&#30103;&#20445;&#20581;&#31038;&#21306;&#12290;&#25105;&#20204;&#37319;&#29992;LLaMa2-13b-chat&#21644;ChatGPT-3.5&#20316;&#20026;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;ROUGE&#20998;&#25968;&#21644;BERT&#20998;&#25968;&#22312;LiveQA&#27979;&#35797;&#38598;&#30340;104&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26681;&#25454;&#20107;&#23454;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#35835;&#24615;&#21644;&#30456;&#20851;&#24615;&#22235;&#20010;&#32500;&#24230;&#24314;&#31435;&#20102;&#21307;&#29983;&#35780;&#20272;&#26631;&#20934;&#12290;ChatGPT-3.5&#29992;&#20110;&#21307;&#29983;&#35780;&#20272;&#65292;&#38024;&#23545;LiveQA&#27979;&#35797;&#38598;&#30340;20&#20010;&#38382;&#39064;&#12290;&#22810;&#20301;&#20303;&#38498;&#21307;&#24072;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conduct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02772</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks. (arXiv:2310.02772v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#12290;&#24050;&#30693;SNNs&#20855;&#26377;&#39640;&#33021;&#25928;&#20294;&#38590;&#20197;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;&#35768;&#22810;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#38388;&#19978;&#30340;&#22312;&#32447;&#35757;&#32451;&#65288;OTTT&#65289;&#26159;&#19968;&#31181;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25233;&#21046;&#20869;&#23384;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;GPU&#19978;&#39640;&#25928;&#35745;&#31639;&#65292;OTTT&#38656;&#35201;&#36827;&#34892;&#33033;&#20914;&#24207;&#21015;&#25805;&#20316;&#21644;&#33033;&#20914;&#24207;&#21015;&#21152;&#26435;&#27714;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;OTTT&#19982;Spike Representation&#65288;&#21478;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65289;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#65292;&#20294;&#19982;Spike Representation&#30340;&#29702;&#35770;&#19968;&#33268;&#24615;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21363;SAF&#21487;&#20197;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20943;&#23569;&#19968;&#21322;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;SAF&#20998;&#21035;&#19982;Spike Representation&#21644;OTTT&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#35748;&#20102;......
&lt;/p&gt;
&lt;p&gt;
In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#22810;&#26679;&#24615;&#24378;&#12289;&#21487;&#25511;&#21046;&#30340;&#36924;&#30495;&#19977;&#32500;&#20154;&#22836;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#30340;&#32593;&#32476;&#35774;&#35745;&#12290;&#26041;&#27861;&#21253;&#25324;&#20960;&#20309;&#29983;&#25104;&#22120;&#12289;&#28210;&#26579;&#22270;&#29983;&#25104;&#22120;&#21644;&#39068;&#33394;&#21464;&#25442;&#27169;&#22411;&#12290;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#29420;&#29305;&#24615;&#21644;&#26032;&#39062;&#24615;&#30340;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.02753</link><description>&lt;p&gt;
MUNCH: &#24314;&#27169;&#29420;&#29305;&#19988;&#21487;&#25511;&#21046;&#30340;&#22836;&#37096;
&lt;/p&gt;
&lt;p&gt;
MUNCH: Modelling Unique 'N Controllable Heads. (arXiv:2310.02753v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#22810;&#26679;&#24615;&#24378;&#12289;&#21487;&#25511;&#21046;&#30340;&#36924;&#30495;&#19977;&#32500;&#20154;&#22836;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#30340;&#32593;&#32476;&#35774;&#35745;&#12290;&#26041;&#27861;&#21253;&#25324;&#20960;&#20309;&#29983;&#25104;&#22120;&#12289;&#28210;&#26579;&#22270;&#29983;&#25104;&#22120;&#21644;&#39068;&#33394;&#21464;&#25442;&#27169;&#22411;&#12290;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#29420;&#29305;&#24615;&#21644;&#26032;&#39062;&#24615;&#30340;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#33258;&#21160;&#29983;&#25104;&#19977;&#32500;&#20154;&#22836;&#19968;&#30452;&#26159;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#21512;&#25104;&#36924;&#30495;&#30340;&#35282;&#33394;&#65292;&#20294;&#23545;&#20110;&#29983;&#25104;&#32467;&#26524;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#30340;&#25511;&#21046;&#26377;&#38480;&#65292;&#24182;&#19988;&#24418;&#29366;&#21644;&#32441;&#29702;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20063;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#12289;&#25511;&#21046;&#21644;&#36924;&#30495;&#24615;&#26041;&#38754;&#37117;&#20855;&#22791;&#20102;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32593;&#32476;&#35774;&#35745;&#65292;&#23545;&#20110;&#28216;&#25103;&#35774;&#35745;&#33402;&#26415;&#23478;&#26469;&#35828;&#36825;&#20123;&#37117;&#26159;&#29702;&#24819;&#30340;&#29305;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20960;&#20309;&#29983;&#25104;&#22120;&#21487;&#20197;&#35782;&#21035;&#33073;&#32806;&#30340;&#28508;&#22312;&#26041;&#21521;&#24182;&#29983;&#25104;&#26032;&#39062;&#19988;&#22810;&#26679;&#30340;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#28210;&#26579;&#22270;&#29983;&#25104;&#22120;&#23398;&#20064;&#21512;&#25104;&#22810;&#20010;&#39640;&#20445;&#30495;&#24230;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#28210;&#26579;&#22270;&#65292;&#21253;&#25324;&#21453;&#29031;&#29575;&#12289;&#20809;&#27901;&#24230;&#12289;&#38236;&#38754;&#21453;&#23556;&#21644;&#27861;&#32447;&#26041;&#21521;&#12290;&#23545;&#20110;&#26356;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#36755;&#20986;&#30340;&#33402;&#26415;&#23478;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39068;&#33394;&#21464;&#25442;&#27169;&#22411;&#65292;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#39068;&#33394;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#37327;&#21270;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#29420;&#29305;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automated generation of 3D human heads has been an intriguing and challenging task for computer vision researchers. Prevailing methods synthesize realistic avatars but with limited control over the diversity and quality of rendered outputs and suffer from limited correlation between shape and texture of the character. We propose a method that offers quality, diversity, control, and realism along with explainable network design, all desirable features to game-design artists in the domain. First, our proposed Geometry Generator identifies disentangled latent directions and generate novel and diverse samples. A Render Map Generator then learns to synthesize multiply high-fidelty physically-based render maps including Albedo, Glossiness, Specular, and Normals. For artists preferring fine-grained control over the output, we introduce a novel Color Transformer Model that allows semantic color control over generated maps. We also introduce quantifiable metrics called Uniqueness and Novelt
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#35748;&#20026;&#27431;&#30431;AI&#27861;&#26696;&#23545;AI&#31995;&#32479;&#30340;&#36136;&#37327;&#20445;&#35777;&#26041;&#24335;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#25351;&#20986;&#22522;&#20110;&#32479;&#35745;&#23398;&#26377;&#25928;&#27979;&#35797;&#21450;&#20934;&#30830;&#23450;&#20041;&#24212;&#29992;&#26159;&#30830;&#20445;AI&#31995;&#32479;&#21151;&#33021;&#21487;&#20449;&#24230;&#30340;&#26680;&#24515;&#12290;</title><link>http://arxiv.org/abs/2310.02727</link><description>&lt;p&gt;
AI&#31995;&#32479;&#30340;&#21151;&#33021;&#21487;&#20449;&#24230;&#36890;&#36807;&#32479;&#35745;&#23398;&#26377;&#25928;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Functional trustworthiness of AI systems by statistically valid testing. (arXiv:2310.02727v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02727
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#35748;&#20026;&#27431;&#30431;AI&#27861;&#26696;&#23545;AI&#31995;&#32479;&#30340;&#36136;&#37327;&#20445;&#35777;&#26041;&#24335;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#25351;&#20986;&#22522;&#20110;&#32479;&#35745;&#23398;&#26377;&#25928;&#27979;&#35797;&#21450;&#20934;&#30830;&#23450;&#20041;&#24212;&#29992;&#26159;&#30830;&#20445;AI&#31995;&#32479;&#21151;&#33021;&#21487;&#20449;&#24230;&#30340;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#20851;&#27880;&#27431;&#27954;&#20844;&#27665;&#30340;&#23433;&#20840;&#12289;&#20581;&#24247;&#21644;&#26435;&#30410;&#38382;&#39064;&#65292;&#22240;&#20026;&#24403;&#21069;&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27861;&#26696;&#30340;&#33609;&#26696;&#23545;AI&#31995;&#32479;&#30340;&#31526;&#21512;&#24615;&#35780;&#20272;&#25152;&#38656;&#30340;&#25514;&#26045;&#21644;&#31243;&#24207;&#19981;&#36275;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#27431;&#30431;AI&#27861;&#26696;&#30340;&#24403;&#21069;&#33609;&#26696;&#20197;&#21450;&#22312;CEN/CENELEC&#36827;&#34892;&#30340;&#37197;&#22871;&#26631;&#20934;&#21270;&#24037;&#20316;&#65292;&#37117;&#37319;&#21462;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;AI&#31995;&#32479;&#30340;&#23454;&#38469;&#21151;&#33021;&#20445;&#35777;&#20284;&#20046;&#26159;&#19981;&#20999;&#23454;&#38469;&#19988;&#36807;&#20110;&#22797;&#26434;&#30340;&#12290;&#28982;&#32780;&#65292;&#21046;&#23450;&#19968;&#20010;&#31526;&#21512;&#24615;&#35780;&#20272;&#31243;&#24207;&#65292;&#20351;&#26410;&#32463;&#20805;&#20998;&#35780;&#20272;&#30340;AI&#31995;&#32479;&#20135;&#29983;&#34394;&#20551;&#30340;&#20449;&#20219;&#24187;&#35937;&#65292;&#20805;&#20854;&#37327;&#26159;&#24188;&#31258;&#30340;&#65292;&#20805;&#20854;&#26356;&#31967;&#30340;&#24773;&#20917;&#26159;&#20005;&#37325;&#30095;&#24573;&#30340;&#12290;&#22240;&#27492;&#65292;&#27431;&#30431;AI&#27861;&#26696;&#38169;&#36807;&#20102;&#30830;&#20445;&#36890;&#36807;&#21151;&#33021;&#21487;&#20449;&#24230;&#26469;&#30830;&#20445;&#36136;&#37327;&#21644;&#27491;&#30830;&#20998;&#37197;&#36131;&#20219;&#30340;&#30446;&#30340;&#12290;AI&#20915;&#31574;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#39318;&#20808;&#22312;&#20110;&#23545;&#38543;&#26426;&#36873;&#25321;&#30340;&#26679;&#26412;&#36827;&#34892;&#27491;&#30830;&#30340;&#32479;&#35745;&#27979;&#35797;&#65292;&#24182;&#22312;&#23450;&#20041;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The authors are concerned about the safety, health, and rights of the European citizens due to inadequate measures and procedures required by the current draft of the EU Artificial Intelligence (AI) Act for the conformity assessment of AI systems. We observe that not only the current draft of the EU AI Act, but also the accompanying standardization efforts in CEN/CENELEC, have resorted to the position that real functional guarantees of AI systems supposedly would be unrealistic and too complex anyways. Yet enacting a conformity assessment procedure that creates the false illusion of trust in insufficiently assessed AI systems is at best naive and at worst grossly negligent. The EU AI Act thus misses the point of ensuring quality by functional trustworthiness and correctly attributing responsibilities.  The trustworthiness of an AI decision system lies first and foremost in the correct statistical testing on randomly selected samples and in the precision of the definition of the applica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#27169;&#22411;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02717</link><description>&lt;p&gt;
&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#22312;&#32447;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Clustering of Bandits with Misspecified User Models. (arXiv:2310.02717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#27169;&#22411;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#27599;&#36718;&#20013;&#65292;&#32473;&#23450;&#33218;&#30340;&#29305;&#24449;&#65292;&#23398;&#20064;&#20195;&#29702;&#36873;&#25321;&#19968;&#20010;&#33218;&#26469;&#26368;&#22823;&#21270;&#38271;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31995;&#21015;&#24037;&#20316;&#65292;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#24182;&#22312;&#32463;&#20856;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#27491;&#30830;&#35268;&#23450;&#32447;&#24615;&#29992;&#25143;&#27169;&#22411;&#65292;&#24403;&#36825;&#20010;&#20851;&#38190;&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#65292;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#22914;&#20309;&#20026;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#35774;&#35745;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#27169;&#22411;&#20013;&#30340;&#26399;&#26395;&#22870;&#21169;&#21487;&#33021;&#26377;&#20559;&#24046;&#65292;&#19981;&#26159;&#23436;&#32654;&#30340;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCLUMB&#21644;RSCLUMB&#65288;&#20998;&#21035;&#29992;&#21160;&#24577;&#22270;&#21644;&#38598;&#21512;&#34920;&#31034;&#23398;&#20064;&#21040;&#30340;&#32858;&#31867;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, resp
&lt;/p&gt;</description></item><item><title>scHyena&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#21333;&#32454;&#32990;Hyena(scHyena)&#65292;&#26088;&#22312;&#22788;&#29702;&#22823;&#33041;&#20013;&#30340;&#20840;&#38271;scRNA-seq&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02713</link><description>&lt;p&gt;
scHyena: &#22522;&#20110;&#20840;&#38271;&#21333;&#32454;&#32990;RNA-Seq&#30340;&#22823;&#33041;&#20998;&#26512;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain. (arXiv:2310.02713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02713
&lt;/p&gt;
&lt;p&gt;
scHyena&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#21333;&#32454;&#32990;Hyena(scHyena)&#65292;&#26088;&#22312;&#22788;&#29702;&#22823;&#33041;&#20013;&#30340;&#20840;&#38271;scRNA-seq&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;RNA&#27979;&#24207;(scRNA-seq)&#22312;&#25581;&#31034;&#22797;&#26434;&#32452;&#32455;&#20013;&#24494;&#22937;&#30340;&#32454;&#32990;&#22810;&#26679;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#22312;&#22823;&#33041;&#20013;&#23588;&#20026;&#20851;&#38190;&#65292;&#22240;&#20026;&#22823;&#33041;&#27604;&#20854;&#20182;&#32452;&#32455;&#31867;&#22411;&#26377;&#26356;&#22810;&#31181;&#31867;&#30340;&#32454;&#32990;&#65292;&#20197;&#20415;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#19981;&#21516;&#32454;&#32990;&#29615;&#22659;&#19979;&#30340;&#22823;&#33041;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#22833;&#20107;&#20214;&#25152;&#20135;&#29983;&#30340;&#22266;&#26377;&#27979;&#37327;&#22122;&#22768;&#20197;&#21450;&#23545;&#22823;&#37327;&#22522;&#22240;&#34920;&#36798;&#20449;&#24687;&#30340;&#26377;&#38480;&#21033;&#29992;&#65292;&#20998;&#26512;scRNA-seq&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;scHyena&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#24182;&#25552;&#39640;&#22823;&#33041;&#20013;scRNA-seq&#20998;&#26512;&#20934;&#30830;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21463;&#21040;&#26368;&#36817;&#30340;Hyena&#31639;&#23376;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#26550;&#26500;&#65292;&#31216;&#20026;&#21333;&#32454;&#32990;Hyena(scHyena)&#65292;&#23427;&#37197;&#22791;&#20102;&#32447;&#24615;&#36866;&#37197;&#22120;&#23618;&#12289;&#36890;&#36807;&#22522;&#22240;&#23884;&#20837;&#23454;&#29616;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#19968;&#20010;&#21452;&#21521;Hyena&#31639;&#23376;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22788;&#29702;&#20840;&#38271;&#30340;scRNA-seq&#25968;&#25454;&#32780;&#19981;&#20002;&#22833;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-cell RNA sequencing (scRNA-seq) has made significant strides in unraveling the intricate cellular diversity within complex tissues. This is particularly critical in the brain, presenting a greater diversity of cell types than other tissue types, to gain a deeper understanding of brain function within various cellular contexts. However, analyzing scRNA-seq data remains a challenge due to inherent measurement noise stemming from dropout events and the limited utilization of extensive gene expression information. In this work, we introduce scHyena, a foundation model designed to address these challenges and enhance the accuracy of scRNA-seq analysis in the brain. Specifically, inspired by the recent Hyena operator, we design a novel Transformer architecture called singe-cell Hyena (scHyena) that is equipped with a linear adaptor layer, the positional encoding via gene-embedding, and a {bidirectional} Hyena operator. This enables us to process full-length scRNA-seq data without losi
&lt;/p&gt;</description></item><item><title>ED-NeRF &#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; 3D &#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22330;&#26223;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#24471;&#21040;&#26356;&#24555;&#36895;&#19988;&#26356;&#26131;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#12290;</title><link>http://arxiv.org/abs/2310.02712</link><description>&lt;p&gt;
ED-NeRF: &#20351;&#29992;&#28508;&#31354;&#38388; NeRF &#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#26412;&#24341;&#23548;&#30340; 3D &#22330;&#26223;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF. (arXiv:2310.02712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02712
&lt;/p&gt;
&lt;p&gt;
ED-NeRF &#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; 3D &#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22330;&#26223;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#24471;&#21040;&#26356;&#24555;&#36895;&#19988;&#26356;&#26131;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#20108;&#32500;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#36827;&#23637;&#24050;&#32463;&#25193;&#23637;&#21040;&#19977;&#32500;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#26032;&#30340;&#19977;&#32500;&#23545;&#35937;&#12290;&#36825;&#28436;&#21464;&#25104;&#20102; NeRF &#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#20801;&#35768;&#23545;&#29616;&#26377;&#30340;&#19977;&#32500;&#23545;&#35937;&#36827;&#34892;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; NeRF &#32534;&#36753;&#25216;&#26415;&#22312;&#24615;&#33021;&#19978;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#35757;&#32451;&#36895;&#24230;&#24930;&#21644;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#20805;&#20998;&#32771;&#34385;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; 3D NeRF &#32534;&#36753;&#26041;&#27861;&#65292;&#31216;&#20026; ED-NeRF&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#25104;&#21151;&#23884;&#20837;&#21040;&#28508;&#25193;&#25955;&#27169;&#22411; (LDM) &#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32454;&#21270;&#23618;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#19968;&#20010;&#19981;&#20165;&#26356;&#24555;&#65292;&#32780;&#19988;&#26356;&#36866;&#21512;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#65292;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#31354;&#38388; NeRF &#32534;&#36753;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss 
&lt;/p&gt;</description></item><item><title>COCONUT&#26159;&#19968;&#31181;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32463;&#39564;&#37325;&#25773;&#21644;&#23545;&#27604;&#24335;&#23398;&#20064;&#65292;&#22312;&#35821;&#38899;&#29702;&#35299;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#27169;&#22411;&#22312;&#25345;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38590;&#20197;&#20445;&#25345;&#20043;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02699</link><description>&lt;p&gt;
&#25345;&#32493;&#23545;&#27604;&#24335;&#35821;&#38899;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Spoken Language Understanding. (arXiv:2310.02699v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02699
&lt;/p&gt;
&lt;p&gt;
COCONUT&#26159;&#19968;&#31181;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32463;&#39564;&#37325;&#25773;&#21644;&#23545;&#27604;&#24335;&#23398;&#20064;&#65292;&#22312;&#35821;&#38899;&#29702;&#35299;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#27169;&#22411;&#22312;&#25345;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38590;&#20197;&#20445;&#25345;&#20043;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;&#35821;&#38899;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#31361;&#30772;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#24222;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25345;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24448;&#24448;&#38590;&#20197;&#20445;&#25345;&#20043;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#20960;&#20046;&#24635;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#19979;&#23398;&#20064;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;COCONUT&#30340;CIL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#32463;&#39564;&#37325;&#25773;&#21644;&#23545;&#27604;&#24335;&#23398;&#20064;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#23545;&#20165;&#23545;&#22238;&#25918;&#26679;&#26412;&#24212;&#29992;&#25913;&#36827;&#29256;&#26412;&#30340;&#26631;&#20934;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#65292;COCONUT&#36890;&#36807;&#23558;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#25289;&#36817;&#24182;&#23558;&#20854;&#20182;&#26679;&#26412;&#25512;&#24320;&#65292;&#20445;&#30041;&#20102;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22270;&#21305;&#37197;&#26469;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#65292;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#20197;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02692</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22270;&#21305;&#37197;&#26469;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Domain Gap by Clustering-based Image-Text Graph Matching. (arXiv:2310.02692v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02692
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22270;&#21305;&#37197;&#26469;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#65292;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#20197;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#23545;&#20110;&#35757;&#32451;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30446;&#26631;&#20219;&#21153;&#39046;&#22495;&#30340;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#25991;&#26412;&#25551;&#36848;&#26412;&#36523;&#21253;&#21547;&#27010;&#24565;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#36825;&#26679;&#30340;&#36741;&#21161;&#35821;&#20041;&#32447;&#32034;&#21487;&#20197;&#29992;&#20316;&#39046;&#22495;&#27010;&#25324;&#38382;&#39064;&#30340;&#26377;&#25928;&#26530;&#32445;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#34701;&#21512;&#30340;&#22270;&#34920;&#31034;&#26469;&#33719;&#24471;&#22312;&#23616;&#37096;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#31526;&#20043;&#38388;&#32771;&#34385;&#20869;&#22312;&#35821;&#20041;&#32467;&#26500;&#30340;&#39046;&#22495;&#19981;&#21464;&#26530;&#32445;&#23884;&#20837;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;(i)&#29992;&#22270;&#34920;&#31034;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#65292;&#20197;&#21450;(ii)&#23558;&#22522;&#20110;&#22270;&#20687;&#33410;&#28857;&#29305;&#24449;&#30340;&#32858;&#31867;&#21644;&#21305;&#37197;&#24212;&#29992;&#21040;&#25991;&#26412;&#22270;&#20013;&#65292;&#26469;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;CUB-DG&#21644;DomainBed&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#20986;&#29256;&#21518;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning domain-invariant representations is important to train a model that can generalize well to unseen target task domains. Text descriptions inherently contain semantic structures of concepts and such auxiliary semantic cues can be used as effective pivot embedding for domain generalization problems. Here, we use multimodal graph representations, fusing images and text, to get domain-invariant pivot embeddings by considering the inherent semantic structure between local images and text descriptors. Specifically, we aim to learn domain-invariant features by (i) representing the image and text descriptions with graphs, and by (ii) clustering and matching the graph-based image node features into textual graphs simultaneously. We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. Our code will be publicly available upon publication.
&lt;/p&gt;</description></item><item><title>USB-NeRF&#26159;&#19968;&#31181;&#35299;&#20915;&#28378;&#21160;&#24555;&#38376;&#30456;&#26426;&#38382;&#39064;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#33021;&#22815;&#32416;&#27491;&#28378;&#21160;&#24555;&#38376;&#22833;&#30495;&#24182;&#24674;&#22797;&#20934;&#30830;&#30340;&#30456;&#26426;&#36816;&#21160;&#36712;&#36857;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;RS&#25928;&#24212;&#21435;&#38500;&#21644;&#26032;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02687</link><description>&lt;p&gt;
USB-NeRF: &#35299;&#21367;&#26354;&#24555;&#38376;&#26463;&#35843;&#25972;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields. (arXiv:2310.02687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02687
&lt;/p&gt;
&lt;p&gt;
USB-NeRF&#26159;&#19968;&#31181;&#35299;&#20915;&#28378;&#21160;&#24555;&#38376;&#30456;&#26426;&#38382;&#39064;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#31639;&#27861;&#65292;&#33021;&#22815;&#32416;&#27491;&#28378;&#21160;&#24555;&#38376;&#22833;&#30495;&#24182;&#24674;&#22797;&#20934;&#30830;&#30340;&#30456;&#26426;&#36816;&#21160;&#36712;&#36857;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;RS&#25928;&#24212;&#21435;&#38500;&#21644;&#26032;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#26469;&#34920;&#31034;3D&#22330;&#26223;&#21644;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#36755;&#20837;&#22270;&#20687;&#26159;&#30001;&#20840;&#23616;&#24555;&#38376;&#30456;&#26426;&#25293;&#25668;&#30340;&#12290;&#22240;&#27492;&#65292;&#28378;&#21160;&#24555;&#38376;&#65288;RS&#65289;&#22270;&#20687;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;NeRF&#31639;&#27861;&#36827;&#34892;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;&#28378;&#21160;&#24555;&#38376;&#25928;&#24212;&#36824;&#20250;&#24433;&#21709;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#65288;&#20363;&#22914;&#36890;&#36807;COLMAP&#65289;&#65292;&#36827;&#19968;&#27493;&#38459;&#30861;&#20102;&#20351;&#29992;RS&#22270;&#20687;&#30340;NeRF&#31639;&#27861;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21367;&#26354;&#24555;&#38376;&#26463;&#35843;&#25972;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;USB-NeRF&#65289;&#12290;USB-NeRF&#33021;&#22815;&#22312;NeRF&#26694;&#26550;&#19979;&#32416;&#27491;&#28378;&#21160;&#24555;&#38376;&#22833;&#30495;&#24182;&#21516;&#26102;&#24674;&#22797;&#20934;&#30830;&#30340;&#30456;&#26426;&#36816;&#21160;&#36712;&#36857;&#65292;&#36890;&#36807;&#23545;RS&#30456;&#26426;&#30340;&#29289;&#29702;&#22270;&#20687;&#24418;&#25104;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;USB-NeRF&#30456;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;RS&#25928;&#24212;&#21435;&#38500;&#21644;&#26032;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) has received much attention recently due to its impressive capability to represent 3D scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter effect would also affect the accuracy of the camera pose estimation (e.g. via COLMAP), which further prevents the success of NeRF algorithm with RS images. In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and recover accurate camera motion trajectory simultaneously under the framework of NeRF, by modeling the physical image formation process of a RS camera. Experimental results demonstrate that USB-NeRF achieves better performance compared to prior works, in terms of RS effect removal, novel view image sy
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02679</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65306;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#20248;&#21270;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02679
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#30340;&#38543;&#26426;&#36807;&#31243;&#26469;&#27169;&#25311;&#36825;&#20123;&#30446;&#26631;&#23494;&#24230;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#35757;&#32451;&#30446;&#26631;&#38656;&#35201;&#35745;&#31639;&#23436;&#25972;&#30340;&#36712;&#36857;&#65292;&#23548;&#33268;&#30001;&#20110;&#20351;&#29992;&#23436;&#25972;&#36712;&#36857;&#21644;&#21482;&#22312;&#32456;&#31471;&#26102;&#38388;&#23384;&#22312;&#30340;&#23398;&#20064;&#20449;&#21495;&#30340;&#20351;&#29992;&#32780;&#20135;&#29983;&#32531;&#24930;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#23398;&#20064;&#36807;&#31243;&#21487;&#34892;&#22320;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#27969;&#20989;&#25968;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#29702;&#35770;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#24182;&#20174;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities. The main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional "flow function". Our method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals and benefit from off-policy exploration capabilitie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#26550;&#26500;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2310.02674</link><description>&lt;p&gt;
&#21033;&#29992;&#37197;&#23545;&#30340;OpenStreetMap&#25968;&#25454;&#21644;&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer. (arXiv:2310.02674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#26550;&#26500;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;OpenStreetMap&#65288;OSM&#65289;&#25968;&#25454;&#26159;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#30340;&#20004;&#20010;&#37325;&#35201;&#25968;&#25454;&#28304;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;OSM&#25968;&#25454;&#26469;&#36741;&#21161;&#22810;&#26102;&#26399;&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#21464;&#21270;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#26356;&#22810;&#21160;&#24577;&#22320;&#29699;&#35266;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#65288;ObjFormer&#65289;&#26550;&#26500;&#65292;&#23558;&#27969;&#34892;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#22270;&#20687;&#20998;&#26512;&#65288;OBIA&#65289;&#25216;&#26415;&#19982;&#20808;&#36827;&#30340;&#35270;&#35273;Transformer&#26550;&#26500;&#33258;&#28982;&#22320;&#32467;&#21512;&#36215;&#26469;&#12290;&#24341;&#20837;OBIA&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;ObjFormer&#20855;&#26377;&#23618;&#27425;&#20266;&#23402;&#29983;&#32534;&#30721;&#22120;&#65292;&#21253;&#21547;&#23545;&#35937;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical high-resolution imagery and OpenStreetMap (OSM) data are two important data sources for land-cover change detection. Previous studies in these two data sources focus on utilizing the information in OSM data to aid the change detection on multi-temporal optical high-resolution images. This paper pioneers the direct detection of land-cover changes utilizing paired OSM data and optical imagery, thereby broadening the horizons of change detection tasks to encompass more dynamic earth observations. To this end, we propose an object-guided Transformer (ObjFormer) architecture by naturally combining the prevalent object-based image analysis (OBIA) technique with the advanced vision Transformer architecture. The introduction of OBIA can significantly reduce the computational overhead and memory burden in the self-attention module. Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder consisting of object-guided self-attention modules that extract representative
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#21457;&#29616;&#35760;&#24518;&#21270;&#20542;&#21521;&#20110;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#21457;&#29983;&#12290;&#36890;&#36807;&#23450;&#20041;&#26377;&#25928;&#27169;&#22411;&#35760;&#24518;&#21270; (EMM) &#36825;&#19968;&#25351;&#26631;&#65292;&#37327;&#21270;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#37197;&#32622;&#23545;&#35760;&#24518;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.02664</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#35760;&#24518;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Memorization in Diffusion Models. (arXiv:2310.02664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#21457;&#29616;&#35760;&#24518;&#21270;&#20542;&#21521;&#20110;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#21457;&#29983;&#12290;&#36890;&#36807;&#23450;&#20041;&#26377;&#25928;&#27169;&#22411;&#35760;&#24518;&#21270; (EMM) &#36825;&#19968;&#25351;&#26631;&#65292;&#37327;&#21270;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#37197;&#32622;&#23545;&#35760;&#24518;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#29983;&#25104;&#26032;&#39062;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;&#25193;&#25955;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20856;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#65292;&#25193;&#25955;&#27169;&#22411;&#21482;&#33021;&#29983;&#25104;&#22797;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26679;&#26412;&#65292;&#36825;&#34920;&#26126;&#22312;&#29702;&#35770;&#19978;&#20250;&#20986;&#29616;&#35760;&#24518;&#21270;&#30340;&#34892;&#20026;&#65292;&#36825;&#19982;&#29616;&#26377;&#20808;&#36827;&#25193;&#25955;&#27169;&#22411;&#30340;&#26222;&#36941;&#27867;&#21270;&#33021;&#21147;&#30456;&#30683;&#30462;&#65292;&#22240;&#27492;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#35760;&#24518;&#21270;&#34892;&#20026;&#20542;&#21521;&#20110;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#21457;&#29983;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#27169;&#22411;&#35760;&#24518;&#21270;(EMM)&#30340;&#23450;&#20041;&#65292;&#36825;&#26159;&#19968;&#31181;&#34913;&#37327;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#22823;&#25968;&#25454;&#38598;&#19978;&#36817;&#20284;&#20854;&#29702;&#35770;&#26368;&#20248;&#28857;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#24433;&#21709;&#36825;&#20123;&#35760;&#24518;&#21270;&#34892;&#20026;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a learned diffusion model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuratio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Choco Solver&#36827;&#34892;&#22810;&#37197;&#32622;&#38382;&#39064;&#27714;&#35299;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20197;&#21450;&#23545;&#32422;&#26463;&#27714;&#35299;&#22120;&#24615;&#33021;&#20998;&#26512;&#30340;&#30740;&#31350;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#30456;&#20851;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02658</link><description>&lt;p&gt;
&#35299;&#20915;&#22810;&#37197;&#32622;&#38382;&#39064;&#65306;&#20351;&#29992;Choco Solver&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Solving Multi-Configuration Problems: A Performance Analysis with Choco Solver. (arXiv:2310.02658v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Choco Solver&#36827;&#34892;&#22810;&#37197;&#32622;&#38382;&#39064;&#27714;&#35299;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20197;&#21450;&#23545;&#32422;&#26463;&#27714;&#35299;&#22120;&#24615;&#33021;&#20998;&#26512;&#30340;&#30740;&#31350;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#30456;&#20851;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#37197;&#32622;&#22120;&#25903;&#25345;&#37197;&#32622;&#28385;&#36275;&#21333;&#20010;&#29992;&#25143;&#20559;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#37197;&#32622;&#30340;&#27010;&#24565;&#22522;&#20110;&#37197;&#32622;&#19968;&#32452;&#37197;&#32622;&#30340;&#24819;&#27861;&#12290;&#36825;&#31181;&#21151;&#33021;&#22312;&#37197;&#32622;&#20010;&#24615;&#21270;&#32771;&#35797;&#65292;&#37197;&#32622;&#39033;&#30446;&#22242;&#38431;&#21644;&#20026;&#26053;&#28216;&#22242;&#38431;&#30340;&#27599;&#20010;&#25104;&#21592;&#37197;&#32622;&#19981;&#21516;&#30340;&#26053;&#34892;&#65288;&#20363;&#22914;&#65292;&#22312;&#35775;&#38382;&#29305;&#23450;&#22478;&#24066;&#26102;&#65289;&#31561;&#22330;&#26223;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31034;&#20363;&#20102;&#22810;&#37197;&#32622;&#24212;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#32771;&#35797;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#32422;&#26463;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#20998;&#26512;&#65292;&#24110;&#21161;&#25105;&#20204;&#23545;&#30456;&#24212;&#30340;&#24615;&#33021;&#38382;&#39064;&#26377;&#19968;&#20123;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, configurators support the configuration of a solution that satisfies the preferences of a single user. The concept of \emph{multi-configuration} is based on the idea of configuring a set of configurations. Such a functionality is relevant in scenarios such as the configuration of personalized exams, the configuration of project teams, and the configuration of different trips for individual members of a tourist group (e.g., when visiting a specific city). In this paper, we exemplify the application of multi-configuration for generating individualized exams. We also provide a constraint solver performance analysis which helps to gain some insights into corresponding performance issues.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#21305;&#37197;&#37327;&#21270;&#26041;&#26696;&#19982;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#24182;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21644;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26102;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#27169;&#22411;&#37327;&#21270;&#21644;&#37096;&#32626;&#20915;&#31574;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#24182;&#25512;&#36827;&#20102;&#37327;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.02654</link><description>&lt;p&gt;
&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;FPGA&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs. (arXiv:2310.02654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#21305;&#37197;&#37327;&#21270;&#26041;&#26696;&#19982;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#24182;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21644;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26102;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#27169;&#22411;&#37327;&#21270;&#21644;&#37096;&#32626;&#20915;&#31574;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#24182;&#25512;&#36827;&#20102;&#37327;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#22312;QAT&#38454;&#27573;&#21160;&#24577;&#36873;&#25321;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#23558;&#37327;&#21270;&#26041;&#26696;&#19982;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#21305;&#37197;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21644;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26102;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#23545;&#35937;&#34987;&#37327;&#21270;&#20026;4&#20301;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#27169;&#22411;&#37327;&#21270;&#21644;&#37096;&#32626;&#20915;&#31574;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#24182;&#20026;&#25512;&#36827;&#37327;&#21270;&#25216;&#26415;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the quantisation-aware training (QAT) on time series Transformer models. We propose a novel adaptive quantisation scheme that dynamically selects between symmetric and asymmetric schemes during the QAT phase. Our approach demonstrates that matching the quantisation scheme to the real data distribution can reduce computational overhead while maintaining acceptable precision. Moreover, our approach is robust when applied to real-world data and mixed-precision quantisation, where most objects are quantised to 4 bits. Our findings inform model quantisation and deployment decisions while providing a foundation for advancing quantisation techniques.
&lt;/p&gt;</description></item><item><title>GET&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20107;&#20214;&#35270;&#35273;&#30340;&#26032;&#22411;&#22242;&#20307;&#20107;&#20214;&#21464;&#25442;&#22120;&#65292;&#23427;&#33021;&#22815;&#23558;&#20107;&#20214;&#30340;&#26102;&#38388;&#21644;&#26497;&#24615;&#19982;&#31354;&#38388;&#20449;&#24687;&#35299;&#32806;&#65292;&#36890;&#36807;&#20107;&#20214;&#21452;&#33258;&#27880;&#24847;&#22359;&#21644;&#22242;&#20307;&#26631;&#35760;&#32858;&#21512;&#27169;&#22359;&#23454;&#29616;&#20102;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;-&#26497;&#24615;&#39046;&#22495;&#30340;&#29305;&#24449;&#36890;&#20449;&#21644;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.02642</link><description>&lt;p&gt;
GET: &#29992;&#20110;&#20107;&#20214;&#35270;&#35273;&#30340;&#22242;&#20307;&#20107;&#20214;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
GET: Group Event Transformer for Event-Based Vision. (arXiv:2310.02642v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02642
&lt;/p&gt;
&lt;p&gt;
GET&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20107;&#20214;&#35270;&#35273;&#30340;&#26032;&#22411;&#22242;&#20307;&#20107;&#20214;&#21464;&#25442;&#22120;&#65292;&#23427;&#33021;&#22815;&#23558;&#20107;&#20214;&#30340;&#26102;&#38388;&#21644;&#26497;&#24615;&#19982;&#31354;&#38388;&#20449;&#24687;&#35299;&#32806;&#65292;&#36890;&#36807;&#20107;&#20214;&#21452;&#33258;&#27880;&#24847;&#22359;&#21644;&#22242;&#20307;&#26631;&#35760;&#32858;&#21512;&#27169;&#22359;&#23454;&#29616;&#20102;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;-&#26497;&#24615;&#39046;&#22495;&#30340;&#29305;&#24449;&#36890;&#20449;&#21644;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#24418;&#24577;&#20256;&#24863;&#22120;&#65292;&#22791;&#21463;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#39592;&#24178;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#35774;&#35745;&#26469;&#25552;&#21462;&#20174;&#20107;&#20214;&#36716;&#25442;&#32780;&#26469;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#26102;&#38388;&#21644;&#26497;&#24615;&#31561;&#37325;&#35201;&#30340;&#20107;&#20214;&#23646;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20107;&#20214;&#35270;&#35273;&#30340;&#26032;&#22411;&#22242;&#20307;&#35270;&#35273;&#21464;&#25442;&#22120;&#39592;&#24178;&#65292;&#21517;&#20026;&#22242;&#20307;&#20107;&#20214;&#21464;&#25442;&#22120;(GET)&#65292;&#23427;&#33021;&#22815;&#22312;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#20013;&#23558;&#26102;&#38388;&#21644;&#26497;&#24615;&#20174;&#31354;&#38388;&#20449;&#24687;&#20013;&#35299;&#32806;&#20986;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;GET&#30340;&#26032;&#20107;&#20214;&#34920;&#31034;&#26041;&#24335;&#65292;&#21517;&#20026;&#22242;&#20307;&#26631;&#35760;(Group Token)&#65292;&#23427;&#26681;&#25454;&#26102;&#38388;&#25139;&#21644;&#26497;&#24615;&#23545;&#24322;&#27493;&#20107;&#20214;&#36827;&#34892;&#20998;&#32452;&#12290;&#28982;&#21518;&#65292;GET&#24212;&#29992;&#20107;&#20214;&#21452;&#33258;&#27880;&#24847;&#22359;&#21644;&#22242;&#20307;&#26631;&#35760;&#32858;&#21512;&#27169;&#22359;&#65292;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;-&#26497;&#24615;&#39046;&#22495;&#20013;&#20419;&#36827;&#26377;&#25928;&#30340;&#29305;&#24449;&#36890;&#20449;&#21644;&#25972;&#21512;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;GET&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event cameras are a type of novel neuromorphic sen-sor that has been gaining increasing attention. Existing event-based backbones mainly rely on image-based designs to extract spatial information within the image transformed from events, overlooking important event properties like time and polarity. To address this issue, we propose a novel Group-based vision Transformer backbone for Event-based vision, called Group Event Transformer (GET), which de-couples temporal-polarity information from spatial infor-mation throughout the feature extraction process. Specifi-cally, we first propose a new event representation for GET, named Group Token, which groups asynchronous events based on their timestamps and polarities. Then, GET ap-plies the Event Dual Self-Attention block, and Group Token Aggregation module to facilitate effective feature commu-nication and integration in both the spatial and temporal-polarity domains. After that, GET can be integrated with different downstream tasks by con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;DINN&#65289;&#29992;&#20110;&#22788;&#29702;&#21463;&#21040;&#20960;&#20309;&#24418;&#21464;&#24433;&#21709;&#30340;&#22270;&#20687;&#30340;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;DINN&#36890;&#36807;&#34701;&#20837;&#25311;&#20445;&#24418;&#21464;&#25442;&#32593;&#32476;&#65288;QCTN&#65289;&#26469;&#36755;&#20986;&#19968;&#33268;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#20351;&#24471;&#20855;&#26377;&#30456;&#21516;&#21407;&#22987;&#23545;&#35937;&#25110;&#22330;&#26223;&#30340;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#33021;&#22815;&#26356;&#25509;&#36817;&#33258;&#28982;&#25110;&#33391;&#22909;&#22270;&#20687;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2310.02641</link><description>&lt;p&gt;
&#24377;&#24615;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#21464;&#24418;&#22270;&#20687;&#24674;&#22797;&#21644;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis. (arXiv:2310.02641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;DINN&#65289;&#29992;&#20110;&#22788;&#29702;&#21463;&#21040;&#20960;&#20309;&#24418;&#21464;&#24433;&#21709;&#30340;&#22270;&#20687;&#30340;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;DINN&#36890;&#36807;&#34701;&#20837;&#25311;&#20445;&#24418;&#21464;&#25442;&#32593;&#32476;&#65288;QCTN&#65289;&#26469;&#36755;&#20986;&#19968;&#33268;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#20351;&#24471;&#20855;&#26377;&#30456;&#21516;&#21407;&#22987;&#23545;&#35937;&#25110;&#22330;&#26223;&#30340;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#33021;&#22815;&#26356;&#25509;&#36817;&#33258;&#28982;&#25110;&#33391;&#22909;&#22270;&#20687;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20960;&#20309;&#24418;&#21464;&#24433;&#21709;&#30340;&#22270;&#20687;&#23545;&#20110;&#30446;&#26631;&#35782;&#21035;&#31561;&#22270;&#20687;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#23545;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#32473;&#20986;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;DINN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#30340;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;DINN&#20026;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#36755;&#20986;&#19968;&#33268;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#30456;&#21516;&#30340;&#21407;&#22987;&#23545;&#35937;&#25110;&#22330;&#26223;&#12290;DINN&#30340;&#24605;&#24819;&#26159;&#23558;&#19968;&#20010;&#31616;&#21333;&#30340;&#32452;&#20214;&#65292;&#31216;&#20026;&#25311;&#20445;&#24418;&#21464;&#25442;&#32593;&#32476;&#65288;QCTN&#65289;&#65292;&#34701;&#20837;&#21040;&#20854;&#20182;&#29616;&#26377;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;QCTN&#26159;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36755;&#20986;&#19968;&#20010;&#25311;&#20445;&#24418;&#26144;&#23556;&#65292;&#21487;&#20197;&#23558;&#20960;&#20309;&#24418;&#21464;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#26356;&#25509;&#36817;&#33258;&#28982;&#25110;&#33391;&#22909;&#22270;&#20687;&#20998;&#24067;&#30340;&#25913;&#36827;&#29256;&#26412;&#12290;&#23427;&#39318;&#20808;&#36755;&#20986;&#19968;&#20010;&#36125;&#23572;&#29305;&#25289;&#23494;&#31995;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#25311;&#20445;&#24418;&#26144;&#23556;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Images degraded by geometric distortions pose a significant challenge to imaging and computer vision tasks such as object recognition. Deep learning-based imaging models usually fail to give accurate performance for geometrically distorted images. In this paper, we propose the deformation-invariant neural network (DINN), a framework to address the problem of imaging tasks for geometrically distorted images. The DINN outputs consistent latent features for images that are geometrically distorted but represent the same underlying object or scene. The idea of DINN is to incorporate a simple component, called the quasiconformal transformer network (QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep neural network that outputs a quasiconformal map, which can be used to transform a geometrically distorted image into an improved version that is closer to the distribution of natural or good images. It first outputs a Beltrami coefficient, which measures the quasiconf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02635</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65306;&#26397;&#21521;&#20855;&#26377;&#22522;&#30784;&#20808;&#39564;&#36741;&#21161;&#30340;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#20174;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#26159;&#26500;&#24314;&#36890;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27491;&#22914;&#22312;NLP&#20013;&#25152;&#35265;&#12290;&#20026;&#20102;&#26500;&#24314;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21644;&#35768;&#22810;&#20854;&#20182;&#30740;&#31350;&#32773;&#20551;&#35774;&#36825;&#31181;&#22522;&#30784;&#20808;&#39564;&#20063;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#20855;&#20307;&#24418;&#24335;&#34920;&#31034;&#36825;&#20123;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#65292;&#20197;&#21450;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#30452;&#35266;&#26377;&#25928;&#30340;&#20855;&#36523;&#20808;&#39564;&#65292;&#21253;&#25324;&#22522;&#30784;&#31574;&#30053;&#12289;&#20215;&#20540;&#21644;&#25104;&#21151;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;MDP&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#30001;&#36825;&#20123;&#20808;&#39564;&#36741;&#21161;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#22522;&#30784;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#65292;&#22240;&#20026;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#26469;&#36827;&#34892;&#25506;&#32034;&#12289;&#23398;&#20064;&#21644;&#24378;&#21270;&#12290;FRL&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#12290;(1)&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#36890;&#36807;&#22522;&#30784;&#20808;&#39564;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#20943;&#23569;&#26679;&#26412;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;(MAABO-MT&#21644;GS-MRM)&#65292;&#20998;&#21035;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#26641;&#20013;&#39640;&#24615;&#33021;&#20272;&#35745;&#26500;&#24314;&#26641;&#65292;&#20165;&#25552;&#21462;&#21487;&#38752;&#19988;&#19981;&#30456;&#20284;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2310.02633</link><description>&lt;p&gt;
&#22522;&#20110;&#25913;&#36827;&#30340;Aitchison-Aitken&#20989;&#25968;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#29190;&#28856;&#20915;&#31574;&#26641;&#30340;&#22810;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-rules mining algorithm for combinatorially exploded decision trees with modified Aitchison-Aitken function-based Bayesian optimization. (arXiv:2310.02633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02633
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;(MAABO-MT&#21644;GS-MRM)&#65292;&#20998;&#21035;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#26641;&#20013;&#39640;&#24615;&#33021;&#20272;&#35745;&#26500;&#24314;&#26641;&#65292;&#20165;&#25552;&#21462;&#21487;&#38752;&#19988;&#19981;&#30456;&#20284;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#20855;&#26377;&#26131;&#20110;&#35299;&#37322;&#30340;&#20248;&#28857;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#26681;&#25454;if-then&#35268;&#21017;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20915;&#31574;&#26641;&#26159;&#30001;&#19968;&#20010;&#31639;&#27861;&#26500;&#24314;&#30340;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26368;&#23569;&#30340;&#35268;&#21017;&#23454;&#29616;&#28165;&#26224;&#30340;&#20998;&#31867;&#65292;&#32780;&#26080;&#35770;&#25968;&#25454;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#28508;&#22312;&#35268;&#21017;&#65292;&#37117;&#21482;&#33021;&#25552;&#21462;&#26368;&#23567;&#30340;&#35268;&#21017;&#12290;&#30830;&#23454;&#23384;&#22312;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#29305;&#24449;&#23376;&#38598;&#26500;&#24314;&#22810;&#26869;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21487;&#20197;&#26500;&#24314;&#30340;&#26641;&#30340;&#25968;&#37327;&#20173;&#28982;&#22312;&#30456;&#21516;&#30340;&#25968;&#37327;&#32423;&#65292;&#22240;&#20026;&#29305;&#24449;&#23376;&#38598;&#30340;&#25968;&#37327;&#26159;&#19968;&#20010;&#32452;&#21512;&#24615;&#29190;&#28856;&#12290;&#27492;&#22806;&#65292;&#24403;&#26500;&#24314;&#22810;&#26869;&#26641;&#26102;&#65292;&#20250;&#29983;&#25104;&#35768;&#22810;&#35268;&#21017;&#65292;&#20854;&#20013;&#26377;&#20960;&#20010;&#26159;&#19981;&#21487;&#38752;&#21644;/&#25110;&#38750;&#24120;&#30456;&#20284;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MAABO-MT&#8221;&#21644;&#8220;GS-MRM&#8221;&#31639;&#27861;&#65292;&#23427;&#20204;&#20998;&#21035;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#26641;&#20013;&#39640;&#24615;&#33021;&#20272;&#35745;&#26500;&#24314;&#26641;&#65292;&#24182;&#20165;&#25552;&#21462;&#21487;&#38752;&#19988;&#19981;&#30456;&#20284;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees offer the benefit of easy interpretation because they allow the classification of input data based on if--then rules. However, as decision trees are constructed by an algorithm that achieves clear classification with minimum necessary rules, the trees possess the drawback of extracting only minimum rules, even when various latent rules exist in data. Approaches that construct multiple trees using randomly selected feature subsets do exist. However, the number of trees that can be constructed remains at the same scale because the number of feature subsets is a combinatorial explosion. Additionally, when multiple trees are constructed, numerous rules are generated, of which several are untrustworthy and/or highly similar. Therefore, we propose "MAABO-MT" and "GS-MRM" algorithms that strategically construct trees with high estimation performance among all possible trees with small computational complexity and extract only reliable and non-similar rules, respectively. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22810;Agent&#31995;&#32479;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#37327;&#21270;&#30340;&#21487;&#35266;&#23519;&#24615;&#20998;&#26512;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#36879;&#26126;&#24230;&#27010;&#24565;&#21644;&#26102;&#38388;&#36923;&#36753;oPATL&#65292;&#25105;&#20204;&#21487;&#20197;&#23450;&#37327;&#20998;&#26512;&#31995;&#32479;&#34892;&#20026;&#23545;&#35266;&#23519;&#32773;&#30340;&#20449;&#24687;&#36879;&#26126;&#24230;&#65292;&#20174;&#32780;&#20026;&#25805;&#20316;&#21592;&#36741;&#21161;&#20915;&#31574;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.02614</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;Agent&#31995;&#32479;&#20013;&#21487;&#37327;&#21270;&#30340;&#21487;&#35266;&#23519;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Quantified Observability Analysis in Multiagent Systems. (arXiv:2310.02614v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;Agent&#31995;&#32479;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#37327;&#21270;&#30340;&#21487;&#35266;&#23519;&#24615;&#20998;&#26512;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#36879;&#26126;&#24230;&#27010;&#24565;&#21644;&#26102;&#38388;&#36923;&#36753;oPATL&#65292;&#25105;&#20204;&#21487;&#20197;&#23450;&#37327;&#20998;&#26512;&#31995;&#32479;&#34892;&#20026;&#23545;&#35266;&#23519;&#32773;&#30340;&#20449;&#24687;&#36879;&#26126;&#24230;&#65292;&#20174;&#32780;&#20026;&#25805;&#20316;&#21592;&#36741;&#21161;&#20915;&#31574;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;Agent&#31995;&#32479;&#20013;&#65292;Agent&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#35266;&#23519;&#21487;&#33021;&#25913;&#21892;&#25972;&#20307;&#22242;&#38431;&#30340;&#34920;&#29616;&#65292;&#20294;&#20063;&#21487;&#33021;&#21521;&#35266;&#23519;&#32773;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21487;&#37327;&#21270;&#30340;&#21487;&#35266;&#23519;&#24615;&#20998;&#26512;&#21487;&#20197;&#24110;&#21161;&#25805;&#20316;&#21592;&#22312;&#23454;&#36341;&#20013;&#36890;&#36807;&#35266;&#23519;&#26469;&#20248;&#21270;&#24615;&#33021;&#25928;&#26524;&#21644;&#20449;&#24687;&#26292;&#38706;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#36741;&#21161;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;Agent&#31995;&#32479;&#20013;&#23450;&#37327;&#20998;&#26512;&#21487;&#35266;&#23519;&#24615;&#23646;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#19981;&#36879;&#26126;&#24230;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#27491;&#24335;&#34920;&#36798;&#24314;&#27169;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#22810;Agent&#31995;&#32479;&#30340;&#21487;&#35266;&#23519;&#24615;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;Agent&#21487;&#35266;&#23519;&#24615;&#30340;&#26102;&#38388;&#36923;&#36753;oPATL&#65292;&#24182;&#24320;&#21457;&#20102;&#39564;&#35777;&#25216;&#26415;&#26469;&#23450;&#37327;&#20998;&#26512;&#36825;&#20123;&#23646;&#24615;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#20316;&#20026;PRISM&#27169;&#22411;&#26816;&#27979;&#22120;&#30340;&#25193;&#23637;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multiagent systems (MASs), agents' observation upon system behaviours may improve the overall team performance, but may also leak sensitive information to an observer. A quantified observability analysis can thus be useful to assist decision-making in MASs by operators seeking to optimise the relationship between performance effectiveness and information exposure through observations in practice. This paper presents a novel approach to quantitatively analysing the observability properties in MASs. The concept of opacity is applied to formally express the characterisation of observability in MASs modelled as partially observable multiagent systems. We propose a temporal logic oPATL to reason about agents' observability with quantitative goals, which capture the probability of information transparency of system behaviours to an observer, and develop verification techniques for quantitatively analysing such properties. We implement the approach as an extension of the PRISM model checke
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#32593;&#25299;&#25169;&#20248;&#21270;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26377;&#25928;&#22788;&#29702;&#38543;&#30528;&#32593;&#32476;&#22686;&#38271;&#32780;&#25193;&#22823;&#30340;&#22823;&#22411;&#34892;&#21160;&#31354;&#38388;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#24615;&#33021;&#19978;&#19982;&#21333;&#19968;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;RL&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#39640;&#38454;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.02605</link><description>&lt;p&gt;
&#29992;&#20110;&#30005;&#32593;&#25299;&#25169;&#20248;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning for Power Grid Topology Optimization. (arXiv:2310.02605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#32593;&#25299;&#25169;&#20248;&#21270;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26377;&#25928;&#22788;&#29702;&#38543;&#30528;&#32593;&#32476;&#22686;&#38271;&#32780;&#25193;&#22823;&#30340;&#22823;&#22411;&#34892;&#21160;&#31354;&#38388;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#24615;&#33021;&#19978;&#19982;&#21333;&#19968;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;RL&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#39640;&#38454;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38754;&#20020;&#30528;&#33021;&#28304;&#38656;&#27714;&#22686;&#21152;&#21644;&#39118;&#33021;&#12289;&#22826;&#38451;&#33021;&#31561;&#19981;&#21487;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#25361;&#25112;&#65292;&#25805;&#20316;&#30005;&#32593;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#22312;&#31649;&#29702;&#36825;&#20123;&#32593;&#32476;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#24635;&#32447;&#21644;&#32447;&#36335;&#20999;&#25442;&#31561;&#25299;&#25169;&#25805;&#20316;&#65292;&#20294;&#23545;&#20110;&#38543;&#30528;&#32593;&#32476;&#22686;&#38271;&#32780;&#25193;&#22823;&#30340;&#22823;&#22411;&#34892;&#21160;&#31354;&#38388;&#30340;&#39640;&#25928;&#22788;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#25193;&#23637;&#34892;&#21160;&#31354;&#38388;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#30005;&#32593;&#22266;&#26377;&#30340;&#20998;&#23618;&#29305;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MARL&#26694;&#26550;&#19982;&#21333;&#19968;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;RL&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#39640;&#38454;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent challenges in operating power networks arise from increasing energy demands and unpredictable renewable sources like wind and solar. While reinforcement learning (RL) shows promise in managing these networks, through topological actions like bus and line switching, efficiently handling large action spaces as networks grow is crucial. This paper presents a hierarchical multi-agent reinforcement learning (MARL) framework tailored for these expansive action spaces, leveraging the power grid's inherent hierarchical nature. Experimental results indicate the MARL framework's competitive performance with single-agent RL methods. We also compare different RL algorithms for lower-level agents alongside different policies for higher-order agents.
&lt;/p&gt;</description></item><item><title>MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.02601</link><description>&lt;p&gt;
MagicDrive: &#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#19979;&#30340;&#34903;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02601
&lt;/p&gt;
&lt;p&gt;
MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#20855;&#26377;2D&#25511;&#21046;&#30340;&#25968;&#25454;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#34903;&#26223;&#29983;&#25104;&#20013;&#31934;&#30830;&#30340;&#19977;&#32500;&#25511;&#21046;&#22312;&#19977;&#32500;&#24863;&#30693;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#40479;&#30640;&#22270;&#20316;&#20026;&#20027;&#35201;&#26465;&#20214;&#24120;&#24120;&#23548;&#33268;&#20960;&#20309;&#25511;&#21046;&#65288;&#22914;&#39640;&#24230;&#65289;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24433;&#21709;&#29289;&#20307;&#24418;&#29366;&#12289;&#36974;&#25377;&#27169;&#24335;&#21644;&#36947;&#36335;&#34920;&#38754;&#39640;&#31243;&#31561;&#23545;&#24863;&#30693;&#25968;&#25454;&#21512;&#25104;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicDrive&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#36890;&#36807;&#23450;&#21046;&#30340;&#32534;&#30721;&#31574;&#30053;&#23454;&#29616;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#36824;&#37319;&#29992;&#20102;&#36328;&#35270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#30830;&#20445;&#22810;&#20010;&#30456;&#26426;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;MagicDrive&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#25429;&#25417;&#21040;&#20102;&#31934;&#32454;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework offering diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view synthesis that captures nuanced 3D ge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ModelOps&#30340;&#26234;&#33021;&#21307;&#30103;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#20302;&#20195;&#30721;&#31995;&#32479;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#12289;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#20415;&#21033;&#32473;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#21644;&#37096;&#32626;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.02593</link><description>&lt;p&gt;
&#22522;&#20110;ModelOps&#30340;&#26234;&#33021;&#21307;&#30103;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A ModelOps-based Framework for Intelligent Medical Knowledge Extraction. (arXiv:2310.02593v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02593
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ModelOps&#30340;&#26234;&#33021;&#21307;&#30103;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#20302;&#20195;&#30721;&#31995;&#32479;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#12289;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#20415;&#21033;&#32473;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#21644;&#37096;&#32626;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21307;&#30103;&#25991;&#26412;&#20013;&#25552;&#21462;&#21307;&#30103;&#30693;&#35782;&#21487;&#20197;&#22686;&#24378;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#20020;&#24202;&#20915;&#31574;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30693;&#35782;&#25552;&#21462;&#27169;&#22411;&#30340;&#26500;&#24314;&#21644;&#24212;&#29992;&#32570;&#20047;&#33258;&#21160;&#21270;&#12289;&#21487;&#37325;&#29992;&#24615;&#21644;&#32479;&#19968;&#31649;&#29702;&#65292;&#36825;&#32473;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#20302;&#25928;&#65292;&#20063;&#32473;&#38750;AI&#19987;&#23478;&#22914;&#21307;&#29983;&#31561;&#21033;&#29992;&#30693;&#35782;&#25552;&#21462;&#24102;&#26469;&#20102;&#39640;&#38376;&#27099;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ModelOps&#30340;&#26234;&#33021;&#21307;&#30103;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20026;&#27169;&#22411;&#36873;&#25321;&#12289;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#25552;&#20379;&#20102;&#20302;&#20195;&#30721;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#22522;&#20110;&#22810;&#23618;&#22238;&#35843;&#20989;&#25968;&#30340;&#25968;&#25454;&#38598;&#25277;&#35937;&#26426;&#21046;&#21644;&#21487;&#37325;&#29992;&#30340;&#27169;&#22411;&#35757;&#32451;&#12289;&#30417;&#25511;&#21644;&#31649;&#29702;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#38598;&#30456;&#20284;&#24615;&#30340;&#27169;&#22411;&#25512;&#33616;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#25214;&#21040;&#36866;&#21512;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#27169;&#22411;&#21644;&#37096;&#32626;&#27169;&#22411;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting medical knowledge from healthcare texts enhances downstream tasks like medical knowledge graph construction and clinical decision-making. However, the construction and application of knowledge extraction models lack automation, reusability and unified management, leading to inefficiencies for researchers and high barriers for non-AI experts such as doctors, to utilize knowledge extraction. To address these issues, we propose a ModelOps-based intelligent medical knowledge extraction framework that offers a low-code system for model selection, training, evaluation and optimization. Specifically, the framework includes a dataset abstraction mechanism based on multi-layer callback functions, a reusable model training, monitoring and management mechanism. We also propose a model recommendation method based on dataset similarity, which helps users quickly find potentially suitable models for a given dataset. Our framework provides convenience for researchers to develop models and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02579</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#36798;&#20301;&#32622;&#32534;&#30721;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#22270;&#20301;&#32622;&#32534;&#30721;&#23545;&#26500;&#24314;&#24378;&#22823;&#30340;&#22270;&#36716;&#25442;&#22120;&#21644;&#22686;&#24378;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#20851;&#38190;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#65292;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#65288;1&#65289;\emph{&#38750;&#21807;&#19968;&#24615;}&#65306;&#21516;&#19968;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#35299;&#65292;&#20197;&#21450;&#65288;2&#65289;\emph{&#19981;&#31283;&#23450;&#24615;}&#65306;&#23545;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#23436;&#20840;&#19981;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23548;&#33268;&#20301;&#32622;&#32534;&#30721;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21464;&#21270;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#23581;&#35797;&#35299;&#20915;&#38750;&#21807;&#19968;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24573;&#35270;&#20102;&#31283;&#23450;&#24615;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#31283;&#23450;&#24615;&#30340;&#21407;&#22240;&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;"&#30828;&#20998;&#21106;"&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#65288;SPE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#29305;&#24449;&#21521;&#37327;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#24449;&#20540;&#23558;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#12290;SPE&#26159;&#39318;&#20010;&#65288;1&#65289;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#26550;&#26500;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26222;&#36866;&#22320;&#25552;&#21319;&#22270;&#32467;&#26500;&#27867;&#21270;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a "hard partition" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally exp
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31435;&#22330;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;stance-aware GNN&#65289;&#39044;&#27979;&#35875;&#35328;&#20256;&#25773;&#12290;&#19982;&#27809;&#26377;&#29992;&#25143;&#31435;&#22330;&#30340;GNN&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;32.65%&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#27880;&#24847;&#26435;&#37325;&#34920;&#26126;&#29992;&#25143;&#30340;&#21453;&#23545;&#31435;&#22330;&#23545;&#37051;&#23621;&#34892;&#20026;&#30340;&#24433;&#21709;&#26356;&#22823;&#65292;&#21487;&#20197;&#20316;&#20026;&#31038;&#20250;&#32416;&#27491;&#25514;&#26045;&#38459;&#27490;&#35875;&#35328;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2310.02568</link><description>&lt;p&gt;
&#20026;&#20102;&#26576;&#20107;&#32780;&#31449;&#31435;&#65292;&#21542;&#21017;&#23601;&#20250;&#20026;&#19968;&#20999;&#22446;&#25481;&#65306;&#20351;&#29992;&#31435;&#22330;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35875;&#35328;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Stand for Something or Fall for Everything: Predict Misinformation Spread with Stance-Aware Graph Neural Networks. (arXiv:2310.02568v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02568
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31435;&#22330;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;stance-aware GNN&#65289;&#39044;&#27979;&#35875;&#35328;&#20256;&#25773;&#12290;&#19982;&#27809;&#26377;&#29992;&#25143;&#31435;&#22330;&#30340;GNN&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;32.65%&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#27880;&#24847;&#26435;&#37325;&#34920;&#26126;&#29992;&#25143;&#30340;&#21453;&#23545;&#31435;&#22330;&#23545;&#37051;&#23621;&#34892;&#20026;&#30340;&#24433;&#21709;&#26356;&#22823;&#65292;&#21487;&#20197;&#20316;&#20026;&#31038;&#20250;&#32416;&#27491;&#25514;&#26045;&#38459;&#27490;&#35875;&#35328;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#27969;&#34892;&#30340;&#35875;&#35328;&#20256;&#25773;&#24050;&#25104;&#20026;&#36843;&#20999;&#30340;&#25361;&#25112;&#65292;&#20294;&#29616;&#26377;&#30340;&#24179;&#21488;&#24178;&#39044;&#25514;&#26045;&#22312;&#36943;&#21046;&#20854;&#20256;&#25773;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31435;&#22330;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;stance-aware GNN&#65289;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#31435;&#22330;&#20027;&#21160;&#39044;&#27979;&#35875;&#35328;&#20256;&#25773;&#12290;&#30001;&#20110;&#19981;&#21516;&#29992;&#25143;&#30340;&#31435;&#22330;&#21487;&#20197;&#24418;&#25104;&#29420;&#29305;&#30340;&#22238;&#22768;&#23460;&#65292;&#25105;&#20204;&#22312;&#31435;&#22330;&#24863;&#30693;&#30340;GNN&#20013;&#23450;&#21046;&#20102;&#22235;&#20010;&#20449;&#24687;&#20256;&#36882;&#36335;&#24452;&#65292;&#32780;&#21487;&#35757;&#32451;&#30340;&#27880;&#24847;&#26435;&#37325;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#27599;&#20010;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#26469;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#31435;&#22330;&#24863;&#30693;&#30340;GNN&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;32.65&#65285;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#27809;&#26377;&#29992;&#25143;&#31435;&#22330;&#30340;&#20808;&#36827;GNN 4.69&#65285;&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#27880;&#24847;&#26435;&#37325;&#34920;&#26126;&#65292;&#29992;&#25143;&#30340;&#21453;&#23545;&#31435;&#22330;&#23545;&#37051;&#23621;&#34892;&#20026;&#30340;&#24433;&#21709;&#39640;&#20110;&#25903;&#25345;&#31435;&#22330;&#65292;&#36825;&#36215;&#21040;&#20102;&#31038;&#20250;&#32416;&#27491;&#20316;&#29992;&#65292;&#38459;&#27490;&#20102;&#35875;&#35328;&#30340;&#20256;&#25773;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pervasive spread of misinformation on social media platforms has become a pressing challenge, existing platform interventions have shown limited success in curbing its dissemination. In this study, we propose a stance-aware graph neural network (stance-aware GNN) that leverages users' stances to proactively predict misinformation spread. As different user stances can form unique echo chambers, we customize four information passing paths in stance-aware GNN, while the trainable attention weights provide explainability by highlighting each structure's importance. Evaluated on a real-world dataset, stance-aware GNN outperforms benchmarks by 32.65% and exceeds advanced GNNs without user stance by over 4.69%. Furthermore, the attention weights indicate that users' opposition stances have a higher impact on their neighbors' behaviors than supportive ones, which function as social correction to halt misinformation propagation. Overall, our study provides an effective predictive model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02567</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20986;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;8&#24180;&#21518;&#65292;&#20934;&#30830;&#29575;&#20173;&#28982;&#26159;&#33258;&#21160;&#35780;&#20272;&#30340;&#20027;&#35201;&#25351;&#26631;&#12290;&#22312;IID&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;VQA&#20934;&#30830;&#24230;&#19968;&#30452;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#31038;&#21306;&#27491;&#22312;&#36716;&#21521;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#21644;OOD&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#20013;&#65292;&#29616;&#26377;&#30340;VQA&#20934;&#30830;&#24230;&#25351;&#26631;&#36807;&#20110;&#20005;&#26684;&#65292;&#20302;&#20272;&#20102;VQA&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#33258;&#21160;VQA&#24230;&#37327;&#65292;&#20316;&#20026;&#20154;&#31867;&#21028;&#26029;&#30340;&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;VQA&#24230;&#37327;&#12290;&#25105;&#20204;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#25351;&#31034;&#26681;&#25454;&#19968;&#32452;&#21442;&#32771;&#31572;&#26696;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#22312;&#20960;&#20010;VQA&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We ho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#36716;&#25442;&#32593;&#32476;&#30340;&#25913;&#36827;&#40723;&#26426;&#22120;&#20154;&#65292;&#21487;&#20197;&#33258;&#21160;&#23436;&#25104;&#38899;&#20048;&#36716;&#24405;&#65292;&#24182;&#24110;&#21161;&#26426;&#22120;&#20154;&#25552;&#21319;&#40723;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02565</link><description>&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147;&#36716;&#25442;&#32593;&#32476;&#25913;&#36827;&#40723;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Improving Drumming Robot Via Attention Transformer Network. (arXiv:2310.02565v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#36716;&#25442;&#32593;&#32476;&#30340;&#25913;&#36827;&#40723;&#26426;&#22120;&#20154;&#65292;&#21487;&#20197;&#33258;&#21160;&#23436;&#25104;&#38899;&#20048;&#36716;&#24405;&#65292;&#24182;&#24110;&#21161;&#26426;&#22120;&#20154;&#25552;&#21319;&#40723;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#29616;&#20170;&#31038;&#20250;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#22312;&#20892;&#19994;&#12289;&#21046;&#36896;&#19994;&#21644;&#23089;&#20048;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#23089;&#20048;&#39046;&#22495;&#20013;&#30340;&#40723;&#26426;&#22120;&#20154;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#40723;&#26426;&#22120;&#20154;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28909;&#38376;&#35270;&#35273;&#36716;&#25442;&#32593;&#32476;&#65292;&#21487;&#20197;&#33258;&#21160;&#23436;&#25104;&#38899;&#20048;&#36716;&#24405;&#12290;&#36890;&#36807;&#37197;&#22791;&#27880;&#24847;&#21147;&#36716;&#25442;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#39034;&#24207;&#38899;&#39057;&#23884;&#20837;&#36755;&#20837;&#65292;&#24182;&#24314;&#27169;&#20854;&#20840;&#23616;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#31639;&#27861;&#21487;&#20197;&#24110;&#21161;&#40723;&#26426;&#22120;&#20154;&#25552;&#21319;&#40723;&#20998;&#31867;&#24615;&#33021;&#65292;&#20063;&#21487;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#20139;&#21463;&#21508;&#31181;&#26234;&#33021;&#24212;&#29992;&#21644;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic technology has been widely used in nowadays society, which has made great progress in various fields such as agriculture, manufacturing and entertainment. In this paper, we focus on the topic of drumming robots in entertainment. To this end, we introduce an improving drumming robot that can automatically complete music transcription based on the popular vision transformer network based on the attention mechanism. Equipped with the attention transformer network, our method can efficiently handle the sequential audio embedding input and model their global long-range dependencies. Massive experimental results demonstrate that the improving algorithm can help the drumming robot promote drum classification performance, which can also help the robot to enjoy a variety of smart applications and services.
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#33258;&#21160;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#65288;Auto-FP&#65289;&#65292;&#23558;&#20854;&#24314;&#27169;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#25110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#21508;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;Auto-FP&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02540</link><description>&lt;p&gt;
Auto-FP:&#33258;&#21160;&#21270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Auto-FP: An Experimental Study of Automated Feature Preprocessing for Tabular Data. (arXiv:2310.02540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#33258;&#21160;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#65288;Auto-FP&#65289;&#65292;&#23558;&#20854;&#24314;&#27169;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#25110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#21508;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;Auto-FP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65292;&#22312;&#24037;&#19994;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#25935;&#24863;&#65292;&#22240;&#27492;&#29305;&#24449;&#39044;&#22788;&#29702;&#26159;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#33391;&#22909;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#39044;&#22788;&#29702;&#27969;&#31243;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#38656;&#35201;&#22312;&#36873;&#25321;&#21738;&#20123;&#39044;&#22788;&#29702;&#22120;&#20197;&#21450;&#20197;&#20160;&#20040;&#39034;&#24207;&#32452;&#21512;&#23427;&#20204;&#26041;&#38754;&#20316;&#20986;&#22256;&#38590;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#33258;&#21160;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#65288;Auto-FP&#65289;&#12290;&#30001;&#20110;&#25628;&#32034;&#31354;&#38388;&#36739;&#22823;&#65292;&#26292;&#21147;&#35299;&#20915;&#26041;&#26696;&#20195;&#20215;&#22826;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#26377;&#36259;&#22320;&#35266;&#23519;&#21040;Auto-FP&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#25110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#38382;&#39064;&#12290;&#36825;&#20010;&#35266;&#23519;&#20351;&#25105;&#20204;&#33021;&#22815;&#25193;&#23637;&#21508;&#31181;HPO&#21644;NAS&#31639;&#27861;&#26469;&#35299;&#20915;Auto-FP&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#20849;&#36827;&#34892;&#20102;15&#20010;...
&lt;/p&gt;
&lt;p&gt;
Classical machine learning models, such as linear models and tree-based models, are widely used in industry. These models are sensitive to data distribution, thus feature preprocessing, which transforms features from one distribution to another, is a crucial step to ensure good model quality. Manually constructing a feature preprocessing pipeline is challenging because data scientists need to make difficult decisions about which preprocessors to select and in which order to compose them. In this paper, we study how to automate feature preprocessing (Auto-FP) for tabular data. Due to the large search space, a brute-force solution is prohibitively expensive. To address this challenge, we interestingly observe that Auto-FP can be modelled as either a hyperparameter optimization (HPO) or a neural architecture search (NAS) problem. This observation enables us to extend a variety of HPO and NAS algorithms to solve the Auto-FP problem. We conduct a comprehensive evaluation and analysis of 15 
&lt;/p&gt;</description></item><item><title>MIDDAG&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#31038;&#20132;&#23186;&#20307;&#19978;&#30001;COVID-19&#30456;&#20851;&#26032;&#38395;&#35302;&#21457;&#30340;&#20449;&#24687;&#20256;&#25773;&#36335;&#24452;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#27934;&#23519;&#21147;&#65292;&#24182;&#33021;&#26500;&#24314;&#29992;&#25143;&#31038;&#21306;&#21644;&#39044;&#27979;&#20449;&#24687;&#20256;&#25773;&#65292;&#20174;&#32780;&#36861;&#36394;&#21644;&#29702;&#35299;&#20449;&#24687;&#30340;&#20256;&#25773;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.02529</link><description>&lt;p&gt;
MIDDAG&#65306;&#26032;&#38395;&#36208;&#21521;&#20309;&#26041;&#65311;&#36890;&#36807;&#31038;&#21306;&#32423;&#20449;&#24687;&#36335;&#24452;&#30740;&#31350;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
MIDDAG: Where Does Our News Go? Investigating Information Diffusion via Community-Level Information Pathways. (arXiv:2310.02529v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02529
&lt;/p&gt;
&lt;p&gt;
MIDDAG&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#31038;&#20132;&#23186;&#20307;&#19978;&#30001;COVID-19&#30456;&#20851;&#26032;&#38395;&#35302;&#21457;&#30340;&#20449;&#24687;&#20256;&#25773;&#36335;&#24452;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#27934;&#23519;&#21147;&#65292;&#24182;&#33021;&#26500;&#24314;&#29992;&#25143;&#31038;&#21306;&#21644;&#39044;&#27979;&#20449;&#24687;&#20256;&#25773;&#65292;&#20174;&#32780;&#36861;&#36394;&#21644;&#29702;&#35299;&#20449;&#24687;&#30340;&#20256;&#25773;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MIDDAG&#65292;&#19968;&#20010;&#30452;&#35266;&#12289;&#20132;&#20114;&#24335;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#21487;&#35270;&#21270;&#30001;COVID-19&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#35302;&#21457;&#30340;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#20256;&#25773;&#36335;&#24452;&#65292;&#24182;&#25552;&#20379;&#21253;&#25324;&#29992;&#25143;/&#31038;&#21306;&#26131;&#24863;&#24615;&#27700;&#24179;&#12289;&#20197;&#21450;&#20449;&#24687;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#21457;&#30340;&#20107;&#20214;&#21644;&#24191;&#22823;&#32676;&#20247;&#30340;&#28909;&#38376;&#35266;&#28857;&#30340;&#20840;&#38754;&#27934;&#23519;&#12290;&#38500;&#20102;&#21457;&#29616;&#29992;&#25143;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#27169;&#24335;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#21306;&#65292;&#24182;&#24320;&#21457;&#20102;&#20256;&#25773;&#39044;&#27979;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#39640;&#23618;&#27425;&#20449;&#24687;&#20256;&#25773;&#26041;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MIDDAG, an intuitive, interactive system that visualizes the information propagation paths on social media triggered by COVID-19-related news articles accompanied by comprehensive insights including user/community susceptibility level, as well as events and popular opinions raised by the crowd while propagating the information. Besides discovering information flow patterns among users, we construct communities among users and develop the propagation forecasting capability, enabling tracing and understanding of how information is disseminated at a higher level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25351;&#23548;&#25945;&#24072;&#26469;&#35757;&#32451;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#35838;&#31243;&#26469;&#36827;&#34892;&#25351;&#23548;&#35843;&#20248;&#65292;&#31216;&#20026;CITING&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#21046;&#23450;&#35780;&#20998;&#26631;&#20934;&#65292;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#36981;&#24490;&#35780;&#20998;&#26631;&#20934;&#21644;&#33258;&#25105;&#32416;&#27491;&#36827;&#34892;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25163;&#24037;&#21046;&#20316;&#25351;&#23548;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#23545;&#40784;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02527</link><description>&lt;p&gt;
CITING: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35838;&#31243;&#35774;&#35745;&#36827;&#34892;&#25351;&#23548;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
CITING: Large Language Models Create Curriculum for Instruction Tuning. (arXiv:2310.02527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25351;&#23548;&#25945;&#24072;&#26469;&#35757;&#32451;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#35838;&#31243;&#26469;&#36827;&#34892;&#25351;&#23548;&#35843;&#20248;&#65292;&#31216;&#20026;CITING&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#21046;&#23450;&#35780;&#20998;&#26631;&#20934;&#65292;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#36981;&#24490;&#35780;&#20998;&#26631;&#20934;&#21644;&#33258;&#25105;&#32416;&#27491;&#36827;&#34892;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25163;&#24037;&#21046;&#20316;&#25351;&#23548;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#23545;&#40784;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26159;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#21644;&#20154;&#24037;&#23545;&#40784;&#30340;&#32467;&#21512;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#25163;&#24037;&#21046;&#20316;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#21644;&#36827;&#34892;&#20154;&#24037;&#23545;&#40784;&#25104;&#20026;&#20102;&#25193;&#23637;LLMs&#24320;&#21457;&#30340;&#29942;&#39048;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;AI&#27169;&#22411;&#20195;&#26367;&#20154;&#31867;&#20316;&#20026;&#25945;&#24072;&#26469;&#35757;&#32451;&#23398;&#29983;LLMs&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#23398;&#29983;&#36890;&#36807;&#36981;&#24490;&#35780;&#20998;&#26631;&#20934;&#21644;&#20174;&#23548;&#24072;&#25552;&#20379;&#30340;&#20462;&#25913;&#20013;&#23398;&#20064;&#26469;&#25552;&#39640;&#20889;&#20316;&#25216;&#24039;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#25945;&#24072;LLM&#26469;&#20026;&#23398;&#29983;LLM&#30340;&#25351;&#23548;&#35843;&#20248;&#21019;&#24314;&#35838;&#31243;&#65292;&#21363;Curriculum Instruction TunING (CITING)&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#65288;1&#65289;&#25945;&#24072;LLM&#21046;&#23450;&#35780;&#20272;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#31572;&#26696;&#30340;&#35780;&#20998;&#26631;&#20934;&#65307;&#65288;2&#65289;&#23398;&#29983;LLM&#23398;&#20064;&#36981;&#24490;&#35780;&#20998;&#26631;&#20934;&#24182;&#36890;&#36807;&#25945;&#24072;&#30340;&#20462;&#25913;&#36827;&#34892;&#33258;&#25105;&#32416;&#27491;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36845;&#20195;&#36827;&#34892;&#36825;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancement of large language models (LLMs) has been achieved through a combo of instruction tuning and human alignment. However, building manually crafted instruction datasets and performing human alignment become the bottleneck for scaling the development of LLMs. In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs. Our method is inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors. Specifically, we employ a teacher LLM to create a curriculum for instruction tuning of the student LLM, namely Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;(FCSG)&#65292;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#21152;&#36895;&#31639;&#27861;(Acc-FCSG-M)&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.02524</link><description>&lt;p&gt;
&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated Conditional Stochastic Optimization. (arXiv:2310.02524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;(FCSG)&#65292;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#21152;&#36895;&#31639;&#27861;(Acc-FCSG-M)&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;&#19981;&#21464;&#23398;&#20064;&#12289;AUPRC&#26368;&#22823;&#21270;&#21644;&#20803;&#23398;&#20064;&#12290;&#38543;&#30528;&#36825;&#20123;&#24212;&#29992;&#20013;&#23545;&#20351;&#29992;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#23545;&#20110;&#39640;&#25928;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#20363;&#22914;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22823;&#12290;&#26412;&#25991;&#32771;&#34385;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;(FCSG)&#65292;&#20854;&#20013;&#21253;&#25324;&#26465;&#20214;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#22522;&#20110;&#21160;&#37327;&#30340;&#31639;&#27861;(FCSG-M)&#12290;&#20026;&#20102;&#36798;&#21040;&#21333;&#26426;&#35774;&#23450;&#19979;&#30340;&#19979;&#30028;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#26041;&#24046;&#20943;&#23569;&#35774;&#35745;&#20102;&#21152;&#36895;&#31639;&#27861;(Acc-FCSG-M)&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#19982;&#29616;&#26377;&#30340;FL&#20013;MAML&#30340;&#20248;&#21270;&#20998;&#26512;&#30456;&#27604;&#65292;&#32852;&#37030;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#32771;&#34385;&#20102;&#26679;&#26412;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional stochastic optimization has found applications in a wide range of machine learning tasks, such as invariant learning, AUPRC maximization, and meta-learning. As the demand for training models with large-scale distributed data grows in these applications, there is an increasing need for communication-efficient distributed optimization algorithms, such as federated learning algorithms. This paper considers the nonconvex conditional stochastic optimization in federated learning and proposes the first federated conditional stochastic optimization algorithm (FCSG) with a conditional stochastic gradient estimator and a momentum-based algorithm (FCSG-M). To match the lower bound complexity in the single-machine setting, we design an accelerated algorithm (Acc-FCSG-M) via the variance reduction to achieve the best sample and communication complexity. Compared with the existing optimization analysis for MAML in FL, federated conditional stochastic optimization considers the sample of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MedDiffusion&#30340;&#26032;&#22411;&#12289;&#31471;&#21040;&#31471;&#30340;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21319;&#20102;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02520</link><description>&lt;p&gt;
MedDiffusion: &#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation. (arXiv:2310.02520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MedDiffusion&#30340;&#26032;&#22411;&#12289;&#31471;&#21040;&#31471;&#30340;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21319;&#20102;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;&#39044;&#27979;&#24314;&#27169;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#26088;&#22312;&#21033;&#29992;&#21382;&#21490;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#21487;&#33021;&#38754;&#20020;&#30340;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#26469;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20363;&#22914;&#20854;&#24207;&#21015;&#29305;&#24615;&#65292;&#39640;&#32500;&#24230;&#21644;&#22266;&#26377;&#22122;&#38899;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24433;&#21709;&#23427;&#20204;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#25968;&#25454;&#29983;&#25104;&#21644;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#26469;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#24448;&#24448;&#21463;&#21040;&#20219;&#21153;&#26080;&#20851;&#35774;&#35745;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;MedDiffusion&#65292;&#26469;&#22686;&#24378;&#39118;&#38505;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health risk prediction is one of the fundamental tasks under predictive modeling in the medical domain, which aims to forecast the potential health risks that patients may face in the future using their historical Electronic Health Records (EHR). Researchers have developed several risk prediction models to handle the unique challenges of EHR data, such as its sequential nature, high dimensionality, and inherent noise. These models have yielded impressive results. Nonetheless, a key issue undermining their effectiveness is data insufficiency. A variety of data generation and augmentation methods have been introduced to mitigate this issue by expanding the size of the training data set through the learning of underlying data distributions. However, the performance of these methods is often limited due to their task-unrelated design. To address these shortcomings, this paper introduces a novel, end-to-end diffusion-based risk prediction model, named MedDiffusion. It enhances risk predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#36827;&#34892;&#20027;&#21160;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#28508;&#22312;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32447;&#32034;&#26469;&#25512;&#26029;&#19978;&#19979;&#25991;&#65292;&#35782;&#21035;&#21644;&#20027;&#21160;&#39044;&#27979;&#29992;&#25143;&#24847;&#22270;&#35201;&#23454;&#29616;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.02506</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#36827;&#34892;&#20027;&#21160;&#30340;&#20154;&#26426;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Proactive Human-Robot Interaction using Visuo-Lingual Transformers. (arXiv:2310.02506v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#36716;&#25442;&#22120;&#36827;&#34892;&#20027;&#21160;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#28508;&#22312;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32447;&#32034;&#26469;&#25512;&#26029;&#19978;&#19979;&#25991;&#65292;&#35782;&#21035;&#21644;&#20027;&#21160;&#39044;&#27979;&#29992;&#25143;&#24847;&#22270;&#35201;&#23454;&#29616;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#25552;&#21462;&#28508;&#22312;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32447;&#32034;&#24182;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#25512;&#26029;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#22312;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#36825;&#20351;&#24471;&#33021;&#22815;&#20027;&#21160;&#39044;&#27979;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#26426;&#22120;&#20154;&#26234;&#33021;&#22320;&#36981;&#24490;&#22522;&#26412;&#25351;&#20196;&#23436;&#25104;&#20219;&#21153;&#65292;&#25110;&#32773;&#22312;&#26397;&#30528;&#30446;&#26631;&#23436;&#25104;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#29305;&#23450;&#30340;&#25163;&#24037;&#35302;&#21457;&#22120;&#26469;&#21551;&#21160;&#20027;&#21160;&#21512;&#20316;&#12290;&#36171;&#20104;&#36825;&#26679;&#30340;&#26426;&#22120;&#20154;&#24605;&#32771;&#26368;&#32456;&#30446;&#26631;&#21644;&#20027;&#21160;&#24314;&#35758;&#20013;&#38388;&#20219;&#21153;&#30340;&#33021;&#21147;&#23558;&#20026;&#20154;&#26426;&#21512;&#20316;&#25552;&#20379;&#19968;&#31181;&#26356;&#30452;&#35266;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22330;&#26223;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#12289;&#29992;&#25143;&#30340;&#35821;&#35328;&#21629;&#20196;&#20197;&#21450;&#20808;&#21069;&#30340;&#29289;&#20307;-&#29289;&#20307;&#20132;&#20114;&#30693;&#35782;&#26469;&#35782;&#21035;&#21644;&#20027;&#21160;&#39044;&#27979;&#29992;&#25143;&#24847;&#22270;&#35201;&#23454;&#29616;&#30340;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ViLing-MMT&#65292;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#65292;&#23427;&#25429;&#25417;&#21040;&#20102;&#20013;&#38388;&#30446;&#26631;&#21644;&#30446;&#26631;&#30446;&#26631;&#20043;&#38388;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess the innate ability to extract latent visuo-lingual cues to infer context through human interaction. During collaboration, this enables proactive prediction of the underlying intention of a series of tasks. In contrast, robotic agents collaborating with humans naively follow elementary instructions to complete tasks or use specific hand-crafted triggers to initiate proactive collaboration when working towards the completion of a goal. Endowing such robots with the ability to reason about the end goal and proactively suggest intermediate tasks will engender a much more intuitive method for human-robot collaboration. To this end, we propose a learning-based method that uses visual cues from the scene, lingual commands from a user and knowledge of prior object-object interaction to identify and proactively predict the underlying goal the user intends to achieve. Specifically, we propose ViLing-MMT, a vision-language multimodal transformer-based architecture that captures int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35745;&#21010;&#35782;&#21035;&#21644;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#23454;&#29616;&#23545;&#20154;&#31867;&#24847;&#22270;&#30340;&#25913;&#36827;&#25512;&#29702;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#32416;&#27491;&#29992;&#25143;&#35745;&#21010;&#30340;&#20559;&#31163;&#21644;&#27425;&#20248;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.02462</link><description>&lt;p&gt;
&#32467;&#21512;&#35745;&#21010;&#35782;&#21035;&#21644;&#35821;&#35328;&#21453;&#39304;&#25913;&#36827;&#20154;&#31867;&#24847;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Improved Inference of Human Intent by Combining Plan Recognition and Language Feedback. (arXiv:2310.02462v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35745;&#21010;&#35782;&#21035;&#21644;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#23454;&#29616;&#23545;&#20154;&#31867;&#24847;&#22270;&#30340;&#25913;&#36827;&#25512;&#29702;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#32416;&#27491;&#29992;&#25143;&#35745;&#21010;&#30340;&#20559;&#31163;&#21644;&#27425;&#20248;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#36741;&#21161;&#26426;&#22120;&#20154;&#33021;&#22815;&#24110;&#21161;&#20154;&#20204;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#35748;&#30693;&#33021;&#21147;&#21463;&#25439;&#30340;&#20154;&#65292;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#28921;&#39274;&#39277;&#33756;&#65292;&#36827;&#34892;&#38203;&#28860;&#65292;&#25110;&#25805;&#20316;&#26426;&#22120;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#20174;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;&#34892;&#20026;&#20013;&#35782;&#21035;&#20154;&#31867;&#30340;&#35745;&#21010;&#21644;&#30446;&#26631;&#65292;&#21363;&#20351;&#29992;&#25143;&#30340;&#34892;&#20026;&#19981;&#22815;&#20248;&#21270;&#12290;&#30446;&#21069;&#30340;&#35745;&#21010;&#21644;&#30446;&#26631;&#35782;&#21035;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#23618;&#27425;&#20219;&#21153;&#32593;&#32476;&#26469;&#24314;&#27169;&#20154;&#31867;&#34892;&#20026;&#32773;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#19981;&#36275;&#20197;&#36890;&#36807;&#33258;&#28982;&#20132;&#20114;&#30340;&#26041;&#24335;&#23454;&#29616;&#29992;&#25143;&#21442;&#19982;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20063;&#27809;&#26377;&#26426;&#21046;&#26469;&#35753;&#29992;&#25143;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#35748;&#30693;&#33021;&#21147;&#21463;&#25439;&#30340;&#20154;&#65292;&#30693;&#36947;&#20182;&#20204;&#21407;&#22987;&#35745;&#21010;&#30340;&#20559;&#31163;&#25110;&#26397;&#30528;&#30446;&#26631;&#37319;&#21462;&#30340;&#20219;&#20309;&#27425;&#20248;&#34892;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#26469;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#20013;&#30340;&#35745;&#21010;&#21644;&#30446;&#26631;&#35782;&#21035;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#32416;&#27491;&#23545;&#20154;&#31867;&#30340;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational assistive robots can aid people, especially those with cognitive impairments, to accomplish various tasks such as cooking meals, performing exercises, or operating machines. However, to interact with people effectively, robots must recognize human plans and goals from noisy observations of human actions, even when the user acts sub-optimally. Previous works on Plan and Goal Recognition (PGR) as planning have used hierarchical task networks (HTN) to model the actor/human. However, these techniques are insufficient as they do not have user engagement via natural modes of interaction such as language. Moreover, they have no mechanisms to let users, especially those with cognitive impairments, know of a deviation from their original plan or about any sub-optimal actions taken towards their goal. We propose a novel framework for plan and goal recognition in partially observable domains -- Dialogue for Goal Recognition (D4GR) enabling a robot to rectify its belief in human pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#19978;&#23398;&#21040;&#30340;&#26159;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;&#32780;&#19981;&#26159;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#38169;&#35823;&#30340;&#20351;&#29992;&#26041;&#24335;&#34429;&#28982;&#19981;&#29305;&#21035;&#26377;&#23475;&#65292;&#20294;&#19982;&#27491;&#30830;&#30340;&#36138;&#23146;&#26368;&#22823;&#21270;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;&#30456;&#27604;&#20173;&#19981;&#22815;&#29702;&#24819;&#12290;</title><link>http://arxiv.org/abs/2310.02456</link><description>&lt;p&gt;
&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#26368;&#20339;&#20248;&#21183;&#65292;&#24182;&#23558;&#20854;&#35823;&#35299;&#20026;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Advantage from Preferences and Mistaking it for Reward. (arXiv:2310.02456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#19978;&#23398;&#21040;&#30340;&#26159;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;&#32780;&#19981;&#26159;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#31181;&#38169;&#35823;&#30340;&#20351;&#29992;&#26041;&#24335;&#34429;&#28982;&#19981;&#29305;&#21035;&#26377;&#23475;&#65292;&#20294;&#19982;&#27491;&#30830;&#30340;&#36138;&#23146;&#26368;&#22823;&#21270;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;&#30456;&#27604;&#20173;&#19981;&#22815;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#20154;&#31867;&#23545;&#36712;&#36857;&#29255;&#27573;&#23545;&#30340;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#65292;&#36825;&#22312;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#20013;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20551;&#35774;&#20154;&#31867;&#20559;&#22909;&#20165;&#22522;&#20110;&#36825;&#20123;&#29255;&#27573;&#20013;&#31215;&#32047;&#30340;&#22870;&#21169;&#25110;&#20854;&#37096;&#20998;&#22238;&#25253;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#23545;&#36825;&#19968;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#24576;&#30097;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#25022;&#30340;&#26367;&#20195;&#20559;&#22909;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#20551;&#35774;&#20559;&#22909;&#26159;&#22522;&#20110;&#37096;&#20998;&#22238;&#25253;&#32780;&#23454;&#38469;&#19978;&#26469;&#33258;&#36951;&#25022;&#26102;&#30340;&#21518;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#23398;&#21040;&#30340;&#20989;&#25968;&#26159;&#26368;&#20339;&#20248;&#21183;&#20989;&#25968;$\hat{A^*_r}$&#30340;&#36817;&#20284;&#65292;&#32780;&#19981;&#26159;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#35299;&#20915;&#20102;&#29305;&#23450;&#30340;&#38519;&#38449;&#65292;&#36825;&#31181;&#38169;&#35823;&#20551;&#35774;&#24182;&#19981;&#29305;&#21035;&#26377;&#23475;&#65292;&#32467;&#26524;&#26159;&#19968;&#20010;&#39640;&#24230;&#21464;&#24418;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#31181;&#38169;&#35823;&#20351;&#29992;$\hat{A^*_r}$&#30340;&#26041;&#24335;&#19981;&#22914;&#36866;&#24403;&#19988;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#8212;&#8212;&#36138;&#23146;&#26368;&#22823;&#21270;$\hat{A^*_r}$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider algorithms for learning reward functions from human preferences over pairs of trajectory segments, as used in reinforcement learning from human feedback (RLHF). Most recent work assumes that human preferences are generated based only upon the reward accrued within those segments, or their partial return. Recent work casts doubt on the validity of this assumption, proposing an alternative preference model based upon regret. We investigate the consequences of assuming preferences are based upon partial return when they actually arise from regret. We argue that the learned function is an approximation of the optimal advantage function, $\hat{A^*_r}$, not a reward function. We find that if a specific pitfall is addressed, this incorrect assumption is not particularly harmful, resulting in a highly shaped reward function. Nonetheless, this incorrect usage of $\hat{A^*_r}$ is less desirable than the appropriate and simpler approach of greedy maximization of $\hat{A^*_r}$. From th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;</title><link>http://arxiv.org/abs/2310.02446</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#36234;&#29425; GPT-4
&lt;/p&gt;
&lt;p&gt;
Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02446
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#22521;&#35757;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32418;&#38431;&#27979;&#35797;&#26159;&#20943;&#23569;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25104;&#21151;&#32469;&#36807;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#30340;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#22312;AdvBenchmark&#20013;&#65292;GPT-4&#38024;&#23545;&#19981;&#23433;&#20840;&#30340;&#32763;&#35793;&#36755;&#20837;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;79%&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#26696;&#65292;&#20351;&#29992;&#25143;&#23454;&#29616;&#20854;&#26377;&#23475;&#30446;&#26631;&#65292;&#36825;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#20854;&#20182;&#39640;/&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#36328;&#35821;&#35328;&#28431;&#27934;&#20027;&#35201;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20197;&#21069;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#38480;&#35757;&#32451;&#20027;&#35201;&#24433;&#21709;&#37027;&#20123;&#20351;&#29992;&#36825;&#20123;&#35821;&#35328;&#30340;&#20154;&#65292;&#36896;&#25104;&#25216;&#26415;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#36716;&#21464;&#65306;
&lt;/p&gt;
&lt;p&gt;
AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift:
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#20248;&#21270;&#35270;&#35282;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#20013;&#25552;&#21462;&#22810;&#26679;&#24615;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#25968;&#20540;&#20989;&#25968;&#36827;&#34892;&#32422;&#26463;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#22810;&#26679;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#24341;&#20837;&#21560;&#24341;-&#25490;&#26021;&#22870;&#21169;&#39033;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#19968;&#27493;&#25511;&#21046;&#22810;&#26679;&#24615;&#27700;&#24179;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#23616;&#37096;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23637;&#29616;&#20986;&#22810;&#26679;&#30340;&#28789;&#25935;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.02440</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#37325;&#32422;&#26463;&#19979;&#23616;&#37096;&#23548;&#33322;&#30340;&#22810;&#26679;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Skills for Local Navigation under Multi-constraint Optimality. (arXiv:2310.02440v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#20248;&#21270;&#35270;&#35282;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#20013;&#25552;&#21462;&#22810;&#26679;&#24615;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#25968;&#20540;&#20989;&#25968;&#36827;&#34892;&#32422;&#26463;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#22810;&#26679;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#24341;&#20837;&#21560;&#24341;-&#25490;&#26021;&#22870;&#21169;&#39033;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#19968;&#27493;&#25511;&#21046;&#22810;&#26679;&#24615;&#27700;&#24179;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#23616;&#37096;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23637;&#29616;&#20986;&#22810;&#26679;&#30340;&#28789;&#25935;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#26377;&#35768;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20294;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#22810;&#26679;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#38656;&#35201;&#22312;&#20219;&#21153;&#25191;&#34892;&#19978;&#20570;&#20986;&#22949;&#21327;&#12290;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#20219;&#21153;&#35201;&#27714;&#34987;&#25351;&#23450;&#20026;&#22810;&#20010;&#22870;&#21169;&#39033;&#65292;&#27599;&#20010;&#22870;&#21169;&#39033;&#37117;&#38656;&#35201;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20197;&#32422;&#26463;&#20248;&#21270;&#35270;&#35282;&#30740;&#31350;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#26435;&#34913;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#25968;&#20540;&#20989;&#25968;&#36827;&#34892;&#32422;&#26463;&#26469;&#33719;&#24471;&#22810;&#26679;&#30340;&#31574;&#30053;&#65292;&#20854;&#20013;&#25968;&#20540;&#20989;&#25968;&#26159;&#36890;&#36807;&#19981;&#21516;&#30340;&#22870;&#21169;&#23450;&#20041;&#30340;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19968;&#33268;&#65292;&#36890;&#36807;Van der Waals&#21147;&#28608;&#21457;&#30340;&#21560;&#24341;-&#25490;&#26021;&#22870;&#21169;&#39033;&#21487;&#20197;&#23454;&#29616;&#23545;&#22810;&#26679;&#24615;&#27700;&#24179;&#30340;&#36827;&#19968;&#27493;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#20010;&#23616;&#37096;&#23548;&#33322;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#19968;&#20010;&#22235;&#36275;&#26426;&#22120;&#20154;&#38656;&#35201;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#21040;&#36798;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#30495;&#23454;&#30340;12-Dof&#22235;&#36275;&#26426;&#22120;&#20154;Solo12&#19978;&#24471;&#21040;&#33391;&#22909;&#30340;&#36716;&#31227;&#65292;&#24182;&#23637;&#29616;&#20986;&#22810;&#26679;&#30340;&#28789;&#25935;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Despite many successful applications of data-driven control in robotics, extracting meaningful diverse behaviors remains a challenge. Typically, task performance needs to be compromised in order to achieve diversity. In many scenarios, task requirements are specified as a multitude of reward terms, each requiring a different trade-off. In this work, we take a constrained optimization viewpoint on the quality-diversity trade-off and show that we can obtain diverse policies while imposing constraints on their value functions which are defined through distinct rewards. In line with previous work, further control of the diversity level can be achieved through an attract-repel reward term motivated by the Van der Waals force. We demonstrate the effectiveness of our method on a local navigation task where a quadruped robot needs to reach the target within a finite horizon. Finally, our trained policies transfer well to the real 12-DoF quadruped robot, Solo12, and exhibit diverse agile behavi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#26234;&#33021;&#20307;&#23398;&#20064;&#20309;&#26102;&#21644;&#22914;&#20309;&#36827;&#34892;&#36890;&#20449;&#65292;&#20174;&#32780;&#23454;&#29616;&#33391;&#22909;&#30340;&#21327;&#35843;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02435</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#24449;&#20132;&#27969;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22823;&#35268;&#27169;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control. (arXiv:2310.02435v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#26234;&#33021;&#20307;&#23398;&#20064;&#20309;&#26102;&#21644;&#22914;&#20309;&#36827;&#34892;&#36890;&#20449;&#65292;&#20174;&#32780;&#23454;&#29616;&#33391;&#22909;&#30340;&#21327;&#35843;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#36827;&#34892;&#35299;&#20915;&#12290;&#23613;&#31649;&#23545;&#20110;&#22823;&#35268;&#27169;TSC&#38382;&#39064;&#26469;&#35828;&#65292;&#38598;&#20013;&#24335;&#26041;&#27861;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20294;&#20998;&#25955;&#24335;&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#27604;&#22914;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#12290;&#22312;&#20998;&#25955;&#24335;MARL&#20013;&#65292;&#36890;&#20449;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#24517;&#39035;&#36890;&#36807;&#28040;&#24687;&#20132;&#25442;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#31995;&#32479;&#24182;&#23454;&#29616;&#26377;&#25928;&#21327;&#35843;&#12290;&#28145;&#24230;MARL&#24050;&#32463;&#34987;&#29992;&#26469;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#23398;&#20064;&#36890;&#20449;&#21327;&#35758;&#20174;&#32780;&#23454;&#29616;&#26234;&#33021;&#20307;&#38388;&#30340;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20026;TSC&#25552;&#20986;&#30340;&#28145;&#24230;MARL&#36890;&#20449;&#26694;&#26550;&#20801;&#35768;&#26234;&#33021;&#20307;&#22312;&#20219;&#20309;&#26102;&#20505;&#19982;&#20854;&#20182;&#25152;&#26377;&#26234;&#33021;&#20307;&#36827;&#34892;&#36890;&#20449;&#65292;&#36825;&#21487;&#33021;&#20250;&#22686;&#21152;&#31995;&#32479;&#20013;&#30340;&#22122;&#22768;&#24182;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#30340;&#22823;&#35268;&#27169;TSC&#30340;MARL&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#27599;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#22312;&#20309;&#26102;&#21644;&#22914;&#20309;&#36827;&#34892;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#21327;&#35843;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic signal control (TSC) is a challenging problem within intelligent transportation systems and has been tackled using multi-agent reinforcement learning (MARL). While centralized approaches are often infeasible for large-scale TSC problems, decentralized approaches provide scalability but introduce new challenges, such as partial observability. Communication plays a critical role in decentralized MARL, as agents must learn to exchange information using messages to better understand the system and achieve effective coordination. Deep MARL has been used to enable inter-agent communication by learning communication protocols in a differentiable manner. However, many deep MARL communication frameworks proposed for TSC allow agents to communicate with all other agents at all times, which can add to the existing noise in the system and degrade overall performance. In this study, we propose a communication-based MARL framework for large-scale TSC. Our framework allows each agent to learn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29255;&#27573;&#35760;&#24518;&#29702;&#35770;(EMT)&#65292;&#23558;&#21453;&#22797;&#31070;&#32463;&#32593;&#32476;(RNN)&#27010;&#24565;&#21270;&#20026;&#36890;&#29992;&#24207;&#21015;&#29255;&#27573;&#35760;&#24518;&#27169;&#22411;&#30340;&#31163;&#25955;&#26102;&#38388;&#31867;&#27604;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;EMT&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31639;&#27861;&#20219;&#21153;&#65292;&#21457;&#29616;&#21463;&#35757;&#32451;&#30340;RNN&#22987;&#32456;&#20250;&#25910;&#25947;&#21040;&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;&#65292;&#25581;&#31034;&#20102;RNN&#21160;&#21147;&#23398;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#25581;&#31034;&#21464;&#37327;&#30340;&#26102;&#38388;&#23384;&#20648;&#21644;&#32452;&#21512;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#12290;</title><link>http://arxiv.org/abs/2310.02430</link><description>&lt;p&gt;
&#21453;&#22797;&#31070;&#32463;&#32593;&#32476;(RNN)&#26426;&#21046;&#35299;&#37322;&#30340;&#29255;&#27573;&#35760;&#24518;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks. (arXiv:2310.02430v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02430
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29255;&#27573;&#35760;&#24518;&#29702;&#35770;(EMT)&#65292;&#23558;&#21453;&#22797;&#31070;&#32463;&#32593;&#32476;(RNN)&#27010;&#24565;&#21270;&#20026;&#36890;&#29992;&#24207;&#21015;&#29255;&#27573;&#35760;&#24518;&#27169;&#22411;&#30340;&#31163;&#25955;&#26102;&#38388;&#31867;&#27604;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;EMT&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31639;&#27861;&#20219;&#21153;&#65292;&#21457;&#29616;&#21463;&#35757;&#32451;&#30340;RNN&#22987;&#32456;&#20250;&#25910;&#25947;&#21040;&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;&#65292;&#25581;&#31034;&#20102;RNN&#21160;&#21147;&#23398;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#25581;&#31034;&#21464;&#37327;&#30340;&#26102;&#38388;&#23384;&#20648;&#21644;&#32452;&#21512;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#21453;&#22797;&#31070;&#32463;&#32593;&#32476;(RNN)&#30340;&#22797;&#26434;&#25805;&#20316;&#23545;&#20110;&#25512;&#21160;&#20854;&#33021;&#21147;&#21644;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#36861;&#27714;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29255;&#27573;&#35760;&#24518;&#29702;&#35770;(EMT)&#65292;&#35828;&#26126;&#20102;RNN&#21487;&#20197;&#34987;&#27010;&#24565;&#21270;&#20026;&#26368;&#36817;&#25552;&#20986;&#30340;&#36890;&#29992;&#24207;&#21015;&#29255;&#27573;&#35760;&#24518;&#27169;&#22411;&#30340;&#31163;&#25955;&#26102;&#38388;&#31867;&#27604;&#12290;&#20026;&#20102;&#35777;&#23454;EMT&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#31639;&#27861;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;RNN&#30340;&#21464;&#37327;&#32465;&#23450;&#34892;&#20026;&#12290;&#21033;&#29992;EMT&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#25968;&#23398;&#20005;&#35880;&#30340;&#30005;&#36335;&#65292;&#29992;&#20110;&#20419;&#36827;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#21464;&#37327;&#32465;&#23450;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;RNN&#22987;&#32456;&#20250;&#25910;&#25947;&#21040;&#21464;&#37327;&#32465;&#23450;&#30005;&#36335;&#65292;&#20174;&#32780;&#34920;&#26126;&#20102;RNN&#21160;&#21147;&#23398;&#30340;&#26222;&#36941;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#23450;&#20041;&#19968;&#20010;&#29305;&#26435;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#22312;&#26102;&#38388;&#20648;&#23384;&#21644;&#32452;&#21512;&#21464;&#37327;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#65292;&#36825;&#26159;&#25104;&#21151;&#25512;&#24191;&#36825;&#20123;&#20219;&#21153;&#30340;&#20851;&#38190;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the intricate operations of Recurrent Neural Networks (RNNs) mechanistically is pivotal for advancing their capabilities and applications. In this pursuit, we propose the Episodic Memory Theory (EMT), illustrating that RNNs can be conceptualized as discrete-time analogs of the recently proposed General Sequential Episodic Memory Model. To substantiate EMT, we introduce a novel set of algorithmic tasks tailored to probe the variable binding behavior in RNNs. Utilizing the EMT, we formulate a mathematically rigorous circuit that facilitates variable binding in these tasks. Our empirical investigations reveal that trained RNNs consistently converge to the variable binding circuit, thus indicating universality in the dynamics of RNNs. Building on these findings, we devise an algorithm to define a privileged basis, which reveals hidden neurons instrumental in the temporal storage and composition of variables, a mechanism vital for the successful generalization in these tasks. 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#37325;&#25918;&#26080;&#38556;&#30861;&#27979;&#35797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20687;&#32032;&#30340;&#29992;&#25143;&#30028;&#38754;&#29702;&#35299;&#27169;&#22411;&#25191;&#34892;&#27979;&#35797;&#24182;&#29983;&#25104;&#21487;&#23548;&#33322;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#24320;&#21457;&#20154;&#21592;&#21644;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#26080;&#38556;&#30861;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02424</link><description>&lt;p&gt;
AXNav: &#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#37325;&#25918;&#26080;&#38556;&#30861;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AXNav: Replaying Accessibility Tests from Natural Language. (arXiv:2310.02424v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#37325;&#25918;&#26080;&#38556;&#30861;&#27979;&#35797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20687;&#32032;&#30340;&#29992;&#25143;&#30028;&#38754;&#29702;&#35299;&#27169;&#22411;&#25191;&#34892;&#27979;&#35797;&#24182;&#29983;&#25104;&#21487;&#23548;&#33322;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#24320;&#21457;&#20154;&#21592;&#21644;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#26080;&#38556;&#30861;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#32773;&#21644;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#36890;&#24120;&#20381;&#36182;&#25163;&#21160;&#27979;&#35797;&#26469;&#22312;&#20135;&#21697;&#29983;&#21629;&#21608;&#26399;&#20013;&#27979;&#35797;&#26080;&#38556;&#30861;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#27979;&#35797;&#21487;&#33021;&#24456;&#20047;&#21619;&#65292;&#33539;&#22260;&#24222;&#22823;&#65292;&#24182;&#19988;&#24456;&#38590;&#23433;&#25490;&#22312;&#20854;&#20182;&#24320;&#21457;&#37324;&#31243;&#30865;&#20043;&#38388;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#29992;&#25143;&#30028;&#38754;&#65292;&#20294;&#25454;&#25105;&#20204;&#20102;&#35299;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#20154;&#25506;&#32034;&#36807;&#23427;&#20204;&#22312;&#25511;&#21046;&#36741;&#21161;&#25216;&#26415;&#20197;&#25903;&#25345;&#26080;&#38556;&#30861;&#27979;&#35797;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#39033;&#24418;&#25104;&#24615;&#30740;&#31350;&#24320;&#22987;&#65292;&#25506;&#35752;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#26080;&#38556;&#30861;&#27979;&#35797;&#24037;&#20316;&#27969;&#31243;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20197;&#25163;&#21160;&#26080;&#38556;&#30861;&#27979;&#35797;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#8220;&#22312;VoiceOver&#20013;&#25628;&#32034;&#19968;&#20010;&#33410;&#30446;&#8221;&#65289;&#65292;&#24182;&#20351;&#29992;LLM&#32467;&#21512;&#22522;&#20110;&#20687;&#32032;&#30340;&#29992;&#25143;&#30028;&#38754;&#29702;&#35299;&#27169;&#22411;&#26469;&#25191;&#34892;&#27979;&#35797;&#24182;&#29983;&#25104;&#31456;&#33410;&#21010;&#20998;&#30340;&#21487;&#23548;&#33322;&#35270;&#39057;&#12290;&#22312;&#27599;&#20010;&#35270;&#39057;&#20013;&#65292;&#20026;&#20102;&#24110;&#21161;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#65292;&#25105;&#20204;&#24212;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#26631;&#35760;ac&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs, however to our knowledge no one has yet explored their use in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes as input a manual accessibility test (e.g., ``Search for a show in VoiceOver'') and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers we apply heuristics to detect and flag ac
&lt;/p&gt;</description></item><item><title>OneAdapt&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#65292;&#28385;&#36275;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#22312;&#37197;&#32622;&#21442;&#25968;&#26041;&#38754;&#30340;&#19977;&#20010;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.02422</link><description>&lt;p&gt;
OneAdapt&#65306;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24555;&#36895;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation. (arXiv:2310.02422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02422
&lt;/p&gt;
&lt;p&gt;
OneAdapt&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#65292;&#28385;&#36275;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#22312;&#37197;&#32622;&#21442;&#25968;&#26041;&#38754;&#30340;&#19977;&#20010;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27969;&#23186;&#20307;&#25968;&#25454;&#30340;&#25512;&#26029;&#26041;&#38754;&#24050;&#32463;&#26222;&#21450;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;LiDAR&#25968;&#25454;&#21644;&#38899;&#39057;&#27874;&#24418;&#20013;&#30340;&#25991;&#26412;&#25552;&#21462;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25512;&#26029;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#32593;&#32476;&#24102;&#23485;&#26469;&#25910;&#38598;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#24182;&#19988;&#38656;&#35201;&#24191;&#27867;&#30340;GPU&#36164;&#28304;&#26469;&#36816;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#23613;&#31649;&#36890;&#36807;&#20248;&#21270;&#37197;&#32622;&#21442;&#25968;&#65288;&#22914;&#35270;&#39057;&#20998;&#36776;&#29575;&#21644;&#24103;&#29575;&#65289;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#23545;&#32593;&#32476;&#24102;&#23485;&#21644;GPU&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#20294;&#30446;&#21069;&#30340;&#33258;&#36866;&#24212;&#25216;&#26415;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#19977;&#20010;&#35201;&#27714;&#65306;&#65288;i&#65289;&#20197;&#26368;&#23567;&#30340;&#39069;&#22806;GPU&#25110;&#24102;&#23485;&#24320;&#38144;&#26469;&#33258;&#36866;&#24212;&#37197;&#32622;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#25968;&#25454;&#23545;&#26368;&#32456;DNN&#30340;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26469;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#20915;&#31574;&#65307;&#65288;iii&#65289;&#38024;&#23545;&#19968;&#31995;&#21015;&#37197;&#32622;&#21442;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OneAdapt&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#33258;&#36866;&#24212;&#37197;&#32622;&#21442;&#25968;&#65292;&#28385;&#36275;&#20102;&#36825;&#20123;&#35201;&#27714;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20805;&#20998;&#21033;&#29992;DNN&#30340;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
Deep learning inference on streaming media data, such as object detection in video or LiDAR feeds and text extraction from audio waves, is now ubiquitous. To achieve high inference accuracy, these applications typically require significant network bandwidth to gather high-fidelity data and extensive GPU resources to run deep neural networks (DNNs). While the high demand for network bandwidth and GPU resources could be substantially reduced by optimally adapting the configuration knobs, such as video resolution and frame rate, current adaptation techniques fail to meet three requirements simultaneously: adapt configurations (i) with minimum extra GPU or bandwidth overhead; (ii) to reach near-optimal decisions based on how the data affects the final DNN's accuracy, and (iii) do so for a range of configuration knobs. This paper presents OneAdapt, which meets these requirements by leveraging a gradient-ascent strategy to adapt configuration knobs. The key idea is to embrace DNNs' different
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20851;&#38190;&#21407;&#29702;&#21644;&#25104;&#21151;&#35201;&#32032;&#65292;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25351;&#20986;&#30693;&#35782;&#33976;&#39311;&#26377;&#28508;&#21147;&#25104;&#20026;&#20851;&#38190;&#30340;&#25216;&#26415;&#36716;&#25240;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02421</link><description>&lt;p&gt;
&#23398;&#29983;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#19982;&#20854;&#25945;&#24072;&#19968;&#26679;&#34920;&#29616;&#20986;&#33394;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can a student Large Language Model perform as well as it's teacher?. (arXiv:2310.02421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02421
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20851;&#38190;&#21407;&#29702;&#21644;&#25104;&#21151;&#35201;&#32032;&#65292;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25351;&#20986;&#30693;&#35782;&#33976;&#39311;&#26377;&#28508;&#21147;&#25104;&#20026;&#20851;&#38190;&#30340;&#25216;&#26415;&#36716;&#25240;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#34429;&#28982;&#33021;&#23454;&#29616;&#26080;&#19982;&#20262;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#19981;&#21487;&#36991;&#20813;&#22320;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#24102;&#26469;&#37096;&#32626;&#25361;&#25112;&#12290;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#19968;&#31181;&#23558;&#39640;&#23481;&#37327;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#31616;&#21270;&#30340;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#24378;&#35843;&#20102;&#20854;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#36719;&#26631;&#31614;&#30340;&#23454;&#29992;&#24615;&#21644;&#28201;&#24230;&#32553;&#25918;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#25104;&#21151;&#33976;&#39311;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#23398;&#29983;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#25945;&#24072;&#30340;&#27700;&#24179;&#20197;&#21450;&#36229;&#21442;&#25968;&#30340;&#24179;&#34913;&#12290;&#21516;&#26102;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#19968;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#20984;&#26174;&#20102;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#36716;&#25240;&#28857;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity "teacher" model to a streamlined "student" model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal 
&lt;/p&gt;</description></item><item><title>Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.02409</link><description>&lt;p&gt;
Nugget 2D&#65306;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02409
&lt;/p&gt;
&lt;p&gt;
Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20013;&#32553;&#25918;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23558;Qin&#65286;Van Durme&#65288;2023&#24180;&#65289;&#30340;Nugget&#26041;&#27861;&#20174;BERT&#31867;&#26694;&#26550;&#25193;&#23637;&#21040;&#20165;&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#24314;&#27169;&#20026;&#21387;&#32553;&#30340;&#8220;nuggets&#8221;&#65292;&#36825;&#20123;&#8220;nuggets&#8221;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36827;&#34892;&#37325;&#24314;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#35832;&#22914;LLaMA&#20043;&#31867;&#30340;&#29616;&#25104;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Nugget2D&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#20445;&#30041;&#20102;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#32534;&#30721;&#23454;&#39564;&#20013;&#65292;Nugget2D&#21487;&#20197;&#20197;20&#20493;&#30340;&#21387;&#32553;&#27604;&#25910;&#32553;&#19978;&#19979;&#25991;&#65292;&#37325;&#24314;&#26102;&#30340;BLEU&#24471;&#20998;&#20026;98&#65285;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin &amp; Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PCGPT&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#20256;&#32479;PCG&#26041;&#27861;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#20102;&#26356;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#28216;&#25103;&#20869;&#23481;&#65292;&#24182;&#19988;&#22312;&#26356;&#23569;&#30340;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02405</link><description>&lt;p&gt;
PCGPT&#65306;&#21033;&#29992;&#36716;&#25442;&#22120;&#36827;&#34892;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PCGPT: Procedural Content Generation via Transformers. (arXiv:2310.02405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PCGPT&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#20256;&#32479;PCG&#26041;&#27861;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#20102;&#26356;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#28216;&#25103;&#20869;&#23481;&#65292;&#24182;&#19988;&#22312;&#26356;&#23569;&#30340;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PCGPT&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;Transformer&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;(PCG)&#30340;&#21019;&#26032;&#26041;&#27861;&#12290; PCGPT&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#36845;&#20195;&#29983;&#25104;&#28216;&#25103;&#20851;&#21345;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;PCG&#26041;&#27861;&#20013;&#37325;&#22797;&#12289;&#21487;&#39044;&#27979;&#25110;&#19981;&#19968;&#33268;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;Transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23545;&#21160;&#20316;&#12289;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#36712;&#36857;&#36827;&#34892;&#24314;&#27169;&#12290;&#35813;&#26041;&#27861;&#22312;Sokoban&#30410;&#26234;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#27169;&#22411;&#39044;&#27979;&#25152;&#38656;&#29289;&#21697;&#21450;&#20854;&#30456;&#24212;&#20301;&#32622;&#12290;&#22312;Sokoban&#28216;&#25103;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PCGPT&#29983;&#25104;&#20102;&#26356;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#28216;&#25103;&#20869;&#23481;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20197;&#26174;&#33879;&#36739;&#23569;&#30340;&#27493;&#39588;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#28216;&#25103;&#35774;&#35745;&#21644;&#22312;&#32447;&#20869;&#23481;&#29983;&#25104;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the PCGPT framework, an innovative approach to procedural content generation (PCG) using offline reinforcement learning and transformer networks. PCGPT utilizes an autoregressive model based on transformers to generate game levels iteratively, addressing the challenges of traditional PCG methods such as repetitive, predictable, or inconsistent content. The framework models trajectories of actions, states, and rewards, leveraging the transformer's self-attention mechanism to capture temporal dependencies and causal relationships. The approach is evaluated in the Sokoban puzzle game, where the model predicts items that are needed with their corresponding locations. Experimental results on the game Sokoban demonstrate that PCGPT generates more complex and diverse game content. Interestingly, it achieves these results in significantly fewer steps compared to existing methods, showcasing its potential for enhancing game design and online content generation. Our model repr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02391</link><description>&lt;p&gt;
SE(3)-&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#20013;&#30340;&#38543;&#26426;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#19977;&#32500;&#21018;&#20307;&#36816;&#21160;&#65288;&#21363;SE(3)&#32676;&#65289;&#30340;&#27969;&#21305;&#37197;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#26029;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65306;FoldFlow&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#20027;&#38142;&#30340;&#20934;&#30830;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FoldFlow-Base&#65292;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#23398;&#20064;&#30830;&#23450;&#24615;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#21305;&#37197;&#19981;&#21464;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Riemannian&#26368;&#20248;&#20256;&#36755;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;FoldFlow-OT&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#26356;&#31616;&#21333;&#21644;&#31283;&#23450;&#30340;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;FoldFlow-SFM&#65292;&#23558;Riemannian&#26368;&#20248;&#20256;&#36755;&#21644;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;SE(3)&#19978;&#30340;&#38543;&#26426;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;FoldFlow&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23398;&#20064;&#30340;&#29289;&#29702;&#36215;&#28304;&#65292;&#24182;&#25552;&#20986;&#23398;&#20064;&#21487;&#33021;&#20855;&#26377;&#38750;&#29983;&#29289;&#21644;&#38750;&#36827;&#21270;&#36215;&#28304;&#30340;&#21487;&#33021;&#24615;&#12290;&#20316;&#32773;&#21457;&#29616;&#33021;&#22815;&#22312;&#31616;&#21333;&#30340;&#29289;&#29702;&#27169;&#22411;&#20013;&#35266;&#23519;&#12289;&#35299;&#37322;&#21644;&#20934;&#30830;&#37325;&#29616;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02375</link><description>&lt;p&gt;
&#20851;&#20110;&#23398;&#20064;&#30340;&#29289;&#29702;&#36215;&#28304;
&lt;/p&gt;
&lt;p&gt;
On Physical Origins of Learning. (arXiv:2310.02375v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23398;&#20064;&#30340;&#29289;&#29702;&#36215;&#28304;&#65292;&#24182;&#25552;&#20986;&#23398;&#20064;&#21487;&#33021;&#20855;&#26377;&#38750;&#29983;&#29289;&#21644;&#38750;&#36827;&#21270;&#36215;&#28304;&#30340;&#21487;&#33021;&#24615;&#12290;&#20316;&#32773;&#21457;&#29616;&#33021;&#22815;&#22312;&#31616;&#21333;&#30340;&#29289;&#29702;&#27169;&#22411;&#20013;&#35266;&#23519;&#12289;&#35299;&#37322;&#21644;&#20934;&#30830;&#37325;&#29616;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26234;&#33021;&#30340;&#36215;&#28304;&#30340;&#25506;&#32034;&#24341;&#21457;&#20102;&#26377;&#20851;&#33258;&#28982;&#31995;&#32479;&#20013;&#23398;&#20064;&#33021;&#21147;&#30340;&#36827;&#21270;&#30340;&#26377;&#36259;&#38382;&#39064;&#12290;&#20026;&#20160;&#20040;&#29983;&#29289;&#20307;&#20855;&#26377;&#33719;&#21462;&#26410;&#30693;&#30693;&#35782;&#30340;&#22266;&#26377;&#21160;&#21147;&#65311;&#36825;&#31181;&#21160;&#21147;&#26159;&#21542;&#20165;&#36890;&#36807;&#33258;&#28982;&#36873;&#25321;&#26469;&#35299;&#37322;&#65292;&#22240;&#20026;&#33021;&#22815;&#23398;&#20064;&#30340;&#31995;&#32479;&#30001;&#20110;&#22686;&#21152;&#20102;&#29983;&#23384;&#26426;&#20250;&#32780;&#21463;&#21040;&#38738;&#30544;&#65311;&#36824;&#26159;&#23384;&#22312;&#20854;&#20182;&#26356;&#24555;&#36895;&#30340;&#26426;&#21046;&#65292;&#20197;&#8220;&#27491;&#30830;&#30340;&#26041;&#24335;&#8221;&#21363;&#21051;&#22870;&#21169;&#36827;&#20837;&#8220;&#23398;&#20064;&#27169;&#24335;&#8221;&#30340;&#31995;&#32479;&#65311;&#26412;&#25991;&#25506;&#35752;&#20102;&#21518;&#19968;&#31181;&#21487;&#33021;&#24615;&#65292;&#24182;&#21162;&#21147;&#25581;&#31034;&#36825;&#20123;&#26041;&#24335;&#30340;&#21487;&#33021;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#21487;&#33021;&#20855;&#26377;&#38750;&#29983;&#29289;&#21644;&#38750;&#36827;&#21270;&#36215;&#28304;&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;&#21487;&#20197;&#22312;&#31616;&#21333;&#30340;&#29289;&#29702;&#27169;&#22411;&#20013;&#35266;&#23519;&#12289;&#35299;&#37322;&#21644;&#20934;&#30830;&#37325;&#29616;&#25551;&#36848;&#24320;&#25918;&#35856;&#25391;&#22411;&#31995;&#32479;&#20013;&#33021;&#37327;&#31215;&#32047;&#26426;&#21046;&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest to comprehend the origins of intelligence raises intriguing questions about the evolution of learning abilities in natural systems. Why do living organisms possess an inherent drive to acquire knowledge of the unknown? Is this motivation solely explicable through natural selection, favoring systems capable of learning due to their increased chances of survival? Or do there exist additional, more rapid mechanisms that offer immediate rewards to systems entering the "learning mode" in the "right ways"? This article explores the latter possibility and endeavors to unravel the possible nature of these ways. We propose that learning may have non-biological and non-evolutionary origin. It turns out that key properties of learning can be observed, explained, and accurately reproduced within simple physical models that describe energy accumulation mechanisms in open resonant-type systems with dissipation.
&lt;/p&gt;</description></item><item><title>ProtoNER&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;KVP&#25552;&#21462;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#28155;&#21152;&#26032;&#31867;&#21035;&#65292;&#32780;&#21482;&#38656;&#26368;&#23569;&#25968;&#37327;&#30340;&#26032;&#27880;&#37322;&#35757;&#32451;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.02372</link><description>&lt;p&gt;
ProtoNER: &#20351;&#29992;&#21407;&#22411;&#32593;&#32476;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#23569;&#26679;&#26412;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks. (arXiv:2310.02372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02372
&lt;/p&gt;
&lt;p&gt;
ProtoNER&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;KVP&#25552;&#21462;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#28155;&#21152;&#26032;&#31867;&#21035;&#65292;&#32780;&#21482;&#38656;&#26368;&#23569;&#25968;&#37327;&#30340;&#26032;&#27880;&#37322;&#35757;&#32451;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38190;&#20540;&#23545;&#65288;KVP&#65289;&#25552;&#21462;&#25110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#25991;&#26723;&#29702;&#35299;&#21644;&#25968;&#25454;&#25552;&#21462;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#19968;&#20123;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#22914;LayoutLMv2&#12289;LayoutLMv3&#21644;LiLT&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#19968;&#20010;&#26032;&#31867;&#21035;&#65292;&#20063;&#38656;&#35201;&#37325;&#26032;&#27880;&#37322;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#20123;&#38382;&#39064;&#37117;&#20005;&#37325;&#24433;&#21709;&#20102;&#26356;&#26032;&#27169;&#22411;&#30340;&#37096;&#32626;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key value pair (KVP) extraction or Named Entity Recognition(NER) from visually rich documents has been an active area of research in document understanding and data extraction domain. Several transformer based models such as LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art results. However, addition of even a single new class to the existing model requires (a) re-annotation of entire training dataset to include this new class and (b) retraining the model again. Both of these issues really slow down the deployment of updated model. \\ We present \textbf{ProtoNER}: Prototypical Network based end-to-end KVP extraction model that allows addition of new classes to an existing model while requiring minimal number of newly annotated training samples. The key contributions of our model are: (1) No dependency on dataset used for initial training of the model, which alleviates the need to retain original training dataset for longer duration as well as data re-annotation w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23383;&#20856;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#20808;&#32423;&#36719;Q&#20998;&#35299;&#31639;&#27861;&#65288;PSQD&#65289;&#65292;&#33021;&#22815;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#20855;&#26377;&#23383;&#20856;&#22411;&#20248;&#20808;&#32423;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20808;&#21069;&#23398;&#20064;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#38646;-shot&#32452;&#25104;&#21644;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.02360</link><description>&lt;p&gt;
&#20248;&#20808;&#32423;&#36719;Q&#20998;&#35299;&#29992;&#20110;&#23383;&#20856;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning. (arXiv:2310.02360v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23383;&#20856;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#20808;&#32423;&#36719;Q&#20998;&#35299;&#31639;&#27861;&#65288;PSQD&#65289;&#65292;&#33021;&#22815;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#20855;&#26377;&#23383;&#20856;&#22411;&#20248;&#20808;&#32423;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20808;&#21069;&#23398;&#20064;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#38646;-shot&#32452;&#25104;&#21644;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35774;&#35745;&#26631;&#37327;&#22870;&#21169;&#20989;&#25968;&#30340;&#22256;&#38590;&#20197;&#21450;&#20174;&#22836;&#24320;&#21457;&#27169;&#22411;&#30340;&#22266;&#26377;&#20302;&#25928;&#24615;&#12290;&#30456;&#21453;&#65292;&#26368;&#22909;&#26159;&#23558;&#22797;&#26434;&#20219;&#21153;&#20197;&#22522;&#26412;&#23376;&#20219;&#21153;&#30340;&#24418;&#24335;&#25351;&#23450;&#65292;&#24182;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#37325;&#22797;&#20351;&#29992;&#23376;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36830;&#32493;&#31354;&#38388;&#30340;&#23383;&#20856;&#22411;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20248;&#20808;&#32423;&#23376;&#20219;&#21153;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#24456;&#38590;&#35299;&#20915;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23376;&#20219;&#21153;&#36716;&#25442;&#36827;&#34892;&#26631;&#37327;&#21270;&#65292;&#24182;&#20351;&#29992;&#20215;&#20540;&#20998;&#35299;&#36880;&#27493;&#35299;&#20915;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#20808;&#32423;&#36719;Q&#20998;&#35299;&#65288;PSQD&#65289;&#65292;&#19968;&#31181;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#21644;&#36866;&#24212;&#20855;&#26377;&#23383;&#20856;&#22411;&#20248;&#20808;&#32423;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#31639;&#27861;&#12290;PSQD&#33021;&#22815;&#22312;&#38646;-shot&#32452;&#25104;&#20043;&#21518;&#37325;&#22797;&#20351;&#29992;&#20808;&#21069;&#23398;&#20064;&#30340;&#23376;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#36866;&#24212;&#27493;&#39588;&#12290;&#23427;&#20855;&#22791;&#20445;&#30041;&#23376;&#20219;&#21153;&#35757;&#32451;&#20449;&#24687;&#24182;&#22312;&#22797;&#21512;&#20219;&#21153;&#20013;&#36866;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask trai
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02357</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#27602;&#24615;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#12290;&#35895;&#27468;&#26071;&#19979;&#30340;&#22242;&#38431;Jigsaw&#26159;&#35813;&#39046;&#22495;&#30340;&#39046;&#23548;&#32773;&#20043;&#19968;&#65292;&#20182;&#20204;&#20351;&#29992;Dixon&#31561;&#20154;&#32473;&#20986;&#30340;&#27602;&#24615;&#23450;&#20041;&#65306;&#8220;&#31895;&#40065;&#12289;&#19981;&#23562;&#37325;&#25110;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#35753;&#26576;&#20154;&#31163;&#24320;&#35752;&#35770;&#8221;&#12290;&#20154;&#20204;&#21487;&#20197;&#31435;&#21363;&#30475;&#21040;&#36825;&#20010;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32473;&#20986;&#27602;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#65292;&#32780;&#19988;&#28041;&#21450;&#39640;&#24230;&#20027;&#35266;&#30340;&#25991;&#21270;&#26415;&#35821;&#12290;&#23613;&#31649;&#23384;&#22312;&#27169;&#31946;&#21644;&#32570;&#38519;&#65292;&#20294;&#36825;&#20010;&#23450;&#20041;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#30452;&#35273;&#22411;&#35745;&#31639;&#26641;&#36923;&#36753;&#30340;&#29256;&#26412;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#24418;&#24335;&#39564;&#35777;&#26041;&#38754;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;CTL&#30340;&#19981;&#21160;&#28857;&#20844;&#29702;&#22312;&#35813;&#30452;&#35273;&#22411;&#29256;&#26412;&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2310.02355</link><description>&lt;p&gt;
&#25506;&#31350;&#30452;&#35273;&#22411;&#35745;&#31639;&#26641;&#36923;&#36753;&#30340;&#25512;&#29702;&#65288;arXiv:2310.02355v1 [cs.LO]&#65289;
&lt;/p&gt;
&lt;p&gt;
Reasoning about Intuitionistic Computation Tree Logic. (arXiv:2310.02355v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#30452;&#35273;&#22411;&#35745;&#31639;&#26641;&#36923;&#36753;&#30340;&#29256;&#26412;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#24418;&#24335;&#39564;&#35777;&#26041;&#38754;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;CTL&#30340;&#19981;&#21160;&#28857;&#20844;&#29702;&#22312;&#35813;&#30452;&#35273;&#22411;&#29256;&#26412;&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#30452;&#35273;&#22411;&#35745;&#31639;&#26641;&#36923;&#36753;&#30340;&#29256;&#26412;&#12290;&#22312;&#35299;&#37322;&#30452;&#35273;&#36923;&#36753;&#30340;&#35821;&#20041;&#29305;&#28857;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#22312;&#24418;&#24335;&#39564;&#35777;&#26041;&#38754;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#30452;&#35273;&#22411;CTL&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#65292;&#24182;&#30740;&#31350;&#20102;&#25152;&#24471;&#36923;&#36753;&#30340;&#19968;&#20123;&#31616;&#21333;&#23646;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20123;CTL&#30340;&#19981;&#21160;&#28857;&#20844;&#29702;&#22312;&#25105;&#20204;&#23450;&#20041;&#30340;&#30452;&#35273;&#22411;CTL&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we define an intuitionistic version of Computation Tree Logic. After explaining the semantic features of intuitionistic logic, we examine how these characteristics can be interesting for formal verification purposes. Subsequently, we define the syntax and semantics of our intuitionistic version of CTL and study some simple properties of the so obtained logic. We conclude by demonstrating that some fixed-point axioms of CTL are not valid in the intuitionistic version of CTL we have defined.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;POMDP&#24314;&#27169;&#20026;&#38543;&#26426;&#20381;&#36182;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;POMCP&#39640;&#24230;&#20381;&#36182;&#23637;&#24320;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02345</link><description>&lt;p&gt;
&#22312;&#32447;&#38543;&#26426;&#21363;&#26102;&#35268;&#21010;&#30340;&#23637;&#24320;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rollout Heuristics for Online Stochastic Contingent Planning. (arXiv:2310.02345v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;POMDP&#24314;&#27169;&#20026;&#38543;&#26426;&#20381;&#36182;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;POMCP&#39640;&#24230;&#20381;&#36182;&#23637;&#24320;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#26159;&#19968;&#31181;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#38543;&#26426;&#21160;&#20316;&#19979;&#36827;&#34892;&#20915;&#31574;&#30340;&#26377;&#29992;&#27169;&#22411;&#12290;&#37096;&#20998;&#21487;&#35266;&#23519;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#65288;POMCP&#65289;&#26159;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#20915;&#23450;&#19979;&#19968;&#27493;&#35201;&#25191;&#34892;&#30340;&#21160;&#20316;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#65288;&#24212;&#29992;&#20110;&#26641;&#65289;&#31639;&#27861;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#38024;&#23545;&#23436;&#20840;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#12290;POMCP&#29983;&#25104;&#19968;&#20010;&#21160;&#20316;-&#35266;&#23519;&#26641;&#65292;&#22312;&#21494;&#23376;&#33410;&#28857;&#20351;&#29992;&#23637;&#24320;&#31574;&#30053;&#20026;&#21494;&#23376;&#33410;&#28857;&#25552;&#20379;&#20272;&#35745;&#20540;&#12290;&#22240;&#27492;&#65292;POMCP&#39640;&#24230;&#20381;&#36182;&#23637;&#24320;&#31574;&#30053;&#26469;&#35745;&#31639;&#33391;&#22909;&#30340;&#20272;&#35745;&#20540;&#65292;&#24182;&#22240;&#27492;&#30830;&#23450;&#33391;&#22909;&#30340;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#20351;&#29992;POMCP&#30340;&#23454;&#36341;&#32773;&#38656;&#35201;&#21019;&#24314;&#24378;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;POMDP&#24314;&#27169;&#20026;&#38543;&#26426;&#20381;&#36182;&#35268;&#21010;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#22312;&#35268;&#21010;&#31038;&#21306;&#20013;&#24320;&#21457;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#31532;&#19968;&#31181;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;h_add&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDP) are a useful model for decision-making under partial observability and stochastic actions. Partially Observable Monte-Carlo Planning is an online algorithm for deciding on the next action to perform, using a Monte-Carlo tree search approach, based on the UCT (UCB applied to trees) algorithm for fully observable Markov-decision processes. POMCP develops an action-observation tree, and at the leaves, uses a rollout policy to provide a value estimate for the leaf. As such, POMCP is highly dependent on the rollout policy to compute good estimates, and hence identify good actions. Thus, many practitioners who use POMCP are required to create strong, domain-specific heuristics.  In this paper, we model POMDPs as stochastic contingent planning problems. This allows us to leverage domain-independent heuristics that were developed in the planning community. We suggest two heuristics, the first is based on the well-known h_add heuristic from
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#22312;&#33521;&#22269;&#26680;&#29615;&#22659;&#20013;&#24320;&#21457;&#33258;&#20027;&#26426;&#22120;&#20154;&#37096;&#32626;&#30340;&#23433;&#20840;&#26696;&#20363;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#26696;&#20363;&#12290;&#36825;&#20026;&#36827;&#19968;&#27493;&#35752;&#35770;&#26680;&#31449;&#28857;&#35768;&#21487;&#25345;&#26377;&#20154;&#12289;&#26680;&#23433;&#20840;&#21150;&#20844;&#23460;&#12289;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#20043;&#38388;&#30340;&#21512;&#20316;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.02344</link><description>&lt;p&gt;
&#29992;&#20110;&#33521;&#22269;&#26680;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#31995;&#32479;&#23433;&#20840;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Autonomous Systems' Safety Cases for use in UK Nuclear Environments. (arXiv:2310.02344v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#22312;&#33521;&#22269;&#26680;&#29615;&#22659;&#20013;&#24320;&#21457;&#33258;&#20027;&#26426;&#22120;&#20154;&#37096;&#32626;&#30340;&#23433;&#20840;&#26696;&#20363;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#26696;&#20363;&#12290;&#36825;&#20026;&#36827;&#19968;&#27493;&#35752;&#35770;&#26680;&#31449;&#28857;&#35768;&#21487;&#25345;&#26377;&#20154;&#12289;&#26680;&#23433;&#20840;&#21150;&#20844;&#23460;&#12289;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#20043;&#38388;&#30340;&#21512;&#20316;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#20102;&#22312;&#33521;&#22269;&#26680;&#31449;&#28857;&#37096;&#32626;&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#26696;&#20363;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#24037;&#26234;&#33021;&#30340;&#20551;&#35774;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#26696;&#20363;&#12290;&#36825;&#26159;&#21521;&#37096;&#32626;&#36808;&#20986;&#30340;&#31532;&#19968;&#27493;&#65292;&#23637;&#31034;&#20102;&#30446;&#21069;&#21487;&#33021;&#30340;&#24773;&#20917;&#20197;&#21450;&#36890;&#36807;&#24037;&#20855;&#24320;&#21457;&#21487;&#33021;&#23454;&#29616;&#30340;&#24773;&#20917;&#12290;&#23427;&#20026;&#26680;&#31449;&#28857;&#35768;&#21487;&#25345;&#26377;&#20154;&#12289;&#26680;&#23433;&#20840;&#21150;&#20844;&#23460;&#65288;ONR&#65289;&#12289;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#20043;&#38388;&#36827;&#19968;&#27493;&#35752;&#35770;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
An overview of the process to develop a safety case for an autonomous robot deployment on a nuclear site in the UK is described and a safety case for a hypothetical robot incorporating AI is presented. This forms a first step towards a deployment, showing what is possible now and what may be possible with development of tools. It forms the basis for further discussion between nuclear site licensees, the Office for Nuclear Regulation (ONR), industry and academia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#20809;&#35889;&#35299;&#28151;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#38750;&#32447;&#24615;&#21644;&#31471;&#20803;&#21487;&#21464;&#24615;&#65292;&#24182;&#37319;&#29992;&#27010;&#29575;&#21464;&#20998;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#35299;&#38050;&#29748;&#23398;&#20064;&#30340;&#24605;&#24819;&#36827;&#34892;&#24314;&#27169;&#21644;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.02340</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#21435;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39640;&#20809;&#35889;&#35299;&#28151;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Deep Disentangled Neural Networks for Hyperspectral Unmixing. (arXiv:2310.02340v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#20809;&#35889;&#35299;&#28151;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#38750;&#32447;&#24615;&#21644;&#31471;&#20803;&#21487;&#21464;&#24615;&#65292;&#24182;&#37319;&#29992;&#27010;&#29575;&#21464;&#20998;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#35299;&#38050;&#29748;&#23398;&#20064;&#30340;&#24605;&#24819;&#36827;&#34892;&#24314;&#27169;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#39640;&#20809;&#35889;&#35299;&#28151;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#24050;&#32463;&#20184;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#22797;&#26434;&#30340;&#36752;&#23556;&#25955;&#23556;&#21644;&#31471;&#20803;&#21487;&#21464;&#24615;&#31561;&#38750;&#29702;&#24819;&#24773;&#20917;&#20173;&#28982;&#23545;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20855;&#26377;&#24456;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24050;&#32463;&#34987;&#25506;&#32034;&#29992;&#20110;&#39640;&#20809;&#35889;&#35299;&#28151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#35201;&#20040;&#27809;&#26377;&#35299;&#20915;&#35299;&#28151;&#38382;&#39064;&#30340;&#38750;&#29702;&#24819;&#24773;&#20917;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#40657;&#30418;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#20809;&#35889;&#35299;&#28151;&#38382;&#39064;&#65292;&#20197;&#32771;&#34385;&#38750;&#32447;&#24615;&#21644;&#31471;&#20803;&#21487;&#21464;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#27010;&#29575;&#21464;&#20998;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#35299;&#38050;&#29748;&#23398;&#20064;&#26469;&#27491;&#30830;&#20998;&#31163;&#20809;&#35889;&#21644;&#31471;&#20803;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although considerable effort has been dedicated to improving the solution to the hyperspectral unmixing problem, non-idealities such as complex radiation scattering and endmember variability negatively impact the performance of most existing algorithms and can be very challenging to address. Recently, deep learning-based frameworks have been explored for hyperspectral umixing due to their flexibility and powerful representation capabilities. However, such techniques either do not address the non-idealities of the unmixing problem, or rely on black-box models which are not interpretable. In this paper, we propose a new interpretable deep learning method for hyperspectral unmixing that accounts for nonlinearity and endmember variability. The proposed method leverages a probabilistic variational deep-learning framework, where disentanglement learning is employed to properly separate the abundances and endmembers. The model is learned end-to-end using stochastic backpropagation, and traine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20046;&#31561;&#21464;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EquivQCNNs&#65289;&#65292;&#38024;&#23545;&#22270;&#20687;&#20013;&#30340;&#24179;&#38754;$p4m$&#23545;&#31216;&#24615;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02323</link><description>&lt;p&gt;
&#20960;&#20046;&#31561;&#21464;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22270;&#20687;&#20013;$p4m$&#32676;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Approximately Equivariant Quantum Neural Network for $p4m$ Group Symmetries in Images. (arXiv:2310.02323v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20046;&#31561;&#21464;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;EquivQCNNs&#65289;&#65292;&#38024;&#23545;&#22270;&#20687;&#20013;&#30340;&#24179;&#38754;$p4m$&#23545;&#31216;&#24615;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#20302;&#28145;&#24230;&#39640;&#25928;&#27169;&#25311;&#30340;&#37327;&#23376;&#31639;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQAs&#65289;&#26550;&#26500;&#65292;&#38382;&#39064;&#26080;&#20851;&#30340;&#27169;&#22411;&#36890;&#24120;&#36935;&#21040;&#21487;&#35757;&#32451;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;GQML&#65289;&#65292;&#20351;&#29992;&#20102;&#19982;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#24213;&#23618;&#23545;&#31216;&#24615;&#31561;&#21464;&#30340;QNNs&#12290;GQML&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#21040;&#27169;&#22411;&#20013;&#26469;&#20026;&#27169;&#22411;&#28155;&#21152;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#22312;&#32422;&#26463;&#25628;&#32034;&#31354;&#38388;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#20248;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#24179;&#38754;$p4m$&#23545;&#31216;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;EquivQCNNs&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#27979;&#35797;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Neural Networks (QNNs) are suggested as one of the quantum algorithms which can be efficiently simulated with a low depth on near-term quantum hardware in the presence of noises. However, their performance highly relies on choosing the most suitable architecture of Variational Quantum Algorithms (VQAs), and the problem-agnostic models often suffer issues regarding trainability and generalization power. As a solution, the most recent works explore Geometric Quantum Machine Learning (GQML) using QNNs equivariant with respect to the underlying symmetry of the dataset. GQML adds an inductive bias to the model by incorporating the prior knowledge on the given dataset and leads to enhancing the optimization performance while constraining the search space. This work proposes equivariant Quantum Convolutional Neural Networks (EquivQCNNs) for image classification under planar $p4m$ symmetry, including reflectional and $90^\circ$ rotational symmetry. We present the results tested in diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.02304</link><description>&lt;p&gt;
&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65306;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;&#24605;&#32500;&#26641;&#21644;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#8220;&#33050;&#25163;&#26550;&#8221;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#31243;&#24207;&#26500;&#24314;&#20102;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;&#33050;&#25163;&#26550;&#31243;&#24207;&#36890;&#24120;&#20351;&#29992;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31181;&#23376;&#8220;&#25913;&#36827;&#22120;&#8221;&#24320;&#22987;&#65292;&#36890;&#36807;&#22810;&#27425;&#26597;&#35810;&#35821;&#35328;&#27169;&#22411;&#24182;&#36820;&#22238;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#32473;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25913;&#36827;&#36755;&#20837;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36816;&#34892;&#36825;&#20010;&#31181;&#23376;&#25913;&#36827;&#22120;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#22312;&#19968;&#31995;&#21015;&#32454;&#20998;&#20219;&#21153;&#20013;&#65292;&#24471;&#21040;&#30340;&#25913;&#36827;&#25913;&#36827;&#22120;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#31181;&#23376;&#25913;&#36827;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#30340;&#21508;&#31181;&#33258;&#25105;&#25913;&#36827;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#27874;&#26463;&#25628;&#32034;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#27169;&#25311;&#36864;&#28779;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#31181;&#22686;&#38271;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.02299</link><description>&lt;p&gt;
3D&#29289;&#29702;&#31995;&#32479;&#20013;&#23398;&#20064;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31561;&#20215;&#27169;&#22411;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#23436;&#32654;&#23545;&#31216;&#24615;&#30340;&#20551;&#35774;&#26377;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#19982;&#36825;&#20123;&#23545;&#31216;&#24615;&#19981;&#23436;&#20840;&#19968;&#33268;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#21367;&#31215;&#25216;&#26415;&#33021;&#22815;&#22312;&#20445;&#25345;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25581;&#31034;&#30456;&#21464;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#22312;&#27969;&#20307;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22768;&#23398;&#29305;&#24615;&#29983;&#25104;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#24773;&#24863;&#34920;&#36798;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22768;&#23398;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#24773;&#24863;&#38899;&#39057;&#26816;&#32034;&#21644;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02298</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#23398;&#29305;&#24615;&#20026;&#24773;&#24863;&#34920;&#36798;&#29983;&#25104;&#38899;&#39057;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting Audios Using Acoustic Properties For Emotion Representation. (arXiv:2310.02298v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22768;&#23398;&#29305;&#24615;&#29983;&#25104;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#24773;&#24863;&#34920;&#36798;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22768;&#23398;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#24773;&#24863;&#38899;&#39057;&#26816;&#32034;&#21644;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#23384;&#22312;&#20110;&#19968;&#20010;&#36830;&#32493;&#30340;&#33539;&#22260;&#20869;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#23558;&#24773;&#24863;&#35270;&#20026;&#26377;&#38480;&#31163;&#25955;&#20540;&#21464;&#37327;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#24335;&#26080;&#27861;&#25429;&#25417;&#24773;&#24863;&#34920;&#36798;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34920;&#31034;&#24773;&#24863;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65288;&#25110;&#25552;&#31034;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#36825;&#20123;&#25552;&#31034;&#24182;&#35757;&#32451;&#27169;&#22411;&#20174;&#38899;&#39057;&#21644;&#25552;&#31034;&#23545;&#20013;&#26356;&#22909;&#22320;&#23398;&#20064;&#24773;&#24863;&#34920;&#36798;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#22768;&#23398;&#29305;&#24615;&#65288;&#22914;&#38899;&#35843;&#12289;&#24378;&#24230;&#12289;&#35821;&#36895;&#21644;&#21457;&#38899;&#36895;&#24230;&#65289;&#26469;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#65292;&#21363;&#8220;&#22768;&#23398;&#25552;&#31034;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#23558;&#35821;&#38899;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#22768;&#23398;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#24773;&#24863;&#38899;&#39057;&#26816;&#32034;&#21644;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22768;&#23398;&#25552;&#31034;&#22312;EAR&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;Precision@K&#25351;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;SER&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;Ravdess&#25968;&#25454;&#19978;&#30340;&#30456;&#23545;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;3.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#30452;&#25509;-&#20276;&#38543;&#24490;&#29615;&#12289;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#32534;&#31243;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#19979;&#30340;&#26368;&#20248;&#25511;&#21046;&#20013;&#65292;&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#19979;&#30340;&#20351;&#29992;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2310.02286</link><description>&lt;p&gt;
&#26080;&#32593;&#26684;&#21487;&#24494;&#20998;&#32534;&#31243;&#19982;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#19979;&#30340;&#26368;&#20248;&#25511;&#21046;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints. (arXiv:2310.02286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#30452;&#25509;-&#20276;&#38543;&#24490;&#29615;&#12289;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#32534;&#31243;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#19979;&#30340;&#26368;&#20248;&#25511;&#21046;&#20013;&#65292;&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#19979;&#30340;&#20351;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#21160;&#24494;&#20998;&#24211;&#30340;&#24433;&#21709;&#19979;&#65292;&#20559;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#19979;&#30340;&#26368;&#20248;&#25511;&#21046;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21464;&#21270;&#12290;&#25105;&#20204;&#23545;&#30452;&#25509;-&#20276;&#38543;&#24490;&#29615;(DAL)&#12289;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;(PINN)&#21644;&#21487;&#24494;&#20998;&#32534;&#31243;(DP)&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#20351;&#29992;&#22522;&#20110;&#24452;&#21521;&#22522;&#20989;&#25968;&#30340;&#36890;&#29992;&#26080;&#32593;&#26684;&#21487;&#24494;&#20998;PDE&#27714;&#35299;&#22120;&#12290;&#22312;&#25289;&#26222;&#25289;&#26031;&#21644;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;DP&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20135;&#29983;&#20102;&#26368;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#21363;&#20351;DAL&#22833;&#36133;&#21644;PINN&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#22522;&#20934;&#65292;&#31361;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26368;&#20248;&#25511;&#21046;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#25351;&#21335;&#65292;&#24182;&#36827;&#19968;&#27493;&#36830;&#25509;&#20102;&#20182;&#20204;&#19982;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Optimal Control under Partial Differential Equations (PDE) constraints is rapidly changing under the influence of Deep Learning and the accompanying automatic differentiation libraries. Novel techniques like Physics-Informed Neural Networks (PINNs) and Differentiable Programming (DP) are to be contrasted with established numerical schemes like Direct-Adjoint Looping (DAL). We present a comprehensive comparison of DAL, PINN, and DP using a general-purpose mesh-free differentiable PDE solver based on Radial Basis Functions. Under Laplace and Navier-Stokes equations, we found DP to be extremely effective as it produces the most accurate gradients; thriving even when DAL fails and PINNs struggle. Additionally, we provide a detailed benchmark highlighting the limited conditions under which any of those methods can be efficiently used. Our work provides a guide to Optimal Control practitioners and connects them further to the Deep Learning community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PASTA&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#21382;&#21490;&#20154;&#27969;&#30340;&#26102;&#31354;&#27169;&#24335;&#26469;&#39044;&#27979;&#26410;&#26469;&#20840;&#24066;&#33539;&#22260;&#20869;&#30340;&#20154;&#32676;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#12289;&#22810;&#23610;&#24230;&#27531;&#24046;&#22359;&#21644;&#26102;&#24207;&#27880;&#24847;&#21147;&#38376;&#25511;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#19981;&#35268;&#21017;&#26102;&#31354;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.02284</link><description>&lt;p&gt;
PASTA: &#24182;&#34892;&#26102;&#31354;&#27880;&#24847;&#21147;&#19982;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#29992;&#20110;&#32454;&#31890;&#24230;&#20154;&#32676;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PASTA: PArallel Spatio-Temporal Attention with spatial auto-correlation gating for fine-grained crowd flow prediction. (arXiv:2310.02284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PASTA&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#21382;&#21490;&#20154;&#27969;&#30340;&#26102;&#31354;&#27169;&#24335;&#26469;&#39044;&#27979;&#26410;&#26469;&#20840;&#24066;&#33539;&#22260;&#20869;&#30340;&#20154;&#32676;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#12289;&#22810;&#23610;&#24230;&#27531;&#24046;&#22359;&#21644;&#26102;&#24207;&#27880;&#24847;&#21147;&#38376;&#25511;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#19981;&#35268;&#21017;&#26102;&#31354;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22478;&#24066;&#20013;&#29289;&#20307;&#65288;&#22914;&#20154;&#31867;&#21644;&#36710;&#36742;&#65289;&#30340;&#36816;&#21160;&#27169;&#24335;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#22478;&#24066;&#35268;&#21010;&#21644;&#31649;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#27169;&#32454;&#31890;&#24230;&#22478;&#24066;&#22320;&#22270;&#20013;&#21382;&#21490;&#20154;&#27969;&#30340;&#26102;&#31354;&#27169;&#24335;&#26469;&#39044;&#27979;&#26410;&#26469;&#20840;&#24066;&#33539;&#22260;&#20869;&#20154;&#32676;&#27969;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;PASTA&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#19981;&#35268;&#21017;&#26102;&#31354;&#27169;&#24335;&#12290;&#25105;&#20204;&#26041;&#27861;&#20013;&#30340;&#26032;&#39062;&#32452;&#20214;&#21253;&#25324;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#12289;&#22810;&#23610;&#24230;&#27531;&#24046;&#22359;&#21644;&#26102;&#24207;&#27880;&#24847;&#21147;&#38376;&#25511;&#27169;&#22359;&#12290;&#31354;&#38388;&#33258;&#30456;&#20851;&#38376;&#25511;&#37319;&#29992;&#31354;&#38388;&#32479;&#35745;&#30340;&#27010;&#24565;&#26469;&#35782;&#21035;&#19981;&#35268;&#21017;&#30340;&#31354;&#38388;&#21306;&#22495;&#12290;&#22810;&#23610;&#24230;&#27531;&#24046;&#22359;&#36127;&#36131;&#22788;&#29702;&#32454;&#31890;&#24230;&#22320;&#22270;&#20013;&#30340;&#22810;&#20010;&#33539;&#22260;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26102;&#24207;&#27880;&#24847;&#21147;&#38376;&#25511;&#21017;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the movement patterns of objects (e.g., humans and vehicles) in a city is essential for many applications, including city planning and management. This paper proposes a method for predicting future city-wide crowd flows by modeling the spatio-temporal patterns of historical crowd flows in fine-grained city-wide maps. We introduce a novel neural network named PArallel Spatio-Temporal Attention with spatial auto-correlation gating (PASTA) that effectively captures the irregular spatio-temporal patterns of fine-grained maps. The novel components in our approach include spatial auto-correlation gating, multi-scale residual block, and temporal attention gating module. The spatial auto-correlation gating employs the concept of spatial statistics to identify irregular spatial regions. The multi-scale residual block is responsible for handling multiple range spatial dependencies in the fine-grained map, and the temporal attention gating filters out irrelevant temporal information
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#22823;&#37327;&#21382;&#21490;&#36895;&#24230;&#25968;&#25454;&#30340;&#36895;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#30340;&#36947;&#36335;&#22320;&#24418;&#29305;&#24449;&#26469;&#25311;&#21512;&#20849;&#20139;&#26435;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#25913;&#36827;&#65292;&#21516;&#26102;&#20026;&#20132;&#36890;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.02282</link><description>&lt;p&gt;
&#20351;&#29992;&#36947;&#36335;&#22320;&#24418;&#29305;&#24449;&#30340;&#20849;&#20139;&#26435;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#21450;&#20854;&#22312;&#36710;&#36742;&#36712;&#36857;&#36895;&#24230;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SWMLP: Shared Weight Multilayer Perceptron for Car Trajectory Speed Prediction using Road Topographical Features. (arXiv:2310.02282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#22823;&#37327;&#21382;&#21490;&#36895;&#24230;&#25968;&#25454;&#30340;&#36895;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#30340;&#36947;&#36335;&#22320;&#24418;&#29305;&#24449;&#26469;&#25311;&#21512;&#20849;&#20139;&#26435;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#25913;&#36827;&#65292;&#21516;&#26102;&#20026;&#20132;&#36890;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20132;&#36890;&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#20294;&#36890;&#24120;&#21482;&#22312;&#29305;&#23450;&#21306;&#22495;&#21487;&#29992;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#34429;&#28982;&#26377;&#30740;&#31350;&#23545;&#36825;&#20123;&#25968;&#25454;&#32473;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#22320;&#21306;&#30340;&#25968;&#25454;&#21487;&#33021;&#19981;&#36275;&#20197;&#20805;&#20998;&#25551;&#36848;&#20840;&#29699;&#20854;&#20182;&#22320;&#21306;&#30340;&#25152;&#26377;&#20132;&#36890;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#22823;&#37327;&#21382;&#21490;&#36895;&#24230;&#25968;&#25454;&#30340;&#36895;&#24230;&#39044;&#27979;&#26041;&#27861;&#12290;&#20026;&#20102;&#39044;&#27979;&#36710;&#36742;&#30340;&#36895;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#30340;&#36947;&#36335;&#22320;&#24418;&#29305;&#24449;&#26469;&#25311;&#21512;&#19968;&#20010;&#20849;&#20139;&#26435;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#26631;&#20934;&#22238;&#24402;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#20026;&#35774;&#35745;&#20132;&#36890;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although traffic is one of the massively collected data, it is often only available for specific regions. One concern is that, although there are studies that give good results for these data, the data from these regions may not be sufficiently representative to describe all the traffic patterns in the rest of the world. In quest of addressing this concern, we propose a speed prediction method that is independent of large historical speed data. To predict a vehicle's speed, we use the trajectory road topographical features to fit a Shared Weight Multilayer Perceptron learning model. Our results show significant improvement, both qualitative and quantitative, over standard regression analysis. Moreover, the proposed framework sheds new light on the way to design new approaches for traffic analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#23458;&#26381;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;CusEmo&#65292;&#37319;&#29992;&#32500;&#24230;&#24773;&#24863;&#27880;&#37322;&#26041;&#27861;&#25429;&#25417;&#24773;&#24863;&#30340;&#24494;&#22937;&#12289;&#22797;&#26434;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02281</link><description>&lt;p&gt;
&#23454;&#26102;&#23458;&#26381;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#30340;&#31471;&#21040;&#31471;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
End-to-End Continuous Speech Emotion Recognition in Real-life Customer Service Call Center Conversations. (arXiv:2310.02281v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#23458;&#26381;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;CusEmo&#65292;&#37319;&#29992;&#32500;&#24230;&#24773;&#24863;&#27880;&#37322;&#26041;&#27861;&#25429;&#25417;&#24773;&#24863;&#30340;&#24494;&#22937;&#12289;&#22797;&#26434;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#24050;&#32463;&#25104;&#20026;&#35780;&#20272;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20043;&#38388;&#20114;&#21160;&#36136;&#37327;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#19982;&#21463;&#25511;&#23454;&#39564;&#23460;&#29615;&#22659;&#19981;&#21516;&#65292;&#23454;&#38469;&#23545;&#35805;&#21457;&#29983;&#22312;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#19979;&#65292;&#24182;&#21463;&#21040;&#24433;&#21709;&#24773;&#24863;&#34920;&#36798;&#30340;&#24773;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#26500;&#24314;&#22823;&#35268;&#27169;&#23454;&#26102;&#25968;&#25454;&#38598;&#65288;CusEmo&#65289;&#29992;&#20110;&#23458;&#25143;&#26381;&#21153;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#32500;&#24230;&#24773;&#24863;&#27880;&#37322;&#26041;&#27861;&#25429;&#25417;&#23454;&#38469;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#24773;&#24863;&#30340;&#24494;&#22937;&#12289;&#22797;&#26434;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#23545;&#24773;&#22659;&#20449;&#24687;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#30740;&#31350;&#36824;&#35299;&#20915;&#20102;&#23558;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30830;&#23450;&#36866;&#24403;&#30340;&#26631;&#31614;&#37319;&#26679;&#29575;&#21644;&#36755;&#20837;&#29255;&#27573;&#38271;&#24230;&#65292;&#20197;&#21450;&#25972;&#21512;&#24773;&#22659;&#20449;&#24687;&#65288;&#23545;&#35805;&#32773;&#30340;&#24615;&#21035;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion recognition (SER) in call center conversations has emerged as a valuable tool for assessing the quality of interactions between clients and agents. In contrast to controlled laboratory environments, real-life conversations take place under uncontrolled conditions and are subject to contextual factors that influence the expression of emotions. In this paper, we present our approach to constructing a large-scale reallife dataset (CusEmo) for continuous SER in customer service call center conversations. We adopted the dimensional emotion annotation approach to capture the subtlety, complexity, and continuity of emotions in real-life call center conversations, while annotating contextual information. The study also addresses the challenges encountered during the application of the End-to-End (E2E) SER system to the dataset, including determining the appropriate label sampling rate and input segment length, as well as integrating contextual information (interlocutor's gender 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E-DTWA&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#24182;&#21152;&#20837;&#20102;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#30456;&#20851;&#30340;&#39069;&#22806;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#24378;&#28872;&#32771;&#34385;&#21040;&#19987;&#23478;&#30340;&#26816;&#27979;&#21453;&#39304;&#30340;&#22522;&#30784;&#19978;&#28789;&#27963;&#22320;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#35745;&#31639;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.02280</link><description>&lt;p&gt;
&#19987;&#23478;&#22686;&#24378;&#30340;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Expert enhanced dynamic time warping based anomaly detection. (arXiv:2310.02280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E-DTWA&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#24182;&#21152;&#20837;&#20102;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#30456;&#20851;&#30340;&#39069;&#22806;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#26816;&#27979;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#24378;&#28872;&#32771;&#34385;&#21040;&#19987;&#23478;&#30340;&#26816;&#27979;&#21453;&#39304;&#30340;&#22522;&#30784;&#19978;&#28789;&#27963;&#22320;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#35745;&#31639;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#26159;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24377;&#24615;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#33879;&#21517;&#31639;&#27861;&#12290;&#20854;&#22788;&#29702;&#38750;&#32447;&#24615;&#26102;&#38388;&#25197;&#26354;&#30340;&#33021;&#21147;&#20351;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#20013;&#24456;&#26377;&#24110;&#21161;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#26159;&#24322;&#24120;&#26816;&#27979;&#65292;&#23427;&#35797;&#22270;&#25581;&#31034;&#20986;&#24847;&#22806;&#30340;&#34892;&#20026;&#32780;&#27809;&#26377;&#38169;&#35823;&#30340;&#26816;&#27979;&#35686;&#25253;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19987;&#23478;&#22686;&#24378;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#24322;&#24120;&#26816;&#27979;&#65288;E-DTWA&#65289;&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;DTW&#65292;&#24182;&#22312;&#20854;&#20013;&#21152;&#20837;&#20102;&#19982;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#30456;&#20851;&#30340;&#39069;&#22806;&#25913;&#36827;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#21253;&#25324;&#39640;&#25928;&#30340;&#26816;&#27979;&#65292;&#22312;&#24378;&#28872;&#32771;&#34385;&#21040;&#19987;&#23478;&#30340;&#26816;&#27979;&#21453;&#39304;&#30340;&#22522;&#30784;&#19978;&#28789;&#27963;&#22320;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#35745;&#31639;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic time warping (DTW) is a well-known algorithm for time series elastic dissimilarity measure. Its ability to deal with non-linear time distortions makes it helpful in variety of data mining tasks. Such a task is also anomaly detection which attempts to reveal unexpected behaviour without false detection alarms. In this paper, we propose a novel anomaly detection method named Expert enhanced dynamic time warping anomaly detection (E-DTWA). It is based on DTW with additional enhancements involving human-in-the-loop concept. The main benefits of our approach comprise efficient detection, flexible retraining based on strong consideration of the expert's detection feedback while retaining low computational and space complexity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02279</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65306;&#23398;&#20064;&#25193;&#25955;&#30340;&#27010;&#29575;&#27969;ODE&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#21152;&#36895;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#65292;&#20294;&#20197;&#29306;&#29298;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#65292;&#32570;&#20047;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#26435;&#34913;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#26159;&#21253;&#25324;CM&#21644;&#22522;&#20110;&#24471;&#20998;&#27169;&#22411;&#22312;&#20869;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;CTM&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#36755;&#20986;&#24471;&#20998;&#65288;&#21363;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65289;&#65292;&#24182;&#20801;&#35768;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20219;&#24847;&#21021;&#22987;&#21644;&#26368;&#32456;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#19981;&#21463;&#38480;&#21046;&#30340;&#36941;&#21382;&#27010;&#29575;&#27969;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;CTM&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#26377;&#25928;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#65288;FID 1.73&#65289;&#21644;64X64&#20998;&#36776;&#29575;&#30340;ImageNet&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;FID&#12290;CTM&#36824;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#30340;ODE&#35299;&#20013;&#30340;&#38271;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;</title><link>http://arxiv.org/abs/2310.02277</link><description>&lt;p&gt;
"&#22403;&#22334;DNA&#20551;&#35774;&#65306;&#36890;&#36807;&#31232;&#30095;&#24615;&#23545;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#36827;&#34892;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#20998;&#26512;"
&lt;/p&gt;
&lt;p&gt;
Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity. (arXiv:2310.02277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#23545;"&#22403;&#22334;DNA"&#30340;&#27010;&#24565;&#38271;&#26399;&#20197;&#26469;&#19982;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#30340;&#38750;&#32534;&#30721;&#29255;&#27573;&#30456;&#20851;&#32852;&#65292;&#21344;&#20854;&#32452;&#25104;&#30340;&#22823;&#32422;98%&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20123;&#36825;&#20123;&#30475;&#20284;&#26080;&#21151;&#33021;&#30340;DNA&#24207;&#21015;&#22312;&#32454;&#32990;&#36807;&#31243;&#20013;&#36215;&#21040;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#19982;&#20154;&#31867;&#22522;&#22240;&#20013;&#35266;&#23519;&#21040;&#30340;&#20887;&#20313;&#24615;&#26377;&#30528;&#26174;&#33879;&#30340;&#30456;&#20284;&#24615;&#12290;&#20154;&#20204;&#35748;&#20026;&#65292;&#24222;&#22823;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21253;&#21547;&#20102;&#36807;&#22810;&#30340;&#20887;&#20313;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#21453;&#35770;&#26469;&#25361;&#25112;&#36825;&#20010;&#20256;&#32479;&#35266;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#31232;&#30095;&#24615;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#26469;&#29420;&#31435;&#32780;&#20934;&#30830;&#22320;&#37327;&#21270;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#20302;&#24133;&#24230;&#26435;&#37325;&#30340;&#32454;&#24494;&#37325;&#35201;&#24615;&#65292;&#20174;&#19979;&#28216;&#20219;&#21153;&#20013;&#24515;&#30340;&#35282;&#24230;&#29702;&#35299;&#23427;&#20204;&#21253;&#21547;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25903;&#25345;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#30340;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional notion of "Junk DNA" has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed by our in-depth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;LLM&#20013;&#36951;&#24536;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;Llama2-7b&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30701;&#26102;&#38388;&#30340;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#25830;&#38500;&#27169;&#22411;&#20851;&#20110;&#21704;&#21033;&#183;&#27874;&#29305;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#20854;&#20182;&#24120;&#35265;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#20960;&#20046;&#19981;&#21464;&#12290;</title><link>http://arxiv.org/abs/2310.02238</link><description>&lt;p&gt;
&#35841;&#26159;&#21704;&#21033;&#183;&#27874;&#29305;&#65311;LLMs&#20013;&#30340;&#36817;&#20284;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Who's Harry Potter? Approximate Unlearning in LLMs. (arXiv:2310.02238v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;LLM&#20013;&#36951;&#24536;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;Llama2-7b&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30701;&#26102;&#38388;&#30340;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#25830;&#38500;&#27169;&#22411;&#20851;&#20110;&#21704;&#21033;&#183;&#27874;&#29305;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#20854;&#20182;&#24120;&#35265;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#20960;&#20046;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22312;&#21253;&#21547;&#29256;&#26435;&#20869;&#23481;&#30340;&#28023;&#37327;&#20114;&#32852;&#32593;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#36825;&#32473;&#27169;&#22411;&#30340;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#65292;&#20197;&#21450;&#21407;&#22987;&#20316;&#32773;&#21644;&#20986;&#29256;&#21830;&#24102;&#26469;&#20102;&#27861;&#24459;&#21644;&#20262;&#29702;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;LLM&#20013;&#36951;&#24536;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;Llama2-7b&#27169;&#22411;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Meta&#26368;&#36817;&#24320;&#28304;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;&#35813;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#33457;&#36153;&#20102;&#36229;&#36807;184K GPU&#23567;&#26102;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#32422;1&#20010;GPU&#23567;&#26102;&#30340;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25830;&#38500;&#20102;&#27169;&#22411;&#29983;&#25104;&#25110;&#22238;&#24518;&#21704;&#21033;&#183;&#27874;&#29305;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#24120;&#35265;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;Winogrande&#65292;Hellaswag&#65292;arc&#65292;boolq&#21644;piqa&#65289;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;HuggingFace&#19978;&#20844;&#24320;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;&#20379;&#31038;&#21306;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23454;&#29616;&#36825;&#26679;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch.  We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#31070;&#32463;&#31639;&#23376;&#65288;CoNO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#23376;&#36890;&#36807;&#22797;&#20998;&#25968;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#25429;&#33719;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12289;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02094</link><description>&lt;p&gt;
CoNO: &#22797;&#26434;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#36830;&#32493;&#21160;&#21147;&#23398;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoNO: Complex Neural Operator for Continuous Dynamical Systems. (arXiv:2310.02094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#31070;&#32463;&#31639;&#23376;&#65288;CoNO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#23376;&#36890;&#36807;&#22797;&#20998;&#25968;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#25429;&#33719;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12289;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#25193;&#23637;&#20102;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#29992;&#20110;&#26144;&#23556;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#30001;&#24494;&#20998;&#26041;&#31243;&#34920;&#31034;&#30340;&#36830;&#32493;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#22914;&#22825;&#27668;&#39044;&#25253;&#12289;&#27969;&#20307;&#27969;&#21160;&#25110;&#22266;&#20307;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#23376;&#20173;&#28982;&#20381;&#36182;&#20110;&#23454;&#25968;&#31354;&#38388;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#21040;&#22312;&#22797;&#25968;&#31354;&#38388;&#20013;&#36890;&#36807;&#20989;&#25968;&#21464;&#25442;&#21487;&#33021;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#31070;&#32463;&#31639;&#23376;&#65288;CoNO&#65289;&#65292;&#35813;&#31639;&#23376;&#22312;&#22797;&#20998;&#25968;&#20613;&#37324;&#21494;&#22495;&#20013;&#21442;&#25968;&#21270;&#31215;&#20998;&#26680;&#12290;&#21478;&#22806;&#65292;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#26080;&#28151;&#21472;&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#20445;&#30041;&#20102;&#22797;&#25968;&#20540;&#21644;&#22797;&#20195;&#25968;&#29305;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12289;&#23545;&#22122;&#22768;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21333;&#19968;&#30340;&#22797;&#20998;&#25968;&#20613;&#37324;&#21494;&#21464;&#25442;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#28508;&#22312;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators extend data-driven models to map between infinite-dimensional functional spaces. These models have successfully solved continuous dynamical systems represented by differential equations, viz weather forecasting, fluid flow, or solid mechanics. However, the existing operators still rely on real space, thereby losing rich representations potentially captured in the complex space by functional transforms. In this paper, we introduce a Complex Neural Operator (CoNO), that parameterizes the integral kernel in the complex fractional Fourier domain. Additionally, the model employing a complex-valued neural network along with aliasing-free activation functions preserves the complex values and complex algebraic properties, thereby enabling improved representation, robustness to noise, and generalization. We show that the model effectively captures the underlying partial differential equation with a single complex fractional Fourier transform. We perform an extensive empirical e
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#35299;&#20915;&#39640;&#32500;&#34920;&#31034;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01892</link><description>&lt;p&gt;
FiGURe: &#20351;&#29992;&#36807;&#28388;&#22120;&#22686;&#24378;&#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations. (arXiv:2310.01892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#35299;&#20915;&#39640;&#32500;&#34920;&#31034;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#33410;&#28857;&#34920;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#27169;&#25311;&#20302;&#36890;&#28388;&#27874;&#22120;&#30340;&#22686;&#24378;&#65292;&#38480;&#21046;&#20102;&#22312;&#38656;&#35201;&#19981;&#21516;&#29305;&#24449;&#39057;&#35889;&#37096;&#20998;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#36807;&#28388;&#22120;&#30340;&#22686;&#24378;&#26041;&#27861;&#26469;&#25429;&#25417;&#29305;&#24449;&#39057;&#35889;&#30340;&#19981;&#21516;&#37096;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#36807;&#28388;&#22120;&#22686;&#24378;&#20043;&#38388;&#20849;&#20139;&#30456;&#21516;&#26435;&#37325;&#26159;&#21487;&#33021;&#30340;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#38656;&#35201;&#39640;&#32500;&#34920;&#31034;&#12290;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#25968;&#25454;&#26102;&#65292;&#29305;&#21035;&#26159;&#24403;&#28041;&#21450;&#22810;&#20010;&#22686;&#24378;&#26041;&#27861;&#26102;&#65292;&#22686;&#21152;&#20102;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426; Fourier &#29305;&#24449;&#25237;&#24433;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#24182;&#24674;&#22797;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861; FiGURe &#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe achieves an ave
&lt;/p&gt;</description></item><item><title>LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01852</link><description>&lt;p&gt;
LanguageBind:&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#23545;&#40784;&#30340;&#35821;&#35328;&#23558;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#25193;&#23637;&#21040;N&#27169;&#24577;&#65288;arXiv:2310.01852v1[cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01852
&lt;/p&gt;
&lt;p&gt;
LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VL&#39044;&#35757;&#32451;&#26694;&#26550;&#38590;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#38500;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#22806;&#30340;&#22810;&#27169;&#24577;&#65288;N&#27169;&#24577;&#65292;N&gt;=3&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LanguageBind&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#32445;&#24102;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#24577;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#32034;&#65292;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;VL&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#12290;&#32467;&#26524;&#26159;&#65292;&#25152;&#26377;&#27169;&#24577;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#34429;&#28982;LanguageBind&#21487;&#20197;&#25193;&#23637;VL&#27169;&#24577;&#21040;N&#27169;&#24577;&#65292;&#20294;&#25105;&#20204;&#36824;&#38656;&#35201;&#19968;&#20010;&#24102;&#26377;&#20197;&#35821;&#35328;&#20026;&#20013;&#24515;&#30340;&#23545;&#40784;&#25968;&#25454;&#23545;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VIDAL-10M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35270;&#39057;&#12289;&#32418;&#22806;&#12289;&#28145;&#24230;&#12289;&#38899;&#39057;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21629;&#21517;&#20026;VIDAL-10M&#12290;
&lt;/p&gt;
&lt;p&gt;
The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N&gt;=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01737</link><description>&lt;p&gt;
&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#40065;&#26834;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Blending Imitation and Reinforcement Learning for Robust Policy Improvement. (arXiv:2310.01737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#19987;&#23478;&#31034;&#33539;&#30340;&#36136;&#37327;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#20174;&#22810;&#31181;&#40657;&#30418;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the o
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.01469</link><description>&lt;p&gt;
LLM&#35854;&#35328;: &#24187;&#35273;&#19981;&#26159;&#28431;&#27934;&#65292;&#32780;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01469
&lt;/p&gt;
&lt;p&gt;
LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21253;&#25324;GPT-3.5&#12289;LLaMA&#21644;PaLM&#65292;&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;LLM&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#32780;&#19981;&#34987;&#23519;&#35273;&#12290;&#24187;&#35273;&#23384;&#22312;&#30340;&#21407;&#22240;&#21644;&#26222;&#36941;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#12290;&#36825;&#20010;&#29616;&#35937;&#36843;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24187;&#35273;&#21487;&#33021;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#19988;&#23427;&#19982;&#24120;&#35268;&#30340;&#23545;&#25239;&#26679;&#26412;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#20316;&#20026;LLM&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#23558;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#24418;&#24335;&#21270;&#20026;&#24187;&#35273;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#34987;&#25915;&#20987;&#30340;&#23545;&#25239;&#25552;&#31034;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21453;&#20107;&#23454;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20154;&#31867;&#19987;&#23478;&#30340;&#21453;&#39304;&#26469;&#26816;&#27979;&#21644;&#28040;&#38500;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21463;&#30417;&#31649;&#25110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.01011</link><description>&lt;p&gt;
&#35299;&#20915;&#21033;&#29992;&#21453;&#20107;&#23454;&#30693;&#35782;&#33976;&#39311;&#30340;&#32874;&#26126;&#27721;&#26031;&#39044;&#27979;&#27169;&#22411;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards Fixing Clever-Hans Predictors with Counterfactual Knowledge Distillation. (arXiv:2310.01011v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01011
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21453;&#20107;&#23454;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20154;&#31867;&#19987;&#23478;&#30340;&#21453;&#39304;&#26469;&#26816;&#27979;&#21644;&#28040;&#38500;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21463;&#30417;&#31649;&#25110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#21453;&#20107;&#23454;&#30693;&#35782;&#33976;&#39311;&#65288;CFKD&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#20511;&#21161;&#20154;&#31867;&#19987;&#23478;&#21453;&#39304;&#26469;&#26816;&#27979;&#21644;&#28040;&#38500;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#28151;&#28102;&#22240;&#32032;&#30340;&#20381;&#36182;&#12290;&#28151;&#28102;&#22240;&#32032;&#26159;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#30340;&#34394;&#20551;&#29305;&#24449;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#21463;&#30417;&#31649;&#25110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20986;&#29616;&#24847;&#22806;&#38169;&#35823;&#12290;&#35770;&#25991;&#24378;&#35843;&#20102;CFKD&#22312;&#27492;&#31867;&#39046;&#22495;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#30456;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#35299;&#37322;&#30340;&#19968;&#20123;&#20248;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#26041;&#26696;&#26469;&#23450;&#37327;&#35780;&#20272;CFKD&#30340;&#25104;&#21151;&#24615;&#21644;&#21487;&#20197;&#20026;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#30340;&#19981;&#21516;&#25945;&#24072;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19982;&#30495;&#23454;&#27979;&#35797;&#24615;&#33021;&#26356;&#30456;&#20851;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#38752;&#39564;&#35777;&#31934;&#24230;&#12290;&#35770;&#25991;&#23637;&#31034;&#20102;CFKD&#22312;&#21512;&#25104;&#22686;&#24378;&#25968;&#25454;&#38598;&#21644;&#29616;&#23454;&#19990;&#30028;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel technique called counterfactual knowledge distillation (CFKD) to detect and remove reliance on confounders in deep learning models with the help of human expert feedback. Confounders are spurious features that models tend to rely on, which can result in unexpected errors in regulated or safety-critical domains. The paper highlights the benefit of CFKD in such domains and shows some advantages of counterfactual explanations over other types of explanations. We propose an experiment scheme to quantitatively evaluate the success of CFKD and different teachers that can give feedback to the model. We also introduce a new metric that is better correlated with true test performance than validation accuracy. The paper demonstrates the effectiveness of CFKD on synthetically augmented datasets and on real-world histopathological datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00526</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#20316;&#20026;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20351;&#29992;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#24378;&#22823;&#30340;&#31639;&#27861;&#24037;&#20855;&#26469;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#21487;&#20197;&#34920;&#31034;&#26368;&#24378;&#22823;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21069;&#25552;&#26159;&#20551;&#35774;&#21807;&#19968;&#28216;&#25103;&#29468;&#24819;&#25104;&#31435;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#23427;&#22312;&#35832;&#22914;&#26368;&#22823;&#21106;&#21644;&#26368;&#22823;&#29420;&#31435;&#38598;&#31561;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#65292;&#36824;&#36229;&#36807;&#20102;&#20256;&#32479;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;OptGNN&#25429;&#25417;&#20984;&#26494;&#24347;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#65288;&#30830;&#23450;&#24615;&#19978;&#30028;&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
&lt;/p&gt;</description></item><item><title>ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17452</link><description>&lt;p&gt;
ToRA&#65306;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. (arXiv:2309.17452v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17452
&lt;/p&gt;
&lt;p&gt;
ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38598;&#25104;&#24037;&#20855;&#30340;&#25512;&#29702;&#20195;&#29702;ToRA&#65292;&#23427;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;&#35745;&#31639;&#24211;&#21644;&#31526;&#21495;&#27714;&#35299;&#22120;&#65289;&#30340;&#21033;&#29992;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#23558;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#19982;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#20026;&#20102;&#35757;&#32451;ToRA&#65292;&#25105;&#20204;&#31934;&#36873;&#20102;&#25968;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#20114;&#21160;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#24212;&#29992;&#27169;&#20223;&#23398;&#20064;&#20110;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#36755;&#20986;&#31354;&#38388;&#25972;&#24418;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ToRA&#27169;&#22411;&#22312;10&#20010;&#28085;&#30422;&#21508;&#31181;&#35268;&#27169;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;13%&#33267;19%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ToRA-7B &#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#65292;&#36229;&#36234;&#20102;&#26368;&#20339;&#24320;&#28304;&#27169;&#22411;WizardMath&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#26435;&#37325;&#20540;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#21487;&#21387;&#32553;&#24615;&#12290;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.13575</link><description>&lt;p&gt;
&#27010;&#29575;&#26435;&#37325;&#22266;&#23450;&#65306;&#29992;&#20110;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization. (arXiv:2309.13575v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#26435;&#37325;&#20540;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#21487;&#21387;&#32553;&#24615;&#12290;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#38480;&#21046;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#20540;&#19978;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#33021;&#37327;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#26041;&#27861;&#24120;&#24120;&#22522;&#20110;&#26435;&#37325;&#20540;&#26412;&#36523;&#36827;&#34892;&#20551;&#35774;&#65292;&#24182;&#24573;&#35270;&#20102;&#26435;&#37325;&#20301;&#32622;&#22312;&#20854;&#20013;&#25198;&#28436;&#30340;&#29420;&#29305;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#21464;&#20998;&#26494;&#24347;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#26681;&#25454;&#21333;&#20010;&#26435;&#37325;&#30340;&#20301;&#32622;&#29305;&#23450;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#26469;&#30830;&#23450;&#21487;&#20197;&#23558;&#21738;&#20123;&#26435;&#37325;&#31227;&#21160;&#21040;&#21738;&#20010;&#32858;&#31867;&#20013;&#24515;&#20197;&#21450;&#31227;&#21160;&#21040;&#20160;&#20040;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21021;&#22987;&#21270;&#35774;&#32622;&#21644;&#27491;&#21017;&#21270;&#39033;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;-&#27169;&#22411;&#32452;&#21512;&#19979;&#35757;&#32451;BNNs&#12290;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#25429;&#25417;&#21040;&#30340;&#26435;&#37325;&#20540;&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#19979;&#28216;&#30340;&#21487;&#21387;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#32858;&#31867;&#36807;&#31243;&#23637;&#31034;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight-sharing quantization has emerged as a technique to reduce energy expenditure during inference in large neural networks by constraining their weights to a limited set of values. However, existing methods for weight-sharing quantization often make assumptions about the treatment of weights based on value alone that neglect the unique role weight position plays. This paper proposes a probabilistic framework based on Bayesian neural networks (BNNs) and a variational relaxation to identify which weights can be moved to which cluster centre and to what degree based on their individual position-specific learned uncertainty distributions. We introduce a new initialisation setting and a regularisation term which allow for the training of BNNs under complex dataset-model combinations. By leveraging the flexibility of weight values captured through a probability distribution, we enhance noise resilience and downstream compressibility. Our iterative clustering procedure demonstrates superio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#31070;&#32463;&#32467;&#26500;&#21644;&#35270;&#35273;&#26426;&#21046;&#30340;&#21551;&#31034;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#36229;&#20687;&#32032;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#24378;&#31579;&#36873;&#27169;&#22359;&#65288;ESM&#65289;&#21644;&#36793;&#30028;&#24863;&#30693;&#26631;&#31614;&#65288;BAL&#65289;&#65292;&#26088;&#22312;&#20135;&#29983;&#20005;&#26684;&#36981;&#24490;&#29289;&#20307;&#36793;&#30028;&#19988;&#20256;&#36798;&#20016;&#23500;&#35270;&#35273;&#31526;&#21495;&#30340;&#36229;&#20687;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.13438</link><description>&lt;p&gt;
&#20174;&#29983;&#29289;&#23398;&#21551;&#21457;&#26426;&#21046;&#37325;&#26032;&#24605;&#32771;&#36229;&#20687;&#32032;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Rethinking superpixel segmentation from biologically inspired mechanisms. (arXiv:2309.13438v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#31070;&#32463;&#32467;&#26500;&#21644;&#35270;&#35273;&#26426;&#21046;&#30340;&#21551;&#31034;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#36229;&#20687;&#32032;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#24378;&#31579;&#36873;&#27169;&#22359;&#65288;ESM&#65289;&#21644;&#36793;&#30028;&#24863;&#30693;&#26631;&#31614;&#65288;BAL&#65289;&#65292;&#26088;&#22312;&#20135;&#29983;&#20005;&#26684;&#36981;&#24490;&#29289;&#20307;&#36793;&#30028;&#19988;&#20256;&#36798;&#20016;&#23500;&#35270;&#35273;&#31526;&#21495;&#30340;&#36229;&#20687;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20687;&#32032;&#20998;&#21106;&#26041;&#27861;&#30340;&#21457;&#23637;&#22312;&#20998;&#21106;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#20135;&#29983;&#20005;&#26684;&#36981;&#24490;&#29289;&#20307;&#36793;&#30028;&#19988;&#20256;&#36798;&#20016;&#23500;&#35270;&#35273;&#31526;&#21495;&#30340;&#36229;&#20687;&#32032;&#65292;&#29305;&#21035;&#26159;&#24403;&#20132;&#21449;&#34920;&#38754;&#39068;&#33394;&#30456;&#20851;&#24615;&#21487;&#33021;&#24178;&#25200;&#29289;&#20307;&#26102;&#12290;&#21463;&#31070;&#32463;&#32467;&#26500;&#21644;&#35270;&#35273;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#29289;&#32593;&#32476;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22686;&#24378;&#31579;&#36873;&#27169;&#22359;&#65288;ESM&#65289;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#36793;&#30028;&#24863;&#30693;&#26631;&#31614;&#65288;BAL&#65289;&#65292;&#29992;&#20110;&#36229;&#20687;&#32032;&#20998;&#21106;&#12290;ESM&#36890;&#36807;&#27169;&#25311;&#35270;&#35273;&#30382;&#36136;&#30340;&#20132;&#20114;&#24335;&#25237;&#24433;&#26426;&#21046;&#26469;&#22686;&#24378;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;BAL&#27169;&#25311;&#20102;&#35270;&#35273;&#30382;&#36136;&#32454;&#32990;&#30340;&#31354;&#38388;&#39057;&#29575;&#29305;&#24615;&#65292;&#20197;&#20419;&#36827;&#29983;&#25104;&#20855;&#26377;&#24378;&#36793;&#30028;&#36981;&#24490;&#24615;&#30340;&#36229;&#20687;&#32032;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;BSDS500&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, advancements in deep learning-based superpixel segmentation methods have brought about improvements in both the efficiency and the performance of segmentation. However, a significant challenge remains in generating superpixels that strictly adhere to object boundaries while conveying rich visual significance, especially when cross-surface color correlations may interfere with objects. Drawing inspiration from neural structure and visual mechanisms, we propose a biological network architecture comprising an Enhanced Screening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel segmentation. The ESM enhances semantic information by simulating the interactive projection mechanisms of the visual cortex. Additionally, the BAL emulates the spatial frequency characteristics of visual cortical cells to facilitate the generation of superpixels with strong boundary adherence. We demonstrate the effectiveness of our approach through evaluations on both the BSDS500 dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.13409</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#25968;&#25454;&#37322;&#25918;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#31574;&#30053;&#65292;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#65288;FD&#65289;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#19982;&#20256;&#32479;&#30340;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;FD&#22312;&#20445;&#25345;&#31995;&#21015;&#35760;&#24518;&#30340;&#21516;&#26102;&#31283;&#23450;&#20102;&#23427;&#20197;&#20379;&#24314;&#27169;&#30446;&#30340;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#26469;&#33258;SPY&#25351;&#25968;&#30340;&#37329;&#34701;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#26032;&#38395;&#25253;&#36947;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36825;&#20010;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;FD&#19982;&#30446;&#26631;&#21464;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;&#37319;&#29992;&#20102;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#26469;&#39564;&#35777;FD&#31995;&#21015;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;FD&#30456;&#27604;&#25972;&#25968;&#24046;&#20998;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#36825;&#19968;&#28857;&#36890;&#36807;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;/&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;ROCAUC&#65289;&#21644;&#39532;&#20462;&#26031;&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#30340;&#35780;&#20272;&#24471;&#21040;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel forecasting strategy that leverages the power of fractional differencing (FD) to capture both short- and long-term dependencies in time series data. Unlike traditional integer differencing methods, FD preserves memory in series while stabilizing it for modeling purposes. By applying FD to financial data from the SPY index and incorporating sentiment analysis from news reports, this empirical analysis explores the effectiveness of FD in conjunction with binary classification of target variables. Supervised classification algorithms were employed to validate the performance of FD series. The results demonstrate the superiority of FD over integer differencing, as confirmed by Receiver Operating Characteristic/Area Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>GPTFUZZER&#26159;&#19968;&#31181;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#24037;&#24037;&#31243;&#65292;&#24182;&#36890;&#36807;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10253</link><description>&lt;p&gt;
GPTFUZZER : &#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. (arXiv:2309.10253v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10253
&lt;/p&gt;
&lt;p&gt;
GPTFUZZER&#26159;&#19968;&#31181;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#24037;&#24037;&#31243;&#65292;&#24182;&#36890;&#36807;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#24191;&#27867;&#29992;&#20110;&#26085;&#24120;&#23545;&#35805;&#21040;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;LLMs&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#21487;&#33021;&#20250;&#25552;&#20379;&#26377;&#20851;&#36827;&#34892;&#26377;&#23475;&#25110;&#38750;&#27861;&#27963;&#21160;&#30340;&#35814;&#32454;&#25351;&#23548;&#12290;&#34429;&#28982;&#23433;&#20840;&#25514;&#26045;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#36755;&#20986;&#30340;&#39118;&#38505;&#65292;&#20294;&#23545;&#25239;&#24615;&#30340;"&#36234;&#29425;"&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#21033;&#29992;LLMs&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#20123;&#36234;&#29425;&#27169;&#26495;&#36890;&#24120;&#26159;&#25163;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#65292;&#20351;&#22823;&#35268;&#27169;&#27979;&#35797;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;\fuzzer&#65292;&#21463;AFL&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#30340;&#21551;&#21457;&#12290;&#19982;&#25163;&#24037;&#24037;&#31243;&#19981;&#21516;&#65292;\fuzzer&#33258;&#21160;&#21270;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;LLMs&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#22312;&#26680;&#24515;&#37096;&#20998;&#65292;\fuzzer&#20174;&#20154;&#24037;&#32534;&#20889;&#30340;&#27169;&#26495;&#20316;&#20026;&#31181;&#23376;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#21464;&#24322;&#25805;&#20316;&#23545;&#20854;&#36827;&#34892;&#21464;&#24322;&#20197;&#29983;&#25104;&#26032;&#30340;&#27169;&#26495;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;\fuzzer&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#29992;&#20110;&#24179;&#34913;&#25928;&#29575;&#30340;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial "jailbreak" attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce \fuzzer, a novel black-box jailbreak fuzzing framework inspired by AFL fuzzing framework. Instead of manual engineering, \fuzzer automates the generation of jailbreak templates for red-teaming LLMs. At its core, \fuzzer starts with human-written templates as seeds, then mutates them using mutate operators to produce new templates. We detail three key components of \fuzzer: a seed selection strategy for balancing efficiency 
&lt;/p&gt;</description></item><item><title>&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06799</link><description>&lt;p&gt;
&#24403;&#22320;&#29699;&#31185;&#23398;&#36935;&#35265;&#22522;&#30784;&#27169;&#22411;&#65306;&#36208;&#21521;&#36890;&#29992;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
When Geoscience Meets Foundation Models: Towards General Geoscience Artificial Intelligence System. (arXiv:2309.06799v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06799
&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20195;&#34920;&#20102;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#23427;&#20204;&#20174;&#30334;&#19975;&#20159;&#23383;&#33410;&#30340;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25581;&#31034;&#20986;&#27934;&#23519;&#21147;&#12290;&#28789;&#27963;&#30340;&#20219;&#21153;&#35268;&#33539;&#12289;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#20197;&#21450;&#22810;&#27169;&#24577;&#30340;&#30693;&#35782;&#34920;&#31034;&#20351;&#24471;&#32508;&#21512;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22320;&#29699;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#20801;&#35768;&#35299;&#20915;&#19982;&#22320;&#29699;&#31995;&#32479;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#30340;&#22810;&#31181;&#39044;&#27979;&#12289;&#27169;&#25311;&#21644;&#20915;&#31574;&#25361;&#25112;&#12290;&#39046;&#22495;&#19987;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#20043;&#38388;&#30340;&#21512;&#20316;&#25512;&#21160;&#20102;&#36825;&#20123;&#23453;&#36149;&#24037;&#20855;&#22312;&#29702;&#35299;&#25105;&#20204;&#22320;&#29699;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#26041;&#38754;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#22686;&#24378;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geoscience foundation models represent a revolutionary approach in the field of Earth sciences by integrating massive cross-disciplinary data to simulate and understand the Earth systems dynamics. As a data-centric artificial intelligence (AI) paradigm, they uncover insights from petabytes of structured and unstructured data. Flexible task specification, diverse inputs and outputs and multi-modal knowledge representation enable comprehensive analysis infeasible with individual data sources. Critically, the scalability and generalizability of geoscience models allow for tackling diverse prediction, simulation, and decision challenges related to Earth systems interactions. Collaboration between domain experts and computer scientists leads to innovations in these invaluable tools for understanding the past, present, and future of our planet. However, challenges remain in validation and verification, scale, interpretability, knowledge representation, and social bias. Going forward, enhanci
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEABASE&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28304;&#20998;&#31163;&#20013;&#30340;&#29305;&#24449;&#20559;&#22909;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#34987;&#24573;&#35270;&#29305;&#24449;&#30340;&#38544;&#34255;&#20449;&#24687;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.05287</link><description>&lt;p&gt;
&#22788;&#29702;&#22768;&#38899;&#28304;&#20998;&#31163;&#20013;&#30340;&#29305;&#24449;&#22833;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Feature Imbalance in Sound Source Separation. (arXiv:2309.05287v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEABASE&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28304;&#20998;&#31163;&#20013;&#30340;&#29305;&#24449;&#20559;&#22909;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#34987;&#24573;&#35270;&#29305;&#24449;&#30340;&#38544;&#34255;&#20449;&#24687;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24120;&#24120;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#23384;&#22312;&#29305;&#24449;&#20559;&#22909;&#30340;&#38382;&#39064;&#65292;&#21363;&#36807;&#24230;&#20381;&#36182;&#26576;&#20123;&#29305;&#23450;&#29305;&#24449;&#32780;&#24573;&#35270;&#20854;&#20182;&#29305;&#24449;&#65292;&#21363;&#20351;&#37027;&#20123;&#34987;&#24573;&#35270;&#30340;&#29305;&#24449;&#23545;&#20219;&#21153;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29305;&#24449;&#20559;&#22909;&#38382;&#39064;&#20027;&#35201;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29305;&#24449;&#20559;&#22909;&#20063;&#23384;&#22312;&#20110;&#39640;&#32500;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#28304;&#20998;&#31163;&#20219;&#21153;&#20013;&#12290;&#20026;&#20102;&#20943;&#36731;&#28304;&#20998;&#31163;&#20013;&#30340;&#29305;&#24449;&#20559;&#22909;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEAture BAlancing by Suppressing Easy feature&#65288;FEABASE&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#34987;&#24573;&#35270;&#29305;&#24449;&#30340;&#38544;&#34255;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22810;&#36890;&#36947;&#28304;&#20998;&#31163;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20986;&#29616;&#20102;&#31354;&#38388;&#29305;&#24449;&#21644;&#38899;&#33394;&#29305;&#24449;&#20043;&#38388;&#30340;&#29305;&#24449;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often suffer from a feature preference problem, where they tend to overly rely on specific features to solve a task while disregarding other features, even if those neglected features are essential for the task. Feature preference problems have primarily been investigated in classification task. However, we observe that feature preference occurs in high-dimensional regression task, specifically, source separation. To mitigate feature preference in source separation, we propose FEAture BAlancing by Suppressing Easy feature (FEABASE). This approach enables efficient data utilization by learning hidden information about the neglected feature. We evaluate our method in a multi-channel source separation task, where feature preference between spatial feature and timbre feature appears.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04296</link><description>&lt;p&gt;
&#22312;COVID-19&#26399;&#38388;&#23548;&#33322;&#19981;&#22312;&#20998;&#24067;&#33539;&#22260;&#20869;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65306;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#25968;&#25454;&#20998;&#24067;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#38750;&#20998;&#24067;&#26399;&#38388;&#26102;&#65292;&#22914;COVID-19&#30340;&#23553;&#38145;&#26399;&#65292;&#25968;&#25454;&#20998;&#24067;&#19982;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#35265;&#30340;&#26126;&#26174;&#20559;&#31163;&#12290;&#26412;&#25991;&#37319;&#29992;&#21452;&#37325;&#31574;&#30053;&#65306;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#26356;&#26032;&#27169;&#22411;&#30340;&#26032;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#22312;&#24314;&#31569;&#29289;&#22806;&#37096;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#34892;&#20154;&#35745;&#25968;&#22120;&#25910;&#38598;&#30340;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#12290;&#19982;&#22312;&#32447;&#23398;&#20064;&#30456;&#27604;&#65292;&#21518;&#32773;&#24120;&#24120;&#20250;&#36973;&#21463;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#26032;&#33719;&#24471;&#30340;&#30693;&#35782;&#24120;&#24120;&#20250;&#25273;&#21435;&#20808;&#21069;&#30340;&#20449;&#24687;&#65292;&#25345;&#32493;&#23398;&#20064;&#21017;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23558;FSNet&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#22696;&#23572;&#26412;&#24066;13&#20010;&#24314;&#31569;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#20449;&#21644;&#21327;&#21516;&#30340;&#36719;&#20214;&#24037;&#31243;&#20154;&#24037;&#26234;&#33021;&#30340;&#24895;&#26223;&#21644;&#36335;&#32447;&#22270;&#65292;&#26088;&#22312;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#21644;&#36719;&#20214;&#36136;&#37327;&#12290;&#23454;&#29616;&#36825;&#20010;&#24895;&#26223;&#21487;&#33021;&#23548;&#33268;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36716;&#21464;&#65292;&#21363;&#36719;&#20214;&#24037;&#31243;2.0.</title><link>http://arxiv.org/abs/2309.04142</link><description>&lt;p&gt;
&#21487;&#20449;&#21644;&#21327;&#21516;&#30340;&#36719;&#20214;&#24037;&#31243;&#20154;&#24037;&#26234;&#33021;&#65306;&#24895;&#26223;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps. (arXiv:2309.04142v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#20449;&#21644;&#21327;&#21516;&#30340;&#36719;&#20214;&#24037;&#31243;&#20154;&#24037;&#26234;&#33021;&#30340;&#24895;&#26223;&#21644;&#36335;&#32447;&#22270;&#65292;&#26088;&#22312;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#21644;&#36719;&#20214;&#36136;&#37327;&#12290;&#23454;&#29616;&#36825;&#20010;&#24895;&#26223;&#21487;&#33021;&#23548;&#33268;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36716;&#21464;&#65292;&#21363;&#36719;&#20214;&#24037;&#31243;2.0.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#21313;&#24180;&#26469;&#65292;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#21644;&#25552;&#21319;&#36719;&#20214;&#36136;&#37327;&#12290;&#36807;&#21435;&#30340;&#20004;&#20010;&#21313;&#24180;&#35265;&#35777;&#20102;&#19987;&#38376;&#20026;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#26029;&#28044;&#29616;&#12290;&#36825;&#20010;&#21183;&#22836;&#24314;&#31435;&#20102;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#26368;&#27963;&#36291;&#21644;&#26368;&#21463;&#27426;&#36814;&#30340; Artificial Intelligence for Software Engineering (AI4SE) &#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20960;&#20010;&#37325;&#28857;&#12290;&#23427;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#21644;&#22238;&#39038;&#20102;AI4SE&#30340;&#21382;&#21490;&#12290;&#38543;&#21518;&#65292;&#23427;&#24378;&#35843;&#20102;AI4SE&#22266;&#26377;&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24378;&#35843;&#20102;&#23454;&#29616;&#21487;&#20449;&#21644;&#21327;&#21516;&#30340;AI4SE&#30340;&#38656;&#27714;&#12290;&#36827;&#19968;&#27493;&#65292;&#26412;&#25991;&#25551;&#32472;&#20102;&#19968;&#31181;&#24895;&#26223;&#65292;&#21363;&#22914;&#26524;&#20811;&#26381;&#20102;AI4SE&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#23558;&#23454;&#29616;&#21487;&#36798;&#21040;&#30340;&#28508;&#22312;&#39134;&#36291;&#65292;&#24314;&#35758;&#36716;&#21521;&#36719;&#20214;&#24037;&#31243;2.0.
&lt;/p&gt;
&lt;p&gt;
For decades, much software engineering research has been dedicated to devising automated solutions aimed at enhancing developer productivity and elevating software quality. The past two decades have witnessed an unparalleled surge in the development of intelligent solutions tailored for software engineering tasks. This momentum established the Artificial Intelligence for Software Engineering (AI4SE) area, which has swiftly become one of the most active and popular areas within the software engineering field.  This Future of Software Engineering (FoSE) paper navigates through several focal points. It commences with a succinct introduction and history of AI4SE. Thereafter, it underscores the core challenges inherent to AI4SE, particularly highlighting the need to realize trustworthy and synergistic AI4SE. Progressing, the paper paints a vision for the potential leaps achievable if AI4SE's key challenges are surmounted, suggesting a transition towards Software Engineering 2.0. Two strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#32622;&#20449;&#24230;&#37327;&#21270;&#20102;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16175</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#21644;&#22806;&#22312;&#32622;&#20449;&#24230;&#35780;&#20272;&#26469;&#37327;&#21270;&#20219;&#24847;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment. (arXiv:2308.16175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#32622;&#20449;&#24230;&#37327;&#21270;&#20102;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20219;&#20309;&#36755;&#20986;&#30340;&#25968;&#20540;&#32622;&#20449;&#24230;&#26469;&#26816;&#27979;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#36866;&#29992;&#20110;&#20165;&#36890;&#36807;&#40657;&#30418;API&#35775;&#38382;&#30340;&#20219;&#20309;LLM&#65292;&#24182;&#23558;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#30340;&#32622;&#20449;&#24230;&#32467;&#21512;&#20026;&#23545;&#32473;&#23450;&#25552;&#31034;&#19979;LLM&#21709;&#24212;&#30340;&#21333;&#20010;&#21487;&#20449;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#36890;&#29992;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24403;&#20170;&#25152;&#26377;&#26368;&#22909;&#30340;LLM&#65288;&#20854;&#35757;&#32451;&#25968;&#25454;&#26410;&#30693;&#65289;&#12290;&#36890;&#36807;&#39069;&#22806;&#30340;&#35745;&#31639;&#65292;&#20219;&#20309;LLM API&#30340;&#29992;&#25143;&#29616;&#22312;&#21487;&#20197;&#33719;&#24471;&#19982;&#36890;&#24120;&#30456;&#21516;&#30340;&#21709;&#24212;&#65292;&#20197;&#21450;&#19968;&#20010;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#20197;&#20415;&#22312;&#19981;&#20449;&#20219;&#35813;&#21709;&#24212;&#26102;&#20445;&#25345;&#35880;&#24910;&#12290;&#23545;&#20110;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BSDetector&#27604;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65288;&#23545;&#20110;GPT-3&#21644;ChatGPT&#65289;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#38169;&#35823;&#30340;LLM&#21709;&#24212;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#21709;&#24212;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, and combines intrinsic and extrinsic assessments of confidence into a single trustworthiness estimate for any LLM response to a given prompt. Our method is extremely general and can applied to all of the best LLMs available today (whose training data remains unknown). By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that caution when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>AutoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#19979;&#19968;&#20195;LLM&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#20248;&#38597;&#22320;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.08155</link><description>&lt;p&gt;
AutoGen:&#36890;&#36807;&#22810;&#20195;&#29702;&#23545;&#35805;&#26694;&#26550;&#23454;&#29616;&#19979;&#19968;&#20195;LLM&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. (arXiv:2308.08155v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08155
&lt;/p&gt;
&lt;p&gt;
AutoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#19979;&#19968;&#20195;LLM&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#20248;&#38597;&#22320;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;AutoGen&#65292;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#26469;&#24320;&#21457;LLM&#24212;&#29992;&#31243;&#24207;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;AutoGen&#20195;&#29702;&#21487;&#20197;&#23450;&#21046;&#12289;&#21487;&#23545;&#35805;&#65292;&#24182;&#19988;&#21487;&#20197;&#26080;&#32541;&#22320;&#20801;&#35768;&#20154;&#31867;&#21442;&#19982;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#21508;&#31181;&#27169;&#24335;&#19979;&#36816;&#34892;&#65292;&#21033;&#29992;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#24037;&#20855;&#30340;&#32452;&#21512;&#12290;AutoGen&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#22810;&#20010;&#20248;&#21183;&#65306;a&#65289;&#23427;&#33021;&#22815;&#20248;&#38597;&#22320;&#22788;&#29702;&#36825;&#20123;LLM&#30340;&#24378;&#22823;&#20294;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65307;b&#65289;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#36890;&#36807;&#20195;&#29702;&#20043;&#38388;&#30340;&#23545;&#35805;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#33258;&#21160;&#21270;&#65307;c&#65289;&#23427;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#22797;&#26434;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29616;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#24320;&#21457;&#20154;&#21592;&#22914;&#20309;&#36731;&#26494;&#20351;&#29992;AutoGen&#26377;&#25928;&#22320;&#35299;&#20915;&#20219;&#21153;&#25110;&#26500;&#24314;&#24212;&#29992;&#31243;&#24207;&#65292;&#28085;&#30422;&#32534;&#31243;&#12289;&#25968;&#23398;&#12289;&#36816;&#31609;&#23398;&#12289;&#23089;&#20048;&#12289;&#22312;&#32447;&#20915;&#31574;&#12289;&#38382;&#31572;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents AutoGen, a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen's design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.
&lt;/p&gt;</description></item><item><title>PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.07327</link><description>&lt;p&gt;
PokerKit: &#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#30340;&#20840;&#38754;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations. (arXiv:2308.07327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07327
&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#21644;&#25163;&#29260;&#35780;&#20272;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#21482;&#25903;&#25345;&#23569;&#37327;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#19988;&#22312;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;PokerKit&#36890;&#36807;&#25903;&#25345;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#30340;&#26550;&#26500;&#20379;&#29992;&#25143;&#23450;&#20041;&#33258;&#23450;&#20041;&#28216;&#25103;&#65292;&#26174;&#33879;&#25193;&#22823;&#20102;&#36825;&#19968;&#33539;&#22260;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;PokerKit&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#21253;&#25324;&#20854;&#30452;&#35266;&#30340;&#32534;&#31243;API&#65292;&#22810;&#21464;&#20307;&#28216;&#25103;&#25903;&#25345;&#20197;&#21450;&#32479;&#19968;&#30340;&#25163;&#29260;&#35780;&#20272;&#22871;&#20214;&#22312;&#19981;&#21516;&#25163;&#29260;&#31867;&#22411;&#38388;&#30340;&#24212;&#29992;&#12290;PokerKit&#30340;&#28789;&#27963;&#24615;&#20351;&#20854;&#33021;&#22815;&#22312;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#12290;PokerKit&#30340;&#21487;&#38752;&#24615;&#36890;&#36807;&#38745;&#24577;&#31867;&#22411;&#26816;&#26597;&#12289;&#24191;&#27867;&#30340;doctest&#21644;&#21333;&#20803;&#27979;&#35797;&#26469;&#30830;&#20445;&#65292;&#36798;&#21040;&#20102;97%&#30340;&#20195;&#30721;&#35206;&#30422;&#29575;&#12290;&#24341;&#20837;PokerKit&#20195;&#34920;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
PokerKit is an open-source Python library designed to overcome the restrictions of existing poker game simulation and hand evaluation tools, which typically support only a handful of poker variants and lack flexibility in game state control. In contrast, PokerKit significantly expands this scope by supporting an extensive array of poker variants and it provides a flexible architecture for users to define their custom games. This paper details the design and implementation of PokerKit, including its intuitive programmatic API, multi-variant game support, and a unified hand evaluation suite across different hand types. The flexibility of PokerKit allows for applications in diverse areas, such as poker AI development, tool creation, and online poker casino implementation. PokerKit's reliability has been established through static type checking, extensive doctests, and unit tests, achieving 97\% code coverage. The introduction of PokerKit represents a significant contribution to the field 
&lt;/p&gt;</description></item><item><title>Robustified ANNs&#36890;&#36807;&#21457;&#29616;&#20302;&#33539;&#25968;&#22270;&#20687;&#25200;&#21160;&#65292;&#25581;&#31034;&#20102;&#22312;&#8220;&#20154;&#31867;&#20551;&#23450;&#31283;&#23450;&#8221;&#33539;&#22260;&#20869;&#20154;&#31867;&#31867;&#21035;&#30693;&#35273;&#20043;&#38388;&#30340;&#24040;&#22823;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2308.06887</link><description>&lt;p&gt;
Robustified ANNs&#25581;&#31034;&#20102;&#20154;&#31867;&#31867;&#21035;&#30693;&#35273;&#20043;&#38388;&#30340;&#34411;&#27934;
&lt;/p&gt;
&lt;p&gt;
Robustified ANNs Reveal Wormholes Between Human Category Percepts. (arXiv:2308.06887v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06887
&lt;/p&gt;
&lt;p&gt;
Robustified ANNs&#36890;&#36807;&#21457;&#29616;&#20302;&#33539;&#25968;&#22270;&#20687;&#25200;&#21160;&#65292;&#25581;&#31034;&#20102;&#22312;&#8220;&#20154;&#31867;&#20551;&#23450;&#31283;&#23450;&#8221;&#33539;&#22260;&#20869;&#20154;&#31867;&#31867;&#21035;&#30693;&#35273;&#20043;&#38388;&#30340;&#24040;&#22823;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#30340;&#35270;&#35273;&#23545;&#35937;&#31867;&#21035;&#25253;&#21578;&#23545;&#24494;&#23567;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#12290;&#30001;&#20110;&#20154;&#31867;&#31867;&#21035;&#25253;&#21578;&#65288;&#21448;&#31216;&#20154;&#31867;&#30693;&#35273;&#65289;&#34987;&#35748;&#20026;&#23545;&#21516;&#26679;&#23567;&#33539;&#25968;&#30340;&#25200;&#21160;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#23616;&#37096;&#31283;&#23450;&#30340;&#65292;&#36825;&#34920;&#26126;ANNs&#19981;&#23436;&#20840;&#26159;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#30340;&#31185;&#23398;&#27169;&#22411;&#12290;&#19982;&#27492;&#19968;&#33268;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#26631;&#20934;ANN&#27169;&#22411;&#29983;&#25104;&#23567;&#33539;&#25968;&#22270;&#20687;&#25200;&#21160;&#26102;&#65292;&#20154;&#31867;&#23545;&#35937;&#31867;&#21035;&#30693;&#35273;&#30830;&#23454;&#38750;&#24120;&#31283;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#8220;&#20154;&#31867;&#20551;&#23450;&#31283;&#23450;&#8221;&#30340;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#21457;&#29616;Robustified ANNs&#21487;&#38752;&#22320;&#21457;&#29616;&#20302;&#33539;&#25968;&#22270;&#20687;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#24378;&#28872;&#24178;&#25200;&#20154;&#31867;&#30693;&#35273;&#12290;&#36825;&#20123;&#20197;&#21069;&#26080;&#27861;&#26816;&#27979;&#30340;&#20154;&#31867;&#30693;&#35273;&#24178;&#25200;&#22312;&#24133;&#24230;&#19978;&#26159;&#24040;&#22823;&#30340;&#65292;&#25509;&#36817;&#20110;Robustified ANNs&#20013;&#35266;&#23519;&#21040;&#30340;&#25935;&#24863;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The visual object category reports of artificial neural networks (ANNs) are notoriously sensitive to tiny, adversarial image perturbations. Because human category reports (aka human percepts) are thought to be insensitive to those same small-norm perturbations -- and locally stable in general -- this argues that ANNs are incomplete scientific models of human visual perception. Consistent with this, we show that when small-norm image perturbations are generated by standard ANN models, human object category percepts are indeed highly stable. However, in this very same "human-presumed-stable" regime, we find that robustified ANNs reliably discover low-norm image perturbations that strongly disrupt human percepts. These previously undetectable human perceptual disruptions are massive in amplitude, approaching the same level of sensitivity seen in robustified ANNs. Further, we show that robustified ANNs support precise perceptual state interventions: they guide the construction of low-norm 
&lt;/p&gt;</description></item><item><title>FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10928</link><description>&lt;p&gt;
FLASK: &#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. (arXiv:2307.10928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10928
&lt;/p&gt;
&lt;p&gt;
FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25351;&#20196;&#38656;&#35201;&#19982;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25216;&#33021;&#38598;&#26681;&#25454;&#25351;&#20196;&#32780;&#24322;&#65292;&#22240;&#27492;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#35780;&#20272;&#65288;&#21363;&#22522;&#20110;&#25972;&#20307;&#20559;&#22909;&#30340;&#35780;&#20272;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#26410;&#32771;&#34385;&#38656;&#35201;&#23454;&#20363;&#32423;&#25216;&#33021;&#32452;&#21512;&#30340;&#29992;&#25143;&#25351;&#20196;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FLASK&#65288;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#23427;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#23545;&#20110;&#33719;&#24471;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;FLASK&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22810;&#20010;&#24320;&#28304;&#21644;&#19987;&#26377;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#39640;&#24230;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlati
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#25480;&#26435;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35745;&#31639;&#25480;&#26435;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#19979;&#30028;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#25480;&#26435;&#35745;&#31639;&#65292;&#24182;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.02728</link><description>&lt;p&gt;
&#20998;&#23618;&#25480;&#26435;&#65306;&#26397;&#30528;&#21487;&#34892;&#30340;&#22522;&#20110;&#25480;&#26435;&#30340;&#25216;&#33021;&#23398;&#20064;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning. (arXiv:2307.02728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02728
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25480;&#26435;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35745;&#31639;&#25480;&#26435;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#19979;&#30028;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#25480;&#26435;&#35745;&#31639;&#65292;&#24182;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26234;&#33021;&#20307;&#38656;&#35201;&#22823;&#37327;&#30340;&#25216;&#33021;&#12290; &#25480;&#26435; - &#25216;&#33021;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#26368;&#22823;&#20114;&#20449;&#24687; - &#20026;&#23398;&#20064;&#22823;&#37327;&#19981;&#21516;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#26465;&#36335;&#24452;&#65292;&#20294;&#20114;&#20449;&#24687;&#24456;&#38590;&#20248;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20998;&#23618;&#25480;&#26435;&#65292;&#36890;&#36807;&#38598;&#25104;&#30446;&#26631;&#26465;&#20214;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;&#35745;&#31639;&#25480;&#26435;&#26356;&#21152;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#21487;&#29992;&#20110;&#35745;&#31639;&#30701;&#26399;&#35270;&#35282;&#19979;&#30340;&#25480;&#26435;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#25351;&#25968;&#26102;&#38388;&#23610;&#24230;&#19979;&#30340;&#25480;&#26435;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#36129;&#29486;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#34434;&#34433;&#23548;&#33322;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#22235;&#32423;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#35206;&#30422;&#38754;&#31215;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
General purpose agents will require large repertoires of skills. Empowerment -- the maximum mutual information between skills and the states -- provides a pathway for learning large collections of distinct skills, but mutual information is difficult to optimize. We introduce a new framework, Hierarchical Empowerment, that makes computing empowerment more tractable by integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning. Our framework makes two specific contributions. First, we introduce a new variational lower bound on mutual information that can be used to compute empowerment over short horizons. Second, we introduce a hierarchical architecture for computing empowerment over exponentially longer time scales. We verify the contributions of the framework in a series of simulated robotics tasks. In a popular ant navigation domain, our four level agents are able to learn skills that cover a surface area over two orders of magnitude larger than prior work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#21629;&#21517;&#23454;&#20307;&#36951;&#28431;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#30340;&#21253;&#21547;&#24773;&#20917;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.02570</link><description>&lt;p&gt;
&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Named Entity Inclusion in Abstractive Text Summarization. (arXiv:2307.02570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#21629;&#21517;&#23454;&#20307;&#36951;&#28431;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#30340;&#21253;&#21547;&#24773;&#20917;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#35768;&#22810;&#24403;&#21069;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#22120;&#30340;&#32570;&#28857;&#65292;&#21363;&#21629;&#21517;&#23454;&#20307;&#30340;&#36951;&#28431;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#30340;&#27880;&#24847;&#21147;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;RoBERTa&#26469;&#30830;&#23450;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35813;&#27169;&#22411;&#23545;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#36827;&#34892;&#23631;&#34109;&#65292;&#20877;&#20351;&#29992;BART&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#37325;&#24314;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;BART&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#21253;&#21547;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the named entity omission - the drawback of many current abstractive text summarizers. We suggest a custom pretraining objective to enhance the model's attention on the named entities in a text. At first, the named entity recognition model RoBERTa is trained to determine named entities in the text. After that, this model is used to mask named entities in the text and the BART model is trained to reconstruct them. Next, the BART model is fine-tuned on the summarization task. Our experiments showed that this pretraining approach improves named entity inclusion precision and recall metrics.
&lt;/p&gt;</description></item><item><title>MedCPT&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;PubMed&#25628;&#32034;&#26085;&#24535;&#36827;&#34892;&#35757;&#32451;&#65292;MedCPT&#22312;&#20845;&#20010;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2307.00589</link><description>&lt;p&gt;
MedCPT: &#20351;&#29992;&#22823;&#35268;&#27169;PubMed&#25628;&#32034;&#26085;&#24535;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#36827;&#34892;&#38646;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval. (arXiv:2307.00589v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00589
&lt;/p&gt;
&lt;p&gt;
MedCPT&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;PubMed&#25628;&#32034;&#26085;&#24535;&#36827;&#34892;&#35757;&#32451;&#65292;MedCPT&#22312;&#20845;&#20010;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#22312;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#33719;&#21462;&#21644;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#22120;&#22312;&#35821;&#20041;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#26597;&#35810;-&#25991;&#31456;&#27880;&#37322;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24456;&#38590;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21482;&#36827;&#34892;&#35789;&#27719;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MedCPT&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#21019;&#30340;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;&#20026;&#20102;&#35757;&#32451;MedCPT&#65292;&#25105;&#20204;&#20174;PubMed&#25910;&#38598;&#20102;255 million&#20010;&#29992;&#25143;&#28857;&#20987;&#26085;&#24535;&#65292;&#36825;&#26159;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#35757;&#32451;&#19968;&#23545;&#23494;&#20999;&#38598;&#25104;&#30340;&#26816;&#32034;&#22120;&#21644;&#37325;&#25490;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MedCPT&#22312;&#20845;&#20010;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#20248;&#20110;&#21253;&#25324;&#26356;&#22823;&#27169;&#22411;&#65288;&#22914;GPT-3&#22823;&#23567;&#30340;cpt-text-XL&#65289;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;MedCPT&#36824;&#33021;&#22815;&#29983;&#25104;&#26356;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) is essential in biomedical knowledge acquisition and clinical decision support. While recent progress has shown that language model encoders perform better semantic retrieval, training such models requires abundant query-article annotations that are difficult to obtain in biomedicine. As a result, most biomedical IR systems only conduct lexical matching. In response, we introduce MedCPT, a first-of-its-kind Contrastively Pre-trained Transformer model for zero-shot semantic IR in biomedicine. To train MedCPT, we collected an unprecedented scale of 255 million user click logs from PubMed. With such data, we use contrastive learning to train a pair of closely-integrated retriever and re-ranker. Experimental results show that MedCPT sets new state-of-the-art performance on six biomedical IR tasks, outperforming various baselines including much larger models such as GPT-3-sized cpt-text-XL. In addition, MedCPT also generates better biomedical article and sentence 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15724</link><description>&lt;p&gt;
REFLECT:&#23545;&#26426;&#22120;&#20154;&#32463;&#21382;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#29992;&#20110;&#22833;&#36133;&#35299;&#37322;&#21644;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#22833;&#36133;&#25191;&#34892;&#26159;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#31283;&#20581;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#36755;&#20837;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;LLM&#30340;&#21147;&#37327;&#36827;&#34892;&#26426;&#22120;&#20154;&#22833;&#36133;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;REFLECT&#65292;&#23558;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#36807;&#21435;&#32463;&#39564;&#30340;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#36880;&#27493;&#22833;&#36133;&#35299;&#37322;&#31639;&#27861;&#26597;&#35810;LLM&#12290;&#22522;&#20110;&#35299;&#37322;&#65292;&#22833;&#36133;&#32416;&#27491;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#20010;&#21487;&#25191;&#34892;&#35745;&#21010;&#65292;&#20197;&#32416;&#27491;&#22833;&#36133;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;RoboFail&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#20174;&#32780;&#24110;&#21161;&#25104;&#21151;&#30340;&#32416;&#27491;&#35268;&#21010;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://roboreflect.github.io/
&lt;/p&gt;
&lt;p&gt;
The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
&lt;/p&gt;</description></item><item><title>GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;AI&#27169;&#22411;GPT-4&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#65292;&#33021;&#22815;&#21457;&#29616;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#31995;&#21015;&#12290;</title><link>http://arxiv.org/abs/2306.14915</link><description>&lt;p&gt;
GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#29992;&#20110;MOF&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Reticular Chemist for MOF Discovery. (arXiv:2306.14915v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14915
&lt;/p&gt;
&lt;p&gt;
GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;AI&#27169;&#22411;GPT-4&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#65292;&#33021;&#22815;&#21457;&#29616;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;AI&#27169;&#22411;GPT-4&#38598;&#25104;&#21040;&#32593;&#29366;&#21270;&#23398;&#23454;&#39564;&#30340;&#36845;&#20195;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;AI&#19982;&#20154;&#31867;&#23398;&#24466;&#20043;&#38388;&#30340;&#21512;&#20316;&#24037;&#20316;&#27969;&#12290;&#36825;&#20010;GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#26159;&#19968;&#20010;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#30340;&#32508;&#21512;&#31995;&#32479;&#12290;&#27599;&#20010;&#38454;&#27573;&#37117;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#21033;&#29992;GPT-4&#65292;&#20854;&#20013;GPT-4&#20026;&#21270;&#23398;&#23454;&#39564;&#25552;&#20379;&#35814;&#32454;&#30340;&#25351;&#23548;&#65292;&#23398;&#24466;&#21017;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#21453;&#39304;&#65292;&#21253;&#25324;&#25104;&#21151;&#21644;&#22833;&#36133;&#65292;&#20197;&#20415;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;AI&#36827;&#34892;&#20869;&#37096;&#23398;&#20064;&#12290;&#36825;&#31181;&#36845;&#20195;&#30340;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#20351;&#24471;GPT-4&#33021;&#22815;&#20687;&#32463;&#39564;&#20016;&#23500;&#30340;&#21270;&#23398;&#23478;&#19968;&#26679;&#20174;&#32467;&#26524;&#20013;&#23398;&#20064;&#65292;&#37319;&#29992;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#31995;&#32479;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#24320;&#21457;&#21644;&#25805;&#20316;&#65292;&#26080;&#38656;&#32534;&#30721;&#25216;&#33021;&#65292;&#22240;&#27492;&#23545;&#25152;&#26377;&#21270;&#23398;&#23478;&#37117;&#20855;&#26377;&#21487;&#35775;&#38382;&#24615;&#12290;&#25105;&#20204;&#30340;GPT-4&#32593;&#29366;&#21270;&#23398;&#23478;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new framework integrating the AI model GPT-4 into the iterative process of reticular chemistry experimentation, leveraging a cooperative workflow of interaction between AI and a human apprentice. This GPT-4 Reticular Chemist is an integrated system composed of three phases. Each of these utilizes GPT-4 in various capacities, wherein GPT-4 provides detailed instructions for chemical experimentation and the apprentice provides feedback on the experimental outcomes, including both success and failures, for the in-text learning of AI in the next iteration. This iterative human-AI interaction enabled GPT-4 to learn from the outcomes, much like an experienced chemist, by a prompt-learning strategy. Importantly, the system is based on natural language for both development and operation, eliminating the need for coding skills, and thus, make it accessible to all chemists. Our GPT-4 Reticular Chemist demonstrated the discovery of an isoreticular series of metal-organic frameworks (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2306.13196</link><description>&lt;p&gt;
DiMSam:&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#19982;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#22320;&#35745;&#21010;&#38271;&#21608;&#26399;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#35268;&#21010;&#27169;&#22411;&#65292;&#22240;&#27492;&#22312;&#29615;&#22659;&#21644;&#20854;&#21160;&#24577;&#19981;&#23436;&#20840;&#20102;&#35299;&#30340;&#39046;&#22495;&#20013;&#24212;&#29992;&#23427;&#20204;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23398;&#20064;&#25429;&#33719;&#35268;&#21010;&#27169;&#22411;&#20013;&#38590;&#20197;&#35774;&#35745;&#30340;&#32422;&#26463;&#21644;&#37319;&#26679;&#22120;&#12290;&#36825;&#20123;&#23398;&#20064;&#37319;&#26679;&#22120;&#22312;TAMP&#27714;&#35299;&#22120;&#20013;&#32452;&#21512;&#21644;&#21512;&#24182;&#65292;&#20197;&#32852;&#21512;&#25214;&#21040;&#28385;&#36275;&#35268;&#21010;&#20013;&#32422;&#26463;&#30340;&#34892;&#21160;&#21442;&#25968;&#20540;&#12290;&#20026;&#20102;&#20415;&#20110;&#23545;&#29615;&#22659;&#20013;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#37319;&#26679;&#22120;&#23450;&#20041;&#20026;&#23398;&#20064;&#30340;&#20302;&#32500;&#28508;&#21464;&#37327;&#23884;&#20837;&#30340;&#21487;&#21464;&#23545;&#35937;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20851;&#33410;&#24335;&#29289;&#20307;&#25805;&#20316;&#39046;&#22495;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#32463;&#20856;TAMP&#12289;&#29983;&#25104;&#23398;&#20064;&#21644;&#28508;&#22312;&#23884;&#20837;&#30340;&#32452;&#21512;&#22914;&#20309;&#20351;&#24471;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#36827;&#34892;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09222</link><description>&lt;p&gt;
&#38543;&#26426;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09222
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;f-&#25955;&#24230;&#30340;&#21551;&#21457;&#65292;&#24050;&#30693;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#25913;&#36827;&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21152;&#26435;&#26041;&#26696;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#32467;&#21512;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;DomainBed&#21644;Tabular&#20998;&#31867;&#22522;&#20934;&#19978;&#20998;&#21035;&#27604;&#29616;&#26377;&#26368;&#20339;&#32467;&#26524;&#25552;&#21319;&#20102;0.7%&#21644;1.44%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;BERT&#22312;GLUE&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.94%&#65292;&#23558;ViT&#22312;ImageNet-1K&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.01%&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#39044;&#31034;&#30528;&#23427;&#22312;&#25913;&#21892;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25193;&#25955;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#38598;&#25104;&#21040;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#26174;&#33879;&#21152;&#36895;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.06991</link><description>&lt;p&gt;
&#24555;&#36895;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast Diffusion Model. (arXiv:2306.06991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25193;&#25955;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#38598;&#25104;&#21040;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#26174;&#33879;&#21152;&#36895;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#20197;&#20854;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25193;&#25955;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#20174;&#38543;&#26426;&#20248;&#21270;&#30340;&#35282;&#24230;&#26174;&#33879;&#21152;&#36895;DMs&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;DMs&#30340;&#25193;&#25955;&#36807;&#31243;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#38543;&#26426;&#26102;&#21464;&#38382;&#39064;&#30340;&#38543;&#26426;&#20248;&#21270;&#36807;&#31243;&#30456;&#31526;&#21512;&#12290;&#28982;&#21518;&#65292;&#21463;&#21040;&#21160;&#37327;SGD&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26799;&#24230;&#21644;&#39069;&#22806;&#30340;&#21160;&#37327;&#65292;&#20197;&#23454;&#29616;&#27604;SGD&#26356;&#24555;&#21644;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#65292;&#25105;&#20204;&#23558;&#21160;&#37327;&#38598;&#25104;&#21040;DMs&#30340;&#25193;&#25955;&#36807;&#31243;&#20013;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21363;&#20174;&#22522;&#20110;&#21160;&#37327;&#30340;&#25193;&#25955;&#36807;&#31243;&#20013;&#23548;&#20986;&#22122;&#22768;&#25200;&#21160;&#26680;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#26500;&#24314;&#20026;&#19968;&#20010;&#38459;&#23612;&#25391;&#33633;&#31995;&#32479;&#65292;&#20020;&#30028;&#38459;&#23612;&#29366;&#24577;-&#26680;&#35299;&#20915;&#26041;&#26696;-&#36991;&#20813;&#25391;&#33633;&#65292;&#20351;&#25193;&#25955;&#36807;&#31243;&#30340;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FDM&#22312;&#21152;&#36895;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) have been adopted across diverse fields with its remarkable abilities in capturing intricate data distributions. In this paper, we propose a Fast Diffusion Model (FDM) to significantly speed up DMs from a stochastic optimization perspective for both faster training and sampling. We first find that the diffusion process of DMs accords with the stochastic optimization process of stochastic gradient descent (SGD) on a stochastic time-variant problem. Then, inspired by momentum SGD that uses both gradient and an extra momentum to achieve faster and more stable convergence than SGD, we integrate momentum into the diffusion process of DMs. This comes with a unique challenge of deriving the noise perturbation kernel from the momentum-based diffusion process. To this end, we frame the process as a Damped Oscillation system whose critically damped state -- the kernel solution -avoids oscillation and yields a faster convergence speed of the diffusion process. Empirical r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03872</link><description>&lt;p&gt;
&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#39564;&#35777;&#24605;&#32500;&#38142;&#30340;&#25512;&#29702;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#20351;&#27169;&#22411;&#20135;&#29983;&#26356;&#20840;&#38754;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24605;&#32500;&#38142;&#30340;&#24378;&#35843;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21487;&#33021;&#20250;&#19981;&#24910;&#23548;&#33268;&#20135;&#29983;&#24187;&#35273;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#20174;&#32780;&#38480;&#21046;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#22914;&#20309;&#36827;&#34892;&#32454;&#33268;&#30340;&#28436;&#32462;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#25105;&#20204;&#26088;&#22312;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#39564;&#35777;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#20687;ChatGPT&#36825;&#26679;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30452;&#25509;&#39564;&#35777;&#25972;&#20010;&#28436;&#32462;&#25512;&#29702;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25512;&#29702;&#39564;&#35777;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36880;&#27493;&#30340;&#23376;&#36807;&#31243;&#65292;&#27599;&#20010;&#36807;&#31243;&#21482;&#25509;&#25910;&#20854;&#24517;&#35201;&#30340;&#19978;&#19979;&#25991;&#21644;&#21069;&#25552;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#36275;&#30340;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38169;&#35823;&#20998;&#31867;&#35299;&#37322;&#38382;&#39064;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#12289;&#35780;&#20998;&#21644;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2306.03531</link><description>&lt;p&gt;
&#25299;&#23637;&#21487;&#35299;&#37322;&#24615;&#35270;&#37326;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#31995;&#32479;&#29992;&#20110;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38169;&#35823;&#20998;&#31867;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expanding Explainability Horizons: A Unified Concept-Based System for Local, Global, and Misclassification Explanations. (arXiv:2306.03531v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#36275;&#30340;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38169;&#35823;&#20998;&#31867;&#35299;&#37322;&#38382;&#39064;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#12289;&#35780;&#20998;&#21644;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26234;&#33021;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#21508;&#31181;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#27010;&#24565;&#30340;&#25216;&#26415;&#20197;&#21033;&#29992;&#19968;&#32452;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20026;&#29305;&#28857;&#65292;&#32780;&#19981;&#26159;&#20851;&#27880;&#20110;&#21333;&#20010;&#20687;&#32032;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#20379;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#65292;&#24182;&#33021;&#35299;&#37322;&#38169;&#35823;&#20998;&#31867;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#31995;&#32479;&#65292;&#23558;&#22810;&#20010;&#36229;&#20687;&#32032;&#22270;&#20687;&#36755;&#20837;&#32593;&#32476;&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#30446;&#26631;&#23545;&#35937;&#20197;&#21450;&#30446;&#26631;&#27010;&#24565;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#12289;&#35780;&#20998;&#21644;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#38500;&#20102;&#25552;&#39640;&#24615;&#33021;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#39044;&#27979;&#65292;&#24182;&#38416;&#26126;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability of intelligent models has been garnering increasing attention in recent years. Of the various explainability approaches, concept-based techniques are notable for utilizing a set of human-meaningful concepts instead of focusing on individual pixels. However, there is a scarcity of methods that consistently provide both local and global explanations. Moreover, most of the methods have no offer to explain misclassification cases. To address these challenges, our study follows a straightforward yet effective approach. We propose a unified concept-based system, which inputs a number of super-pixelated images into the networks, allowing them to learn better representations of the target's objects as well as the target's concepts. This method automatically learns, scores, and extracts local and global concepts. Our experiments revealed that, in addition to enhancing performance, the models could provide deeper insights into predictions and elucidate false classifications.
&lt;/p&gt;</description></item><item><title>RepoBench&#26159;&#19968;&#20010;&#35780;&#20272;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#22312;&#20195;&#30721;&#24211;&#32423;&#21035;&#19978;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#25903;&#25345;Python&#21644;Java&#65292;&#21253;&#21547;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#22635;&#34917;&#24403;&#21069;&#22522;&#20934;&#22312;&#22810;&#25991;&#20214;&#32534;&#31243;&#22330;&#26223;&#26041;&#38754;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#20026;&#19981;&#21516;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.03091</link><description>&lt;p&gt;
RepoBench&#65306;&#35780;&#20272;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#30340;&#20195;&#30721;&#24211;&#32423;&#21035;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems. (arXiv:2306.03091v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03091
&lt;/p&gt;
&lt;p&gt;
RepoBench&#26159;&#19968;&#20010;&#35780;&#20272;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#22312;&#20195;&#30721;&#24211;&#32423;&#21035;&#19978;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#25903;&#25345;Python&#21644;Java&#65292;&#21253;&#21547;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#22635;&#34917;&#24403;&#21069;&#22522;&#20934;&#22312;&#22810;&#25991;&#20214;&#32534;&#31243;&#22330;&#26223;&#26041;&#38754;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#20026;&#19981;&#21516;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#65292;&#26377;&#28508;&#21147;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#26174;&#33879;&#30340;&#29983;&#20135;&#21147;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#25991;&#20214;&#20219;&#21153;&#19978;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#25991;&#20214;&#32534;&#31243;&#22330;&#26223;&#30340;&#35780;&#20272;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RepoBench&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#24211;&#32423;&#21035;&#30340;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#30340;&#26032;&#22522;&#20934;&#12290;RepoBench&#25903;&#25345;Python&#21644;Java&#65292;&#24182;&#21253;&#21547;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#35780;&#20272;&#20219;&#21153;&#65306;RepoBench-R&#65288;&#26816;&#32034;&#65289;&#12289;RepoBench-C&#65288;&#20195;&#30721;&#34917;&#20840;&#65289;&#21644;RepoBench-P&#65288;&#27969;&#27700;&#32447;&#65289;&#12290;&#27599;&#20010;&#20219;&#21153;&#20998;&#21035;&#27979;&#37327;&#31995;&#32479;&#26816;&#32034;&#20854;&#20182;&#25991;&#20214;&#20013;&#26368;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#30340;&#33021;&#21147;&#12289;&#20351;&#29992;&#36328;&#25991;&#20214;&#21644;&#25991;&#20214;&#20869;&#19978;&#19979;&#25991;&#39044;&#27979;&#19979;&#19968;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#20197;&#21450;&#22788;&#29702;&#38656;&#35201;&#26816;&#32034;&#21644;&#19979;&#19968;&#34892;&#39044;&#27979;&#32452;&#21512;&#30340;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;RepoBench&#26088;&#22312;&#20419;&#36827;&#23545;&#24615;&#33021;&#30340;&#26356;&#20840;&#38754;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.01095</link><description>&lt;p&gt;
&#22823;&#25209;&#37327;&#31070;&#32463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20840;&#23616;&#20248;&#21270;&#40657;&#30418;&#39640;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#40664;&#35748;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#23427;&#22312;&#22788;&#29702;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#19987;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20195;&#29702;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22823;&#25209;&#37327;&#25968;&#25454;&#65292;&#24314;&#27169;&#22797;&#26434;&#38382;&#39064;&#20197;&#21450;&#20135;&#29983;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;NSGA-II&#30340;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#12290;&#36825;&#31181;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#31574;&#30053;&#20419;&#36827;&#20102;&#26410;&#21208;&#25506;&#21306;&#22495;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#22312;&#25968;&#25454;&#23494;&#38598;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#20998;&#36776;&#29575;&#30340;&#37327;&#21270;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992; langevin &#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#22122;&#22768;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#26080;&#38656;&#28155;&#21152;&#22122;&#22768;&#25110;&#35843;&#25972;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644; ResNet-50 &#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18864</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#22686;&#20998;&#36776;&#29575;&#30340;&#37327;&#21270;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution. (arXiv:2305.18864v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#20998;&#36776;&#29575;&#30340;&#37327;&#21270;&#38543;&#26426;&#26799;&#24230; langevin &#21160;&#21147;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992; langevin &#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#22122;&#22768;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#26080;&#38656;&#28155;&#21152;&#22122;&#22768;&#25110;&#35843;&#25972;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644; ResNet-50 &#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; langevin &#25110; levy &#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#38543;&#26426;&#23398;&#20064;&#21160;&#21147;&#23398;&#36890;&#36807;&#25913;&#21464;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#25110;&#30452;&#25509;&#27880;&#20837;&#22122;&#22768;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#22122;&#22768;&#30340;&#26041;&#24046;&#12290;&#30001;&#20110;&#22122;&#22768;&#26041;&#24046;&#20250;&#24433;&#21709;&#36817;&#20284;&#24615;&#33021;&#65292;&#22312;&#22522;&#20110; SDE &#30340;&#23398;&#20064;&#21644;&#23454;&#38469;&#23454;&#29616;&#20013;&#65292;&#28155;&#21152;&#22122;&#22768;&#30340;&#35774;&#35745;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#20248;&#21270;&#30340;&#38543;&#26426;&#19979;&#38477;&#23398;&#20064;&#26041;&#31243;&#65292;&#37319;&#29992;&#38543;&#26426;&#20998;&#26512;&#30340;&#35270;&#35282;&#65292;&#29992;&#20110;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#21033;&#29992; langevin SDE &#21160;&#21147;&#23398;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#21487;&#25511;&#22122;&#22768;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#22122;&#22768;&#25110;&#35843;&#25972;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545; vanilla &#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#21644; ResNet-50 &#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic learning dynamics based on Langevin or Levy stochastic differential equations (SDEs) in deep neural networks control the variance of noise by varying the size of the mini-batch or directly those of injecting noise. Since the noise variance affects the approximation performance, the design of the additive noise is significant in SDE-based learning and practical implementation. In this paper, we propose an alternative stochastic descent learning equation based on quantized optimization for non-convex objective functions, adopting a stochastic analysis perspective. The proposed method employs a quantized optimization approach that utilizes Langevin SDE dynamics, allowing for controllable noise with an identical distribution without the need for additive noise or adjusting the mini-batch size. Numerical experiments demonstrate the effectiveness of the proposed algorithm on vanilla convolution neural network(CNN) models and the ResNet-50 architecture across various data sets. Fur
&lt;/p&gt;</description></item><item><title>DNA-GPT&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;Divergent N-Gram&#20998;&#26512;&#26469;&#21457;&#29616;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.17359</link><description>&lt;p&gt;
DNA-GPT: &#26080;&#38656;&#35757;&#32451;&#30340;Divergent N-Gram&#20998;&#26512;&#29992;&#20110;&#26816;&#27979;GPT&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. (arXiv:2305.17359v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17359
&lt;/p&gt;
&lt;p&gt;
DNA-GPT&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;Divergent N-Gram&#20998;&#26512;&#26469;&#21457;&#29616;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#30528;&#25552;&#39640;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#20063;&#32473;&#26816;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#26469;&#28304;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#24182;&#19988;&#30446;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#30740;&#31350;&#28382;&#21518;&#20110;LLMs&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#26041;&#27861;&#22312;&#28789;&#27963;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26816;&#27979;&#31574;&#30053;&#65292;&#31216;&#20026;Divergent N-Gram&#20998;&#26512;&#65288;DNA-GPT&#65289;&#12290;&#32473;&#23450;&#19968;&#27573;&#25991;&#26412;&#65292;&#25105;&#20204;&#39318;&#20808;&#25130;&#26029;&#20854;&#20013;&#38388;&#37096;&#20998;&#65292;&#28982;&#21518;&#20165;&#20351;&#29992;&#21069;&#38754;&#30340;&#37096;&#20998;&#20316;&#20026;&#36755;&#20837;&#26469;&#37325;&#26032;&#29983;&#25104;&#21097;&#19979;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#22312;&#40657;&#30418;&#20013;&#36827;&#34892;N-gram&#20998;&#26512;&#25110;&#22312;&#30333;&#30418;&#20013;&#36827;&#34892;&#27010;&#29575;&#20998;&#24067;&#24046;&#24322;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#24067;&#19982;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text. We conducted
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.13650</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#22343;&#34913;&#25968;&#25454;&#30340;&#40065;&#26834;&#22522;&#20110;&#27169;&#22411;&#30340;&#35774;&#35745;&#30340;&#23646;&#24615;&#24341;&#23548;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Property-Guided Generative Modelling for Robust Model-Based Design with Imbalanced Data. (arXiv:2305.13650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#25506;&#32034;&#20855;&#26377;&#26497;&#24230;&#31232;&#30095;&#30340;&#26377;&#24847;&#20041;&#21306;&#22495;&#30340;&#39640;&#32500;&#34507;&#30333;&#36136;&#24207;&#21015;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#27169;&#22411;&#20248;&#21270;&#65288;MBO&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#20351;&#29992;&#30001;&#24207;&#21015;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#24341;&#23548;&#30340;&#26377;&#25928;&#25628;&#32034;&#27169;&#22411;&#26469;&#36741;&#21161;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#19981;&#24179;&#34913;&#24615;&#20351;&#24471;&#29616;&#26377;&#30340;MBO&#26041;&#27861;&#24456;&#38590;&#25110;&#26681;&#26412;&#26080;&#27861;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#20854;&#28508;&#22312;&#31354;&#38388;&#30001;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#25353;&#29031;&#36825;&#20123;&#23646;&#24615;&#20540;&#20248;&#20808;&#32771;&#34385;&#26679;&#26412;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#21644;&#21322;&#21512;&#25104;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MBO&#19982;PGVAE&#31283;&#20581;&#22320;&#21457;&#29616;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36830;&#32493;&#35774;&#35745;&#31354;&#38388;&#30340;&#26222;&#36866;&#24615;&#21450;&#20854;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of designing protein sequences with desired properties is challenging, as it requires to explore a high-dimensional protein sequence space with extremely sparse meaningful regions. This has led to the development of model-based optimization (MBO) techniques that aid in the design, by using effective search models guided by the properties over the sequence space. However, the intrinsic imbalanced nature of experimentally derived datasets causes existing MBO approaches to struggle or outright fail. We propose a property-guided variational auto-encoder (PGVAE) whose latent space is explicitly structured by the property values such that samples are prioritized according to these properties. Through extensive benchmarking on real and semi-synthetic protein datasets, we demonstrate that MBO with PGVAE robustly finds sequences with improved properties despite significant dataset imbalances. We further showcase the generality of our approach to continuous design spaces, and its rob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11244</link><description>&lt;p&gt;
&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#65288;PEL&#65289;&#25216;&#26415;&#65292;&#20197;&#37325;&#26032;&#21033;&#29992;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65288;GSM&#65289;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65288;ADI&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#23558;GSM&#36866;&#24212;&#20110;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;vanilla fine-tuning&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#65292;&#20351;&#29992;&#39069;&#22806;2.5&#65285;&#30340;&#32593;&#32476;&#21487;&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;fine-tuning&#31934;&#24230;&#30340;1.86&#65285;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#35782;&#21035;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). This is challenging due to the high variation in vocabulary and pronunciation among the numerous regional dialects. We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open sou
&lt;/p&gt;</description></item><item><title>PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.11203</link><description>&lt;p&gt;
PDP&#65306;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#21363;&#21487;&#25630;&#23450;
&lt;/p&gt;
&lt;p&gt;
PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11203
&lt;/p&gt;
&lt;p&gt;
PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#21098;&#26525;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24182;&#26368;&#23567;&#21270;DNN&#21152;&#36895;&#22120;&#19978;&#30340;&#21151;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#12289;&#26114;&#36149;&#25110;&#26080;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;/&#35821;&#35328;&#20219;&#21153;&#12289;DNN&#20307;&#31995;&#32467;&#26500;&#24182;&#36981;&#23432;&#32467;&#26500;&#21270;&#21098;&#26525;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#26102;&#38388;&#21098;&#26525;&#26041;&#26696;&#8212;&#8212;PDP&#65288;&#21442;&#25968;&#33258;&#30001;&#21487;&#24494;&#21098;&#26525;&#65289;&#65292;&#23427;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;PDP&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#30340;&#21160;&#24577;&#20989;&#25968;&#65292;&#20197;&#21442;&#25968;&#26080;&#20851;&#30340;&#26041;&#24335;&#20026;&#32473;&#23450;&#30340;&#21098;&#26525;&#30446;&#26631;&#29983;&#25104;&#36719;&#21098;&#26525;&#25513;&#30721;&#12290;&#34429;&#28982;&#26159;&#21487;&#24494;&#30340;&#65292;&#20294;&#26159;PDP&#30340;&#31616;&#21333;&#21644;&#39640;&#25928;&#20351;&#20854;&#36275;&#22815;&#26222;&#36941;&#65292;&#20197;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;/&#32467;&#26500;&#21270;/&#36890;&#36947;&#21098;&#26525;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;MobileNet-v1&#65292;PDP&#21487;&#20197;&#22312;86.6%&#30340;&#31232;&#30095;&#24230;&#19979;&#36798;&#21040;68.2%&#30340;ImageNet1k top-1&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03048</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#19968;&#27425;&#24615;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#25512;&#21160;&#19979;&#65292;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#27169;&#22411;&#65288;SAM&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#38761;&#26032;&#20102;&#20998;&#21106;&#27169;&#22411;&#39046;&#22495;&#12290;&#23613;&#31649;SAM&#38750;&#24120;&#36890;&#29992;&#65292;&#20294;&#33258;&#21160;&#20026;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#23450;&#21046;SAM&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#25552;&#31034;&#65292;&#22914;&#22312;&#19981;&#21516;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#20320;&#30340;&#23456;&#29289;&#29399;&#31561;&#65292; &#36824;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;PerSAM&#12290;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#65292;PerSAM&#39318;&#20808;&#36890;&#36807;&#20301;&#32622;&#20808;&#39564;&#23450;&#20301;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#26469;&#22312;&#20854;&#20182;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#20998;&#21106;&#23427;&#65306;&#30446;&#26631;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#30446;&#26631;&#35821;&#20041;&#25552;&#31034;&#21644;&#32423;&#32852;&#21518;&#22788;&#29702;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;SAM&#30340;&#31169;&#20154;&#20351;&#29992;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32531;&#35299;&#25513;&#27169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;&#65292;&#21363;PerSAM-F&#12290;&#20923;&#32467;&#25972;&#20010;SAM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21487;&#23398;&#20064;&#26435;&#37325;&#29992;&#20110;&#22810;&#23610;&#24230;&#25513;&#27169;&#65292;&#20165;&#35757;&#32451;2&#20010;&#21442;&#25968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05727</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#39044;&#38450;&#24615;&#20462;&#21098;Clever Hans&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#24050;&#25104;&#20026;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#19982;&#29992;&#25143;&#30340;&#39046;&#22495;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65288;&#20363;&#22914;Clever Hans&#25928;&#24212;&#65289;&#20063;&#34987;&#35748;&#20026;&#26159;&#25913;&#36827;&#38169;&#35823;&#27169;&#22411;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#35299;&#37322;&#36798;&#25104;&#19968;&#33268;&#26102;&#65292;&#35201;&#24590;&#20040;&#20570;&#23601;&#19981;&#37027;&#20040;&#28165;&#26970;&#20102;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#29992;&#25143;&#25509;&#21463;&#35299;&#37322;&#24182;&#19981;&#20445;&#35777;ML&#27169;&#22411;&#30340;&#33391;&#22909;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#19968;&#20123;&#38544;&#34255;&#30340;Clever Hans&#25928;&#24212;&#21487;&#33021;&#20173;&#28982;&#26410;&#34987;&#21457;&#29616;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#36129;&#29486;&#19968;&#20010;&#26032;&#26041;&#27861;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#12290;&#33258;&#28982;&#30011;&#20687;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#27169;&#22411;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;&#30340;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#22240;&#27492;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a ML model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that premptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#20316;&#20026;&#19968;&#31181;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.17841</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Generative Probabilistic Model for Weak Supervised Learning. (arXiv:2303.17841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20934;&#29983;&#25104;&#24615;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#29983;&#25104;&#20266;&#26631;&#31614;&#20316;&#20026;&#19968;&#31181;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#30456;&#20851;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#23454;&#36341;&#32773;&#26469;&#35828;&#26159;&#19968;&#20010;&#20027;&#35201; bottleneck&#12290;&#32780;&#19988;&#65292;&#20026;&#20102;&#35299;&#20915;&#37326;&#24515;&#21187;&#21187;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#38382;&#39064;&#65292;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#38468;&#24102;&#24102;&#26377;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#26631;&#31614;&#65292;&#20197;&#20415;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25163;&#21160;&#26631;&#35760;&#20855;&#26377;&#39640;&#36136;&#37327;&#26631;&#31614;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24448;&#24448;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#29942;&#39048;&#12290;&#24369;&#30417;&#30563;&#23398;&#20064; (WSL) &#26041;&#27861;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#36890;&#36807;&#26681;&#25454;&#21551;&#21457;&#24335;&#12289;&#36828;&#31243;&#30417;&#35270;&#21644;&#30693;&#35782;&#24211;&#26469;&#36171;&#20104;&#26410;&#26631;&#35760;&#25968;&#25454;&#22823;&#32422;&#26631;&#31614; (&#20266;&#26631;&#31614;) &#30340;&#33258;&#21160;&#26041;&#24335;&#65292;&#20174;&#32780;&#20943;&#36731;&#27880;&#37322;&#36127;&#25285;&#12290;&#25105;&#20204;&#24212;&#29992;&#27010;&#29575;&#29983;&#25104;&#38544;&#21464;&#37327;&#27169;&#22411; (PLVMs)&#65292;&#22312;&#21551;&#21457;&#24335;&#26631;&#27880;&#34920;&#31034;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20316;&#20026;&#19968;&#31181;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#32463;&#27982;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; PLVMs &#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#22810;&#25165;&#22810;&#33402;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding relevant and high-quality datasets to train machine learning models is a major bottleneck for practitioners. Furthermore, to address ambitious real-world use-cases there is usually the requirement that the data come labelled with high-quality annotations that can facilitate the training of a supervised model. Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project. Weak Supervised Learning (WSL) approaches have been developed to alleviate the annotation burden by offering an automatic way of assigning approximate labels (pseudo-labels) to unlabelled data based on heuristics, distant supervision and knowledge bases. We apply probabilistic generative latent variable models (PLVMs), trained on heuristic labelling representations of the original dataset, as an accurate, fast and cost-effective way to generate pseudo-labels. We show that the PLVMs achieve state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#24179;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12959</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21435;&#32416;&#32544;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variantional autoencoder with decremental information bottleneck for disentanglement. (arXiv:2303.12959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#24179;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#21435;&#32416;&#32544;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#26435;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20043;&#21069;&#20165;&#22312;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#36880;&#27493;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25193;&#23637;&#20102;&#20449;&#24687;&#29942;&#39048;&#65292;&#20197;&#20174;&#21435;&#32416;&#32544;&#21040;&#37325;&#26500;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#29942;&#39048;&#20250;&#22833;&#21435;&#21435;&#32416;&#32544;&#30340;&#32422;&#26463;&#65292;&#23548;&#33268;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#20248;&#21270;&#19981;&#21516;&#23618;&#30340;&#22810;&#20010;&#30446;&#26631;&#65292;&#31216;&#20026;DeVAE&#12290;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#19981;&#21516;&#28508;&#22312;&#31354;&#38388;&#30340;&#20449;&#24687;&#29942;&#39048;&#65292;DeVAE &#24179;&#34913;&#20102;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#12290;&#30001;&#20110;&#20855;&#26377;&#22810;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;DeVAE &#20801;&#35768;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#20197;&#22312;&#20445;&#25345;&#21435;&#32416;&#32544;&#32422;&#26463;&#30340;&#21516;&#26102;&#20248;&#21270;&#37325;&#26500;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity. Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction. However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem. To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to optimize multiple objectives in different layers, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding info
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;-&#35270;&#35273;&#25552;&#31034;&#65288;TVP&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#20849;&#21516;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#19988;&#20351;&#29992;&#21482;&#26377;&#20302;&#22797;&#26434;&#24230;&#31232;&#30095;&#20108;&#32500;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#36328;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#23545;&#35805;&#25490;&#21517;&#65288;TDR&#65289;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#30417;&#30563;TVP&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#19988;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.04995</link><description>&lt;p&gt;
&#25991;&#26412;-&#35270;&#35273;&#25552;&#31034;&#29992;&#20110;&#39640;&#25928;&#30340;&#20108;&#32500;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Text-Visual Prompting for Efficient 2D Temporal Video Grounding. (arXiv:2303.04995v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;-&#35270;&#35273;&#25552;&#31034;&#65288;TVP&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#20849;&#21516;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#19988;&#20351;&#29992;&#21482;&#26377;&#20302;&#22797;&#26434;&#24230;&#31232;&#30095;&#20108;&#32500;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#36328;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#23545;&#35805;&#25490;&#21517;&#65288;TDR&#65289;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#30417;&#30563;TVP&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#19988;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#65288;TVG&#65289;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#39044;&#27979;&#25991;&#23383;&#25551;&#36848;&#30340;&#26102;&#21051;&#22312;&#38271;&#26102;&#38388;&#26410;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#30340;&#24320;&#22987;/&#32467;&#26463;&#26102;&#38388;&#28857;&#12290;&#21463;&#30410;&#20110;&#32454;&#31890;&#24230;&#30340;&#19977;&#32500;&#35270;&#35273;&#29305;&#24449;&#65292;TVG&#25216;&#26415;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#39640;&#22797;&#26434;&#24615;&#20351;&#24471;&#25552;&#21462;&#23494;&#38598;&#30340;&#19977;&#32500;&#35270;&#35273;&#29305;&#24449;&#26159;&#32791;&#26102;&#30340;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#38024;&#23545;&#39640;&#25928;&#30340;TVG&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;-&#35270;&#35273;&#25552;&#31034;&#65288;TVP&#65289;&#26694;&#26550;&#65292;&#24182;&#23558;&#20248;&#21270;&#30340;&#25200;&#21160;&#27169;&#24335;&#65288;&#31216;&#20026;&#8220;&#25552;&#31034;&#8221;&#65289;&#32467;&#21512;&#21040;TVG&#27169;&#22411;&#30340;&#35270;&#35273;&#36755;&#20837;&#21644;&#25991;&#26412;&#29305;&#24449;&#20013;&#12290;&#19982;&#19977;&#32500;CNN&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#20316;&#32773;&#34920;&#26126;TVP&#20801;&#35768;&#25105;&#20204;&#22312;&#20108;&#32500;TVG&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#20849;&#21516;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#21482;&#26377;&#20302;&#22797;&#26434;&#24230;&#31232;&#30095;&#20108;&#32500;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#36328;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#23545;&#35805;&#25490;&#21517;&#65288;TDR&#65289;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#30417;&#30563;TVP&#30340;&#23398;&#20064;&#24182;&#20419;&#36827;&#35270;&#39057;&#27573;&#19982;&#30456;&#24212;&#25991;&#26412;&#26597;&#35810;&#30340;&#23545;&#40784;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;TVP&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call 'prompts') into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#27934;&#23519;&#21147;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21160;&#24577;&#21644;&#34892;&#20026;&#29305;&#24449;&#19978;&#23494;&#20999;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#27934;&#23519;&#21147;&#65292;&#34920;&#29616;&#20986;&#27934;&#23519;&#21147;&#30340;&#24310;&#36831;&#12289;&#31361;&#28982;&#24615;&#21644;&#36873;&#25321;&#24615;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2302.11351</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#20154;&#31867;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Regularised neural networks mimic human insight. (arXiv:2302.11351v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#27934;&#23519;&#21147;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21160;&#24577;&#21644;&#34892;&#20026;&#29305;&#24449;&#19978;&#23494;&#20999;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#27934;&#23519;&#21147;&#65292;&#34920;&#29616;&#20986;&#27934;&#23519;&#21147;&#30340;&#24310;&#36831;&#12289;&#31361;&#28982;&#24615;&#21644;&#36873;&#25321;&#24615;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#26102;&#20250;&#23637;&#29616;&#20986;&#31361;&#28982;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#30340;&#24773;&#20917;&#65292;&#36825;&#19982;&#27934;&#23519;&#21147;&#26102;&#21051;&#30456;&#20851;&#12290;&#36825;&#31181;&#27934;&#23519;&#21147;&#30456;&#20851;&#30340;&#24615;&#33021;&#25552;&#21319;&#20284;&#20046;&#24456;&#29305;&#27530;&#65292;&#22240;&#20026;&#23427;&#20204;&#21069;&#38754;&#26377;&#19968;&#20010;&#36739;&#38271;&#26102;&#38388;&#30340;&#20725;&#23616;&#65292;&#21464;&#21270;&#24322;&#24120;&#31361;&#28982;&#65292;&#24182;&#19988;&#21482;&#21457;&#29983;&#22312;&#19968;&#37096;&#20998;&#23398;&#20064;&#32773;&#36523;&#19978;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#26159;&#21542;&#20063;&#23384;&#22312;&#31867;&#20284;&#27934;&#23519;&#21147;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#20915;&#31574;&#20219;&#21153;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#38544;&#34255;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#20542;&#21521;&#20110;&#36890;&#36807;&#27934;&#23519;&#21147;&#21457;&#29616;&#36825;&#31181;&#35268;&#24459;&#65292;&#32780;&#19981;&#26159;&#36880;&#28176;&#21457;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24102;&#26377;&#27491;&#21017;&#21270;&#38376;&#25511;&#35843;&#33410;&#30340;&#31070;&#32463;&#32593;&#32476;&#32039;&#23494;&#27169;&#20223;&#20102;&#20154;&#31867;&#27934;&#23519;&#21147;&#30340;&#34892;&#20026;&#29305;&#24449;&#65292;&#34920;&#29616;&#20986;&#27934;&#23519;&#21147;&#30340;&#24310;&#36831;&#12289;&#31361;&#28982;&#24615;&#21644;&#36873;&#25321;&#24615;&#21457;&#29983;&#12290;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27934;&#23519;&#21147;&#34892;&#20026;&#20851;&#38190;&#22320;&#21462;&#20915;&#20110;&#22122;&#22768;&#28155;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans sometimes show sudden improvements in task performance that have been linked to moments of insight. Such insight-related performance improvements appear special because they are preceded by an extended period of impasse, are unusually abrupt, and occur only in some, but not all, learners. Here, we ask whether insight-like behaviour also occurs in artificial neural networks trained with gradient descent algorithms. We compared learning dynamics in humans and regularised neural networks in a perceptual decision task that provided a hidden opportunity which allowed to solve the task more efficiently. We show that humans tend to discover this regularity through insight, rather than gradually. Notably, neural networks with regularised gate modulation closely mimicked behavioural characteristics of human insights, exhibiting delay of insight, suddenness and selective occurrence. Analyses of network learning dynamics revealed that insight-like behaviour crucially depended on noise adde
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;XAI&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05624</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#30340;&#20855;&#26377;XAI&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to generate datasets with XAI ground truth to evaluate image models. (arXiv:2302.05624v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;XAI&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#27169;&#22411;&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#38656;&#27714;&#25512;&#21160;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26032;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#29702;&#35770;&#19978;&#30830;&#23450;AI&#20915;&#31574;&#30340;&#21407;&#22240;&#12290;XAI&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#39564;&#35777;&#35813;&#39046;&#22495;&#30340;&#24037;&#20316;&#65292;&#32771;&#34385;&#21040;&#32570;&#20047;&#22522;&#20934;&#65288;GT&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#24102;&#26377;GT&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#23558;&#25105;&#20204;&#30340;GT&#19982;&#30495;&#23454;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#33719;&#24471;&#20102;&#21331;&#36234;&#30340;&#32467;&#26524;&#65292;&#35777;&#23454;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increased usage of artificial intelligence (AI), it is imperative to understand how these models work internally. These needs have led to the development of a new field called eXplainable artificial intelligence (XAI). This field consists of on a set of techniques that allows us to theoretically determine the cause of the AI decisions. One main issue of XAI is how to verify the works on this field, taking into consideration the lack of ground truth (GT). In this study, we propose a new method to generate datasets with GT. We conducted a set of experiments that compared our GT with real model explanations and obtained excellent results confirming that our proposed method is correct.
&lt;/p&gt;</description></item><item><title>RayNet&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#20223;&#30495;&#24179;&#21488;&#65292;&#29992;&#20110;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#21327;&#35758;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#26377;&#24207;&#22320;&#36827;&#34892;&#21327;&#35758;&#30340;&#24320;&#21457;&#65292;&#32780;&#26080;&#38656;&#20851;&#27880;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2302.04519</link><description>&lt;p&gt;
RayNet: &#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#32593;&#32476;&#21327;&#35758;&#30340;&#20223;&#30495;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
RayNet: A Simulation Platform for Developing Reinforcement Learning-Driven Network Protocols. (arXiv:2302.04519v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04519
&lt;/p&gt;
&lt;p&gt;
RayNet&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#20223;&#30495;&#24179;&#21488;&#65292;&#29992;&#20110;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#21327;&#35758;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#26377;&#24207;&#22320;&#36827;&#34892;&#21327;&#35758;&#30340;&#24320;&#21457;&#65292;&#32780;&#26080;&#38656;&#20851;&#27880;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#32593;&#32476;&#21327;&#35758;&#30340;&#24320;&#21457;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21327;&#35758;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#30740;&#31350;&#26469;&#26500;&#24314;&#21487;&#37096;&#32626;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#21327;&#35758;&#26159;&#19968;&#20010;&#22797;&#26434;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#65292;&#28041;&#21450;&#22810;&#20010;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#38656;&#35201;&#22312;&#30495;&#23454;&#21644;&#27169;&#25311;&#30340;&#32593;&#32476;&#25299;&#25169;&#20013;&#36827;&#34892;&#37325;&#35201;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#32593;&#32476;&#27169;&#25311;&#22120;&#20026;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21327;&#35758;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#29615;&#22659;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#21487;&#20197;&#24182;&#34892;&#36816;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#20223;&#30495;&#24179;&#21488;RayNet&#65292;&#29992;&#20110;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#21327;&#35758;&#12290;RayNet&#23558;&#20840;&#38754;&#21487;&#32534;&#31243;&#30340;&#32593;&#32476;&#27169;&#25311;&#22120;OMNeT++&#19982;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#24179;&#21488;Ray/RLlib&#36827;&#34892;&#20102;&#25972;&#21512;&#12290;RayNet&#26088;&#22312;&#26377;&#24207;&#22320;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#21327;&#35758;&#65292;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#19987;&#27880;&#20110;&#38382;&#39064;&#26412;&#36523;&#32780;&#19981;&#26159;&#23454;&#29616;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has gained significant momentum in the development of network protocols. However, RL-based protocols are still in their infancy, and substantial research is required to build deployable solutions. Developing a protocol based on RL is a complex and challenging process that involves several model design decisions and requires significant training and evaluation in real and simulated network topologies. Network simulators offer an efficient training environment for RL-based protocols, because they are deterministic and can run in parallel. In this paper, we introduce \textit{RayNet}, a scalable and adaptable simulation platform for the development of RL-based network protocols. RayNet integrates OMNeT++, a fully programmable network simulator, with Ray/RLlib, a scalable training platform for distributed RL. RayNet facilitates the methodical development of RL-based network protocols so that researchers can focus on the problem at hand and not on implementation d
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.02173</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02173
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#39057;&#29575;&#21464;&#25442;&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#24615;&#21644;&#20840;&#23616;&#35270;&#35282;&#65292;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#34987;&#36805;&#36895;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23637;&#31034;&#20102;&#39057;&#29575;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31995;&#32479;&#22238;&#39038;&#21644;&#28145;&#20837;&#20998;&#26512;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#39057;&#29575;&#21464;&#25442;&#21487;&#20197;&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#23427;&#22312;&#35813;&#39046;&#22495;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20027;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#35299;&#37322;&#27010;&#24565;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#31616;&#27905;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#20415;&#20110;&#38750;&#19987;&#23478;&#29702;&#35299;&#21644;&#37319;&#21462;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.05109</link><description>&lt;p&gt;
&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#35299;&#37322;$\mathcal{ELH}$&#27010;&#24565;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explaining $\mathcal{ELH}$ Concept Descriptions through Counterfactual Reasoning. (arXiv:2301.05109v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#35299;&#37322;&#27010;&#24565;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#31616;&#27905;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#20415;&#20110;&#38750;&#19987;&#23478;&#29702;&#35299;&#21644;&#37319;&#21462;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20449;&#24687;&#31649;&#29702;&#65292;&#33021;&#22815;&#25903;&#25345;&#39640;&#24433;&#21709;&#21147;&#30340;&#24212;&#29992;&#31243;&#24207;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#23427;&#20204;&#20063;&#20316;&#20026;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#21307;&#30103;&#35786;&#26029;&#21644;&#20449;&#29992;&#35780;&#20998;&#12290;&#30001;&#20110;&#21463;&#21040;&#36825;&#20123;&#20915;&#31574;&#24433;&#21709;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#24076;&#26395;&#20102;&#35299;&#20182;&#20204;&#30340;&#24773;&#20917;&#24182;&#39564;&#35777;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#37322;&#26041;&#27861;&#12290;&#25551;&#36848;&#36923;&#36753;&#20013;&#20351;&#29992;&#27010;&#24565;&#26469;&#36827;&#34892;&#20998;&#31867;&#26159;&#19968;&#31181;&#22266;&#26377;&#36879;&#26126;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#21475;&#22836;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#27010;&#24565;&#20063;&#20250;&#21464;&#24471;&#20887;&#38271;&#19988;&#38590;&#20197;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#19987;&#23478;&#32780;&#35328;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#26159;&#20351;&#29992;&#21453;&#20107;&#23454;&#26469;&#22238;&#31572;&#38382;&#39064;&#65306;&#8220;&#20026;&#20102;&#24471;&#21040;&#19981;&#21516;&#30340;&#20998;&#31867;&#65292;&#29305;&#24449;&#20540;&#24212;&#22914;&#20309;&#25913;&#21464;&#65311;&#8221;&#36890;&#36807;&#20851;&#27880;&#26368;&#23567;&#30340;&#29305;&#24449;&#21464;&#21270;&#65292;&#35299;&#37322;&#21464;&#24471;&#30701;&#23567;&#12289;&#26131;&#20110;&#29702;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21464;&#21270;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#30340;&#26126;&#30830;&#34892;&#21160;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge bases are widely used for information management, enabling high-impact applications such as web search, question answering, and natural language processing. They also serve as the backbone for automatic decision systems, e.g., for medical diagnostics and credit scoring. As stakeholders affected by these decisions would like to understand their situation and verify how fair the decisions are, a number of explanation approaches have been proposed. An intrinsically transparent way to do classification is by using concepts in description logics. However, these concepts can become long and difficult to fathom for non-experts, even when verbalized. One solution is to employ counterfactuals to answer the question, ``How must feature values be changed to obtain a different classification?'' By focusing on the minimal feature changes, the explanations are short, human-friendly, and provide a clear path of action regarding the change in prediction. While previous work investigated coun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PuzzleFusion"&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31354;&#38388;&#25340;&#22270;&#21644;&#25151;&#38388;&#24067;&#23616;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;&#31616;&#21333;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31354;&#38388;&#25340;&#22270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.13785</link><description>&lt;p&gt;
PuzzleFusion&#65306;&#37322;&#25918;&#25193;&#25955;&#27169;&#22411;&#22312;&#31354;&#38388;&#25340;&#22270;&#35299;&#20915;&#20013;&#30340;&#23041;&#21147;
&lt;/p&gt;
&lt;p&gt;
PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving. (arXiv:2211.13785v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PuzzleFusion"&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31354;&#38388;&#25340;&#22270;&#21644;&#25151;&#38388;&#24067;&#23616;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;&#31616;&#21333;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31354;&#38388;&#25340;&#22270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31354;&#38388;&#25340;&#22270;&#21644;&#25151;&#38388;&#24067;&#23616;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;"PuzzleFusion"&#38024;&#23545;&#21518;&#32773;&#20219;&#21153;&#65292;&#23558;&#19968;&#32452;&#25151;&#38388;&#24067;&#23616;&#35270;&#20026;&#20463;&#35270;&#22270;&#20013;&#30340;&#22810;&#36793;&#24418;&#26354;&#32447;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#23427;&#20204;&#30340;&#20108;&#32500;&#24179;&#31227;&#21644;&#26059;&#36716;&#26469;&#23545;&#40784;&#25151;&#38388;&#24067;&#23616;&#22359;&#65292;&#31867;&#20284;&#20110;&#35299;&#20915;&#31354;&#38388;&#25340;&#22270;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#26159;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#31616;&#21333;&#20351;&#29992;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31354;&#38388;&#25340;&#22270;&#20219;&#21153;&#35299;&#20915;&#20026;&#26465;&#20214;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#31471;&#21040;&#31471;&#31070;&#32463;&#31995;&#32479;&#30340;&#23398;&#20064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26377;&#22320;&#38754;&#23454;&#20917;&#23433;&#25490;&#30340;&#25968;&#25454;&#38598;&#65306;1&#65289;2D Voronoi &#25340;&#22270;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807; 2D &#28857;&#38598;&#30340; Voronoi &#22270;&#29983;&#25104;&#25340;&#22270;&#22359;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65307;&#20197;&#21450;2&#65289;MagicPlan &#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174; MagicPlan &#29983;&#20135;&#32447;&#25552;&#20379;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25340;&#22270;&#22359;&#26159;&#30001;&#22686;&#24378;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#30340;&#25151;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an end-to-end neural architecture based on Diffusion Models for spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks. In the latter task, for instance, the proposed system ``PuzzleFusion'' takes a set of room layouts as polygonal curves in the top-down view and aligns the room layout pieces by estimating their 2D translations and rotations, akin to solving the jigsaw puzzle of room layouts. A surprising discovery of the paper is that the simple use of a Diffusion Model effectively solves these challenging spatial puzzle tasks as a conditional generation process. To enable learning of an end-to-end neural system, the paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi jigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram of 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from its production pipeline, where pieces are room layouts constructed by augmented reality App by rea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24322;&#27493;&#26356;&#26032;&#19979;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31227;&#38500;&#21516;&#27493;&#36890;&#20449;&#20551;&#35774;&#21644;&#21435;&#38500;&#26799;&#24230;&#33539;&#25968;&#26377;&#30028;&#24615;&#20551;&#35774;&#26469;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.01176</link><description>&lt;p&gt;
PersA-FL&#65306;&#20010;&#24615;&#21270;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PersA-FL: Personalized Asynchronous Federated Learning. (arXiv:2210.01176v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24322;&#27493;&#26356;&#26032;&#19979;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31227;&#38500;&#21516;&#27493;&#36890;&#20449;&#20551;&#35774;&#21644;&#21435;&#38500;&#26799;&#24230;&#33539;&#25968;&#26377;&#30028;&#24615;&#20551;&#35774;&#26469;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24322;&#27493;&#26356;&#26032;&#19979;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#24076;&#26395;&#33719;&#24471;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#33021;&#22815;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#21644;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#20010;&#24615;&#21270;&#26694;&#26550;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#21644;&#65288;ii&#65289;Moreau&#21253;&#32476;&#65288;ME&#65289;&#12290;MAML&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#36866;&#24212;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#32852;&#21512;&#27169;&#22411;&#65292;&#32780;ME&#36890;&#36807;&#38544;&#24335;&#26799;&#24230;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26469;&#36890;&#36807;&#35268;&#33539;&#21270;&#25439;&#22833;&#23454;&#29616;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#23545;&#26377;&#30028;&#28382;&#21518;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#32479;&#19968;&#35777;&#26126;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MAML&#21644;ME&#20010;&#24615;&#21270;&#26694;&#26550;&#12290;&#38024;&#23545;&#24179;&#28369;&#21644;&#38750;&#20984;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#25152;&#30740;&#31350;&#30340;&#20989;&#25968;&#31867;&#65292;&#21435;&#38500;&#20102;&#26799;&#24230;&#33539;&#25968;&#30340;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the personalized federated learning problem under asynchronous updates. In this problem, each client seeks to obtain a personalized model that simultaneously outperforms local and global models. We consider two optimization-based frameworks for personalization: (i) Model-Agnostic Meta-Learning (MAML) and (ii) Moreau Envelope (ME). MAML involves learning a joint model adapted for each client through fine-tuning, whereas ME requires a bi-level optimization problem with implicit gradients to enforce personalization via regularized losses. We focus on improving the scalability of personalized federated learning by removing the synchronous communication assumption. Moreover, we extend the studied function class by removing boundedness assumptions on the gradient norm. Our main technical contribution is a unified proof for asynchronous federated learning with bounded staleness that we apply to MAML and ME personalization frameworks. For the smooth and non-convex functions class, we 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38750;&#35821;&#35328;&#32447;&#32034;&#36827;&#34892;&#20849;&#21516;&#20154;&#38469;&#20114;&#21160;&#20998;&#26512;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#20174;2010&#24180;&#20197;&#26469;&#30340;&#35745;&#31639;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#24120;&#29992;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#20197;&#21450;&#26377;&#20851;&#20114;&#21160;&#20998;&#26512;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2207.10574</link><description>&lt;p&gt;
&#21033;&#29992;&#38750;&#35821;&#35328;&#32447;&#32034;&#20998;&#26512;&#20849;&#21516;&#20154;&#38469;&#20114;&#21160;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A Survey. (arXiv:2207.10574v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10574
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38750;&#35821;&#35328;&#32447;&#32034;&#36827;&#34892;&#20849;&#21516;&#20154;&#38469;&#20114;&#21160;&#20998;&#26512;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#20174;2010&#24180;&#20197;&#26469;&#30340;&#35745;&#31639;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#24120;&#29992;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#20197;&#21450;&#26377;&#20851;&#20114;&#21160;&#20998;&#26512;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38750;&#35821;&#35328;&#27807;&#36890;&#20316;&#20026;&#31038;&#20250;&#21644;&#24515;&#29702;&#29616;&#35937;&#27979;&#37327;&#30340;&#35777;&#25454;&#65292;&#33258;&#21160;&#21270;&#30340;&#20849;&#21516;&#20154;&#38469;&#20114;&#21160;&#20998;&#26512;&#24471;&#21040;&#20102;&#35299;&#20915;&#12290;&#25105;&#20204;&#23545;&#20174;2010&#24180;&#20197;&#26469;&#30340;&#35745;&#31639;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#26816;&#27979;&#19982;&#31038;&#20132;&#29305;&#24449;&#65288;&#22914;&#39046;&#23548;&#21147;&#12289;&#25903;&#37197;&#21147;&#12289;&#20010;&#24615;&#29305;&#24449;&#65289;&#12289;&#31038;&#20132;&#35282;&#33394;/&#20851;&#31995;&#21644;&#20114;&#21160;&#21160;&#24577;&#65288;&#22914;&#22242;&#38431;&#20957;&#32858;&#21147;&#12289;&#21442;&#19982;&#24230;&#12289;&#34701;&#27965;&#24230;&#65289;&#30456;&#20851;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#23548;&#33268;&#26377;&#25928;&#24615;&#33021;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#36825;&#39033;&#35843;&#26597;&#22312;&#28041;&#21450;&#26368;&#24191;&#27867;&#30340;&#31038;&#20250;&#29616;&#35937;&#21644;&#20114;&#21160;&#22330;&#26223;&#65288;&#33258;&#30001;&#23545;&#35805;&#12289;&#20250;&#35758;&#12289;&#20108;&#20803;&#32452;&#21644;&#20154;&#32676;&#65289;&#26041;&#38754;&#19982;&#20854;&#21516;&#31867;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#24635;&#32467;&#65292;&#24182;&#27010;&#36848;&#20102;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#23454;&#26045;&#12289;&#25968;&#25454;&#38598;&#31574;&#21010;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20114;&#21160;&#20998;&#26512;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#19968;&#20123;&#37325;&#35201;&#35266;&#23519;&#32467;&#26524;&#26159;&#65306;&#26368;&#24120;&#29992;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#26159;&#20849;&#21516; ...
&lt;/p&gt;
&lt;p&gt;
Automated co-located human-human interaction analysis has been addressed by the use of nonverbal communication as measurable evidence of social and psychological phenomena. We survey the computing studies (since 2010) detecting phenomena related to social traits (e.g., leadership, dominance, personality traits), social roles/relations, and interaction dynamics (e.g., group cohesion, engagement, rapport). Our target is to identify the nonverbal cues and computational methodologies resulting in effective performance. This survey differs from its counterparts by involving the widest spectrum of social phenomena and interaction settings (free-standing conversations, meetings, dyads, and crowds). We also present a comprehensive summary of the related datasets and outline future research directions which are regarding the implementation of artificial intelligence, dataset curation, and privacy-preserving interaction analysis. Some major observations are: the most often used nonverbal cue, co
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26497;&#31471;&#27004;&#23618;&#24179;&#38754;&#22270;&#37325;&#24314;&#20219;&#21153;&#21644;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#35813;&#20219;&#21153;&#65292;&#36890;&#36807;Transformer&#35299;&#30721;&#22120;&#32423;&#32852;&#30340;&#26041;&#24335;&#26469;&#24187;&#21270;&#19981;&#21487;&#35265;&#30340;&#25151;&#38388;&#21644;&#38376;&#20197;&#37325;&#24314;&#25972;&#20010;&#27004;&#23618;&#24179;&#38754;&#22270;&#65292;&#24182;&#22312;701&#20010;&#25151;&#23627;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.00645</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#24187;&#21270;Transformer&#32423;&#32852;&#23454;&#29616;&#27004;&#23618;&#24179;&#38754;&#22270;&#30340;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Floorplan Restoration by Structure Hallucinating Transformer Cascades. (arXiv:2206.00645v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00645
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26497;&#31471;&#27004;&#23618;&#24179;&#38754;&#22270;&#37325;&#24314;&#20219;&#21153;&#21644;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#35813;&#20219;&#21153;&#65292;&#36890;&#36807;Transformer&#35299;&#30721;&#22120;&#32423;&#32852;&#30340;&#26041;&#24335;&#26469;&#24187;&#21270;&#19981;&#21487;&#35265;&#30340;&#25151;&#38388;&#21644;&#38376;&#20197;&#37325;&#24314;&#25972;&#20010;&#27004;&#23618;&#24179;&#38754;&#22270;&#65292;&#24182;&#22312;701&#20010;&#25151;&#23627;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26497;&#31471;&#30340;&#27004;&#23618;&#24179;&#38754;&#22270;&#37325;&#24314;&#20219;&#21153;&#65292;&#20026;&#35813;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#31070;&#32463;&#26550;&#26500;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;&#32473;&#23450;&#20174;&#20840;&#26223;&#22270;&#20687;&#20013;&#25512;&#26029;&#25110;&#31574;&#21010;&#30340;&#37096;&#20998;&#27004;&#23618;&#24179;&#38754;&#22270;&#37325;&#24314;&#65292;&#20219;&#21153;&#26159;&#37325;&#24314;&#21253;&#25324;&#19981;&#21487;&#35265;&#24314;&#31569;&#32467;&#26500;&#22312;&#20869;&#30340;&#23436;&#25972;&#27004;&#23618;&#24179;&#38754;&#22270;&#12290;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;1&#65289;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#23558;&#36755;&#20837;&#30340;&#37096;&#20998;&#27004;&#23618;&#24179;&#38754;&#22270;&#32534;&#30721;&#20026;&#19968;&#32452;&#28508;&#22312;&#21521;&#37327;&#65307;2&#65289;&#36890;&#36807;&#32423;&#32852;Transformer&#35299;&#30721;&#22120;&#65292;&#22312;&#24187;&#21270;&#19981;&#21487;&#35265;&#30340;&#25151;&#38388;&#21644;&#38376;&#30340;&#21516;&#26102;&#37325;&#24314;&#25972;&#20010;&#27004;&#23618;&#24179;&#38754;&#22270;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;701&#20010;&#25151;&#23627;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#25216;&#26415;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#23558;&#20998;&#20139;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an extreme floorplan reconstruction task, a new benchmark for the task, and a neural architecture as a solution. Given a partial floorplan reconstruction inferred or curated from panorama images, the task is to reconstruct a complete floorplan including invisible architectural structures. The proposed neural network 1) encodes an input partial floorplan into a set of latent vectors by convolutional neural networks and a Transformer; and 2) reconstructs an entire floorplan while hallucinating invisible rooms and doors by cascading Transformer decoders. Qualitative and quantitative evaluations demonstrate effectiveness of our approach over the benchmark of 701 houses, outperforming the state-of-the-art reconstruction techniques. We will share our code, models, and data.
&lt;/p&gt;</description></item><item><title>NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2110.14053</link><description>&lt;p&gt;
NeuroBack: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14053
&lt;/p&gt;
&lt;p&gt;
NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#21040;&#35268;&#21010;&#12289;&#39564;&#35777;&#21644;&#23433;&#20840;&#31561;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;NP&#23436;&#20840;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#31639;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22686;&#24378;CDCL SAT&#27714;&#35299;&#22120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#20351;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#36164;&#28304;&#36827;&#34892;&#39057;&#32321;&#30340;&#22312;&#32447;&#27169;&#22411;&#25512;&#26029;&#12290;&#20026;&#20102;&#20351;GNN&#30340;&#25913;&#36827;&#21464;&#24471;&#23454;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroBack&#30340;&#26041;&#27861;&#65292;&#23427;&#24314;&#31435;&#22312;&#20004;&#20010;&#27934;&#23519;&#19978;&#65306;&#65288;1&#65289;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#65288;&#29978;&#33267;&#20840;&#37096;&#65289;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65288;&#21363;&#20540;&#65289;&#23545;&#20110;CDCL SAT&#27714;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#65288;2&#65289;&#22312;SAT&#27714;&#35299;&#24320;&#22987;&#20043;&#21069;&#65292;&#21482;&#38656;&#26597;&#35810;&#19968;&#27425;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21363;&#21487;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#31163;&#32447;&#27169;&#22411;&#25512;&#26029;&#20351;NeuroBack&#33021;&#22815;&#20165;&#22312;CPU&#19978;&#25191;&#34892;&#65292;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;SR-HetGNN&#65292;&#36890;&#36807;&#23398;&#20064;&#20250;&#35805;&#23884;&#20837;&#24182;&#25429;&#25417;&#21311;&#21517;&#29992;&#25143;&#30340;&#29305;&#23450;&#20559;&#22909;&#65292;&#20197;&#25913;&#36827;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.05641</link><description>&lt;p&gt;
SR-HetGNN:&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural Network. (arXiv:2108.05641v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.05641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;SR-HetGNN&#65292;&#36890;&#36807;&#23398;&#20064;&#20250;&#35805;&#23884;&#20837;&#24182;&#25429;&#25417;&#21311;&#21517;&#29992;&#25143;&#30340;&#29305;&#23450;&#20559;&#22909;&#65292;&#20197;&#25913;&#36827;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#30446;&#30340;&#26159;&#26681;&#25454;&#20808;&#21069;&#30340;&#20250;&#35805;&#24207;&#21015;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#27425;&#28857;&#20987;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#26681;&#25454;&#29992;&#25143;&#20250;&#35805;&#24207;&#21015;&#20013;&#30340;&#39033;&#30446;&#36716;&#25442;&#26469;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#20250;&#35805;&#24207;&#21015;&#20013;&#30340;&#20854;&#20182;&#26377;&#25928;&#20449;&#24687;&#65292;&#22914;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#26080;&#27861;&#23398;&#20064;&#29992;&#25143;&#30340;&#20855;&#20307;&#20559;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SR-HetGNN&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HetGNN&#65289;&#23398;&#20064;&#20250;&#35805;&#23884;&#20837;&#65292;&#24182;&#25429;&#25417;&#21311;&#21517;&#29992;&#25143;&#30340;&#29305;&#23450;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SR-HetGNN&#39318;&#20808;&#26681;&#25454;&#20250;&#35805;&#24207;&#21015;&#26500;&#24314;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#33410;&#28857;&#30340;&#24322;&#26500;&#22270;&#65292;&#21487;&#20197;&#25429;&#25417;&#39033;&#30446;&#12289;&#29992;&#25143;&#21644;&#20250;&#35805;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;HetGNN&#25429;&#25417;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#36716;&#25442;&#24182;&#23398;&#20064;&#21253;&#21547;&#39033;&#30446;&#23884;&#20837;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of the Session-Based Recommendation System is to predict the user's next click according to the previous session sequence. The current studies generally learn user preferences according to the transitions of items in the user's session sequence. However, other effective information in the session sequence, such as user profiles, are largely ignored which may lead to the model unable to learn the user's specific preferences. In this paper, we propose a heterogeneous graph neural network-based session recommendation method, named SR-HetGNN, which can learn session embeddings by heterogeneous graph neural network (HetGNN), and capture the specific preferences of anonymous users. Specifically, SR-HetGNN first constructs heterogeneous graphs containing various types of nodes according to the session sequence, which can capture the dependencies among items, users, and sessions. Second, HetGNN captures the complex transitions between items and learns the item embeddings containing
&lt;/p&gt;</description></item><item><title>AKE-GNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30693;&#35782;&#20132;&#25442;&#31574;&#30053;&#22312;&#22810;&#20010;&#22270;&#35270;&#22270;&#20043;&#38388;&#20132;&#25442;&#36890;&#36947;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2106.05455</link><description>&lt;p&gt;
AKE-GNN: &#33258;&#36866;&#24212;&#30693;&#35782;&#20132;&#27969;&#30340;&#26377;&#25928;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AKE-GNN: Effective Graph Learning with Adaptive Knowledge Exchange. (arXiv:2106.05455v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05455
&lt;/p&gt;
&lt;p&gt;
AKE-GNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30693;&#35782;&#20132;&#25442;&#31574;&#30053;&#22312;&#22810;&#20010;&#22270;&#35270;&#22270;&#20043;&#38388;&#20132;&#25442;&#36890;&#36947;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#22270;&#25366;&#25496;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;GNN&#20013;&#23398;&#21040;&#30340;&#26435;&#37325;(&#36890;&#36947;)&#26159;&#39640;&#24230;&#20887;&#20313;&#30340;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#38480;&#21046;&#20102;GNN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#20026;&#20102;&#25928;&#29575;&#32771;&#34385;&#32780;&#31227;&#38500;&#36825;&#20123;&#20887;&#20313;&#36890;&#36947;&#65292;&#32780;&#26159;&#35797;&#22270;&#37325;&#26032;&#28608;&#27963;&#23427;&#20204;&#65292;&#20197;&#25193;&#22823;GNN&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AKE-GNN&#30340;&#26032;&#22411;GNN&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23545;&#22270;&#22686;&#24378;&#29983;&#25104;&#30340;&#22810;&#20010;&#22270;&#35270;&#22270;&#20043;&#38388;&#36827;&#34892;&#33258;&#36866;&#24212;&#30693;&#35782;&#20132;&#25442;&#31574;&#30053;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;AKE-GNN&#39318;&#20808;&#35757;&#32451;&#22810;&#20010;GNN&#65292;&#27599;&#20010;&#23545;&#24212;&#19968;&#20010;&#22270;&#35270;&#22270;&#65292;&#20197;&#33719;&#24471;&#20449;&#24687;&#36890;&#36947;&#12290;&#28982;&#21518;&#65292;AKE-GNN&#36845;&#20195;&#22320;&#20197;&#36880;&#23618;&#26041;&#24335;&#22312;&#19968;&#20010;GNN&#30340;&#26435;&#37325;&#21442;&#25968;&#30697;&#38453;&#20013;&#36827;&#34892;&#20887;&#20313;&#36890;&#36947;&#19982;&#21478;&#19968;&#20010;GNN&#30340;&#20449;&#24687;&#36890;&#36947;&#20043;&#38388;&#30340;&#20132;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have already been widely used in various graph mining tasks. However, recent works reveal that the learned weights (channels) in well-trained GNNs are highly redundant, which inevitably limits the performance of GNNs. Instead of removing these redundant channels for efficiency consideration, we aim to reactivate them to enlarge the representation capacity of GNNs for effective graph learning. In this paper, we propose to substitute these redundant channels with other informative channels to achieve this goal. We introduce a novel GNN learning framework named AKE-GNN, which performs the Adaptive Knowledge Exchange strategy among multiple graph views generated by graph augmentations. AKE-GNN first trains multiple GNNs each corresponding to one graph view to obtain informative channels. Then, AKE-GNN iteratively exchanges redundant channels in the weight parameter matrix of one GNN with informative channels of another GNN in a layer-wise manner. Additionally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wise-SrNet&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#29305;&#24449;&#22270;&#21644;&#20998;&#31867;&#23618;&#20043;&#38388;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2104.12294</link><description>&lt;p&gt;
Wise-SrNet: &#19968;&#31181;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Wise-SrNet: A Novel Architecture for Enhancing Image Classification by Learning Spatial Resolution of Feature Maps. (arXiv:2104.12294v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.12294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wise-SrNet&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#29305;&#24449;&#22270;&#21644;&#20998;&#31867;&#23618;&#20043;&#38388;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#20197;&#26469;&#65292;&#23558;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#19982;&#26368;&#32456;&#30340;&#20998;&#31867;&#23618;&#36830;&#25509;&#36215;&#26469;&#19968;&#30452;&#26159;&#20027;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;VGG&#27169;&#22411;&#20351;&#29992;&#20004;&#32452;&#20840;&#36830;&#25509;&#23618;&#29992;&#20110;&#26550;&#26500;&#30340;&#20998;&#31867;&#37096;&#20998;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#27169;&#22411;&#26435;&#37325;&#30340;&#25968;&#37327;&#12290;ResNet&#31561;&#28145;&#24230;&#21367;&#31215;&#27169;&#22411;&#20351;&#29992;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65288;GAP&#65289;&#23618;&#23558;&#29305;&#24449;&#22270;&#21387;&#32553;&#24182;&#36755;&#20837;&#20998;&#31867;&#23618;&#12290;&#23613;&#31649;&#20351;&#29992;GAP&#23618;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25439;&#22833;&#65292;&#20174;&#32780;&#38477;&#20302;&#23398;&#20064;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Wise-SrNet&#30340;&#26032;&#22411;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#21463;&#21040;&#20102;&#28145;&#24230;&#21367;&#31215;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#24182;&#19988;&#19987;&#20026;&#22788;&#29702;&#31354;&#38388;&#20998;&#36776;&#29575;&#32780;&#35774;&#35745;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;Intel&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
One of the main challenges since the advancement of convolutional neural networks is how to connect the extracted feature map to the final classification layer. VGG models used two sets of fully connected layers for the classification part of their architectures, which significantly increased the number of models' weights. ResNet and the next deep convolutional models used the Global Average Pooling (GAP) layer to compress the feature map and feed it to the classification layer. Although using the GAP layer reduces the computational cost, but also causes losing spatial resolution of the feature map, which results in decreasing learning efficiency. In this paper, we aim to tackle this problem by replacing the GAP layer with a new architecture called Wise-SrNet. It is inspired by the depthwise convolutional idea and is designed for processing spatial resolution while not increasing computational cost. We have evaluated our method using three different datasets: Intel Image Classification
&lt;/p&gt;</description></item></channel></rss>