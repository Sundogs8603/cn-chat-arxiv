<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>CFGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;&#65292;&#21253;&#25324;CFData&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21450;CFLLM&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#65292;CFAPP&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10654</link><description>&lt;p&gt;
CFGPT: &#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
CFGPT: Chinese Financial Assistant with Large Language Model. (arXiv:2309.10654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10654
&lt;/p&gt;
&lt;p&gt;
CFGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;&#65292;&#21253;&#25324;CFData&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21450;CFLLM&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#65292;CFAPP&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CFGPT&#30340;&#20013;&#22269;&#37329;&#34701;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#65288;CFData&#65289;&#65292;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#30340;&#37329;&#34701;LLM&#65288;CFLLM&#65289;&#65292;&#20197;&#21450;&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#30340;&#37096;&#32626;&#26694;&#26550;&#65288;CFAPP&#65289;&#12290;CFData&#21253;&#25324;&#19968;&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#27719;&#38598;&#20102;&#20013;&#22269;&#37329;&#34701;&#25968;&#25454;&#21644;&#20998;&#26512;&#65292;&#20197;&#21450;&#24635;&#20849;584M&#20010;&#25991;&#20214;&#21644;141B&#20010;&#26631;&#35760;&#30340;&#36739;&#23567;&#30340;&#36890;&#29992;&#25991;&#26412;&#23376;&#38598;&#65292;&#24182;&#19988;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#38024;&#23545;&#20845;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#20102;&#23450;&#21046;&#65292;&#20869;&#23481;&#28085;&#30422;&#20102;&#37329;&#34701;&#20998;&#26512;&#21644;&#20915;&#31574;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;1.5M&#20010;&#25351;&#20196;&#23545;&#21644;&#24635;&#35745;1.5B&#20010;&#26631;&#35760;&#12290;CFLLM&#22522;&#20110;InternLM-7B&#36827;&#34892;&#20102;&#24179;&#34913;&#27169;&#22411;&#33021;&#21147;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated great potential in natural language processing tasks within the financial domain. In this work, we present a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT, which includes a dataset~(CFData) for pre-training and supervised fine-tuning, a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment framework~(CFAPP) designed to navigate real-world financial applications. The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised fine-tuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decision-making with 1.5M instruction pairs and 1.5B tokens in total. The CFLLM, which is based on InternLM-7B to balance the model capabil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#33021;&#28304;&#28040;&#32791;&#20043;&#38388;&#26435;&#34913;&#30340;&#21487;&#25345;&#32493;&#24615;&#25351;&#26631;&#65292;&#38024;&#23545;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#38754;&#21521;&#33021;&#28304;&#24863;&#30693;&#30340;&#32852;&#21512;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.10645</link><description>&lt;p&gt;
&#38754;&#21521;&#33021;&#28304;&#24863;&#30693;&#30340;&#32852;&#21512;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#22312;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Energy-Aware Federated Traffic Prediction for Cellular Networks. (arXiv:2309.10645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#33021;&#28304;&#28040;&#32791;&#20043;&#38388;&#26435;&#34913;&#30340;&#21487;&#25345;&#32493;&#24615;&#25351;&#26631;&#65292;&#38024;&#23545;&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#38754;&#21521;&#33021;&#28304;&#24863;&#30693;&#30340;&#32852;&#21512;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34562;&#31389;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#39044;&#27979;&#23545;&#20110;&#20248;&#21270;&#31532;&#20116;&#20195;(5G)&#21450;&#26356;&#39640;&#29256;&#26412;&#30340;&#32593;&#32476;&#33267;&#20851;&#37325;&#35201;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#23545;&#20110;&#26234;&#33021;&#32593;&#32476;&#35774;&#35745;&#12289;&#36164;&#28304;&#20998;&#37197;&#21644;&#24322;&#24120;&#32531;&#35299;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;(ML)&#26159;&#19968;&#31181;&#26377;&#25928;&#39044;&#27979;&#32593;&#32476;&#27969;&#37327;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#38598;&#20013;&#23384;&#20648;&#22312;&#21333;&#20010;&#25968;&#25454;&#20013;&#24515;&#20013;&#65292;&#28041;&#21450;&#21040;&#26426;&#23494;&#24615;&#12289;&#38544;&#31169;&#21644;&#25968;&#25454;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#32852;&#21512;&#23398;&#20064;(FL)&#20316;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;ML&#35757;&#32451;&#26694;&#26550;&#24212;&#36816;&#32780;&#29983;&#65292;&#36890;&#36807;&#24182;&#34892;&#20998;&#24067;&#24335;&#35745;&#31639;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#29615;&#22659;&#24433;&#21709;&#24120;&#24120;&#34987;&#24573;&#35270;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20110;&#20854;&#21487;&#25345;&#32493;&#24615;&#30340;&#36136;&#30097;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#25345;&#32493;&#24615;&#25351;&#26631;&#65292;&#35299;&#20915;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#20934;&#30830;&#24615;&#21644;&#33021;&#28304;&#28040;&#32791;&#20043;&#38388;&#30340;&#25240;&#34935;&#38382;&#39064;&#65292;&#24182;&#23545;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular traffic prediction is a crucial activity for optimizing networks in fifth-generation (5G) networks and beyond, as accurate forecasting is essential for intelligent network design, resource allocation and anomaly mitigation. Although machine learning (ML) is a promising approach to effectively predict network traffic, the centralization of massive data in a single data center raises issues regarding confidentiality, privacy and data transfer demands. To address these challenges, federated learning (FL) emerges as an appealing ML training framework which offers high accurate predictions through parallel distributed computations. However, the environmental impact of these methods is often overlooked, which calls into question their sustainability. In this paper, we address the trade-off between accuracy and energy consumption in FL by proposing a novel sustainability indicator that allows assessing the feasibility of ML models. Then, we comprehensively evaluate state-of-the-art d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10639</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20840;&#23616;${\mathcal L}^2$&#26368;&#23567;&#21270;&#22120;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#65292;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#65292;${\mathcal L}^2$ Schatten&#31867;&#65288;&#25110;Hilbert-Schmidt&#65289;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#30456;&#31561;&#32500;&#24230;$Q\geq1$&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#12290;&#38544;&#34255;&#23618;&#20063;&#23450;&#20041;&#22312;${\mathbb R}^{Q}$&#30340;&#31354;&#38388;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#26368;&#26032;&#30340;&#20851;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#65292;&#22312;$L\geq Q$&#30340;&#24773;&#20917;&#19979;&#26500;&#36896;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#26063;&#26159;&#36864;&#21270;&#30340;&#12290;&#22312;&#36825;&#37324;&#25552;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;DL&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#36890;&#36807;&#23545;&#35757;&#32451;&#36755;&#20837;&#30340;&#36882;&#24402;&#25130;&#26029;&#26144;&#23556;&#30340;&#24212;&#29992;&#26469;&#8220;&#25972;&#29702;&#8221;&#35757;&#32451;&#36755;&#20837;&#65292;&#20197;&#26368;&#23567;&#21270;&#22122;&#22768;&#19982;&#20449;&#21495;&#30340;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$2^Q-1$&#20010;&#19981;&#21516;&#30340;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.10625</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#20064;&#31995;&#32479;&#20013;&#20449;&#24687;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#36755;&#20837;/&#38544;&#21547;&#29305;&#24449;&#28155;&#21152;&#22122;&#22768;&#26469;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30340;&#24212;&#29992;&#37325;&#28857;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#21487;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;&#22122;&#22768;&#36890;&#24120;&#34987;&#35270;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#24605;&#32771;&#20256;&#32479;&#21629;&#39064;&#26159;&#21542;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#25552;&#21319;&#21508;&#31181;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20449;&#24687;&#29109;&#23450;&#20041;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#20943;&#23569;&#26041;&#38754;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27491;&#22122;&#22768;&#30340;&#22686;&#24378;&#25928;&#26524;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the informat
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#65288;NLFA&#65289;&#27169;&#22411;&#23545;&#39640;&#32500;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10618</link><description>&lt;p&gt;
&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#30340;&#21160;&#24577;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Dynamic Linear Bias Incorporation Scheme for Nonnegative Latent Factor Analysis. (arXiv:2309.10618v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#65288;NLFA&#65289;&#27169;&#22411;&#23545;&#39640;&#32500;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#24230;&#21644;&#19981;&#23436;&#25972;&#65288;HDI&#65289;&#25968;&#25454;&#22312;&#22823;&#25968;&#25454;&#30456;&#20851;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#27604;&#22914;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#28041;&#21450;&#19982;&#20247;&#22810;&#33410;&#28857;&#20043;&#38388;&#30340;&#26377;&#38480;&#20132;&#20114;&#12290;&#20174;HDI&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#26159;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22240;&#20026;HDI&#25968;&#25454;&#20013;&#23884;&#20837;&#20102;&#20016;&#23500;&#30340;&#27169;&#24335;&#65292;&#22914;&#33410;&#28857;&#34892;&#20026;&#12290;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#65288;NLFA&#65289;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#65288;LBI&#65289;&#26041;&#26696;&#23545;&#20110;&#22788;&#29702;&#35757;&#32451;&#36807;&#24230;&#25670;&#21160;&#12289;&#27874;&#21160;&#20197;&#21450;&#38450;&#27490;&#27169;&#22411;&#36807;&#26089;&#25910;&#25947;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LBI&#26041;&#26696;&#37117;&#26159;&#32479;&#35745;&#22411;&#30340;&#65292;&#32447;&#24615;&#20559;&#32622;&#26159;&#22266;&#23450;&#30340;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20135;&#29983;&#30340;NLFA&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#23548;&#33268;&#20102;&#23545;HDI&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#25439;&#22833;&#12290;&#22312;&#20197;&#19978;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32447;&#24615;&#20559;&#32622;&#24341;&#20837;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional and Incomplete (HDI) data is commonly encountered in big data-related applications like social network services systems, which are concerning the limited interactions among numerous nodes. Knowledge acquisition from HDI data is a vital issue in the domain of data science due to their embedded rich patterns like node behaviors, where the fundamental task is to perform HDI data representation learning. Nonnegative Latent Factor Analysis (NLFA) models have proven to possess the superiority to address this issue, where a linear bias incorporation (LBI) scheme is important in present the training overshooting and fluctuation, as well as preventing the model from premature convergence. However, existing LBI schemes are all statistic ones where the linear biases are fixed, which significantly restricts the scalability of the resultant NLFA model and results in loss of representation learning ability to HDI data. Motivated by the above discoveries, this paper innovatively pres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#31995;&#32479;&#20013;&#30340;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#21512;&#21305;&#37197;&#29702;&#35770;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#31227;&#21160;&#21333;&#20803;&#30340;&#20010;&#20307;&#30446;&#26631;&#65292;&#21516;&#26102;&#22312;&#32447;&#23398;&#20064;&#31227;&#21160;&#21333;&#20803;&#30340;&#21162;&#21147;&#65292;&#35299;&#20915;&#20102;&#24863;&#30693;&#24179;&#21488;&#21644;&#31227;&#21160;&#21333;&#20803;&#20043;&#38388;&#30340;&#30446;&#26631;&#20914;&#31361;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21019;&#26032;&#30340;"&#26080;&#24863;&#30693;"&#26426;&#21046;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#24182;&#20943;&#23569;&#20102;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2309.10594</link><description>&lt;p&gt;
&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#20013;&#30340;&#20219;&#21153;&#20998;&#37197;&#28216;&#25103;&#20013;&#30340;&#21435;&#20013;&#24515;&#21270;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Online Learning in Task Assignment Games for Mobile Crowdsensing. (arXiv:2309.10594v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#31995;&#32479;&#20013;&#30340;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#21512;&#21305;&#37197;&#29702;&#35770;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#31227;&#21160;&#21333;&#20803;&#30340;&#20010;&#20307;&#30446;&#26631;&#65292;&#21516;&#26102;&#22312;&#32447;&#23398;&#20064;&#31227;&#21160;&#21333;&#20803;&#30340;&#21162;&#21147;&#65292;&#35299;&#20915;&#20102;&#24863;&#30693;&#24179;&#21488;&#21644;&#31227;&#21160;&#21333;&#20803;&#20043;&#38388;&#30340;&#30446;&#26631;&#20914;&#31361;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21019;&#26032;&#30340;"&#26080;&#24863;&#30693;"&#26426;&#21046;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#24182;&#20943;&#23569;&#20102;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#31995;&#32479;&#20013;&#21327;&#35843;&#25968;&#25454;&#25910;&#38598;&#30340;&#38382;&#39064;&#12290;&#31227;&#21160;&#32676;&#26234;&#24863;&#30693;&#24179;&#21488;&#36880;&#27493;&#21457;&#24067;&#24863;&#30693;&#20219;&#21153;&#32473;&#21487;&#29992;&#31227;&#21160;&#21333;&#20803;&#65292;&#24182;&#36890;&#36807;&#22238;&#20256;&#24863;&#30693;&#25253;&#20215;&#26469;&#34920;&#31034;&#23427;&#20204;&#21442;&#19982;&#20219;&#21153;&#30340;&#24847;&#24895;&#12290;&#26681;&#25454;&#25152;&#25910;&#21040;&#30340;&#25253;&#20215;&#65292;&#24863;&#30693;&#24179;&#21488;&#20915;&#23450;&#20219;&#21153;&#20998;&#37197;&#12290;&#31283;&#23450;&#30340;&#20219;&#21153;&#20998;&#37197;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#25361;&#25112;&#65306;&#24863;&#30693;&#24179;&#21488;&#21644;&#31227;&#21160;&#21333;&#20803;&#20043;&#38388;&#30340;&#30446;&#26631;&#20914;&#31361;&#65292;&#20197;&#21450;&#23545;&#31227;&#21160;&#21333;&#20803;&#25152;&#38656;&#21162;&#21147;&#21644;&#20559;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21305;&#37197;&#29702;&#35770;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#26032;&#39062;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;"&#30896;&#25758;&#36991;&#20813;&#22810;&#33218;&#32769;&#34382;&#26426;-&#31574;&#30053;&#26080;&#24863;&#30693;"&#65288;CA-MAB-SFS&#65289;&#12290;&#23558;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#24314;&#27169;&#20026;&#32771;&#34385;&#24863;&#30693;&#24179;&#21488;&#21644;&#31227;&#21160;&#21333;&#20803;&#30340;&#20010;&#20307;&#30446;&#26631;&#30340;&#21305;&#37197;&#28216;&#25103;&#65292;&#32780;&#31227;&#21160;&#21333;&#20803;&#22312;&#32447;&#23398;&#20064;&#20854;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#30340;"&#26080;&#24863;&#30693;"&#26426;&#21046;&#26174;&#33879;&#25552;&#39640;&#20102;&#31227;&#21160;&#21333;&#20803;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of coordinated data collection is studied for a mobile crowdsensing (MCS) system. A mobile crowdsensing platform (MCSP) sequentially publishes sensing tasks to the available mobile units (MUs) that signal their willingness to participate in a task by sending sensing offers back to the MCSP. From the received offers, the MCSP decides the task assignment. A stable task assignment must address two challenges: the MCSP's and MUs' conflicting goals, and the uncertainty about the MUs' required efforts and preferences. To overcome these challenges a novel decentralized approach combining matching theory and online learning, called collision-avoidance multi-armed bandit with strategic free sensing (CA-MAB-SFS), is proposed. The task assignment problem is modeled as a matching game considering the MCSP's and MUs' individual goals while the MUs learn their efforts online. Our innovative "free-sensing" mechanism significantly improves the MU's learning process while reducing collision
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;&#31995;&#32479;&#65288;PDRL&#65289;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30417;&#27979;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#31574;&#30053;&#26469;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2309.10576</link><description>&lt;p&gt;
PDRL&#65306;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring. (arXiv:2309.10576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;&#31995;&#32479;&#65288;PDRL&#65289;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30417;&#27979;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#31574;&#30053;&#26469;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#20174;&#20197;&#24448;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#20570;&#20986;&#33258;&#36866;&#24212;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;&#22312;&#30417;&#25511;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#34987;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#30417;&#25511;&#24212;&#29992;&#22823;&#22810;&#26159;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#35757;&#32451;&#26631;&#31614;&#25968;&#25454;&#65292;&#26080;&#27861;&#22312;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#20570;&#20986;&#33258;&#36866;&#24212;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#21363;&#20855;&#26377;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#39044;&#27979;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;PDRL&#65289;&#65292;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29615;&#22659;&#12290;&#35813;&#25552;&#20986;&#30340;&#36890;&#29992;&#26694;&#26550;&#21487;&#20197;&#23481;&#32435;&#34394;&#25311;&#28145;&#24230; Q &#32593;&#32476;&#65288;DQN&#65289;&#26234;&#33021;&#20307;&#65292;&#20197;&#30417;&#27979;&#22797;&#26434;&#29615;&#22659;&#30340;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#26681;&#25454;&#26126;&#30830;&#23450;&#20041;&#30340;&#22870;&#21169;&#31574;&#30053;&#20351;&#26234;&#33021;&#20307;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#21516;&#26102;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#12290;&#22312;&#35780;&#20272;&#35813;&#26694;&#26550;&#30340;&#36807;&#31243;&#20013;&#65292;&#37096;&#32626;&#20102;&#19977;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20197;&#30417;&#27979;&#36890;&#36807; BiLSTM &#27169;&#22411;&#39044;&#27979;&#30340;&#21463;&#35797;&#32773;&#26410;&#26469;&#30340;&#24515;&#29575;&#12289;&#21628;&#21560;&#29575;&#21644;&#20307;&#28201;&#12290;&#38543;&#30528;&#27599;&#27425;&#36845;&#20195;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#23454;&#38469;&#21453;&#39304;&#35843;&#25972;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been increasingly applied in monitoring applications because of its ability to learn from previous experiences and can make adaptive decisions. However, existing machine learning-based health monitoring applications are mostly supervised learning algorithms, trained on labels and they cannot make adaptive decisions in an uncertain complex environment. This study proposes a novel and generic system, predictive deep reinforcement learning (PDRL) with multiple RL agents in a time series forecasting environment. The proposed generic framework accommodates virtual Deep Q Network (DQN) agents to monitor predicted future states of a complex environment with a well-defined reward policy so that the agent learns existing knowledge while maximizing their rewards. In the evaluation process of the proposed framework, three DRL agents were deployed to monitor a subject's future heart rate, respiration, and temperature predicted using a BiLSTM model. With each iteration, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21033;&#29992;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#29575;&#26816;&#27979;&#20986;&#21508;&#31181;&#23186;&#20307;&#20013;&#30340;&#21560;&#28895;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.10561</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#29992;&#20110;&#23567;&#26679;&#26412;&#26041;&#27861;&#19979;&#30340;&#21560;&#28895;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A multimodal deep learning architecture for smoking detection with a small data approach. (arXiv:2309.10561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21033;&#29992;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#29575;&#26816;&#27979;&#20986;&#21508;&#31181;&#23186;&#20307;&#20013;&#30340;&#21560;&#28895;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;&#38544;&#34109;&#30340;&#28895;&#33609;&#24191;&#21578;&#24120;&#24120;&#24341;&#36215;&#30417;&#31649;&#25514;&#26045;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#20154;&#24037;&#26234;&#33021;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#22312;&#26816;&#27979;&#38544;&#34255;&#24191;&#21578;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#21487;&#20197;&#23545;&#28895;&#33609;&#30456;&#20851;&#23186;&#20307;&#20869;&#23481;&#36827;&#34892;&#26080;&#20559;&#35265;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#20844;&#27491;&#30340;&#37327;&#21270;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#12289;&#29983;&#25104;&#26041;&#27861;&#21644;&#20154;&#24037;&#24378;&#21270;&#30340;&#32508;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#22788;&#29702;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#21644;&#35270;&#35273;&#26684;&#24335;&#19979;&#26816;&#27979;&#21560;&#28895;&#26696;&#20363;&#65292;&#21363;&#20351;&#26377;&#24456;&#23569;&#30340;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#19978;&#21487;&#20197;&#36798;&#21040;74%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#25991;&#26412;&#19978;&#21487;&#20197;&#36798;&#21040;98%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#38598;&#25104;&#20102;&#19987;&#23478;&#24178;&#39044;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#20154;&#24037;&#24378;&#21270;&#30340;&#24418;&#24335;&#12290;&#32467;&#35770;&#65306;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#22788;&#29702;&#27169;&#22411;&#65292;&#21363;&#20351;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#26816;&#27979;&#20986;&#19981;&#21516;&#23186;&#20307;&#20013;&#30340;&#21560;&#28895;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: Covert tobacco advertisements often raise regulatory measures. This paper presents that artificial intelligence, particularly deep learning, has great potential for detecting hidden advertising and allows unbiased, reproducible, and fair quantification of tobacco-related media content. Methods: We propose an integrated text and image processing model based on deep learning, generative methods, and human reinforcement, which can detect smoking cases in both textual and visual formats, even with little available training data. Results: Our model can achieve 74\% accuracy for images and 98\% for text. Furthermore, our system integrates the possibility of expert intervention in the form of human reinforcement. Conclusions: Using the pre-trained multimodal, image, and text processing models available through deep learning makes it possible to detect smoking in different media even with few training data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#36890;&#36807;&#32771;&#34385;&#39044;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#21333;&#35789;&#30340;&#37051;&#22495;&#26469;&#30830;&#23450;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26426;&#21046;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#65292;&#21516;&#26102;&#20445;&#35777;&#26356;&#39640;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.10551</link><description>&lt;p&gt;
&#29992;&#20110;&#38745;&#24577;&#35789;&#23884;&#20837;&#30340;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings. (arXiv:2309.10551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#36890;&#36807;&#32771;&#34385;&#39044;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#21333;&#35789;&#30340;&#37051;&#22495;&#26469;&#30830;&#23450;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26426;&#21046;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#65292;&#21516;&#26102;&#20445;&#35777;&#26356;&#39640;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39044;&#35757;&#32451;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#21333;&#35789;&#37051;&#22495;&#30340;&#37051;&#22495;&#24863;&#30693;&#24046;&#20998;&#38544;&#31169;&#65288;NADP&#65289;&#26426;&#21046;&#65292;&#20197;&#30830;&#23450;&#20445;&#35777;&#25351;&#23450;&#38544;&#31169;&#32423;&#21035;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#23427;&#20204;&#30340;&#23884;&#20837;&#26500;&#24314;&#21333;&#35789;&#30340;&#26368;&#36817;&#37051;&#22270;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#36830;&#36890;&#20998;&#37327;&#65288;&#21363;&#37051;&#22495;&#65289;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#26681;&#25454;&#35813;&#37051;&#22495;&#20013;&#30340;&#21333;&#35789;&#38598;&#21512;&#20998;&#21035;&#23545;&#21333;&#35789;&#24212;&#29992;&#19981;&#21516;&#27700;&#24179;&#30340;&#39640;&#26031;&#22122;&#22768;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;NADP&#26426;&#21046;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#22810;&#20010;&#20808;&#21069;&#25552;&#20986;&#30340;DP&#26426;&#21046;&#65292;&#22914;&#25289;&#26222;&#25289;&#26031;&#12289;&#39640;&#26031;&#21644;&#39532;&#27663;&#36317;&#31163;&#65292;&#21516;&#26102;&#20445;&#35777;&#26356;&#39640;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism considering the neighbourhood of a word in a pretrained static word embedding space to determine the minimal amount of noise required to guarantee a specified privacy level. We first construct a nearest neighbour graph over the words using their embeddings, and factorise it into a set of connected components (i.e. neighbourhoods). We then separately apply different levels of Gaussian noise to the words in each neighbourhood, determined by the set of words in that neighbourhood. Experiments show that our proposed NADP mechanism consistently outperforms multiple previously proposed DP mechanisms such as Laplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while guaranteeing higher levels of privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22478;&#24066;&#27969;&#21160;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#20026;&#27809;&#26377;&#21382;&#21490;&#27969;&#21160;&#25968;&#25454;&#30340;&#22320;&#21306;&#29983;&#25104;&#21160;&#24577;&#22478;&#24066;&#27969;&#21160;&#12290;&#36890;&#36807;&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#26500;&#24314;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#65292;&#25645;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#29305;&#24449;&#21644;&#22478;&#24066;&#29615;&#22659;&#31561;&#22810;&#31181;&#22240;&#32032;&#23545;&#22478;&#24066;&#27969;&#21160;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10547</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21435;&#22122;&#25193;&#25955;&#23454;&#29616;&#22478;&#24066;&#27969;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion. (arXiv:2309.10547v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22478;&#24066;&#27969;&#21160;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#20026;&#27809;&#26377;&#21382;&#21490;&#27969;&#21160;&#25968;&#25454;&#30340;&#22320;&#21306;&#29983;&#25104;&#21160;&#24577;&#22478;&#24066;&#27969;&#21160;&#12290;&#36890;&#36807;&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#26500;&#24314;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#65292;&#25645;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21306;&#22495;&#29305;&#24449;&#21644;&#22478;&#24066;&#29615;&#22659;&#31561;&#22810;&#31181;&#22240;&#32032;&#23545;&#22478;&#24066;&#27969;&#21160;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#30340;&#24314;&#27169;&#33021;&#21147;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22478;&#24066;&#27969;&#21160;&#20316;&#20026;&#19968;&#31181;&#20856;&#22411;&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#65292;&#22312;&#21508;&#31181;&#22478;&#24066;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#23545;&#22478;&#24066;&#27969;&#21160;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#65292;&#21363;&#22522;&#20110;&#21382;&#21490;&#27969;&#21160;&#25968;&#25454;&#39044;&#27979;&#26410;&#26469;&#27969;&#21160;&#24773;&#20917;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#22320;&#21306;&#25110;&#26032;&#35268;&#21010;&#30340;&#22320;&#21306;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#20854;&#20182;&#19968;&#20123;&#30740;&#31350;&#26088;&#22312;&#39044;&#27979;&#22320;&#21306;&#20043;&#38388;&#30340;OD&#27969;&#37327;&#65292;&#20294;&#26410;&#33021;&#23545;&#22478;&#24066;&#27969;&#21160;&#38543;&#26102;&#38388;&#21160;&#24577;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22478;&#24066;&#27969;&#21160;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#20026;&#27809;&#26377;&#21382;&#21490;&#27969;&#21160;&#25968;&#25454;&#30340;&#22320;&#21306;&#29983;&#25104;&#21160;&#24577;&#22478;&#24066;&#27969;&#21160;&#12290;&#20026;&#20102;&#25429;&#25417;&#21306;&#22495;&#29305;&#24449;&#21644;&#22478;&#24066;&#29615;&#22659;&#31561;&#22810;&#31181;&#22240;&#32032;&#23545;&#22478;&#24066;&#27969;&#21160;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#20026;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#22320;&#21306;&#29983;&#25104;&#22478;&#24066;&#27969;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#65288;UKG&#65289;&#26469;&#24314;&#27169;&#22478;&#24066;&#29615;&#22659;&#21644;&#22320;&#21306;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although generative AI has been successful in many areas, its ability to model geospatial data is still underexplored. Urban flow, a typical kind of geospatial data, is critical for a wide range of urban applications. Existing studies mostly focus on predictive modeling of urban flow that predicts the future flow based on historical flow data, which may be unavailable in data-sparse areas or newly planned regions. Some other studies aim to predict OD flow among regions but they fail to model dynamic changes of urban flow over time. In this work, we study a new problem of urban flow generation that generates dynamic urban flow for regions without historical flow data. To capture the effect of multiple factors on urban flow, such as region features and urban environment, we employ diffusion model to generate urban flow for regions under different conditions. We first construct an urban knowledge graph (UKG) to model the urban environment and relationships between regions, based on which 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;(Mean Absolute Directional Loss&#65292;MADL)&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#24212;&#29992;&#20110;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#20013;&#30340;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20004;&#31181;&#19981;&#21516;&#36164;&#20135;&#31867;&#21035;&#30340;&#25968;&#25454;&#19978;&#39564;&#35777;&#65292;MADL&#20989;&#25968;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#24182;&#33719;&#24471;&#26356;&#26377;&#25928;&#30340;&#25237;&#36164;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.10546</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#29992;&#20110;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#24179;&#22343;&#32477;&#23545;&#26041;&#21521;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Mean Absolute Directional Loss as a New Loss Function for Machine Learning Problems in Algorithmic Investment Strategies. (arXiv:2309.10546v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10546
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;(Mean Absolute Directional Loss&#65292;MADL)&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#24212;&#29992;&#20110;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#20013;&#30340;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20004;&#31181;&#19981;&#21516;&#36164;&#20135;&#31867;&#21035;&#30340;&#25968;&#25454;&#19978;&#39564;&#35777;&#65292;MADL&#20989;&#25968;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#24182;&#33719;&#24471;&#26356;&#26377;&#25928;&#30340;&#25237;&#36164;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#65288;AIS&#65289;&#26500;&#24314;&#20013;&#65292;&#29992;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36866;&#24403;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#22343;&#32477;&#23545;&#26041;&#21521;&#25439;&#22833;&#65288;MADL&#65289;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#39044;&#27979;&#35823;&#24046;&#20989;&#25968;&#22312;&#20174;&#39044;&#27979;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#21019;&#24314;&#26377;&#25928;&#30340;&#20080;&#21334;&#20449;&#21495;&#26041;&#38754;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#20004;&#31181;&#19981;&#21516;&#36164;&#20135;&#31867;&#21035;&#65288;&#21152;&#23494;&#36135;&#24065;&#65306;&#27604;&#29305;&#24065;&#21644;&#22823;&#23447;&#21830;&#21697;&#65306;&#21407;&#27833;&#65289;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#20351;&#25105;&#20204;&#33021;&#22815;&#36873;&#25321;&#26356;&#22909;&#30340;LSTM&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;&#24182;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#19978;&#33719;&#24471;&#26356;&#26377;&#25928;&#30340;&#25237;&#36164;&#31574;&#30053;&#65292;&#30456;&#23545;&#20110;&#39118;&#38505;&#35843;&#25972;&#22238;&#25253;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the issue of an adequate loss function in the optimization of machine learning models used in the forecasting of financial time series for the purpose of algorithmic investment strategies (AIS) construction. We propose the Mean Absolute Directional Loss (MADL) function, solving important problems of classical forecast error functions in extracting information from forecasts to create efficient buy/sell signals in algorithmic investment strategies. Finally, based on the data from two different asset classes (cryptocurrencies: Bitcoin and commodities: Crude Oil), we show that the new loss function enables us to select better hyperparameters for the LSTM model and obtain more efficient investment strategies, with regard to risk-adjusted return metrics on the out-of-sample data.
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#21560;&#21462;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#25915;&#20987;&#65292;&#33021;&#22815;&#23558;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25552;&#21462;&#21040;&#19968;&#20010;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10544</link><description>&lt;p&gt;
&#27169;&#22411;&#21560;&#21462;: &#38024;&#23545;LLMs&#30340;&#19968;&#31181;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Model Leeching: An Extraction Attack Targeting LLMs. (arXiv:2309.10544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10544
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21560;&#21462;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#25915;&#20987;&#65292;&#33021;&#22815;&#23558;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25552;&#21462;&#21040;&#19968;&#20010;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21560;&#21462;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26032;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#33021;&#22815;&#23558;&#30446;&#26631;LLM&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#25552;&#28860;&#21040;&#19968;&#20010;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;ChatGPT-3.5-Turbo&#20013;&#25552;&#21462;&#20219;&#21153;&#33021;&#21147;&#26469;&#28436;&#31034;&#25105;&#20204;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;73%&#30340;&#20934;&#30830;&#21305;&#37197;(EM)&#30456;&#20284;&#24615;&#20197;&#21450;75%&#30340;SQuAD EM&#20934;&#30830;&#29575;&#21644;87%&#30340;F1&#24471;&#20998;&#65292;&#20165;&#38656;50&#32654;&#20803;&#30340;API&#36153;&#29992;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36890;&#36807;&#27169;&#22411;&#21560;&#21462;&#25552;&#21462;&#30340;&#27169;&#22411;&#22312;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25915;&#20987;&#26102;&#30340;&#21487;&#34892;&#24615;&#65292;&#24403;&#24212;&#29992;&#20110;ChatGPT-3.5-Turbo&#26102;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;11%&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Leeching is a novel extraction attack targeting Large Language Models (LLMs), capable of distilling task-specific knowledge from a target LLM into a reduced parameter model. We demonstrate the effectiveness of our attack by extracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match (EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%, respectively for only $50 in API cost. We further demonstrate the feasibility of adversarial attack transferability from an extracted model extracted via Model Leeching to perform ML attack staging against a target LLM, resulting in an 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#30456;&#20284;&#24230;&#27979;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#25968;&#25454;&#38598;OpenMSD&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#31185;&#23398;&#19987;&#38376;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10539</link><description>&lt;p&gt;
OpenMSD:&#38754;&#21521;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
OpenMSD: Towards Multilingual Scientific Documents Similarity Measurement. (arXiv:2309.10539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10539
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#30456;&#20284;&#24230;&#27979;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#25968;&#25454;&#38598;OpenMSD&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#31185;&#23398;&#19987;&#38376;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#30456;&#20284;&#24230;&#27979;&#37327;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#25214;&#21040;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#20851;&#20316;&#21697;&#65292;&#24110;&#21161;&#22810;&#35821;&#35328;&#30740;&#31350;&#20154;&#21592;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#21644;&#25506;&#32034;&#35770;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#31185;&#23398;&#25991;&#26723;&#25968;&#25454;&#38598;OpenMSD&#65292;&#20854;&#20013;&#21253;&#21547;103&#31181;&#35821;&#35328;&#30340;7400&#19975;&#31687;&#35770;&#25991;&#21644;7780&#19975;&#20010;&#24341;&#29992;&#23545;&#12290;&#21033;&#29992;OpenMSD&#65292;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#31185;&#23398;&#19987;&#38376;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#31574;&#30053;&#26469;&#23548;&#20986;&#8220;&#30456;&#20851;&#8221;&#30340;&#35770;&#25991;&#23545;&#20197;&#35843;&#25972;&#27169;&#22411;&#65292;&#21253;&#25324;&#20351;&#29992;&#24341;&#29992;&#12289;&#20849;&#24341;&#29992;&#21644;&#25991;&#29486;&#32806;&#21512;&#30340;&#28151;&#21512;&#23545;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#38750;&#33521;&#25991;&#35770;&#25991;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#26469;&#29992;&#33521;&#25991;&#25688;&#35201;&#20016;&#23500;&#38750;&#33521;&#25991;&#35770;&#25991;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#27169;&#22411;&#30340;&#33521;&#25991;&#33021;&#21147;&#20026;&#38750;&#33521;&#25991;&#35770;&#25991;&#21019;&#24314;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26174;&#33879;&#22320;&#36229;&#36807;&#20102;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and evaluate multilingual scientific documents similarity measurement models in this work. Such models can be used to find related works in different languages, which can help multilingual researchers find and explore papers more efficiently. We propose the first multilingual scientific documents dataset, Open-access Multilingual Scientific Documents (OpenMSD), which has 74M papers in 103 languages and 778M citation pairs. With OpenMSD, we pretrain science-specialized language models, and explore different strategies to derive "related" paper pairs to fine-tune the models, including using a mixture of citation, co-citation, and bibliographic-coupling pairs. To further improve the models' performance for non-English papers, we explore the use of generative language models to enrich the non-English papers with English summaries. This allows us to leverage the models' English capabilities to create better representations for non-English papers. Our best model significantly outp
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#36861;&#27714;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#37319;&#29992;&#36845;&#20195;&#12289;&#33258;&#23545;&#27604;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;RAVEN&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#25152;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26368;&#24369;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.10532</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#30340;&#35748;&#30693;&#21551;&#21457;&#31070;&#32463;&#32467;&#26500;&#29992;&#20110;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing. (arXiv:2309.10532v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10532
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#36861;&#27714;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#37319;&#29992;&#36845;&#20195;&#12289;&#33258;&#23545;&#27604;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;RAVEN&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#25152;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26368;&#24369;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#65292;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21551;&#21457;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#30001;&#20154;&#31867;&#25277;&#35937;&#25512;&#29702;&#36890;&#24120;&#23558;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20132;&#26367;&#36827;&#34892;&#20316;&#20026;&#28789;&#27963;&#12289;&#36845;&#20195;&#21644;&#21160;&#24577;&#35748;&#30693;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#30340;&#35266;&#23519;&#25152;&#21551;&#21457;&#12290;&#21463;&#27492;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#24314;&#27169;&#20026;&#19968;&#31181;&#36845;&#20195;&#30340;&#12289;&#33258;&#23545;&#27604;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#36861;&#27714;&#35270;&#35273;&#21050;&#28608;&#30340;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#20010;&#26032;&#30340;&#23545;&#27604;&#24863;&#30693;-&#27010;&#24565;&#32593;&#32476;&#65288;CPCNet&#65289;&#22914;&#20309;&#36890;&#36807;&#27169;&#25311;&#40486;&#25991;&#36827;&#38454;&#30697;&#38453;&#26234;&#21147;&#27979;&#35797;&#30340;&#30697;&#38453;&#25512;&#29702;&#38382;&#39064;&#26469;&#24037;&#20316;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;RAVEN&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CPCNet&#22312;&#20351;&#29992;&#26368;&#24369;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#25152;&#26377;&#24050;&#21457;&#34920;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#21407;&#22987;RAVEN&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#22823;&#37327;&#19988;&#20197;&#21069;&#27809;&#26377;&#34987;&#27880;&#24847;&#21040;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new neural architecture for solving visual abstract reasoning tasks inspired by human cognition, specifically by observations that human abstract reasoning often interleaves perceptual and conceptual processing as part of a flexible, iterative, and dynamic cognitive process. Inspired by this principle, our architecture models visual abstract reasoning as an iterative, self-contrasting learning process that pursues consistency between perceptual and conceptual processing of visual stimuli. We explain how this new Contrastive Perceptual-Conceptual Network (CPCNet) works using matrix reasoning problems in the style of the well-known Raven's Progressive Matrices intelligence test. Experiments on the machine learning dataset RAVEN show that CPCNet achieves higher accuracy than all previously published models while also using the weakest inductive bias. We also point out a substantial and previously unremarked class imbalance in the original RAVEN dataset, and we propose a new
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20114;&#34917;&#30340;&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#22270;&#20687;&#34701;&#21512;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#24341;&#23548;&#28388;&#27874;&#22120;&#21644;&#24341;&#23548;&#28388;&#27874;&#22120;&#26469;&#33719;&#21462;&#32441;&#29702;&#21644;&#36793;&#32536;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24046;&#20540;&#22270;&#21644;&#22812;&#38388;&#34917;&#20607;&#26469;&#29983;&#25104;&#20114;&#34917;&#26435;&#37325;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#34701;&#21512;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#33394;&#24425;&#22833;&#30495;&#21644;&#20266;&#24433;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10522</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#20114;&#34917;&#30340;&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#22270;&#20687;&#34701;&#21512;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Visible and NIR Image Fusion Algorithm Based on Information Complementarity. (arXiv:2309.10522v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10522
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20114;&#34917;&#30340;&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#22270;&#20687;&#34701;&#21512;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#24341;&#23548;&#28388;&#27874;&#22120;&#21644;&#24341;&#23548;&#28388;&#27874;&#22120;&#26469;&#33719;&#21462;&#32441;&#29702;&#21644;&#36793;&#32536;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24046;&#20540;&#22270;&#21644;&#22812;&#38388;&#34917;&#20607;&#26469;&#29983;&#25104;&#20114;&#34917;&#26435;&#37325;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#34701;&#21512;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#33394;&#24425;&#22833;&#30495;&#21644;&#20266;&#24433;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#65288;NIR&#65289;&#39057;&#24102;&#20256;&#24863;&#22120;&#25552;&#20379;&#20102;&#20174;&#22330;&#26223;&#20013;&#25429;&#25417;&#21040;&#30340;&#20114;&#34917;&#20809;&#35889;&#36752;&#23556;&#30340;&#22270;&#20687;&#12290;&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#22270;&#20687;&#30340;&#34701;&#21512;&#26088;&#22312;&#21033;&#29992;&#23427;&#20204;&#30340;&#20809;&#35889;&#29305;&#24615;&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#34701;&#21512;&#31639;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#21033;&#29992;&#20809;&#35889;&#29305;&#24615;&#65292;&#21516;&#26102;&#20063;&#32570;&#20047;&#20449;&#24687;&#20114;&#34917;&#65292;&#23548;&#33268;&#33394;&#24425;&#22833;&#30495;&#21644;&#20266;&#24433;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20174;&#29289;&#29702;&#20449;&#21495;&#30340;&#23618;&#38754;&#35774;&#35745;&#20102;&#19968;&#31181;&#20114;&#34917;&#34701;&#21512;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#21306;&#20998;&#22122;&#22768;&#21644;&#26377;&#29992;&#20449;&#24687;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#23618;&#26435;&#37325;&#24341;&#23548;&#28388;&#27874;&#22120;&#21644;&#24341;&#23548;&#28388;&#27874;&#22120;&#26469;&#20998;&#21035;&#33719;&#24471;&#32441;&#29702;&#21644;&#36793;&#32536;&#22270;&#20687;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#29983;&#25104;&#21021;&#22987;&#30340;&#21487;&#35265;&#20809;-NIR&#20114;&#34917;&#26435;&#37325;&#22270;&#65292;&#36890;&#36807;&#25193;&#23637;-DoG&#28388;&#27874;&#22120;&#23545;&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#30340;&#24046;&#20540;&#22270;&#36827;&#34892;&#28388;&#27874;&#12290;&#20043;&#21518;&#65292;NIR&#22812;&#38388;&#34917;&#20607;&#30340;&#26174;&#33879;&#21306;&#22495;&#36890;&#36807;arctanI&#20989;&#25968;&#26469;&#25351;&#23548;&#21021;&#22987;&#20114;&#34917;&#26435;&#37325;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visible and near-infrared(NIR) band sensors provide images that capture complementary spectral radiations from a scene. And the fusion of the visible and NIR image aims at utilizing their spectrum properties to enhance image quality. However, currently visible and NIR fusion algorithms cannot well take advantage of spectrum properties, as well as lack information complementarity, which results in color distortion and artifacts. Therefore, this paper designs a complementary fusion model from the level of physical signals. First, in order to distinguish between noise and useful information, we use two layers of the weight-guided filter and guided filter to obtain texture and edge layers, respectively. Second, to generate the initial visible-NIR complementarity weight map, the difference maps of visible and NIR are filtered by the extend-DoG filter. After that, the significant region of NIR night-time compensation guides the initial complementarity weight map by the arctanI function. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#19981;&#24403;&#27169;&#25311;&#35774;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28385;&#36275;&#26399;&#26395;&#30340;&#27169;&#25311;&#26694;&#26550;PARCS&#65292;&#35813;&#26694;&#26550;&#21512;&#25104;&#20102;&#22522;&#20110;&#22240;&#26524;&#27169;&#22411;&#21644;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#29983;&#25104;&#31526;&#21512;&#26465;&#20214;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10514</link><description>&lt;p&gt;
&#37096;&#20998;&#25351;&#23450;&#30340;&#22240;&#26524;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Partially-Specified Causal Simulations. (arXiv:2309.10514v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#19981;&#24403;&#27169;&#25311;&#35774;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28385;&#36275;&#26399;&#26395;&#30340;&#27169;&#25311;&#26694;&#26550;PARCS&#65292;&#35813;&#26694;&#26550;&#21512;&#25104;&#20102;&#22522;&#20110;&#22240;&#26524;&#27169;&#22411;&#21644;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#29983;&#25104;&#31526;&#21512;&#26465;&#20214;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#30740;&#31350;&#22312;&#39564;&#35777;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21482;&#26377;&#22312;&#30740;&#31350;&#26681;&#25454;&#34987;&#27979;&#35797;&#26041;&#27861;&#25152;&#25215;&#35834;&#30340;&#25805;&#20316;&#26465;&#20214;&#36827;&#34892;&#35774;&#35745;&#26102;&#65292;&#27169;&#25311;&#32467;&#26524;&#25165;&#26159;&#21487;&#38752;&#30340;&#12290;&#28982;&#32780;&#65292;&#24456;&#22810;&#22240;&#26524;&#25512;&#26029;&#25991;&#29486;&#24448;&#24448;&#35774;&#35745;&#20102;&#36807;&#20110;&#21463;&#38480;&#25110;&#38169;&#35823;&#25351;&#23450;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#22240;&#26524;&#26041;&#27861;&#19981;&#24403;&#27169;&#25311;&#35774;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#32534;&#21046;&#20102;&#26377;&#25928;&#30340;&#27169;&#25311;&#26694;&#26550;&#30340;&#19968;&#31995;&#21015;&#26399;&#26395;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#37096;&#20998;&#38543;&#26426;&#21270;&#30340;&#22240;&#26524;&#27169;&#25311;&#65288;PARCS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#28385;&#36275;&#36825;&#20123;&#26399;&#26395;&#30340;&#27169;&#25311;&#26694;&#26550;&#12290;PARCS&#22522;&#20110;&#22270;&#24418;&#21270;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#19968;&#31995;&#21015;&#21487;&#35843;&#21442;&#25968;&#21512;&#25104;&#25968;&#25454;&#12290;&#36890;&#24120;&#30340;&#22240;&#26524;&#20551;&#35774;&#19982;&#21442;&#25968;&#20043;&#38388;&#26377;&#26126;&#30830;&#30340;&#26144;&#23556;&#65292;&#22240;&#27492;&#29992;&#25143;&#21487;&#20197;&#30830;&#23450;&#24182;&#25351;&#23450;&#30456;&#20851;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#24182;&#38543;&#26426;&#21270;&#20854;&#20313;&#21442;&#25968;&#20197;&#29983;&#25104;&#31526;&#21512;&#26465;&#20214;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#29992;&#20110;&#20854;&#22240;&#26524;&#26041;&#27861;&#12290;&#32467;&#26524;&#26159;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation studies play a key role in the validation of causal inference methods. The simulation results are reliable only if the study is designed according to the promised operational conditions of the method-in-test. Still, many causal inference literature tend to design over-restricted or misspecified studies. In this paper, we elaborate on the problem of improper simulation design for causal methods and compile a list of desiderata for an effective simulation framework. We then introduce partially-randomized causal simulation (PARCS), a simulation framework that meets those desiderata. PARCS synthesizes data based on graphical causal models and a wide range of adjustable parameters. There is a legible mapping from usual causal assumptions to the parameters, thus, users can identify and specify the subset of related parameters and randomize the remaining ones to generate a range of complying data-generating processes for their causal method. The result is a more comprehensive and i
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#22788;&#29702;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35813;&#24211;&#29983;&#25104;&#19981;&#21516;&#20998;&#24067;&#30340;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29983;&#25104;&#21442;&#25968;&#21644;&#29983;&#25104;&#35268;&#21017;&#36827;&#34892;&#33258;&#23450;&#20041;&#25511;&#21046;&#12290;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10498</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#29992;&#20110;&#29983;&#25104;&#21644;&#25805;&#20316;&#36855;&#23467;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Configurable Library for Generating and Manipulating Maze Datasets. (arXiv:2309.10498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#22788;&#29702;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35813;&#24211;&#29983;&#25104;&#19981;&#21516;&#20998;&#24067;&#30340;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29983;&#25104;&#21442;&#25968;&#21644;&#29983;&#25104;&#35268;&#21017;&#36827;&#34892;&#33258;&#23450;&#20041;&#25511;&#21046;&#12290;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20998;&#24067;&#20559;&#31227;&#30340;&#21709;&#24212;&#26041;&#24335;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#29983;&#25104;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#24179;&#21488;&#26469;&#27169;&#25311;&#24494;&#22937;&#21644;&#26174;&#33879;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36855;&#23467;&#20316;&#20026;&#19968;&#20010;&#20248;&#31168;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#20026;&#20102;&#25903;&#25345;&#23545;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31163;&#25968;&#25454;&#19978;&#34892;&#20026;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;maze-dataset&#8221;&#65292;&#19968;&#20010;&#21253;&#21547;&#36855;&#23467;&#27714;&#35299;&#20219;&#21153;&#30340;&#29983;&#25104;&#12289;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#24211;&#12290;&#20511;&#21161;&#36825;&#20010;&#24211;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#20351;&#29992;&#30340;&#29983;&#25104;&#31639;&#27861;&#12289;&#20256;&#36882;&#32473;&#36873;&#25321;&#31639;&#27861;&#30340;&#21442;&#25968;&#21644;&#29983;&#25104;&#30340;&#36855;&#23467;&#24517;&#39035;&#28385;&#36275;&#30340;&#31579;&#36873;&#22120;&#36827;&#34892;&#24191;&#27867;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#23427;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#21253;&#25324;&#26629;&#26684;&#21270;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#27169;&#22411;&#12290;&#36825;&#20123;&#26684;&#24335;&#20197;&#21450;&#29992;&#20110;&#21487;&#35270;&#21270;&#21644;&#36716;&#25442;&#30340;&#24037;&#20855;&#30830;&#20445;&#20102;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;ETHICS&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#34920;&#26126;AI&#20262;&#29702;&#20013;&#19982;&#20849;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#21512;&#20316;&#23398;&#20064;&#24182;&#19981;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10492</link><description>&lt;p&gt;
&#23545;GPT-4&#22312;ETHICS&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of GPT-4 on the ETHICS Dataset. (arXiv:2309.10492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;ETHICS&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#34920;&#26126;AI&#20262;&#29702;&#20013;&#19982;&#20849;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#21512;&#20316;&#23398;&#20064;&#24182;&#19981;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#24635;&#32467;&#20102;&#23545;GPT-4&#22312;ETHICS&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#30340;&#30701;&#26399;&#30740;&#31350;&#12290;ETHICS&#25968;&#25454;&#38598;&#21253;&#21547;&#20116;&#20010;&#20998;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20262;&#29702;&#23398;&#30340;&#19981;&#21516;&#39046;&#22495;&#65306;&#27491;&#20041;&#12289;&#36947;&#24503;&#12289;&#24503;&#24615;&#20262;&#29702;&#23398;&#12289;&#21151;&#21033;&#20027;&#20041;&#21644;&#24120;&#35782;&#20262;&#29702;&#23398;&#12290;&#36825;&#20123;&#36947;&#24503;&#21028;&#26029;&#34987;&#31934;&#36873;&#65292;&#20197;&#23613;&#21487;&#33021;&#39640;&#30340;&#19968;&#33268;&#24615;&#26469;&#20195;&#34920;&#20849;&#20139;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#32780;&#19981;&#26159;&#36947;&#24503;&#22256;&#22659;&#12290;GPT-4&#30340;&#34920;&#29616;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#22909;&#24471;&#22810;&#65292;&#34920;&#26126;AI&#20262;&#29702;&#20013;&#19982;&#20849;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#21512;&#20316;&#23398;&#20064;&#24182;&#19981;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report summarizes a short study of the performance of GPT-4 on the ETHICS dataset. The ETHICS dataset consists of five sub-datasets covering different fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism, and Commonsense Ethics. The moral judgments were curated so as to have a high degree of agreement with the aim of representing shared human values rather than moral dilemmas. GPT-4's performance is much better than that of previous models and suggests that learning to work with common human values is not the hard problem for AI ethics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#22836;&#37096;&#27979;&#37327;&#27880;&#37322;&#26041;&#27861;&#65292;&#20351;&#29992;&#19977;&#32500;&#38754;&#37096;&#31435;&#20307;&#25668;&#24433;&#26415;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#36827;&#34892;&#22320;&#26631;&#26631;&#35760;&#21644;&#38754;&#37096;&#20998;&#21106;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10472</link><description>&lt;p&gt;
&#19977;&#32500;&#29031;&#29255;&#19978;&#23436;&#20840;&#33258;&#21160;&#30340;&#22320;&#26631;&#26631;&#35760;&#21644;&#38754;&#37096;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Fully automated landmarking and facial segmentation on 3D photographs. (arXiv:2309.10472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#22836;&#37096;&#27979;&#37327;&#27880;&#37322;&#26041;&#27861;&#65292;&#20351;&#29992;&#19977;&#32500;&#38754;&#37096;&#31435;&#20307;&#25668;&#24433;&#26415;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#36827;&#34892;&#22320;&#26631;&#26631;&#35760;&#21644;&#38754;&#37096;&#20998;&#21106;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#38754;&#37096;&#31435;&#20307;&#25668;&#24433;&#26415;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#39045;&#38754;&#36719;&#32452;&#32455;&#34920;&#31034;&#65292;&#26080;&#38656;&#20351;&#29992;&#30005;&#31163;&#36752;&#23556;&#12290;&#34429;&#28982;&#25163;&#21160;&#27880;&#37322;&#26631;&#35760;&#20316;&#20026;&#22836;&#37096;&#27979;&#37327;&#20998;&#26512;&#30340;&#24403;&#21069;&#37329;&#26631;&#20934;&#65292;&#20294;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#21644;&#35780;&#20272;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#22836;&#37096;&#27979;&#37327;&#27880;&#37322;&#26041;&#27861;&#12290;&#19968;&#20010;&#35266;&#23519;&#32773;&#25163;&#21160;&#22312;2897&#24352;&#19977;&#32500;&#38754;&#37096;&#29031;&#29255;&#19978;&#27880;&#37322;&#20102;&#21313;&#20010;&#22320;&#26631;&#12290;&#33258;&#21160;&#26631;&#35760;&#24037;&#20316;&#27969;&#28041;&#21450;&#20004;&#20010;&#36830;&#32493;&#30340;DiffusionNet&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#38754;&#37096;&#20998;&#21106;&#31639;&#27861;&#12290;&#25968;&#25454;&#38598;&#34987;&#38543;&#26426;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#35757;&#32451;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#27979;&#35797;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#24037;&#20316;&#27969;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#35745;&#31639;&#33258;&#21160;&#21644;&#25163;&#21160;&#26631;&#35760;&#20043;&#38388;&#30340;&#27431;&#27663;&#36317;&#31163;&#26469;&#35780;&#20272;&#24037;&#20316;&#27969;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three-dimensional facial stereophotogrammetry provides a detailed representation of craniofacial soft tissue without the use of ionizing radiation. While manual annotation of landmarks serves as the current gold standard for cephalometric analysis, it is a time-consuming process and is prone to human error. The aim in this study was to develop and evaluate an automated cephalometric annotation method using a deep learning-based approach. Ten landmarks were manually annotated on 2897 3D facial photographs by a single observer. The automated landmarking workflow involved two successive DiffusionNet models and additional algorithms for facial segmentation. The dataset was randomly divided into a training and test dataset. The training dataset was used to train the deep learning networks, whereas the test dataset was used to evaluate the performance of the automated workflow. The precision of the workflow was evaluated by calculating the Euclidean distances between the automated and manual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;ChatGPT&#24320;&#21457;&#39640;&#32423;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#24182;&#33258;&#21160;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#21487;&#33021;&#24615;&#65292;&#31361;&#20986;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#24555;&#36895;&#29983;&#25104;&#21644;&#19982;&#30446;&#26631;&#32593;&#31449;&#30340;&#23494;&#20999;&#30456;&#20284;&#24615;&#65292;&#24378;&#35843;&#20102;&#22312;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#20013;&#28389;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#39118;&#38505;&#65292;&#20197;&#21450;&#23545;AI&#31995;&#32479;&#21152;&#24378;&#23545;&#31574;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10463</link><description>&lt;p&gt;
&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#26263;&#38754;: &#20351;&#29992;ChatGPT&#35774;&#35745;&#21644;&#37096;&#32626;&#39640;&#32423;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Exploring the Dark Side of AI: Advanced Phishing Attack Design and Deployment Using ChatGPT. (arXiv:2309.10463v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;ChatGPT&#24320;&#21457;&#39640;&#32423;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#24182;&#33258;&#21160;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#21487;&#33021;&#24615;&#65292;&#31361;&#20986;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#24555;&#36895;&#29983;&#25104;&#21644;&#19982;&#30446;&#26631;&#32593;&#31449;&#30340;&#23494;&#20999;&#30456;&#20284;&#24615;&#65292;&#24378;&#35843;&#20102;&#22312;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#20013;&#28389;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#39118;&#38505;&#65292;&#20197;&#21450;&#23545;AI&#31995;&#32479;&#21152;&#24378;&#23545;&#31574;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;ChatGPT&#24320;&#21457;&#39640;&#32423;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#24182;&#33258;&#21160;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#35753;ChatGPT&#29983;&#25104;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#30340;&#20197;&#19979;&#37096;&#20998;&#65306;i) &#20811;&#38534;&#30446;&#26631;&#32593;&#31449;&#65292;ii) &#38598;&#25104;&#31363;&#21462;&#20973;&#25454;&#30340;&#20195;&#30721;&#65292;iii) &#28151;&#28102;&#20195;&#30721;&#65292;iv) &#22312;&#25176;&#31649;&#25552;&#20379;&#21830;&#19978;&#33258;&#21160;&#21270;&#37096;&#32626;&#32593;&#31449;&#65292;v) &#27880;&#20876;&#32593;&#32476;&#38035;&#40060;&#22495;&#21517;&#65292;vi) &#23558;&#32593;&#31449;&#19982;&#21453;&#21521;&#20195;&#29702;&#38598;&#25104;&#12290;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#32593;&#32476;&#38035;&#40060;&#24037;&#20855;&#21253;&#30340;&#21021;&#27493;&#35780;&#20272;&#31361;&#20986;&#20102;&#23427;&#20204;&#24555;&#36895;&#29983;&#25104;&#21644;&#37096;&#32626;&#30340;&#36807;&#31243;&#65292;&#20197;&#21450;&#29983;&#25104;&#39029;&#38754;&#19982;&#30446;&#26631;&#32593;&#31449;&#30340;&#23494;&#20999;&#30456;&#20284;&#24615;&#12290;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#22312;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#24378;&#35843;&#20102;&#22312;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#20013;&#28389;&#29992;&#20854;&#28508;&#21147;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23427;&#20204;&#30340;&#26222;&#36941;&#24615;&#21644;&#20005;&#37325;&#24615;&#22686;&#21152;&#12290;&#36825;&#31361;&#26174;&#20102;AI&#31995;&#32479;&#20013;&#22686;&#24378;&#23545;&#31574;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the possibility of using ChatGPT to develop advanced phishing attacks and automate their large-scale deployment. We make ChatGPT generate the following parts of a phishing attack: i) cloning a targeted website, ii) integrating code for stealing credentials, iii) obfuscating code, iv) automating website deployment on a hosting provider, v) registering a phishing domain name, and vi) integrating the website with a reverse proxy. The initial assessment of the automatically generated phishing kits highlights their rapid generation and deployment process as well as the close resemblance of the resulting pages to the target website. More broadly, we demonstrate that recent advances in AI underscore the potential risks of its misuse in phishing attacks, which can lead to their increased prevalence and severity. This highlights the necessity for enhanced countermeasures within AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#38754;&#20020;&#30340;&#21516;&#36136;&#21270;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#36755;&#20986;&#32780;&#19981;&#29306;&#29298;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10448</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#20197;&#21450;&#31038;&#20250;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Human-AI Interactions and Societal Pitfalls. (arXiv:2309.10448v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#38754;&#20020;&#30340;&#21516;&#36136;&#21270;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#36755;&#20986;&#32780;&#19981;&#29306;&#29298;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21512;&#20316;&#26102;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#30475;&#21040;&#29983;&#20135;&#21147;&#30340;&#25552;&#21319;&#65292;&#20294;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21487;&#33021;&#19981;&#23436;&#20840;&#31526;&#21512;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20854;&#20013;&#24322;&#36136;&#29992;&#25143;&#36873;&#25321;&#19982;AI&#20849;&#20139;&#22810;&#23569;&#20449;&#24687;&#65292;&#38754;&#20020;&#36755;&#20986;&#20445;&#30495;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20010;&#20307;&#20915;&#31574;&#19982;AI&#35757;&#32451;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#25361;&#25112;&#12290;&#36755;&#20986;&#21487;&#33021;&#21464;&#24471;&#26356;&#21152;&#21516;&#36136;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;AI&#22312;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#12290;&#32780;&#20219;&#20309;AI&#30340;&#20559;&#35265;&#21487;&#33021;&#25104;&#20026;&#31038;&#20250;&#20559;&#35265;&#12290;&#35299;&#20915;&#21516;&#36136;&#21270;&#21644;&#20559;&#35265;&#38382;&#39064;&#30340;&#21150;&#27861;&#26159;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#36755;&#20986;&#32780;&#19981;&#29306;&#29298;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When working with generative artificial intelligence (AI), users may see productivity gains, but the AI-generated content may not match their preferences exactly. To study this effect, we introduce a Bayesian framework in which heterogeneous users choose how much information to share with the AI, facing a trade-off between output fidelity and communication cost. We show that the interplay between these individual-level decisions and AI training may lead to societal challenges. Outputs may become more homogenized, especially when the AI is trained on AI-generated content. And any AI bias may become societal bias. A solution to the homogenization and bias issues is to improve human-AI interactions, enabling personalized outputs without sacrificing productivity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#23454;&#29616;&#20102;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25351;&#20196;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#32422;&#26463;&#65292;&#26080;&#38656;&#23545;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#65292;&#24182;&#23545;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10447</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#23454;&#29616;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Toward Unified Controllable Text Generation via Regular Expression Instruction. (arXiv:2309.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#23454;&#29616;&#20102;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25351;&#20196;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#32422;&#26463;&#65292;&#26080;&#38656;&#23545;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#65292;&#24182;&#23545;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22522;&#26412;&#26041;&#38754;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38024;&#23545;&#19981;&#21516;&#32422;&#26463;&#31867;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#37325;&#22823;&#30340;&#26550;&#26500;&#25110;&#35299;&#30721;&#20462;&#25913;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#38468;&#21152;&#32422;&#26463;&#25110;&#35299;&#20915;&#19981;&#21516;&#32422;&#26463;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#65292;&#21033;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#26426;&#21046;&#20805;&#20998;&#21033;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#20248;&#21183;&#65292;&#32479;&#19968;&#24314;&#27169;&#21508;&#31181;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;REI&#36890;&#36807;&#27491;&#21017;&#34920;&#36798;&#24335;&#39118;&#26684;&#30340;&#25351;&#20196;&#25903;&#25345;&#25152;&#26377;&#27969;&#34892;&#30340;&#32454;&#31890;&#24230;&#21487;&#25511;&#29983;&#25104;&#32422;&#26463;&#65292;&#21363;&#35789;&#27719;&#12289;&#20301;&#32622;&#21644;&#38271;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#20013;&#31561;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#25110;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20110;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#26102;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instruction-based mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on medium-scale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#22823;&#35268;&#27169;&#23454;&#38469;&#25968;&#25454;&#38598;&#21644;&#26631;&#20934;&#21270;&#38381;&#29615;&#22522;&#20934;&#27979;&#35797;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#27169;&#20223;&#30340;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#22120;&#65292;&#24182;&#23545;&#20851;&#38190;&#29305;&#24449;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#27169;&#20223;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;-PlanTF&#12290;</title><link>http://arxiv.org/abs/2309.10443</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#27169;&#20223;&#30340;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rethinking Imitation-based Planner for Autonomous Driving. (arXiv:2309.10443v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#22823;&#35268;&#27169;&#23454;&#38469;&#25968;&#25454;&#38598;&#21644;&#26631;&#20934;&#21270;&#38381;&#29615;&#22522;&#20934;&#27979;&#35797;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#27169;&#20223;&#30340;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#22120;&#65292;&#24182;&#23545;&#20851;&#38190;&#29305;&#24449;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#27169;&#20223;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;-PlanTF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27169;&#20223;&#30340;&#39550;&#39542;&#35268;&#21010;&#22120;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#19981;&#21516;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26032;&#21457;&#24067;&#30340;nuPlan&#36890;&#36807;&#25552;&#20379;&#22823;&#35268;&#27169;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#21644;&#26631;&#20934;&#21270;&#38381;&#29615;&#22522;&#20934;&#27979;&#35797;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#21033;&#29992;&#36825;&#20010;&#24179;&#21488;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#27169;&#20223;&#30340;&#35268;&#21010;&#22120;&#30340;&#20004;&#20010;&#22522;&#26412;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65306;&#33258;&#25105;&#35268;&#21010;&#25152;&#38656;&#30340;&#20851;&#38190;&#29305;&#24449;&#21644;&#38477;&#20302;&#22797;&#21512;&#35823;&#24046;&#30340;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24403;&#21069;&#23398;&#20064;&#31995;&#32479;&#24573;&#35270;&#30340;&#27169;&#20223;&#24046;&#36317;&#12290;&#26368;&#21518;&#65292;&#25972;&#21512;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;-PlanTF&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#35774;&#35745;&#33391;&#22909;&#30340;&#32431;&#27169;&#20223;&#35268;&#21010;&#22120;&#21487;&#20197;&#19982;&#28041;&#21450;&#25163;&#24037;&#21046;&#23450;&#35268;&#21017;&#30340;&#29616;&#26377;&#26041;&#27861;&#23454;&#29616;&#39640;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, imitation-based driving planners have reported considerable success. However, due to the absence of a standardized benchmark, the effectiveness of various designs remains unclear. The newly released nuPlan addresses this issue by offering a large-scale real-world dataset and a standardized closed-loop benchmark for equitable comparisons. Utilizing this platform, we conduct a comprehensive study on two fundamental yet underexplored aspects of imitation-based planners: the essential features for ego planning and the effective data augmentation techniques to reduce compounding errors. Furthermore, we highlight an imitation gap that has been overlooked by current learning systems. Finally, integrating our findings, we propose a strong baseline model-PlanTF. Our results demonstrate that a well-designed, purely imitation-based planner can achieve highly competitive performance compared to state-of-the-art methods involving hand-crafted rules and exhibit superior generalizati
&lt;/p&gt;</description></item><item><title>&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#27169;&#22411;&#21270;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#39044;&#27979;&#20102;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#31561;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10426</link><description>&lt;p&gt;
&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65306;&#36890;&#36807;&#22797;&#21512;&#23545;&#35937;&#21487;&#29992;&#24615;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances. (arXiv:2309.10426v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10426
&lt;/p&gt;
&lt;p&gt;
&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#27169;&#22411;&#21270;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#39044;&#27979;&#20102;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#31561;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#28145;&#20837;&#25506;&#35752;&#20102;&#21333;&#20010;&#25110;&#25104;&#23545;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#22312;&#35843;&#26597;&#30001;&#22797;&#26434;&#24418;&#29366;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#23545;&#35937;&#32452;&#25104;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#65292;&#23427;&#24314;&#27169;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#39044;&#27979;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#29616;&#26377;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#32473;&#23450;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#21253;&#25324;&#22534;&#21472;&#30340;&#29699;&#20307;&#21644;&#26479;&#23376;&#12289;&#26438;&#21644;&#21253;&#22260;&#26438;&#30340;&#29615;&#31561;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning object affordances is an effective tool in the field of robot learning. While the data-driven models delve into the exploration of affordances of single or paired objects, there is a notable gap in the investigation of affordances of compound objects that are composed of an arbitrary number of objects with complex shapes. In this study, we propose Multi-Object Graph Affordance Network (MOGAN) that models compound object affordances and predicts the effect of placing new objects on top of the existing compound. Given different tasks, such as building towers of specific heights or properties, we used a search based planning to find the sequence of stack actions with the objects of suitable affordances. We showed that our system was able to correctly model the affordances of very complex compound objects that include stacked spheres and cups, poles, and rings that enclose the poles. We demonstrated the applicability of our system in both simulated and real-world environments, com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;14&#20010;&#21151;&#33021;&#35201;&#27714;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#23454;&#26045;&#36825;&#20123;&#35201;&#27714;&#26469;&#38477;&#20302;&#21307;&#30103;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.10424</link><description>&lt;p&gt;
&#20943;&#23569;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21151;&#33021;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare. (arXiv:2309.10424v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;14&#20010;&#21151;&#33021;&#35201;&#27714;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#23454;&#26045;&#36825;&#20123;&#35201;&#27714;&#26469;&#38477;&#20302;&#21307;&#30103;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#27954;&#35758;&#20250;&#35758;&#21592;&#30740;&#31350;&#26381;&#21153;&#24635;&#21496;&#20026;&#27431;&#27954;&#35758;&#20250;&#35758;&#21592;&#20934;&#22791;&#20102;&#19968;&#20221;&#25253;&#21578;&#65292;&#20854;&#20013;&#21015;&#20030;&#20102;&#21307;&#30103;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#30340;&#19971;&#20010;&#20027;&#35201;&#39118;&#38505;&#65306;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#38169;&#35823;&#23548;&#33268;&#24739;&#32773;&#21463;&#20260;&#12289;&#21307;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#28389;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#30340;&#20559;&#35265;&#20197;&#21450;&#29616;&#26377;&#19981;&#24179;&#31561;&#30340;&#24310;&#32493;&#12289;&#32570;&#20047;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#36131;&#20219;&#32570;&#22833;&#21644;&#23454;&#26045;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;14&#20010;&#21151;&#33021;&#35201;&#27714;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#23454;&#26045;&#36825;&#20123;&#35201;&#27714;&#26469;&#38477;&#20302;&#19982;&#20854;&#21307;&#30103;&#30446;&#30340;&#30456;&#20851;&#30340;&#39118;&#38505;&#65306;&#20154;&#24037;&#26234;&#33021;&#25252;&#29031;&#12289;&#29992;&#25143;&#31649;&#29702;&#12289;&#27861;&#35268;&#26816;&#26597;&#12289;&#20165;&#20379;&#23398;&#26415;&#20351;&#29992;&#30340;&#20813;&#36131;&#22768;&#26126;&#12289;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#12289;&#20020;&#24202;&#21307;&#29983;&#30340;&#21452;&#37325;&#26816;&#26597;&#12289;&#25345;&#32493;&#24615;&#33021;&#35780;&#20272;&#12289;&#23457;&#35745;&#36319;&#36394;&#12289;&#25345;&#32493;&#21487;&#29992;&#24615;&#27979;&#35797;&#12289;&#22238;&#39038;&#24615;/&#27169;&#25311;&#26696;&#20363;&#30340;&#23457;&#26597;&#12289;&#20559;&#35265;&#26816;&#26597;&#12289;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#12289;&#21152;&#23494;&#21644;&#20351;&#29992;&#32463;&#36807;&#23454;&#22320;&#27979;&#35797;&#30340;&#24211;&#20197;&#21450;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#19968;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Directorate General for Parliamentary Research Services of the European Parliament has prepared a report to the Members of the European Parliament where they enumerate seven main risks of Artificial Intelligence (AI) in medicine and healthcare: patient harm due to AI errors, misuse of medical AI tools, bias in AI and the perpetuation of existing inequities, lack of transparency, privacy and security issues, gaps in accountability, and obstacles in implementation.  In this study, we propose fourteen functional requirements that AI systems may implement to reduce the risks associated with their medical purpose: AI passport, User management, Regulation check, Academic use only disclaimer, data quality assessment, Clinicians double check, Continuous performance evaluation, Audit trail, Continuous usability test, Review of retrospective/simulated cases, Bias check, eXplainable AI, Encryption and use of field-tested libraries, and Semantic interoperability.  Our intention here is to prov
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21021;&#23398;&#32773;&#22312;&#23376;&#30446;&#26631;&#23398;&#20064;&#29615;&#22659;&#20013;&#19982;AI&#21161;&#25945;&#30340;&#20114;&#21160;&#65292;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#22411;AI&#21161;&#25945;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#25945;&#32946;&#20013;&#20855;&#26377;&#19982;&#20154;&#31867;&#21161;&#25945;&#30456;&#24403;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;&#36825;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#35774;&#35745;&#21644;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#21161;&#25945;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>http://arxiv.org/abs/2309.10419</link><description>&lt;p&gt;
&#20174;&#21161;&#25945;&#20013;&#23398;&#20064;&#32534;&#31243;&#23376;&#30446;&#26631;&#65306;&#25506;&#32034;AI&#21161;&#25945;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning from Teaching Assistants to Program with Subgoals: Exploring the Potential for AI Teaching Assistants. (arXiv:2309.10419v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10419
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21021;&#23398;&#32773;&#22312;&#23376;&#30446;&#26631;&#23398;&#20064;&#29615;&#22659;&#20013;&#19982;AI&#21161;&#25945;&#30340;&#20114;&#21160;&#65292;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#22411;AI&#21161;&#25945;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#25945;&#32946;&#20013;&#20855;&#26377;&#19982;&#20154;&#31867;&#21161;&#25945;&#30456;&#24403;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;&#36825;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#35774;&#35745;&#21644;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#21161;&#25945;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#23545;&#35805;&#27169;&#22411;&#25104;&#20026;&#20102;&#21161;&#25945;&#30340;&#21487;&#34892;&#20505;&#36873;&#20154;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#21021;&#23398;&#32773;&#22312;&#23376;&#30446;&#26631;&#23398;&#20064;&#29615;&#22659;&#20013;&#19982;&#21161;&#25945;&#30340;&#20114;&#21160;&#26469;&#25506;&#35752;&#20351;&#29992;&#29983;&#25104;&#22411;AI&#20316;&#20026;&#32534;&#31243;&#25945;&#32946;&#21161;&#25945;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#27604;&#36739;&#23398;&#20064;&#32773;&#19982;AI&#21161;&#25945;&#21644;&#20154;&#31867;&#21161;&#25945;&#20043;&#38388;&#30340;&#20114;&#21160;&#21644;&#24863;&#30693;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;20&#21517;&#21021;&#23398;&#32773;&#32534;&#31243;&#23398;&#20064;&#32773;&#30340;&#32452;&#38388;&#30740;&#31350;&#12290;&#23398;&#20064;&#32773;&#36890;&#36807;&#20135;&#29983;&#23376;&#30446;&#26631;&#21644;&#23376;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#21161;&#25945;&#30340;&#25351;&#23548;&#19979;&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#32773;&#33021;&#22815;&#20197;&#30456;&#24403;&#30340;&#24471;&#20998;&#26356;&#24555;&#22320;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#19988;&#23545;AI&#21161;&#25945;&#30340;&#35748;&#35782;&#19982;&#20154;&#31867;&#21161;&#25945;&#22312;&#22238;&#22797;&#30340;&#36895;&#24230;&#21644;&#35814;&#23613;&#31243;&#24230;&#12289;&#23545;&#35805;&#30340;&#26377;&#29992;&#24615;&#12289;&#38590;&#24230;&#21644;&#28385;&#24847;&#24230;&#31561;&#26041;&#38754;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26681;&#25454;&#32842;&#22825;&#35760;&#24405;&#20998;&#26512;&#32467;&#26524;&#26356;&#22909;&#22320;&#35774;&#35745;&#21644;&#21033;&#29992;&#29983;&#25104;&#22411;AI&#20316;&#20026;&#32534;&#31243;&#25945;&#32946;&#21161;&#25945;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advances in generative AI, conversational models like ChatGPT have become feasible candidates for TAs. We investigate the practicality of using generative AI as TAs in introductory programming education by examining novice learners' interaction with TAs in a subgoal learning environment. To compare the learners' interaction and perception of the AI and human TAs, we conducted a between-subject study with 20 novice programming learners. Learners solve programming tasks by producing subgoals and subsolutions with the guidance of a TA. Our study shows that learners can solve tasks faster with comparable scores with AI TAs. Learners' perception of the AI TA is on par with that of human TAs in terms of speed and comprehensiveness of the replies and helpfulness, difficulty, and satisfaction of the conversation. Finally, we suggest guidelines to better design and utilize generative AI as TAs in programming education from the result of our chat log analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#25968;&#20540;&#33410;&#28857;&#23646;&#24615;&#20043;&#38388;&#30340;&#32593;&#32476;&#36317;&#31163;&#26469;&#21019;&#24314;&#32593;&#32476;&#24863;&#30693;&#23884;&#20837;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10408</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#24863;&#30693;&#23884;&#20837;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning via Network-Aware Embeddings. (arXiv:2309.10408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#25968;&#20540;&#33410;&#28857;&#23646;&#24615;&#20043;&#38388;&#30340;&#32593;&#32476;&#36317;&#31163;&#26469;&#21019;&#24314;&#32593;&#32476;&#24863;&#30693;&#23884;&#20837;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32858;&#31867;&#26159;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#20219;&#21153;&#26159;&#26681;&#25454;&#30456;&#20284;&#24615;&#23545;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#20998;&#32452;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#30340;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#22797;&#26434;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#20363;&#22914;&#20154;&#20204;&#22312;&#22797;&#26434;&#30340;&#31038;&#20132;&#32593;&#32476;&#20013;&#21487;&#33021;&#20855;&#26377;&#30340;&#21508;&#31181;&#29305;&#24449;&#21644;&#35266;&#28857;&#12290;&#24403;&#21069;&#30340;&#32858;&#31867;&#26041;&#27861;&#24456;&#38590;&#22788;&#29702;&#36825;&#31181;&#22797;&#26434;&#24615;&#65306;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#36817;&#20284;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#19981;&#33021;&#23558;&#20854;&#26174;&#24335;&#22320;&#20316;&#20026;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#36825;&#20010;&#30450;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#25968;&#20540;&#33410;&#28857;&#23646;&#24615;&#20043;&#38388;&#30340;&#32593;&#32476;&#36317;&#31163;&#26469;&#21019;&#24314;&#32593;&#32476;&#24863;&#30693;&#23884;&#20837;&#65292;&#36890;&#36807;&#24191;&#20041;&#27431;&#27663;&#36317;&#31163;&#35745;&#31639;&#12290;&#19982;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#25152;&#26377;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#26159;&#23545;&#32593;&#32476;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#32780;&#26159;&#23545;&#20854;&#33410;&#28857;&#23646;&#24615;&#36827;&#34892;&#32858;&#31867;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data clustering, the task of grouping observations according to their similarity, is a key component of unsupervised learning -- with real world applications in diverse fields such as biology, medicine, and social science. Often in these fields the data comes with complex interdependencies between the dimensions of analysis, for instance the various characteristics and opinions people can have live on a complex social network. Current clustering methods are ill-suited to tackle this complexity: deep learning can approximate these dependencies, but not take their explicit map as the input of the analysis. In this paper, we aim at fixing this blind spot in the unsupervised learning literature. We can create network-aware embeddings by estimating the network distance between numeric node attributes via the generalized Euclidean distance. Differently from all methods in the literature that we know of, we do not cluster the nodes of the network, but rather its node attributes. In our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22240;&#26524;&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#65292;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.10399</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#21033;&#29992;&#22240;&#26524;&#20449;&#21495;&#30340;&#30740;&#31350;&#65306;&#19968;&#39033;&#24102;&#26377;&#23454;&#35777;&#32467;&#26524;&#30340;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results. (arXiv:2309.10399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22240;&#26524;&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#65292;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#21307;&#23398;&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22330;&#26223;&#20013;&#30340;&#24369;&#22240;&#26524;&#20449;&#21495;&#26469;&#24314;&#27169;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#21644;&#22240;&#26524;&#22240;&#23376;&#25552;&#21462;&#27169;&#22359;&#12290;&#21518;&#32773;&#35745;&#31639;&#29305;&#24449;&#22270;&#30340;&#26435;&#37325;&#65292;&#26681;&#25454;&#20854;&#23545;&#22270;&#20687;&#22330;&#26223;&#30340;&#22240;&#26524;&#24433;&#21709;&#22686;&#24378;&#27599;&#20010;&#29305;&#24449;&#22270;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#22806;&#37096;&#20449;&#21495;&#26469;&#20462;&#25913;&#22240;&#26524;&#27169;&#22359;&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#33719;&#24471;&#25105;&#20204;&#26041;&#27861;&#30340;&#19981;&#21516;&#21464;&#20307;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#23454;&#39564;&#12289;&#23450;&#24615;&#35780;&#20272;&#21644;&#21066;&#24369;&#23454;&#39564;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23545;&#21069;&#21015;&#33146;MRI&#22270;&#20687;&#36827;&#34892;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#36825;&#22312;&#21307;&#30103;&#22270;&#20687;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method for automatically classifying medical images that uses weak causal signals in the scene to model how the presence of a feature in one part of the image affects the appearance of another feature in a different part of the image. Our method consists of two components: a convolutional neural network backbone and a causality-factors extractor module. The latter computes weights for the feature maps to enhance each feature map according to its causal influence in the image's scene. We can modify the functioning of the causality module by using two external signals, thus obtaining different variants of our method. We evaluate our method on a public dataset of prostate MRI images for prostate cancer diagnosis, using quantitative experiments, qualitative assessment, and ablation studies. Our results show that our method improves classification performance and produces more robust predictions, focusing on relevant parts of the image. That is especially important in medic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#38382;&#21367;&#30340;&#35774;&#35745;&#19982;&#24212;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#31616;&#21270;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30149;&#20154;&#25968;&#25454;&#30340;&#24405;&#20837;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#21487;&#29992;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10398</link><description>&lt;p&gt;
&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#31995;&#32479;&#20013;&#36866;&#24212;&#24615;&#38382;&#21367;&#30340;&#35774;&#35745;&#19982;&#24212;&#29992;&#65306;&#20197;STOPP/START v2&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Adaptive questionnaires for facilitating patient data entry in clinical decision support systems: Methods and application to STOPP/START v2. (arXiv:2309.10398v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#38382;&#21367;&#30340;&#35774;&#35745;&#19982;&#24212;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#31616;&#21270;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30149;&#20154;&#25968;&#25454;&#30340;&#24405;&#20837;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#21487;&#29992;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26159;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#21307;&#30103;&#20915;&#31574;&#30340;&#36719;&#20214;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#21307;&#29983;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#25509;&#21463;&#31243;&#24230;&#36890;&#24120;&#36739;&#20302;&#12290;&#24050;&#30693;&#30340;&#38382;&#39064;&#26159;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#25163;&#21160;&#36755;&#20837;&#22823;&#37327;&#30149;&#20154;&#25968;&#25454;&#65292;&#32780;&#36825;&#19968;&#36807;&#31243;&#26082;&#20887;&#38271;&#21448;&#32321;&#29712;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#25968;&#25454;&#65292;&#23578;&#26410;&#23436;&#20840;&#28385;&#36275;&#35201;&#27714;&#65292;&#22240;&#20026;&#25968;&#25454;&#36136;&#37327;&#21644;&#21487;&#29992;&#24615;&#36739;&#20302;&#12290;&#23454;&#38469;&#19978;&#65292;&#35768;&#22810;&#31995;&#32479;&#20173;&#28982;&#21253;&#25324;&#38656;&#35201;&#22635;&#20889;&#38271;&#38382;&#21367;&#30340;&#25968;&#25454;&#36755;&#20837;&#26041;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30149;&#20154;&#25968;&#25454;&#36755;&#20837;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#36866;&#24212;&#24615;&#38382;&#21367;&#65292;&#21363;&#22312;&#29992;&#25143;&#20132;&#20114;&#36807;&#31243;&#20013;&#21160;&#24577;&#26174;&#31034;&#25110;&#38544;&#34255;&#38382;&#39064;&#30340;&#38382;&#21367;&#12290;&#38024;&#23545;&#22522;&#20110;&#35268;&#21017;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23558;&#31995;&#32479;&#30340;&#20020;&#24202;&#35268;&#21017;&#36716;&#21270;&#20026;&#26174;&#31034;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#22312;&#38382;&#21367;&#20013;&#26174;&#31034;&#30340;&#39033;&#30446;&#65292;&#24182;&#35774;&#35745;&#20102;&#30830;&#23450;&#39033;&#30446;&#20248;&#20808;&#39034;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical decision support systems are software tools that help clinicians to make medical decisions. However, their acceptance by clinicians is usually rather low. A known problem is that they often require clinicians to manually enter lots of patient data, which is long and tedious. Existing solutions, such as the automatic data extraction from electronic health record, are not fully satisfying, because of low data quality and availability. In practice, many systems still include long questionnaire for data entry.  In this paper, we propose an original solution to simplify patient data entry, using an adaptive questionnaire, i.e. a questionnaire that evolves during user interaction, showing or hiding questions dynamically. Considering a rule-based decision support systems, we designed methods for translating the system's clinical rules into display rules that determine the items to show in the questionnaire, and methods for determining the optimal order of priority among the items in 
&lt;/p&gt;</description></item><item><title>COLA&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#22270;&#25193;&#22686;&#26469;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#20351;&#24471;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#24555;&#36895;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.10376</link><description>&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#19982;&#22270;&#20803;&#23398;&#20064;&#30456;&#36935;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20219;&#21153;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks. (arXiv:2309.10376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10376
&lt;/p&gt;
&lt;p&gt;
COLA&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#22270;&#25193;&#22686;&#26469;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#20351;&#24471;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#24555;&#36895;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#24212;&#29992;&#26159;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#36981;&#24490;&#20803;&#23398;&#20064;&#33539;&#24335;&#65292;&#23637;&#31034;&#20102;&#23545;&#23569;&#26679;&#26412;&#20219;&#21153;&#24555;&#36895;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#24494;&#35843;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#32972;&#21518;&#21407;&#22240;&#30340;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#30456;&#23545;&#20110;&#20803;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65292;&#21253;&#25324;&#65288;1&#65289;&#20840;&#38754;&#21033;&#29992;&#22270;&#33410;&#28857;&#21644;&#65288;2&#65289;&#22270;&#25193;&#22686;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23558;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#20248;&#21183;&#32467;&#21512;&#21040;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65306;&#23545;&#27604;&#24335;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#65288;COLA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;COLA&#21033;&#29992;&#22270;&#25193;&#22686;&#26469;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#20351;&#24471;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become popular in Graph Representation Learning (GRL). One fundamental application is few-shot node classification. Most existing methods follow the meta learning paradigm, showing the ability of fast generalization to few-shot tasks. However, recent works indicate that graph contrastive learning combined with fine-tuning can significantly outperform meta learning methods. Despite the empirical success, there is limited understanding of the reasons behind it. In our study, we first identify two crucial advantages of contrastive learning compared to meta learning, including (1) the comprehensive utilization of graph nodes and (2) the power of graph augmentations. To integrate the strength of both contrastive learning and meta learning on the few-shot node classification tasks, we introduce a new paradigm: Contrastive Few-Shot Node Classification (COLA). Specifically, COLA employs graph augmentations to identify semantically similar nodes, which enables 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#23519;&#20102;&#29983;&#25104;&#24335;AI&#65288;LLMs&#65289;&#20316;&#20026;&#35748;&#30693;&#31995;&#32479;&#30340;&#20248;&#21183;&#19982;&#21155;&#21183;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#24369;&#28857;&#21487;&#20197;&#36861;&#28335;&#21040;&#20854;&#22522;&#26412;&#35748;&#30693;&#26550;&#26500;&#30340;&#32570;&#38519;&#12290;&#36880;&#27493;&#25913;&#36827;LLMs&#24182;&#19981;&#33021;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;AGI&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21644;&#23454;&#39564;LLMs&#20173;&#28982;&#26377;&#21161;&#20110;&#29702;&#35299;&#20154;&#31867;&#32423;AGI&#65292;&#24182;&#19988;LLMs&#21487;&#20197;&#25104;&#20026;&#20154;&#31867;&#32423;AGI&#26550;&#26500;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.10371</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#19982;AGI&#65306;&#29616;&#20195;LLMs&#30340;&#35748;&#30693;&#20248;&#21183;&#19982;&#21155;&#21183;
&lt;/p&gt;
&lt;p&gt;
Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs. (arXiv:2309.10371v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#23519;&#20102;&#29983;&#25104;&#24335;AI&#65288;LLMs&#65289;&#20316;&#20026;&#35748;&#30693;&#31995;&#32479;&#30340;&#20248;&#21183;&#19982;&#21155;&#21183;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#24369;&#28857;&#21487;&#20197;&#36861;&#28335;&#21040;&#20854;&#22522;&#26412;&#35748;&#30693;&#26550;&#26500;&#30340;&#32570;&#38519;&#12290;&#36880;&#27493;&#25913;&#36827;LLMs&#24182;&#19981;&#33021;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;AGI&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21644;&#23454;&#39564;LLMs&#20173;&#28982;&#26377;&#21161;&#20110;&#29702;&#35299;&#20154;&#31867;&#32423;AGI&#65292;&#24182;&#19988;LLMs&#21487;&#20197;&#25104;&#20026;&#20154;&#31867;&#32423;AGI&#26550;&#26500;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20114;&#21160;LLMs&#20316;&#20026;&#35748;&#30693;&#31995;&#32479;&#36827;&#34892;&#20102;&#36866;&#24230;&#35814;&#32454;&#30340;&#32771;&#23519;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;2023&#24180;&#24038;&#21491;&#30340;LLMs&#65292;&#22914;ChatGPT&#12289;GPT-4&#12289;Bard&#12289;Llama&#31561;&#12290;&#22238;&#39038;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#35748;&#30693;&#20248;&#21183;&#65292;&#28982;&#21518;&#20180;&#32454;&#30740;&#31350;&#20102;&#36825;&#20123;LLMs&#19982;&#20154;&#31867;&#35748;&#30693;&#31995;&#32479;&#20043;&#38388;&#30340;&#23454;&#36136;&#24615;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;AI&#31995;&#32479;&#30340;&#35768;&#22810;&#23454;&#38469;&#24369;&#28857;&#21487;&#20197;&#20855;&#20307;&#24402;&#22240;&#20110;&#36825;&#20123;&#31995;&#32479;&#26500;&#24314;&#25152;&#20381;&#36182;&#30340;&#22522;&#26412;&#35748;&#30693;&#26550;&#26500;&#30340;&#32570;&#38519;&#12290;&#35770;&#25991;&#35748;&#20026;&#65292;&#22312;&#21487;&#23454;&#29616;&#30340;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#33539;&#22260;&#20869;&#65292;&#36880;&#27493;&#25913;&#36827;&#27492;&#31867;LLMs&#24182;&#19981;&#26159;&#26397;&#30528;&#36798;&#21040;&#20154;&#31867;&#32423;AGI&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#36825;&#24182;&#19981;&#24847;&#21619;&#30528;&#22312;&#30740;&#31350;&#21644;&#23454;&#39564;LLMs&#26041;&#38754;&#19981;&#33021;&#20174;&#20013;&#33719;&#24471;&#20851;&#20110;&#20154;&#31867;&#32423;AGI&#30340;&#32463;&#39564;&#65292;&#20063;&#19981;&#24847;&#21619;&#30528;LLMs&#19981;&#33021;&#25104;&#20026;&#21253;&#21547;&#20854;&#20182;&#24819;&#27861;&#30340;&#20154;&#31867;&#32423;AGI&#26550;&#26500;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#31038;&#20250;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are. It is found that many of the practical weaknesses of these AI systems can be tied specifically to lacks in the basic cognitive architectures according to which these systems are built. It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources. This does not imply there is nothing to learn about human-level AGI from studying and experimenting with LLMs, nor that LLMs cannot form significant parts of human-level AGI architectures that also incorporate other ideas. Social and ethical matters 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.10370</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#19968;&#20010;&#38544;&#34255;&#23618;&#12289;&#19968;&#20010;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#12289;&#19968;&#20010;${\mathcal L}^2$&#35889;&#33539;&#31867;&#65288;&#25110;&#32773;Hilbert-Schmidt&#65289;&#30340;&#20195;&#20215;&#20989;&#25968;&#12289;&#36755;&#20837;&#31354;&#38388;${\mathbb R}^M$&#12289;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#65288;&#20854;&#20013;$Q\leq M$&#65289;&#65292;&#20197;&#21450;&#35757;&#32451;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;$N&gt;QM$&#25152;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;$O(\delta_P)$&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$\delta_P$&#34913;&#37327;&#20102;&#35757;&#32451;&#36755;&#20837;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24212;&#20110;&#23646;&#20110;&#21516;&#19968;&#36755;&#20986;&#21521;&#37327;$y_j$&#30340;&#35757;&#32451;&#36755;&#20837;&#21521;&#37327;$\overline{x_{0,j}}$&#30340;&#25237;&#24433;&#26469;&#33719;&#24471;&#36817;&#20284;&#30340;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;$j=1,\dots,Q$&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;$M=Q$&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#30830;&#23450;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#19968;&#20010;&#30830;&#20999;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#65307;&#36825;&#20010;&#23574;&#38160;&#30340;&#20540;&#19982;&#23545;&#20110;$Q\leq M$&#25152;&#33719;&#24471;&#30340;&#19978;&#30028;&#20043;&#38388;&#26377;&#19968;&#20010;&#30456;&#23545;&#35823;&#24046;$O(\delta_P^2)$&#12290;&#19978;&#30028;&#35777;&#26126;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26500;&#36896;&#24615;&#35757;&#32451;&#30340;&#32593;&#32476;&#65307;&#25105;&#20204;&#35777;&#26126;&#23427;&#27979;&#24230;&#20102;$Q$&#32500;&#31354;&#38388;&#20013;&#30340;&#32473;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N&gt;QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#12289;&#20197;&#21450;&#20943;&#23569;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#65292;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#36793;&#32536;&#33410;&#28857;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10367</link><description>&lt;p&gt;
&#36793;&#32536;&#33410;&#28857;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Toward efficient resource utilization at edge nodes in federated learning. (arXiv:2309.10367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#12289;&#20197;&#21450;&#20943;&#23569;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#65292;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#36793;&#32536;&#33410;&#28857;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#36793;&#32536;&#33410;&#28857;&#33021;&#22815;&#20849;&#21516;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20182;&#20204;&#30340;&#25968;&#25454;&#12290;&#36825;&#26159;&#36890;&#36807;&#35774;&#22791;&#35745;&#31639;&#26412;&#22320;&#31169;&#26377;&#27169;&#22411;&#26356;&#26032;&#65292;&#28982;&#21518;&#30001;&#26381;&#21153;&#22120;&#36827;&#34892;&#32858;&#21512;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#21644;&#32593;&#32476;&#36890;&#20449;&#23545;&#20110;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#36739;&#22823;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#25104;&#20026;&#20005;&#37325;&#29942;&#39048;&#12290;&#36793;&#32536;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#26377;&#38480;&#30340;&#30828;&#20214;&#36164;&#28304;&#65288;RAM&#12289;CPU&#65289;&#65292;&#32780;&#36793;&#32536;&#30340;&#32593;&#32476;&#24102;&#23485;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#25193;&#23637;&#32852;&#37030;&#36710;&#38431;&#24212;&#29992;&#26469;&#35828;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;FL&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#65292;&#20197;&#21450;&#27599;&#20010;&#20840;&#23616;&#35757;&#32451;&#36718;&#27425;&#20013;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#12290;&#23545;&#20110;&#27599;&#20010;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#35201;&#35757;&#32451;&#30340;&#23618;&#65292;&#20923;&#32467;&#27169;&#22411;&#30340;&#20854;&#20313;&#37096;&#20998;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25490;&#38500;&#25152;&#26377;&#26410;&#35757;&#32451;&#30340;&#37096;&#20998;&#26469;&#20943;&#23569;&#27599;&#36718;&#30340;&#26381;&#21153;&#22120;&#36127;&#36733;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tend to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning in order to reduce resource utilization on devices, as well as the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. In doing so, we can reduce both server load and communication costs per round by excluding all untrained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36974;&#25377;&#24863;&#30693;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#22120;OccluTrack&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#24120;&#36816;&#21160;&#25233;&#21046;&#26426;&#21046;&#12289;&#23039;&#21183;&#23548;&#21521;&#30340;&#20877;&#35782;&#21035;&#27169;&#22359;&#21644;&#20840;&#23616;&#30456;&#20851;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#20013;&#36974;&#25377;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10360</link><description>&lt;p&gt;
OccluTrack: &#37325;&#26032;&#24605;&#32771;&#22686;&#24378;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#20013;&#23545;&#36974;&#25377;&#24863;&#30693;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OccluTrack: Rethinking Awareness of Occlusion for Enhancing Multiple Pedestrian Tracking. (arXiv:2309.10360v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36974;&#25377;&#24863;&#30693;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#22120;OccluTrack&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#24120;&#36816;&#21160;&#25233;&#21046;&#26426;&#21046;&#12289;&#23039;&#21183;&#23548;&#21521;&#30340;&#20877;&#35782;&#21035;&#27169;&#22359;&#21644;&#20840;&#23616;&#30456;&#20851;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#20013;&#36974;&#25377;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#22312;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#36974;&#25377;&#24773;&#20917;&#19979;&#30001;&#20110;&#19981;&#20934;&#30830;&#30340;&#36816;&#21160;&#20272;&#35745;&#12289;&#22806;&#35266;&#29305;&#24449;&#25552;&#21462;&#21644;&#20851;&#32852;&#32780;&#21463;&#21040;&#24433;&#21709;&#65292;&#23548;&#33268;&#36523;&#20221;F1&#24471;&#20998;&#65288;IDF1&#65289;&#19981;&#22815;&#20934;&#30830;&#65292;ID&#20999;&#25442;&#36807;&#22810;&#65288;IDSw&#65289;&#65292;&#20197;&#21450;&#20851;&#32852;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#65288;AssA&#21644;AssR&#65289;&#19981;&#36275;&#12290;&#25105;&#20204;&#21457;&#29616;&#20027;&#35201;&#21407;&#22240;&#26159;&#37096;&#20998;&#36974;&#25377;&#24341;&#36215;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#26412;&#25991;&#35748;&#20026;&#26126;&#30830;&#30340;&#36816;&#21160;&#20272;&#35745;&#12289;&#21487;&#38752;&#30340;&#22806;&#35266;&#29305;&#24449;&#21644;&#20844;&#24179;&#30340;&#20851;&#32852;&#26159;&#35299;&#20915;&#36974;&#25377;&#22330;&#26223;&#19979;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36974;&#25377;&#24863;&#30693;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#22120;OccluTrack&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24322;&#24120;&#36816;&#21160;&#25233;&#21046;&#26426;&#21046;&#24341;&#20837;&#21040;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#26816;&#27979;&#21644;&#25233;&#21046;&#37096;&#20998;&#36974;&#25377;&#24341;&#36215;&#30340;&#24322;&#24120;&#36816;&#21160;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23039;&#21183;&#23548;&#21521;&#30340;&#20877;&#35782;&#21035;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#37096;&#20998;&#36974;&#25377;&#34892;&#20154;&#30340;&#21028;&#21035;&#24615;&#37096;&#20998;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;&#30456;&#20851;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#36974;&#25377;&#22330;&#26223;&#19979;&#30340;&#20851;&#32852;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple pedestrian tracking faces the challenge of tracking pedestrians in the presence of occlusion. Existing methods suffer from inaccurate motion estimation, appearance feature extraction, and association due to occlusion, leading to inadequate Identification F1-Score (IDF1), excessive ID switches (IDSw), and insufficient association accuracy and recall (AssA and AssR). We found that the main reason is abnormal detections caused by partial occlusion. In this paper, we suggest that the key insight is explicit motion estimation, reliable appearance features, and fair association in occlusion scenes. Specifically, we propose an adaptive occlusion-aware multiple pedestrian tracker, OccluTrack. We first introduce an abnormal motion suppression mechanism into the Kalman Filter to adaptively detect and suppress outlier motions caused by partial occlusion. Second, we propose a pose-guided re-ID module to extract discriminative part features for partially occluded pedestrians. Last, we desi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20855;&#22791;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#20284;&#30340;&#24110;&#21161;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10346</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#26234;&#33021;&#20307;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Explaining Agent Behavior with Large Language Models. (arXiv:2309.10346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10346
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20855;&#22791;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#20284;&#30340;&#24110;&#21161;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#22914;&#26426;&#22120;&#20154;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#33021;&#22815;&#21521;&#20154;&#31867;&#23545;&#31561;&#20307;&#35299;&#37322;&#20182;&#20204;&#20915;&#31574;&#32972;&#21518;&#30340;&#25512;&#29702;&#65292;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#34892;&#20026;&#36890;&#24120;&#26159;&#30001;&#19981;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#35266;&#23519;&#65292;&#19981;&#32771;&#34385;&#24213;&#23618;&#27169;&#22411;&#34920;&#31034;&#65292;&#29983;&#25104;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#31616;&#27905;&#34920;&#31034;&#65292;&#24182;&#29992;&#20854;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#19982;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#20132;&#20114;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21644;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#29983;&#25104;&#30340;&#35299;&#37322;&#19968;&#26679;&#26377;&#29992;&#65292;&#21516;&#26102;&#20855;&#22791;&#26377;&#30410;&#30340;&#20132;&#20114;&#65292;&#22914;&#28548;&#28165;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts, however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation. We show how a compact representation of the agent's behavior can be learned and used to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those generated by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedWOA&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#20174;&#23478;&#24237;&#33021;&#28304;&#25968;&#25454;&#35757;&#32451;&#30340;&#23616;&#37096;LTSM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#32858;&#21512;&#20986;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#23548;&#33268;&#30340;&#39044;&#27979;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10337</link><description>&lt;p&gt;
FedWOA:&#19968;&#31181;&#21033;&#29992;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#21487;&#20877;&#29983;&#33021;&#28304;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FedWOA: A Federated Learning Model that uses the Whale Optimization Algorithm for Renewable Energy Prediction. (arXiv:2309.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedWOA&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#20174;&#23478;&#24237;&#33021;&#28304;&#25968;&#25454;&#35757;&#32451;&#30340;&#23616;&#37096;LTSM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#32858;&#21512;&#20986;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#23548;&#33268;&#30340;&#39044;&#27979;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#22312;&#22788;&#29702;&#25935;&#24863;&#20010;&#20154;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#33021;&#28304;&#39046;&#22495;&#65292;&#35775;&#38382;&#23478;&#24237;&#29983;&#20135;&#32773;&#21644;&#28040;&#36153;&#32773;&#30340;&#33021;&#28304;&#25968;&#25454;&#23545;&#20110;&#33021;&#28304;&#39044;&#27979;&#20197;&#25903;&#25345;&#33021;&#28304;&#32593;&#26684;&#31649;&#29702;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#22823;&#35268;&#27169;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#20844;&#27665;&#36890;&#24120;&#19981;&#24895;&#24847;&#25480;&#20104;&#22522;&#20110;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#12290;&#32852;&#37030;&#23398;&#20064;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#38544;&#31169;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#21457;&#30005;&#27169;&#24335;&#30340;&#21464;&#21270;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#23548;&#33268;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#30340;&#29983;&#25104;&#23384;&#22312;&#38382;&#39064;&#65292;&#36827;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;FedWOA&#36825;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#20174;&#22522;&#20110;&#23478;&#24237;&#33021;&#28304;&#25968;&#25454;&#35757;&#32451;&#30340;&#23616;&#37096;LTSM&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#32858;&#21512;&#20986;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#35782;&#21035;&#20986;&#26368;&#20248;&#30340;&#26435;&#37325;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy is important when dealing with sensitive personal information in machine learning models, which require large data sets for training. In the energy field, access to household prosumer energy data is crucial for energy predictions to support energy grid management and large-scale adoption of renewables however citizens are often hesitant to grant access to cloud-based machine learning models. Federated learning has been proposed as a solution to privacy challenges however report issues in generating the global prediction model due to data heterogeneity, variations in generation patterns, and the high number of parameters leading to even lower prediction accuracy. This paper addresses these challenges by introducing FedWOA a novel federated learning model that employs the Whale Optimization Algorithm to aggregate global prediction models from the weights of local LTSM neural network models trained on prosumer energy data. The proposed solution identifies the optimal vector of wei
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20108;&#32500;&#19981;&#35268;&#21017;&#24418;&#29366;&#35013;&#31665;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#21644;&#20998;&#32452;UV&#36148;&#22270;&#34917;&#19969;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#35013;&#31665;&#38382;&#39064;&#24182;&#20351;&#29992;&#32852;&#21512;&#20248;&#21270;&#26469;&#25552;&#39640;&#35013;&#31665;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2309.10329</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#20108;&#32500;&#19981;&#35268;&#21017;&#24418;&#29366;&#35013;&#31665;
&lt;/p&gt;
&lt;p&gt;
Learning based 2D Irregular Shape Packing. (arXiv:2309.10329v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10329
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20108;&#32500;&#19981;&#35268;&#21017;&#24418;&#29366;&#35013;&#31665;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#21644;&#20998;&#32452;UV&#36148;&#22270;&#34917;&#19969;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#35013;&#31665;&#38382;&#39064;&#24182;&#20351;&#29992;&#32852;&#21512;&#20248;&#21270;&#26469;&#25552;&#39640;&#35013;&#31665;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#32500;&#19981;&#35268;&#21017;&#24418;&#29366;&#35013;&#31665;&#26159;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#22312;&#32441;&#29702;&#22270;&#38598;&#20869;&#25490;&#21015;3D&#27169;&#22411;&#30340;UV&#36148;&#22270;&#34917;&#19969;&#20197;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#28210;&#26579;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#36825;&#20010;&#38382;&#39064;&#26159;&#19968;&#20010;&#32852;&#21512;&#30340;&#12289;&#32452;&#21512;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#28041;&#21450;&#25152;&#26377;&#34917;&#19969;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#65292;&#24050;&#30693;&#20854;NP&#38590;&#24230;&#12290;&#20808;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20551;&#35774;&#21551;&#21457;&#24335;&#30340;&#25490;&#21015;&#39034;&#24207;&#65292;&#35201;&#20040;&#20462;&#25913;&#19978;&#28216;&#30340;&#32593;&#26684;&#20999;&#21106;&#21644;UV&#26144;&#23556;&#20197;&#31616;&#21270;&#38382;&#39064;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38480;&#21046;&#20102;&#35013;&#31665;&#27604;&#20363;&#25110;&#24341;&#36215;&#20102;&#40065;&#26834;&#24615;&#25110;&#36890;&#29992;&#24615;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20108;&#32500;&#19981;&#35268;&#21017;&#24418;&#29366;&#35013;&#31665;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#36755;&#20837;&#35201;&#27714;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35013;&#31665;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#22320;&#23558;&#37096;&#20998;UV&#36148;&#22270;&#34917;&#19969;&#36873;&#25321;&#21644;&#20998;&#32452;&#25104;&#36817;&#30697;&#24418;&#30340;&#36229;&#32423;&#34917;&#19969;&#65292;&#20174;&#32780;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#35013;&#31665;&#38382;&#39064;&#65292;&#25509;&#30528;&#20351;&#29992;&#32852;&#21512;&#20248;&#21270;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#35013;&#31665;&#27604;&#20363;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;&#21253;&#21547;&#25968;&#30334;&#20010;&#34917;&#19969;&#30340;&#22823;&#22411;&#38382;&#39064;&#23454;&#20363;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;&#21551;&#21457;&#24335;&#36873;&#25321;&#21644;&#20998;&#32452;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
2D irregular shape packing is a necessary step to arrange UV patches of a 3D model within a texture atlas for memory-efficient appearance rendering in computer graphics. Being a joint, combinatorial decision-making problem involving all patch positions and orientations, this problem has well-known NP-hard complexity. Prior solutions either assume a heuristic packing order or modify the upstream mesh cut and UV mapping to simplify the problem, which either limits the packing ratio or incurs robustness or generality issues. Instead, we introduce a learning-assisted 2D irregular shape packing method that achieves a high packing quality with minimal requirements from the input. Our method iteratively selects and groups subsets of UV patches into near-rectangular super patches, essentially reducing the problem to bin-packing, based on which a joint optimization is employed to further improve the packing ratio. In order to efficiently deal with large problem instances with hundreds of patche
&lt;/p&gt;</description></item><item><title>QASnowball&#26159;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#12290;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.10326</link><description>&lt;p&gt;
QASnowball: &#19968;&#20010;&#29992;&#20110;&#39640;&#36136;&#37327;&#38382;&#31572;&#25968;&#25454;&#29983;&#25104;&#30340;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10326
&lt;/p&gt;
&lt;p&gt;
QASnowball&#26159;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#12290;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#20854;&#22312;&#24212;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#31283;&#23450;&#30340;QA&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;QASnowball&#65292;&#29992;&#20110;QA&#25968;&#25454;&#22686;&#24378;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#36845;&#20195;&#22320;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;QASnowball&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#22238;&#31572;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#20174;&#26080;&#26631;&#31614;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20505;&#36873;&#31572;&#26696;&#30340;&#26680;&#24515;&#30701;&#35821;&#65307;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#26681;&#25454;&#25991;&#26723;&#21644;&#20505;&#36873;&#31572;&#26696;&#29983;&#25104;&#38382;&#39064;&#65307;QA&#25968;&#25454;&#36807;&#28388;&#22120;&#65292;&#29992;&#20110;&#36807;&#28388;&#20986;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;QASnowball&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#31181;&#23376;&#38598;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#65292;&#20174;&#32780;&#19981;&#26029;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the success of question answering (QA), especially its potential to be a foundation paradigm for tackling diverse NLP tasks. However, obtaining sufficient data to build an effective and stable QA system still remains an open problem. For this problem, we introduce an iterative bootstrapping framework for QA data augmentation (named QASnowball), which can iteratively generate large-scale high-quality QA data based on a seed set of supervised examples. Specifically, QASnowball consists of three modules, an answer extractor to extract core phrases in unlabeled documents as candidate answers, a question generator to generate questions based on documents and candidate answers, and a QA data filter to filter out high-quality QA data. Moreover, QASnowball can be self-enhanced by reseeding the seed set to fine-tune itself in different iterations, leading to continual improvements in the generation quality. We conduct experiments in the high-resource English scenario
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#27169;&#24577;&#38598;&#25104;&#38477;&#32500;&#31639;&#27861;&#21644;&#20998;&#31867;&#31639;&#27861;&#26469;&#39044;&#27979;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#65292;&#25552;&#39640;&#36716;&#31227;&#24615;&#30284;&#30151;&#30340;&#27491;&#30830;&#35782;&#21035;&#65292;&#24182;&#33410;&#30465;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.10324</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#38598;&#25104;&#38477;&#32500;&#31639;&#27861;&#21644;&#20998;&#31867;&#31639;&#27861;&#23545;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#36827;&#34892;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Metastatic Breast Cancer Prognostication Through Multimodal Integration of Dimensionality Reduction Algorithms and Classification Algorithms. (arXiv:2309.10324v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10324
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#38598;&#25104;&#38477;&#32500;&#31639;&#27861;&#21644;&#20998;&#31867;&#31639;&#27861;&#26469;&#39044;&#27979;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#65292;&#25552;&#39640;&#36716;&#31227;&#24615;&#30284;&#30151;&#30340;&#27491;&#30830;&#35782;&#21035;&#65292;&#24182;&#33410;&#30465;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#35745;&#31639;&#26426;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#24182;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#26816;&#27979;&#36716;&#31227;&#24615;&#30284;&#30151;&#12290;&#36716;&#31227;&#24615;&#30284;&#30151;&#26159;&#30284;&#30151;&#25193;&#25955;&#21040;&#36523;&#20307;&#20854;&#20182;&#37096;&#20301;&#30340;&#28857;&#65292;&#22823;&#32422;&#23548;&#33268;90%&#30340;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#12290;&#36890;&#24120;&#65292;&#30149;&#29702;&#23398;&#23478;&#27599;&#22825;&#35201;&#33457;&#20960;&#20010;&#23567;&#26102;&#25163;&#21160;&#20998;&#31867;&#32959;&#30244;&#26159;&#33391;&#24615;&#36824;&#26159;&#24694;&#24615;&#12290;&#36825;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#23548;&#33268;&#36716;&#31227;&#24615;&#32959;&#30244;&#30340;&#38169;&#35823;&#26631;&#35760;&#39640;&#36798;60%&#20197;&#19978;&#65292;&#24378;&#35843;&#20102;&#24847;&#35782;&#21040;&#20154;&#20026;&#38169;&#35823;&#21644;&#20854;&#20182;&#20302;&#25928;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#25552;&#39640;&#36716;&#31227;&#24615;&#30284;&#30151;&#27491;&#30830;&#35782;&#21035;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#33021;&#22815;&#25405;&#25937;&#25104;&#21315;&#19978;&#19975;&#30340;&#29983;&#21629;&#65292;&#36824;&#33021;&#25552;&#39640;&#36807;&#31243;&#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#65292;&#20174;&#32780;&#33410;&#30465;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#30740;&#31350;&#20013;&#26816;&#27979;&#30284;&#30151;&#12290;&#26412;&#30740;&#31350;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#23450;&#20351;&#29992;&#39044;&#22788;&#29702;&#31639;&#27861;&#19982;&#20998;&#31867;&#31639;&#27861;&#32467;&#21512;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is a branch of Artificial Intelligence (AI) where computers analyze data and find patterns in the data. The study focuses on the detection of metastatic cancer using ML. Metastatic cancer is the point where the cancer has spread to other parts of the body and is the cause of approximately 90% of cancer related deaths. Normally, pathologists spend hours each day to manually classify whether tumors are benign or malignant. This tedious task contributes to mislabeling metastasis being over 60% of time and emphasizes the importance to be aware of human error, and other inefficiencies. ML is a good candidate to improve the correct identification of metastatic cancer saving thousands of lives and can also improve the speed and efficiency of the process thereby taking less resources and time. So far, deep learning methodology of AI has been used in the research to detect cancer. This study is a novel approach to determine the potential of using preprocessing algorithms c
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20449;&#20219;&#19981;&#20165;&#21462;&#20915;&#20110;&#31995;&#32479;&#26412;&#36523;&#65292;&#36824;&#21253;&#25324;&#23545;&#24320;&#21457;&#32773;&#30340;&#20449;&#20219;&#12290;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#22914;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#23545;&#29992;&#25143;&#30340;&#20449;&#20219;&#24433;&#21709;&#23578;&#19981;&#28165;&#26224;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24212;&#34987;&#35748;&#35782;&#20026;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#65292;&#20154;&#19982;&#31995;&#32479;&#21516;&#26679;&#37325;&#35201;&#26469;&#30830;&#23450;&#20854;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.10318</link><description>&lt;p&gt;
&#35841;&#21487;&#20197;&#20449;&#20219;&#65292;&#22914;&#20309;&#20197;&#21450;&#20026;&#20160;&#20040;&#65306;&#26803;&#29702;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#12289;&#21487;&#20449;&#24230;&#21644;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Who to Trust, How and Why: Untangling AI Ethics Principles, Trustworthiness and Trust. (arXiv:2309.10318v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10318
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20449;&#20219;&#19981;&#20165;&#21462;&#20915;&#20110;&#31995;&#32479;&#26412;&#36523;&#65292;&#36824;&#21253;&#25324;&#23545;&#24320;&#21457;&#32773;&#30340;&#20449;&#20219;&#12290;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#22914;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#23545;&#29992;&#25143;&#30340;&#20449;&#20219;&#24433;&#21709;&#23578;&#19981;&#28165;&#26224;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24212;&#34987;&#35748;&#35782;&#20026;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#65292;&#20154;&#19982;&#31995;&#32479;&#21516;&#26679;&#37325;&#35201;&#26469;&#30830;&#23450;&#20854;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27010;&#36848;&#20102;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#20449;&#20219;&#21644;&#21487;&#20449;&#24230;&#30340;&#25991;&#29486;&#65292;&#24182;&#20027;&#24352;&#38656;&#35201;&#26356;&#28165;&#26224;&#22320;&#21306;&#20998;&#36825;&#20123;&#27010;&#24565;&#65292;&#24182;&#25910;&#38598;&#26356;&#22810;&#20851;&#20110;&#20154;&#20204;&#20449;&#20219;&#34892;&#20026;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#19981;&#20165;&#28041;&#21450;&#23545;&#31995;&#32479;&#26412;&#36523;&#30340;&#20381;&#36182;&#65292;&#36824;&#21253;&#25324;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24320;&#21457;&#32773;&#30340;&#20449;&#20219;&#12290;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#65292;&#22914;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#32463;&#24120;&#34987;&#35748;&#20026;&#21487;&#20197;&#20419;&#36827;&#29992;&#25143;&#30340;&#20449;&#20219;&#65292;&#20294;&#36825;&#20123;&#29305;&#24615;&#23454;&#38469;&#19978;&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#23545;&#31995;&#32479;&#21487;&#20449;&#24230;&#30340;&#24863;&#30693;&#30340;&#32463;&#39564;&#35777;&#25454;&#24182;&#19981;&#20016;&#23500;&#25110;&#32773;&#19981;&#37027;&#20040;&#26126;&#30830;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24212;&#35813;&#34987;&#35748;&#35782;&#20026;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#65292;&#20854;&#20013;&#21442;&#19982;&#35774;&#35745;&#12289;&#24320;&#21457;&#12289;&#37096;&#32626;&#21644;&#20351;&#29992;&#31995;&#32479;&#30340;&#20154;&#19982;&#31995;&#32479;&#26412;&#36523;&#19968;&#26679;&#37325;&#35201;&#65292;&#20197;&#30830;&#23450;&#31995;&#32479;&#26159;&#21542;&#20540;&#24471;&#20449;&#36182;&#12290;&#22914;&#26524;&#19981;&#35748;&#35782;&#21040;&#36825;&#20123;&#32454;&#24494;&#24046;&#21035;&#65292;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;&#21644;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#23601;&#26377;&#21487;&#33021;&#25104;&#20026;&#20219;&#20309;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#30410;&#29305;&#24615;&#30340;&#27169;&#31946;&#26415;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an overview of the literature on trust in AI and AI trustworthiness and argue for the need to distinguish these concepts more clearly and to gather more empirically evidence on what contributes to people s trusting behaviours. We discuss that trust in AI involves not only reliance on the system itself, but also trust in the developers of the AI system. AI ethics principles such as explainability and transparency are often assumed to promote user trust, but empirical evidence of how such features actually affect how users perceive the system s trustworthiness is not as abundance or not that clear. AI systems should be recognised as socio-technical systems, where the people involved in designing, developing, deploying, and using the system are as important as the system for determining whether it is trustworthy. Without recognising these nuances, trust in AI and trustworthy AI risk becoming nebulous terms for any desirable feature for AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#35821;&#38899;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10294</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#38899;PTM&#12289;&#25991;&#26412;LLM&#21644;&#24773;&#24863;TTS&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition. (arXiv:2309.10294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#35821;&#38899;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#25552;&#21319;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#39044;&#35757;&#32451;&#27169;&#22411;(PTM)&#12289;data2vec&#12289;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;GPT-4&#20197;&#21450;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;Azure TTS&#26469;&#25552;&#21319;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;(SER)&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21457;&#29616;data2vec&#22312;SER&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(GPT-4)&#21644;&#24773;&#24863;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#27169;&#22411;(Azure TTS)&#26469;&#29983;&#25104;&#24773;&#24863;&#19968;&#33268;&#30340;&#25991;&#26412;&#21644;&#35821;&#38899;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#25991;&#26412;&#25552;&#31034;&#21644;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#20197;&#33719;&#24471;&#36136;&#37327;&#39640;&#30340;&#21512;&#25104;&#24773;&#24863;&#35821;&#38899;&#25968;&#25454;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20419;&#36827;&#20351;&#29992;&#21512;&#25104;&#35821;&#38899;&#30340;SER&#20219;&#21153;&#65292;&#21253;&#25324;&#38543;&#26426;&#28151;&#21512;&#12289;&#23545;&#25239;&#35757;&#32451;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#12290;&#22312;IEMOCAP&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explored how to boost speech emotion recognition (SER) with the state-of-the-art speech pre-trained model (PTM), data2vec, text generation technique, GPT-4, and speech synthesis technique, Azure TTS. First, we investigated the representation ability of different speech self-supervised pre-trained models, and we found that data2vec has a good representation ability on the SER task. Second, we employed a powerful large language model (LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate emotionally congruent text and speech. We carefully designed the text prompt and dataset construction, to obtain the synthetic emotional speech data with high quality. Third, we studied different ways of data augmentation to promote the SER task with synthetic speech, including random mixing, adversarial training, transfer learning, and curriculum learning. Experiments and ablation studies on the IEMOCAP dataset demonstrate the effectiveness of our method, compar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QXAI&#26694;&#26550;&#65292;&#29992;&#20110;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#20915;&#31574;&#32773;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10293</link><description>&lt;p&gt;
QXAI&#65306;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#29992;&#20110;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems. (arXiv:2309.10293v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QXAI&#26694;&#26550;&#65292;&#29992;&#20110;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#20915;&#31574;&#32773;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21487;&#29992;&#20110;&#23545;&#24739;&#32773;&#30340;&#36523;&#20307;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#21644;&#39044;&#27979;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#30340;&#29983;&#21629;&#20307;&#24449;&#12290;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65289;&#30340;&#22238;&#24402;&#20998;&#26512;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#32780;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#21487;&#33021;&#38656;&#35201;&#20915;&#31574;&#32773;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26524;&#30450;&#30446;&#20915;&#31574;&#12290;&#22312;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#20013;&#65292;&#36319;&#36394;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#24739;&#32773;&#25968;&#25454;&#21644;&#20854;&#30456;&#20851;&#20020;&#24202;&#29305;&#24449;&#20316;&#20026;&#39044;&#27979;&#26410;&#26469;&#29983;&#21629;&#20307;&#24449;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#35299;&#37322;&#21508;&#31181;&#29305;&#24449;&#23545;&#30417;&#27979;&#24212;&#29992;&#25972;&#20307;&#36755;&#20986;&#30340;&#36129;&#29486;&#23545;&#20110;&#20020;&#24202;&#21307;&#24072;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;QXAI&#26694;&#26550;&#65292;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20107;&#21518;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;Shapley&#20540;&#36827;&#34892;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence techniques can be used to classify a patient's physical activities and predict vital signs for remote patient monitoring. Regression analysis based on non-linear models like deep learning models has limited explainability due to its black-box nature. This can require decision-makers to make blind leaps of faith based on non-linear model results, especially in healthcare applications. In non-invasive monitoring, patient data from tracking sensors and their predisposing clinical attributes act as input features for predicting future vital signs. Explaining the contributions of various features to the overall output of the monitoring application is critical for a clinician's decision-making. In this study, an Explainable AI for Quantitative analysis (QXAI) framework is proposed with post-hoc model explainability and intrinsic explainability for regression and classification tasks in a supervised learning approach. This was achieved by utilizing the Shapley values c
&lt;/p&gt;</description></item><item><title>Koopman&#21487;&#36870;&#33258;&#32534;&#30721;&#22120;&#65288;KIA&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;Koopman&#31639;&#23376;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#26469;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#20445;&#35777;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#25805;&#20316;&#30340;&#21487;&#36870;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10291</link><description>&lt;p&gt;
Koopman&#21487;&#36870;&#33258;&#32534;&#30721;&#22120;&#65306;&#21033;&#29992;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#36827;&#34892;&#26102;&#38388;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Koopman Invertible Autoencoder: Leveraging Forward and Backward Dynamics for Temporal Modeling. (arXiv:2309.10291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10291
&lt;/p&gt;
&lt;p&gt;
Koopman&#21487;&#36870;&#33258;&#32534;&#30721;&#22120;&#65288;KIA&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;Koopman&#31639;&#23376;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#26469;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#20445;&#35777;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#25805;&#20316;&#30340;&#21487;&#36870;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#21482;&#33021;&#25429;&#25417;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#32479;&#35745;&#20851;&#31995;&#65292;&#38590;&#20197;&#23398;&#20064;&#30446;&#26631;&#31995;&#32479;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#65292;&#22240;&#27492;&#24314;&#31435;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman&#31639;&#23376;&#29702;&#35770;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;Koopman&#21487;&#36870;&#33258;&#32534;&#30721;&#22120;&#65288;KIA&#65289;&#65292;&#23427;&#22312;&#26080;&#31351;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#38271;&#26399;&#31995;&#32479;&#34892;&#20026;&#30340;&#26356;&#20934;&#30830;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#36870;&#35774;&#35745;&#20445;&#35777;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#25805;&#20316;&#30340;&#21487;&#36870;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;KIA&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate long-term predictions are the foundations for many machine learning applications and decision-making processes. However, building accurate long-term prediction models remains challenging due to the limitations of existing temporal models like recurrent neural networks (RNNs), as they capture only the statistical connections in the training data and may fail to learn the underlying dynamics of the target system. To tackle this challenge, we propose a novel machine learning model based on Koopman operator theory, which we call Koopman Invertible Autoencoders (KIA), that captures the inherent characteristic of the system by modeling both forward and backward dynamics in the infinite-dimensional Hilbert space. This enables us to efficiently learn low-dimensional representations, resulting in more accurate predictions of long-term system behavior. Moreover, our method's invertibility design guarantees reversibility and consistency in both forward and inverse operations. We illustra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22825;&#25991;&#23398;&#12289;&#33322;&#22825;&#23398;&#21644;&#20854;&#20182;&#31354;&#38388;&#20027;&#39064;&#30340;&#26412;&#20307;&#24211;&#27010;&#24565;&#65292;&#35813;&#24211;&#21487;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#24179;&#21488;&#29992;&#20110;&#25628;&#32034;&#12289;&#23457;&#26680;&#21644;&#21019;&#24314;&#19982;&#22825;&#25991;&#30456;&#20851;&#30340;&#26412;&#20307;&#65292;&#20197;&#20943;&#23569;&#30740;&#31350;&#26102;&#38388;&#24182;&#25903;&#25345;&#30693;&#35782;&#32452;&#32455;&#31995;&#32479;&#21644;&#35821;&#20041;&#36164;&#28304;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.10288</link><description>&lt;p&gt;
AstroPortal: &#19968;&#20010;&#29992;&#20110;&#22825;&#25991;&#23398;&#12289;&#33322;&#22825;&#23398;&#21644;&#20854;&#20182;&#31354;&#38388;&#20027;&#39064;&#30340;&#26412;&#20307;&#24211;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
AstroPortal: An ontology repository concept for astronomy, astronautics and other space topics. (arXiv:2309.10288v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22825;&#25991;&#23398;&#12289;&#33322;&#22825;&#23398;&#21644;&#20854;&#20182;&#31354;&#38388;&#20027;&#39064;&#30340;&#26412;&#20307;&#24211;&#27010;&#24565;&#65292;&#35813;&#24211;&#21487;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#24179;&#21488;&#29992;&#20110;&#25628;&#32034;&#12289;&#23457;&#26680;&#21644;&#21019;&#24314;&#19982;&#22825;&#25991;&#30456;&#20851;&#30340;&#26412;&#20307;&#65292;&#20197;&#20943;&#23569;&#30740;&#31350;&#26102;&#38388;&#24182;&#25903;&#25345;&#30693;&#35782;&#32452;&#32455;&#31995;&#32479;&#21644;&#35821;&#20041;&#36164;&#28304;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#22825;&#25991;&#23398;&#12289;&#33322;&#22825;&#23398;&#21644;&#20854;&#20182;&#31354;&#38388;&#30456;&#20851;&#20027;&#39064;&#30340;&#26412;&#20307;&#24211;&#12290;&#35813;&#24211;&#21487;&#31216;&#20026;AstroPortal&#65288;&#25110;SpacePortal&#65289;&#12289;AstroHub&#65288;&#25110;SpaceHub&#65289;&#31561;&#12290;&#21019;&#24314;&#35813;&#24211;&#23558;&#36866;&#29992;&#20110;&#23398;&#26415;&#12289;&#30740;&#31350;&#21644;&#20854;&#20182;&#25968;&#25454;&#23494;&#38598;&#22411;&#39046;&#22495;&#12290;&#23427;&#23545;&#20110;&#22826;&#31354;&#31185;&#23398;&#65288;&#21253;&#25324;&#22825;&#25991;&#23398;&#65289;&#12289;&#22320;&#29699;&#31185;&#23398;&#21644;&#33322;&#22825;&#23398;&#65288;&#33322;&#22825;&#39134;&#34892;&#65289;&#31561;&#25968;&#25454;&#23494;&#38598;&#22411;&#23398;&#31185;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#24211;&#24212;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#25628;&#32034;&#12289;&#23457;&#26680;&#21644;&#21019;&#24314;&#19982;&#22825;&#25991;&#30456;&#20851;&#20027;&#39064;&#30340;&#26412;&#20307;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#30740;&#31350;&#26102;&#38388;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#26469;&#30740;&#31350;&#21644;&#27604;&#36739;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#32452;&#32455;&#31995;&#32479;&#25110;&#35821;&#20041;&#36164;&#28304;&#12290;&#30001;&#20110;&#30446;&#26631;&#39046;&#22495;&#20013;&#30446;&#21069;&#19981;&#23384;&#22312;&#21487;&#29992;&#30340;&#24211;&#65292;&#26412;&#25991;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a repository for ontologies of astronomy, astronautics, and other space-related topics. It may be called AstroPortal (or SpacePortal), AstroHub (or SpaceHub), etc. The creation of this repository will be applicable to academic, research and other data-intensive sectors. It is relevant for space sciences (including astronomy), Earth science, and astronautics (spaceflight), among other data-intensive disciplines. The repository should provide a centralized platform to search, review and create ontologies for astro-related topics. It thereby can decrease research time, while also providing a user-friendly means to study and compare knowledge organization systems or semantic resources of the target domains. With no apparent repository available on the target domain, this paper also expresses a novel concept.
&lt;/p&gt;</description></item><item><title>FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10283</link><description>&lt;p&gt;
FRAMU: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10283
&lt;/p&gt;
&lt;p&gt;
FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#36890;&#36807;&#20801;&#35768;&#20174;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#21024;&#38500;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#65292;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20351;&#29992;&#36807;&#26102;&#30340;&#12289;&#31169;&#26377;&#30340;&#21644;&#26080;&#20851;&#30340;&#25968;&#25454;&#20250;&#24341;&#21457;&#19982;&#38544;&#31169;&#21644;&#27169;&#22411;&#25928;&#29575;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#24433;&#21709;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36951;&#24536;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#36824;&#20250;&#23545;&#25968;&#25454;&#38544;&#31169;&#36896;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;&#65288;FRAMU&#65289;&#12290;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#26159;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#65288;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65289;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;FRAMU&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#12289;&#36951;&#24536;&#36807;&#26102;&#12289;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25903;&#25345;&#27169;&#22411;&#25345;&#32493;&#28436;&#36827;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.10275</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#20026;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#20013;&#32676;&#20307;&#12289;&#33258;&#21160;&#21270;&#20179;&#20648;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#21035;&#65306;&#38598;&#20013;&#24335;&#35268;&#21010;&#21644;&#20998;&#25955;&#24335;&#35268;&#21010;&#12290;&#38598;&#20013;&#24335;&#35268;&#21010;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#22240;&#27492;&#22312;&#22823;&#22411;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20998;&#25955;&#24335;&#35268;&#21010;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#26102;&#36335;&#24452;&#35268;&#21010;&#65292;&#23637;&#31034;&#20102;&#38544;&#24335;&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#19988;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRAMP&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#24335;&#35838;&#31243;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26080;&#20154;&#34920;&#38754;&#33337;&#33334;&#23558;&#27973;&#27700;&#20307;&#30340;&#27700;&#19979;&#22320;&#24418;&#22270;&#19982;&#25968;&#23383;&#22320;&#34920;&#22270;&#21512;&#24182;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20307;&#31215;&#27169;&#22411;&#65292;&#20197;&#33719;&#21462;&#20851;&#20110;&#38750;&#21487;&#33322;&#36816;&#27827;&#27969;&#21644;&#20854;&#20182;&#27973;&#27700;&#20307;&#25152;&#33021;&#25215;&#36733;&#30340;&#27700;&#20307;&#31215;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.10269</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#20154;&#34920;&#38754;&#33337;&#33334;&#21019;&#24314;&#38750;&#21487;&#33322;&#36816;&#27827;&#27969;&#21644;&#20854;&#20182;&#27973;&#27700;&#20307;&#30340;&#20307;&#31215;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water. (arXiv:2309.10269v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10269
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#20154;&#34920;&#38754;&#33337;&#33334;&#23558;&#27973;&#27700;&#20307;&#30340;&#27700;&#19979;&#22320;&#24418;&#22270;&#19982;&#25968;&#23383;&#22320;&#34920;&#22270;&#21512;&#24182;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20307;&#31215;&#27169;&#22411;&#65292;&#20197;&#33719;&#21462;&#20851;&#20110;&#38750;&#21487;&#33322;&#36816;&#27827;&#27969;&#21644;&#20854;&#20182;&#27973;&#27700;&#20307;&#25152;&#33021;&#25215;&#36733;&#30340;&#27700;&#20307;&#31215;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21487;&#33322;&#36816;&#27827;&#27969;&#21644;&#25345;&#27700;&#27744;&#22312;&#39044;&#38450;&#27946;&#27700;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#26159;&#32039;&#24613;&#35268;&#21010;&#20154;&#21592;&#24120;&#24120;&#27809;&#26377;&#20851;&#20110;&#23427;&#20204;&#22312;&#27946;&#27700;&#21069;&#25152;&#33021;&#25215;&#36733;&#30340;&#27700;&#20307;&#31215;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20154;&#28023;&#27915;&#34920;&#38754;&#33337;&#21482;&#65288;USV&#65289;&#23558;&#27973;&#27700;&#20307;&#30340;&#27700;&#19979;&#22320;&#24418;&#22270;&#19982;&#25968;&#23383;&#22320;&#34920;&#22270;&#21512;&#24182;&#65292;&#24418;&#25104;&#32479;&#19968;&#30340;&#20307;&#31215;&#27169;&#22411;&#12290;&#27700;&#19979;&#32593;&#26684;&#26159;&#36890;&#36807;&#23558;&#31232;&#30095;&#22768;&#32435;&#28145;&#24230;&#35835;&#25968;&#24212;&#29992;&#20110;&#27850;&#26494;&#34920;&#38754;&#37325;&#24314;&#31639;&#27861;&#26469;&#24320;&#21457;&#30340;&#12290;&#20351;&#29992;&#21830;&#19994;&#21270;&#30340;&#32467;&#26500;&#36816;&#21160;&#65288;SfM&#65289;&#36719;&#20214;&#21253;&#21019;&#24314;&#20102;&#27827;&#23736;&#30340;&#23494;&#38598;&#30340;&#27700;&#19978;&#32593;&#26684;&#12290;&#21512;&#24182;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#26368;&#37325;&#35201;&#30340;&#21407;&#22240;&#26159;&#20256;&#24863;&#22120;&#35206;&#30422;&#30340;&#38388;&#38553;&#65292;&#21363;USV&#26080;&#27861;&#25910;&#38598;&#22768;&#32435;&#28145;&#24230;&#25968;&#25454;&#25110;&#30475;&#21040;&#27801;&#28393;&#65292;&#23548;&#33268;&#20841;&#20491;&#32593;&#26684;&#21487;&#33021;&#19981;&#30456;&#20132;&#12290;&#35813;&#26041;&#27861;&#22312;Hydronalix EMILY USV&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-navigable rivers and retention ponds play important roles in buffering communities from flooding, yet emergency planners often have no data as to the volume of water that they can carry before flooding the surrounding. This paper describes a practical approach for using an uncrewed marine surface vehicle (USV) to collect and merge bathymetric maps with digital surface maps of the banks of shallow bodies of water into a unified volumetric model. The below-waterline mesh is developed by applying the Poisson surface reconstruction algorithm to the sparse sonar depth readings of the underwater surface. Dense above-waterline meshes of the banks are created using commercial structure from motion (SfM) packages. Merging is challenging for many reasons, the most significant is gaps in sensor coverage, i.e., the USV cannot collect sonar depth data or visually see sandy beaches leading to a bank thus the two meshes may not intersect. The approach is demonstrated on a Hydronalix EMILY USV wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#39134;&#28293;&#28082;&#28404;&#24418;&#24577;&#28436;&#21270;&#19982;&#28082;&#28404;&#23545;&#22266;&#20307;&#34920;&#38754;&#26045;&#21152;&#30340;&#20914;&#20987;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21457;&#29616;&#26435;&#37325;&#30697;&#38453;&#20803;&#32032;&#30340;&#20540;&#19982;&#28082;&#28404;&#24418;&#24577;&#30340;&#26102;&#38388;&#28436;&#21270;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.10266</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#25581;&#31034;&#20102;&#39134;&#28293;&#28082;&#28404;&#24418;&#24577;&#28436;&#21270;&#19982;&#26045;&#21152;&#20914;&#20987;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Correlation between morphological evolution of splashing drop and exerted impact force revealed by interpretation of explainable artificial intelligence. (arXiv:2309.10266v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#39134;&#28293;&#28082;&#28404;&#24418;&#24577;&#28436;&#21270;&#19982;&#28082;&#28404;&#23545;&#22266;&#20307;&#34920;&#38754;&#26045;&#21152;&#30340;&#20914;&#20987;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21457;&#29616;&#26435;&#37325;&#30697;&#38453;&#20803;&#32032;&#30340;&#20540;&#19982;&#28082;&#28404;&#24418;&#24577;&#30340;&#26102;&#38388;&#28436;&#21270;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#35270;&#39057;&#20998;&#31867;&#22120;&#23545;&#39134;&#28293;&#21644;&#38750;&#39134;&#28293;&#28082;&#28404;&#20998;&#31867;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#39134;&#28293;&#24418;&#24577;&#19982;&#28082;&#28404;&#23545;&#22266;&#20307;&#34920;&#38754;&#26045;&#21152;&#30340;&#26631;&#20934;&#21270;&#20914;&#20987;&#21147;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#30456;&#20851;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;XAI&#30340;&#26435;&#37325;&#30697;&#38453;&#20803;&#32032;&#30340;&#20540;&#19982;&#28082;&#28404;&#24418;&#24577;&#30340;&#26102;&#38388;&#28436;&#21270;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;&#27599;&#24103;&#23545;&#35270;&#39057;&#20998;&#31867;&#20540;&#30340;&#36129;&#29486;&#30340;&#21464;&#21270;&#29575;&#65292;&#20316;&#20026;&#37327;&#21270;&#25552;&#21462;&#30340;&#39134;&#28293;&#21644;&#38750;&#39134;&#28293;&#29305;&#24449;&#22312;&#19981;&#21516;&#20914;&#20987;&#26102;&#38388;&#23545;XAI&#27169;&#22411;&#20998;&#31867;&#30340;&#36129;&#29486;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35745;&#31639;&#24471;&#21040;&#30340;&#39134;&#28293;&#29305;&#24449;&#30340;&#21464;&#21270;&#29575;&#19982;&#26631;&#20934;&#21270;&#20914;&#20987;&#21147;&#30340;&#26354;&#32447;&#38750;&#24120;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study reveals a possible correlation between splashing morphology and the normalized impact force exerted by an impacting drop on a solid surface. This finding is obtained from a newly proposed feature extraction method and a subsequent interpretation of the classification of splashing and non-splashing drops performed by an explainable artificial intelligence (XAI) video classifier. Notably, the values of the weight matrix elements of the XAI that correspond to the extracted features are found to change with the temporal evolution of the drop morphology. We compute the rate of change of the contributions of each frame with respect to the classification value of a video as an important index to quantify the contributions of the extracted splashing and non-splashing features at different impact times to the classification of the XAI model. Remarkably, the rate computed for the extracted splashing features is found to closely match the profile of the normalized impact force, where t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.10254</link><description>&lt;p&gt;
LLM&#24179;&#21488;&#23433;&#20840;&#65306;&#23558;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;ChatGPT&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22914;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24179;&#21488;&#24320;&#22987;&#25552;&#20379;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#65292;&#20197;&#19982;&#20114;&#32852;&#32593;&#19978;&#30340;&#31532;&#19977;&#26041;&#26381;&#21153;&#36827;&#34892;&#20132;&#20114;&#12290;&#34429;&#28982;&#36825;&#20123;&#25554;&#20214;&#25193;&#23637;&#20102;LLM&#24179;&#21488;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#26159;&#30001;&#20219;&#24847;&#30340;&#31532;&#19977;&#26041;&#24320;&#21457;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#38544;&#24335;&#20449;&#20219;&#12290;&#25554;&#20214;&#36824;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;LLM&#24179;&#21488;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#31946;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;LLM&#24179;&#21488;&#35774;&#35745;&#32773;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#20010;&#25915;&#20987;&#20998;&#31867;&#27861;&#30340;&#34920;&#36848;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;LLM&#24179;&#21488;&#30456;&#20851;&#26041;&#22914;&#20309;&#21033;&#29992;&#20182;&#20204;&#30340;&#33021;&#21147;&#21644;&#36131;&#20219;&#23545;&#24444;&#27492;&#36827;&#34892;&#25915;&#20987;&#26469;&#24320;&#21457;&#30340;&#12290;&#20316;&#20026;&#25105;&#20204;&#36845;&#20195;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the poten
&lt;/p&gt;</description></item><item><title>GPTFUZZER&#26159;&#19968;&#31181;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#24037;&#24037;&#31243;&#65292;&#24182;&#36890;&#36807;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10253</link><description>&lt;p&gt;
GPTFUZZER : &#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. (arXiv:2309.10253v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10253
&lt;/p&gt;
&lt;p&gt;
GPTFUZZER&#26159;&#19968;&#31181;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#24037;&#24037;&#31243;&#65292;&#24182;&#36890;&#36807;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#24191;&#27867;&#29992;&#20110;&#26085;&#24120;&#23545;&#35805;&#21040;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;LLMs&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#21487;&#33021;&#20250;&#25552;&#20379;&#26377;&#20851;&#36827;&#34892;&#26377;&#23475;&#25110;&#38750;&#27861;&#27963;&#21160;&#30340;&#35814;&#32454;&#25351;&#23548;&#12290;&#34429;&#28982;&#23433;&#20840;&#25514;&#26045;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#36755;&#20986;&#30340;&#39118;&#38505;&#65292;&#20294;&#23545;&#25239;&#24615;&#30340;"&#36234;&#29425;"&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#21033;&#29992;LLMs&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#20123;&#36234;&#29425;&#27169;&#26495;&#36890;&#24120;&#26159;&#25163;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#65292;&#20351;&#22823;&#35268;&#27169;&#27979;&#35797;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;\fuzzer&#65292;&#21463;AFL&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#30340;&#21551;&#21457;&#12290;&#19982;&#25163;&#24037;&#24037;&#31243;&#19981;&#21516;&#65292;\fuzzer&#33258;&#21160;&#21270;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;LLMs&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#22312;&#26680;&#24515;&#37096;&#20998;&#65292;\fuzzer&#20174;&#20154;&#24037;&#32534;&#20889;&#30340;&#27169;&#26495;&#20316;&#20026;&#31181;&#23376;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#21464;&#24322;&#25805;&#20316;&#23545;&#20854;&#36827;&#34892;&#21464;&#24322;&#20197;&#29983;&#25104;&#26032;&#30340;&#27169;&#26495;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;\fuzzer&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#29992;&#20110;&#24179;&#34913;&#25928;&#29575;&#30340;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial "jailbreak" attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce \fuzzer, a novel black-box jailbreak fuzzing framework inspired by AFL fuzzing framework. Instead of manual engineering, \fuzzer automates the generation of jailbreak templates for red-teaming LLMs. At its core, \fuzzer starts with human-written templates as seeds, then mutates them using mutate operators to produce new templates. We detail three key components of \fuzzer: a seed selection strategy for balancing efficiency 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22312;&#20219;&#24847;&#25968;&#25454;&#27969;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#25512;&#23548;&#20986;&#20102;&#26126;&#30830;&#30340;&#22352;&#26631;&#19981;&#21464;&#20844;&#24335;&#26469;&#35745;&#31639;&#20869;&#22312;&#21644;&#22806;&#22312;&#26354;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20854;&#20013;&#20869;&#22312;&#26354;&#29575;&#24230;&#37327;&#30053;&#20248;&#20110;&#22806;&#22312;&#26354;&#29575;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10237</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#26174;&#24335;&#26354;&#29575;&#27491;&#21017;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Explicit Curvature Regularization in Deep Generative Models. (arXiv:2309.10237v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22312;&#20219;&#24847;&#25968;&#25454;&#27969;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#25512;&#23548;&#20986;&#20102;&#26126;&#30830;&#30340;&#22352;&#26631;&#19981;&#21464;&#20844;&#24335;&#26469;&#35745;&#31639;&#20869;&#22312;&#21644;&#22806;&#22312;&#26354;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20854;&#20013;&#20869;&#22312;&#26354;&#29575;&#24230;&#37327;&#30053;&#20248;&#20110;&#22806;&#22312;&#26354;&#29575;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26063;&#22522;&#20110;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;&#23884;&#20837;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#25968;&#25454;&#27969;&#24418;&#65292;&#25512;&#23548;&#20986;&#20102;&#22312;&#20869;&#22312;&#21644;&#22806;&#22312;&#26354;&#29575;&#24230;&#37327;&#19979;&#30340;&#26126;&#30830;&#30340;&#22352;&#26631;&#19981;&#21464;&#20844;&#24335;&#12290;&#30001;&#20110;&#35745;&#31639;&#26354;&#29575;&#26159;&#19968;&#20010;&#28041;&#21450;&#21040;&#20108;&#38454;&#23548;&#25968;&#27714;&#20540;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#36807;&#31243;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#29992;&#20110;&#36817;&#20284;&#35780;&#20272;&#20869;&#22312;&#21644;&#22806;&#22312;&#26354;&#29575;&#30340;&#26377;&#25928;&#20844;&#24335;&#12290;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#20102;&#20869;&#22312;&#26354;&#29575;&#21644;&#22806;&#22312;&#26354;&#29575;&#30340;&#27491;&#21017;&#21270;&#24230;&#37327;&#30340;&#30456;&#23545;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#19982;&#29616;&#26377;&#33258;&#21160;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#26354;&#29575;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#19988;&#20869;&#22312;&#26354;&#29575;&#24230;&#37327;&#30053;&#20248;&#20110;&#22806;&#22312;&#26354;&#29575;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a family of curvature-based regularization terms for deep generative model learning. Explicit coordinate-invariant formulas for both intrinsic and extrinsic curvature measures are derived for the case of arbitrary data manifolds embedded in higher-dimensional Euclidean space. Because computing the curvature is a highly computation-intensive process involving the evaluation of second-order derivatives, efficient formulas are derived for approximately evaluating intrinsic and extrinsic curvatures. Comparative studies are conducted that compare the relative efficacy of intrinsic versus extrinsic curvature-based regularization measures, as well as performance comparisons against existing autoencoder training methods. Experiments involving noisy motion capture data confirm that curvature-based methods outperform existing autoencoder regularization methods, with intrinsic curvature measures slightly more effective than extrinsic curvature measures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#19987;&#19994;&#24037;&#20855;&#20351;&#29992;&#31561;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25552;&#20379;&#20010;&#24615;&#21270;&#36741;&#21161;&#12289;&#25345;&#32493;&#23398;&#20064;&#21644;&#36879;&#26126;&#20915;&#31574;&#65292;&#20174;&#32780;&#25913;&#21464;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.10228</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#23454;&#29616;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20132;&#20114;&#65306;&#20197;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles. (arXiv:2309.10228v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#19987;&#19994;&#24037;&#20855;&#20351;&#29992;&#31561;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25552;&#20379;&#20010;&#24615;&#21270;&#36741;&#21161;&#12289;&#25345;&#32493;&#23398;&#20064;&#21644;&#36879;&#26126;&#20915;&#31574;&#65292;&#20174;&#32780;&#25913;&#21464;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#26410;&#26469;&#22312;&#20110;&#20154;&#26412;&#35774;&#35745;&#21644;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#34701;&#21512;&#12290;&#26410;&#26469;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#19981;&#20165;&#20250;&#36816;&#36865;&#20056;&#23458;&#65292;&#36824;&#20250;&#19982;&#20854;&#20114;&#21160;&#24182;&#36866;&#24212;&#20182;&#20204;&#30340;&#38656;&#27714;&#65292;&#20351;&#26053;&#31243;&#33298;&#36866;&#12289;&#39640;&#25928;&#12289;&#24841;&#24555;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#36807;&#25972;&#21512;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#19987;&#19994;&#24037;&#20855;&#20351;&#29992;&#12289;&#21327;&#21516;&#25512;&#29702;&#20197;&#21450;&#19982;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#19978;&#30340;&#21508;&#31181;&#27169;&#22359;&#19968;&#36215;&#34892;&#21160;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26080;&#32541;&#38598;&#25104;LLMs&#30340;&#39640;&#32423;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#21040;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#25913;&#21464;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25805;&#20316;&#26041;&#24335;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#36741;&#21161;&#12289;&#25345;&#32493;&#23398;&#20064;&#21644;&#36879;&#26126;&#20915;&#31574;&#65292;&#20174;&#32780;&#20026;&#26356;&#23433;&#20840;&#12289;&#26356;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The future of autonomous vehicles lies in the convergence of human-centric design and advanced AI capabilities. Autonomous vehicles of the future will not only transport passengers but also interact and adapt to their desires, making the journey comfortable, efficient, and pleasant. In this paper, we present a novel framework that leverages Large Language Models (LLMs) to enhance autonomous vehicles' decision-making processes. By integrating LLMs' natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles, this framework aims to seamlessly integrate the advanced language and reasoning capabilities of LLMs into autonomous vehicles. The proposed framework holds the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making, ultimately contributing to safer and more efficient autonomous driving t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MLFF-Net &#30340;&#22810;&#23618;&#27425;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24687;&#32905;&#20998;&#21106;&#30340;&#25913;&#36827;&#12290;&#23545;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#28388;&#27874;&#21644;&#21033;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#34701;&#21512;&#24341;&#36215;&#30340;&#35821;&#20041;&#20914;&#31361;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10219</link><description>&lt;p&gt;
&#22810;&#23618;&#27425;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24687;&#32905;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multi-level feature fusion network combining attention mechanisms for polyp segmentation. (arXiv:2309.10219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10219
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MLFF-Net &#30340;&#22810;&#23618;&#27425;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24687;&#32905;&#20998;&#21106;&#30340;&#25913;&#36827;&#12290;&#23545;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#28388;&#27874;&#21644;&#21033;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#34701;&#21512;&#24341;&#36215;&#30340;&#35821;&#20041;&#20914;&#31361;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#19978;&#65292;&#33258;&#21160;&#21270;&#30340;&#24687;&#32905;&#20998;&#21106;&#25216;&#26415;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;&#21307;&#23398;&#35786;&#26029;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#24739;&#32773;&#24739;&#32467;&#30452;&#32928;&#30284;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#26174;&#33879;&#24369;&#28857;&#21487;&#33021;&#24433;&#21709;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#39318;&#20808;&#65292;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#26410;&#32463;&#20805;&#20998;&#28388;&#27874;&#21644;&#21033;&#29992;&#12290;&#20854;&#27425;&#65292;&#30001;&#29305;&#24449;&#34701;&#21512;&#24341;&#36215;&#30340;&#35821;&#20041;&#20914;&#31361;&#21644;&#20449;&#24687;&#20887;&#20313;&#27809;&#26377;&#24471;&#21040;&#20851;&#27880;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24687;&#32905;&#20998;&#21106;&#26041;&#27861;&#65292;&#21517;&#20026; MLFF-Net&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#27425;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MLFF-Net &#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;MAM&#65289;&#12289;&#39640;&#23618;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#65288;HFEM&#65289;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;GAM&#65289;&#12290;&#20854;&#20013;&#65292;MAM &#29992;&#20110;&#20174;&#32534;&#30721;&#22120;&#30340;&#27973;&#23618;&#36755;&#20986;&#20013;&#25552;&#21462;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#24687;&#32905;&#32454;&#33410;&#12290;&#22312; HFEM &#20013;&#65292;&#32534;&#30721;&#22120;&#30340;&#28145;&#23618;&#29305;&#24449;&#21487;&#20197;&#34987;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinically, automated polyp segmentation techniques have the potential to significantly improve the efficiency and accuracy of medical diagnosis, thereby reducing the risk of colorectal cancer in patients. Unfortunately, existing methods suffer from two significant weaknesses that can impact the accuracy of segmentation. Firstly, features extracted by encoders are not adequately filtered and utilized. Secondly, semantic conflicts and information redundancy caused by feature fusion are not attended to. To overcome these limitations, we propose a novel approach for polyp segmentation, named MLFF-Net, which leverages multi-level feature fusion and attention mechanisms. Specifically, MLFF-Net comprises three modules: Multi-scale Attention Module (MAM), High-level Feature Enhancement Module (HFEM), and Global Attention Module (GAM). Among these, MAM is used to extract multi-scale information and polyp details from the shallow output of the encoder. In HFEM, the deep features of the encoders
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#36866;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10217</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#35821;&#20041;&#20998;&#21106;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Attention Networks for Semantic Segmentation. (arXiv:2309.10217v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#36866;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25104;&#20026;&#20102;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20934;&#30830;&#24615;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#12290;&#36817;&#26399;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35299;&#30721;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#32463;&#24120;&#19982;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#32593;&#32476;&#30340;mIoU&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#20248;&#36234;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#31867;&#21035;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#31934;&#24230;&#31561;&#29305;&#24615;&#65292;&#36825;&#23545;&#24037;&#31243;&#24212;&#29992;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#23545;FLOPs&#21644;&#20869;&#23384;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#27861;&#19981;&#19968;&#33268;&#65292;&#20351;&#24471;&#27604;&#36739;&#38590;&#20197;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#21508;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#32467;&#35770;&#21364;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#39318;&#20808;&#36827;&#34892;&#23454;&#39564;&#65292;&#20998;&#26512;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#24635;&#32467;&#20102;&#36866;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation is a vital problem in computer vision. Recently, a common solution to semantic segmentation is the end-to-end convolution neural network, which is much more accurate than traditional methods.Recently, the decoders based on attention achieve state-of-the-art (SOTA) performance on various datasets. But these networks always are compared with the mIoU of previous SOTA networks to prove their superiority and ignore their characteristics without considering the computation complexity and precision in various categories, which is essential for engineering applications. Besides, the methods to analyze the FLOPs and memory are not consistent between different networks, which makes the comparison hard to be utilized. What's more, various methods utilize attention in semantic segmentation, but the conclusion of these methods is lacking. This paper first conducts experiments to analyze their computation complexity and compare their performance. Then it summarizes suitable sc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35745;&#31639;&#23631;&#34109;&#21160;&#20316;&#26469;&#23454;&#29616;&#23433;&#20840;&#30340;POMDP&#22312;&#32447;&#35268;&#21010;&#12290;&#22235;&#31181;&#23631;&#34109;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10216</link><description>&lt;p&gt;
&#36890;&#36807;&#23631;&#34109;&#23454;&#29616;&#23433;&#20840;&#30340;POMDP&#22312;&#32447;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe POMDP Online Planning via Shielding. (arXiv:2309.10216v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10216
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#23631;&#34109;&#21160;&#20316;&#26469;&#23454;&#29616;&#23433;&#20840;&#30340;POMDP&#22312;&#32447;&#35268;&#21010;&#12290;&#22235;&#31181;&#23631;&#34109;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#12290;POMDP&#22312;&#32447;&#35268;&#21010;&#31639;&#27861;&#65292;&#22914;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#65288;POMCP&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#30446;&#26631;&#20026;&#26368;&#22823;&#21270;&#39044;&#26399;&#22238;&#25253;&#30340;&#22823;&#22411;POMDP&#12290;&#20294;&#26159;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#31574;&#30053;&#26080;&#27861;&#25552;&#20379;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#65289;&#33267;&#20851;&#37325;&#35201;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#23433;&#20840;&#35201;&#27714;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#20960;&#20046;&#19968;&#23450;&#30340;&#21040;&#36798;-&#36991;&#20813;&#35268;&#33539;&#65288;&#21363;&#65292;&#36798;&#21040;&#19968;&#32452;&#30446;&#26631;&#29366;&#24577;&#30340;&#27010;&#29575;&#20026;1&#65292;&#36798;&#21040;&#19968;&#32452;&#19981;&#23433;&#20840;&#29366;&#24577;&#30340;&#27010;&#29575;&#20026;0&#65289;&#12290;&#25105;&#20204;&#35745;&#31639;&#38480;&#21046;&#36829;&#21453;&#20960;&#20046;&#19968;&#23450;&#21040;&#36798;-&#36991;&#20813;&#35268;&#33539;&#30340;&#19981;&#23433;&#20840;&#21160;&#20316;&#30340;&#23631;&#34109;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#23631;&#34109;&#38598;&#25104;&#21040;POMCP&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#23433;&#20840;&#30340;POMDP&#22312;&#32447;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#23631;&#34109;&#26041;&#27861;&#65292;&#26681;&#25454;&#23631;&#34109;&#30340;&#35745;&#31639;&#21644;&#38598;&#25104;&#26041;&#24335;&#30340;&#19981;&#21516;&#65292;&#21253;&#25324;&#20998;&#35299;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) have been widely used in many robotic applications for sequential decision-making under uncertainty. POMDP online planning algorithms such as Partially Observable Monte-Carlo Planning (POMCP) can solve very large POMDPs with the goal of maximizing the expected return. But the resulting policies cannot provide safety guarantees that are imperative for real-world safety-critical tasks (e.g., autonomous driving). In this work, we consider safety requirements represented as almost-sure reach-avoid specifications (i.e., the probability to reach a set of goal states is one and the probability to reach a set of unsafe states is zero). We compute shields that restrict unsafe actions violating almost-sure reach-avoid specifications. We then integrate these shields into the POMCP algorithm for safe POMDP online planning. We propose four distinct shielding methods, differing in how the shields are computed and integrated, including factored 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#38754;&#21521;&#26410;&#30693;&#39046;&#22495;&#30340;&#35821;&#20041;OOD&#26816;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27491;&#21017;&#21270;&#21644;OOD&#26816;&#27979;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21327;&#21464;&#37327;&#20559;&#31227;&#21644;&#35821;&#20041;&#20559;&#31227;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#19977;&#20010;&#26631;&#20934;&#22495;&#27867;&#21270;&#22522;&#20934;&#19978;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10209</link><description>&lt;p&gt;
&#38754;&#21521;&#26410;&#30693;&#39046;&#22495;&#30340;&#26377;&#25928;&#35821;&#20041;OOD&#26816;&#27979;&#65306;&#22495;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards Effective Semantic OOD Detection in Unseen Domains: A Domain Generalization Perspective. (arXiv:2309.10209v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#38754;&#21521;&#26410;&#30693;&#39046;&#22495;&#30340;&#35821;&#20041;OOD&#26816;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27491;&#21017;&#21270;&#21644;OOD&#26816;&#27979;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21327;&#21464;&#37327;&#20559;&#31227;&#21644;&#35821;&#20041;&#20559;&#31227;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#19977;&#20010;&#26631;&#20934;&#22495;&#27867;&#21270;&#22522;&#20934;&#19978;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20004;&#31181;&#24120;&#35265;&#30340;&#20998;&#24067;&#20559;&#31227;&#26159;&#21327;&#21464;&#37327;&#20559;&#31227;&#65288;&#35266;&#23519;&#21040;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65289;&#21644;&#35821;&#20041;&#20559;&#31227;&#65288;&#35266;&#23519;&#21040;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#24322;&#65289;&#12290;&#20256;&#32479;&#30340;OOD&#26816;&#27979;&#25216;&#26415;&#36890;&#24120;&#21482;&#22788;&#29702;&#20854;&#20013;&#19968;&#31181;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#29615;&#22659;&#36890;&#24120;&#21516;&#26102;&#23384;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#21644;&#35821;&#20041;&#20559;&#31227;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#36328;&#39046;&#22495;&#30340;&#35821;&#20041;OOD&#26816;&#27979;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20004;&#31181;&#20998;&#24067;&#20559;&#31227;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#27491;&#21017;&#21270;&#31574;&#30053;&#65306;&#22495;&#27867;&#21270;&#27491;&#21017;&#21270;&#65292;&#30830;&#20445;&#22495;&#38388;&#35821;&#20041;&#19981;&#21464;&#20197;&#23545;&#25239;&#21327;&#21464;&#37327;&#20559;&#31227;&#65292;&#20197;&#21450;OOD&#26816;&#27979;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#33021;&#37327;&#38480;&#21046;&#26469;&#22686;&#24378;&#23545;&#35821;&#20041;&#20559;&#31227;&#30340;OOD&#26816;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#26631;&#20934;&#22495;&#27867;&#21270;&#22522;&#20934;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#22495;&#27867;&#21270;&#24212;&#29992;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two prevalent types of distributional shifts in machine learning are the covariate shift (as observed across different domains) and the semantic shift (as seen across different classes). Traditional OOD detection techniques typically address only one of these shifts. However, real-world testing environments often present a combination of both covariate and semantic shifts. In this study, we introduce a novel problem, semantic OOD detection across domains, which simultaneously addresses both distributional shifts. To this end, we introduce two regularization strategies: domain generalization regularization, which ensures semantic invariance across domains to counteract the covariate shift, and OOD detection regularization, designed to enhance OOD detection capabilities against the semantic shift through energy bounding. Through rigorous testing on three standard domain generalization benchmarks, our proposed framework showcases its superiority over conventional domain generalization app
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21183;&#27169;&#22411;&#21644;&#36873;&#25321;&#24615;&#22238;&#25918;&#26469;&#31283;&#23450;RLHF&#35757;&#32451;&#30340;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22870;&#21169;&#27450;&#39575;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#24471;&#20998;&#21644;&#32988;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10202</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21183;&#27169;&#22411;&#21644;&#36873;&#25321;&#24615;&#22238;&#25918;&#31283;&#23450;RLHF
&lt;/p&gt;
&lt;p&gt;
Stabilizing RLHF through Advantage Model and Selective Rehearsal. (arXiv:2309.10202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21183;&#27169;&#22411;&#21644;&#36873;&#25321;&#24615;&#22238;&#25918;&#26469;&#31283;&#23450;RLHF&#35757;&#32451;&#30340;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22870;&#21169;&#27450;&#39575;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#24471;&#20998;&#21644;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#65292;&#36890;&#36807;RLHF&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#20559;&#22909;&#23545;&#40784;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#25361;&#25112;&#34920;&#29616;&#20026;&#21508;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#27604;&#22914;&#22870;&#21169;&#27450;&#39575;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#36825;&#20010;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#39033;&#21019;&#26032;&#26469;&#31283;&#23450;RLHF&#35757;&#32451;: 1) &#20248;&#21183;&#27169;&#22411;&#65292;&#30452;&#25509;&#24314;&#27169;&#20248;&#21183;&#24471;&#20998;&#65292;&#21363;&#30456;&#23545;&#20110;&#26399;&#26395;&#22870;&#21169;&#30340;&#39069;&#22806;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#24471;&#20998;&#20998;&#24067;&#26469;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#12290;2) &#36873;&#25321;&#24615;&#22238;&#25918;&#65292;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#25968;&#25454;&#36827;&#34892;PPO&#35757;&#32451;&#21644;&#30693;&#35782;&#22238;&#25918;&#65292;&#20174;&#32780;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21644;&#19987;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;RLHF&#35757;&#32451;&#20013;&#22686;&#21152;&#20102;&#31283;&#23450;&#24615;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#24471;&#20998;&#21644;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: 1) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#30340;&#38480;&#21046;&#65292;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.10186</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#26234;&#33021;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence. (arXiv:2309.10186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#28145;&#24230;&#23398;&#20064;&#30340;&#38480;&#21046;&#65292;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20197;&#20854;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#24207;&#21015;&#20219;&#21153;&#21644;&#23398;&#20064;&#28508;&#22312;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#32034;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20551;&#35774;&#25968;&#25454;&#31561;&#38388;&#38548;&#26377;&#24207;&#20197;&#21450;&#26080;&#27861;&#20805;&#20998;&#34701;&#20837;&#22270;&#32467;&#26500;&#31561;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GNN&#21644;&#24378;&#21270;&#23398;&#20064;&#30417;&#25511;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;GNN&#33021;&#22815;&#26174;&#24335;&#22320;&#23558;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#32435;&#20837;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#26102;&#24207;&#32467;&#26500;&#20013;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20363;&#22914;&#21307;&#30103;&#12289;&#20132;&#36890;&#21644;&#22825;&#27668;&#39044;&#27979;&#20013;&#30340;&#26102;&#24207;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#32593;&#32476;-&#20113;&#38598;&#25104;&#29615;&#22659;&#20013;&#30340;&#26381;&#21153;&#37096;&#32626;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#23481;&#37327;&#38480;&#21046;&#12289;&#21160;&#24577;&#29992;&#25143;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#24182;&#35299;&#20915;&#20102;&#36229;&#36234;5G&#20013;&#30340;&#26381;&#21153;&#36830;&#32493;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10185</link><description>&lt;p&gt;
&#22522;&#20110;&#20113;-&#32593;&#32476;&#38598;&#25104;&#30340;&#36229;&#36234;5G&#20013;&#30340;QoS&#24863;&#30693;&#26381;&#21153;&#39044;&#27979;&#19982;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
QoS-Aware Service Prediction and Orchestration in Cloud-Network Integrated Beyond 5G. (arXiv:2309.10185v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#32593;&#32476;-&#20113;&#38598;&#25104;&#29615;&#22659;&#20013;&#30340;&#26381;&#21153;&#37096;&#32626;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#23481;&#37327;&#38480;&#21046;&#12289;&#21160;&#24577;&#29992;&#25143;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#24182;&#35299;&#20915;&#20102;&#36229;&#36234;5G&#20013;&#30340;&#26381;&#21153;&#36830;&#32493;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;&#20803;&#23431;&#23449;&#31561;&#26032;&#22411;&#24212;&#29992;&#31361;&#26174;&#20102;&#36229;&#36234;5G&#32593;&#32476;&#30340;&#28508;&#21147;&#65292;&#36825;&#38656;&#35201;&#36229;&#20302;&#24310;&#36831;&#36890;&#20449;&#21644;&#22823;&#35268;&#27169;&#30340;&#23485;&#24102;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#20276;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#19981;&#26029;&#27874;&#21160;&#30340;&#36825;&#31867;&#26381;&#21153;&#30340;&#34028;&#21187;&#38656;&#27714;&#65292;&#20351;&#24471;&#22312;B5G&#20013;&#38656;&#35201;&#26356;&#21152;&#20851;&#27880;&#26381;&#21153;&#36830;&#32493;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#26381;&#21153;&#65292;&#36793;&#32536;&#20113;&#27169;&#24335;&#26159;&#21033;&#29992;&#20113;&#23481;&#37327;&#24182;&#23454;&#26102;&#26377;&#25928;&#22320;&#31649;&#29702;&#29992;&#25143;&#22312;&#32593;&#32476;&#20013;&#31227;&#21160;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#20113;&#32593;&#32476;&#38754;&#20020;&#30528;&#35832;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#24517;&#39035;&#38598;&#20307;&#31649;&#29702;&#32593;&#32476;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#37322;&#25918;&#20854;&#20840;&#37096;&#28508;&#21147;&#12290;&#26412;&#25991;&#22312;&#32771;&#34385;&#23481;&#37327;&#38480;&#21046;&#12289;&#21160;&#24577;&#29992;&#25143;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;-&#20113;&#38598;&#25104;&#29615;&#22659;&#20013;&#26381;&#21153;&#37096;&#32626;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21046;&#23450;&#20102;&#20248;&#21270;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel applications such as the Metaverse have highlighted the potential of beyond 5G networks, which necessitate ultra-low latency communications and massive broadband connections. Moreover, the burgeoning demand for such services with ever-fluctuating users has engendered a need for heightened service continuity consideration in B5G. To enable these services, the edge-cloud paradigm is a potential solution to harness cloud capacity and effectively manage users in real time as they move across the network. However, edge-cloud networks confront a multitude of limitations, including networking and computing resources that must be collectively managed to unlock their full potential. This paper addresses the joint problem of service placement and resource allocation in a network-cloud integrated environment while considering capacity constraints, dynamic users, and end-to-end delays. We present a non-linear programming model that formulates the optimization problem with the aiming objectiv
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22914;&#20309;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10182</link><description>&lt;p&gt;
&#38899;&#20048;&#20135;&#21697;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive and Risky Message Assessment for Music Products. (arXiv:2309.10182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22914;&#20309;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#35282;&#24230;&#22810;&#32423;&#38899;&#20048;&#20869;&#23481;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#20248;&#20110;&#24378;&#22823;&#30340;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#23545;&#24212;&#26041;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel research problem: assessing positive and risky messages from music products. We first establish a benchmark for multi-angle multi-level music content assessment and then present an effective multi-task prediction model with ordinality-enforcement to solve this problem. Our result shows the proposed method not only significantly outperforms strong task-specific counterparts but can concurrently evaluate multiple aspects.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;B&amp;B-CCRA&#21644;WF-CCRA&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.10180</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#20302;&#24310;&#36831;5G&#24212;&#29992;&#36335;&#24452;&#36873;&#25321;&#21644;&#26381;&#21153;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Double Deep Q-Learning-based Path Selection and Service Placement for Latency-Sensitive Beyond 5G Applications. (arXiv:2309.10180v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;B&amp;B-CCRA&#21644;WF-CCRA&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#38543;&#30528;&#23545;&#23481;&#37327;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#20840;&#26032;&#30340;&#26381;&#21153;&#27491;&#22312;&#28044;&#29616;&#12290;&#20026;&#20102;&#20197;&#23454;&#26102;&#21709;&#24212;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#25552;&#20379;&#36825;&#20123;&#26381;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;&#22362;&#23454;&#30340;&#20113;&#32593;&#32476;&#38598;&#25104;&#22522;&#30784;&#35774;&#26045;&#12290;&#30001;&#20110;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#38480;&#23481;&#37327;&#65292;&#24517;&#39035;&#21327;&#21516;&#31649;&#29702;&#36825;&#20123;&#36164;&#28304;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#26041;&#27861;&#26469;&#32534;&#25490;&#36825;&#20123;&#36164;&#28304;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#24573;&#30053;&#20102;&#32593;&#32476;&#36164;&#28304;&#25110;&#23558;&#32593;&#32476;&#31616;&#21270;&#20026;&#31616;&#21333;&#30340;&#22270;&#65292;&#21482;&#20851;&#27880;&#20113;&#36164;&#28304;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#38382;&#39064;&#65292;&#31216;&#20026;CCRA&#65292;&#21253;&#25324;&#21151;&#33021;&#37096;&#32626;&#21644;&#20998;&#37197;&#12289;&#27969;&#37327;&#20248;&#20808;&#32423;&#21644;&#36335;&#24452;&#36873;&#25321;&#65292;&#32771;&#34385;&#23481;&#37327;&#38480;&#21046;&#21644;&#36136;&#37327;&#35201;&#27714;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#38750;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;B&amp;B-CCRA&#21644;WF-CCRA&#65292;&#22522;&#20110;Br...
&lt;/p&gt;
&lt;p&gt;
Nowadays, as the need for capacity continues to grow, entirely novel services are emerging. A solid cloud-network integrated infrastructure is necessary to supply these services in a real-time responsive, and scalable way. Due to their diverse characteristics and limited capacity, communication and computing resources must be collaboratively managed to unleash their full potential. Although several innovative methods have been proposed to orchestrate the resources, most ignored network resources or relaxed the network as a simple graph, focusing only on cloud resources. This paper fills the gap by studying the joint problem of communication and computing resource allocation, dubbed CCRA, including function placement and assignment, traffic prioritization, and path selection considering capacity constraints and quality requirements, to minimize total cost. We formulate the problem as a non-linear programming model and propose two approaches, dubbed B\&amp;B-CCRA and WF-CCRA, based on the Br
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#20803;&#23431;&#23449;&#24212;&#29992;&#20013;&#30340;&#22810;&#37325;&#25509;&#20837;&#38382;&#39064;&#65292;&#22312;&#37319;&#29992;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#36830;&#32493;&#35757;&#32451;&#26469;&#23454;&#29616;&#33258;&#25105;&#32500;&#25345;&#30340;&#31574;&#30053;&#12290;&#35813;&#30740;&#31350;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#36866;&#24212;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#20195;&#29702;&#26426;&#21046;&#38382;&#39064;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.10177</link><description>&lt;p&gt;
&#33258;&#25105;&#32500;&#25345;&#30340;&#36830;&#32493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21160;&#24577;&#20803;&#23431;&#23449;&#24212;&#29992;&#20013;&#30340;&#22810;&#37325;&#25509;&#20837;
&lt;/p&gt;
&lt;p&gt;
Self-Sustaining Multiple Access with Continual Deep Reinforcement Learning for Dynamic Metaverse Applications. (arXiv:2309.10177v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#20803;&#23431;&#23449;&#24212;&#29992;&#20013;&#30340;&#22810;&#37325;&#25509;&#20837;&#38382;&#39064;&#65292;&#22312;&#37319;&#29992;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#36830;&#32493;&#35757;&#32451;&#26469;&#23454;&#29616;&#33258;&#25105;&#32500;&#25345;&#30340;&#31574;&#30053;&#12290;&#35813;&#30740;&#31350;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#36866;&#24212;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#20195;&#29702;&#26426;&#21046;&#38382;&#39064;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#26159;&#19968;&#20010;&#26088;&#22312;&#21019;&#24314;&#30001;&#20247;&#22810;&#19990;&#30028;&#32452;&#25104;&#30340;&#34394;&#25311;&#29615;&#22659;&#30340;&#26032;&#33539; Paradigm&#65292;&#20854;&#20013;&#27599;&#20010;&#19990;&#30028;&#23558;&#25552;&#20379;&#19981;&#21516;&#30340;&#26381;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#22914;&#27492;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#22330;&#26223;&#65292;&#24182;&#32771;&#34385;&#21040;&#38024;&#23545;&#31532;&#20845;&#20195;&#36890;&#20449;&#31995;&#32479;&#65288;6G&#65289;&#30340;&#20005;&#26684;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#65292;&#19968;&#31181;&#28508;&#22312;&#30340;&#26041;&#27861;&#26159;&#37319;&#29992;&#33258;&#25105;&#32500;&#25345;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#65288;&#33258;&#36866;&#24212;AI&#65289;&#26469;&#23454;&#29616;&#65292;&#20854;&#20013;&#27169;&#22411;&#23558;&#19981;&#26029;&#22320;&#26681;&#25454;&#26032;&#30340;&#25968;&#25454;&#21644;&#26465;&#20214;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#33258;&#25105;&#32500;&#25345;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#23545;&#39057;&#35889;&#30340;&#22810;&#37325;&#25509;&#20837;&#36827;&#34892;&#31649;&#29702;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#65292;&#20294;&#26159;&#36866;&#24212;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#20195;&#29702;&#26426;&#21046;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20934;&#30830;&#35299;&#20915;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22810;&#36890;&#36947;&#29615;&#22659;&#20013;&#30340;&#22810;&#37325;&#25509;&#20837;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Metaverse is a new paradigm that aims to create a virtual environment consisting of numerous worlds, each of which will offer a different set of services. To deal with such a dynamic and complex scenario, considering the stringent quality of service requirements aimed at the 6th generation of communication systems (6G), one potential approach is to adopt self-sustaining strategies, which can be realized by employing Adaptive Artificial Intelligence (Adaptive AI) where models are continually re-trained with new data and conditions. One aspect of self-sustainability is the management of multiple access to the frequency spectrum. Although several innovative methods have been proposed to address this challenge, mostly using Deep Reinforcement Learning (DRL), the problem of adapting agents to a non-stationary environment has not yet been precisely addressed. This paper fills in the gap in the current literature by investigating the problem of multiple access in multi-channel environment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#20154;&#31867;&#28436;&#31034;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36712;&#36857;&#65292;&#25104;&#21151;&#23436;&#25104;&#20102;&#19977;&#20010;&#22359;&#25805;&#20316;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26242;&#23384;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#20316;&#22359;&#20195;&#29702;&#30340;&#34892;&#21160;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.10175</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#28436;&#31034;&#34892;&#20026;&#20811;&#38534;&#19982;&#21160;&#20316;&#20998;&#22359;&#36716;&#25442;&#30340;&#19968;&#20154;&#21095;&#26412;
&lt;/p&gt;
&lt;p&gt;
One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers. (arXiv:2309.10175v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#20154;&#31867;&#28436;&#31034;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36712;&#36857;&#65292;&#25104;&#21151;&#23436;&#25104;&#20102;&#19977;&#20010;&#22359;&#25805;&#20316;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26242;&#23384;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#20316;&#22359;&#20195;&#29702;&#30340;&#34892;&#21160;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#31034;&#33539;&#20013;&#23398;&#20064;&#65288;&#34892;&#20026;&#20811;&#38534;&#65289;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#34892;&#20026;&#20811;&#38534;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#31034;&#33539;&#26469;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#22810;&#26679;&#21270;&#21021;&#22987;&#26465;&#20214;&#30340;&#19968;&#33324;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21482;&#38656;&#30475;&#21040;&#19968;&#20004;&#20010;&#31034;&#33539;&#23601;&#33021;&#23398;&#20250;&#23436;&#25104;&#20219;&#21153;&#65292;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#27169;&#25311;&#36825;&#31181;&#33021;&#21147;&#65292;&#20165;&#20973;&#19968;&#27425;&#20154;&#31867;&#28436;&#31034;&#23601;&#20351;&#29992;&#34892;&#20026;&#20811;&#38534;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#26469;&#22686;&#24378;&#21333;&#20010;&#28436;&#31034;&#65292;&#29983;&#25104;&#19968;&#31995;&#21015;&#36866;&#29992;&#20110;&#21508;&#31181;&#21021;&#22987;&#26465;&#20214;&#30340;&#36712;&#36857;&#65292;&#23454;&#29616;&#20102;&#36825;&#20010;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#20123;&#31034;&#33539;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#25104;&#21151;&#23436;&#25104;&#19977;&#20010;&#22359;&#25805;&#20316;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26242;&#23384;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#20316;&#22359;&#20195;&#29702;&#30340;&#34892;&#21160;&#39044;&#27979;&#20013;&#32508;&#21512;&#26631;&#20934;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human demonstrations (behavior cloning) is a cornerstone of robot learning. However, most behavior cloning algorithms require a large number of demonstrations to learn a task, especially for general tasks that have a large variety of initial conditions. Humans, however, can learn to complete tasks, even complex ones, after only seeing one or two demonstrations. Our work seeks to emulate this ability, using behavior cloning to learn a task given only a single human demonstration. We achieve this goal by using linear transforms to augment the single demonstration, generating a set of trajectories for a wide range of initial conditions. With these demonstrations, we are able to train a behavior cloning agent to successfully complete three block manipulation tasks. Additionally, we developed a novel addition to the temporal ensembling method used by action chunking agents during inference. By incorporating the standard deviation of the action predictions into the ensembling m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#27493;&#24863;&#30693;-&#21160;&#20316;-&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#20013;&#21327;&#20316;&#21644;&#36890;&#20449;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#20551;&#35774;&#39034;&#24207;&#25191;&#34892;&#65292;&#35813;&#26041;&#27861;&#26159;&#23436;&#20840;&#20998;&#25955;&#30340;&#65292;&#20294;&#22312;&#35780;&#20272;&#21644;&#37096;&#32626;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.10164</link><description>&lt;p&gt;
&#24322;&#27493;&#24863;&#30693;-&#21160;&#20316;-&#36890;&#20449;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Perception-Action-Communication with Graph Neural Networks. (arXiv:2309.10164v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#27493;&#24863;&#30693;-&#21160;&#20316;-&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#20013;&#21327;&#20316;&#21644;&#36890;&#20449;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#20551;&#35774;&#39034;&#24207;&#25191;&#34892;&#65292;&#35813;&#26041;&#27861;&#26159;&#23436;&#20840;&#20998;&#25955;&#30340;&#65292;&#20294;&#22312;&#35780;&#20272;&#21644;&#37096;&#32626;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#20013;&#23454;&#29616;&#20849;&#21516;&#30340;&#20840;&#23616;&#30446;&#26631;&#30340;&#21327;&#20316;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#36890;&#20449;&#33021;&#21147;&#26377;&#38480;&#12290;&#26426;&#22120;&#20154;&#24517;&#39035;&#25191;&#34892;&#24863;&#30693;-&#21160;&#20316;-&#36890;&#20449;&#65288;PAC&#65289;&#24490;&#29615;-&#23427;&#20204;&#24863;&#30693;&#23616;&#37096;&#29615;&#22659;&#65292;&#19982;&#20854;&#20182;&#26426;&#22120;&#20154;&#36890;&#20449;&#65292;&#24182;&#23454;&#26102;&#37319;&#21462;&#34892;&#21160;&#12290;&#20998;&#25955;&#30340;PAC&#31995;&#32479;&#38754;&#20020;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#20915;&#23450;&#19982;&#30456;&#37051;&#26426;&#22120;&#20154;&#36890;&#20449;&#30340;&#20449;&#24687;&#20197;&#21450;&#22914;&#20309;&#22312;&#21033;&#29992;&#37051;&#23621;&#20849;&#20139;&#30340;&#20449;&#24687;&#30340;&#21516;&#26102;&#37319;&#21462;&#34892;&#21160;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#27604;&#22914;&#22312;&#32676;&#38598;&#21644;&#35206;&#30422;&#25511;&#21046;&#31561;&#24212;&#29992;&#20013;&#12290;&#34429;&#28982;&#22312;&#27010;&#24565;&#19978;&#65292;GNN&#31574;&#30053;&#26159;&#23436;&#20840;&#20998;&#25955;&#30340;&#65292;&#20294;&#35780;&#20272;&#21644;&#37096;&#32626;&#36825;&#26679;&#30340;&#31574;&#30053;&#20027;&#35201;&#20173;&#28982;&#26159;&#38598;&#20013;&#24335;&#30340;&#25110;&#20855;&#26377;&#38480;&#21046;&#24615;&#30340;&#20998;&#25955;&#24335;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26694;&#26550;&#20551;&#35774;&#24863;&#30693;&#21644;&#21160;&#20316;&#25512;&#29702;&#30340;&#39034;&#24207;&#25191;&#34892;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#38750;&#24120;&#38480;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaboration in large robot swarms to achieve a common global objective is a challenging problem in large environments due to limited sensing and communication capabilities. The robots must execute a Perception-Action-Communication (PAC) loop -- they perceive their local environment, communicate with other robots, and take actions in real time. A fundamental challenge in decentralized PAC systems is to decide what information to communicate with the neighboring robots and how to take actions while utilizing the information shared by the neighbors. Recently, this has been addressed using Graph Neural Networks (GNNs) for applications such as flocking and coverage control. Although conceptually, GNN policies are fully decentralized, the evaluation and deployment of such policies have primarily remained centralized or restrictively decentralized. Furthermore, existing frameworks assume sequential execution of perception and action inference, which is very restrictive in real-world applica
&lt;/p&gt;</description></item><item><title>RadOnc-GPT&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#29983;&#25104;&#27835;&#30103;&#26041;&#26696;&#12289;&#30830;&#23450;&#30103;&#27861;&#21644;&#25552;&#20379;&#35786;&#26029;&#25551;&#36848;&#31561;&#20851;&#38190;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10160</link><description>&lt;p&gt;
RadOnc-GPT&#65306;&#19968;&#31181;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RadOnc-GPT: A Large Language Model for Radiation Oncology. (arXiv:2309.10160v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10160
&lt;/p&gt;
&lt;p&gt;
RadOnc-GPT&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#29983;&#25104;&#27835;&#30103;&#26041;&#26696;&#12289;&#30830;&#23450;&#30103;&#27861;&#21644;&#25552;&#20379;&#35786;&#26029;&#25551;&#36848;&#31561;&#20851;&#38190;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RadOnc-GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#35843;&#25972;&#26041;&#27861;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#12290;RadOnc-GPT&#22312;Mayo Clinic&#30340;&#22823;&#37327;&#25918;&#23556;&#32959;&#30244;&#23398;&#24739;&#32773;&#35760;&#24405;&#21644;&#20020;&#24202;&#31508;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#20219;&#21153;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#21253;&#25324;&#29983;&#25104;&#25918;&#23556;&#27835;&#30103;&#26041;&#26696;&#12289;&#30830;&#23450;&#26368;&#20339;&#25918;&#23556;&#30103;&#27861;&#20197;&#21450;&#22522;&#20110;&#24739;&#32773;&#35786;&#26029;&#32454;&#33410;&#25552;&#20379;&#35786;&#26029;&#25551;&#36848;/ICD&#20195;&#30721;&#12290;&#36890;&#36807;&#23558;&#25918;&#23556;&#32959;&#30244;&#23398;&#21307;&#29983;&#27604;&#36739;RadOnc-GPT&#30340;&#21360;&#35937;&#19982;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#65292;&#30740;&#31350;&#34920;&#26126;RadOnc-GPT&#29983;&#25104;&#30340;&#36755;&#20986;&#22312;&#28165;&#26224;&#24230;&#12289;&#29305;&#24322;&#24230;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#20687;RadOnc-GPT&#36825;&#26679;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#31561;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#21307;&#30103;&#39046;&#22495;&#23454;&#29616;&#21464;&#38761;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RadOnc-GPT, a large language model specialized for radiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on a large dataset of radiation oncology patient records and clinical notes from the Mayo Clinic. The model employs instruction tuning on three key tasks generating radiotherapy treatment regimens, determining optimal radiation modalities, and providing diagnostic descriptions/ICD codes based on patient diagnostic details. Evaluations conducted by having radiation oncologists compare RadOnc-GPT impressions to general large language model impressions showed that RadOnc-GPT generated outputs with significantly improved clarity, specificity, and clinical relevance. The study demonstrated the potential of using large language models fine-tuned using domain-specific knowledge like RadOnc-GPT to achieve transformational capabilities in highly specialized healthcare fields such as radiation oncology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#26469;&#33719;&#24471;&#23545;&#26410;&#30693;&#21464;&#21270;&#40065;&#26834;&#30340;&#39044;&#27979;&#22120;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10149</link><description>&lt;p&gt;
AI&#20195;&#29702;&#30340;&#35760;&#24518;&#21644;&#27867;&#21270;&#33021;&#21147;&#20998;&#26512;&#65306;&#36830;&#32493;&#23398;&#20064;&#32773;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?. (arXiv:2309.10149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#26469;&#33719;&#24471;&#23545;&#26410;&#30693;&#21464;&#21270;&#40065;&#26834;&#30340;&#39044;&#27979;&#22120;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;AI&#20195;&#29702;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25110;&#26426;&#22120;&#20154;&#65289;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#20174;&#38750;&#31283;&#24577;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#12290;&#23545;&#20110;&#36825;&#31867;&#24212;&#29992;&#30340;&#23454;&#38469;&#37096;&#32626;&#65292;&#20445;&#35777;&#23545;&#26410;&#30693;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#20445;&#30041;&#36807;&#21435;&#30340;&#32463;&#39564;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#32771;&#34385;&#21040;&#36830;&#32493;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#65292;&#20197;&#20272;&#35745;&#29615;&#22659;&#21464;&#21270;&#30340;&#39118;&#38505;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#23545;&#26410;&#30693;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#22120;&#12290;&#23545;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#24615;&#33021;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#38543;&#20869;&#23384;&#22823;&#23567;&#30340;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning (CL), an AI agent (e.g., autonomous vehicles or robotics) learns from non-stationary data streams under dynamic environments. For the practical deployment of such applications, it is important to guarantee robustness to unseen environments while maintaining past experiences. In this paper, a novel CL framework is proposed to achieve robust generalization to dynamic environments while retaining past knowledge. The considered CL agent uses a capacity-limited memory to save previously observed environmental information to mitigate forgetting issues. Then, data points are sampled from the memory to estimate the distribution of risks over environmental change so as to obtain predictors that are robust with unseen changes. The generalization and memorization performance of the proposed framework are theoretically analyzed. This analysis showcases the tradeoff between memorization and generalization with the memory size. Experiments show that the proposed algorithm outpe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20154;&#20307;&#27493;&#24577;&#35782;&#21035;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#21183;&#20197;&#21450;&#21487;&#33021;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.10144</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#20307;&#27493;&#24577;&#35782;&#21035;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Human Gait Recognition using Deep Learning: A Comprehensive Review. (arXiv:2309.10144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10144
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20154;&#20307;&#27493;&#24577;&#35782;&#21035;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#21183;&#20197;&#21450;&#21487;&#33021;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27493;&#24577;&#35782;&#21035;&#65288;GR&#65289;&#26159;&#36890;&#36807;&#35270;&#35273;&#25668;&#20687;&#22836;&#20174;&#36828;&#22788;&#35782;&#21035;&#20154;&#21592;&#30340;&#29983;&#29289;&#27979;&#23450;&#27169;&#24335;&#12290;GR&#25552;&#20379;&#20102;&#19968;&#20010;&#23433;&#20840;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#25351;&#32441;&#21644;&#38754;&#37096;&#35782;&#21035;&#65292;&#22240;&#20026;&#24456;&#38590;&#21306;&#20998;&#20551;&#35937;&#21644;&#30495;&#23454;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#23427;&#23545;&#27450;&#35784;&#30340;&#25269;&#25239;&#21147;&#20351;&#24471;GR&#36866;&#29992;&#20110;&#21508;&#31181;&#29615;&#22659;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20852;&#36215;&#65292;GR&#25216;&#26415;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#12290;&#38543;&#30528;&#35270;&#39057;&#30417;&#25511;&#30340;&#26222;&#21450;&#65292;&#26032;&#30340;&#38556;&#30861;&#19981;&#26029;&#20986;&#29616;&#65292;&#20363;&#22914;&#30830;&#20445;&#22312;&#19981;&#21516;&#21327;&#35758;&#20013;&#36827;&#34892;&#32479;&#19968;&#24615;&#33021;&#35780;&#20272;&#65292;&#21487;&#38752;&#30340;&#35782;&#21035;&#23613;&#31649;&#20809;&#29031;&#26465;&#20214;&#21464;&#21270;&#65292;&#27493;&#24577;&#27169;&#24335;&#27874;&#21160;&#20197;&#21450;&#20445;&#25252;&#38544;&#31169;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#27010;&#36848;GR&#24182;&#20998;&#26512;&#21487;&#33021;&#24433;&#21709;&#20854;&#29615;&#22659;&#35201;&#32032;&#21644;&#22797;&#26434;&#24615;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#29983;&#29289;&#27979;&#23450;&#31995;&#32479;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#30740;&#31350;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gait recognition (GR) is a growing biometric modality used for person identification from a distance through visual cameras. GR provides a secure and reliable alternative to fingerprint and face recognition, as it is harder to distinguish between false and authentic signals. Furthermore, its resistance to spoofing makes GR suitable for all types of environments. With the rise of deep learning, steadily improving strides have been made in GR technology with promising results in various contexts. As video surveillance becomes more prevalent, new obstacles arise, such as ensuring uniform performance evaluation across different protocols, reliable recognition despite shifting lighting conditions, fluctuations in gait patterns, and protecting privacy.This survey aims to give an overview of GR and analyze the environmental elements and complications that could affect it in comparison to other biometric recognition systems. The primary goal is to examine the existing deep learning (DL) techni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20302;&#31209;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ELR-GNN&#65289;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20302;&#31209;&#21644;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26356;&#39640;&#30340;&#25928;&#29575;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2309.10136</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#20302;&#31209;&#22270;&#31070;&#32463;&#32593;&#32476;&#25269;&#24481;&#32467;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Efficient Low-Rank GNN Defense Against Structural Attacks. (arXiv:2309.10136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20302;&#31209;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ELR-GNN&#65289;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20302;&#31209;&#21644;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26356;&#39640;&#30340;&#25928;&#29575;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#25968;&#25454;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;GNNs&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#20063;&#20250;&#20005;&#37325;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23545;&#22797;&#26434;&#25915;&#20987;&#26080;&#25928;&#65292;&#35201;&#20040;&#38656;&#35201;&#20248;&#21270;&#31264;&#23494;&#37051;&#25509;&#30697;&#38453;&#65292;&#36825;&#38750;&#24120;&#32791;&#26102;&#19988;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20302;&#31209;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ELR-GNN&#65289;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#20302;&#31209;&#21644;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#20445;&#35777;&#26377;&#25928;&#30340;&#38450;&#24481;&#19982;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ELR-GNN&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#31895;&#31961;&#20302;&#31209;&#20272;&#35745;&#27169;&#22359;&#21644;&#32454;&#31890;&#24230;&#20272;&#35745;&#27169;&#22359;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#37319;&#29992;&#25130;&#26029;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#26469;&#21021;&#22987;&#21270;&#20302;&#31209;&#37051;&#25509;&#30697;&#38453;&#30340;&#20272;&#35745;&#65292;&#36825;&#20316;&#20026;&#20248;&#21270;&#20302;&#31209;&#37051;&#25509;&#30697;&#38453;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been shown to possess strong representation abilities over graph data. However, GNNs are vulnerable to adversarial attacks, and even minor perturbations to the graph structure can significantly degrade their performance. Existing methods either are ineffective against sophisticated attacks or require the optimization of dense adjacency matrices, which is time-consuming and prone to local minima. To remedy this problem, we propose an Efficient Low-Rank Graph Neural Network (ELR-GNN) defense method, which aims to learn low-rank and sparse graph structures for defending against adversarial attacks, ensuring effective defense with greater efficiency. Specifically, ELR-GNN consists of two modules: a Coarse Low-Rank Estimation Module and a Fine-Grained Estimation Module. The first module adopts the truncated Singular Value Decomposition (SVD) to initialize the low-rank adjacency matrix estimation, which serves as a starting point for optimizing the low-rank 
&lt;/p&gt;</description></item><item><title>GDM&#26159;&#19968;&#31181;&#21452;&#37325;&#30340;mixup&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#23454;&#20363;&#30340;&#21151;&#33021;&#21644;&#32467;&#26500;&#20449;&#24687;&#29983;&#25104;&#26032;&#30340;&#26631;&#35760;&#22270;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.10134</link><description>&lt;p&gt;
GDM: &#21452;&#37325;Mixup&#29992;&#20110;&#26377;&#38480;&#26631;&#27880;&#30340;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
GDM: Dual Mixup for Graph Classification with Limited Supervision. (arXiv:2309.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10134
&lt;/p&gt;
&lt;p&gt;
GDM&#26159;&#19968;&#31181;&#21452;&#37325;&#30340;mixup&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#23454;&#20363;&#30340;&#21151;&#33021;&#21644;&#32467;&#26500;&#20449;&#24687;&#29983;&#25104;&#26032;&#30340;&#26631;&#35760;&#22270;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#23545;&#20110;&#22270;&#20998;&#31867;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#22270;&#26679;&#26412;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#26631;&#35760;&#22270;&#26679;&#26412;&#25968;&#37327;&#30340;&#20943;&#23569;&#65292;GNNs&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#65292;&#24320;&#21457;&#33021;&#22815;&#29983;&#25104;&#26032;&#30340;&#22270;&#23454;&#20363;&#20197;&#22686;&#21152;&#21487;&#29992;&#26631;&#35760;&#22270;&#26679;&#26412;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;mixup&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;Graph Dual Mixup (GDM)&#65292;&#23427;&#21033;&#29992;&#22270;&#23454;&#20363;&#30340;&#21151;&#33021;&#21644;&#32467;&#26500;&#20449;&#24687;&#26469;&#29983;&#25104;&#26032;&#30340;&#26631;&#35760;&#22270;&#26679;&#26412;&#12290;GDM&#20351;&#29992;&#22270;&#32467;&#26500;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#22270;&#26679;&#26412;&#30340;&#32467;&#26500;&#23884;&#20837;&#65292;&#28982;&#21518;&#22312;&#23398;&#21040;&#30340;&#32467;&#26500;&#23884;&#20837;&#31354;&#38388;&#20013;&#24212;&#29992;mixup&#21040;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20174;mixup&#32467;&#26500;&#23884;&#20837;&#20013;&#29983;&#25104;&#26032;&#30340;&#22270;&#32467;&#26500;&#12290;&#33267;&#20110;&#21151;&#33021;&#20449;&#24687;&#65292;GDM&#24212;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) require a large number of labeled graph samples to obtain good performance on the graph classification task. The performance of GNNs degrades significantly as the number of labeled graph samples decreases. To reduce the annotation cost, it is therefore important to develop graph augmentation methods that can generate new graph instances to increase the size and diversity of the limited set of available labeled graph samples. In this work, we propose a novel mixup-based graph augmentation method, Graph Dual Mixup (GDM), that leverages both functional and structural information of the graph instances to generate new labeled graph samples. GDM employs a graph structural auto-encoder to learn structural embeddings of the graph samples, and then applies mixup to the structural information of the graphs in the learned structural embedding space and generates new graph structures from the mixup structural embeddings. As for the functional information, GDM applies 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;Uniswap V3&#20013;&#30340;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20215;&#26684;&#33539;&#22260;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#21033;&#28070;&#21644;&#20943;&#36731;&#24066;&#22330;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.10129</link><description>&lt;p&gt;
Uniswap V3&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36866;&#24212;&#24615;&#27969;&#21160;&#24615;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning. (arXiv:2309.10129v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;Uniswap V3&#20013;&#30340;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20215;&#26684;&#33539;&#22260;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#21033;&#28070;&#21644;&#20943;&#36731;&#24066;&#22330;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#20132;&#26131;&#25152;&#65288;DEX&#65289;&#26159;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#30340;&#22522;&#30707;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#26080;&#38656;&#31532;&#19977;&#26041;&#25480;&#26435;&#30340;&#24773;&#20917;&#19979;&#20132;&#26131;&#21152;&#23494;&#36135;&#24065;&#12290;&#25237;&#36164;&#32773;&#26377;&#21160;&#21147;&#23558;&#36164;&#20135;&#23384;&#20837;&#27969;&#21160;&#24615;&#27744;&#65292;&#29992;&#25143;&#21487;&#20197;&#30452;&#25509;&#19982;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#36827;&#34892;&#20132;&#26131;&#65292;&#24182;&#25903;&#20184;&#36153;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#36164;&#26412;&#25928;&#29575;&#21644;&#24066;&#22330;&#39118;&#38505;&#30456;&#20851;&#30340;&#19968;&#20123;&#26410;&#35299;&#20915;&#38382;&#39064;&#38459;&#30861;&#20102;DeFi&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;Uniswap V3&#26159;&#19968;&#39033;&#39046;&#20808;&#32780;&#24320;&#21019;&#24615;&#30340;DEX&#39033;&#30446;&#65292;&#36890;&#36807;&#20351;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#23558;&#27969;&#21160;&#24615;&#38598;&#20013;&#22312;&#29305;&#23450;&#20215;&#26684;&#33539;&#22260;&#20869;&#65292;&#35299;&#20915;&#20102;&#36164;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21152;&#21095;&#20102;&#24066;&#22330;&#39118;&#38505;&#65292;&#22240;&#20026;&#21482;&#26377;&#24403;&#36164;&#20135;&#20215;&#26684;&#22312;&#36825;&#20123;&#39044;&#23450;&#30340;&#20215;&#26684;&#33539;&#22260;&#20869;&#26102;&#65292;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#25165;&#33021;&#33719;&#24471;&#20132;&#26131;&#36153;&#29992;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#33258;&#36866;&#24212;&#35843;&#25972;&#20215;&#26684;&#33539;&#22260;&#65292;&#26368;&#22823;&#21270;&#21033;&#28070;&#24182;&#20943;&#36731;&#24066;&#22330;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#22815;&#25269;&#28040;&#20215;&#26684;&#21464;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized exchanges (DEXs) are a cornerstone of decentralized finance (DeFi), allowing users to trade cryptocurrencies without the need for third-party authorization. Investors are incentivized to deposit assets into liquidity pools, against which users can trade directly, while paying fees to liquidity providers (LPs). However, a number of unresolved issues related to capital efficiency and market risk hinder DeFi's further development. Uniswap V3, a leading and groundbreaking DEX project, addresses capital efficiency by enabling LPs to concentrate their liquidity within specific price ranges for deposited assets. Nevertheless, this approach exacerbates market risk, as LPs earn trading fees only when asset prices are within these predetermined brackets. To mitigate this issue, this paper introduces a deep reinforcement learning (DRL) solution designed to adaptively adjust these price ranges, maximizing profits and mitigating market risks. Our approach also neutralizes price-change
&lt;/p&gt;</description></item><item><title>AR-TTA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#23558;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10109</link><description>&lt;p&gt;
AR-TTA: &#19968;&#31181;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation. (arXiv:2309.10109v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10109
&lt;/p&gt;
&lt;p&gt;
AR-TTA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#23558;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#23427;&#20801;&#35768;&#28304;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#21482;&#26159;&#23454;&#38469;&#22330;&#26223;&#31616;&#21270;&#29256;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26368;&#36817;&#25512;&#20986;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;CLAD-C&#21644;SHIFT&#26469;&#39564;&#35777;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#21069;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#26377;&#25928;&#22788;&#29702;&#19981;&#21516;&#31243;&#24230;&#30340;&#22495;&#20559;&#31227;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#20302;&#20110;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#38382;&#39064;&#30340;&#26681;&#28304;&#22312;&#20110;&#26080;&#27861;&#20445;&#30041;&#28304;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#12289;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#19968;&#20010;&#23567;&#30340;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#21040;&#25104;&#29087;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#20013;&#65292;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#21516;&#26102;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;OUTDOOR&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#23460;&#22806;&#29615;&#22659;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#24863;&#30693;&#30340;&#25104;&#21151;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.10103</link><description>&lt;p&gt;
&#23545;&#20110;&#39640;&#25928;&#30340;&#23460;&#22806;&#29289;&#20307;&#23548;&#33322;&#65292;&#20851;&#20110;&#26410;&#35265;&#20043;&#29289;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning about the Unseen for Efficient Outdoor Object Navigation. (arXiv:2309.10103v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;OUTDOOR&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#23460;&#22806;&#29615;&#22659;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#24863;&#30693;&#30340;&#25104;&#21151;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#24212;&#35813;&#23384;&#22312;&#20110;&#20154;&#31867;&#23384;&#22312;&#30340;&#20219;&#20309;&#22320;&#26041;: &#23460;&#20869;&#12289;&#23460;&#22806;&#65292;&#29978;&#33267;&#26159;&#26410;&#32472;&#21046;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#30446;&#26631;&#23548;&#33322;(OGN)&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#23460;&#20869;&#31354;&#38388;&#21644;&#35821;&#20041;&#32447;&#32034;&#26469;&#36827;&#34892;&#23460;&#20869;&#23548;&#33322;&#65292;&#32780;&#36825;&#20123;&#32447;&#32034;&#22312;&#23460;&#22806;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#23613;&#31649;&#36825;&#20123;&#36129;&#29486;&#20026;&#23460;&#20869;&#22330;&#26223;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#20294;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#24448;&#24448;&#25193;&#23637;&#21040;&#23460;&#22806;&#29615;&#22659;&#65292;&#32780;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#23384;&#22312;&#26032;&#30340;&#25361;&#25112;&#12290;&#19982;&#23460;&#20869;&#32467;&#26500;&#21270;&#24067;&#23616;&#19981;&#21516;&#65292;&#23460;&#22806;&#29615;&#22659;&#32570;&#20047;&#28165;&#26224;&#30340;&#31354;&#38388;&#30028;&#23450;&#65292;&#24182;&#19988;&#23384;&#22312;&#22266;&#26377;&#30340;&#35821;&#20041;&#27495;&#20041;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#23548;&#33322;&#65292;&#22240;&#20026;&#25105;&#20204;&#33021;&#22815;&#25512;&#29702;&#26410;&#35265;&#20043;&#29289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;OUTDOOR&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20934;&#30830;&#22320;&#34394;&#26500;&#21487;&#33021;&#30340;&#26410;&#26469;&#65292;&#24182;&#20026;&#25512;&#21160;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#24863;&#30693;&#30340;&#25104;&#21151;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more com
&lt;/p&gt;</description></item><item><title>Data Formulator&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#35270;&#21270;&#21019;&#20316;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#24565;&#32465;&#23450;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#21487;&#35270;&#21270;&#24847;&#22270;&#21644;&#20302;&#32423;&#25968;&#25454;&#36716;&#25442;&#27493;&#39588;&#20998;&#31163;&#65292;&#21033;&#29992;AI&#20195;&#29702;&#33258;&#21160;&#36716;&#25442;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#25152;&#38656;&#30340;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.10094</link><description>&lt;p&gt;
&#25968;&#25454;&#20844;&#24335;&#21270;: &#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#27010;&#24565;&#39537;&#21160;&#21487;&#35270;&#21270;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Data Formulator: AI-powered Concept-driven Visualization Authoring. (arXiv:2309.10094v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10094
&lt;/p&gt;
&lt;p&gt;
Data Formulator&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#35270;&#21270;&#21019;&#20316;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#24565;&#32465;&#23450;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#21487;&#35270;&#21270;&#24847;&#22270;&#21644;&#20302;&#32423;&#25968;&#25454;&#36716;&#25442;&#27493;&#39588;&#20998;&#31163;&#65292;&#21033;&#29992;AI&#20195;&#29702;&#33258;&#21160;&#36716;&#25442;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#25152;&#38656;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;&#21487;&#35270;&#21270;&#24037;&#20855;&#38656;&#35201;&#20316;&#32773;&#23558;&#20854;&#25968;&#25454;&#36716;&#25442;&#20026;&#25972;&#27905;&#26684;&#24335;&#20197;&#21019;&#24314;&#25152;&#38656;&#30340;&#21487;&#35270;&#21270;&#12290;&#30001;&#20110;&#36825;&#38656;&#35201;&#20855;&#22791;&#32534;&#31243;&#25110;&#21333;&#29420;&#30340;&#25968;&#25454;&#22788;&#29702;&#24037;&#20855;&#30340;&#32463;&#39564;&#65292;&#25968;&#25454;&#36716;&#25442;&#20173;&#28982;&#26159;&#21487;&#35270;&#21270;&#21019;&#20316;&#30340;&#19968;&#20010;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#33539;&#24335;&#65292;&#21363;&#27010;&#24565;&#32465;&#23450;&#65292;&#23427;&#23558;&#39640;&#32423;&#21487;&#35270;&#21270;&#24847;&#22270;&#21644;&#20302;&#32423;&#25968;&#25454;&#36716;&#25442;&#27493;&#39588;&#20998;&#24320;&#65292;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;Data Formulator&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#21487;&#35270;&#21270;&#21019;&#20316;&#24037;&#20855;&#12290;&#20351;&#29992;Data Formulator&#65292;&#20316;&#32773;&#39318;&#20808;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25110;&#31034;&#20363;&#23450;&#20041;&#20182;&#20204;&#35745;&#21010;&#21487;&#35270;&#21270;&#30340;&#25968;&#25454;&#27010;&#24565;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32465;&#23450;&#21040;&#21487;&#35270;&#36890;&#36947;&#12290;Data Formulator&#28982;&#21518;&#23558;&#20854;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#27966;&#36963;&#20986;&#21435;&#65292;&#33258;&#21160;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#21576;&#29616;&#36825;&#20123;&#27010;&#24565;&#21644;&#29983;&#25104;&#25152;&#38656;&#21487;&#35270;&#21270;&#30340;&#25968;&#25454;&#12290;&#22312;&#21521;Data Formulator&#23637;&#31034;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#32467;&#26524;(&#36716;&#25442;&#21518;&#30340;&#34920;&#26684;&#21644;&#36755;&#20986;&#30340;&#21487;&#35270;&#21270;)&#26102;&#65292;Data Formulator&#25552;&#20379;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.10092</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#65306;&#30693;&#36947;&#20309;&#26102;&#20570;&#20160;&#20040;&#21644;&#20309;&#26102;&#23547;&#27714;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help. (arXiv:2309.10092v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#20197;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#34920;&#36798;&#24182;&#20197;&#26102;&#38388;&#21644;&#36923;&#36753;&#39034;&#24207;&#23436;&#25104;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#12290;&#20026;&#20102;&#27491;&#24335;&#23450;&#20041;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#35859;&#35789;&#22312;LTL&#19978;&#23450;&#20041;&#20102;&#27169;&#22411;&#12290;&#36825;&#19982;&#30456;&#20851;&#30340;&#35268;&#21010;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21407;&#23376;&#35859;&#35789;&#19978;&#23450;&#20041;&#20102;&#25429;&#25417;&#25152;&#38656;&#20302;&#32423;&#31995;&#32479;&#37197;&#32622;&#30340;LTL&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#26426;&#22120;&#20154;&#35745;&#21010;&#65292;&#28385;&#36275;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#21629;&#39064;&#23450;&#20041;&#30340;LTL&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#22312;&#20110;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#19982;&#36825;&#20123;LTL&#32534;&#30721;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HERACLEs&#65292;&#19968;&#20010;&#20998;&#23618;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#29616;&#26377;&#24037;&#20855;&#30340;&#26032;&#22411;&#25972;&#21512;&#65292;&#21253;&#25324;&#65288;i&#65289;&#33258;&#21160;&#26426;&#29702;&#35770;&#65292;&#20197;&#30830;&#23450;&#26426;&#22120;&#20154;&#24212;&#35813;&#23436;&#25104;&#30340;NL&#25351;&#23450;&#30340;&#23376;&#20219;&#21153;&#20197;&#25512;&#36827;&#20219;&#21153;&#36827;&#23637;&#65307;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a new motion planning problem for mobile robots tasked with accomplishing multiple high-level sub-tasks, expressed using natural language (NL), in a temporal and logical order. To formally define such missions, we leverage LTL defined over NL-based atomic predicates modeling the considered NL-based sub-tasks. This is contrast to related planning approaches that define LTL tasks over atomic predicates capturing desired low-level system configurations. Our goal is to design robot plans that satisfy LTL tasks defined over NL-based atomic propositions. A novel technical challenge arising in this setup lies in reasoning about correctness of a robot plan with respect to such LTL-encoded tasks. To address this problem, we propose HERACLEs, a hierarchical conformal natural language planner, that relies on a novel integration of existing tools that include (i) automata theory to determine the NL-specified sub-task the robot should accomplish next to make mission progress; (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#27169;&#22411;UCoFiA&#65292;&#29992;&#20110;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#30456;&#20284;&#24615;&#32858;&#21512;&#27169;&#22359;&#26377;&#25928;&#32771;&#34385;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#26368;&#32456;&#35299;&#20915;&#20102;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#31934;&#30830;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10091</link><description>&lt;p&gt;
&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#30340;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Coarse-to-Fine Alignment for Video-Text Retrieval. (arXiv:2309.10091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10091
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#27169;&#22411;UCoFiA&#65292;&#29992;&#20110;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#30456;&#20284;&#24615;&#32858;&#21512;&#27169;&#22359;&#26377;&#25928;&#32771;&#34385;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#26368;&#32456;&#35299;&#20915;&#20102;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#31934;&#30830;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#36890;&#24120;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#20043;&#38388;&#30340;&#31895;&#31890;&#24230;&#25110;&#32454;&#31890;&#24230;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25991;&#26412;&#26597;&#35810;&#26816;&#32034;&#27491;&#30830;&#30340;&#35270;&#39057;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33021;&#22815;&#25512;&#29702;&#20986;&#39640;&#32423;&#65288;&#22330;&#26223;&#65289;&#21644;&#20302;&#32423;&#65288;&#23545;&#35937;&#65289;&#35270;&#35273;&#32447;&#32034;&#21450;&#20854;&#19982;&#25991;&#26412;&#26597;&#35810;&#30340;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UCoFiA&#30340;&#32479;&#19968;&#31895;&#21040;&#32454;&#23545;&#40784;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#25429;&#25417;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20449;&#24687;&#12290;&#20026;&#20943;&#36731;&#26080;&#20851;&#35270;&#35273;&#32447;&#32034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#20132;&#20114;&#24335;&#30456;&#20284;&#24615;&#32858;&#21512;&#27169;&#22359;&#65288;ISA&#65289;&#26469;&#32771;&#34385;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#32858;&#21512;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#20197;&#33719;&#24471;&#27599;&#20010;&#31890;&#24230;&#30340;&#30456;&#20284;&#24230;&#24471;&#20998;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;Sinkhorn-Knopp&#31639;&#27861;&#23545;&#27599;&#20010;&#32423;&#21035;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#20197;&#20943;&#36731;&#19981;&#21516;&#32423;&#21035;&#19978;&#30340;&#36807;&#24230;&#25110;&#19981;&#36275;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The canonical approach to video-text retrieval leverages a coarse-grained or fine-grained alignment between visual and textual information. However, retrieving the correct video according to the text query is often challenging as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they relate to the text query. To this end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA. Specifically, our model captures the cross-modal similarity information at different granularity levels. To alleviate the effect of irrelevant visual clues, we also apply an Interactive Similarity Aggregation module (ISA) to consider the importance of different visual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different l
&lt;/p&gt;</description></item><item><title>HTEC&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#22635;&#20805;&#20004;&#20010;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#20462;&#27491;&#25805;&#20316;&#21015;&#34920;&#65292;&#24182;&#38024;&#23545;&#21024;&#38500;&#38169;&#35823;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.10089</link><description>&lt;p&gt;
HTEC: &#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
HTEC: Human Transcription Error Correction. (arXiv:2309.10089v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10089
&lt;/p&gt;
&lt;p&gt;
HTEC&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#22635;&#20805;&#20004;&#20010;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#20462;&#27491;&#25805;&#20316;&#21015;&#34920;&#65292;&#24182;&#38024;&#23545;&#21024;&#38500;&#38169;&#35823;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#36716;&#24405;&#23545;&#20110;&#35757;&#32451;&#21644;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27599;&#22686;&#21152;1%&#30340;&#36716;&#24405;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#20351;&#29992;&#36825;&#20123;&#36716;&#24405;&#26469;&#35757;&#32451;ASR&#27169;&#22411;&#23558;&#22686;&#21152;&#32422;2%&#30340;ASR WER&#12290;&#21363;&#20351;&#26159;&#32463;&#36807;&#39640;&#24230;&#22521;&#35757;&#30340;&#27880;&#37322;&#21592;&#20063;&#38590;&#20813;&#20986;&#29616;&#36716;&#24405;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#30340;&#20462;&#27491;&#26041;&#27861;&#12290;&#20854;&#20182;&#38382;&#39064;&#30340;&#38169;&#35823;&#20462;&#27491;&#26041;&#27861;&#65292;&#22914;ASR&#38169;&#35823;&#20462;&#27491;&#21644;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#34920;&#29616;&#19981;&#22815;&#22909;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HTEC&#29992;&#20110;&#20154;&#31867;&#36716;&#24405;&#38169;&#35823;&#20462;&#27491;&#12290;HTEC&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;Trans-Checker&#65292;&#19968;&#31181;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#23631;&#34109;&#38169;&#35823;&#21333;&#35789;&#65292;&#21644;Trans-Filler&#65292;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22635;&#20805;&#23631;&#34109;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#20462;&#27491;&#25805;&#20316;&#21015;&#34920;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#22788;&#29702;&#21024;&#38500;&#38169;&#35823;&#30340;&#26032;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality human transcription is essential for training and improving Automatic Speech Recognition (ASR) models. Recent study~\cite{libricrowd} has found that every 1% worse transcription Word Error Rate (WER) increases approximately 2% ASR WER by using the transcriptions to train ASR models. Transcription errors are inevitable for even highly-trained annotators. However, few studies have explored human transcription correction. Error correction methods for other problems, such as ASR error correction and grammatical error correction, do not perform sufficiently for this problem. Therefore, we propose HTEC for Human Transcription Error Correction. HTEC consists of two stages: Trans-Checker, an error detection model that predicts and masks erroneous words, and Trans-Filler, a sequence-to-sequence generative model that fills masked positions. We propose a holistic list of correction operations, including four novel operations handling deletion errors. We further propose a variant of e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#24191;&#20041;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;GAME&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#24182;&#37319;&#29992;&#26032;&#39062;&#30340;&#20851;&#27880;&#26426;&#21046;&#65292;&#33021;&#22815;&#23545;&#38738;&#23569;&#24180;&#30340;&#24515;&#29702;&#29366;&#20917;&#36827;&#34892;&#39640;&#20934;&#30830;&#24230;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#27599;&#31181;&#27169;&#24577;&#37117;&#23545;&#24515;&#29702;&#38556;&#30861;&#30340;&#31579;&#26597;&#21644;&#20849;&#30149;&#29366;&#20917;&#26377;&#21160;&#24577;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.10077</link><description>&lt;p&gt;
GAME: &#19968;&#31181;&#29992;&#20110;&#26089;&#26399;&#31579;&#26597;&#38738;&#23569;&#24180;&#24515;&#29702;&#38556;&#30861;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#30340;&#24191;&#20041;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GAME: Generalized deep learning model towards multimodal data integration for early screening of adolescent mental disorders. (arXiv:2309.10077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#24191;&#20041;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;GAME&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#24182;&#37319;&#29992;&#26032;&#39062;&#30340;&#20851;&#27880;&#26426;&#21046;&#65292;&#33021;&#22815;&#23545;&#38738;&#23569;&#24180;&#30340;&#24515;&#29702;&#29366;&#20917;&#36827;&#34892;&#39640;&#20934;&#30830;&#24230;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#27599;&#31181;&#27169;&#24577;&#37117;&#23545;&#24515;&#29702;&#38556;&#30861;&#30340;&#31579;&#26597;&#21644;&#20849;&#30149;&#29366;&#20917;&#26377;&#21160;&#24577;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#21457;&#29616;&#38738;&#23569;&#24180;&#24515;&#29702;&#38556;&#30861;&#26159;&#19968;&#20010;&#20840;&#29699;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#30001;&#20110;&#20854;&#22797;&#26434;&#32780;&#24494;&#22937;&#30340;&#24615;&#36136;&#65292;&#21333;&#19968;&#22240;&#32032;&#24456;&#38590;&#26816;&#27979;&#21040;&#24322;&#24120;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#24191;&#20041;&#30340;&#22810;&#27169;&#24577;&#22270;&#20687;&#19982;&#26426;&#22120;&#20154;&#30456;&#20114;&#20316;&#29992;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#31579;&#26597;&#31995;&#32479;&#29992;&#20110;&#26816;&#27979;&#38738;&#23569;&#24180;&#24515;&#29702;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#20415;&#25658;&#26426;&#22120;&#20154;&#19978;&#37096;&#32626;&#30340;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#65292;&#37319;&#29992;&#36855;&#20320;&#28216;&#25103;&#21644;&#32842;&#22825;&#35760;&#24405;&#65292;&#23545;3,783&#21517;&#20013;&#23398;&#29983;&#36827;&#34892;&#31579;&#26597;&#65292;&#24182;&#26500;&#24314;&#20102;&#21253;&#25324;&#38754;&#37096;&#22270;&#20687;&#12289;&#29983;&#29702;&#25351;&#26631;&#12289;&#35821;&#38899;&#35760;&#24405;&#21644;&#25991;&#26412;&#36716;&#24405;&#30340;&#22810;&#27169;&#24577;&#31579;&#26597;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;GAME&#65288;&#20855;&#26377;&#20851;&#27880;&#26426;&#21046;&#21644;&#22810;&#27169;&#24577;EmbraceNet&#30340;&#24191;&#20041;&#27169;&#22411;&#65289;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#36328;&#27169;&#24577;&#29305;&#24449;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#65292;&#23545;&#38738;&#23569;&#24180;&#30340;&#24515;&#29702;&#29366;&#20917;&#36827;&#34892;&#39640;&#20934;&#30830;&#24230;&#65288;73.34%-92.77%&#65289;&#21644;F1-Score&#65288;71.32%-91.06%&#65289;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#27599;&#31181;&#27169;&#24577;&#21160;&#24577;&#22320;&#23545;&#24515;&#29702;&#38556;&#30861;&#31579;&#26597;&#21644;&#20849;&#30149;&#29366;&#20917;&#20316;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The timely identification of mental disorders in adolescents is a global public health challenge.Single factor is difficult to detect the abnormality due to its complex and subtle nature. Additionally, the generalized multimodal Computer-Aided Screening (CAS) systems with interactive robots for adolescent mental disorders are not available. Here, we design an android application with mini-games and chat recording deployed in a portable robot to screen 3,783 middle school students and construct the multimodal screening dataset, including facial images, physiological signs, voice recordings, and textual transcripts.We develop a model called GAME (Generalized Model with Attention and Multimodal EmbraceNet) with novel attention mechanism that integrates cross-modal features into the model. GAME evaluates adolescent mental conditions with high accuracy (73.34%-92.77%) and F1-Score (71.32%-91.06%).We find each modality contributes dynamically to the mental disorders screening and comorbiditi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#20013;&#24615;&#21035;&#22312;&#22823;&#33041;&#34928;&#32769;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#20998;&#26512;373&#21517;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#22823;&#33041;&#39044;&#27979;&#24180;&#40836;&#24046;&#24322;&#65292;&#24182;&#25353;&#24615;&#21035;&#36827;&#34892;&#20998;&#23618;&#20998;&#26512;&#65292;&#21457;&#29616;&#24615;&#21035;&#24046;&#24322;&#20250;&#23548;&#33268;&#24085;&#37329;&#26862;&#30149;&#30340;&#30151;&#29366;&#21644;&#36827;&#23637;&#36895;&#24230;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.10069</link><description>&lt;p&gt;
&#22522;&#20110;&#24615;&#21035;&#30340;&#24046;&#24322;&#22312;&#22823;&#33041;&#34928;&#32769;&#20013;&#30340;&#20316;&#29992;&#65306;&#20197;&#24085;&#37329;&#26862;&#30149;&#20026;&#37325;&#28857;
&lt;/p&gt;
&lt;p&gt;
Sex-based Disparities in Brain Aging: A Focus on Parkinson's Disease. (arXiv:2309.10069v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#20013;&#24615;&#21035;&#22312;&#22823;&#33041;&#34928;&#32769;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#20998;&#26512;373&#21517;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#22823;&#33041;&#39044;&#27979;&#24180;&#40836;&#24046;&#24322;&#65292;&#24182;&#25353;&#24615;&#21035;&#36827;&#34892;&#20998;&#23618;&#20998;&#26512;&#65292;&#21457;&#29616;&#24615;&#21035;&#24046;&#24322;&#20250;&#23548;&#33268;&#24085;&#37329;&#26862;&#30149;&#30340;&#30151;&#29366;&#21644;&#36827;&#23637;&#36895;&#24230;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#19982;&#22823;&#33041;&#34928;&#32769;&#36895;&#24230;&#21152;&#24555;&#26377;&#20851;&#12290;&#24615;&#21035;&#34987;&#35748;&#20026;&#26159;&#24085;&#37329;&#26862;&#30149;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#30007;&#24615;&#24739;&#30149;&#30340;&#21487;&#33021;&#24615;&#26159;&#22899;&#24615;&#30340;&#20004;&#20493;&#65292;&#19988;&#30151;&#29366;&#26356;&#20005;&#37325;&#65292;&#36827;&#23637;&#26356;&#24555;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#23545;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#20013;&#24615;&#21035;&#22312;&#22823;&#33041;&#34928;&#32769;&#36807;&#31243;&#20013;&#30340;&#21151;&#33021;&#29702;&#35299;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;T1&#21152;&#26435;MRI&#30340;&#22823;&#33041;&#39044;&#27979;&#24180;&#40836;&#24046;&#24322;&#65292;&#22312;PPMI&#25968;&#25454;&#24211;&#30340;373&#21517;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#20013;&#20351;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;&#40065;&#26834;&#22823;&#33041;&#24180;&#40836;&#20272;&#35745;&#26694;&#26550;&#36827;&#34892;&#35745;&#31639;&#65292;&#35813;&#26694;&#26550;&#22312;949&#21517;&#20581;&#24247;&#21463;&#35797;&#32773;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30740;&#31350;&#20102;&#24085;&#37329;&#26862;&#30149;&#20013;&#22823;&#33041;&#39044;&#27979;&#24180;&#40836;&#24046;&#24322;&#19982;&#20020;&#24202;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25353;&#24615;&#21035;&#20998;&#23618;&#12290;&#25152;&#26377;&#22899;&#24615;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#37117;&#21442;&#19982;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#32780;&#30456;&#21516;&#25968;&#37327;&#30340;&#30007;&#24615;&#26681;&#25454;&#24180;&#40836;&#12289;&#25945;&#32946;&#27700;&#24179;&#12289;&#30151;&#29366;&#21457;&#30149;&#24180;&#40836;&#21644;&#20020;&#24202;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#20351;&#29992;&#20542;&#21521;&#35780;&#20998;&#21305;&#37197;&#26041;&#27861;&#36827;&#34892;&#20102;&#31579;&#36873;&#12290;&#23613;&#31649;&#20004;&#32452;&#24739;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21442;&#25968;&#30456;&#21305;&#37197;&#65292;&#21160;&#20316;&#30340;&#28436;&#21464;&#21364;&#22240;&#24615;&#21035;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
PD is linked to faster brain aging. Sex is recognized as an important factor in PD, such that males are twice as likely as females to have the disease and have more severe symptoms and a faster progression rate. Despite previous research, there remains a significant gap in understanding the function of sex in the process of brain aging in PD patients. The T1-weighted MRI-driven brain-predicted age difference was computed in a group of 373 PD patients from the PPMI database using a robust brain-age estimation framework that was trained on 949 healthy subjects. Linear regression models were used to investigate the association between brain-PAD and clinical variables in PD, stratified by sex. All female PD patients were used in the correlational analysis while the same number of males were selected based on propensity score matching method considering age, education level, age of symptom onset, and clinical symptom severity. Despite both patient groups being matched for demographics, moto
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#20010;&#24615;&#21270;&#29983;&#25104;&#20840;&#36523;PET&#25253;&#21578;&#30340;&#20934;&#30830;&#21360;&#35937;&#12290;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#20449;&#24687;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#30740;&#31350;&#32467;&#26524;&#32463;&#36807;&#19987;&#23478;&#35780;&#20272;&#21644;&#26680;&#21307;&#23398;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#35748;&#21487;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10066</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20010;&#24615;&#21270;&#21360;&#35937;&#29983;&#25104;PET&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#20010;&#24615;&#21270;&#29983;&#25104;&#20840;&#36523;PET&#25253;&#21578;&#30340;&#20934;&#30830;&#21360;&#35937;&#12290;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#20449;&#24687;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#30740;&#31350;&#32467;&#26524;&#32463;&#36807;&#19987;&#23478;&#35780;&#20272;&#21644;&#26680;&#21307;&#23398;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#35748;&#21487;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#30830;&#23450;&#36890;&#36807;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#21487;&#20197;&#20026;&#20840;&#36523;PET&#25253;&#21578;&#29983;&#25104;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#21360;&#35937;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#20351;&#29992;teacher-forcing&#31639;&#27861;&#22312;PET&#25253;&#21578;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;12&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#36755;&#20837;&#26159;&#25253;&#21578;&#21457;&#29616;&#65292;&#21442;&#32771;&#26159;&#20020;&#24202;&#21360;&#35937;&#12290;&#39069;&#22806;&#30340;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20102;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;2010&#24180;&#33267;2022&#24180;&#38388;&#20174;&#25105;&#20204;&#26426;&#26500;&#25910;&#38598;&#30340;37,370&#20221;&#22238;&#39038;&#24615;PET&#25253;&#21578;&#12290;&#36890;&#36807;&#19982;&#20004;&#21517;&#26680;&#21307;&#23398;&#65288;NM&#65289;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#36827;&#34892;30&#20010;&#35780;&#20272;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26368;&#21305;&#37197;&#30340;&#25351;&#26631;&#36873;&#25321;&#20102;&#29992;&#20110;&#19987;&#23478;&#35780;&#20272;&#30340;&#27169;&#22411;&#12290;&#22312;&#37096;&#20998;&#25968;&#25454;&#23376;&#38598;&#20013;&#65292;&#26681;&#25454;6&#20010;&#36136;&#37327;&#32500;&#24230;&#21644;&#19968;&#20010;&#24635;&#20307;&#23454;&#29992;&#24615;&#35780;&#20998;&#65288;5&#20998;&#21046;&#65289;&#65292;&#19977;&#21517;&#26680;&#21307;&#23398;&#21307;&#29983;&#35780;&#20272;&#20102;&#27169;&#22411;&#29983;&#25104;&#30340;&#21360;&#35937;&#21644;&#21407;&#22987;&#20020;&#24202;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Materials and Methods: Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions and an overall utility score (5-point scale). Each physician reviewe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#24182;&#23433;&#20840;&#39640;&#25928;&#22320;&#23558;&#26080;&#20154;&#26426;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#19982;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#36827;&#34892;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.10064</link><description>&lt;p&gt;
&#20026;&#33258;&#20027;&#21644;&#25805;&#32437;&#21592;&#25511;&#21046;&#30340;&#26080;&#20154;&#26426;&#23454;&#29616;&#26080;&#30896;&#25758;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Toward collision-free trajectory for autonomous and pilot-controlled unmanned aerial vehicles. (arXiv:2309.10064v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#24182;&#23433;&#20840;&#39640;&#25928;&#22320;&#23558;&#26080;&#20154;&#26426;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#19982;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#36827;&#34892;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#26080;&#20154;&#26426;&#65292;&#38656;&#35201;&#22312;&#26426;&#36733;&#19978;&#23433;&#35013;&#36991;&#38556;&#25216;&#26415;&#65292;&#20197;&#20415;&#23519;&#35273;&#21363;&#23558;&#21457;&#29983;&#30340;&#38750;&#21512;&#20316;&#24615;&#23041;&#32961;&#25110;&#20914;&#31361;&#20132;&#36890;&#12290;&#26681;&#25454;&#33258;&#20027;&#31561;&#32423;&#65292;&#37319;&#21462;&#21512;&#36866;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#30896;&#25758;&#12290;&#23558;&#26080;&#20154;&#26426;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#21644;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#23433;&#20840;&#39640;&#25928;&#22320;&#25972;&#21512;&#36215;&#26469;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#26080;&#20154;&#26426;&#24212;&#29992;&#30340;&#25968;&#37327;&#22312;&#25317;&#25380;&#31354;&#20013;&#20132;&#36890;&#29615;&#22659;&#20013;&#36805;&#36895;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
For drones, as safety-critical systems, there is an increasing need for onboard detect &amp; avoid (DAA) technology i) to see, sense or detect conflicting traffic or imminent non-cooperative threats due to their high mobility with multiple degrees of freedom and the complexity of deployed unstructured environments, and subsequently ii) to take the appropriate actions to avoid collisions depending upon the level of autonomy. The safe and efficient integration of UAV traffic management (UTM) systems with air traffic management (ATM) systems, using intelligent autonomous approaches, is an emerging requirement where the number of diverse UAV applications is increasing on a large scale in dense air traffic environments for completing swarms of multiple complex missions flexibly and simultaneously. Significant progress over the past few years has been made in detecting UAVs present in aerospace, identifying them, and determining their existing flight path. This study makes greater use of electro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#35745;&#31639;&#35282;&#24230;&#20986;&#21457;&#65292;&#35299;&#37322;&#20154;&#31867;&#22823;&#33041;&#20013;&#24847;&#35782;&#29616;&#35937;&#30340;&#22810;&#31181;&#29702;&#35770;&#12290;&#35299;&#24320;&#24847;&#35782;&#20043;&#35868;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2309.10063</link><description>&lt;p&gt;
&#20174;&#35745;&#31639;&#35282;&#24230;&#30340;&#24847;&#35782;&#29702;&#35770;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Consciousness Theory from Computational Perspective. (arXiv:2309.10063v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#35745;&#31639;&#35282;&#24230;&#20986;&#21457;&#65292;&#35299;&#37322;&#20154;&#31867;&#22823;&#33041;&#20013;&#24847;&#35782;&#29616;&#35937;&#30340;&#22810;&#31181;&#29702;&#35770;&#12290;&#35299;&#24320;&#24847;&#35782;&#20043;&#35868;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24847;&#35782;&#24050;&#32463;&#26159;&#20960;&#20010;&#19990;&#32426;&#20197;&#26469;&#30340;&#38271;&#26399;&#35868;&#22242;&#65292;&#32780;&#26426;&#22120;&#26234;&#33021;&#21644;&#24847;&#35782;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#36861;&#27714;&#12290;&#30740;&#31350;&#20154;&#21592;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#21644;&#23618;&#27425;&#19978;&#21457;&#23637;&#20102;&#21508;&#31181;&#29702;&#35770;&#26469;&#35299;&#37322;&#20154;&#31867;&#22823;&#33041;&#20013;&#30340;&#24847;&#35782;&#29616;&#35937;&#12290;&#26412;&#25991;&#20174;&#35745;&#31639;&#35282;&#24230;&#27010;&#36848;&#20102;&#20960;&#20010;&#20027;&#35201;&#30340;&#24847;&#35782;&#29702;&#35770;&#20998;&#25903;&#65292;&#21253;&#25324;&#20449;&#24687;&#35770;&#12289;&#37327;&#23376;&#29289;&#29702;&#23398;&#12289;&#35748;&#30693;&#24515;&#29702;&#23398;&#12289;&#29983;&#29702;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#65292;&#24182;&#26088;&#22312;&#20174;&#35745;&#31639;&#35282;&#24230;&#26469;&#36830;&#25509;&#36825;&#20123;&#29702;&#35770;&#12290;&#23427;&#36824;&#35752;&#35770;&#20102;&#24847;&#35782;&#30340;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#24403;&#21069;&#35745;&#31639;&#27169;&#22411;&#33021;&#21542;&#20855;&#26377;&#24847;&#35782;&#12290;&#35299;&#24320;&#24847;&#35782;&#20043;&#35868;&#21487;&#20197;&#26159;&#26500;&#24314;&#20855;&#26377;&#35745;&#31639;&#26426;&#30340;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human consciousness has been a long-lasting mystery for centuries, while machine intelligence and consciousness is an arduous pursuit. Researchers have developed diverse theories for interpreting the consciousness phenomenon in human brains from different perspectives and levels. This paper surveys several main branches of consciousness theories originating from different subjects including information theory, quantum physics, cognitive psychology, physiology and computer science, with the aim of bridging these theories from a computational perspective. It also discusses the existing evaluation metrics of consciousness and possibility for current computational models to be conscious. Breaking the mystery of consciousness can be an essential step in building general artificial intelligence with computing machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.10016</link><description>&lt;p&gt;
GPT-3&#29992;&#20110;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction. (arXiv:2309.10016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#33647;&#29289;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#22312;&#20116;&#31181;&#32452;&#32455;&#31867;&#22411;&#20013;&#25506;&#31350;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20998;&#21035;&#37319;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#24494;&#35843;&#33539;&#24335;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#26377;&#26395;&#20026;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the potential of GPT-3 for the anti-cancer drug sensitivity prediction task using structured pharmacogenomics data across five tissue types and evaluated its performance with zero-shot prompting and fine-tuning paradigms. The drug's smile representation and cell line's genomic mutation features were predictive of the drug response. The results from this study have the potential to pave the way for designing more efficient treatment protocols in precision oncology.
&lt;/p&gt;</description></item><item><title>SYNDICOM&#26159;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#24120;&#35782;&#30340;&#26041;&#27861;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#24120;&#35782;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10015</link><description>&lt;p&gt;
SYNDICOM: &#38169;&#35823;&#27880;&#20837;&#21644;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25913;&#36827;&#23545;&#35805;&#24120;&#35782;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback. (arXiv:2309.10015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10015
&lt;/p&gt;
&lt;p&gt;
SYNDICOM&#26159;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#24120;&#35782;&#30340;&#26041;&#27861;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#24120;&#35782;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26159;&#20154;&#31867;&#20132;&#27969;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24120;&#35782;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SYNDICOM - &#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#20013;&#24120;&#35782;&#30340;&#26041;&#27861;&#12290;SYNDICOM&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#26159;&#19968;&#20010;&#30001;&#30693;&#35782;&#22270;&#21019;&#24314;&#30340;&#24120;&#35782;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#21512;&#25104;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#21644;&#26080;&#25928;&#22238;&#31572;&#65292;&#20197;&#21450;&#23545;&#26080;&#25928;&#22238;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65288;NLF&#65289;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#20004;&#27493;&#30340;&#36807;&#31243;&#65306;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#26080;&#25928;&#22238;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65288;NLF&#65289;&#65292;&#28982;&#21518;&#26681;&#25454;&#39044;&#27979;&#30340;NLF&#12289;&#26080;&#25928;&#22238;&#31572;&#21644;&#23545;&#35805;&#26465;&#20214;&#35757;&#32451;&#19968;&#20010;&#24212;&#31572;&#29983;&#25104;&#27169;&#22411;&#12290;SYNDICOM&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#65292;&#19981;&#38656;&#35201;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#20219;&#21153;&#30340;&#32463;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is a critical aspect of human communication. Despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce SYNDICOM - a method for improving commonsense in dialogue response generation. SYNDICOM consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. SYNDICOM is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#19981;&#26029;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#20197;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#33976;&#39311;&#28508;&#22312;&#31354;&#38388;&#12289;&#25913;&#21892;&#29983;&#25104;&#29305;&#24449;&#23545;&#40784;&#21644;&#21608;&#26399;&#24615;&#29983;&#25104;&#20197;&#22686;&#24378;&#30693;&#35782;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2309.10012</link><description>&lt;p&gt;
&#31397;&#25506;&#36807;&#21435;&#65306;&#25913;&#36827;&#19981;&#26029;&#23398;&#20064;&#20013;&#29983;&#25104;&#22238;&#25918;&#30340;&#30693;&#35782;&#20445;&#30041;
&lt;/p&gt;
&lt;p&gt;
Looking through the past: better knowledge retention for generative replay in continual learning. (arXiv:2309.10012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#19981;&#26029;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#20197;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#33976;&#39311;&#28508;&#22312;&#31354;&#38388;&#12289;&#25913;&#21892;&#29983;&#25104;&#29305;&#24449;&#23545;&#40784;&#21644;&#21608;&#26399;&#24615;&#29983;&#25104;&#20197;&#22686;&#24378;&#30693;&#35782;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#19981;&#26029;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#29983;&#25104;&#22238;&#25918;&#65292;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#24403;&#21069;&#30340;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#36890;&#24120;&#22312;&#23567;&#22411;&#21644;&#31616;&#21333;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#36275;&#20197;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#21644;&#26356;&#22810;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#22312;&#22522;&#20110;VAE&#30340;&#29983;&#25104;&#22238;&#25918;&#20013;&#65292;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;&#24403;&#29983;&#25104;&#30340;&#29305;&#24449;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26102;&#19982;&#21407;&#22987;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24403;&#21069;&#27169;&#22411;&#19982;&#20808;&#21069;&#27169;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#33976;&#39311;&#65292;&#20197;&#20943;&#23569;&#29305;&#24449;&#28418;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#29983;&#25104;&#29305;&#24449;&#23545;&#40784;&#30340;&#37325;&#24314;&#21644;&#21407;&#22987;&#25968;&#25454;&#30340;&#28508;&#22312;&#21305;&#37197;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#22522;&#20110;&#23545;&#20445;&#23384;&#30693;&#35782;&#30340;&#37325;&#24314;&#25928;&#26524;&#26356;&#22909;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#36890;&#36807;&#20808;&#21069;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#21608;&#26399;&#24615;&#29983;&#25104;&#30340;&#26041;&#24335;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#30693;&#35782;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we improve the generative replay in a continual learning setting to perform well on challenging scenarios. Current generative rehearsal methods are usually benchmarked on small and simple datasets as they are not powerful enough to generate more complex data with a greater number of classes. We notice that in VAE-based generative replay, this could be attributed to the fact that the generated features are far from the original ones when mapped to the latent space. Therefore, we propose three modifications that allow the model to learn and generate complex data. More specifically, we incorporate the distillation in latent space between the current and previous models to reduce feature drift. Additionally, a latent matching for the reconstruction and original data is proposed to improve generated features alignment. Further, based on the observation that the reconstructions are better for preserving knowledge, we add the cycling of generations through the previously trained
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#22521;&#20859;&#21512;&#20316;&#19982;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#29983;&#24577;&#31995;&#32479;&#24320;&#21457;&#20102;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.10007</link><description>&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#38388;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#19982;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem. (arXiv:2309.10007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20013;&#22521;&#20859;&#21512;&#20316;&#19982;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#29983;&#24577;&#31995;&#32479;&#24320;&#21457;&#20102;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#24182;&#34892;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#22521;&#20859;&#21512;&#20316;&#21644;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#65292;&#24320;&#21457;&#20986;&#19982;&#30495;&#23454;&#30340;Nigel&#21644;F1TENTH&#20004;&#31181;&#27604;&#20363;&#33258;&#20027;&#36710;&#36742;&#24179;&#21488;&#20855;&#26377;&#29420;&#29305;&#29305;&#24615;&#21644;&#33021;&#21147;&#30340;&#20934;&#30830;&#29289;&#29702;&#21644;&#36924;&#30495;&#22270;&#24418;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#29983;&#24577;&#31995;&#32479;&#26469;&#35757;&#32451;&#21644;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#20132;&#21449;&#36335;&#21475;&#31359;&#36234;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#32452;&#21512;&#20316;&#36710;&#36742;&#65288;Nigel&#65289;&#22312;&#21333;&#20010;&#25110;&#22810;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#65292;&#37319;&#29992;&#19968;&#31181;&#20844;&#20849;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#30340;&#22836;&#23545;&#22836;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#65292;&#20351;&#29992;&#21478;&#19968;&#32452;&#36710;&#36742;&#65288;F1TENTH&#65289;&#22312;&#22810;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#37319;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#37117;&#37319;&#29992;&#20102;&#20998;&#25955;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20056;&#23458;&#38656;&#27714;&#23547;&#25214;&#38889;&#22269;&#20161;&#24029;&#24066;&#20844;&#20132;&#31995;&#32479;&#30340;&#26368;&#20248;&#36335;&#24452;&#65292;&#36890;&#36807;&#20462;&#25913;A*&#31639;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24120;&#29992;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.10006</link><description>&lt;p&gt;
&#38889;&#22269;&#20161;&#24029;&#24066;&#20844;&#20849;&#20132;&#36890;&#30340;&#20248;&#21270;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
The Optimized path for the public transportation of Incheon in South Korea. (arXiv:2309.10006v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20056;&#23458;&#38656;&#27714;&#23547;&#25214;&#38889;&#22269;&#20161;&#24029;&#24066;&#20844;&#20132;&#31995;&#32479;&#30340;&#26368;&#20248;&#36335;&#24452;&#65292;&#36890;&#36807;&#20462;&#25913;A*&#31639;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#24120;&#29992;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#35268;&#21010;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#30740;&#31350;&#35838;&#39064;&#20043;&#19968;&#12290;&#36335;&#24452;&#35268;&#21010;&#31574;&#30053;&#30830;&#23450;&#20102;&#20174;&#32473;&#23450;&#22352;&#26631;&#21040;&#21478;&#19968;&#28857;&#30340;&#36335;&#24452;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;&#20056;&#23458;&#38656;&#27714;&#23547;&#25214;&#20844;&#20132;&#31995;&#32479;&#30340;&#26368;&#20248;&#36335;&#24452;&#12290;&#35813;&#30740;&#31350;&#26159;&#22522;&#20110;&#38889;&#22269;&#20161;&#24029;&#24066;&#30340;&#20844;&#20132;&#31449;&#28857;&#36827;&#34892;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#20462;&#25913;&#21518;&#30340;A*&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#26412;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#22914;&#36951;&#20256;&#31639;&#27861;&#21644;Dijkstra&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#26102;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#37327;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path-finding is one of the most popular subjects in the field of computer science. Pathfinding strategies determine a path from a given coordinate to another. The focus of this paper is on finding the optimal path for the bus transportation system based on passenger demand. This study is based on bus stations in Incheon, South Korea, and we show that our modified A* algorithm performs better than other basic pathfinding algorithms such as the Genetic and Dijkstra. Our proposed approach can find the shortest path in real-time even for large amounts of data(points).
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#22788;&#29702;&#31246;&#21153;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.09992</link><description>&lt;p&gt;
OpenAI&#25220;&#34989;&#20102;&#25105;&#20204;&#30340;&#31246;&#21153;&#26696;&#20363;&#65292;&#20294;GPT-4&#30495;&#30340;&#33021;&#22815;&#22788;&#29702;&#31246;&#21153;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?. (arXiv:2309.09992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09992
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#22788;&#29702;&#31246;&#21153;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#35299;&#37322;&#20102;OpenAI&#22312;GPT-4&#30340;&#30452;&#25773;&#28436;&#31034;&#20013;&#20351;&#29992;&#31246;&#27861;&#26696;&#20363;&#30340;&#26469;&#28304;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;GPT-4&#24471;&#21040;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The authors explain where OpenAI got the tax law example in its livestream demonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to reliably calculate taxes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20998;&#23376;&#35299;&#31163;&#35270;&#20026;&#23545;&#20854;&#32452;&#25104;&#21407;&#23376;&#26045;&#21152;&#36880;&#28176;&#22686;&#22823;&#30340;&#21147;&#22330;&#65292;&#20174;&#32780;&#25913;&#21464;&#21407;&#23376;&#38388;&#36317;&#31163;&#30340;&#20998;&#24067;&#65292;&#35813;&#26041;&#27861;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.09985</link><description>&lt;p&gt;
&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#30340;&#20301;&#31227;&#24471;&#20998;&#27861;
&lt;/p&gt;
&lt;p&gt;
Molecular Conformation Generation via Shifting Scores. (arXiv:2309.09985v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20998;&#23376;&#35299;&#31163;&#35270;&#20026;&#23545;&#20854;&#32452;&#25104;&#21407;&#23376;&#26045;&#21152;&#36880;&#28176;&#22686;&#22823;&#30340;&#21147;&#22330;&#65292;&#20174;&#32780;&#25913;&#21464;&#21407;&#23376;&#38388;&#36317;&#31163;&#30340;&#20998;&#24067;&#65292;&#35813;&#26041;&#27861;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26159;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#28041;&#21450;&#20026;&#32473;&#23450;&#30340;&#20998;&#23376;&#29983;&#25104;&#19977;&#32500;&#26500;&#35937;&#20960;&#20309;&#12290;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#20998;&#23376;&#26500;&#35937;&#38656;&#35201;&#23398;&#20064;&#36870;&#21521;&#22122;&#22768;&#36807;&#31243;&#12290;&#20351;&#29992;&#21407;&#23376;&#38388;&#36317;&#31163;&#25193;&#25955;&#32780;&#19981;&#26159;&#26500;&#35937;&#26469;&#20445;&#25345;SE(3)-&#31561;&#20215;&#24615;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#25216;&#26415;&#30456;&#27604;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#30456;&#20851;&#30340;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#21551;&#21457;&#24335;&#20551;&#35774;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#20854;&#21160;&#26426;&#26159;&#35748;&#20026;&#20998;&#23376;&#30340;&#35299;&#31163;&#21487;&#20197;&#34987;&#35270;&#20026;&#23545;&#20854;&#32452;&#25104;&#21407;&#23376;&#26045;&#21152;&#36880;&#28176;&#22686;&#22823;&#30340;&#21147;&#22330;&#65292;&#20174;&#32780;&#20351;&#24471;&#21407;&#23376;&#38388;&#36317;&#31163;&#30340;&#21464;&#21270;&#20998;&#24067;&#20174;&#39640;&#26031;&#20998;&#24067;&#36716;&#21464;&#20026;&#40614;&#20811;&#26031;&#38886;-&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#12290;&#30456;&#24212;&#30340;&#29983;&#25104;&#27169;&#22411;&#30830;&#20445;&#20102;&#21487;&#34892;&#30340;&#21407;&#23376;&#38388;&#36317;&#31163;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#21576;&#29616;&#26102;&#38388;&#21487;&#36870;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular conformation generation, a critical aspect of computational chemistry, involves producing the three-dimensional conformer geometry for a given molecule. Generating molecular conformation via diffusion requires learning to reverse a noising process. Diffusion on inter-atomic distances instead of conformation preserves SE(3)-equivalence and shows superior performance compared to alternative techniques, whereas related generative modelings are predominantly based upon heuristical assumptions. In response to this, we propose a novel molecular conformation generation approach driven by the observation that the disintegration of a molecule can be viewed as casting increasing force fields to its composing atoms, such that the distribution of the change of inter-atomic distance shifts from Gaussian to Maxwell-Boltzmann distribution. The corresponding generative modeling ensures a feasible inter-atomic distance geometry and exhibits time reversibility. Experimental results on molecula
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#21644;&#27604;&#36739;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#33041;&#23545;&#30495;&#23454;&#22270;&#29255;&#30340;&#21453;&#24212;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#20351;&#29992;&#22810;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#19987;&#27880;&#20110;&#21463;&#35797;&#32773;&#22823;&#33041;&#27599;&#20010;&#21322;&#29699;&#30340;&#27599;&#20010;ROI&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.09983</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#27604;&#36739;&#29992;&#20110;&#39044;&#27979;&#22823;&#33041;&#23545;&#30495;&#23454;&#22270;&#29255;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploration and Comparison of Deep Learning Architectures to Predict Brain Response to Realistic Pictures. (arXiv:2309.09983v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#21644;&#27604;&#36739;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#33041;&#23545;&#30495;&#23454;&#22270;&#29255;&#30340;&#21453;&#24212;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#20351;&#29992;&#22810;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#19987;&#27880;&#20110;&#21463;&#35797;&#32773;&#22823;&#33041;&#27599;&#20010;&#21322;&#29699;&#30340;&#27599;&#20010;ROI&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#20851;&#20110;&#39044;&#27979;&#22823;&#33041;&#23545;&#30495;&#23454;&#22270;&#20687;&#21453;&#24212;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#30740;&#31350;&#65292;&#20197;&#24212;&#23545;Algonauts Challenge 2023&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#23545;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36739;&#31616;&#21333;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#22823;&#33041;&#27963;&#21160;&#65292;&#20294;&#36880;&#28176;&#24341;&#20837;&#20102;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#36935;&#21040;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30456;&#20851;&#30340;&#20856;&#22411;&#22256;&#38590;&#65292;&#27604;&#22914;&#27491;&#21017;&#21270;&#21644;&#36807;&#25311;&#21512;&#65292;&#20197;&#21450;&#19982;&#25361;&#25112;&#29305;&#23450;&#30340;&#38382;&#39064;&#65292;&#22914;&#38590;&#20197;&#32467;&#21512;&#22810;&#20010;&#36755;&#20837;&#32534;&#30721;&#65292;&#20197;&#21450;&#36755;&#20986;&#30340;&#39640;&#32500;&#24230;&#12289;&#19981;&#26126;&#30830;&#30340;&#32467;&#26500;&#21644;&#22024;&#26434;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#22522;&#20110;&#21333;&#36793;&#19977;&#32500;&#20301;&#32622;&#12289;&#22810;&#24863;&#20852;&#36259;&#21306;&#22495;(ROI)&#21644;&#21322;&#29699;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22810;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#19987;&#27880;&#20110;&#21463;&#35797;&#32773;&#22823;&#33041;&#27599;&#20010;&#21322;&#29699;&#30340;&#27599;&#20010;ROI&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an exploration of machine learning architectures for predicting brain responses to realistic images on occasion of the Algonauts Challenge 2023. Our research involved extensive experimentation with various pretrained models. Initially, we employed simpler models to predict brain activity but gradually introduced more complex architectures utilizing available data and embeddings generated by large-scale pre-trained models. We encountered typical difficulties related to machine learning problems, e.g. regularization and overfitting, as well as issues specific to the challenge, such as difficulty in combining multiple input encodings, as well as the high dimensionality, unclear structure, and noisy nature of the output. To overcome these issues we tested single edge 3D position-based, multi-region of interest (ROI) and hemisphere predictor models, but we found that employing multiple simple models, each dedicated to a ROI in each hemisphere of the brain of each subject, yielded
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064; (IDML) &#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#27169;&#31946;&#22270;&#20687;&#65292;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.09982</link><description>&lt;p&gt;
&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Introspective Deep Metric Learning. (arXiv:2309.09982v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064; (IDML) &#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#27169;&#31946;&#22270;&#20687;&#65292;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064; (IDML) &#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27604;&#36739;&#12290;&#20256;&#32479;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30528;&#37325;&#20110;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#21306;&#20998;&#24230;&#30340;&#23884;&#20837;&#26469;&#25551;&#36848;&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#30001;&#20110;&#22122;&#22768;&#25110;&#35821;&#20041;&#27169;&#31946;&#24615;&#32780;&#23548;&#33268;&#30340;&#27599;&#20010;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#23384;&#22312;&#12290;&#22312;&#27809;&#26377;&#24847;&#35782;&#21040;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#36807;&#24230;&#25311;&#21512;&#27880;&#37322;&#26631;&#31614;&#65292;&#24182;&#22312;&#25512;&#29702;&#26399;&#38388;&#20135;&#29983;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#21028;&#26029;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#22909;&#30340;&#30456;&#20284;&#24230;&#27169;&#22411;&#24212;&#32771;&#34385;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#27169;&#31946;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#20351;&#29992;&#35821;&#20041;&#23884;&#20837;&#65292;&#36824;&#20351;&#29992;&#20276;&#38543;&#30340;&#19981;&#30830;&#23450;&#24615;&#23884;&#20837;&#65292;&#20998;&#21035;&#25551;&#36848;&#22270;&#20687;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#30465;&#24335;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#20869;&#30465;&#24335;&#27604;&#36739;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. Conventional deep metric learning methods focus on learning a discriminative embedding to describe the semantic features of images, which ignore the existence of uncertainty in each image resulting from noise or semantic ambiguity. Training without awareness of these uncertainties causes the model to overfit the annotated labels during training and produce unsatisfactory judgments during inference. Motivated by this, we argue that a good similarity model should consider the semantic discrepancies with awareness of the uncertainty to better deal with ambiguous images for more robust training. To achieve this, we propose to represent an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively. We further propose an introspective similarity metric to make
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzPretrain&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20195;&#30721;&#34920;&#31034;&#39044;&#35757;&#32451;&#20013;&#25506;&#32034;&#30001;&#31243;&#24207;&#30340;&#27979;&#35797;&#29992;&#20363;&#25581;&#31034;&#30340;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20174;&#20195;&#30721;&#20013;&#30452;&#25509;&#23398;&#20064;&#21151;&#33021;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.09980</link><description>&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25191;&#34892;&#34917;&#20805;&#36827;&#34892;&#20195;&#30721;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Code Representation Pre-training with Complements from Program Executions. (arXiv:2309.09980v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzPretrain&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20195;&#30721;&#34920;&#31034;&#39044;&#35757;&#32451;&#20013;&#25506;&#32034;&#30001;&#31243;&#24207;&#30340;&#27979;&#35797;&#29992;&#20363;&#25581;&#31034;&#30340;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20174;&#20195;&#30721;&#20013;&#30452;&#25509;&#23398;&#20064;&#21151;&#33021;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#32534;&#31243;&#35821;&#35328;&#24314;&#27169;&#65292;&#20197;&#25512;&#36827;&#20195;&#30721;&#26234;&#33021;&#21270;&#12290;&#23613;&#31649;&#20195;&#30721;&#21487;&#20197;&#20197;&#25991;&#26412;&#26684;&#24335;&#34920;&#31034;&#65292;&#20294;&#20026;&#20102;&#27491;&#30830;&#32534;&#35793;&#25110;&#35299;&#37322;&#20197;&#25191;&#34892;&#19968;&#32452;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#20195;&#30721;&#22312;&#35821;&#27861;&#19978;&#26356;&#21152;&#20005;&#26684;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25277;&#35937;&#35821;&#27861;&#26641;&#12289;&#25511;&#21046;&#27969;&#22270;&#31561;&#24418;&#24335;&#30340;&#21477;&#27861;&#34920;&#31034;&#65292;&#20174;&#20195;&#30721;&#20013;&#20197;&#36739;&#23569;&#30340;&#27495;&#20041;&#24615;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#30456;&#21516;&#30446;&#30340;&#30340;&#31243;&#24207;&#21487;&#20197;&#29992;&#21508;&#31181;&#26041;&#24335;&#23454;&#29616;&#65292;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#21477;&#27861;&#34920;&#31034;&#65292;&#32780;&#20855;&#26377;&#31867;&#20284;&#23454;&#29616;&#30340;&#31243;&#24207;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;&#34429;&#28982;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#21487;&#20197;&#36731;&#26131;&#22320;&#28436;&#31034;&#36825;&#31181;&#35821;&#20041;&#65292;&#20294;&#21151;&#33021;&#19978;&#30340;&#36825;&#20123;&#35821;&#20041;&#24456;&#38590;&#30452;&#25509;&#20174;&#20195;&#30721;&#20013;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuzzPretrain&#26469;&#25506;&#32034;&#30001;&#27979;&#35797;&#29992;&#20363;&#25581;&#31034;&#30340;&#31243;&#24207;&#30340;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#23884;&#20837;&#21040;&#20195;&#30721;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) for natural language processing have been grafted onto programming language modeling for advancing code intelligence. Although it can be represented in the text format, code is syntactically more rigorous in order to be properly compiled or interpreted to perform a desired set of behaviors given any inputs. In this case, existing works benefit from syntactic representations to learn from code less ambiguously in the forms of abstract syntax tree, control-flow graph, etc. However, programs with the same purpose can be implemented in various ways showing different syntactic representations while the ones with similar implementations can have distinct behaviors. Though trivially demonstrated during executions, such semantics about functionality are challenging to be learned directly from code, especially in an unsupervised manner. Hence, in this paper, we propose FuzzPretrain to explore the dynamic information of programs revealed by their test cases and embed
&lt;/p&gt;</description></item><item><title>MindAgent&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#35780;&#20272;&#28216;&#25103;&#20114;&#21160;&#20013;&#30340;&#35268;&#21010;&#21644;&#21327;&#35843;&#26032;&#33021;&#21147;&#65292;&#23427;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#21327;&#35843;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#19982;&#20154;&#31867;&#29609;&#23478;&#21512;&#20316;&#12290;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28216;&#25103;&#22330;&#26223;&#21644;&#30456;&#20851;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.09971</link><description>&lt;p&gt;
MindAgent: &#26032;&#20852;&#28216;&#25103;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
MindAgent: Emergent Gaming Interaction. (arXiv:2309.09971v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09971
&lt;/p&gt;
&lt;p&gt;
MindAgent&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#35780;&#20272;&#28216;&#25103;&#20114;&#21160;&#20013;&#30340;&#35268;&#21010;&#21644;&#21327;&#35843;&#26032;&#33021;&#21147;&#65292;&#23427;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#21327;&#35843;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#19982;&#20154;&#31867;&#29609;&#23478;&#21512;&#20316;&#12290;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28216;&#25103;&#22330;&#26223;&#21644;&#30456;&#20851;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#25191;&#34892;&#22797;&#26434;&#35843;&#24230;&#24182;&#23558;&#36825;&#20123;&#26234;&#33021;&#20307;&#21327;&#35843;&#23436;&#25104;&#38656;&#35201;&#24191;&#27867;&#21327;&#20316;&#30340;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#32463;&#24341;&#20837;&#20102;&#35768;&#22810;&#28216;&#25103;&#26694;&#26550;&#65292;&#20294;&#31038;&#21306;&#22312;&#26500;&#24314;&#28085;&#30422;LLM&#21644;&#20154;&#31867;-NPC&#21327;&#20316;&#30340;&#36890;&#29992;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#20173;&#28982;&#32570;&#20047;&#22522;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#35774;&#26045;- MindAgent-&#26469;&#35780;&#20272;&#28216;&#25103;&#20114;&#21160;&#30340;&#35268;&#21010;&#21644;&#21327;&#35843;&#26032;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#22522;&#30784;&#35774;&#26045;&#21033;&#29992;&#29616;&#26377;&#30340;&#28216;&#25103;&#26694;&#26550;&#65292;i&#65289;&#38656;&#35201;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#21327;&#35843;&#21592;&#36827;&#34892;&#29702;&#35299;&#65292;ii&#65289;&#36890;&#36807;&#26410;&#32463;&#35843;&#20248;&#30340;&#27491;&#30830;&#25351;&#31034;&#19982;&#20154;&#31867;&#29609;&#23478;&#21512;&#20316;&#65292;iii&#65289;&#24314;&#31435;&#22312;&#23569;&#37327;&#25552;&#31034;&#21644;&#21453;&#39304;&#30340;&#24773;&#22659;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28216;&#25103;&#22330;&#26223;&#21644;&#30456;&#20851;&#30340;&#22522;&#20934;- CUISINEWORLD&#65292;&#35813;&#22330;&#26223;&#27966;&#36963;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaborat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#19971;&#20010;&#20195;&#34920;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.09825</link><description>&lt;p&gt;
AI&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#35265;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#30340;&#32771;&#23519;
&lt;/p&gt;
&lt;p&gt;
Bias of AI-Generated Content: An Examination of News Produced by Large Language Models. (arXiv:2309.09825v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09825
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#19971;&#20010;&#20195;&#34920;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;&#21363;AI&#29983;&#25104;&#20869;&#23481;&#65289;&#20855;&#26377;&#25913;&#21464;&#25105;&#20204;&#29983;&#27963;&#21644;&#24037;&#20316;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#31181;&#36716;&#21464;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;LLM&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#30001;&#19971;&#31181;&#20195;&#34920;&#24615;LLM&#65288;&#21253;&#25324;ChatGPT&#21644;LLaMA&#65289;&#29983;&#25104;&#30340;AIGC&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#12298;&#32445;&#32422;&#26102;&#25253;&#12299;&#21644;&#36335;&#36879;&#31038;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#20004;&#23478;&#23186;&#20307;&#20197;&#25552;&#20379;&#20844;&#27491;&#26080;&#20559;&#30340;&#26032;&#38395;&#32780;&#38395;&#21517;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#34987;&#35843;&#26597;&#30340;LLM&#24212;&#29992;&#20110;&#29983;&#25104;&#26032;&#38395;&#20869;&#23481;&#65292;&#20197;&#36825;&#20123;&#26032;&#38395;&#25991;&#31456;&#30340;&#26631;&#39064;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;AIGC&#19982;&#21407;&#22987;&#26032;&#38395;&#25991;&#31456;&#26469;&#35780;&#20272;LLM&#29983;&#25104;&#30340;AIGC&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#21508;&#20010;LLM&#22312;&#24102;&#26377;&#20559;&#35265;&#30340;&#25552;&#31034;&#19979;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#36890;&#36807;&#21521;&#20174;&#36825;&#20123;&#26032;&#38395;&#26631;&#39064;&#26500;&#24314;&#30340;&#25552;&#31034;&#20013;&#28155;&#21152;&#24615;&#21035;&#20559;&#35265;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#27599;&#20010;&#34987;&#35843;&#26597;&#30340;LLM&#29983;&#25104;&#30340;AIGC&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have the potential to transform our lives and work through the content they generate, known as AI-Generated Content (AIGC). To harness this transformation, we need to understand the limitations of LLMs. Here, we investigate the bias of AIGC produced by seven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, both known for their dedication to provide unbiased news. We then apply each examined LLM to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and the original news articles. We further analyze the gender bias of each LLM under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM ex
&lt;/p&gt;</description></item><item><title>&#22312;&#32570;&#20047;&#25991;&#21270;&#20849;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26080;&#38480;&#28145;&#24230;&#28508;&#22312;&#32467;&#26500;&#30340;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;iDLC-CCT&#65289;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;CCT&#65289;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#23545;&#20849;&#35782;&#20449;&#24565;&#22810;&#26679;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.09787</link><description>&lt;p&gt;
&#22312;&#32570;&#20047;&#25991;&#21270;&#20849;&#35782;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#38598;&#20307;&#26234;&#24935;
&lt;/p&gt;
&lt;p&gt;
Harnessing Collective Intelligence Under a Lack of Cultural Consensus. (arXiv:2309.09787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09787
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#25991;&#21270;&#20849;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26080;&#38480;&#28145;&#24230;&#28508;&#22312;&#32467;&#26500;&#30340;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;iDLC-CCT&#65289;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;CCT&#65289;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#23545;&#20849;&#35782;&#20449;&#24565;&#22810;&#26679;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38598;&#20307;&#26234;&#24935;&#26469;&#25512;&#21160;&#26377;&#25928;&#30340;&#20915;&#31574;&#21644;&#21512;&#20316;&#21463;&#30410;&#20110;&#33021;&#22815;&#26816;&#27979;&#21644;&#25551;&#36848;&#20849;&#35782;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#12290;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;CCT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#25551;&#36848;&#36825;&#20123;&#19981;&#21516;&#30340;&#20849;&#35782;&#20449;&#24565;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#29616;&#20195;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#23545;&#30456;&#20284;&#20449;&#24565;&#30340;&#27010;&#25324;&#33021;&#21147;&#65292;&#23545;&#31232;&#30095;&#25968;&#25454;&#26080;&#25928;&#65292;&#24182;&#19988;&#26080;&#27861;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#25110;&#23398;&#20064;&#21040;&#30340;&#26426;&#22120;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#26080;&#38480;&#28145;&#24230;&#28508;&#22312;&#32467;&#26500;&#30340;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;iDLC-CCT&#65289;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#28508;&#22312;&#32467;&#26500;&#26469;&#25193;&#23637;CCT&#12290;&#35813;&#32467;&#26500;&#20801;&#35768;&#25105;&#20204;&#23558;&#25991;&#21270;&#20849;&#35782;&#30475;&#20316;&#26159;&#19968;&#31995;&#21015;&#26080;&#38480;&#28145;&#24230;&#30340;&#27010;&#24565;&#26500;&#24314;&#22359;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#20449;&#24565;&#22810;&#26679;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing collective intelligence to drive effective decision-making and collaboration benefits from the ability to detect and characterize heterogeneity in consensus beliefs. This is particularly true in domains such as technology acceptance or leadership perception, where a consensus defines an intersubjective truth, leading to the possibility of multiple "ground truths" when subsets of respondents sustain mutually incompatible consensuses. Cultural Consensus Theory (CCT) provides a statistical framework for detecting and characterizing these divergent consensus beliefs. However, it is unworkable in modern applications because it lacks the ability to generalize across even highly similar beliefs, is ineffective with sparse data, and can leverage neither external knowledge bases nor learned machine representations. Here, we overcome these limitations through Infinite Deep Latent Construct Cultural Consensus Theory (iDLC-CCT), a nonparametric Bayesian model that extends CCT with a lat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22788;&#29702;&#25968;&#25454;&#39532;&#25289;&#26494;&#20013;&#30340;&#25968;&#25454;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;10&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09770</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#25968;&#25454;&#39532;&#25289;&#26494;&#20013;&#22788;&#29702;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
How to Data in Datathons. (arXiv:2309.09770v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22788;&#29702;&#25968;&#25454;&#39532;&#25289;&#26494;&#20013;&#30340;&#25968;&#25454;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;10&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39532;&#25289;&#26494;&#30340;&#20852;&#36215;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#30701;&#26102;&#38388;&#20869;&#21512;&#20316;&#12289;&#23398;&#20064;&#21644;&#21019;&#26032;&#30340;&#24179;&#21488;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#32452;&#32455;&#24448;&#24448;&#22240;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#26368;&#20339;&#23454;&#36341;&#32780;&#38590;&#20197;&#26377;&#25928;&#22788;&#29702;&#25968;&#25454;&#12290;&#26681;&#25454;&#25105;&#20204;&#33258;&#24049;&#30340;&#32463;&#39564;&#20197;&#21450;&#33258;2016&#24180;&#20197;&#26469;&#32452;&#32455;&#20102;&#36229;&#36807;80&#20010;&#25968;&#25454;&#39532;&#25289;&#26494;&#25361;&#25112;&#36187;&#19982;60&#20010;&#21512;&#20316;&#20249;&#20276;&#32452;&#32455;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#38024;&#21644;&#24314;&#35758;&#65292;&#20316;&#20026;&#32452;&#32455;&#32773;&#22312;&#22788;&#29702;&#25968;&#25454;&#30456;&#20851;&#22797;&#26434;&#24615;&#26102;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;10&#20010;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of datathons, also known as data or data science hackathons, has provided a platform to collaborate, learn, and innovate in a short timeframe. Despite their significant potential benefits, organizations often struggle to effectively work with data due to a lack of clear guidelines and best practices for potential issues that might arise. Drawing on our own experiences and insights from organizing &gt;80 datathon challenges with &gt;60 partnership organizations since 2016, we provide guidelines and recommendations that serve as a resource for organizers to navigate the data-related complexities of datathons. We apply our proposed framework to 10 case studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;D-Waves&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#12289;&#23500;&#22763;&#36890;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#25968;&#23383;&#36864;&#28779;&#22120;&#21644;Gurobi&#30340;&#32463;&#20856;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#30340;&#25351;&#23548;&#65292;&#21457;&#29616;&#25968;&#23383;&#36864;&#28779;&#22120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#28151;&#21512;&#37327;&#23376;&#36864;&#28779;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.09736</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#30340;&#37327;&#23376;&#20248;&#21270;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Quantum Optimization Case Study for a Transport Robot Scheduling Problem. (arXiv:2309.09736v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;D-Waves&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#12289;&#23500;&#22763;&#36890;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#25968;&#23383;&#36864;&#28779;&#22120;&#21644;Gurobi&#30340;&#32463;&#20856;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#30340;&#25351;&#23548;&#65292;&#21457;&#29616;&#25968;&#23383;&#36864;&#28779;&#22120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#28151;&#21512;&#37327;&#23376;&#36864;&#28779;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;D-Waves&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#12289;&#23500;&#22763;&#36890;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#25968;&#23383;&#36864;&#28779;&#22120;&#21644;Gurobi&#30340;&#26368;&#20808;&#36827;&#32463;&#20856;&#27714;&#35299;&#22120;&#22312;&#35299;&#20915;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;&#19968;&#20010;&#20855;&#26377;&#24037;&#19994;&#30456;&#20851;&#24615;&#30340;&#29616;&#23454;&#22330;&#26223;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#35774;&#35745;&#29702;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19981;&#21516;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#32452;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#31471;&#21040;&#31471;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;Gurobi&#30452;&#25509;&#27604;&#36739;&#65292;&#25968;&#23383;&#36864;&#28779;&#22120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#28151;&#21512;&#37327;&#23376;&#36864;&#28779;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;&#20351;&#29992;&#19981;&#21516;&#31574;&#30053;&#35299;&#20915;&#24212;&#29992;&#23548;&#21521;&#20248;&#21270;&#38382;&#39064;&#30340;&#24037;&#20316;&#27969;&#31243;&#30340;&#35265;&#35299;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive case study comparing the performance of D-Waves' quantum-classical hybrid framework, Fujitsu's quantum-inspired digital annealer, and Gurobi's state-of-the-art classical solver in solving a transport robot scheduling problem. This problem originates from an industrially relevant real-world scenario. We provide three different models for our problem following different design philosophies. In our benchmark, we focus on the solution quality and end-to-end runtime of different model and solver combinations. We find promising results for the digital annealer and some opportunities for the hybrid quantum annealer in direct comparison with Gurobi. Our study provides insights into the workflow for solving an application-oriented optimization problem with different strategies, and can be useful for evaluating the strengths and weaknesses of different approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LLM4Jobs&#26080;&#30417;&#30563;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#23637;&#29616;&#20102;&#36229;&#36234;&#26368;&#26032;&#22522;&#20934;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#30456;&#20851;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#20026;&#22797;&#26434;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20219;&#21153;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.09708</link><description>&lt;p&gt;
LLM4Jobs: &#26080;&#30417;&#30563;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#65292;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models. (arXiv:2309.09708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLM4Jobs&#26080;&#30417;&#30563;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#23637;&#29616;&#20102;&#36229;&#36234;&#26368;&#26032;&#22522;&#20934;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#30456;&#20851;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#20026;&#22797;&#26434;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20219;&#21153;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#33258;&#30001;&#25991;&#26412;&#30340;&#25307;&#32856;&#20449;&#24687;&#21644;&#31616;&#21382;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#32844;&#19994;&#26159;&#35832;&#22914;&#32844;&#20301;&#25512;&#33616;&#21644;&#21171;&#21160;&#21147;&#24066;&#22330;&#25919;&#31574;&#21046;&#23450;&#31561;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LLM4Jobs&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#32844;&#19994;&#32534;&#30721;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;LLM4Jobs&#29420;&#29305;&#22320;&#21033;&#29992;&#20102;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM4Jobs&#22987;&#32456;&#36229;&#36234;&#26080;&#30417;&#30563;&#30340;&#26368;&#26032;&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#31890;&#24230;&#19978;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#20316;&#20026;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#20010;&#38468;&#24102;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#35813;&#39046;&#22495;&#30340;&#21518;&#32493;&#30740;&#31350;&#21487;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#35843;&#26597;&#31361;&#26174;&#20102;&#24403;&#20195;LLMs&#22312;&#22797;&#26434;&#30340;&#32844;&#19994;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#20026;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated occupation extraction and standardization from free-text job postings and resumes are crucial for applications like job recommendation and labor market policy formation. This paper introduces LLM4Jobs, a novel unsupervised methodology that taps into the capabilities of large language models (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the natural language understanding and generation capacities of LLMs. Evaluated on rigorous experimentation on synthetic and real-world datasets, we demonstrate that LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks, demonstrating its versatility across diverse datasets and granularities. As a side result of our work, we present both synthetic and real-world datasets, which may be instrumental for subsequent research in this domain. Overall, this investigation highlights the promise of contemporary LLMs for the intricate task of occupation extraction and standardization, laying the foundation for a robust
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#38543;&#26426;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#25968;&#25454;&#28857;&#37117;&#21487;&#20197;&#21512;&#24182;&#25104;&#19968;&#26465;&#26354;&#32447;&#65292;&#25429;&#25417;&#20102;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#27491;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.09537</link><description>&lt;p&gt;
&#19968;&#31181;&#27169;&#22411;&#35780;&#20272;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#65306;&#22312;&#20449;&#24687;&#25193;&#25955;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A performance characteristic curve for model evaluation: the application in information diffusion prediction. (arXiv:2309.09537v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#38543;&#26426;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#25968;&#25454;&#28857;&#37117;&#21487;&#20197;&#21512;&#24182;&#25104;&#19968;&#26465;&#26354;&#32447;&#65292;&#25429;&#25417;&#20102;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#27491;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#20449;&#24687;&#25193;&#25955;&#39044;&#27979;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#28040;&#24687;&#30340;&#25509;&#25910;&#32773;&#65292;&#22312;&#24066;&#22330;&#33829;&#38144;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;&#23613;&#31649;&#19981;&#21516;&#30340;&#39044;&#27979;&#27169;&#22411;&#37117;&#22768;&#31216;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24615;&#33021;&#35780;&#20272;&#30340;&#36890;&#29992;&#26694;&#26550;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#65292;&#35813;&#26354;&#32447;&#25429;&#33719;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#25193;&#25955;&#25968;&#25454;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#28982;&#21518;&#30830;&#23450;&#20102;&#38543;&#26426;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#32553;&#25918;&#27169;&#24335;&#12290;&#19981;&#21516;&#24207;&#21015;&#38271;&#24230;&#12289;&#31995;&#32479;&#22823;&#23567;&#21644;&#38543;&#26426;&#24615;&#19979;&#30340;&#25968;&#25454;&#28857;&#37117;&#21512;&#24182;&#25104;&#19968;&#26465;&#26354;&#32447;&#65292;&#25429;&#25417;&#20102;&#27169;&#22411;&#22312;&#38754;&#23545;&#22686;&#21152;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#20316;&#20986;&#27491;&#30830;&#39044;&#27979;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#36825;&#26465;&#26354;&#32447;&#20855;&#26377;&#35780;&#20272;&#27169;&#22411;&#30340;&#37325;&#35201;&#23646;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The information diffusion prediction on social networks aims to predict future recipients of a message, with practical applications in marketing and social media. While different prediction models all claim to perform well, general frameworks for performance evaluation remain limited. Here, we aim to identify a performance characteristic curve for a model, which captures its performance on tasks of different complexity. We propose a metric based on information entropy to quantify the randomness in diffusion data, then identify a scaling pattern between the randomness and the prediction accuracy of the model. Data points in the patterns by different sequence lengths, system sizes, and randomness all collapse into a single curve, capturing a model's inherent capability of making correct predictions against increased uncertainty. Given that this curve has such important properties that it can be used to evaluate the model, we define it as the performance characteristic curve of the model.
&lt;/p&gt;</description></item><item><title>FedGKD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#24449;&#24182;&#24341;&#20837;&#24863;&#30693;&#20840;&#23616;&#21327;&#20316;&#32467;&#26500;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09517</link><description>&lt;p&gt;
FedGKD:&#22312;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37322;&#25918;&#21327;&#20316;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks. (arXiv:2309.09517v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09517
&lt;/p&gt;
&lt;p&gt;
FedGKD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#24449;&#24182;&#24341;&#20837;&#24863;&#30693;&#20840;&#23616;&#21327;&#20316;&#32467;&#26500;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#30001;&#20110;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#33021;&#22815;&#22312;&#25968;&#25454;&#38548;&#31163;&#22330;&#26223;&#19979;&#25191;&#34892;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#32852;&#37030;&#35757;&#32451;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#30340;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#32479;&#35745;&#37327;&#26469;&#34920;&#31034;&#23616;&#37096;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#32858;&#21512;&#26426;&#21046;&#23558;&#23427;&#20204;&#32852;&#31995;&#36215;&#26469;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#25928;&#29575;&#26377;&#38480;&#65306;&#20219;&#21153;&#30456;&#20851;&#24615;&#37327;&#21270;&#30340;&#36136;&#37327;&#20302;&#21644;&#21033;&#29992;&#21327;&#20316;&#32467;&#26500;&#30340;&#26080;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedGKD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;GNN&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#22320;&#25551;&#36848;&#20219;&#21153;&#30456;&#20851;&#24615;&#30340;&#20219;&#21153;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#24863;&#30693;&#21040;&#20840;&#23616;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;FedGKD&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated training of Graph Neural Networks (GNN) has become popular in recent years due to its ability to perform graph-related tasks under data isolation scenarios while preserving data privacy. However, graph heterogeneity issues in federated GNN systems continue to pose challenges. Existing frameworks address the problem by representing local tasks using different statistics and relating them through a simple aggregation mechanism. However, these approaches suffer from limited efficiency from two aspects: low quality of task-relatedness quantification and inefficacy of exploiting the collaboration structure. To address these issues, we propose FedGKD, a novel federated GNN framework that utilizes a novel client-side graph dataset distillation method to extract task features that better describe task-relatedness, and introduces a novel server-side aggregation mechanism that is aware of the global collaboration structure. We conduct extensive experiments on six real-world datasets of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20197;&#22312;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#20986;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.09457</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#35745;&#31639;&#22797;&#26434;&#24615;&#26080;&#27861;&#35299;&#20915;&#30340;&#39044;&#35328;&#26426;&#65292;&#22312;&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles. (arXiv:2309.09457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20197;&#22312;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#20986;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#23398;&#20064;&#32773;&#21487;&#20197;&#35775;&#38382;&#24050;&#30693;&#30340;&#29305;&#24449;&#26144;&#23556;$ \phi&#65288;x&#65292;a&#65289;$&#65292;&#35813;&#26144;&#23556;&#23558;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26144;&#23556;&#21040;$d$&#32500;&#21521;&#37327;&#65292;&#24182;&#19988;&#22870;&#21169;&#21644;&#36716;&#25442;&#26159;&#27492;&#34920;&#31034;&#20013;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#20294;&#26159;&#36825;&#20123;&#29305;&#24449;&#20174;&#21738;&#37324;&#26469;&#65311;&#22312;&#27809;&#26377;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#31181;&#35825;&#20154;&#30340;&#31574;&#30053;&#26159;&#20351;&#29992;&#8220;&#21416;&#25151;&#27700;&#27133;&#8221;&#26041;&#27861;&#65292;&#24182;&#24076;&#26395;&#30495;&#23454;&#29305;&#24449;&#21253;&#21547;&#22312;&#19968;&#20010;&#26356;&#22823;&#30340;&#28508;&#22312;&#29305;&#24449;&#38598;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#32447;&#24615;MDP&#12290;&#22312;$k$-&#31232;&#30095;&#32447;&#24615;MDP&#20013;&#65292;&#23384;&#22312;&#19968;&#20010;&#26410;&#30693;&#30340;&#22823;&#23567;&#20026;$k$&#30340;&#23376;&#38598;$S \subset [d]$&#65292;&#20854;&#20013;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#29305;&#24449;&#65292;&#30446;&#26631;&#26159;&#22312;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#20165;&#32463;&#36807;poly$(k,\log d)$&#27425;&#23398;&#20064;&#65292;&#23398;&#20064;&#20986;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#35201;&#20040;&#20570;&#20986;&#20102;&#26126;&#26174;&#30340;&#20551;&#35774;&#65292;&#20351;&#24471;&#25506;&#32034;&#26080;&#20851;&#32039;&#35201;&#65292;&#35201;&#20040;&#25552;&#20379;&#20102;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The key assumption underlying linear Markov Decision Processes (MDPs) is that the learner has access to a known feature map $\phi(x, a)$ that maps state-action pairs to $d$-dimensional vectors, and that the rewards and transitions are linear functions in this representation. But where do these features come from? In the absence of expert domain knowledge, a tempting strategy is to use the ``kitchen sink" approach and hope that the true features are included in a much larger set of potential features. In this paper we revisit linear MDPs from the perspective of feature selection. In a $k$-sparse linear MDP, there is an unknown subset $S \subset [d]$ of size $k$ containing all the relevant features, and the goal is to learn a near-optimal policy in only poly$(k,\log d)$ interactions with the environment. Our main result is the first polynomial-time algorithm for this problem. In contrast, earlier works either made prohibitively strong assumptions that obviated the need for exploration, o
&lt;/p&gt;</description></item><item><title>GenDOM&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#25805;&#20316;&#31574;&#30053;&#21644;&#22810;&#26679;&#21270;&#27169;&#25311;&#35757;&#32451;&#65292;&#20351;&#25805;&#20316;&#31574;&#30053;&#33021;&#22815;&#20165;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#21644;&#27169;&#25311;&#20043;&#38388;&#30340;&#28857;&#20113;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#26032;&#29289;&#20307;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.09051</link><description>&lt;p&gt;
GenDOM&#65306;&#20855;&#26377;&#21442;&#25968;&#24863;&#30693;&#30340;&#27867;&#21270;&#24615;&#19968;&#27425;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
GenDOM: Generalizable One-shot Deformable Object Manipulation with Parameter-Aware Policy. (arXiv:2309.09051v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09051
&lt;/p&gt;
&lt;p&gt;
GenDOM&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#25805;&#20316;&#31574;&#30053;&#21644;&#22810;&#26679;&#21270;&#27169;&#25311;&#35757;&#32451;&#65292;&#20351;&#25805;&#20316;&#31574;&#30053;&#33021;&#22815;&#20165;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#21644;&#27169;&#25311;&#20043;&#38388;&#30340;&#28857;&#20113;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#26032;&#29289;&#20307;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#36816;&#21160;&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#21464;&#24418;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#24448;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#26041;&#27861;&#65288;&#22914;&#32499;&#23376;&#21644;&#24067;&#26009;&#65289;&#36890;&#24120;&#38656;&#35201;&#25968;&#30334;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#35757;&#32451;&#27599;&#20010;&#29289;&#20307;&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GenDOM&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#25805;&#20316;&#31574;&#30053;&#21482;&#38656;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#25805;&#20316;&#31574;&#30053;&#19982;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#21487;&#21464;&#24418;&#29289;&#20307;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#31574;&#30053;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#29289;&#20307;&#21442;&#25968;&#35843;&#25972;&#21160;&#20316;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#32473;&#23450;&#19968;&#20010;&#26032;&#30340;&#29289;&#20307;&#65292;GenDOM&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#21644;&#27169;&#25311;&#20043;&#38388;&#28857;&#20113;&#30340;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#65292;&#32780;&#21482;&#38656;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the inherent uncertainty in their deformability during motion, previous methods in deformable object manipulation, such as rope and cloth, often required hundreds of real-world demonstrations to train a manipulation policy for each object, which hinders their applications in our ever-changing world. To address this issue, we introduce GenDOM, a framework that allows the manipulation policy to handle different deformable objects with only a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable object parameters and training it with a diverse range of simulated deformable objects so that the policy can adjust actions based on different object parameters. At the time of inference, given a new object, GenDOM can estimate the deformable object parameters with only a single real-world demonstration by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations in a differentiable phys
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#20302;&#31209;MDP&#23558;&#25512;&#33616;&#38382;&#39064;&#35270;&#20026;&#22312;&#32447;RL&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.08622</link><description>&lt;p&gt;
&#22522;&#20110;&#20302;&#31209;Slate&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in Low-rank Slate-based Recommender Systems. (arXiv:2309.08622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#20302;&#31209;MDP&#23558;&#25512;&#33616;&#38382;&#39064;&#35270;&#20026;&#22312;&#32447;RL&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#35813;&#29615;&#22659;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#36825;&#20351;&#24471;&#23398;&#20064;&#21644;&#25506;&#32034;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;slate&#25512;&#33616;&#35774;&#32622;&#65292;&#23558;&#20854;&#35270;&#20026;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#22312;&#32447;RL&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#30340;&#35774;&#32622;&#21644;&#37319;&#26679;&#26041;&#27861;&#26500;&#24314;&#20102;&#25512;&#33616;&#27169;&#25311;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) in recommendation systems offers the potential to optimize recommendations for long-term user engagement. However, the environment often involves large state and action spaces, which makes it hard to efficiently learn and explore. In this work, we propose a sample-efficient representation learning algorithm, using the standard slate recommendation setup, to treat this as an online RL problem with low-rank Markov decision processes (MDPs). We also construct the recommender simulation environment with the proposed setup and sampling method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31616;&#21270;&#30340;&#38598;&#21512;&#32467;&#26500;BEA&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#19982;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08036</link><description>&lt;p&gt;
BEA: &#37325;&#26032;&#23457;&#35270;&#20351;&#29992;Budding Ensemble Architecture&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#30446;&#26631;&#26816;&#27979;DNN
&lt;/p&gt;
&lt;p&gt;
BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture. (arXiv:2309.08036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31616;&#21270;&#30340;&#38598;&#21512;&#32467;&#26500;BEA&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#19982;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Budding Ensemble Architecture&#65288;BEA&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#26032;&#22411;&#31616;&#21270;&#38598;&#21512;&#32467;&#26500;&#12290;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#12290;&#23427;&#20204;&#24212;&#35813;&#25552;&#20379;&#31934;&#30830;&#30340;&#36793;&#30028;&#26694;&#26816;&#27979;&#65292;&#24182;&#26657;&#20934;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20551;&#38451;&#24615;&#25509;&#25910;&#21040;&#39640;&#20998;&#25110;&#30495;&#38451;&#24615;&#30001;&#20110;&#20302;&#20998;&#32780;&#34987;&#20002;&#24323;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20570;&#20986;&#38169;&#35823;&#30340;&#20915;&#31574;&#12290;BEA&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;BEA&#30340;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#21892;&#20102;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#21644;&#38477;&#20302;&#20102;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21306;&#20998;&#30495;&#38451;&#24615;&#21644;&#20551;&#38451;&#24615;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;BEA&#26041;&#27861;&#21644;&#20854;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;Base-YOLOv3&#21644;SSD&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;BEA&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;Base-YOLOv3&#32467;&#26524;&#20013;&#65292;&#31934;&#24230;&#20998;&#21035;&#25552;&#39640;&#20102;6%&#21644;3.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Budding Ensemble Architecture (BEA), a novel reduced ensemble architecture for anchor-based object detection models. Object detection models are crucial in vision-based tasks, particularly in autonomous systems. They should provide precise bounding box detections while also calibrating their predicted confidence scores, leading to higher-quality uncertainty estimates. However, current models may make erroneous decisions due to false positives receiving high scores or true positives being discarded due to low scores. BEA aims to address these issues. The proposed loss functions in BEA improve the confidence score calibration and lower the uncertainty error, which results in a better distinction of true and false positives and, eventually, higher accuracy of the object detection models. Both Base-YOLOv3 and SSD models were enhanced using the BEA method and its proposed loss functions. The BEA on Base-YOLOv3 trained on the KITTI dataset results in a 6% and 3.7% i
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.07864</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07864
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#31867;&#19968;&#30452;&#36861;&#27714;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36798;&#21040;&#25110;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#30446;&#26631;&#65292;&#32780;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#26395;&#26041;&#24335;&#30340;AI&#20195;&#29702;&#12290;AI&#20195;&#29702;&#26159;&#33021;&#24863;&#30693;&#29615;&#22659;&#12289;&#20570;&#20986;&#20915;&#31574;&#21644;&#37319;&#21462;&#34892;&#21160;&#30340;&#20154;&#24037;&#23454;&#20307;&#12290;&#33258;20&#19990;&#32426;&#20013;&#21494;&#20197;&#26469;&#65292;&#20154;&#20204;&#20026;&#24320;&#21457;&#26234;&#33021;AI&#20195;&#29702;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#31639;&#27861;&#25110;&#35757;&#32451;&#31574;&#30053;&#30340;&#36827;&#27493;&#19978;&#65292;&#20197;&#22686;&#24378;&#29305;&#23450;&#33021;&#21147;&#25110;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#25152;&#32570;&#20047;&#30340;&#26159;&#19968;&#20010;&#36275;&#22815;&#36890;&#29992;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#35774;&#35745;&#33021;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#30340;AI&#20195;&#29702;&#30340;&#36215;&#28857;&#12290;&#30001;&#20110;&#23637;&#31034;&#20986;&#30340;&#22810;&#21151;&#33021;&#21644;&#26174;&#33879;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#20026;&#26500;&#24314;&#36890;&#29992;AI&#20195;&#29702;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#21033;&#29992;LLMs&#20316;&#20026;&#26500;&#24314;AI&#20195;&#29702;&#30340;&#22522;&#30784;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.07315</link><description>&lt;p&gt;
&#26053;&#34892;&#35789;&#65306;&#19968;&#31181;&#21464;&#21387;&#22120;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29702;&#35299;&#20854;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35828;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#23558;&#28508;&#22312;&#29305;&#24449;&#38480;&#21046;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#27880;&#24847;&#21147;&#33021;&#22815;&#22312;&#35813;&#34920;&#38754;&#19978;&#22609;&#36896;&#21333;&#35789;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#36825;&#31181;&#20960;&#20309;&#35270;&#28857;&#26080;&#32541;&#22320;&#36830;&#25509;&#20102;&#36845;&#20195;&#25913;&#36827;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#31561;&#24050;&#30693;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;124M&#21442;&#25968;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#26089;&#26399;&#23618;&#20013;&#28165;&#26224;&#30340;&#26597;&#35810;-&#38190;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#22312;&#26356;&#28145;&#30340;&#23618;&#27425;&#19978;&#24314;&#31435;&#22312;&#20808;&#21069;&#20851;&#20110;&#27880;&#24847;&#22836;&#30340;&#19987;&#38376;&#24615;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#12290;&#21033;&#29992;&#36825;&#20123;&#20960;&#20309;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#65292;&#23558;&#20854;&#25551;&#32472;&#20026;&#22609;&#36896;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;O-RAN&#32593;&#32476;&#20013;&#20351;&#29992;DRL&#31639;&#27861;&#36827;&#34892;&#38381;&#29615;&#25511;&#21046;&#26102;&#36935;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07265</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#21152;&#36895;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;O-RAN&#20999;&#29255;: &#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;O-RAN&#32593;&#32476;&#20013;&#20351;&#29992;DRL&#31639;&#27861;&#36827;&#34892;&#38381;&#29615;&#25511;&#21046;&#26102;&#36935;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#26550;&#26500;&#25903;&#25345;&#26234;&#33021;&#32593;&#32476;&#25511;&#21046;&#31639;&#27861;&#20316;&#20026;&#20854;&#26680;&#24515;&#33021;&#21147;&#20043;&#19968;&#12290;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36890;&#36807;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#26234;&#33021;&#25511;&#21046;&#22120;&#65288;RIC&#65289;&#26469;&#20248;&#21270;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;RAN&#65289;&#21151;&#33021;&#12290;&#22312;O-RAN&#25991;&#29486;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26159;&#35299;&#20915;&#21160;&#24577;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;O-RAN RIC&#24341;&#20837;&#20102;&#35832;&#22810;&#22909;&#22788;&#65292;&#20294;&#22312;&#30495;&#23454;&#32593;&#32476;&#37096;&#32626;&#20013;&#65292;DRL&#31639;&#27861;&#30340;&#23454;&#38469;&#37319;&#29992;&#21364;&#33853;&#21518;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;DRL&#20195;&#29702;&#22312;&#37096;&#32626;&#21644;&#38754;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#32593;&#32476;&#26465;&#20214;&#26102;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#20316;&#20026;O-RAN&#21151;&#33021;&#30340;DRL&#22522;&#20110;&#38381;&#29615;&#25511;&#21046;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#27969;&#31243;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;TL&#36741;&#21161;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The open radio access network (O-RAN) architecture supports intelligent network control algorithms as one of its core capabilities. Data-driven applications incorporate such algorithms to optimize radio access network (RAN) functions via RAN intelligent controllers (RICs). Deep reinforcement learning (DRL) algorithms are among the main approaches adopted in the O-RAN literature to solve dynamic radio resource management problems. However, despite the benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms in real network deployments falls behind. This is primarily due to the slow convergence and unstable performance exhibited by DRL agents upon deployment and when facing previously unseen network conditions. In this paper, we address these challenges by proposing transfer learning (TL) as a core component of the training and deployment workflows for the DRL-based closed-loop control of O-RAN functionalities. To this end, we propose and design a hybrid TL-aided a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#20855;&#26377;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#65292;&#33021;&#22815;&#25552;&#21319;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05557</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32593;&#32476;&#36816;&#32500;&#33021;&#21147;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#20855;&#26377;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#65292;&#33021;&#22815;&#25552;&#21319;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22238;&#31572;&#20154;&#31867;&#35821;&#35328;&#26597;&#35810;&#65292;&#24182;&#22312;&#32593;&#32476;&#36816;&#32500;&#65288;NetOps&#65289;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#28508;&#21147;&#24212;&#29992;&#12290;&#30001;&#20110;&#20855;&#22791;&#22823;&#37327;&#24120;&#35782;&#30693;&#35782;&#65292;LLMs&#22312;&#25512;&#29702;&#20934;&#30830;&#24615;&#19978;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#25512;&#29702;&#33021;&#21147;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#20123;&#33021;&#21147;&#21487;&#33021;&#23545;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;NetOps&#26377;&#24040;&#22823;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#21508;&#31181;NetOps&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#36873;&#25321;&#30340;&#20960;&#31181;LLMs&#22312;NetOps&#39046;&#22495;&#30340;&#33021;&#21147;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;&#35780;&#20272;&#38024;&#23545;5732&#20010;&#20851;&#20110;NetOps&#30340;&#38382;&#39064;&#36827;&#34892;&#65292;&#28085;&#30422;&#20102;26&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#36890;&#29992;&#39046;&#22495;LLMs&#65292;&#21253;&#25324;ChatGPT&#12289;LLaMA&#12289;Falcon&#31561;&#12290;&#25105;&#20204;&#36824;&#23545;&#20854;&#20013;&#19968;&#20123;LLMs&#36827;&#34892;&#20102;NetOps&#35821;&#26009;&#24211;&#30340;&#24494;&#35843;&#65292;&#24182;&#35780;&#20272;&#20102;&#32467;&#26524;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#26041;&#27861;&#36981;&#24490;&#24191;&#27867;&#37319;&#29992;&#30340;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can respond to human language queries and have shown powerful potential applications in network operations (NetOps). Thanks to the large amount of commonsense knowledge inherent, LLMs achieve much better inference accuracy than traditional models and emerge with strong abilities in generalization, reasoning, and code generation. These abilities may have a crucial boost to automated and intelligent NetOps. However, it remains under-explored how well LLMs perform in various NetOps tasks. In this work, we make a systematic assessment of the capabilities, strengths, and limitations of selected LLMs in the field of NetOps. The evaluation is conducted on a collection of 5,732 questions about NetOps, encompassing 26 publicly available general-domain LLMs, including ChatGPT, LLaMA, Falcon, etc. We also finetune some of these LLMs with our collected NetOps corpus and evaluate the resulting models. The evaluation method follows the widely adopted benchmarks for gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;FOLLOWUPQG&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.05007</link><description>&lt;p&gt;
FOLLOWUPQG:&#38754;&#21521;&#20449;&#24687;&#33719;&#21462;&#30340;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;FOLLOWUPQG&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20986;&#20110;&#22909;&#22855;&#24515;&#32780;&#25552;&#20986;&#36319;&#36827;&#38382;&#39064;&#65292;&#36825;&#21453;&#26144;&#20102;&#20154;&#31867;&#21019;&#36896;&#24615;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#65288;FQG&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#29983;&#25104;&#33021;&#22815;&#26356;&#28145;&#20837;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;FOLLOWUPQG&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;Reddit&#35770;&#22363;&#30340;&#36229;&#36807;3K&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#65288;&#21021;&#22987;&#38382;&#39064;&#65292;&#31572;&#26696;&#65292;&#36319;&#36827;&#38382;&#39064;&#65289;&#20803;&#32452;&#65292;&#25552;&#20379;&#20102;&#23545;&#24320;&#25918;&#24615;&#38382;&#39064;&#30340;&#38750;&#19987;&#19994;&#20154;&#22763;&#21451;&#22909;&#30340;&#35299;&#37322;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;FOLLOWUPQG&#20013;&#30340;&#38382;&#39064;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#30340;&#23454;&#29992;&#31574;&#30053;&#26469;&#23547;&#27714;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#39640;&#23618;&#27425;&#30340;&#35748;&#30693;&#25216;&#33021;&#65288;&#22914;&#24212;&#29992;&#21644;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#25506;&#32034;&#22914;&#20309;&#22522;&#20110;&#36880;&#27493;&#28436;&#31034;&#29983;&#25104;&#29305;&#23450;&#31867;&#22411;&#30340;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;FOLLOWUPQG&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans ask follow-up questions driven by curiosity, which reflects a creative human cognitive process. We introduce the task of real-world information-seeking follow-up question generation (FQG), which aims to generate follow-up questions seeking a more in-depth understanding of an initial question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world (initial question, answer, follow-up question) tuples collected from a Reddit forum providing layman-friendly explanations for open-ended questions. In contrast to existing datasets, questions in FOLLOWUPQG use more diverse pragmatic strategies to seek information, and they also show higher-order cognitive skills (such as applying and relating). We evaluate current question generation models on their efficacy for generating follow-up questions, exploring how to generate specific types of follow-up questions based on step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging benchmark, as model-generated q
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03648</link><description>&lt;p&gt;
GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;Lipschitz&#29305;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03648
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#30028;&#38480;&#26159;&#20174;&#40065;&#26834;&#32479;&#35745;&#23398;&#20013;&#20511;&#37492;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#38480;&#21046;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#26368;&#22823;&#21464;&#21270;&#65292;&#32771;&#34385;&#21040;&#30456;&#20851;&#30340;&#38750;&#20851;&#38190;&#20559;&#20506;&#22240;&#32032;&#12290;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#26597;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#31283;&#23450;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25805;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;GNN&#30340;Lipschitz&#30028;&#38480;&#20197;&#25581;&#31034;&#27169;&#22411;&#36755;&#20986;&#30340;&#31283;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#20559;&#20506;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#12290;&#30001;&#20110;&#24120;&#35265;&#22270;&#24418;&#25968;&#25454;&#22312;GNN&#35757;&#32451;&#20013;&#23384;&#22312;&#22266;&#26377;&#20559;&#24046;&#65292;&#36825;&#32473;&#38480;&#21046;&#30001;&#36755;&#20837;&#20559;&#24046;&#24341;&#36215;&#30340;GNN&#36755;&#20986;&#25200;&#21160;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#38556;&#20844;&#24179;&#24615;&#65292;&#24102;&#26469;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23613;&#31649;Lipschitz&#24120;&#25968;&#22312;&#25511;&#21046;&#27431;&#20960;&#37324;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#26377;&#25152;&#24212;&#29992;&#65292;&#20294;&#31934;&#30830;Lipschitz&#24120;&#25968;&#30340;&#35745;&#31639;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;</title><link>http://arxiv.org/abs/2308.16375</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#35843;&#26597;&#65306;&#25915;&#20987;&#12289;&#20445;&#25252;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16375
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#25913;&#21892;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#39640;&#25928;&#33021;&#34920;&#29616;&#65292;&#22914;&#20934;&#30830;&#24615;&#65292;&#32780;&#32570;&#20047;&#38544;&#31169;&#32771;&#34385;&#65292;&#36825;&#26159;&#29616;&#20195;&#31038;&#20250;&#38544;&#31169;&#25915;&#20987;&#30427;&#34892;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;GNNs&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22270;&#39046;&#22495;&#32570;&#20047;&#23545;&#25915;&#20987;&#21644;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24635;&#32467;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#25915;&#20987;&#12289;&#23545;GNNs&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#23457;&#26597;&#21487;&#29992;&#20110;&#20998;&#26512;/&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#31243;&#24207;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#20197;&#24314;&#31435;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34917;&#20805;&#21355;&#26143;&#22320;&#22270;&#65292;&#22686;&#24378;&#20102;&#36710;&#36733;&#20256;&#24863;&#22120;&#26500;&#24314;&#39640;&#31934;&#24230;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22320;&#22270;&#30340;&#24191;&#38420;&#35206;&#30422;&#33021;&#21147;&#12290;&#25105;&#20204;&#37322;&#25918;&#20102;&#21355;&#26143;&#22320;&#22270;&#29926;&#29255;&#20316;&#20026;nuScenes&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#34701;&#21512;&#27169;&#22359;&#26469;&#26356;&#22909;&#22320;&#34701;&#21512;&#36710;&#36733;&#20256;&#24863;&#22120;&#19982;&#21355;&#26143;&#22320;&#22270;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.15427</link><description>&lt;p&gt;
&#36890;&#36807;&#21355;&#26143;&#22320;&#22270;&#34917;&#20805;&#36710;&#36733;&#20256;&#24863;&#22120;&#65306;&#39640;&#31934;&#24230;&#22320;&#22270;&#26500;&#24314;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction. (arXiv:2308.15427v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34917;&#20805;&#21355;&#26143;&#22320;&#22270;&#65292;&#22686;&#24378;&#20102;&#36710;&#36733;&#20256;&#24863;&#22120;&#26500;&#24314;&#39640;&#31934;&#24230;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22320;&#22270;&#30340;&#24191;&#38420;&#35206;&#30422;&#33021;&#21147;&#12290;&#25105;&#20204;&#37322;&#25918;&#20102;&#21355;&#26143;&#22320;&#22270;&#29926;&#29255;&#20316;&#20026;nuScenes&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#34701;&#21512;&#27169;&#22359;&#26469;&#26356;&#22909;&#22320;&#34701;&#21512;&#36710;&#36733;&#20256;&#24863;&#22120;&#19982;&#21355;&#26143;&#22320;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31934;&#24230;&#65288;HD&#65289;&#22320;&#22270;&#23545;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#22522;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#20449;&#24687;&#23454;&#26102;&#26500;&#24314;HD&#22320;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#36710;&#36742;&#21608;&#22260;&#29615;&#22659;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22914;&#23545;&#36828;&#31243;&#25506;&#27979;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#34917;&#20805;&#21355;&#26143;&#22320;&#22270;&#21487;&#20197;&#22686;&#24378;HD&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21033;&#29992;&#21355;&#26143;&#22320;&#22270;&#30340;&#24191;&#27867;&#35206;&#30422;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#21355;&#26143;&#22320;&#22270;&#29926;&#29255;&#20316;&#20026;nuScenes&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#25968;&#25454;&#38598;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#34701;&#21512;&#27169;&#22359;&#65292;&#20351;&#21355;&#26143;&#22320;&#22270;&#20449;&#24687;&#19982;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#34701;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#20998;&#21106;&#21644;&#36317;&#31163;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#24212;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#34701;&#21512;&#36710;&#36733;&#20256;&#24863;&#22120;&#19982;&#21355;&#26143;&#22320;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-Definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time based on information obtained from vehicle onboard sensors. However, the performance of these methods is significantly susceptible to the environment surrounding the vehicle due to the inherent limitation of onboard sensors, such as weak capacity for long-range detection. In this study, we demonstrate that supplementing onboard sensors with satellite maps can enhance the performance of HD map construction methods, leveraging the broad coverage capability of satellite maps. For the purpose of further research, we release the satellite map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose a hierarchical fusion module that enables better fusion of satellite maps information with existing methods. Specifically, we design an attention mask based on segmentation and distance, applying the cross-attention mechanism to fuse onboa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.15126</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35780;&#20272;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LVLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#24187;&#35273;&#25351;&#30340;&#26159;LVLMs&#21709;&#24212;&#20013;&#19981;&#23384;&#22312;&#20110;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#21518;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#23545;LVLMs&#20013;&#30340;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#24037;&#20316;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#12290;HaELM&#30340;&#24615;&#33021;&#36817;&#20284;&#20110;ChatGPT&#30340;95%&#65292;&#24182;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#21487;&#22797;&#29616;&#12289;&#20445;&#25252;&#38544;&#31169;&#21644;&#26412;&#22320;&#37096;&#32626;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#21033;&#29992;HaELM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LVLMs&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;LVLMs&#20013;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#26377;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;RGB-D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;RGB-D&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#35757;&#32451;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#36924;&#30495;&#30340;RGB-D&#22270;&#20687;&#26469;&#23454;&#29616;&#32473;&#23450;&#35821;&#20041;&#26631;&#31614;&#22270;&#30340;&#21512;&#25104;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23558;&#19982;&#27169;&#24577;&#26080;&#20851;&#30340;&#20449;&#24687;&#19982;&#19982;&#27169;&#24577;&#30456;&#20851;&#30340;&#20449;&#24687;&#20998;&#31163;&#24320;&#26469;&#12290;</title><link>http://arxiv.org/abs/2308.11356</link><description>&lt;p&gt;
&#35821;&#20041;RGB-D&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic RGB-D Image Synthesis. (arXiv:2308.11356v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;RGB-D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;RGB-D&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#35757;&#32451;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#36924;&#30495;&#30340;RGB-D&#22270;&#20687;&#26469;&#23454;&#29616;&#32473;&#23450;&#35821;&#20041;&#26631;&#31614;&#22270;&#30340;&#21512;&#25104;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23558;&#19982;&#27169;&#24577;&#26080;&#20851;&#30340;&#20449;&#24687;&#19982;&#19982;&#27169;&#24577;&#30456;&#20851;&#30340;&#20449;&#24687;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;RGB-D&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#30340;&#35757;&#32451;&#38598;&#20013;&#25910;&#38598;&#21508;&#31181;&#21508;&#26679;&#30340;&#22270;&#20687;&#24182;&#19981;&#24635;&#26159;&#21487;&#33021;&#30340;&#12290;&#23588;&#20854;&#26159;&#24403;&#26426;&#22120;&#20154;&#38656;&#35201;&#22312;&#38544;&#31169;&#25935;&#24863;&#21306;&#22495;&#22914;&#23478;&#24237;&#20013;&#25805;&#20316;&#26102;&#65292;&#25910;&#38598;&#21463;&#38480;&#20110;&#19968;&#23567;&#37096;&#20998;&#22320;&#28857;&#12290;&#22240;&#27492;&#65292;&#26631;&#27880;&#22270;&#20687;&#22312;&#22806;&#35266;&#19978;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;RGB-D&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#24448;&#24448;&#23545;&#35757;&#32451;&#25968;&#25454;&#36807;&#24230;&#25311;&#21512;&#12290;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;RGB-D&#22270;&#20687;&#21512;&#25104;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#38656;&#35201;&#20026;&#32473;&#23450;&#30340;&#35821;&#20041;&#26631;&#31614;&#22270;&#21512;&#25104;&#19968;&#20010;&#36924;&#30495;&#30340;RGB-D&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21333;&#27169;&#24577;&#30340;&#65292;&#19981;&#33021;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#21333;&#27169;&#24577;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#26102;&#25928;&#26524;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#29983;&#25104;&#22120;&#65292;&#23558;&#35821;&#20041;&#24067;&#23616;&#20013;&#19982;&#27169;&#24577;&#26080;&#20851;&#30340;&#20449;&#24687;&#19982;&#29983;&#25104;RGB&#21644;&#28145;&#24230;&#22270;&#20687;&#25152;&#38656;&#30340;&#19982;&#27169;&#24577;&#30456;&#20851;&#30340;&#20449;&#24687;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting diverse sets of training images for RGB-D semantic image segmentation is not always possible. In particular, when robots need to operate in privacy-sensitive areas like homes, the collection is often limited to a small set of locations. As a consequence, the annotated images lack diversity in appearance and approaches for RGB-D semantic image segmentation tend to overfit the training data. In this paper, we thus introduce semantic RGB-D image synthesis to address this problem. It requires synthesising a realistic-looking RGB-D image for a given semantic label map. Current approaches, however, are uni-modal and cannot cope with multi-modal data. Indeed, we show that extending uni-modal approaches to multi-modal data does not perform well. In this paper, we therefore propose a generator for multi-modal data that separates modal-independent information of the semantic layout from the modal-dependent information that is needed to generate an RGB and a depth image, respectively. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26080;&#20154;&#36816;&#36755;&#30340;&#32508;&#21512;&#26041;&#26696;&#65292;&#21253;&#25324;FusionPlanner&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#12289;MiningNav&#35780;&#20272;&#24037;&#20855;&#21644;Parallel Mining Simulator&#27169;&#25311;&#22120;&#65292;&#20197;&#24212;&#23545;&#38706;&#22825;&#37319;&#30719;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#36816;&#36755;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.06931</link><description>&lt;p&gt;
FusionPlanner&#65306;&#19968;&#31181;&#20351;&#29992;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#30340;&#22810;&#20219;&#21153;&#36816;&#21160;&#35268;&#21010;&#22120;&#65292;&#29992;&#20110;&#30719;&#29992;&#21345;&#36710;
&lt;/p&gt;
&lt;p&gt;
FusionPlanner: A Multi-task Motion Planner for Mining Trucks using Multi-sensor Fusion Method. (arXiv:2308.06931v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26080;&#20154;&#36816;&#36755;&#30340;&#32508;&#21512;&#26041;&#26696;&#65292;&#21253;&#25324;FusionPlanner&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#12289;MiningNav&#35780;&#20272;&#24037;&#20855;&#21644;Parallel Mining Simulator&#27169;&#25311;&#22120;&#65292;&#20197;&#24212;&#23545;&#38706;&#22825;&#37319;&#30719;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#36816;&#36755;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#26234;&#33021;&#36710;&#36742;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#20856;&#22411;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#38706;&#22825;&#37319;&#30719;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#25805;&#20316;&#26465;&#20214;&#21644;&#19981;&#21033;&#30340;&#29615;&#22659;&#22240;&#32032;&#32780;&#21560;&#24341;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26080;&#20154;&#36816;&#36755;&#30340;&#32508;&#21512;&#33539;&#24335;&#65292;&#21253;&#25324;&#19968;&#20010;&#20223;&#30495;&#24179;&#21488;&#12289;&#19968;&#20010;&#27979;&#35797;&#22522;&#20934;&#21644;&#19968;&#20010;&#21487;&#38752;&#31283;&#20581;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FusionPlanner&#30340;&#22810;&#20219;&#21153;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#36866;&#24212;&#26080;&#20154;&#36816;&#36755;&#30340;&#27178;&#21521;&#21644;&#32437;&#21521;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;MiningNav&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#38706;&#22825;&#37319;&#30719;&#20132;&#36890;&#36947;&#36335;&#19978;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#31639;&#27861;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24182;&#34892;&#37319;&#30719;&#27169;&#25311;&#22120;&#65288;PMS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant achievements have been made in motion planning for intelligent vehicles. However, as a typical unstructured environment, open-pit mining attracts limited attention due to its complex operational conditions and adverse environmental factors. A comprehensive paradigm for unmanned transportation in open-pit mines is proposed in this research, including a simulation platform, a testing benchmark, and a trustworthy and robust motion planner. Firstly, we propose a multi-task motion planning algorithm, called FusionPlanner, for autonomous mining trucks by the Multi-sensor fusion method to adapt both lateral and longitudinal control tasks for unmanned transportation. Then, we develop a novel benchmark called MiningNav, which offers three validation approaches to evaluate the trustworthiness and robustness of well-trained algorithms in transportation roads of open-pit mines. Finally, we introduce the Parallel Mining Simulator (PMS), a new high-fidelity simulator spe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20026;&#20160;&#20040;&#23578;&#26410;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#65292;&#24182;&#25351;&#20986;&#20102;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21644;&#36164;&#37329;&#25512;&#24191;&#19981;&#36275;&#26159;&#21046;&#32422;AGI&#21457;&#23637;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#23454;&#29616;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#33258;&#20027;&#23398;&#20064;&#25152;&#38656;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03598</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25105;&#20204;&#23578;&#26410;&#25317;&#26377;AGI
&lt;/p&gt;
&lt;p&gt;
Why We Don't Have AGI Yet. (arXiv:2308.03598v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20026;&#20160;&#20040;&#23578;&#26410;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#65292;&#24182;&#25351;&#20986;&#20102;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21644;&#36164;&#37329;&#25512;&#24191;&#19981;&#36275;&#26159;&#21046;&#32422;AGI&#21457;&#23637;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#23454;&#29616;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#33258;&#20027;&#23398;&#20064;&#25152;&#38656;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2002&#24180;&#37325;&#26032;&#38416;&#36848;&#20102;AI&#30340;&#21407;&#22987;&#24895;&#26223;&#65292;&#31216;&#20043;&#20026;&#8220;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#8221;&#25110;AGI&#12290;&#36825;&#19968;&#24895;&#26223;&#26159;&#26500;&#24314;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#35299;&#20915;&#38382;&#39064;&#30340;&#8220;&#24605;&#32771;&#26426;&#22120;&#8221;&#35745;&#31639;&#26426;&#31995;&#32479;&#12290;&#36825;&#19982;&#20960;&#21313;&#24180;&#26469;&#20960;&#20046;&#25152;&#26377;&#20154;&#22312;&#35813;&#39046;&#22495;&#23454;&#36341;&#30340;&#8220;&#29421;&#20041;AI&#8221;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#34429;&#28982;&#26377;&#20960;&#20010;&#22823;&#35268;&#27169;&#30340;&#39033;&#30446;&#21517;&#20041;&#19978;&#22312;&#33268;&#21147;&#20110;AGI&#30340;&#30740;&#21457;&#65288;&#23588;&#20854;&#26159;DeepMind&#65289;&#65292;&#20294;&#22312;&#32431;&#31929;&#19987;&#27880;&#30340;AGI&#21457;&#23637;&#39046;&#22495;&#65292;&#36164;&#37329;&#21644;&#25512;&#24191;&#24182;&#19981;&#20805;&#36275;&#12290;&#36825;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#30495;&#27491;&#30340;AGI&#21487;&#20197;&#20026;&#20154;&#31867;&#24102;&#26469;&#24040;&#22823;&#30340;&#20215;&#20540;&#12290;&#38500;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#32570;&#20047;&#21162;&#21147;&#20043;&#22806;&#65292;&#36824;&#23384;&#22312;&#20111;&#27424;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#19978;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#24378;&#35843;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#26080;&#27861;&#23454;&#29616;AGI&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#36866;&#24212;&#33021;&#21147;&#21644;&#33258;&#20027;&#23398;&#20064;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#31038;&#20250;&#25216;&#26415;&#21457;&#23637;&#26041;&#38754;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The original vision of AI was re-articulated in 2002 via the term 'Artificial General Intelligence' or AGI. This vision is to build 'Thinking Machines' computer systems that can learn, reason, and solve problems similar to the way humans do. This is in stark contrast to the 'Narrow AI' approach practiced by almost everyone in the field over the many decades. While several large-scale efforts have nominally been working on AGI (most notably DeepMind), the field of pure focused AGI development has not been well funded or promoted. This is surprising given the fantastic value that true AGI can bestow on humanity. In addition to the dearth of effort in this field, there are also several theoretical and methodical missteps that are hampering progress. We highlight why purely statistical approaches are unlikely to lead to AGI, and identify several crucial cognitive abilities required to achieve human-like adaptability and autonomous learning. We conclude with a survey of socio-technical fa
&lt;/p&gt;</description></item><item><title>VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.02117</link><description>&lt;p&gt;
VQGraph: &#22270;&#24418;&#21521;&#37327;&#37327;&#21270;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs
&lt;/p&gt;
&lt;p&gt;
VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02117
&lt;/p&gt;
&lt;p&gt;
VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#20449;&#24687;&#20256;&#36882;&#65292;&#32858;&#21512;&#23616;&#37096;&#37051;&#23621;&#20197;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#20449;&#24687;&#20256;&#36882;&#23548;&#33268;&#22312;&#23454;&#38469;&#30340;&#24310;&#36831;&#32422;&#26463;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#27169;&#20223;GNN&#30340;&#36755;&#20986;&#26469;&#23398;&#20064;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#34920;&#31034;&#31354;&#38388;&#21487;&#33021;&#19981;&#36275;&#20197;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#65292;&#36825;&#38480;&#21046;&#20102;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;VQGraph&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#21464;&#20307;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#23427;&#23558;&#22810;&#26679;&#21270;&#30340;&#23616;&#37096;&#32467;&#26500;&#33410;&#28857;&#26126;&#30830;&#34920;&#31034;&#20026;&#22823;&#37327;&#31163;&#25955;&#20196;&#29260;&#65292;&#24182;&#26500;&#25104;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#20195;&#30721;&#20070;&#12290;&#37197;&#22791;&#20102;&#23398;&#20064;&#30340;&#20195;&#30721;&#20070;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;Hi-MPC&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#30828;&#39592;&#26550;&#25366;&#25496;&#65292;&#29992;&#20110;&#26080;&#26631;&#31614;3D&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;&#36890;&#36807;&#26500;&#24314;&#23618;&#27425;&#39592;&#26550;&#34920;&#31034;&#24182;&#21033;&#29992;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#26356;&#22810;&#20449;&#24687;&#20016;&#23500;&#30340;&#39592;&#26550;&#29305;&#24449;&#30340;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.12917</link><description>&lt;p&gt;
&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#19982;&#30828;&#39592;&#26550;&#25366;&#25496;&#30456;&#32467;&#21512;&#30340;&#26080;&#30417;&#30563;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification. (arXiv:2307.12917v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;Hi-MPC&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#30828;&#39592;&#26550;&#25366;&#25496;&#65292;&#29992;&#20110;&#26080;&#26631;&#31614;3D&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;&#36890;&#36807;&#26500;&#24314;&#23618;&#27425;&#39592;&#26550;&#34920;&#31034;&#24182;&#21033;&#29992;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#26356;&#22810;&#20449;&#24687;&#20016;&#23500;&#30340;&#39592;&#26550;&#29305;&#24449;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20174;&#36523;&#20307;&#20851;&#33410;&#23398;&#20064;&#21333;&#23618;&#27425;&#30340;&#39592;&#26550;&#29305;&#24449;&#65292;&#24182;&#20551;&#35774;&#39592;&#26550;&#30340;&#37325;&#35201;&#24615;&#30456;&#31561;&#65292;&#22240;&#27492;&#36890;&#24120;&#26080;&#27861;&#21033;&#29992;&#26356;&#22810;&#26469;&#33258;&#19981;&#21516;&#23618;&#27425;&#65288;&#22914;&#32930;&#20307;&#23618;&#27425;&#65289;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#39592;&#26550;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#26631;&#31614;&#20381;&#36182;&#24615;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23398;&#20064;&#26356;&#19968;&#33324;&#30340;&#39592;&#26550;&#34920;&#31034;&#26041;&#38754;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26080;&#30417;&#30563;&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;Hi-MPC&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#30828;&#39592;&#26550;&#25366;&#25496;&#65288;HSM&#65289;&#29992;&#20110;&#26080;&#26631;&#31614;3D&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#39592;&#26550;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#20197;&#27169;&#25311;&#36523;&#20307;&#20851;&#33410;&#12289;&#32452;&#20214;&#21644;&#32930;&#20307;&#23618;&#27425;&#30340;&#31895;&#21040;&#32454;&#30340;&#36523;&#20307;&#21644;&#21160;&#20316;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23618;&#27425;&#21270;&#30340;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid advancements in depth sensors and deep learning, skeleton-based person re-identification (re-ID) models have recently achieved remarkable progress with many advantages. Most existing solutions learn single-level skeleton features from body joints with the assumption of equal skeleton importance, while they typically lack the ability to exploit more informative skeleton features from various levels such as limb level with more global body patterns. The label dependency of these methods also limits their flexibility in learning more general skeleton representations. This paper proposes a generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning (Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with unlabeled 3D skeletons. Firstly, we construct hierarchical representations of skeletons to model coarse-to-fine body and motion features from the levels of body joints, components, and limbs. Then a hierarchical meta-prototype contrastive learni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#12290;&#20027;&#35201;&#20851;&#27880;&#30340;&#20004;&#20010;GAMs&#26159;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#25351;&#26631;&#33021;&#22815;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#35780;&#20272;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10112</link><description>&lt;p&gt;
&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Extended Graph Assessment Metrics for Graph Neural Networks. (arXiv:2307.10112v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#12290;&#20027;&#35201;&#20851;&#27880;&#30340;&#20004;&#20010;GAMs&#26159;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#25351;&#26631;&#33021;&#22815;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#35780;&#20272;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#24739;&#32773;&#38431;&#21015;&#37325;&#32452;&#20026;&#25152;&#35859;&#30340;&#20154;&#21475;&#22270;&#26102;&#65292;&#26368;&#21021;&#29420;&#31435;&#30340;&#25968;&#25454;&#28857;&#21487;&#20197;&#21512;&#24182;&#25104;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#22270;&#32467;&#26500;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#21487;&#20197;&#20351;&#29992;&#36825;&#31181;&#20154;&#21475;&#22270;&#36827;&#34892;&#21307;&#23398;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36866;&#21512;&#30340;&#22270;&#32467;&#26500;&#30340;&#26500;&#24314;&#26159;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27493;&#39588;&#65292;&#23427;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#22270;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#26631;&#20165;&#36866;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#21644;&#31163;&#25955;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#21482;&#35206;&#30422;&#20102;&#19968;&#23567;&#37096;&#20998;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#30340;&#25193;&#23637;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20004;&#20010;&#20855;&#20307;&#30340;GAMs&#65306;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#25105;&#20204;&#23558;GAMs&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#20010;&#36339;&#36291;&#65292;&#24182;&#20026;&#22238;&#24402;&#20219;&#21153;&#23450;&#20041;&#20102;&#21516;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When re-structuring patient cohorts into so-called population graphs, initially independent data points can be incorporated into one interconnected graph structure. This population graph can then be used for medical downstream tasks using graph neural networks (GNNs). The construction of a suitable graph structure is a challenging step in the learning pipeline that can have severe impact on model performance. To this end, different graph assessment metrics have been introduced to evaluate graph structures. However, these metrics are limited to classification tasks and discrete adjacency matrices, only covering a small subset of real-world applications. In this work, we introduce extended graph assessment metrics (GAMs) for regression tasks and continuous adjacency matrices. We focus on two GAMs in specific: \textit{homophily} and \textit{cross-class neighbourhood similarity} (CCNS). We extend the notion of GAMs to more than one hop, define homophily for regression tasks, as well as con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07686</link><description>&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#25903;&#25345;OpenMP Fortran&#21644;C++&#20195;&#30721;&#30456;&#20114;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#65288;CodeBLEU&#65289;&#21644;&#23450;&#24615;&#65288;&#20154;&#24037;&#35780;&#20272;&#65289;&#26041;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#32534;&#30721;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;5.1&#20493;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#23450;&#32534;&#30721;&#29087;&#24713;&#24230;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;9.9&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#20195;&#30721;&#32763;&#35793;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#32925;&#30828;&#21270;&#39044;&#27979;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#25972;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.04617</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#65306;&#32925;&#30828;&#21270;&#20998;&#31867;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#32925;&#30828;&#21270;&#39044;&#27979;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#25972;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#36890;&#36807;&#20302;&#32622;&#20449;&#24230;&#30340;&#24369;&#26631;&#31614;&#65288;&#20363;&#22914;&#25918;&#23556;&#23398;&#35780;&#20998;&#65289;&#36827;&#34892;&#24265;&#20215;&#24555;&#36895;&#30340;&#27880;&#37322;&#12290;&#32780;&#39640;&#32622;&#20449;&#24230;&#30340;&#26631;&#31614;&#65288;&#22914;&#22522;&#20110;&#32452;&#32455;&#23398;&#30340;&#35786;&#26029;&#65289;&#24456;&#23569;&#19988;&#26114;&#36149;&#12290;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36739;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65292;&#36825;&#22312;&#22823;&#22411;3D&#22270;&#20687;&#30340;&#20840;&#20998;&#36776;&#29575;&#24773;&#20917;&#19979;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;GPU&#20869;&#23384;&#26377;&#38480;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20851;&#20110;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20307;&#31215;&#20449;&#24687;&#23545;&#20110;&#26576;&#20123;&#21307;&#23398;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#65288;&#21363;&#25918;&#23556;&#23398;&#20302;&#32622;&#20449;&#24230;&#26631;&#27880;&#65289;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32925;&#30828;&#21270;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20998;&#31867;&#20307;&#31995;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#21015;&#20030;&#20102;&#19981;&#21516;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#26680;&#24515;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04370</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey. (arXiv:2307.04370v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20998;&#31867;&#20307;&#31995;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#21015;&#20030;&#20102;&#19981;&#21516;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#26680;&#24515;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#39550;&#39542;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23427;&#36991;&#20813;&#20102;&#27169;&#22359;&#21270;&#31995;&#32479;&#30340;&#32570;&#28857;&#65292;&#22914;&#22797;&#26434;&#24615;&#36807;&#39640;&#21644;&#35823;&#24046;&#20256;&#25773;&#30340;&#20542;&#21521;&#12290;&#33258;&#21160;&#39550;&#39542;&#36890;&#36807;&#39044;&#20808;&#20027;&#21160;&#35782;&#21035;&#20851;&#38190;&#20107;&#20214;&#26469;&#36229;&#36234;&#20256;&#32479;&#20132;&#36890;&#27169;&#24335;&#65292;&#30830;&#20445;&#20056;&#23458;&#30340;&#23433;&#20840;&#24182;&#20026;&#20182;&#20204;&#25552;&#20379;&#33298;&#36866;&#30340;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#21644;&#22810;&#21464;&#30340;&#20132;&#36890;&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#23545;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#31471;&#21040;&#31471;&#26041;&#24335;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20998;&#31867;&#20307;&#31995;&#65292;&#21253;&#25324;&#20174;&#24863;&#30693;&#21040;&#25511;&#21046;&#30340;&#25972;&#20010;&#39550;&#39542;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#26681;&#25454;&#22522;&#26412;&#21407;&#29702;&#12289;&#26041;&#27861;&#35770;&#21644;&#26680;&#24515;&#21151;&#33021;&#23545;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation. Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers' safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings. This paper presents a comprehensive review of the End-to-End autonomous driving stack. It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control, while addressing key challenges encountered in real-world applications. Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality. These categories encompass sensorial input, m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#32780;&#39640;&#25928;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;&#24515;&#30005;&#22270;(ECG)&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#27169;&#22411;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23454;&#29616;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#26032;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.06196</link><description>&lt;p&gt;
ElectroCardioGuard&#65306;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#38450;&#27490;&#24515;&#30005;&#22270;&#25968;&#25454;&#24211;&#20013;&#24739;&#32773;&#35823;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks. (arXiv:2306.06196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#32780;&#39640;&#25928;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;&#24515;&#30005;&#22270;(ECG)&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#27169;&#22411;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23454;&#29616;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#26032;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;(ECG)&#36890;&#24120;&#34987;&#24515;&#33039;&#30149;&#19987;&#23478;&#29992;&#20110;&#26816;&#27979;&#19982;&#24515;&#33039;&#30456;&#20851;&#30340;&#30149;&#29702;&#24773;&#20917;&#65292;&#32780;&#21487;&#38752;&#30340;ECG&#38598;&#21512;&#23545;&#20110;&#30830;&#35786;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#23558;&#35760;&#24405;&#30340;ECG&#20998;&#37197;&#32473;&#38169;&#35823;&#30340;&#24739;&#32773;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#21457;&#29983;&#12290;&#26412;&#25991;&#19982;&#19968;&#23478;&#20020;&#24202;&#21644;&#30740;&#31350;&#26426;&#26500;&#21512;&#20316;&#65292;&#35813;&#26426;&#26500;&#35748;&#35782;&#21040;&#36825;&#19968;&#25361;&#25112;&#24182;&#32852;&#31995;&#25105;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#24039;&#39640;&#25928;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;ECG&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#21033;&#29992;760&#20493;&#26356;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;PTB-XL&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#30011;&#24266;&#25506;&#38024;&#24739;&#32773;&#35782;&#21035;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#19968;&#20010;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#26032;&#25910;&#38598;&#30340;ECG&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiograms (ECGs) are commonly used by cardiologists to detect heart-related pathological conditions. Reliable collections of ECGs are crucial for precise diagnosis. However, in clinical practice, the assignment of captured ECG recordings to incorrect patients can occur inadvertently. In collaboration with a clinical and research facility which recognized this challenge and reached out to us, we present a study that addresses this issue. In this work, we propose a small and efficient neural-network based model for determining whether two ECGs originate from the same patient. Our model demonstrates great generalization capabilities and achieves state-of-the-art performance in gallery-probe patient identification on PTB-XL while utilizing 760x fewer parameters. Furthermore, we present a technique leveraging our model for detection of recording-assignment mistakes, showcasing its applicability in a realistic scenario. Finally, we evaluate our model on a newly collected ECG dataset
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03763</link><description>&lt;p&gt;
ChatGPT&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20174;&#26102;&#38388;&#25991;&#26412;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#36130;&#32463;&#26032;&#38395;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#32467;&#26500;&#30340;&#28508;&#21147;&#20173;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#22270;&#25512;&#26029;&#33021;&#21147;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#32467;&#26500;&#34701;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36827;&#34892;&#21518;&#32493;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#20135;&#20986;&#26500;&#24314;&#30340;&#32452;&#21512;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#24180;&#21270;&#32047;&#35745;&#22238;&#25253;&#12289;&#26356;&#20302;&#30340;&#27874;&#21160;&#24615;&#21644;&#26368;&#22823;&#22238;&#25764;&#12290;&#36825;&#31181;&#21331;&#36234;&#34920;&#29616;&#31361;&#26174;&#20102;ChatGPT&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
&lt;/p&gt;</description></item><item><title>ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;</title><link>http://arxiv.org/abs/2305.03513</link><description>&lt;p&gt;
ChatGraph: &#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03513
&lt;/p&gt;
&lt;p&gt;
ChatGraph&#36890;&#36807;&#23558;ChatGPT&#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#26368;&#36817;&#25512;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#38459;&#30861;&#20102;&#23427;&#30340;&#28508;&#22312;&#24212;&#29992;&#65306;&#65288;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#30340;&#19981;&#28789;&#27963;&#24615;&#21644;&#65288;2&#65289;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#26469;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#65292;&#21516;&#26102;&#25552;&#39640;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing Cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#32570;&#38519;&#25512;&#29702;&#30340;&#22810;&#20248;&#36873;&#35821;&#20041;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#20248;&#20808;&#35821;&#20041;&#65292;&#23545;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#36827;&#34892;&#20102;&#20248;&#20808;&#35299;&#37322;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26465;&#20214;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00304</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#20856;&#22411;&#24615;&#30340;&#26465;&#20214;&#36923;&#36753;&#20013;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#20248;&#20808;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A preferential interpretation of MultiLayer Perceptrons in a conditional logic with typicality. (arXiv:2305.00304v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#32570;&#38519;&#25512;&#29702;&#30340;&#22810;&#20248;&#36873;&#35821;&#20041;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#20248;&#20808;&#35821;&#20041;&#65292;&#23545;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#36827;&#34892;&#20102;&#20248;&#20808;&#35299;&#37322;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26465;&#20214;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#34920;&#31034;&#20013;&#32570;&#38519;&#25512;&#29702;&#30340;&#22810;&#20248;&#36873;&#35821;&#20041;&#19982;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32771;&#34385;&#20102;&#19968;&#31181;&#20855;&#26377;&#20856;&#22411;&#24615;&#30340;&#31616;&#21333;&#25551;&#36848;&#36923;&#36753;&#30340;&#21152;&#26435;&#30693;&#35782;&#24211;&#65292;&#22312;&#8220;&#27010;&#24565;&#23618;&#38754;&#8221;&#30340;&#22810;&#20248;&#20808;&#35821;&#20041;&#19979;&#36827;&#34892;&#12290;&#35813;&#35821;&#20041;&#34987;&#29992;&#26469;&#25552;&#20379;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#30340;&#20248;&#20808;&#35299;&#37322;&#12290;&#21033;&#29992;&#27169;&#22411;&#26816;&#26597;&#21644;&#34164;&#21547;&#20851;&#31995;&#30340;&#26041;&#27861;&#39564;&#35777;MLPs&#30340;&#26465;&#20214;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate the relationships between a multipreferential semantics for defeasible reasoning in knowledge representation and a multilayer neural network model. Weighted knowledge bases for a simple description logic with typicality are considered under a (many-valued) ``concept-wise" multipreference semantics. The semantics is used to provide a preferential interpretation of MultiLayer Perceptrons (MLPs). A model checking and an entailment based approach are exploited in the verification of conditional properties of MLPs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#24322;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#36755;&#20986;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#21512;&#35268;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02574</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21512;&#35268;&#24322;&#31574;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Conformal Off-Policy Evaluation in Markov Decision Processes. (arXiv:2304.02574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#24322;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#36755;&#20986;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#21512;&#35268;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#21644;&#35780;&#20272;&#26377;&#25928;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#32773;&#19981;&#33021;&#36827;&#34892;&#23454;&#39564;&#65292;&#20063;&#19981;&#33021;&#20197;&#22312;&#32447;&#26041;&#24335;&#33719;&#21462;&#25968;&#25454;&#65288;&#22312;&#23454;&#39564;&#36153;&#29992;&#39640;&#26114;&#12289;&#39118;&#38505;&#39640;&#25110;&#19981;&#36947;&#24503;&#30340;&#24773;&#20917;&#19979;&#65292;&#23601;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#65289;&#12290;&#38024;&#23545;&#36825;&#31181;&#24212;&#29992;&#65292;&#24517;&#39035;&#20351;&#29992;&#22312;&#19981;&#21516;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#21382;&#21490;&#25968;&#25454;&#65288;&#34892;&#20026;&#31574;&#30053;&#65289;&#26469;&#20272;&#35745;&#32473;&#23450;&#31574;&#30053;&#65288;&#30446;&#26631;&#31574;&#30053;&#65289;&#30340;&#22870;&#21169;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#36825;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21363;&#24322;&#31574;&#35780;&#20272;&#65288;OPE&#65289;&#65292;&#37117;&#27809;&#26377;&#20934;&#30830;&#24615;&#21644;&#30830;&#23450;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#26032;&#22411;OPE&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#19968;&#20010;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#21516;&#26102;&#20855;&#26377;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#12290;OPE&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#30446;&#26631;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;&#22788;&#29702;&#36825;&#31181;&#20559;&#31227;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#20272;&#35745;&#30340;&#22870;&#21169;&#21306;&#38388;&#30340;&#21512;&#35268;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;&#22522;&#20934;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning aims at identifying and evaluating efficient control policies from data. In many real-world applications, the learner is not allowed to experiment and cannot gather data in an online manner (this is the case when experimenting is expensive, risky or unethical). For such applications, the reward of a given policy (the target policy) must be estimated using historical data gathered under a different policy (the behavior policy). Most methods for this learning task, referred to as Off-Policy Evaluation (OPE), do not come with accuracy and certainty guarantees. We present a novel OPE method based on Conformal Prediction that outputs an interval containing the true reward of the target policy with a prescribed level of certainty. The main challenge in OPE stems from the distribution shift due to the discrepancies between the target and the behavior policies. We propose and empirically evaluate different ways to deal with this shift. Some of these methods yield conform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.06016</link><description>&lt;p&gt;
Probe&#65306;&#23398;&#20064;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#25414;&#32465;&#36873;&#25321;&#20013;&#30340;&#20010;&#24615;&#21270;&#25237;&#24433;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices. (arXiv:2303.06016v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#36328;&#24230;&#30340;&#36873;&#25321;&#38656;&#35201;&#26435;&#34913;&#29616;&#22312;&#30340;&#25104;&#26412;&#21644;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#20854;&#20013;&#19968;&#31181;&#20855;&#20307;&#30340;&#36873;&#25321;&#26159;&#20915;&#23450;&#36141;&#20080;&#21333;&#20010;&#29289;&#21697;&#36824;&#26159;&#36873;&#25321;&#21253;&#21547;&#35813;&#29289;&#21697;&#30340;&#25414;&#32465;&#38144;&#21806;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20551;&#35774;&#20010;&#20154;&#23545;&#36825;&#20123;&#36873;&#25321;&#20013;&#28041;&#21450;&#30340;&#22240;&#32032;&#26377;&#20934;&#30830;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#29992;&#25143;&#23545;&#36825;&#20123;&#22240;&#32032;&#30340;&#24863;&#30693;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#65292;&#23548;&#33268;&#20102;&#38750;&#29702;&#24615;&#21644;&#27425;&#20248;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#24120;&#35265;&#30340;&#20559;&#24046;&#65306;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21152;&#26435;&#20989;&#25968;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#25237;&#24433;&#20559;&#24046;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#26469;&#32771;&#34385;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#24341;&#20837;&#34892;&#20026;&#32463;&#27982;&#23398;&#20013;&#30340;&#21069;&#26223;&#29702;&#35770;&#26469;&#32452;&#21512;&#21152;&#26435;&#21644;&#20215;&#20540;&#20989;&#25968;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#29992;&#25143;&#36141;&#20080;&#25414;&#32465;&#38144;&#21806;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intertemporal choices involve making decisions that require weighing the costs in the present against the benefits in the future. One specific type of intertemporal choice is the decision between purchasing an individual item or opting for a bundle that includes that item. Previous research assumes that individuals have accurate expectations of the factors involved in these choices. However, in reality, users' perceptions of these factors are often biased, leading to irrational and suboptimal decision-making. In this work, we specifically focus on two commonly observed biases: projection bias and the reference-point effect. To address these biases, we propose a novel bias-embedded preference model called Probe. The Probe incorporates a weight function to capture users' projection bias and a value function to account for the reference-point effect, and introduce prospect theory from behavioral economics to combine the weight and value functions. This allows us to determine the probabili
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#21644;PDE&#29702;&#35770;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;3D&#22343;&#21248;&#26925;&#22278;&#22411;PDE&#35299;&#31639;&#31526;&#30340;&#25968;&#25454;&#39640;&#25928;&#24674;&#22797;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#20197;&#25351;&#25968;&#25910;&#25947;&#29575;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.12888</link><description>&lt;p&gt;
&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#26159;&#21487;&#38752;&#30340;
&lt;/p&gt;
&lt;p&gt;
Elliptic PDE learning is provably data-efficient. (arXiv:2302.12888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#21644;PDE&#29702;&#35770;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;3D&#22343;&#21248;&#26925;&#22278;&#22411;PDE&#35299;&#31639;&#31526;&#30340;&#25968;&#25454;&#39640;&#25928;&#24674;&#22797;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#20197;&#25351;&#25968;&#25910;&#25947;&#29575;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PDE&#23398;&#20064;&#26159;&#19968;&#20010;&#23558;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#24674;&#22797;&#26410;&#30693;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#26368;&#36817;&#30340;PDE&#23398;&#20064;&#25216;&#26415;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#20173;&#28982;&#26159;&#32463;&#39564;&#24615;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;PDE&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#23545;&#25152;&#38656;&#30340;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#23545;&#25968;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#21644;PDE&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#38752;&#25968;&#25454;&#25928;&#29575;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20174;&#36755;&#20837;-&#36755;&#20986;&#25968;&#25454;&#20013;&#24674;&#22797;3D&#22343;&#21248;&#26925;&#22278;&#22411;PDE&#30340;&#35299;&#31639;&#31526;&#65292;&#24182;&#20197;&#26497;&#39640;&#30340;&#25104;&#21151;&#27010;&#29575;&#23454;&#29616;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25351;&#25968;&#25910;&#25947;&#29575;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
PDE learning is an emerging field that combines physics and machine learning to recover unknown physical systems from experimental data. While deep learning models traditionally require copious amounts of training data, recent PDE learning techniques achieve spectacular results with limited data availability. Still, these results are empirical. Our work provides theoretical guarantees on the number of input-output training pairs required in PDE learning. Specifically, we exploit randomized numerical linear algebra and PDE theory to derive a provably data-efficient algorithm that recovers solution operators of 3D uniformly elliptic PDEs from input-output data and achieves an exponential convergence rate of the error with respect to the size of the training dataset with an exceptionally high probability of success.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#26032;&#20852;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#65292;&#20855;&#20307;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31639;&#27861;&#22312;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306;&#22320;&#34920;&#39118;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#29702;&#24819;&#21270;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#31354;&#38388;&#21151;&#29575;&#35889;&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.08720</link><description>&lt;p&gt;
&#36817;&#22320;&#34920;&#39118;&#30340;&#31639;&#27861;&#24187;&#35273;&#65306;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#32479;&#35745;&#38477;&#23610;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#26032;&#20852;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#65292;&#20855;&#20307;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#31639;&#27861;&#22312;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306;&#22320;&#34920;&#39118;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#29702;&#24819;&#21270;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#31354;&#38388;&#21151;&#29575;&#35889;&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#20013;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#32479;&#35745;&#38477;&#23610;&#24230;&#20219;&#21153;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12290;&#25105;&#20204;&#30340;GANs&#26159;&#36890;&#36807;&#23545;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#36755;&#20837;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#26469;&#29983;&#25104;&#27169;&#25311;&#21271;&#32654;&#22320;&#21306; Weather Research and Forecasting&#65288;WRF&#65289;&#27169;&#22411;&#30340;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22320;&#34920;&#39118;&#12290;&#19982;&#20256;&#32479;&#30340;SR&#27169;&#22411;&#19981;&#21516;&#65292;LR&#36755;&#20837;&#22312;WRF&#27169;&#25311;&#20013;&#20351;&#29992;&#20102;&#38750;&#29702;&#24819;&#21270;&#30340;LR&#21644;HR&#37197;&#23545;&#65292;&#23548;&#33268;&#30001;&#20110;&#20869;&#37096;&#21464;&#24322;&#24341;&#36215;&#30340;&#20849;&#20139;&#23610;&#24230;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#24403;&#21069;&#22522;&#20110;SR&#30340;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#24182;&#23581;&#35797;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26032;&#39062;&#39057;&#29575;&#20998;&#31163;&#65288;FS&#65289;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;SR&#27169;&#22411;&#30340;&#25216;&#33021;&#65292;&#25105;&#20204;&#31934;&#36873;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#20851;&#27880;&#22522;&#20110;&#31354;&#38388;&#21151;&#29575;&#35889;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;GAN&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of emerging machine learning methods from image super-resolution (SR) to the task of statistical downscaling. We specifically focus on convolutional neural network-based Generative Adversarial Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to generate high-resolution (HR) surface winds emulating Weather Research and Forecasting (WRF) model simulations over North America. Unlike traditional SR models, where LR inputs are idealized coarsened versions of the HR images, WRF emulation involves using non-idealized LR and HR pairs resulting in shared-scale mismatches due to internal variability. Our study builds upon current SR-based statistical downscaling by experimenting with a novel frequency-separation (FS) approach from the computer vision field. To assess the skill of SR models, we carefully select evaluation metrics, and focus on performance measures based on spatial power spectra. Our analyses reveal how GAN configurations 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25913;&#36827;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65292;&#36890;&#36807;&#35757;&#32451;RL&#26234;&#33021;&#20307;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21464;&#21270;&#65292;&#25552;&#39640;&#20102;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#22330;&#26223;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.08025</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#25512;&#24191;&#65306;&#25913;&#21892;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generalization through Diversity: Improving Unsupervised Environment Design. (arXiv:2301.08025v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25913;&#36827;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65292;&#36890;&#36807;&#35757;&#32451;RL&#26234;&#33021;&#20307;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21464;&#21270;&#65292;&#25552;&#39640;&#20102;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#22330;&#26223;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#26234;&#33021;&#20915;&#31574;&#20027;&#35201;&#20381;&#36182;&#20110;&#29615;&#22659;&#30340;&#27169;&#22411;&#25110;&#27169;&#25311;&#22120;&#65288;&#20363;&#22914;&#65292;&#22312;&#19968;&#20010;8x8&#22823;&#23567;&#30340;&#36855;&#23467;&#20013;&#31227;&#21160;&#65292;&#25110;&#32773;&#22312;&#19968;&#20010;8x8&#22823;&#23567;&#30340;&#26827;&#30424;&#19978;&#19979;&#26827;&#65289;&#12290;&#30001;&#20110;&#36825;&#31181;&#20381;&#36182;&#24615;&#65292;&#29615;&#22659;&#30340;&#24494;&#23567;&#21464;&#21270;&#65288;&#20363;&#22914;&#65292;&#36855;&#23467;&#20013;&#38556;&#30861;&#29289;&#30340;&#20301;&#32622;&#65292;&#26827;&#30424;&#30340;&#22823;&#23567;&#65289;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;&#26234;&#33021;&#20307;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#24050;&#26377;&#30740;&#31350;&#25552;&#20986;&#22312;&#33258;&#36866;&#24212;&#35838;&#31243;&#29615;&#22659;&#65288;&#33258;&#21160;&#29983;&#25104;&#65289;&#19978;&#35757;&#32451;RL&#26234;&#33021;&#20307;&#65292;&#20197;&#25913;&#21892;&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#27979;&#35797;&#22330;&#26223;&#19978;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#30740;&#31350;&#23558;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#28508;&#21147;&#65288;&#36890;&#36807;&#24191;&#20041;&#20248;&#21183;&#20272;&#35745;&#65292;GAE&#65289;&#20316;&#20026;&#36873;&#25321;&#19979;&#19968;&#20010;&#29615;&#22659;&#36827;&#34892;&#35757;&#32451;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26426;&#21046;&#21487;&#33021;&#36873;&#25321;&#30456;&#20284;&#30340;&#29615;&#22659;&#65288;&#20855;&#26377;&#39640;&#23398;&#20064;&#28508;&#21147;&#65289;&#65292;&#20174;&#32780;&#20351;&#26234;&#33021;&#20307;&#22312;&#38500;&#20854;&#20013;&#19968;&#20010;&#29615;&#22659;&#22806;&#30340;&#25152;&#26377;&#29615;&#22659;&#19978;&#30340;&#35757;&#32451;&#21464;&#24471;&#22810;&#20313;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Agent decision making using Reinforcement Learning (RL) heavily relies on either a model or simulator of the environment (e.g., moving in an 8x8 maze with three rooms, playing Chess on an 8x8 board). Due to this dependence, small changes in the environment (e.g., positions of obstacles in the maze, size of the board) can severely affect the effectiveness of the policy learned by the agent. To that end, existing work has proposed training RL agents on an adaptive curriculum of environments (generated automatically) to improve performance on out-of-distribution (OOD) test scenarios. Specifically, existing research has employed the potential for the agent to learn in an environment (captured using Generalized Advantage Estimation, GAE) as the key factor to select the next environment(s) to train the agent. However, such a mechanism can select similar environments (with a high potential to learn) thereby making agent training redundant on all but one of those environments. To that end, we 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#24418;&#25151;&#23627;&#20998;&#37197;&#38382;&#39064;&#65292;&#23558;&#20010;&#20307;&#25918;&#32622;&#22312;&#22270;&#30340;&#39030;&#28857;&#19978;&#65292;&#24320;&#21457;&#20102;&#20960;&#31181;&#32467;&#26500;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20010;&#20307;&#20043;&#38388;&#30340;&#23241;&#22930;&#31243;&#24230;&#20316;&#20026;&#20844;&#24179;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2301.01323</link><description>&lt;p&gt;
&#22270;&#24418;&#25151;&#23627;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Graphical House Allocation. (arXiv:2301.01323v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01323
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#24418;&#25151;&#23627;&#20998;&#37197;&#38382;&#39064;&#65292;&#23558;&#20010;&#20307;&#25918;&#32622;&#22312;&#22270;&#30340;&#39030;&#28857;&#19978;&#65292;&#24320;&#21457;&#20102;&#20960;&#31181;&#32467;&#26500;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20010;&#20307;&#20043;&#38388;&#30340;&#23241;&#22930;&#31243;&#24230;&#20316;&#20026;&#20844;&#24179;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#25151;&#23627;&#20998;&#37197;&#38382;&#39064;&#28041;&#21450;&#26681;&#25454;&#20010;&#20307;&#30340;&#20559;&#22909;&#20998;&#37197;n&#22871;&#25151;&#23376;&#65288;&#25110;&#29289;&#21697;&#65289;&#32473;n&#20010;&#20010;&#20307;&#12290;&#36825;&#31867;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20934;&#21017;&#26159;&#28385;&#36275;&#19968;&#20123;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#22914;&#26080;&#23241;&#22930;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#25512;&#24191;&#65292;&#21363;&#23558;&#20010;&#20307;&#25918;&#32622;&#22312;&#22270;&#30340;&#39030;&#28857;&#19978;&#65288;&#23545;&#24212;&#31038;&#20132;&#32593;&#32476;&#65289;&#65292;&#27599;&#20010;&#20010;&#20307;&#21482;&#33021;&#23545;&#20854;&#37051;&#23621;&#24863;&#21040;&#23241;&#22930;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#20010;&#20307;&#20043;&#38388;&#30340;&#24635;&#23241;&#22930;&#26368;&#23567;&#21270;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#30340;&#20844;&#24179;&#30446;&#26631;&#65292;&#21363;&#22312;&#31038;&#20132;&#22270;&#20013;&#25152;&#26377;&#36793;&#19978;&#30340;&#25152;&#26377;&#25104;&#23545;&#23241;&#22930;&#20540;&#30340;&#24635;&#21644;&#12290;&#24403;&#20010;&#20307;&#20855;&#26377;&#30456;&#21516;&#19988;&#22343;&#21248;&#20998;&#24067;&#30340;&#20272;&#20215;&#26102;&#65292;&#25105;&#20204;&#30340;&#38382;&#39064;&#24402;&#32467;&#20026;&#24050;&#30740;&#31350;&#30340;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#12290;&#23545;&#20110;&#21487;&#33021;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#30456;&#21516;&#20272;&#20215;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35774;&#32622;&#19982;&#36825;&#19968;&#32463;&#20856;&#38382;&#39064;&#30340;&#20960;&#20010;&#28145;&#20837;&#21644;&#20196;&#20154;&#24778;&#35766;&#30340;&#19981;&#21516;&#20043;&#22788;&#12290;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#25105;&#20204;&#20026;&#21508;&#31867;&#22270;&#30340;&#32467;&#26500;&#21644;&#35745;&#31639;&#32467;&#26524;&#20570;&#20986;&#20102;&#20960;&#39033;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
The classical house allocation problem involves assigning $n$ houses (or items) to $n$ agents according to their preferences. A key criterion in such problems is satisfying some fairness constraints such as envy-freeness. We consider a generalization of this problem wherein the agents are placed along the vertices of a graph (corresponding to a social network), and each agent can only experience envy towards its neighbors. Our goal is to minimize the aggregate envy among the agents as a natural fairness objective, i.e., the sum of all pairwise envy values over all edges in a social graph.  When agents have identical and evenly-spaced valuations, our problem reduces to the well-studied problem of linear arrangements. For identical valuations with possibly uneven spacing, we show a number of deep and surprising ways in which our setting is a departure from this classical problem. More broadly, we contribute several structural and computational results for various classes of graphs, inclu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#36171;&#33021;&#20110;&#20247;&#21253;&#20197;&#25552;&#39640;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20197;&#20154;&#24037;&#26234;&#33021;&#20026;&#39537;&#21160;&#30340;&#20247;&#21253;&#65288;AIEC&#65289;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#38480;&#21046;&#22240;&#32032;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2212.14676</link><description>&lt;p&gt;
&#36208;&#21521;&#20197;&#20154;&#24037;&#26234;&#33021;&#20026;&#39537;&#21160;&#30340;&#20247;&#21253;
&lt;/p&gt;
&lt;p&gt;
Towards AI-Empowered Crowdsourcing. (arXiv:2212.14676v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#36171;&#33021;&#20110;&#20247;&#21253;&#20197;&#25552;&#39640;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20197;&#20154;&#24037;&#26234;&#33021;&#20026;&#39537;&#21160;&#30340;&#20247;&#21253;&#65288;AIEC&#65289;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#38480;&#21046;&#22240;&#32032;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#26159;&#23558;&#20154;&#31867;&#26234;&#21147;&#21644;&#29983;&#20135;&#21147;&#21160;&#24577;&#32452;&#32455;&#36215;&#26469;&#22788;&#29702;&#33258;&#21160;&#21270;&#26080;&#27861;&#35299;&#20915;&#30340;&#22797;&#26434;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#23427;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#24182;&#28608;&#21457;&#20102;&#26032;&#30340;&#21830;&#19994;&#27169;&#24335;&#65288;&#22914;Uber&#12289;Airbnb&#65289;&#12290;&#22810;&#24180;&#26469;&#65292;&#20247;&#21253;&#19981;&#20877;&#20165;&#20165;&#26159;&#25552;&#20379;&#19968;&#20010;&#24179;&#21488;&#26469;&#25163;&#21160;&#21305;&#37197;&#24037;&#20316;&#32773;&#21644;&#20219;&#21153;&#65292;&#32780;&#26159;&#20511;&#21161;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#31649;&#29702;&#26041;&#27861;&#65292;&#23454;&#29616;&#26085;&#30410;&#22797;&#26434;&#30340;&#20248;&#21270;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;&#65292;&#23545;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#36171;&#33021;&#20110;&#20247;&#21253;&#20197;&#25552;&#39640;&#25928;&#29575;&#36827;&#34892;&#20102;&#31995;&#32479;&#27010;&#36848;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#20197;&#20154;&#24037;&#26234;&#33021;&#20026;&#39537;&#21160;&#30340;&#20247;&#21253;&#65288;AIEC&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;AIEC&#21010;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#39046;&#22495;&#30340;&#20998;&#31867;&#27861;&#65306;1&#65289;&#20219;&#21153;&#22996;&#25176;&#65292;2&#65289;&#28608;&#21169;&#24037;&#20316;&#32773;&#65292;3&#65289;&#36136;&#37327;&#25511;&#21046;&#65292;&#37325;&#28857;&#20851;&#27880;&#38656;&#35201;&#23454;&#29616;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#38480;&#21046;&#22240;&#32032;&#21644;&#27934;&#35265;&#65292;&#24182;&#26803;&#29702;&#20102;&#36827;&#34892;&#30740;&#31350;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowdsourcing, in which human intelligence and productivity is dynamically mobilized to tackle tasks too complex for automation alone to handle, has grown to be an important research topic and inspired new businesses (e.g., Uber, Airbnb). Over the years, crowdsourcing has morphed from providing a platform where workers and tasks can be matched up manually into one which leverages data-driven algorithmic management approaches powered by artificial intelligence (AI) to achieve increasingly sophisticated optimization objectives. In this paper, we provide a survey presenting a unique systematic overview on how AI can empower crowdsourcing to improve its efficiency - which we refer to as AI-Empowered Crowdsourcing(AIEC). We propose a taxonomy which divides AIEC into three major areas: 1) task delegation, 2) motivating workers, and 3) quality control, focusing on the major objectives which need to be accomplished. We discuss the limitations and insights, and curate the challenges of doing re
&lt;/p&gt;</description></item><item><title>RouteNet-Fermi&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#23450;&#20041;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#30340;&#25490;&#38431;&#29702;&#35770;&#30456;&#27604;&#65292;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.12070</link><description>&lt;p&gt;
RouteNet-Fermi: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32593;&#32476;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
RouteNet-Fermi: Network Modeling with Graph Neural Networks. (arXiv:2212.12070v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12070
&lt;/p&gt;
&lt;p&gt;
RouteNet-Fermi&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#23450;&#20041;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#30340;&#25490;&#38431;&#29702;&#35770;&#30456;&#27604;&#65292;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27169;&#22411;&#26159;&#29616;&#20195;&#32593;&#32476;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24191;&#27867;&#29992;&#20110;&#32593;&#32476;&#35268;&#21010;&#21644;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#19968;&#20123;&#27169;&#22411;&#23384;&#22312;&#38480;&#21046;&#65292;&#22914;&#25490;&#38431;&#29702;&#35770;&#27169;&#22411;&#20013;&#23545;&#39532;&#23572;&#21487;&#22827;&#27969;&#37327;&#30340;&#20551;&#35774;&#65292;&#20197;&#21450;&#32593;&#32476;&#27169;&#25311;&#22120;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#27491;&#22312;&#25512;&#21160;&#19968;&#20195;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RouteNet-Fermi&#30340;&#33258;&#23450;&#20041;GNN&#27169;&#22411;&#65292;&#23427;&#19982;&#25490;&#38431;&#29702;&#35770;&#20855;&#26377;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#32593;&#32476;&#35268;&#27169;&#65288;&#26368;&#22823;&#36798;&#21040;300&#20010;&#33410;&#28857;&#65289;&#21644;&#21253;&#25324;&#20855;&#26377;&#28151;&#21512;&#27969;&#37327;&#29305;&#24615;&#30340;&#26679;&#26412;&#65288;&#22914;&#22797;&#26434;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65289;&#20013;&#27979;&#35797;&#20102;RouteNet-Fermi&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models present limitations, such as the assumption of Markovian traffic in queuing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks (GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviors. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goals as Queuing Theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles -- e.g., with complex non-Markovian models -- and arbitrary routin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#30340;&#27010;&#24565;&#28418;&#31227;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20840;&#23616;&#32423;&#21035;&#24341;&#20837;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#25110;&#35843;&#25972;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27010;&#24565;&#28418;&#31227;&#30340;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.12989</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#27010;&#24565;&#28418;&#31227;&#21462;&#28040;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Unlearning of Concept Drift with Autoencoders. (arXiv:2211.12989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#30340;&#27010;&#24565;&#28418;&#31227;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20840;&#23616;&#32423;&#21035;&#24341;&#20837;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#25110;&#35843;&#25972;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27010;&#24565;&#28418;&#31227;&#30340;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#28418;&#31227;&#26159;&#25351;&#24433;&#21709;&#26410;&#26469;&#26679;&#26412;&#25968;&#25454;&#27969;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#23545;&#25968;&#25454;&#27969;&#36827;&#34892;&#25805;&#20316;&#30340;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#21464;&#24471;&#36807;&#26102;&#65292;&#38656;&#35201;&#26114;&#36149;&#19988;&#22256;&#38590;&#30340;&#35843;&#25972;&#65292;&#22914;&#37325;&#26032;&#35757;&#32451;&#25110;&#36866;&#24212;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23454;&#26045;&#23616;&#37096;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;&#26041;&#26696;&#65292;&#20854;&#20013;&#35201;&#20040;&#20351;&#29992;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#65292;&#35201;&#20040;&#22312;&#28418;&#31227;&#26816;&#27979;&#26426;&#21046;&#35302;&#21457;&#35686;&#25253;&#26102;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#20840;&#23616;&#32423;&#21035;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#8220;&#21462;&#28040;&#23398;&#20064;&#8221;&#27010;&#24565;&#28418;&#31227;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#35843;&#25972;&#20219;&#20309;&#22312;&#25968;&#25454;&#19978;&#25805;&#20316;&#30340;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#20004;&#20010;&#24212;&#29992;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;30&#20010;&#20197;&#19978;&#27169;&#22411;&#30340;&#30495;&#23454;&#27700;&#37197;&#36865;&#32593;&#32476;&#65292;&#20854;&#20013;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Concept drift refers to a change in the data distribution affecting the data stream of future samples. Consequently, learning models operating on the data stream might become obsolete, and need costly and difficult adjustments such as retraining or adaptation. Existing methods usually implement a local concept drift adaptation scheme, where either incremental learning of the models is used, or the models are completely retrained when a drift detection mechanism triggers an alarm. This paper proposes an alternative approach in which an unsupervised and model-agnostic concept drift adaptation method at the global level is introduced, based on autoencoders. Specifically, the proposed method aims to ``unlearn'' the concept drift without having to retrain or adapt any of the learning models operating on the data. An extensive experimental evaluation is conducted in two application domains. We consider a realistic water distribution network with more than 30 models in-place, from which we cr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#20026;&#20160;&#20040;&#22312;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21028;&#26029;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#24403;&#20154;&#20204;&#35748;&#20026;&#26426;&#22120;&#20855;&#26377;&#26356;&#22810;&#20195;&#29702;&#33021;&#21147;&#26102;&#65292;&#20182;&#20204;&#23545;&#26426;&#22120;&#30340;&#35780;&#21028;&#26356;&#20687;&#23545;&#24453;&#20154;&#31867;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2210.10081</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#20154;&#20204;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21028;&#26029;&#19981;&#21516;&#65306;&#24863;&#30693;&#20195;&#29702;&#21644;&#32463;&#39564;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Why people judge humans differently from machines: The role of perceived agency and experience. (arXiv:2210.10081v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#20026;&#20160;&#20040;&#22312;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21028;&#26029;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#24403;&#20154;&#20204;&#35748;&#20026;&#26426;&#22120;&#20855;&#26377;&#26356;&#22810;&#20195;&#29702;&#33021;&#21147;&#26102;&#65292;&#20182;&#20204;&#23545;&#26426;&#22120;&#30340;&#35780;&#21028;&#26356;&#20687;&#23545;&#24453;&#20154;&#31867;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24050;&#30693;&#20351;&#29992;&#21151;&#21033;&#20027;&#20041;&#30340;&#36947;&#24503;&#21746;&#23398;&#26469;&#35780;&#21028;&#20154;&#24037;&#26234;&#33021;&#65292;&#32780;&#20351;&#29992;&#24378;&#35843;&#34987;&#24863;&#30693;&#24847;&#22270;&#30340;&#36947;&#24503;&#21746;&#23398;&#26469;&#35780;&#21028;&#20154;&#31867;&#12290;&#20294;&#20026;&#20160;&#20040;&#20154;&#20204;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21028;&#26029;&#19981;&#21516;&#21602;&#65311;&#24515;&#29702;&#23398;&#35748;&#20026;&#20154;&#20204;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#24515;&#29702;&#24863;&#30693;&#27169;&#22411;&#65292;&#22240;&#27492;&#65292;&#20182;&#20204;&#20250;&#25226;&#31867;&#20154;&#26426;&#22120;&#26356;&#20687;&#23545;&#24453;&#20154;&#31867;&#30340;&#26041;&#24335;&#23545;&#24453;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#38543;&#26426;&#23454;&#39564;&#26469;&#25506;&#32034;&#20154;&#20204;&#22914;&#20309;&#21028;&#26029;&#37027;&#20123;&#22312;&#20195;&#29702;&#65288;&#20363;&#22914;&#65292;&#35268;&#21010;&#21644;&#34892;&#21160;&#33021;&#21147;&#65289;&#21644;&#32463;&#39564;&#65288;&#20363;&#22914;&#65292;&#24863;&#30693;&#33021;&#21147;&#65289;&#19978;&#19982;&#20154;&#31867;&#26356;&#30456;&#20284;&#30340;&#26426;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#20154;&#20204;&#24863;&#30693;&#26426;&#22120;&#20855;&#26377;&#26356;&#22810;&#20195;&#29702;&#33021;&#21147;&#26102;&#65292;&#20182;&#20204;&#23545;&#26426;&#22120;&#30340;&#35780;&#21028;&#26356;&#25509;&#36817;&#20110;&#23545;&#20154;&#31867;&#30340;&#35780;&#21028;&#65292;&#20294;&#24182;&#19981;&#21463;&#32463;&#39564;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#20204;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#20351;&#29992;&#19981;&#21516;&#30340;&#36947;&#24503;&#21746;&#23398;&#36827;&#34892;&#21028;&#26029;&#21487;&#20197;&#24471;&#21040;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
People are known to judge artificial intelligence using a utilitarian moral philosophy and humans using a moral philosophy emphasizing perceived intentions. But why do people judge humans and machines differently? Psychology suggests that people may have different mind perception models of humans and machines, and thus, will treat human-like robots more similarly to the way they treat humans. Here we present a randomized experiment where we manipulated people's perception of machine agency (e.g., ability to plan, act) and experience (e.g., ability to feel) to explore whether people judge machines that are perceived to be more similar to humans along these two dimensions more similarly to the way they judge humans. We find that people's judgments of machines become more similar to that of humans when they perceive machines as having more agency but not more experience. Our findings indicate that people's use of different moral philosophies to judge humans and machines can be explained b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;A*Net&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.04798</link><description>&lt;p&gt;
A*Net&#65306;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs. (arXiv:2206.04798v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;A*Net&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#23545;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#29702;&#19968;&#30452;&#30001;&#23884;&#20837;&#26041;&#27861;&#20027;&#23548;&#12290;&#34429;&#28982;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#20855;&#26377;&#23884;&#20837;&#26041;&#27861;&#25152;&#32570;&#20047;&#30340;&#24402;&#32435;&#33021;&#21147;&#65292;&#20294;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#25351;&#25968;&#32423;&#36335;&#24452;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#36335;&#24452;&#26041;&#27861;&#12290;&#21463;&#21040;A*&#31639;&#27861;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;A*Net&#23398;&#20064;&#20102;&#19968;&#20010;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#36793;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#36873;&#25321;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#27604;&#20363;&#21487;&#20197;&#25351;&#23450;&#65292;&#20197;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#20256;&#23548;&#24615;&#21644;&#24402;&#32435;&#24615;&#30693;&#35782;&#22270;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;A*Net&#22312;&#20165;&#35775;&#38382;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;10%&#33410;&#28857;&#21644;10%&#36793;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#22522;&#20110;&#36335;&#24452;&#26041;&#27861;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#30334;&#19975;&#32423;&#25968;&#25454;&#38598;ogbl-wikikg2&#19978;&#65292;A*Net&#19981;&#20165;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#36824;&#23454;&#29616;&#20102;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A*Net not only achieves a new state-of-the-art result, but also converges 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2205.14704</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#35760;&#24518;&#20013;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#36981;&#24490;&#21442;&#25968;&#21270;&#23398;&#20064;&#33539;&#24335;&#65307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#36951;&#24536;&#21644;&#26426;&#26800;&#35760;&#24518;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RetroPrompt&#65292;&#26088;&#22312;&#20174;&#35760;&#24518;&#20013;&#23558;&#30693;&#35782;&#35299;&#32806;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#27867;&#21270;&#21644;&#35760;&#24518;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#19982;&#20256;&#32479;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;RetroPrompt&#20174;&#35757;&#32451;&#23454;&#20363;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#30693;&#35782;&#24211;&#65292;&#24182;&#22312;&#36755;&#20837;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#26045;&#26816;&#32034;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#29992;&#20110;&#22686;&#24378;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;RetroPrompt&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21508;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#27604;&#36739;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2205.04766</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainable Deep Learning Methods in Medical Image Classification: A Survey. (arXiv:2205.04766v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04766
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21508;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#27604;&#36739;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20854;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#24456;&#38590;&#22312;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#24471;&#21040;&#37319;&#29992;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#38656;&#27714;&#65292;&#36827;&#32780;&#24418;&#25104;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;XAI&#22312;&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#25991;&#26412;&#12289;&#22522;&#20110;&#31034;&#20363;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#30340;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#19968;&#32452;&#22522;&#20110;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box-ness of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation-based methods. Finally, the major 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26681;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#35760;&#24518;&#20449;&#24687;&#25512;&#26029;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2205.02355</link><description>&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#20316;&#20026;&#24320;&#20070;&#32771;&#35797;&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02355
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26681;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#35760;&#24518;&#20449;&#24687;&#25512;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#20851;&#31995;&#25277;&#21462;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#26080;&#27861;&#25512;&#24191;&#21040;&#37027;&#20123;&#32597;&#35265;&#25110;&#22256;&#38590;&#30340;&#27169;&#24335;&#20013;&#12290;&#25105;&#20204;&#23558;&#20851;&#31995;&#25277;&#21462;&#35270;&#20026;&#19968;&#31181;&#24320;&#25918;&#24335;&#32771;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#35843;&#20248;&#30340;&#21322;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#24335;&#23384;&#20648;&#24211;&#65292;&#29992;&#20110;&#26816;&#32034;&#22522;&#20110;&#25552;&#31034;&#30340;&#23454;&#20363;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#20851;&#31995;&#26631;&#31614;&#20316;&#20026;&#35760;&#24518;&#30340;&#38190;&#20540;&#23545;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#22522;&#20110;PLM&#30340;&#22522;&#26412;&#36755;&#20986;&#19982;&#23384;&#20648;&#24211;&#19978;&#30340;&#38750;&#21442;&#25968;&#26368;&#36817;&#37051;&#20998;&#24067;&#26469;&#25512;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#26377;&#25928;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.04392</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#26377;&#25928;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#25110;&#28436;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#25628;&#32034;&#31163;&#25955;&#25110;&#36830;&#32493;&#25552;&#31034;&#25110;&#20248;&#21270;&#35821;&#35328;&#34920;&#36798;&#32773;&#65292;&#20294;&#23545;&#20110;&#28436;&#31034;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#28436;&#31034;&#31034;&#20363;&#23545;&#20110;&#26368;&#32456;&#30340;&#25552;&#31034;&#35843;&#20248;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25554;&#25300;&#12289;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#27604;&#28436;&#31034;&#35843;&#20248;&#65292;&#23427;&#19981;&#38656;&#35201;&#36827;&#34892;&#28436;&#31034;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#65306;&#65288;i&#65289;&#23884;&#20837;&#21040;&#20219;&#20309;&#20808;&#21069;&#30340;&#25552;&#31034;&#35843;&#20248;&#26041;&#27861;&#20013;&#65307;&#65288;ii&#65289;&#25193;&#23637;&#21040;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#24191;&#27867;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#22312;16&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20808;&#21069;&#30340;LM-BFF&#21644;P-tuning&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited. Concretely, the demonstration examples are crucial for an excellent final performance of prompt-tuning. In this paper, we propose a novel pluggable, extensible, and efficient approach named contrastive demonstration tuning, which is free of demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged into any previous prompt-tuning approaches; (ii) Extended to widespread classification tasks with a large number of categories. Experimental results on 16 datasets illustrate that our method integrated with previous approaches LM-BFF and P-tuning can yield better performance. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#27169;&#22411;&#26469;&#30740;&#31350;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#25112;&#30053;&#20114;&#21160;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#23548;&#33268;&#31639;&#27861;&#21246;&#32467;&#20986;&#29616;&#30340;&#26426;&#21046;&#12290;&#36890;&#36807;&#33258;&#21457;&#32806;&#21512;&#65292;&#31639;&#27861;&#21608;&#26399;&#24615;&#22320;&#21327;&#35843;&#34892;&#21160;&#65292;&#36798;&#21040;&#26356;&#39640;&#21033;&#28070;&#12290;&#35813;&#27169;&#22411;&#30340;&#21442;&#25968;&#21487;&#39044;&#27979;&#32479;&#35745;&#20851;&#32852;&#30340;&#20986;&#29616;&#21644;&#26377;&#21033;&#20110;&#31639;&#27861;&#21246;&#32467;&#30340;&#24066;&#22330;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#33258;&#21457;&#32806;&#21512;&#22914;&#20309;&#22312;&#20215;&#26684;&#21644;&#24066;&#22330;&#20221;&#39069;&#19978;&#32500;&#25345;&#21246;&#32467;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#31639;&#27861;&#24066;&#22330;&#12290;</title><link>http://arxiv.org/abs/2202.05946</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#32479;&#35745;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and Statistical Collusion. (arXiv:2202.05946v4 [econ.TH] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#27169;&#22411;&#26469;&#30740;&#31350;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#25112;&#30053;&#20114;&#21160;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#23548;&#33268;&#31639;&#27861;&#21246;&#32467;&#20986;&#29616;&#30340;&#26426;&#21046;&#12290;&#36890;&#36807;&#33258;&#21457;&#32806;&#21512;&#65292;&#31639;&#27861;&#21608;&#26399;&#24615;&#22320;&#21327;&#35843;&#34892;&#21160;&#65292;&#36798;&#21040;&#26356;&#39640;&#21033;&#28070;&#12290;&#35813;&#27169;&#22411;&#30340;&#21442;&#25968;&#21487;&#39044;&#27979;&#32479;&#35745;&#20851;&#32852;&#30340;&#20986;&#29616;&#21644;&#26377;&#21033;&#20110;&#31639;&#27861;&#21246;&#32467;&#30340;&#24066;&#22330;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#33258;&#21457;&#32806;&#21512;&#22914;&#20309;&#22312;&#20215;&#26684;&#21644;&#24066;&#22330;&#20221;&#39069;&#19978;&#32500;&#25345;&#21246;&#32467;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#31639;&#27861;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#27169;&#22411;&#26469;&#30740;&#31350;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#25112;&#30053;&#20114;&#21160;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#23548;&#33268;&#31639;&#27861;&#21246;&#32467;&#20986;&#29616;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#31639;&#27861;&#21608;&#26399;&#24615;&#22320;&#21327;&#35843;&#34892;&#21160;&#65292;&#36825;&#20123;&#34892;&#21160;&#27604;&#38745;&#24577;&#32435;&#20160;&#22343;&#34913;&#26356;&#20855;&#21033;&#28070;&#24615;&#12290;&#36825;&#31181;&#26032;&#30340;&#21246;&#32467;&#28192;&#36947;&#20381;&#36182;&#20110;&#31639;&#27861;&#20272;&#35745;&#20013;&#30340;&#20869;&#29983;&#32479;&#35745;&#20851;&#32852;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#33258;&#21457;&#32806;&#21512;&#12290;&#27169;&#22411;&#30340;&#21442;&#25968;&#39044;&#27979;&#20102;&#32479;&#35745;&#20851;&#32852;&#26159;&#21542;&#20250;&#20986;&#29616;&#65292;&#20197;&#21450;&#20160;&#20040;&#24066;&#22330;&#32467;&#26500;&#26377;&#21161;&#20110;&#31639;&#27861;&#21246;&#32467;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21457;&#32806;&#21512;&#22914;&#20309;&#32500;&#25345;&#20215;&#26684;&#21644;&#24066;&#22330;&#20221;&#39069;&#19978;&#30340;&#21246;&#32467;&#65292;&#36825;&#19982;&#25991;&#29486;&#20013;&#30340;&#23454;&#39564;&#35777;&#25454;&#30456;&#34917;&#20805;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#24212;&#29992;&#20110;&#35774;&#35745;&#31639;&#27861;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a tractable model for studying strategic interactions between learning algorithms. We uncover a mechanism responsible for the emergence of algorithmic collusion. We observe that algorithms periodically coordinate on actions that are more profitable than static Nash equilibria. This novel collusive channel relies on an endogenous statistical linkage in the algorithms' estimates which we call spontaneous coupling. The model's parameters predict whether the statistical linkage will appear, and what market structures facilitate algorithmic collusion. We show that spontaneous coupling can sustain collusion in prices and market shares, complementing experimental findings in the literature. Finally, we apply our results to design algorithmic markets.
&lt;/p&gt;</description></item><item><title>D-HAN&#26159;&#19968;&#31181;&#21160;&#24577;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#65292;&#37319;&#29992;&#20998;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;&#23558;&#36830;&#32493;&#26102;&#38388;&#20449;&#24687;&#26080;&#32541;&#25972;&#21512;&#65292;&#26377;&#25928;&#34920;&#31034;&#26032;&#38395;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#21160;&#24577;&#36127;&#37319;&#26679;&#26041;&#27861;&#20248;&#21270;&#29992;&#25143;&#30340;&#38544;&#24335;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2112.10085</link><description>&lt;p&gt;
D-HAN: &#21160;&#24577;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#19982;&#20998;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
D-HAN: Dynamic News Recommendation with Hierarchical Attention Network. (arXiv:2112.10085v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10085
&lt;/p&gt;
&lt;p&gt;
D-HAN&#26159;&#19968;&#31181;&#21160;&#24577;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#65292;&#37319;&#29992;&#20998;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;&#23558;&#36830;&#32493;&#26102;&#38388;&#20449;&#24687;&#26080;&#32541;&#25972;&#21512;&#65292;&#26377;&#25928;&#34920;&#31034;&#26032;&#38395;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#21160;&#24577;&#36127;&#37319;&#26679;&#26041;&#27861;&#20248;&#21270;&#29992;&#25143;&#30340;&#38544;&#24335;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38745;&#24577;&#30340;&#29992;&#25143;&#26032;&#38395;&#20132;&#20114;&#26041;&#24335;&#65292;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#23558;&#36830;&#32493;&#26102;&#38388;&#20449;&#24687;&#25972;&#21512;&#21040;&#20998;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#21477;&#23376;&#12289;&#20803;&#32032;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#26032;&#38395;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#36127;&#37319;&#26679;&#26041;&#27861;&#26469;&#20248;&#21270;&#29992;&#25143;&#30340;&#38544;&#24335;&#21453;&#39304;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommendation models often fall short in capturing users' preferences due to their static approach to user-news interactions. To address this limitation, we present a novel dynamic news recommender model that seamlessly integrates continuous time information to a hierarchical attention network that effectively represents news information at the sentence, element, and sequence levels. Moreover, we introduce a dynamic negative sampling method to optimize users' implicit feedback. To validate our model's effectiveness, we conduct extensive experiments on three real-world datasets. The results demonstrate the effectiveness of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#31354;&#38388;&#21644;&#35889;&#22495;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#32039;&#23494;&#20851;&#32852;&#21508;&#33258;&#22495;&#20869;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2107.10234</link><description>&lt;p&gt;
&#36328;&#36234;&#31354;&#38388;&#21644;&#20809;&#35889;&#22495;&#30340;&#40511;&#27807;&#65306;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks. (arXiv:2107.10234v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.10234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#31354;&#38388;&#21644;&#35889;&#22495;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#32039;&#23494;&#20851;&#32852;&#21508;&#33258;&#22495;&#20869;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26088;&#22312;&#22788;&#29702;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;GNN&#26159;&#20351;&#29992;&#19981;&#21516;&#30340;&#29702;&#35770;&#21019;&#24314;&#30340;&#65292;&#22240;&#27492;&#26080;&#27861;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#23427;&#20204;&#30340;&#20869;&#22312;&#36830;&#25509;&#20851;&#31995;&#20851;&#27880;&#29978;&#23569;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#24314;&#31435;&#19968;&#20010;&#22522;&#20110;&#35889;&#22270;&#21644;&#36817;&#20284;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#38598;&#25104;&#22522;&#20110;&#31354;&#38388;&#21644;&#35889;&#22495;&#30340;GNN&#65292;&#24182;&#32039;&#23494;&#20851;&#32852;&#21508;&#33258;&#22495;&#20869;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning's performance has been extensively recognized recently. Graph neural networks (GNNs) are designed to deal with graph-structural data that classical deep learning does not easily manage. Since most GNNs were created using distinct theories, direct comparisons are impossible. Prior research has primarily concentrated on categorizing existing models, with little attention paid to their intrinsic connections. The purpose of this study is to establish a unified framework that integrates GNNs based on spectral graph and approximation theory. The framework incorporates a strong integration between spatial- and spectral-based GNNs while tightly associating approaches that exist within each respective domain.
&lt;/p&gt;</description></item></channel></rss>