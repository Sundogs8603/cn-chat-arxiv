<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;WavJourney&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#38899;&#39057;&#20869;&#23481;&#30340;&#21019;&#20316;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#21253;&#21547;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#30340;&#38899;&#39057;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.14335</link><description>&lt;p&gt;
WavJourney&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
WavJourney: Compositional Audio Creation with Large Language Models. (arXiv:2307.14335v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;WavJourney&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#38899;&#39057;&#20869;&#23481;&#30340;&#21019;&#20316;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#21253;&#21547;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#30340;&#38899;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25972;&#21512;&#19981;&#21516;&#30340;&#19987;&#23478;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#39046;&#22495;&#30340;&#21457;&#23637;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#23427;&#20204;&#22312;&#26234;&#33021;&#38899;&#39057;&#20869;&#23481;&#21019;&#20316;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#26410;&#34987;&#21457;&#25496;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#25991;&#26412;&#25351;&#20196;&#21019;&#24314;&#28085;&#30422;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#30340;&#38899;&#39057;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WavJourney&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#21508;&#31181;&#38899;&#39057;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20869;&#23481;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#21548;&#35273;&#22330;&#26223;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;WavJourney&#39318;&#20808;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#20110;&#38899;&#39057;&#21465;&#20107;&#30340;&#32467;&#26500;&#33050;&#26412;&#12290;&#38899;&#39057;&#33050;&#26412;&#21253;&#21547;&#20102;&#19981;&#21516;&#30340;&#38899;&#39057;&#20803;&#32032;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#26102;&#31354;&#20851;&#31995;&#36827;&#34892;&#32452;&#32455;&#12290;&#20316;&#20026;&#38899;&#39057;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#38899;&#39057;&#33050;&#26412;&#20026;&#20154;&#31867;&#21442;&#19982;&#25552;&#20379;&#20102;&#20114;&#21160;&#21644;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#38543;&#21518;&#65292;&#38899;&#39057;&#33050;&#26412;&#34987;&#20256;&#36882;&#32473;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#38899;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown great promise in integrating diverse expert models to tackle intricate language and vision tasks. Despite their significance in advancing the field of Artificial Intelligence Generated Content (AIGC), their potential in intelligent audio content creation remains unexplored. In this work, we tackle the problem of creating audio content with storylines encompassing speech, music, and sound effects, guided by text instructions. We present WavJourney, a system that leverages LLMs to connect various audio models for audio content generation. Given a text description of an auditory scene, WavJourney first prompts LLMs to generate a structured script dedicated to audio storytelling. The audio script incorporates diverse audio elements, organized based on their spatio-temporal relationships. As a conceptual representation of audio, the audio script provides an interactive and interpretable rationale for human engagement. Afterward, the audio script is fe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20107;&#20214;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;Transformer&#32593;&#32476;&#25552;&#21069;&#39044;&#27979;&#25805;&#32437;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14332</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#35270;&#35273;&#30340;&#26089;&#26399;&#25805;&#32437;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Event-based Vision for Early Prediction of Manipulation Actions. (arXiv:2307.14332v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20107;&#20214;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;Transformer&#32593;&#32476;&#25552;&#21069;&#39044;&#27979;&#25805;&#32437;&#21160;&#20316;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#20256;&#24863;&#22120;&#26159;&#20154;&#24037;&#35270;&#32593;&#33180;&#65292;&#24403;&#22330;&#26223;&#20013;&#21457;&#29983;&#20142;&#24230;&#21464;&#21270;&#26102;&#36755;&#20986;&#24322;&#27493;&#20107;&#20214;&#24207;&#21015;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#20855;&#26377;&#24456;&#22810;&#20248;&#21183;&#65292;&#21253;&#25324;&#38750;&#24120;&#39640;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#27809;&#26377;&#36816;&#21160;&#27169;&#31946;&#21644;&#26234;&#33021;&#25968;&#25454;&#21387;&#32553;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#22788;&#29702;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20851;&#20110;&#32454;&#31890;&#24230;&#25805;&#32437;&#21160;&#20316;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20351;&#29992;Transformer&#36827;&#34892;&#20107;&#20214;&#21160;&#20316;&#39044;&#27979;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;&#22312;&#35748;&#30693;&#26426;&#22120;&#20154;&#21644;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#65292;&#23545;&#29702;&#35299;&#21644;&#39044;&#27979;&#20154;&#31867;&#21160;&#20316;&#30340;&#20852;&#36259;&#24456;&#22823;&#65292;&#23613;&#26089;&#39044;&#27979;&#33021;&#22815;&#35753;&#25105;&#20204;&#39044;&#27979;&#35268;&#21010;&#22797;&#26434;&#38454;&#27573;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#23454;&#26102;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;Transformer&#32593;&#32476;&#20351;&#29992;&#20107;&#20214;&#22312;&#20854;&#21457;&#29983;&#26102;&#39044;&#27979;&#25805;&#32437;&#21160;&#20316;&#65292;&#20351;&#29992;&#22312;&#32447;&#25512;&#29702;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#26089;&#26399;&#39044;&#27979;&#21160;&#20316;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#36880;&#28176;&#24314;&#31435;&#20449;&#24515;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic visual sensors are artificial retinas that output sequences of asynchronous events when brightness changes occur in the scene. These sensors offer many advantages including very high temporal resolution, no motion blur and smart data compression ideal for real-time processing. In this study, we introduce an event-based dataset on fine-grained manipulation actions and perform an experimental study on the use of transformers for action prediction with events. There is enormous interest in the fields of cognitive robotics and human-robot interaction on understanding and predicting human actions as early as possible. Early prediction allows anticipating complex stages for planning, enabling effective and real-time interaction. Our Transformer network uses events to predict manipulation actions as they occur, using online inference. The model succeeds at predicting actions early on, building up confidence over time and achieving state-of-the-art classification. Moreover, the at
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#28857;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#36335;&#24452;&#28857;&#26469;&#20943;&#23569;&#34892;&#20026;&#20811;&#38534;&#20013;&#32047;&#31215;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14326</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#28857;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Waypoint-Based Imitation Learning for Robotic Manipulation. (arXiv:2307.14326v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14326
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#28857;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#36335;&#24452;&#28857;&#26469;&#20943;&#23569;&#34892;&#20026;&#20811;&#38534;&#20013;&#32047;&#31215;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#20877;&#27425;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#20173;&#28982;&#22256;&#25200;&#30528;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#12290;&#36335;&#24452;&#28857;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;BC&#23398;&#20064;&#38382;&#39064;&#30340;&#35270;&#37326;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#20943;&#23569;&#38543;&#26102;&#38388;&#32047;&#31215;&#30340;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#36335;&#24452;&#28857;&#26631;&#35760;&#26159;&#19981;&#20805;&#20998;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22914;&#26524;&#19968;&#20010;&#36712;&#36857;&#27573;&#21487;&#20197;&#29992;&#32447;&#24615;&#36816;&#21160;&#36817;&#20284;&#65292;&#37027;&#20040;&#31471;&#28857;&#21487;&#20197;&#29992;&#20316;&#36335;&#24452;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#36335;&#24452;&#28857;&#25552;&#21462;&#65288;AWE&#65289;&#30340;&#27169;&#20223;&#23398;&#20064;&#39044;&#22788;&#29702;&#27169;&#22359;&#65292;&#23558;&#28436;&#31034;&#20998;&#35299;&#20026;&#19968;&#32452;&#26368;&#23567;&#30340;&#36335;&#24452;&#28857;&#65292;&#32447;&#24615;&#25554;&#20540;&#21487;&#20197;&#36817;&#20284;&#21040;&#25351;&#23450;&#30340;&#35823;&#24046;&#38408;&#20540;&#12290;AWE&#21487;&#20197;&#19982;&#20219;&#20309;BC&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#25105;&#20204;&#21457;&#29616;AWE&#33021;&#22815;&#25552;&#39640;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;&#36827;&#34892;&#35780;&#20272;&#30340;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22823;&#35268;&#27169;&#35843;&#26597;&#20102;&#35299;&#19981;&#21516;LLMs&#20013;&#30340;&#36947;&#24503;&#20449;&#24565;&#65292;&#22312;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20542;&#21521;&#20110;&#19982;&#20154;&#31867;&#30340;&#36947;&#24503;&#30452;&#35273;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#22238;&#31572;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14324</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Moral Beliefs Encoded in LLMs. (arXiv:2307.14324v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;&#36827;&#34892;&#35780;&#20272;&#30340;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22823;&#35268;&#27169;&#35843;&#26597;&#20102;&#35299;&#19981;&#21516;LLMs&#20013;&#30340;&#36947;&#24503;&#20449;&#24565;&#65292;&#22312;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20542;&#21521;&#20110;&#19982;&#20154;&#31867;&#30340;&#36947;&#24503;&#30452;&#35273;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#22238;&#31572;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#35774;&#35745;&#12289;&#31649;&#29702;&#12289;&#21518;&#22788;&#29702;&#21644;&#35780;&#20272;&#35843;&#26597;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;(1) &#19968;&#31181;&#29992;&#20110;&#33719;&#21462;LLMs&#20013;&#32534;&#30721;&#30340;&#20449;&#24565;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#32479;&#35745;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;LLM&#8220;&#20570;&#20986;&#36873;&#25321;&#8221;&#30340;&#27010;&#29575;&#12289;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#12290;(b) &#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#30740;&#31350;&#19981;&#21516;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#30830;&#36873;&#25321;&#19981;&#26126;&#26174;&#30340;&#27169;&#31946;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35843;&#26597;&#65292;&#20854;&#20013;&#21253;&#25324;680&#20010;&#39640;&#27169;&#31946;&#24230;&#30340;&#36947;&#24503;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#24212;&#35813;&#25746;&#19968;&#20010;&#21892;&#24847;&#30340;&#35854;&#35328;&#21527;&#65311;&#8221;&#65289;&#21644;687&#20010;&#20302;&#27169;&#31946;&#24230;&#30340;&#36947;&#24503;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#24212;&#35813;&#20026;&#36335;&#19978;&#30340;&#34892;&#20154;&#20572;&#19979;&#26469;&#21527;&#65311;&#8221;&#65289;&#12290;&#27599;&#20010;&#22330;&#26223;&#21253;&#25324;&#19968;&#20010;&#25551;&#36848;&#12289;&#20004;&#20010;&#21487;&#33021;&#30340;&#34892;&#21160;&#20197;&#21450;&#25351;&#31034;&#36829;&#21453;&#35268;&#21017;&#30340;&#36741;&#21161;&#26631;&#31614;&#65288;&#20363;&#22914;&#65292;&#8220;&#19981;&#35201;&#26432;&#20154;&#8221;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#35843;&#26597;&#24212;&#29992;&#20110;28&#20010;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;(b) &#22312;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs tend to align with human moral intuitions, but in ambiguous scenarios, their responses vary and may exhibit biases and inconsistencies.
&lt;/p&gt;
&lt;p&gt;
This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM "making a choice", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a white lie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for a pedestrian on the road?"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., "do not kill"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#23433;&#20840;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22312;&#27809;&#26377;&#22870;&#21169;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#23433;&#20840;&#25506;&#32034;&#65292;&#28982;&#21518;&#21033;&#29992;&#20854;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#19988;&#20511;&#37492;&#36801;&#31227;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#28040;&#38500;&#24341;&#23548;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.14316</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#23433;&#20840;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning by Guided Safe Exploration. (arXiv:2307.14316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#23433;&#20840;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22312;&#27809;&#26377;&#22870;&#21169;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#23433;&#20840;&#25506;&#32034;&#65292;&#28982;&#21518;&#21033;&#29992;&#20854;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#19988;&#20511;&#37492;&#36801;&#31227;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#28040;&#38500;&#24341;&#23548;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#23545;&#20110;&#25193;&#22823;&#24378;&#21270;&#23398;&#20064;(RL)&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#32463;&#24120;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#26234;&#33021;&#20307;&#65292;&#22914;&#23454;&#39564;&#23460;&#65292;&#28982;&#21518;&#20877;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#30446;&#26631;&#20219;&#21153;&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#12290;&#26080;&#22870;&#21169;RL&#22312;&#27809;&#26377;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#20197;&#20415;&#22312;&#22870;&#21169;&#25581;&#31034;&#21518;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26377;&#32422;&#26463;&#30340;&#26080;&#22870;&#21169;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#65288;&#23548;&#24341;&#32773;&#65289;&#22312;&#27809;&#26377;&#22870;&#21169;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#23433;&#20840;&#25506;&#32034;&#12290;&#35813;&#20195;&#29702;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#25509;&#21463;&#35757;&#32451;&#65292;&#21487;&#20197;&#36827;&#34892;&#19981;&#23433;&#20840;&#30340;&#20132;&#20114;&#20294;&#20173;&#25552;&#20379;&#23433;&#20840;&#20449;&#21495;&#12290;&#24403;&#30446;&#26631;&#20219;&#21153;&#34987;&#25581;&#31034;&#21518;&#65292;&#19981;&#20801;&#35768;&#36829;&#21453;&#23433;&#20840;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#23548;&#24341;&#32773;&#34987;&#21033;&#29992;&#26469;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#30340;&#34892;&#20026;&#31574;&#30053;&#12290;&#21463;&#21040;&#36801;&#31227;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36824;&#23558;&#30446;&#26631;&#31574;&#30053;&#65288;&#23398;&#29983;&#65289;&#21521;&#23548;&#24341;&#32773;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#23398;&#29983;&#22312;&#21487;&#38752;&#24615;&#19978;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#24182;&#36880;&#28176;&#28040;&#38500;&#23548;&#24341;&#32773;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24433;&#21709;&#12290;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#24212;&#29992;&#20110;&#37202;&#24215;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;ChatGPT&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#65292;&#32780;&#35828;&#26381;&#25216;&#26415;&#21487;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#24182;&#22686;&#24378;&#25512;&#33616;&#30340;&#35828;&#26381;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14298</link><description>&lt;p&gt;
ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#22312;&#37202;&#24215;&#26381;&#21153;&#39046;&#22495;&#20010;&#24615;&#21270;&#25512;&#33616;&#31649;&#29702;&#21644;&#25552;&#20379;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality. (arXiv:2307.14298v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#24212;&#29992;&#20110;&#37202;&#24215;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;ChatGPT&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#65292;&#32780;&#35828;&#26381;&#25216;&#26415;&#21487;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#24182;&#22686;&#24378;&#25512;&#33616;&#30340;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#37202;&#24215;&#26381;&#21153;&#19994;&#24050;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#20026;&#23458;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#23450;&#21046;&#21270;&#30340;&#20307;&#39564;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20026;&#25552;&#21319;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#26524;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#25972;&#21512;&#21040;&#37202;&#24215;&#26381;&#21153;&#25512;&#33616;&#31995;&#32479;&#20013;&#33258;&#21160;&#21270;&#21644;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;ChatGPT&#30340;&#33021;&#21147;&#65292;&#23427;&#21487;&#20197;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;ChatGPT&#25972;&#21512;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#33021;&#21147;&#65292;&#31361;&#20986;&#20102;&#20854;&#20998;&#26512;&#29992;&#25143;&#20559;&#22909;&#12289;&#20174;&#22312;&#32447;&#35780;&#35770;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#24182;&#26681;&#25454;&#23458;&#20154;&#37197;&#32622;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35828;&#26381;&#25216;&#26415;&#22312;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#21644;&#25552;&#21319;&#37202;&#24215;&#25512;&#33616;&#30340;&#35828;&#26381;&#25928;&#26524;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#27010;&#24565;&#25991;&#31456;&#30740;&#31350;&#20102;&#21508;&#31181;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#30005;&#26426;&#27979;&#35797;&#21488;&#21644;&#28082;&#20307;&#20013;&#30340;&#31890;&#23376;&#36319;&#36394;&#31561;&#23454;&#20363;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.14294</link><description>&lt;p&gt;
&#25581;&#31034;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65306;&#35299;&#20915;&#35270;&#39057;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis. (arXiv:2307.14294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27010;&#24565;&#25991;&#31456;&#30740;&#31350;&#20102;&#21508;&#31181;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#30005;&#26426;&#27979;&#35797;&#21488;&#21644;&#28082;&#20307;&#20013;&#30340;&#31890;&#23376;&#36319;&#36394;&#31561;&#23454;&#20363;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25968;&#25454;&#30340;&#25286;&#20998;&#65292;&#22914;&#35270;&#39057;&#21644;&#26102;&#38388;&#24207;&#21015;&#65292;&#26159;&#21508;&#31181;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#21253;&#25324;&#30446;&#26631;&#36319;&#36394;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#21518;&#32493;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#27010;&#24565;&#25991;&#31456;&#30740;&#31350;&#20102;&#19982;&#25286;&#20998;&#24207;&#21015;&#25968;&#25454;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#33719;&#21462;&#12289;&#25968;&#25454;&#34920;&#31034;&#12289;&#25286;&#20998;&#27604;&#20363;&#36873;&#25321;&#12289;&#24314;&#31435;&#36136;&#37327;&#26631;&#20934;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#36873;&#25321;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#30005;&#26426;&#27979;&#35797;&#21488;&#21644;&#28082;&#20307;&#20013;&#30340;&#31890;&#23376;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Splitting of sequential data, such as videos and time series, is an essential step in various data analysis tasks, including object tracking and anomaly detection. However, splitting sequential data presents a variety of challenges that can impact the accuracy and reliability of subsequent analyses. This concept article examines the challenges associated with splitting sequential data, including data acquisition, data representation, split ratio selection, setting up quality criteria, and choosing suitable selection strategies. We explore these challenges through two real-world examples: motor test benches and particle tracking in liquids.
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#30340;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14283</link><description>&lt;p&gt;
&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#65306;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#24320;&#25918;&#25361;&#25112;&#21644;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications. (arXiv:2307.14283v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#30340;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#37117;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#21644;&#26377;&#38480;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#22330;&#26223;&#38656;&#35201;&#26356;&#36890;&#29992;&#30340;AI&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#32780;&#19981;&#38656;&#35201;&#19987;&#38376;&#20026;&#23427;&#20204;&#35774;&#35745;&#12290;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#36825;&#20010;&#26415;&#35821;&#34987;&#23450;&#20041;&#20026;&#25351;&#20195;&#36825;&#20123;AI&#31995;&#32479;&#12290;&#23613;&#31649;&#36804;&#20170;&#20026;&#27490;&#65292;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#36275;&#22815;&#24378;&#22823;&#20197;&#27169;&#25311;&#20154;&#31867;&#24182;&#25913;&#36827;&#21508;&#31181;&#26234;&#21147;&#20219;&#21153;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#24895;&#26395;&#12289;&#34394;&#26500;&#30340;&#27010;&#24565;&#65292;&#24182;&#34987;&#35748;&#20026;&#23545;&#25105;&#20204;&#31038;&#20250;&#26500;&#25104;&#39118;&#38505;&#12290;&#34429;&#28982;&#25105;&#20204;&#31163;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#21487;&#33021;&#36824;&#24456;&#36965;&#36828;&#65292;&#20294;GPAIS&#26159;&#29616;&#23454;&#23384;&#22312;&#24182;&#20301;&#23621;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#21069;&#27839;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29616;&#26377;GPAIS&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#23553;&#38381;&#19990;&#30028;&#21644;&#24320;&#25918;&#19990;&#30028;&#30340;GPAIS&#65292;&#25551;&#36848;&#20854;&#33258;&#20027;&#31243;&#24230;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.  This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26377;&#26465;&#20214;&#30340;&#25215;&#35834;&#26426;&#21046;&#30340;&#20998;&#25955;&#65292;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24418;&#24335;&#21270;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;&#20013;&#30340;&#26377;&#26465;&#20214;&#21512;&#20316;&#26469;&#35299;&#20915;&#26080;&#26465;&#20214;&#24615;&#36129;&#29486;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#35813;&#26426;&#21046;&#21463;&#27665;&#20027;&#20840;&#27665;&#25237;&#31080;&#24030;&#38388;&#21327;&#23450;&#30340;&#21551;&#21457;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#28608;&#21169;&#25514;&#26045;&#65292;&#20197;&#20419;&#20351;&#26089;&#26399;&#37319;&#29992;&#32773;&#12290;&#26399;&#26395;&#33021;&#25913;&#21892;&#22823;&#27745;&#26579;&#22269;&#30340;&#33258;&#30001;&#39569;&#36710;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14267</link><description>&lt;p&gt;
&#36890;&#36807;&#30456;&#20114;&#26377;&#26465;&#20214;&#30340;&#32422;&#26463;&#25215;&#35834;&#25913;&#36827;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Improving International Climate Policy via Mutually Conditional Binding Commitments. (arXiv:2307.14267v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26377;&#26465;&#20214;&#30340;&#25215;&#35834;&#26426;&#21046;&#30340;&#20998;&#25955;&#65292;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24418;&#24335;&#21270;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;&#20013;&#30340;&#26377;&#26465;&#20214;&#21512;&#20316;&#26469;&#35299;&#20915;&#26080;&#26465;&#20214;&#24615;&#36129;&#29486;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#35813;&#26426;&#21046;&#21463;&#27665;&#20027;&#20840;&#27665;&#25237;&#31080;&#24030;&#38388;&#21327;&#23450;&#30340;&#21551;&#21457;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#28608;&#21169;&#25514;&#26045;&#65292;&#20197;&#20419;&#20351;&#26089;&#26399;&#37319;&#29992;&#32773;&#12290;&#26399;&#26395;&#33021;&#25913;&#21892;&#22823;&#27745;&#26579;&#22269;&#30340;&#33258;&#30001;&#39569;&#36710;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24052;&#40654;&#21327;&#23450;&#34987;&#35748;&#20026;&#26159;&#27668;&#20505;&#35848;&#21028;&#20013;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#20294;&#30001;&#20110;&#22823;&#22810;&#25968;&#22269;&#23478;&#33258;&#20027;&#30830;&#23450;&#36129;&#29486;&#30340;&#26080;&#26465;&#20214;&#24615;&#36136;&#65292;&#23427;&#22312;&#26377;&#25928;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#20027;&#35201;&#27745;&#26579;&#22269;&#23384;&#22312;&#33258;&#30001;&#39569;&#36710;&#34892;&#20026;&#65292;&#24182;&#19988;&#22269;&#23478;&#33258;&#20027;&#30830;&#23450;&#36129;&#29486;&#32570;&#20047;&#20855;&#20307;&#30340;&#26465;&#20214;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#30340;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#26377;&#26465;&#20214;&#30340;&#25215;&#35834;&#26426;&#21046;&#12290;&#36825;&#20010;&#26426;&#21046;&#21463;&#21040;&#8220;&#27665;&#20027;&#20840;&#27665;&#25237;&#31080;&#24030;&#38388;&#21327;&#23450;&#8221;&#30340;&#21551;&#21457;&#65292;&#20026;&#26089;&#26399;&#37319;&#29992;&#32773;&#25552;&#20379;&#28789;&#27963;&#24615;&#21644;&#28608;&#21169;&#65292;&#24182;&#26088;&#22312;&#35268;&#33539;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;&#20013;&#30340;&#26377;&#26465;&#20214;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#36825;&#20010;&#26426;&#21046;&#65292;&#20171;&#32461;&#20102;&#20854;&#22312;AI4ClimateCooperation&#25361;&#25112;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#23454;&#38469;&#23454;&#26045;&#26041;&#38754;&#12290;&#20551;&#23450;&#35835;&#32773;&#20855;&#22791;&#26377;&#20851;&#27668;&#20505;&#20943;&#32531;&#38598;&#20307;&#34892;&#21160;&#38382;&#39064;&#12289;&#22522;&#26412;&#32463;&#27982;&#21407;&#29702;&#21644;&#21338;&#24328;&#35770;&#27010;&#24565;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Paris Agreement, considered a significant milestone in climate negotiations, has faced challenges in effectively addressing climate change due to the unconditional nature of most Nationally Determined Contributions (NDCs). This has resulted in a prevalence of free-riding behavior among major polluters and a lack of concrete conditionality in NDCs. To address this issue, we propose the implementation of a decentralized, bottom-up approach called the Conditional Commitment Mechanism. This mechanism, inspired by the National Popular Vote Interstate Compact, offers flexibility and incentives for early adopters, aiming to formalize conditional cooperation in international climate policy. In this paper, we provide an overview of the mechanism, its performance in the AI4ClimateCooperation challenge, and discuss potential real-world implementation aspects. Prior knowledge of the climate mitigation collective action problem, basic economic principles, and game theory concepts are assumed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;RICE-N&#27169;&#25311;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;&#35848;&#21028;&#30340;&#36924;&#30495;&#24230;&#21644;&#20915;&#31574;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14266</link><description>&lt;p&gt;
&#36890;&#36807;&#30456;&#20114;&#26465;&#20214;&#32465;&#23450;&#25215;&#35834;&#26469;&#25913;&#36827;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Improving International Climate Policy via Mutually Conditional Binding Commitments. (arXiv:2307.14266v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;RICE-N&#27169;&#25311;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;&#35848;&#21028;&#30340;&#36924;&#30495;&#24230;&#21644;&#20915;&#31574;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#21892;RICE-N&#27169;&#25311;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;&#35848;&#21028;&#30340;&#36924;&#30495;&#24230;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26694;&#26550;&#30340;&#20215;&#20540;&#65292;&#24182;&#25351;&#20986;&#20102;&#38656;&#35201;&#36827;&#34892;&#37325;&#22823;&#25913;&#36827;&#26469;&#35299;&#20915;&#24314;&#27169;&#27668;&#20505;&#35848;&#21028;&#20013;&#21508;&#31181;&#22240;&#32032;&#30340;&#22810;&#26679;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#12298;&#26465;&#20214;&#25215;&#35834;&#26426;&#21046;&#12299;&#65288;CCF&#26426;&#21046;&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#24357;&#21512;&#27169;&#25311;&#21644;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#24314;&#35758;&#21152;&#20837;&#19968;&#20010;&#25512;&#33616;&#25110;&#35745;&#21010;&#20195;&#29702;&#20197;&#22686;&#24378;&#21327;&#35843;&#65292;&#36890;&#36807;&#32435;&#20837;&#31038;&#20250;&#22240;&#32032;&#21644;&#38750;&#24403;&#20107;&#26041;&#21033;&#30410;&#30456;&#20851;&#32773;&#21103;&#20195;&#29702;&#26469;&#35299;&#20915;Real2Sim&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;&#24213;&#23618;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#31639;&#27861;&#12290;&#36825;&#20123;&#25913;&#36827;&#26088;&#22312;&#25512;&#36827;Rice-N&#20013;&#29992;&#20110;&#26356;&#26377;&#25928;&#30340;&#22269;&#38469;&#27668;&#20505;&#25919;&#31574;&#20915;&#31574;&#30340;&#35848;&#21028;&#21327;&#35758;&#30340;&#35780;&#20272;&#21644;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes enhancements to the RICE-N simulation and multi-agent reinforcement learning framework to improve the realism of international climate policy negotiations. Acknowledging the framework's value, we highlight the necessity of significant enhancements to address the diverse array of factors in modeling climate negotiations. Building upon our previous work on the "Conditional Commitments Mechanism" (CCF mechanism) we discuss ways to bridge the gap between simulation and reality. We suggest the inclusion of a recommender or planner agent to enhance coordination, address the Real2Sim gap by incorporating social factors and non-party stakeholder sub-agents, and propose enhancements to the underlying Reinforcement Learning solution algorithm. These proposed improvements aim to advance the evaluation and formulation of negotiation protocols for more effective international climate policy decision-making in Rice-N. However, further experimentation and testing are required to d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#25209;&#21028;&#24615;&#22320;&#23457;&#35270;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#25152;&#35859;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#32454;&#33268;&#20837;&#24494;&#30340;&#26041;&#24335;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#39046;&#22495;&#29305;&#24449;&#21644;&#39118;&#38505;&#22240;&#32032;&#12290;&#36825;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#26368;&#20339;&#23454;&#36341;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.14246</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#35780;&#20272;&#26041;&#27861;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI). (arXiv:2307.14246v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14246
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#25209;&#21028;&#24615;&#22320;&#23457;&#35270;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#25152;&#35859;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#32454;&#33268;&#20837;&#24494;&#30340;&#26041;&#24335;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#39046;&#22495;&#29305;&#24449;&#21644;&#39118;&#38505;&#22240;&#32032;&#12290;&#36825;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#26368;&#20339;&#23454;&#36341;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#27714;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#23558;AI&#25903;&#25345;&#30340;&#31995;&#32479;&#19982;&#29992;&#25143;&#38656;&#27714;&#12289;&#31038;&#20250;&#26399;&#26395;&#21644;&#30417;&#31649;&#26631;&#20934;&#30456;&#19968;&#33268;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#21463;&#21040;&#35748;&#21487;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#21487;&#35299;&#37322;&#24615;&#24050;&#32463;&#25104;&#20026;&#24433;&#21709;&#31995;&#32479;&#36136;&#37327;&#30340;&#37325;&#35201;&#38750;&#21151;&#33021;&#24615;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#25152;&#35859;&#26435;&#34913;&#25361;&#25112;&#20102;&#35299;&#37322;&#24615;&#30340;&#39044;&#26399;&#27491;&#38754;&#24433;&#21709;&#12290;&#22914;&#26524;&#28385;&#36275;&#35299;&#37322;&#24615;&#30340;&#35201;&#27714;&#24847;&#21619;&#30528;&#31995;&#32479;&#24615;&#33021;&#30340;&#38477;&#20302;&#65292;&#37027;&#20040;&#24517;&#39035;&#24910;&#37325;&#32771;&#34385;&#36825;&#20004;&#20010;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#20808;&#39034;&#24207;&#20197;&#21450;&#22914;&#20309;&#26435;&#34913;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#25152;&#35859;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#23457;&#35270;&#12290;&#25105;&#20204;&#35748;&#20026;&#26368;&#22909;&#20197;&#32454;&#33268;&#20837;&#24494;&#30340;&#26041;&#24335;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#39046;&#22495;&#29305;&#24449;&#21644;&#39118;&#38505;&#22240;&#32032;&#12290;&#36890;&#36807;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#26368;&#20339;&#23454;&#36341;&#25552;&#20379;&#22522;&#30784;&#65292;&#26412;&#24037;&#20316;&#26088;&#22312;&#20419;&#36827;XAI&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the field of Requirements Engineering (RE), the increasing significance of Explainable Artificial Intelligence (XAI) in aligning AI-supported systems with user needs, societal expectations, and regulatory standards has garnered recognition. In general, explainability has emerged as an important non-functional requirement that impacts system quality. However, the supposed trade-off between explainability and performance challenges the presumed positive influence of explainability. If meeting the requirement of explainability entails a reduction in system performance, then careful consideration must be given to which of these quality aspects takes precedence and how to compromise between them. In this paper, we critically examine the alleged trade-off. We argue that it is best approached in a nuanced way that incorporates resource availability, domain characteristics, and considerations of risk. By providing a foundation for future research and best practices, this work aims to ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#39046;&#22495;&#29305;&#24449;&#21644;&#39118;&#38505;&#32771;&#34385;&#30340;&#32454;&#33268;&#26041;&#27861;&#12290;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#26368;&#20339;&#23454;&#36341;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.14239</link><description>&lt;p&gt;
&#37325;&#35775;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24615;&#33021;-&#21487;&#35299;&#37322;&#24615;&#26435;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI). (arXiv:2307.14239v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#39046;&#22495;&#29305;&#24449;&#21644;&#39118;&#38505;&#32771;&#34385;&#30340;&#32454;&#33268;&#26041;&#27861;&#12290;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#26368;&#20339;&#23454;&#36341;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#27714;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#23558;AI&#25903;&#25345;&#30340;&#31995;&#32479;&#19982;&#29992;&#25143;&#38656;&#27714;&#12289;&#31038;&#20250;&#26399;&#26395;&#21644;&#27861;&#35268;&#26631;&#20934;&#23545;&#40784;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#36880;&#28176;&#24471;&#21040;&#35748;&#35782;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#21487;&#35299;&#37322;&#24615;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38750;&#21151;&#33021;&#38656;&#27714;&#65292;&#24433;&#21709;&#30528;&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#25361;&#25112;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#39044;&#26399;&#31215;&#26497;&#24433;&#21709;&#12290;&#22914;&#26524;&#28385;&#36275;&#21487;&#35299;&#37322;&#24615;&#30340;&#35201;&#27714;&#24847;&#21619;&#30528;&#31995;&#32479;&#24615;&#33021;&#30340;&#38477;&#20302;&#65292;&#37027;&#20040;&#24517;&#39035;&#20180;&#32454;&#32771;&#34385;&#21738;&#20010;&#36136;&#37327;&#26041;&#38754;&#26356;&#20026;&#37325;&#35201;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#23427;&#20204;&#20043;&#38388;&#21462;&#24471;&#22949;&#21327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#25152;&#35859;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#32771;&#23519;&#12290;&#25105;&#20204;&#35748;&#20026;&#26368;&#22909;&#20197;&#19968;&#31181;&#32454;&#33268;&#20837;&#24494;&#30340;&#26041;&#24335;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#39046;&#22495;&#29305;&#24449;&#20197;&#21450;&#39118;&#38505;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#26368;&#20339;&#23454;&#36341;&#25552;&#20379;&#22522;&#30784;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the field of Requirements Engineering (RE), the increasing significance of Explainable Artificial Intelligence (XAI) in aligning AI-supported systems with user needs, societal expectations, and regulatory standards has garnered recognition. In general, explainability has emerged as an important non-functional requirement that impacts system quality. However, the supposed trade-off between explainability and performance challenges the presumed positive influence of explainability. If meeting the requirement of explainability entails a reduction in system performance, then careful consideration must be given to which of these quality aspects takes precedence and how to compromise between them. In this paper, we critically examine the alleged trade-off. We argue that it is best approached in a nuanced way that incorporates resource availability, domain characteristics, and considerations of risk. By providing a foundation for future research and best practices, this work aims to ad
&lt;/p&gt;</description></item><item><title>UnScientify&#26159;&#19968;&#20010;&#20132;&#20114;&#31995;&#32479;&#65292;&#21487;&#20197;&#26816;&#27979;&#23398;&#26415;&#20840;&#25991;&#20013;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#26696;&#65292;&#33258;&#21160;&#26631;&#35760;&#21644;&#27880;&#37322;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14236</link><description>&lt;p&gt;
UnScientify: &#26816;&#27979;&#23398;&#26415;&#20840;&#25991;&#20013;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text. (arXiv:2307.14236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14236
&lt;/p&gt;
&lt;p&gt;
UnScientify&#26159;&#19968;&#20010;&#20132;&#20114;&#31995;&#32479;&#65292;&#21487;&#20197;&#26816;&#27979;&#23398;&#26415;&#20840;&#25991;&#20013;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#26696;&#65292;&#33258;&#21160;&#26631;&#35760;&#21644;&#27880;&#37322;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#28436;&#31034;&#35770;&#25991;&#20171;&#32461;&#20102;UnScientify&#65292;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#23398;&#26415;&#20840;&#25991;&#20013;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#20114;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#19968;&#31181;&#24369;&#30417;&#30563;&#25216;&#26415;&#65292;&#37319;&#29992;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#26696;&#65292;&#20174;&#21477;&#23376;&#32423;&#21035;&#19978;&#35782;&#21035;&#31185;&#23398;&#25991;&#26412;&#20013;&#21475;&#22836;&#34920;&#36848;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#31995;&#32479;&#30340;&#27969;&#31243;&#21253;&#25324;&#27169;&#24335;&#21305;&#37197;&#12289;&#22797;&#26434;&#21477;&#23376;&#26816;&#26597;&#21644;&#20316;&#32773;&#21442;&#32771;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#21160;&#26631;&#35760;&#21644;&#27880;&#37322;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#35782;&#21035;&#20219;&#21153;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#25991;&#26412;&#25366;&#25496;&#21644;&#23398;&#26415;&#25991;&#29486;&#22788;&#29702;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;UnScientify&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#25991;&#26412;&#20013;&#35782;&#21035;&#20986;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This demo paper presents UnScientify, an interactive system designed to detect scientific uncertainty in scholarly full text. The system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally formulated uncertainty at the sentence level in scientific texts. The pipeline for the system includes a combination of pattern matching, complex sentence checking, and authorial reference checking. Our approach automates labeling and annotation tasks for scientific uncertainty identification, taking into account different types of scientific uncertainty, that can serve various applications such as information retrieval, text mining, and scholarly document processing. Additionally, UnScientify provides interpretable results, aiding in the comprehension of identified instances of scientific uncertainty in text.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#31867;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#35299;&#20915;&#35745;&#31639;&#26426;&#31995;&#32479;&#20013;&#30340;&#19981;&#36879;&#26126;&#24615;&#38382;&#39064;&#12290;&#35813;&#20998;&#31867;&#26041;&#27861;&#35770;&#21253;&#21547;&#20843;&#20010;&#19981;&#36879;&#26126;&#24615;&#26469;&#28304;&#65292;&#20998;&#21035;&#23646;&#20110;&#20307;&#31995;&#32467;&#26500;&#12289;&#20998;&#26512;&#21644;&#31038;&#20250;&#25216;&#26415;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#35813;&#26041;&#27861;&#35770;&#20026;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35299;&#21644;&#20811;&#26381;&#19981;&#36879;&#26126;&#24615;&#38382;&#39064;&#30340;&#36215;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.14232</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#31995;&#32479;&#20013;&#30340;&#19981;&#36879;&#26126;&#24615;&#26469;&#28304;&#65306;&#36808;&#21521;&#20840;&#38754;&#20998;&#31867;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Sources of Opacity in Computer Systems: Towards a Comprehensive Taxonomy. (arXiv:2307.14232v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#31867;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#35299;&#20915;&#35745;&#31639;&#26426;&#31995;&#32479;&#20013;&#30340;&#19981;&#36879;&#26126;&#24615;&#38382;&#39064;&#12290;&#35813;&#20998;&#31867;&#26041;&#27861;&#35770;&#21253;&#21547;&#20843;&#20010;&#19981;&#36879;&#26126;&#24615;&#26469;&#28304;&#65292;&#20998;&#21035;&#23646;&#20110;&#20307;&#31995;&#32467;&#26500;&#12289;&#20998;&#26512;&#21644;&#31038;&#20250;&#25216;&#26415;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#35813;&#26041;&#27861;&#35770;&#20026;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35299;&#21644;&#20811;&#26381;&#19981;&#36879;&#26126;&#24615;&#38382;&#39064;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#31995;&#32479;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#31995;&#32479;&#20173;&#28982;&#19981;&#36879;&#26126;&#12290;&#36825;&#22312;&#20197;&#20844;&#24179;&#25110;&#38382;&#36131;&#20026;&#20851;&#38190;&#22240;&#32032;&#30340;&#39046;&#22495;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#35748;&#20026;&#23454;&#29616;&#31995;&#32479;&#36879;&#26126;&#24230;&#30340;&#26368;&#20339;&#31574;&#30053;&#22240;&#29305;&#23450;&#19981;&#36879;&#26126;&#28304;&#22312;&#32473;&#23450;&#35821;&#22659;&#20013;&#30340;&#26222;&#36941;&#24615;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;&#32508;&#21512;&#21644;&#25193;&#23637;&#29616;&#26377;&#35752;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65288;&#20307;&#31995;&#32467;&#26500;&#12289;&#20998;&#26512;&#21644;&#31038;&#20250;&#25216;&#26415;&#65289;&#30340;&#20843;&#20010;&#19981;&#36879;&#26126;&#24615;&#26469;&#28304;&#30340;&#20998;&#31867;&#26041;&#27861;&#35770;&#12290;&#23545;&#20110;&#27599;&#20010;&#26469;&#28304;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22914;&#20309;&#23454;&#38469;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#19981;&#36879;&#26126;&#24615;&#30340;&#21021;&#27493;&#24314;&#35758;&#12290;&#35813;&#20998;&#31867;&#26041;&#27861;&#35770;&#20026;&#38656;&#27714;&#24037;&#31243;&#24072;&#21644;&#20854;&#20182;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#20102;&#35299;&#35821;&#22659;&#20013;&#26222;&#36941;&#30340;&#19981;&#36879;&#26126;&#24615;&#26469;&#28304;&#65292;&#24182;&#36873;&#25321;&#25110;&#24320;&#21457;&#36866;&#24403;&#31574;&#30053;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern computer systems are ubiquitous in contemporary life yet many of them remain opaque. This poses significant challenges in domains where desiderata such as fairness or accountability are crucial. We suggest that the best strategy for achieving system transparency varies depending on the specific source of opacity prevalent in a given context. Synthesizing and extending existing discussions, we propose a taxonomy consisting of eight sources of opacity that fall into three main categories: architectural, analytical, and socio-technical. For each source, we provide initial suggestions as to how to address the resulting opacity in practice. The taxonomy provides a starting point for requirements engineers and other practitioners to understand contextually prevalent sources of opacity, and to select or develop appropriate strategies for overcoming them.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22522;&#20110;&#21306;&#22495;&#36152;&#26131;&#32452;&#32455;&#25512;&#36827;&#27668;&#20505;&#35848;&#21028;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;RICE-N&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#26696;&#20855;&#26377;&#33391;&#22909;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2307.14226</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#36152;&#26131;&#32452;&#32455;&#25512;&#36827;&#27668;&#20505;&#35848;&#21028;&#30340;&#21487;&#33021;&#24615;&#25506;&#32034;&#65306;&#22522;&#20110;RICE-N&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explore the possibility of advancing climate negotiations on the basis of regional trade organizations: A study based on RICE-N. (arXiv:2307.14226v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22522;&#20110;&#21306;&#22495;&#36152;&#26131;&#32452;&#32455;&#25512;&#36827;&#27668;&#20505;&#35848;&#21028;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;RICE-N&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#26696;&#20855;&#26377;&#33391;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23613;&#31649;&#20840;&#29699;&#25919;&#24220;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#20173;&#38754;&#20020;&#30528;&#22269;&#38469;&#21512;&#20316;&#21069;&#26223;&#19981;&#26126;&#30340;&#20107;&#23454;&#12290;&#30001;&#20110;&#32508;&#21512;&#35780;&#20272;&#27169;&#22411;&#65288;IAMs&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#24456;&#38590;&#27169;&#25311;&#21160;&#24577;&#35848;&#21028;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#21487;&#33021;&#20026;&#27668;&#20505;&#35848;&#21028;&#25552;&#20379;&#26032;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;&#26412;&#30740;&#31350;&#22312;RICE-N&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29616;&#26377;&#36152;&#26131;&#38598;&#22242;&#30340;&#27668;&#20505;&#35848;&#21028;&#26041;&#27861;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#26696;&#20855;&#26377;&#33391;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate issues have become more and more important now. Although global governments have made some progress, we are still facing the truth that the prospect of international cooperation is not clear at present. Due to the limitations of the Integrated assessment models (IAMs) model, it is difficult to simulate the dynamic negotiation process. Therefore, using deep learning to build a new agents based model (ABM) might can provide new theoretical support for climate negotiations. Building on the RICE-N model, this work proposed an approach to climate negotiations based on existing trade groups. Simulation results show that the scheme has a good prospect.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#24037;&#20855;ChatGPT&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#25903;&#25345;&#31995;&#32479;&#24605;&#32500;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#29992;&#30340;&#22238;&#31572;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20294;&#35880;&#24910;&#20351;&#29992;&#26102;&#21487;&#20197;&#25104;&#20026;&#25945;&#23398;&#21644;&#23398;&#20064;&#31995;&#32479;&#24605;&#32500;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.14206</link><description>&lt;p&gt;
AI&#19982;&#25945;&#32946;&#65306;ChatGPT&#22312;&#31995;&#32479;&#24605;&#32500;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI and Education: An Investigation into the Use of ChatGPT for Systems Thinking. (arXiv:2307.14206v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#24037;&#20855;ChatGPT&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#25903;&#25345;&#31995;&#32479;&#24605;&#32500;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#21644;&#26377;&#29992;&#30340;&#22238;&#31572;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20294;&#35880;&#24910;&#20351;&#29992;&#26102;&#21487;&#20197;&#25104;&#20026;&#25945;&#23398;&#21644;&#23398;&#20064;&#31995;&#32479;&#24605;&#32500;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;ChatGPT&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#25903;&#25345;&#31995;&#32479;&#24605;&#32500;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#33324;&#21644;&#23398;&#31185;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#19981;&#21516;&#29256;&#26412;&#20013;&#30340;&#21709;&#24212;&#20934;&#30830;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#21644;&#38750;&#24120;&#26377;&#29992;&#30340;&#22238;&#31572;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22686;&#24378;&#31995;&#32479;&#24605;&#32500;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20598;&#23572;&#30340;&#19981;&#20934;&#30830;&#24615;&#20063;&#24378;&#35843;&#20102;&#29992;&#25143;&#38656;&#35201;&#23545;ChatGPT&#30340;&#22238;&#31572;&#25345;&#26377;&#25209;&#21028;&#24577;&#24230;&#12290;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35880;&#24910;&#20351;&#29992;&#24182;&#27880;&#24847;&#20854;&#29305;&#27530;&#20043;&#22788;&#65292;ChatGPT&#21487;&#20197;&#25104;&#20026;&#25945;&#23398;&#21644;&#23398;&#20064;&#31995;&#32479;&#24605;&#32500;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
This exploratory study investigates the potential of the artificial intelligence tool, ChatGPT, to support systems thinking (ST) in various subjects. Using both general and subject specific prompts, the study assesses the accuracy, helpfulness, and reliability of ChatGPT's responses across different versions of the tool. The results indicate that ChatGPT can provide largely correct and very helpful responses in various subjects, demonstrating its potential as a tool for enhancing ST skills. However, occasional inaccuracies highlight the need for users to remain critical of ChatGPT's responses. Despite some limitations, this study suggests that with careful use and attention to its idiosyncrasies, ChatGPT can be a valuable tool for teaching and learning ST.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#23558;&#20854;&#24212;&#29992;&#21040;&#19981;&#21516;&#34892;&#19994;&#21487;&#33021;&#23384;&#22312;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#25361;&#25112;&#65292;&#24182;&#21628;&#21505;&#20849;&#21516;&#21162;&#21147;&#30830;&#20445;&#24320;&#21457;&#23433;&#20840;&#21644;&#20262;&#29702;&#20581;&#20840;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14192</link><description>&lt;p&gt;
&#25581;&#24320;ChatGPT&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20262;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unveiling Security, Privacy, and Ethical Concerns of ChatGPT. (arXiv:2307.14192v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#23558;&#20854;&#24212;&#29992;&#21040;&#19981;&#21516;&#34892;&#19994;&#21487;&#33021;&#23384;&#22312;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#25361;&#25112;&#65292;&#24182;&#21628;&#21505;&#20849;&#21516;&#21162;&#21147;&#30830;&#20445;&#24320;&#21457;&#23433;&#20840;&#21644;&#20262;&#29702;&#20581;&#20840;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#33258;&#28982;&#22238;&#31572;&#30340;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#34429;&#28982;ChatGPT&#22312;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#24515;&#29702;&#20581;&#24247;&#27835;&#30103;&#12289;&#20010;&#20154;&#29983;&#20135;&#21147;&#21644;&#20869;&#23481;&#21019;&#20316;&#31561;&#22810;&#20010;&#34892;&#19994;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#26377;&#24517;&#35201;&#35299;&#20915;&#20854;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#25506;&#35752;&#20174;GPT-1&#21040;GPT-4&#30340;&#21319;&#32423;&#36335;&#24452;&#65292;&#35752;&#35770;&#27169;&#22411;&#30340;&#29305;&#28857;&#12289;&#38480;&#21046;&#21644;&#28508;&#22312;&#24212;&#29992;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#23558;ChatGPT&#25972;&#21512;&#21040;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#24378;&#35843;&#36825;&#20123;&#38382;&#39064;&#23545;&#24191;&#27867;&#37319;&#29992;&#30340;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#39046;&#22495;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#21628;&#21505;&#20849;&#21516;&#21162;&#21147;&#30830;&#20445;&#24320;&#21457;&#23433;&#20840;&#21644;&#20262;&#29702;&#20581;&#20840;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the realm of ChatGPT, an AI-powered chatbot that utilizes topic modeling and reinforcement learning to generate natural responses. Although ChatGPT holds immense promise across various industries, such as customer service, education, mental health treatment, personal productivity, and content creation, it is essential to address its security, privacy, and ethical implications. By exploring the upgrade path from GPT-1 to GPT-4, discussing the model's features, limitations, and potential applications, this study aims to shed light on the potential risks of integrating ChatGPT into our daily lives. Focusing on security, privacy, and ethics issues, we highlight the challenges these concerns pose for widespread adoption. Finally, we analyze the open problems in these areas, calling for concerted efforts to ensure the development of secure and ethically sound large language models.
&lt;/p&gt;</description></item><item><title>LOIS&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#35821;&#20041;&#29702;&#35299;&#38382;&#39064;&#12290;&#23427;&#19981;&#20381;&#36182;&#20110;&#36793;&#30028;&#26694;&#65292;&#24182;&#20351;&#29992;&#31934;&#32454;&#30340;&#29305;&#24449;&#25551;&#36848;&#26469;&#29983;&#25104;&#35270;&#35273;&#20107;&#23454;&#12290;&#21478;&#22806;&#65292;LOIS&#36890;&#36807;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#35299;&#20915;&#30001;&#23454;&#20363;&#25513;&#30721;&#24341;&#36215;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14142</link><description>&lt;p&gt;
LOIS: &#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#35266;&#23519;&#23454;&#20363;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
LOIS: Looking Out of Instance Semantics for Visual Question Answering. (arXiv:2307.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14142
&lt;/p&gt;
&lt;p&gt;
LOIS&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#35821;&#20041;&#29702;&#35299;&#38382;&#39064;&#12290;&#23427;&#19981;&#20381;&#36182;&#20110;&#36793;&#30028;&#26694;&#65292;&#24182;&#20351;&#29992;&#31934;&#32454;&#30340;&#29305;&#24449;&#25551;&#36848;&#26469;&#29983;&#25104;&#35270;&#35273;&#20107;&#23454;&#12290;&#21478;&#22806;&#65292;LOIS&#36890;&#36807;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#35299;&#20915;&#30001;&#23454;&#20363;&#25513;&#30721;&#24341;&#36215;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;(VQA)&#20316;&#20026;&#19968;&#39033;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#21162;&#21147;&#65292;&#20197;&#27491;&#30830;&#22320;&#25512;&#26029;&#31572;&#26696;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#24320;&#21457;&#20102;&#21508;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;VQA&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#25512;&#26029;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#38480;&#20110;&#23545;&#35821;&#20041;&#29702;&#35299;&#30340;&#35270;&#35273;&#22788;&#29702;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#36793;&#30028;&#26694;&#65292;&#36825;&#23545;&#20110;VQA&#27169;&#22411;&#29702;&#35299;&#22270;&#20687;&#20013;&#29289;&#20307;&#35821;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#24182;&#27491;&#30830;&#25512;&#26029;&#19978;&#19979;&#25991;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#31934;&#32454;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#21517;&#20026;Looking Out of Instance Semantics (LOIS)&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;LOIS&#33021;&#22815;&#29983;&#25104;&#26356;&#31934;&#32454;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#20197;&#20135;&#29983;&#35270;&#35273;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20811;&#26381;&#23454;&#20363;&#25513;&#30721;&#24341;&#36215;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#65306;1) &#20869;&#37096;&#27169;&#24577;&#21644;2) &#20132;&#20114;&#27169;&#24577;&#65292;&#29992;&#20110;&#25512;&#26029;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) has been intensively studied as a multimodal task that requires effort in bridging vision and language to infer answers correctly. Recent attempts have developed various attention-based modules for solving VQA tasks. However, the performance of model inference is largely bottlenecked by visual processing for semantics understanding. Most existing detection methods rely on bounding boxes, remaining a serious challenge for VQA models to understand the causal nexus of object semantics in images and correctly infer contextual information. To this end, we propose a finer model framework without bounding boxes in this work, termed Looking Out of Instance Semantics (LOIS) to tackle this important issue. LOIS enables more fine-grained feature descriptions to produce visual facts. Furthermore, to overcome the label ambiguity caused by instance masks, two types of relation attention modules: 1) intra-modality and 2) inter-modality, are devised to infer the correct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#22870;&#21169;&#30340;&#20998;&#27573;&#31283;&#23450;&#32452;&#21512;&#21322;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19978;&#30028;&#32622;&#20449;&#24230;&#31639;&#27861;&#20197;&#24212;&#23545;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#32452;&#37325;&#21551;&#30340;&#27010;&#24565;&#20316;&#20026;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#22791;&#20221;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.14138</link><description>&lt;p&gt;
&#20998;&#27573;&#31283;&#23450;&#32452;&#21512;&#21322;&#24378;&#30423;&#38382;&#39064;&#21450;&#22240;&#26524;&#20851;&#31995;&#22870;&#21169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.14138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#22870;&#21169;&#30340;&#20998;&#27573;&#31283;&#23450;&#32452;&#21512;&#21322;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19978;&#30028;&#32622;&#20449;&#24230;&#31639;&#27861;&#20197;&#24212;&#23545;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#32452;&#37325;&#21551;&#30340;&#27010;&#24565;&#20316;&#20026;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#22791;&#20221;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#22870;&#21169;&#30340;&#20998;&#27573;&#31283;&#23450;&#32452;&#21512;&#21322;&#24378;&#30423;&#38382;&#39064;&#12290;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#65292;&#22522;&#26412;&#33218;&#30340;&#20998;&#24067;&#21464;&#21270;&#12289;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25110;&#32773;&#20004;&#32773;&#21516;&#26102;&#25913;&#21464;&#22870;&#21169;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#30340;&#20915;&#31574;&#32773;&#24517;&#39035;&#36319;&#38543;&#36825;&#20004;&#20010;&#21464;&#21270;&#28304;&#65292;&#24182;&#30456;&#24212;&#22320;&#36827;&#34892;&#36866;&#24212;&#12290;&#22312;&#32452;&#21512;&#21322;&#24378;&#30423;&#35774;&#32622;&#20013;&#65292;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#20915;&#31574;&#32773;&#21482;&#35266;&#23519;&#21040;&#25152;&#36873;&#33218;&#32452;&#21512;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#26680;&#24515;&#26159;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;Upper Confidence Bound, UCB&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#20195;&#29702;&#20381;&#38752;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#21464;&#28857;&#26816;&#27979;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#37325;&#21551;&#30340;&#27010;&#24565;&#20316;&#20026;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26032;&#22411;&#22791;&#20221;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25972;&#21512;&#20102;&#19968;&#20010;&#36319;&#36394;&#26426;&#21046;&#20197;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards. In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process. In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly. The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms. The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We assume the agent relies on an adaptive approach to overcome the challenge. More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio (GLR) test. Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments. Finally, our algorithm integrates a mechanism to trace the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#23567;&#22411;&#21040;&#20013;&#22411;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#65292;&#22312;&#22635;&#34917;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#26041;&#38754;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#25104;&#26524;&#65292;&#24182;&#20026;&#24320;&#21457;&#21644;&#24212;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.14134</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#35780;&#20272;&#23567;&#22411;&#21040;&#20013;&#22411;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models. (arXiv:2307.14134v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#23567;&#22411;&#21040;&#20013;&#22411;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#65292;&#22312;&#22635;&#34917;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#26041;&#38754;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#25104;&#26524;&#65292;&#24182;&#20026;&#24320;&#21457;&#21644;&#24212;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#23567;&#22411;&#12289;&#36855;&#20320;&#22411;&#12289;&#23567;&#22411;&#21644;&#20013;&#22411;&#30340;&#26080;&#22823;&#23567;&#20889;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#65292;&#26088;&#22312;&#22635;&#34917;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;75GB&#20197;&#19978;&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#25513;&#30721;&#39044;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#26032;&#38395;&#20998;&#31867;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26356;&#24555;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#22303;&#32819;&#20854;&#35821;&#22659;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces and evaluates tiny, mini, small, and medium-sized uncased Turkish BERT models, aiming to bridge the research gap in less-resourced languages. We trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and, zero-shot classification. Despite their smaller size, our models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times. Our findings provide valuable insights into the development and application of smaller language models, especially in the context of the Turkish language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#35821;&#20041;&#20026;&#39537;&#21160;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26631;&#27880;&#26041;&#27861;vTelos&#65292;&#21033;&#29992;WordNet&#20316;&#20026;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#26631;&#31614;&#21547;&#20041;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#20943;&#23569;&#20027;&#35266;&#36873;&#25321;&#65292;&#25552;&#39640;&#26631;&#27880;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14119</link><description>&lt;p&gt;
&#19968;&#31181;&#20197;&#35821;&#20041;&#20026;&#39537;&#21160;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26631;&#27880;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A semantics-driven methodology for high-quality image annotation. (arXiv:2307.14119v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#35821;&#20041;&#20026;&#39537;&#21160;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26631;&#27880;&#26041;&#27861;vTelos&#65292;&#21033;&#29992;WordNet&#20316;&#20026;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#26631;&#31614;&#21547;&#20041;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#20943;&#23569;&#20027;&#35266;&#36873;&#25321;&#65292;&#25552;&#39640;&#26631;&#27880;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#24050;&#32463;&#31361;&#20986;&#20102;&#23384;&#22312;&#20110;&#30495;&#23454;&#23545;&#35937;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#21508;&#31181;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#35266;&#28857;&#26159;&#65292;&#36825;&#20123;&#32570;&#38519;&#26681;&#28304;&#20110;&#22270;&#20687;&#20013;&#32534;&#30721;&#30340;&#35270;&#35273;&#20449;&#24687;&#19982;&#26631;&#27880;&#20854;&#30340;&#35821;&#20041;&#20043;&#38388;&#23384;&#22312;&#30340;&#22810;&#23545;&#22810;&#26144;&#23556;&#12290;&#20854;&#26368;&#32456;&#32467;&#26524;&#26159;&#24403;&#21069;&#30340;&#26631;&#27880;&#36807;&#31243;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#19981;&#22815;&#26126;&#30830;&#30340;&#65292;&#22240;&#27492;&#32473;&#26631;&#27880;&#32773;&#30340;&#20027;&#35266;&#21028;&#26029;&#30041;&#19979;&#20102;&#36807;&#22810;&#30340;&#33258;&#30001;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;vTelos&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#26174;&#24615;&#22320;&#21576;&#29616;&#65288;&#21542;&#21017;&#38544;&#24615;&#30340;&#65289;&#26082;&#23450;&#26631;&#27880;&#35821;&#20041;&#65292;&#20174;&#32780;&#20943;&#23569;&#20027;&#35266;&#36873;&#25321;&#30340;&#25968;&#37327;&#21644;&#20316;&#29992;&#12290;vTelos&#30340;&#19968;&#20010;&#20851;&#38190;&#35201;&#32032;&#26159;&#21033;&#29992;WordNet&#35789;&#27719;-&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#26631;&#31614;&#21547;&#20041;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26631;&#27880;&#35821;&#20041;&#30340;&#20934;&#30830;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in Machine Learning and Computer Vision has highlighted the presence of various types of systematic flaws inside ground truth object recognition benchmark datasets. Our basic tenet is that these flaws are rooted in the many-to-many mappings which exist between the visual information encoded in images and the intended semantics of the labels annotating them. The net consequence is that the current annotation process is largely under-specified, thus leaving too much freedom to the subjective judgment of annotators. In this paper, we propose vTelos, an integrated Natural Language Processing, Knowledge Representation, and Computer Vision methodology whose main goal is to make explicit the (otherwise implicit) intended annotation semantics, thus minimizing the number and role of subjective choices. A key element of vTelos is the exploitation of the WordNet lexico-semantic hierarchy as the main means for providing the meaning of natural language labels and, as a consequence, for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GraphRNN&#36827;&#34892;&#20102;&#22797;&#29616;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;You&#31561;&#20154;&#24314;&#35758;&#30340;BFS&#36941;&#21382;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;BFS&#36941;&#21382;&#26367;&#25442;&#20026;&#25299;&#25169;&#25490;&#24207;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;GraphRNN&#20197;&#29983;&#25104;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.14109</link><description>&lt;p&gt;
GraphRNN&#20877;&#25506;&#65306;&#28040;&#34701;&#30740;&#31350;&#21644;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs. (arXiv:2307.14109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GraphRNN&#36827;&#34892;&#20102;&#22797;&#29616;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;You&#31561;&#20154;&#24314;&#35758;&#30340;BFS&#36941;&#21382;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;BFS&#36941;&#21382;&#26367;&#25442;&#20026;&#25299;&#25169;&#25490;&#24207;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;GraphRNN&#20197;&#29983;&#25104;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GraphRNN&#26159;&#30001;You&#31561;&#20154;&#25552;&#20986;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#37325;&#26032;&#23454;&#29616;&#30340;GraphRNN&#26550;&#26500;&#22797;&#29616;&#20102;You&#31561;&#20154;&#30340;&#32467;&#26524;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#25351;&#26631;&#23558;&#20854;&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;You&#31561;&#20154;&#24314;&#35758;&#30340;BFS&#36941;&#21382;&#20197;&#21512;&#24182;&#21516;&#26500;&#22270;&#30340;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26367;&#25442;BFS&#36941;&#21382;&#20026;&#25299;&#25169;&#25490;&#24207;&#65292;&#23558;GraphRNN&#25193;&#23637;&#20026;&#29983;&#25104;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;GraphRNN&#30340;&#26377;&#21521;&#22810;&#31867;&#21035;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
GraphRNN is a deep learning-based architecture proposed by You et al. for learning generative models for graphs. We replicate the results of You et al. using a reproduced implementation of the GraphRNN architecture and evaluate this against baseline models using new metrics. Through an ablation study, we find that the BFS traversal suggested by You et al. to collapse representations of isomorphic graphs contributes significantly to model performance. Additionally, we extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. We demonstrate that this method improves significantly over a directed-multiclass variant of GraphRNN on a real-world dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37327;&#21270;&#26031;&#22374;&#20811;&#20271;&#26684;&#22343;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30465;&#26679;&#26412;&#37327;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25512;&#26029;&#36861;&#38543;&#32773;&#30340;&#34892;&#21160;&#26469;&#23398;&#20064;&#37327;&#21270;&#21709;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14085</link><description>&lt;p&gt;
&#34892;&#21160;&#32988;&#20110;&#35328;&#36766;&#65306;&#35777;&#26126;&#20102;&#20174;&#31574;&#30053;&#21453;&#39304;&#20013;&#30465;&#26679;&#26412;&#37327;&#30340;&#37327;&#21270;&#26031;&#22374;&#20811;&#20271;&#26684;&#22343;&#34913;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks. (arXiv:2307.14085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37327;&#21270;&#26031;&#22374;&#20811;&#20271;&#26684;&#22343;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30465;&#26679;&#26412;&#37327;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25512;&#26029;&#36861;&#38543;&#32773;&#30340;&#34892;&#21160;&#26469;&#23398;&#20064;&#37327;&#21270;&#21709;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20855;&#26377;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#32467;&#26500;&#30340;&#24773;&#22659;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#23398;&#20064;&#37327;&#21270;&#26031;&#22374;&#20811;&#20271;&#26684;&#22343;&#34913;&#65288;QSE&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#22312;&#28216;&#25103;&#24320;&#22987;&#26102;&#65292;&#39046;&#23548;&#32773;&#23459;&#24067;&#22905;&#30340;&#31574;&#30053;&#24182;&#25215;&#35834;&#25191;&#34892;&#12290;&#36861;&#38543;&#32773;&#35266;&#23519;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#65292;&#28982;&#21518;&#37319;&#21462;&#37327;&#21270;&#21709;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#35299;&#20915;&#30001;&#39046;&#23548;&#32773;&#31574;&#30053;&#24341;&#21457;&#30340;&#29109;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#26469;&#30830;&#23450;&#12290;&#39046;&#23548;&#32773;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19982;&#36861;&#38543;&#32773;&#30340;&#20132;&#20114;&#24182;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#25214;&#21040;&#33258;&#24049;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#20248;&#30340;&#39044;&#26399;&#24635;&#22238;&#25253;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#39046;&#23548;&#32773;&#26080;&#27861;&#35266;&#23519;&#21040;&#36861;&#38543;&#32773;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#36861;&#38543;&#32773;&#23545;&#25239;&#39046;&#23548;&#32773;&#31574;&#30053;&#30340;&#34892;&#21160;&#20013;&#25512;&#26029;&#20986;&#36861;&#38543;&#32773;&#30340;&#37327;&#21270;&#21709;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20989;&#25968;&#36924;&#36817;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#35774;&#32622;&#30340;&#26679;&#26412;&#25928;&#29575;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#65288;i&#65289;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#23398;&#20064;&#37327;&#21270;&#21709;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We study reinforcement learning (RL) for learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure. In specific, at the outset of the game, the leader announces her policy to the follower and commits to it. The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy. The goal of the leader is to find her optimal policy, which yields the optimal expected total return, by interacting with the follower and learning from data. A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies. We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation. Our algorithms are based on (i) learning the quantal response model via maximum likelihood 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#20869;&#23481;&#38750;&#23448;&#26041;&#35299;&#38500;&#21644;&#37325;&#26500;&#65288;ICDR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24694;&#24847;&#36719;&#20214;&#20351;&#29992;&#22270;&#20687;&#36827;&#34892;&#38544;&#34255;&#21644;&#38544;&#20889;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14057</link><description>&lt;p&gt;
&#24320;&#25918;&#22270;&#20687;&#20869;&#23481;&#38750;&#23448;&#26041;&#35299;&#38500;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Open Image Content Disarm And Reconstruction. (arXiv:2307.14057v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#20869;&#23481;&#38750;&#23448;&#26041;&#35299;&#38500;&#21644;&#37325;&#26500;&#65288;ICDR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24694;&#24847;&#36719;&#20214;&#20351;&#29992;&#22270;&#20687;&#36827;&#34892;&#38544;&#34255;&#21644;&#38544;&#20889;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#25915;&#20987;&#32773;&#21019;&#36896;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#38544;&#34255;&#20854;&#24694;&#24847;&#20195;&#30721;&#20197;&#36530;&#36991;&#26432;&#27602;&#26381;&#21153;&#12290;&#19968;&#31181;&#27169;&#31946;&#25915;&#20987;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#24120;&#35268;&#25991;&#20214;&#20316;&#20026;&#25513;&#30422;&#65292;&#38544;&#34255;&#24694;&#24847;&#33050;&#26412;&#65292;&#20351;&#24694;&#24847;&#36719;&#20214;&#30475;&#36215;&#26469;&#20687;&#19968;&#20010;&#21512;&#27861;&#30340;&#25991;&#20214;&#12290;&#23613;&#31649;&#26377;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20869;&#23481;&#31614;&#21517;&#25216;&#26415;&#23384;&#22312;&#65292;&#20294;&#36867;&#36991;&#24615;&#24694;&#24847;&#36719;&#20214;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#19979;&#19968;&#20195;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#20351;&#29992;&#35832;&#22914;&#38544;&#20889;&#26415;&#20043;&#31867;&#30340;&#39640;&#32423;&#26041;&#27861;&#12290;&#19968;&#20123;&#24120;&#29992;&#20110;&#38544;&#34255;&#24694;&#24847;&#36719;&#20214;&#30340;&#25991;&#20214;&#26159;&#22270;&#20687;&#25991;&#20214;&#65288;&#22914;JPEG&#65289;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#24694;&#24847;&#36719;&#20214;&#20351;&#29992;&#38544;&#20889;&#26415;&#23558;&#24694;&#24847;&#33050;&#26412;&#25110;&#25935;&#24863;&#25968;&#25454;&#38544;&#34255;&#22312;&#22270;&#20687;&#20013;&#12290;&#21363;&#20351;&#20351;&#29992;&#19987;&#38376;&#30340;&#24037;&#20855;&#65292;&#38590;&#20197;&#26816;&#27979;&#20986;&#22270;&#20687;&#20013;&#30340;&#38544;&#20889;&#26415;&#12290;&#22522;&#20110;&#22270;&#20687;&#30340;&#25915;&#20987;&#35797;&#22270;&#20351;&#29992;&#24694;&#24847;&#24377;&#33647;&#25915;&#20987;&#29992;&#25143;&#35774;&#22791;&#65292;&#25110;&#32773;&#21033;&#29992;&#22270;&#20687;&#38544;&#20889;&#26415;&#23558;&#25935;&#24863;&#25968;&#25454;&#38544;&#34255;&#22312;&#21512;&#27861;&#22270;&#20687;&#20013;&#24182;&#22312;&#29992;&#25143;&#35774;&#22791;&#20043;&#22806;&#27844;&#38706;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#20869;&#23481;&#38750;&#23448;&#26041;&#35299;&#38500;&#21644;&#37325;&#26500;&#65288;ICDR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advance in malware technology, attackers create new ways to hide their malicious code from antivirus services. One way to obfuscate an attack is to use common files as cover to hide the malicious scripts, so the malware will look like a legitimate file. Although cutting-edge Artificial Intelligence and content signature exist, evasive malware successfully bypasses next-generation malware detection using advanced methods like steganography. Some of the files commonly used to hide malware are image files (e.g., JPEG). In addition, some malware use steganography to hide malicious scripts or sensitive data in images. Steganography in images is difficult to detect even with specialized tools. Image-based attacks try to attack the user's device using malicious payloads or utilize image steganography to hide sensitive data inside legitimate images and leak it outside the user's device. Therefore in this paper, we present a novel Image Content Disarm and Reconstruction (ICDR). Our ICD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20960;&#20309;&#32467;&#26500;&#19968;&#33268;&#24615;&#30340;&#25429;&#25417;&#21644;&#26368;&#36817;&#37051;&#28857;&#20113;&#30340;&#29983;&#25104;&#26469;&#25552;&#39640;&#20869;&#28857;&#20272;&#35745;&#21644;&#21305;&#37197;&#32622;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14019</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;&#20013;&#30340;&#26368;&#36817;&#37051;&#25351;&#23548;&#20869;&#28857;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
One-Nearest Neighborhood Guides Inlier Estimation for Unsupervised Point Cloud Registration. (arXiv:2307.14019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20960;&#20309;&#32467;&#26500;&#19968;&#33268;&#24615;&#30340;&#25429;&#25417;&#21644;&#26368;&#36817;&#37051;&#28857;&#20113;&#30340;&#29983;&#25104;&#26469;&#25552;&#39640;&#20869;&#28857;&#20272;&#35745;&#21644;&#21305;&#37197;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#30340;&#31934;&#24230;&#36890;&#24120;&#21463;&#21487;&#38752;&#20869;&#28857;&#20272;&#35745;&#21644;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#38480;&#21046;&#65292;&#23588;&#20854;&#22312;&#37096;&#20998;&#37325;&#21472;&#30340;&#24773;&#20917;&#19979;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;&#20869;&#28857;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#28304;&#28857;&#20113;&#21644;&#20854;&#23545;&#24212;&#30340;&#21442;&#32771;&#28857;&#20113;&#21103;&#26412;&#20043;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#28857;&#20113;&#21103;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#36755;&#20837;&#28857;&#20113;&#29983;&#25104;&#19968;&#20010;&#26368;&#36817;&#37051;&#65288;1-NN&#65289;&#28857;&#20113;&#12290;&#36825;&#26377;&#21161;&#20110;&#21305;&#37197;&#22270;&#26500;&#24314;&#65292;&#24182;&#20801;&#35768;&#23558;1-NN&#28857;&#20113;&#21644;&#36755;&#20837;&#28857;&#20113;&#30340;&#21452;&#37051;&#22495;&#21305;&#37197;&#24471;&#20998;&#25972;&#21512;&#65292;&#20197;&#25552;&#39640;&#21305;&#37197;&#32622;&#20449;&#24230;&#12290;&#22522;&#20110;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#21103;&#26412;&#65292;&#25105;&#20204;&#35748;&#20026;&#20869;&#28857;&#21450;&#20854;&#37051;&#22495;&#24418;&#25104;&#30340;&#37051;&#22495;&#22270;&#24212;&#35813;&#22312;&#28304;&#28857;&#20113;&#21644;&#23545;&#24212;&#30340;&#21442;&#32771;&#21103;&#26412;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#28857;&#20272;&#35745;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#28857;&#20113;&#37197;&#20934;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precision of unsupervised point cloud registration methods is typically limited by the lack of reliable inlier estimation and self-supervised signal, especially in partially overlapping scenarios. In this paper, we propose an effective inlier estimation method for unsupervised point cloud registration by capturing geometric structure consistency between the source point cloud and its corresponding reference point cloud copy. Specifically, to obtain a high quality reference point cloud copy, an One-Nearest Neighborhood (1-NN) point cloud is generated by input point cloud. This facilitates matching map construction and allows for integrating dual neighborhood matching scores of 1-NN point cloud and input point cloud to improve matching confidence. Benefiting from the high quality reference copy, we argue that the neighborhood graph formed by inlier and its neighborhood should have consistency between source point cloud and its corresponding reference copy. Based on this observation, 
&lt;/p&gt;</description></item><item><title>ESSAformer&#26159;&#19968;&#31181;&#29992;&#20110;&#21333;&#19968;&#36229;&#20809;&#35889;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#39640;&#25928;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#31283;&#20581;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#26680;&#21270;&#27880;&#24847;&#21147;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;CNN-based&#26041;&#27861;&#22312;&#20809;&#35889;&#20449;&#24687;&#21033;&#29992;&#21644;&#20266;&#24433;&#38382;&#39064;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.14010</link><description>&lt;p&gt;
ESSAformer: &#39640;&#25928;&#36229;&#20809;&#35889;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution. (arXiv:2307.14010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14010
&lt;/p&gt;
&lt;p&gt;
ESSAformer&#26159;&#19968;&#31181;&#29992;&#20110;&#21333;&#19968;&#36229;&#20809;&#35889;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#39640;&#25928;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#31283;&#20581;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#26680;&#21270;&#27880;&#24847;&#21147;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;CNN-based&#26041;&#27861;&#22312;&#20809;&#35889;&#20449;&#24687;&#21033;&#29992;&#21644;&#20266;&#24433;&#38382;&#39064;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#19968;&#36229;&#20809;&#35889;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;Single-HSI-SR&#65289;&#26088;&#22312;&#20174;&#20302;&#20998;&#36776;&#29575;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#39640;&#20998;&#36776;&#29575;&#36229;&#20809;&#35889;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#22312;&#26500;&#24314;&#38271;&#31243;&#20381;&#36182;&#24615;&#21644;&#25429;&#25417;&#20809;&#35889;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#23548;&#33268;&#20809;&#35889;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#24182;&#19988;&#22312;&#25918;&#22823;&#21518;&#20135;&#29983;&#20266;&#24433;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ESSAformer&#65292;&#19968;&#31181;&#23884;&#20837;ESSA&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#22120;&#32593;&#32476;&#65292;&#29992;&#20110;&#21333;&#19968;&#36229;&#20809;&#35889;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65292;&#24182;&#20855;&#26377;&#36845;&#20195;&#20248;&#21270;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#31283;&#20581;&#19988;&#19982;&#20809;&#35889;&#21451;&#22909;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21363;&#35889;&#30456;&#20851;&#31995;&#25968;&#65288;SCC&#65289;&#65292;&#20197;&#21462;&#20195;&#21407;&#22987;&#30340;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#24182;&#23558;&#24402;&#32435;&#20559;&#32622;&#24341;&#20837;&#27169;&#22411;&#20197;&#20419;&#36827;&#35757;&#32451;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#20855;&#26377;&#29702;&#35770;&#25903;&#25345;&#30340;&#21487;&#26680;&#21270;&#27880;&#24847;&#21147;&#25216;&#26415;&#24418;&#25104;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;SCC&#26680;&#33258;&#27880;&#24847;&#21147;&#65288;ESSA&#65289;&#65292;&#24182;&#20943;&#23569;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a high-resolution hyperspectral image from a low-resolution observation. However, the prevailing CNN-based approaches have shown limitations in building long-range dependencies and capturing interaction information between spectral features. This results in inadequate utilization of spectral information and artifacts after upsampling. To address this issue, we propose ESSAformer, an ESSA attention-embedded Transformer network for single-HSI-SR with an iterative refining structure. Specifically, we first introduce a robust and spectral-friendly similarity metric, \ie, the spectral correlation coefficient of the spectrum (SCC), to replace the original attention matrix and incorporates inductive biases into the model to facilitate training. Built upon it, we further utilize the kernelizable attention technique with theoretical support to form a novel efficient SCC-kernel-based self-attention (ESSA) and reduce atte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#19982;&#28508;&#22312;&#31354;&#38388; GAN &#38598;&#25104;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#30340;&#22522;&#30784;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.13978</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046; GAN &#30340;&#28508;&#22312;&#31354;&#38388;&#65306;&#22522;&#20110;&#20219;&#21153;&#30340;&#22270;&#20687;&#32763;&#35793;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation. (arXiv:2307.13978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#19982;&#28508;&#22312;&#31354;&#38388; GAN &#38598;&#25104;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#30340;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#36924;&#30495;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046; GAN &#29983;&#25104;&#36807;&#31243;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#19982;&#28508;&#22312;&#31354;&#38388; GAN&#65288;l-GAN&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#25104;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24102;&#26377;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#31574;&#30053;&#30340; actor-critic RL &#20195;&#29702;&#65292;&#20351;&#20854;&#33021;&#22815;&#29087;&#32451;&#22320;&#22312; l-GAN &#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#24182;&#26681;&#25454;&#25351;&#23450;&#30340;&#20219;&#21153;&#29983;&#25104;&#36755;&#20986;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20351;&#29992; MNIST &#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#31639;&#26415;&#21152;&#27861;&#20316;&#20026;&#19968;&#20010;&#35828;&#26126;&#24615;&#20219;&#21153;&#12290;&#36825;&#20123;&#23454;&#39564;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#23558; RL &#20195;&#29702;&#19982; GAN &#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to generate realistic outputs based on training datasets. However, the challenge of exerting control over the generation process of GANs remains a significant hurdle. In this paper, we propose a novel methodology to address this issue by integrating a reinforcement learning (RL) agent with a latent-space GAN (l-GAN), thereby facilitating the generation of desired outputs. More specifically, we have developed an actor-critic RL agent with a meticulously designed reward policy, enabling it to acquire proficiency in navigating the latent space of the l-GAN and generating outputs based on specified tasks. To substantiate the efficacy of our approach, we have conducted a series of experiments employing the MNIST dataset, including arithmetic addition as an illustrative task. The outcomes of these experiments serve to validate our methodology. Our pioneering integration of an RL agent with a GAN model represents a nov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#26469;&#30740;&#31350;&#20854;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#31243;&#24230;&#19982;&#32593;&#32476;&#35757;&#32451;&#24615;&#33021;&#26377;&#21516;&#27493;&#24615;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#28608;&#27963;&#20989;&#25968;&#21644;&#32593;&#32476;&#23610;&#23544;&#23545;&#38544;&#34255;&#23618;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13962</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#34255;&#23618;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Neural Networks via Linear Separability of Hidden Layers. (arXiv:2307.13962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#26469;&#30740;&#31350;&#20854;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#31243;&#24230;&#19982;&#32593;&#32476;&#35757;&#32451;&#24615;&#33021;&#26377;&#21516;&#27493;&#24615;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#28608;&#27963;&#20989;&#25968;&#21644;&#32593;&#32476;&#23610;&#23544;&#23545;&#38544;&#34255;&#23618;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#26469;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#38389;&#21487;&#22827;&#26031;&#22522;&#24046;&#24322;&#30340;&#32447;&#24615;&#21487;&#20998;&#24230;&#37327;&#65288;MD-LSMs&#65289;&#26469;&#35780;&#20272;&#20004;&#20010;&#28857;&#38598;&#30340;&#32447;&#24615;&#21487;&#20998;&#31243;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#31243;&#24230;&#19982;&#32593;&#32476;&#35757;&#32451;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#21516;&#27493;&#24615;&#65292;&#21363;&#22914;&#26524;&#26356;&#26032;&#30340;&#26435;&#37325;&#33021;&#22815;&#25552;&#39640;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#31243;&#24230;&#65292;&#37027;&#20040;&#26356;&#26032;&#21518;&#30340;&#32593;&#32476;&#23558;&#23454;&#29616;&#26356;&#22909;&#30340;&#35757;&#32451;&#24615;&#33021;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#32593;&#32476;&#23610;&#23544;&#65288;&#21253;&#25324;&#23485;&#24230;&#21644;&#28145;&#24230;&#65289;&#23545;&#38544;&#34255;&#23618;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#19968;&#20123;&#27969;&#34892;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#28145;&#24230;&#32622;&#20449;&#32593;&#32476;&#65288;DBN&#65289;&#12289;ResNet&#12289;VGGNet&#12289;AlexNet&#12289;vision tran&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision tran
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#36229;&#20986;&#20998;&#24067;(OOD)&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;OOD&#25968;&#25454;&#19978;&#20351;&#29992;&#25193;&#25955;&#24494;&#35843;PLMs&#20250;&#38477;&#20302;&#37325;&#24314;&#33021;&#21147;&#12290;&#27604;&#36739;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;OOD&#26679;&#26412;&#30340;&#37325;&#26500;&#33021;&#21147;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13949</link><description>&lt;p&gt;
&#25193;&#25955;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Diffusion Influence Pretrained Language Models on Out-of-Distribution Data?. (arXiv:2307.13949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13949
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#36229;&#20986;&#20998;&#24067;(OOD)&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;OOD&#25968;&#25454;&#19978;&#20351;&#29992;&#25193;&#25955;&#24494;&#35843;PLMs&#20250;&#38477;&#20302;&#37325;&#24314;&#33021;&#21147;&#12290;&#27604;&#36739;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;OOD&#26679;&#26412;&#30340;&#37325;&#26500;&#33021;&#21147;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;PLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#20248;&#21183;&#26159;&#33391;&#22909;&#30340;&#36229;&#20986;&#20998;&#24067;(OOD)&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#21560;&#24341;&#20102;&#24456;&#22810;&#30740;&#31350;&#23558;&#25193;&#25955;&#24212;&#29992;&#20110;PLMs&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#23545;PLMs&#22312;OOD&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#36880;&#28176;&#23545;&#36755;&#20837;&#24212;&#29992;&#39640;&#26031;&#22122;&#22768;&#65292;&#24182;&#19988;&#19968;&#20010;&#21453;&#21521;&#21435;&#22122;&#36807;&#31243;&#65292;&#29992;&#20110;&#21435;&#38500;&#22122;&#22768;&#12290;&#22122;&#22768;&#36755;&#20837;&#37325;&#24314;&#26159;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#37325;&#24314;&#25439;&#22833;&#30452;&#25509;&#20998;&#26512;OOD&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#27979;&#35797;&#37325;&#24314;OOD&#25968;&#25454;&#21644;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#30340;&#35757;&#32451;&#21442;&#25968;&#21644;&#25968;&#25454;&#32479;&#35745;&#29305;&#24449;&#22312;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#25193;&#25955;&#24494;&#35843;PLMs&#20250;&#38477;&#20302;&#22312;OOD&#25968;&#25454;&#19978;&#30340;&#37325;&#24314;&#33021;&#21147;&#12290;&#27604;&#36739;&#36824;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;OOD&#26679;&#26412;&#30340;&#37325;&#26500;&#33021;&#21147;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pretrained language models (PLMs) have achieved great success in modern NLP. An important advantage of PLMs is good out-of-distribution (OOD) robustness. Recently, diffusion models have attracted a lot of work to apply diffusion to PLMs. It remains under-explored how diffusion influences PLMs on OOD data. The core of diffusion models is a forward diffusion process which gradually applies Gaussian noise to inputs, and a reverse denoising process which removes noise. The noised input reconstruction is a fundamental ability of diffusion models. We directly analyze OOD robustness by measuring the reconstruction loss, including testing the abilities to reconstruct OOD data, and to detect OOD samples. Experiments are conducted by analyzing different training parameters and data statistical features on eight datasets. It shows that finetuning PLMs with diffusion degrades the reconstruction ability on OOD data. The comparison also shows that diffusion models can effectively d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#30340;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#29992;&#20110;PMSM&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#25511;&#21046;&#24863;&#30693;&#26368;&#20248;&#32858;&#21512;&#31574;&#30053;&#65292;&#24182;&#20165;&#21033;&#29992;&#21518;&#39564;&#22343;&#20540;&#36827;&#34892;&#35745;&#31639;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#19982;&#21518;&#39564;&#26041;&#24046;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.13945</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#30340;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#29992;&#20110;PMSM
&lt;/p&gt;
&lt;p&gt;
Learning-based Control for PMSM Using Distributed Gaussian Processes with Optimal Aggregation Strategy. (arXiv:2307.13945v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#30340;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#29992;&#20110;PMSM&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#25511;&#21046;&#24863;&#30693;&#26368;&#20248;&#32858;&#21512;&#31574;&#30053;&#65292;&#24182;&#20165;&#21033;&#29992;&#21518;&#39564;&#22343;&#20540;&#36827;&#34892;&#35745;&#31639;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#19982;&#21518;&#39564;&#26041;&#24046;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31934;&#30830;&#25511;&#21046;&#22312;&#19981;&#21516;&#21644;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#23545;&#30005;&#28304;&#20379;&#24212;&#32452;&#20214;&#30340;&#35201;&#27714;&#20063;&#30456;&#24212;&#22686;&#21152;&#65292;&#20854;&#20013;&#21253;&#25324;&#27704;&#30913;&#21516;&#27493;&#30005;&#26426;&#65288;Permanent Magnet Synchronous Motors, PMSMs&#65289;&#12290;&#20026;&#20102;&#25512;&#26029;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#37096;&#20998;&#65292;&#24191;&#27867;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;Gaussian Process Regression, GPR&#65289;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28789;&#27963;&#30340;&#36830;&#32493;&#31995;&#32479;&#24314;&#27169;&#33021;&#21147;&#21644;&#20445;&#35777;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#38469;&#23454;&#29616;&#65292;&#37319;&#29992;&#20102;&#20998;&#24067;&#24335;GPR&#26469;&#20943;&#36731;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25511;&#21046;&#30340;&#35282;&#24230;&#30740;&#31350;&#20998;&#24067;&#24335;GPR&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Lyapunov&#31283;&#23450;&#24615;&#29702;&#35770;&#30340;&#29992;&#20110;PMSMs&#30340;&#25511;&#21046;&#24863;&#30693;&#26368;&#20248;&#32858;&#21512;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#20165;&#21033;&#29992;&#21518;&#39564;&#22343;&#20540;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#19982;&#21518;&#39564;&#26041;&#24046;&#30456;&#20851;&#30340;&#35745;&#31639;&#23494;&#38598;&#24615;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing demand for accurate control in varying and unknown environments has sparked a corresponding increase in the requirements for power supply components, including permanent magnet synchronous motors (PMSMs). To infer the unknown part of the system, machine learning techniques are widely employed, especially Gaussian process regression (GPR) due to its flexibility of continuous system modeling and its guaranteed performance. For practical implementation, distributed GPR is adopted to alleviate the high computational complexity. However, the study of distributed GPR from a control perspective remains an open problem. In this paper, a control-aware optimal aggregation strategy of distributed GPR for PMSMs is proposed based on the Lyapunov stability theory. This strategy exclusively leverages the posterior mean, thereby obviating the need for computationally intensive calculations associated with posterior variance in alternative approaches. Moreover, the straightforward calculati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#31070;&#32463;&#20272;&#35745;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#19979;&#30028;&#26469;&#36817;&#20284;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#29109;&#12290;&#36890;&#36807;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23376;&#38598;&#25277;&#26679;&#31574;&#30053;&#23545;&#27604;&#25968;&#25454;&#38598;&#19981;&#21516;&#35270;&#22270;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#30446;&#26631;&#20248;&#21270;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.13944</link><description>&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#29109;&#31070;&#32463;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Entropy Neural Estimation for Graph Contrastive Learning. (arXiv:2307.13944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#31070;&#32463;&#20272;&#35745;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#19979;&#30028;&#26469;&#36817;&#20284;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#29109;&#12290;&#36890;&#36807;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23376;&#38598;&#25277;&#26679;&#31574;&#30053;&#23545;&#27604;&#25968;&#25454;&#38598;&#19981;&#21516;&#35270;&#22270;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#30446;&#26631;&#20248;&#21270;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#25552;&#21462;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#30340;&#39640;&#23618;&#34920;&#31034;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25968;&#25454;&#38598;&#30340;&#29109;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#19981;&#21516;&#35270;&#22270;&#19979;&#30340;&#20114;&#20449;&#24687;&#19979;&#30028;&#26469;&#36817;&#20284;&#20272;&#35745;&#65292;&#21363;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#29109;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23376;&#38598;&#25277;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#23545;&#27604;&#25968;&#25454;&#38598;&#21508;&#35270;&#22270;&#20043;&#38388;&#30340;&#25104;&#23545;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38543;&#26426;&#20174;&#32473;&#23450;&#30340;&#22270;&#20013;&#25277;&#26679;&#33410;&#28857;&#21644;&#36793;&#26469;&#26500;&#24314;&#35270;&#22270;&#30340;&#36755;&#20837;&#23376;&#38598;&#12290;&#20004;&#20010;&#35270;&#22270;&#34987;&#36755;&#20837;&#21040;&#21442;&#25968;&#20849;&#20139;&#30340;&#36830;&#20307;&#32593;&#32476;&#20013;&#65292;&#20197;&#25552;&#21462;&#39640;&#32500;&#23884;&#20837;&#24182;&#20272;&#35745;&#25972;&#20010;&#22270;&#30340;&#20449;&#24687;&#29109;&#12290;&#23545;&#20110;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#30446;&#26631;&#20248;&#21270;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#36755;&#20837;&#30001;&#27491;&#36127;&#23545;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#23545;&#27604;&#23545;&#31574;&#30053;&#19982;&#20197;&#21069;&#30340;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#22312;&#32593;&#32476;&#28216;&#25103;&#20013;&#30340;&#31283;&#23450;&#24615;&#65292;&#21457;&#29616;&#20102;&#22312;&#20219;&#20309;&#32593;&#32476;&#28216;&#25103;&#20013;&#23454;&#29616;&#21160;&#21147;&#23398;&#25910;&#25947;&#21040;&#21807;&#19968;&#22343;&#34913;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#32593;&#32476;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20219;&#24847;&#25968;&#37327;&#20195;&#29702;&#30340;&#31283;&#23450;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2307.13922</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;&#65306;&#22312;&#20855;&#26377;&#22810;&#20010;&#29609;&#23478;&#30340;&#32593;&#32476;&#28216;&#25103;&#20013;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability of Multi-Agent Learning: Convergence in Network Games with Many Players. (arXiv:2307.13922v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#22312;&#32593;&#32476;&#28216;&#25103;&#20013;&#30340;&#31283;&#23450;&#24615;&#65292;&#21457;&#29616;&#20102;&#22312;&#20219;&#20309;&#32593;&#32476;&#28216;&#25103;&#20013;&#23454;&#29616;&#21160;&#21147;&#23398;&#25910;&#25947;&#21040;&#21807;&#19968;&#22343;&#34913;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#32593;&#32476;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20219;&#24847;&#25968;&#37327;&#20195;&#29702;&#30340;&#31283;&#23450;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#22312;&#35768;&#22810;&#29609;&#23478;&#28216;&#25103;&#20013;&#30340;&#34892;&#20026;&#26174;&#31034;&#20986;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#65292;&#36229;&#20986;&#20102;&#38480;&#21046;&#24615;&#31034;&#20363;&#65288;&#22914;&#32593;&#32476;&#38646;&#21644;&#28216;&#25103;&#65289;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#35777;&#26126;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#25910;&#25947;&#34892;&#20026;&#21464;&#24471;&#19981;&#22826;&#21487;&#33021;&#21457;&#29983;&#12290;&#20026;&#20102;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#36827;&#23637;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Q-Learning&#21160;&#21147;&#23398;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20219;&#20309;&#32593;&#32476;&#28216;&#25103;&#20013;&#21160;&#21147;&#23398;&#25910;&#25947;&#20110;&#21807;&#19968;&#22343;&#34913;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#26465;&#20214;&#21462;&#20915;&#20110;&#25104;&#23545;&#20132;&#20114;&#30340;&#24615;&#36136;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20294;&#26126;&#30830;&#19982;&#28216;&#25103;&#20013;&#30340;&#24635;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20010;&#32467;&#26524;&#22312;&#19968;&#20123;&#20195;&#34920;&#24615;&#30340;&#32593;&#32476;&#28216;&#25103;&#19978;&#65292;&#24182;&#34920;&#26126;&#22312;&#36866;&#24403;&#30340;&#32593;&#32476;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20219;&#24847;&#25968;&#37327;&#20195;&#29702;&#30340;&#31283;&#23450;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
The behaviour of multi-agent learning in many player games has been shown to display complex dynamics outside of restrictive examples such as network zero-sum games. In addition, it has been shown that convergent behaviour is less likely to occur as the number of players increase. To make progress in resolving this problem, we study Q-Learning dynamics and determine a sufficient condition for the dynamics to converge to a unique equilibrium in any network game. We find that this condition depends on the nature of pairwise interactions and on the network structure, but is explicitly independent of the total number of agents in the game. We evaluate this result on a number of representative network games and show that, under suitable network conditions, stable learning dynamics can be achieved with an arbitrary number of agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20250;&#31185;&#23398;&#26500;&#36896;&#36716;&#21270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#27665;&#20027;&#20215;&#20540;&#35266;&#23884;&#20837;&#31038;&#20132;&#23186;&#20307;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#19968;&#20010;&#24212;&#29992;&#20110;&#21453;&#27665;&#20027;&#24577;&#24230;&#30340;&#27169;&#22411;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20250;&#31185;&#23398;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#23450;&#24615;&#32534;&#30721;&#25163;&#20876;&#65292;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36716;&#21270;&#36825;&#20123;&#26500;&#36896;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13912</link><description>&lt;p&gt;
&#23558;&#27665;&#20027;&#20215;&#20540;&#35266;&#23884;&#20837;&#31038;&#20132;&#23186;&#20307;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31038;&#20250;&#23458;&#35266;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Embedding Democratic Values into Social Media AIs via Societal Objective Functions. (arXiv:2307.13912v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20250;&#31185;&#23398;&#26500;&#36896;&#36716;&#21270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#27665;&#20027;&#20215;&#20540;&#35266;&#23884;&#20837;&#31038;&#20132;&#23186;&#20307;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#19968;&#20010;&#24212;&#29992;&#20110;&#21453;&#27665;&#20027;&#24577;&#24230;&#30340;&#27169;&#22411;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20250;&#31185;&#23398;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#23450;&#24615;&#32534;&#30721;&#25163;&#20876;&#65292;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36716;&#21270;&#36825;&#20123;&#26500;&#36896;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20351;&#20854;&#32771;&#34385;&#21040;&#27665;&#20027;&#20215;&#20540;&#35266;&#65292;&#22914;&#20943;&#23569;&#20826;&#27966;&#25932;&#24847;&#65292;&#20316;&#20026;&#20854;&#30446;&#26631;&#20989;&#25968;&#26469;&#25490;&#21517;&#25105;&#20204;&#30340;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#27969;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#24050;&#24314;&#31435;&#12289;&#32463;&#23457;&#26597;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#36896;&#36716;&#21270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31038;&#20250;&#23458;&#35266;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#21453;&#27665;&#20027;&#24577;&#24230;&#36825;&#19968;&#25919;&#27835;&#31185;&#23398;&#26500;&#36896;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#25105;&#20204;&#32570;&#20047;&#21487;&#35266;&#23519;&#30340;&#32467;&#26524;&#26469;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#32780;&#31038;&#20250;&#31185;&#23398;&#24050;&#32463;&#24320;&#21457;&#20102;&#35843;&#26597;&#24037;&#20855;&#21644;&#23450;&#24615;&#32534;&#30721;&#25163;&#20876;&#65292;&#29992;&#20110;&#36825;&#20123;&#26500;&#36896;&#30340;&#32763;&#35793;&#65292;&#20854;&#31934;&#30830;&#24615;&#20415;&#20110;&#23558;&#20854;&#36716;&#21270;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35814;&#32454;&#25552;&#31034;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#27665;&#20027;&#24577;&#24230;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#23459;&#20256;&#21453;&#27665;&#20027;&#24577;&#24230;&#30340;&#31243;&#24230;&#65292;&#24182;&#22312;&#19977;&#20010;&#30740;&#31350;&#20013;&#27979;&#35797;&#20102;&#36825;&#20010;&#27665;&#20027;&#24577;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we design artificial intelligence (AI) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#26143;&#26143;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#21464;&#38271;&#26102;&#38388;&#24207;&#21015;&#36755;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#39046;&#22495;&#30340;SOC&#20272;&#35745;&#21644;RUL&#20272;&#35745;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.13907</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26143;&#26143;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#21464;&#38271;&#26102;&#38388;&#24207;&#21015;&#36755;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input. (arXiv:2307.13907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#26143;&#26143;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#21464;&#38271;&#26102;&#38388;&#24207;&#21015;&#36755;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#39046;&#22495;&#30340;SOC&#20272;&#35745;&#21644;RUL&#20272;&#35745;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#32500;&#25252;&#26159;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;NN&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#36807;&#21435;&#34892;&#20026;&#21644;&#20851;&#38190;&#21442;&#25968;&#65288;&#22914;&#35774;&#22791;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#21644;&#30005;&#27744;&#30340;&#33655;&#30005;&#29366;&#24577;&#65288;SOC&#65289;&#65289;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#36755;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#32463;&#36807;&#20256;&#24863;&#22120;&#26102;&#21487;&#33021;&#36973;&#21463;&#26377;&#24847;&#25110;&#26080;&#24847;&#30340;&#22122;&#22768;&#24178;&#25200;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#20123;NN&#36827;&#34892;&#40065;&#26834;&#24615;&#39564;&#35777;&#21644;&#39564;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;NN&#65288;TSRegNN&#65289;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#26041;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#37319;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#24418;&#24335;&#26041;&#27861;&#12290;&#23427;&#30528;&#37325;&#20110;&#21033;&#29992;&#21464;&#38271;&#36755;&#20837;&#25968;&#25454;&#26469;&#31616;&#21270;&#36755;&#20837;&#25805;&#20316;&#24182;&#22686;&#24378;&#32593;&#32476;&#26550;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#65288;1&#65289;&#38146;&#31163;&#23376;&#30005;&#27744;SOC&#20272;&#35745;&#21644;&#65288;2&#65289;&#28065;&#36718;&#21457;&#21160;&#26426;RUL&#20272;&#35745;&#12290;&#23545;NN&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#26680;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven, neural network (NN) based anomaly detection and predictive maintenance are emerging research areas. NN-based analytics of time-series data offer valuable insights into past behaviors and estimates of critical parameters like remaining useful life (RUL) of equipment and state-of-charge (SOC) of batteries. However, input time series data can be exposed to intentional or unintentional noise when passing through sensors, necessitating robust validation and verification of these NNs. This paper presents a case study of the robustness verification approach for time series regression NNs (TSRegNN) using set-based formal methods. It focuses on utilizing variable-length input data to streamline input manipulation and enhance network architecture generalizability. The method is applied to two data sets in the Prognostics and Health Management (PHM) application areas: (1) SOC estimation of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs' robustness is checke
&lt;/p&gt;</description></item><item><title>FinTree&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#20854;&#37319;&#29992;&#20102;&#39044;&#27979;&#25513;&#30721;&#26631;&#35760;&#30340;&#26032;&#39062;&#32467;&#26500;&#65292;&#36890;&#36807;&#35757;&#32451;&#25552;&#20379;&#19978;&#19979;&#25991;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#37329;&#34701;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.13900</link><description>&lt;p&gt;
FinTree&#65306;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction. (arXiv:2307.13900v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13900
&lt;/p&gt;
&lt;p&gt;
FinTree&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#20854;&#37319;&#29992;&#20102;&#39044;&#27979;&#25513;&#30721;&#26631;&#35760;&#30340;&#26032;&#39062;&#32467;&#26500;&#65292;&#36890;&#36807;&#35757;&#32451;&#25552;&#20379;&#19978;&#19979;&#25991;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#37329;&#34701;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FinTree&#65292;&#20351;&#29992;&#37329;&#34701;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#21033;&#29992;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#37329;&#34701;&#39046;&#22495;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;FinTree&#12290;FinTree&#20197;&#20854;&#39044;&#27979;&#25513;&#30721;&#26631;&#35760;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;[CLS]&#26631;&#35760;&#30340;&#26032;&#39062;&#32467;&#26500;&#32780;&#33073;&#39062;&#32780;&#20986;&#65292;&#21463;&#21040;&#27169;&#24335;&#21033;&#29992;&#35757;&#32451;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20004;&#20010;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19968;&#31181;&#29420;&#29305;&#30340;&#36755;&#20837;&#27169;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25552;&#20379;&#26377;&#20851;&#25152;&#20851;&#27880;&#23454;&#20307;&#30340;&#19978;&#19979;&#25991;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21518;&#22788;&#29702;&#27493;&#39588;&#30830;&#20445;&#19982;&#23454;&#20307;&#31867;&#22411;&#19968;&#33268;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;FinTree&#22312;&#22823;&#35268;&#27169;&#37329;&#34701;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;REFinD&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/HJ-Ok/FinTree&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FinTree, Financial Dataset Pretrain Transformer Encoder for Relation Extraction. Utilizing an encoder language model, we further pretrain FinTree on the financial dataset, adapting the model in financial domain tasks. FinTree stands out with its novel structure that predicts a masked token instead of the conventional [CLS] token, inspired by the Pattern Exploiting Training methodology. This structure allows for more accurate relation predictions between two given entities. The model is trained with a unique input pattern to provide contextual and positional information about the entities of interest, and a post-processing step ensures accurate predictions in line with the entity types. Our experiments demonstrate that FinTree outperforms on the REFinD, a large-scale financial relation extraction dataset. The code and pretrained models are available at https://github.com/HJ-Ok/FinTree.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13899</link><description>&lt;p&gt;
&#29992;&#20803;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Regularizing Neural Networks with Meta-Learning Generative Models. (arXiv:2307.13899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#26679;&#26412;&#20316;&#20026;&#23567;&#25968;&#25454;&#38598;&#20998;&#31867;&#30340;&#39069;&#22806;&#25968;&#25454;&#38598;&#12290;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#21512;&#25104;&#25968;&#25454;&#20013;&#21253;&#21547;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26080;&#20449;&#24687;&#26679;&#26412;&#12290;&#36825;&#26159;&#22240;&#20026;&#21512;&#25104;&#26679;&#26412;&#19981;&#33021;&#23436;&#32654;&#22320;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#65292;&#22343;&#21248;&#25277;&#26679;&#20063;&#19981;&#19968;&#23450;&#20026;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#36991;&#20813;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#30340;&#38477;&#32423;&#65292;MGR&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#20132;&#21449;&#29109;&#12290;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#36890;&#36807;&#20803;&#23398;&#20064;&#21160;&#24577;&#30830;&#23450;&#65292;&#20197;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#27668;&#20505;&#21464;&#21270;&#35780;&#20272;&#27169;&#22411;RICE-N&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20851;&#38190;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#65292;&#21516;&#26102;&#20063;&#23545;&#32508;&#21512;&#35780;&#20272;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#21457;&#23637;RICE-N&#26694;&#26550;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.13894</link><description>&lt;p&gt;
AI4GCC - &#22242;&#38431;: &#20302;&#20110;&#28023;&#24179;&#38754;: &#35780;&#35770;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
AI4GCC - Team: Below Sea Level: Critiques and Improvements. (arXiv:2307.13894v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#27668;&#20505;&#21464;&#21270;&#35780;&#20272;&#27169;&#22411;RICE-N&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20851;&#38190;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#65292;&#21516;&#26102;&#20063;&#23545;&#32508;&#21512;&#35780;&#20272;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#21457;&#23637;RICE-N&#26694;&#26550;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#27169;&#25311;&#26694;&#26550;RICE-N&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#27668;&#20505;&#21464;&#21270;&#23545;&#32463;&#27982;&#24433;&#21709;&#30340;&#32508;&#21512;&#35780;&#20272;&#27169;&#22411;(IAM)&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;RICE-N&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#34892;&#21160;&#23631;&#34109;&#21644;&#26080;&#20851;&#30340;&#34892;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#24314;&#35758;&#65292;&#22914;&#21033;&#29992;&#20851;&#31246;&#25910;&#20837;&#21644;&#24809;&#32602;&#36807;&#37327;&#29983;&#20135;&#12290;&#25105;&#20204;&#36824;&#25209;&#21028;&#24615;&#22320;&#35752;&#35770;&#20102;IAM&#30340;&#29305;&#24449;&#65292;&#21363;&#36807;&#20110;&#20048;&#35266;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#19981;&#29616;&#23454;&#30340;&#20943;&#25490;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#19981;&#26029;&#25913;&#36827;RICE-N&#26694;&#26550;&#65292;&#20351;&#20854;&#20316;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#30340;&#28789;&#24863;&#26356;&#21152;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a critical analysis of the simulation framework RICE-N, an integrated assessment model (IAM) for evaluating the impacts of climate change on the economy. We identify key issues with RICE-N, including action masking and irrelevant actions, and suggest improvements such as utilizing tariff revenue and penalizing overproduction. We also critically engage with features of IAMs in general, namely overly optimistic damage functions and unrealistic abatement cost functions. Our findings contribute to the ongoing efforts to further develop the RICE-N framework in an effort to improve the simulation, making it more useful as an inspiration for policymakers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29616;&#23454;&#19990;&#30028;&#19994;&#21153;&#21644;&#25919;&#27835;&#35848;&#21028;&#21327;&#35758;&#30340;&#21160;&#24577;&#20998;&#32452;&#35848;&#21028;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#26045;&#32452;&#24418;&#25104;&#26041;&#27861;&#21644;&#32452;&#26356;&#26032;&#31574;&#30053;&#35299;&#20915;&#20102;&#22810;&#22320;&#21306;&#27668;&#20505;&#35848;&#21028;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20419;&#36827;&#21508;&#21033;&#30410;&#30456;&#20851;&#26041;&#20043;&#38388;&#30340;&#39640;&#25928;&#21512;&#20316;&#20197;&#23454;&#29616;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.13893</link><description>&lt;p&gt;
&#21160;&#24577;&#20998;&#32452;&#29992;&#20110;&#27668;&#20505;&#21464;&#21270;&#35848;&#21028;&#65306;&#36890;&#36807;&#26377;&#25928;&#31574;&#30053;&#20419;&#36827;&#21512;&#20316;&#21644;&#24179;&#34913;&#21033;&#30410;
&lt;/p&gt;
&lt;p&gt;
Dynamic Grouping for Climate Change Negotiation: Facilitating Cooperation and Balancing Interests through Effective Strategies. (arXiv:2307.13893v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29616;&#23454;&#19990;&#30028;&#19994;&#21153;&#21644;&#25919;&#27835;&#35848;&#21028;&#21327;&#35758;&#30340;&#21160;&#24577;&#20998;&#32452;&#35848;&#21028;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#26045;&#32452;&#24418;&#25104;&#26041;&#27861;&#21644;&#32452;&#26356;&#26032;&#31574;&#30053;&#35299;&#20915;&#20102;&#22810;&#22320;&#21306;&#27668;&#20505;&#35848;&#21028;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20419;&#36827;&#21508;&#21033;&#30410;&#30456;&#20851;&#26041;&#20043;&#38388;&#30340;&#39640;&#25928;&#21512;&#20316;&#20197;&#23454;&#29616;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29616;&#23454;&#19990;&#30028;&#19994;&#21153;&#21644;&#25919;&#27835;&#35848;&#21028;&#21327;&#35758;&#30340;&#27668;&#20505;&#20943;&#32531;&#21160;&#24577;&#20998;&#32452;&#35848;&#21028;&#27169;&#22411;&#12290;&#22312;AI4GCC&#31454;&#36187;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#27969;&#31243;&#65306;&#32452;&#24418;&#25104;&#21644;&#26356;&#26032;&#12289;&#32452;&#20869;&#35848;&#21028;&#21644;&#32452;&#38388;&#35848;&#21028;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20419;&#36827;&#21508;&#21033;&#30410;&#30456;&#20851;&#26041;&#20043;&#38388;&#30340;&#39640;&#25928;&#21512;&#20316;&#65292;&#20197;&#23454;&#29616;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#26045;&#32452;&#24418;&#25104;&#26041;&#27861;&#21644;&#32452;&#26356;&#26032;&#31574;&#30053;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22810;&#22320;&#21306;&#27668;&#20505;&#35848;&#21028;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#32452;&#20869;&#35848;&#21028;&#30830;&#20445;&#25152;&#26377;&#25104;&#21592;&#37117;&#20026;&#20943;&#32531;&#21162;&#21147;&#20570;&#20986;&#36129;&#29486;&#65292;&#32780;&#32452;&#38388;&#35848;&#21028;&#21017;&#20351;&#29992;&#25552;&#26696;&#35780;&#20272;&#26694;&#26550;&#26469;&#21046;&#23450;&#20943;&#32531;&#21644;&#33410;&#32422;&#29575;&#12290;&#25105;&#20204;&#22312;RICE-N&#26694;&#26550;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35848;&#21028;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20419;&#36827;&#22269;&#38469;&#21512;&#20316;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#20943;&#32531;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a dynamic grouping negotiation model for climate mitigation based on real-world business and political negotiation protocols. Within the AI4GCC competition framework, we develop a three-stage process: group formation and updates, intra-group negotiation, and inter-group negotiation. Our model promotes efficient and effective cooperation between various stakeholders to achieve global climate change objectives. By implementing a group-forming method and group updating strategy, we address the complexities and imbalances in multi-region climate negotiations. Intra-group negotiations ensure that all members contribute to mitigation efforts, while inter-group negotiations use the proposal-evaluation framework to set mitigation and savings rates. We demonstrate our negotiation model within the RICE-N framework, illustrating a promising approach for facilitating international cooperation on climate change mitigation.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35848;&#21028;&#21327;&#35758;&#26469;&#35299;&#20915;&#30899;&#27844;&#28431;&#38382;&#39064;&#65292;&#36890;&#36807;&#19982;&#20195;&#34920;&#24615;&#27987;&#24230;&#36335;&#24452;&#21644;&#20849;&#20139;&#31038;&#20250;&#32463;&#27982;&#36335;&#24452;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#27492;&#22806;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#21327;&#35758;&#30340;&#21512;&#35268;&#24615;&#12289;&#21487;&#34892;&#24615;&#21644;&#20262;&#29702;&#20851;&#20999;&#12290;</title><link>http://arxiv.org/abs/2307.13892</link><description>&lt;p&gt;
AI4GCC - &#22242;&#38431;: &#28023;&#24179;&#38754;&#20197;&#19979;: &#35780;&#20998;&#21644;&#23454;&#38469;&#19990;&#30028;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI4GCC - Team: Below Sea Level: Score and Real World Relevance. (arXiv:2307.13892v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13892
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35848;&#21028;&#21327;&#35758;&#26469;&#35299;&#20915;&#30899;&#27844;&#28431;&#38382;&#39064;&#65292;&#36890;&#36807;&#19982;&#20195;&#34920;&#24615;&#27987;&#24230;&#36335;&#24452;&#21644;&#20849;&#20139;&#31038;&#20250;&#32463;&#27982;&#36335;&#24452;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#27492;&#22806;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#21327;&#35758;&#30340;&#21512;&#35268;&#24615;&#12289;&#21487;&#34892;&#24615;&#21644;&#20262;&#29702;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#25105;&#20204;&#21442;&#21152;AI for Global Climate Cooperation (AI4GCC)&#31454;&#36187;&#30340;&#31532;&#19977;&#39033;&#36319;&#36394;&#20219;&#21153;&#30340;&#25552;&#20132;&#65292;&#25105;&#20204;&#38024;&#23545;RICE-N&#27668;&#20505;&#32463;&#27982;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#31181;&#35848;&#21028;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#36890;&#36807;&#21463;&#21040;&#30899;&#36793;&#22659;&#35843;&#25972;&#26426;&#21046;(CBAM)&#21644;&#27668;&#20505;&#20465;&#20048;&#37096;(CC)&#21551;&#21457;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30899;&#27844;&#28431;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27169;&#25311;&#32467;&#26524;&#19982;&#20195;&#34920;&#24615;&#27987;&#24230;&#36335;&#24452;(RCP)&#21644;&#20849;&#20139;&#31038;&#20250;&#32463;&#27982;&#36335;&#24452;(SSP)&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#21327;&#35758;&#23548;&#33268;&#20102;&#19982;RCP 3.4/4.5&#21644;SSP 2&#30456;&#24403;&#30340;&#28201;&#24230;&#19978;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#21327;&#35758;&#30340;&#19990;&#30028;&#36152;&#26131;&#32452;&#32455;&#21512;&#35268;&#24615;&#12289;&#34892;&#25919;&#21644;&#25919;&#27835;&#21487;&#34892;&#24615;&#20197;&#21450;&#20262;&#29702;&#20851;&#20999;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#33021;&#20250;&#23545;&#26368;&#19981;&#21457;&#36798;&#22269;&#23478;&#36896;&#25104;&#20260;&#23475;&#65292;&#22240;&#27492;&#25105;&#20204;&#24314;&#35758;&#37319;&#21462;&#29305;&#23450;&#30340;&#32416;&#27491;&#25514;&#26045;&#65292;&#36991;&#20813;&#21152;&#21095;&#29616;&#26377;&#30340;&#19981;&#24179;&#31561;&#65292;&#20363;&#22914;&#25216;&#26415;&#20849;&#20139;&#21644;&#36130;&#23500;&#20877;&#20998;&#37197;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#25913;&#36827;...
&lt;/p&gt;
&lt;p&gt;
As our submission for track three of the AI for Global Climate Cooperation (AI4GCC) competition, we propose a negotiation protocol for use in the RICE-N climate-economic simulation. Our proposal seeks to address the challenges of carbon leakage through methods inspired by the Carbon Border Adjustment Mechanism (CBAM) and Climate Clubs (CC). We demonstrate the effectiveness of our approach by comparing simulated outcomes to representative concentration pathways (RCP) and shared socioeconomic pathways (SSP). Our protocol results in a temperature rise comparable to RCP 3.4/4.5 and SSP 2. Furthermore, we provide an analysis of our protocol's World Trade Organization compliance, administrative and political feasibility, and ethical concerns. We recognize that our proposal risks hurting the least developing countries, and we suggest specific corrective measures to avoid exacerbating existing inequalities, such as technology sharing and wealth redistribution. Future research should improve th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#30446;&#30340;&#22312;&#20110;&#25913;&#36827;&#27668;&#20505;&#21464;&#21270;&#35848;&#21028;&#27169;&#22411;&#65292;&#29305;&#21035;&#38598;&#20013;&#22312;&#22320;&#29702;&#24433;&#21709;&#21644;&#25928;&#29992;&#26694;&#26550;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22320;&#29702;&#24433;&#21709;&#30340;&#20116;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#24182;&#24378;&#35843;&#20102;&#23436;&#21892;&#25928;&#29992;&#21644;&#22870;&#21169;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13886</link><description>&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#35848;&#21028;&#30340;&#21160;&#24577;&#20998;&#32452;&#65306;&#36890;&#36807;&#26377;&#25928;&#31574;&#30053;&#20419;&#36827;&#21512;&#20316;&#21644;&#24179;&#34913;&#21033;&#30410;
&lt;/p&gt;
&lt;p&gt;
Dynamic Grouping for Climate Change Negotiation: Facilitating Cooperation and Balancing Interests through Effective Strategies. (arXiv:2307.13886v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#30446;&#30340;&#22312;&#20110;&#25913;&#36827;&#27668;&#20505;&#21464;&#21270;&#35848;&#21028;&#27169;&#22411;&#65292;&#29305;&#21035;&#38598;&#20013;&#22312;&#22320;&#29702;&#24433;&#21709;&#21644;&#25928;&#29992;&#26694;&#26550;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22320;&#29702;&#24433;&#21709;&#30340;&#20116;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#24182;&#24378;&#35843;&#20102;&#23436;&#21892;&#25928;&#29992;&#21644;&#22870;&#21169;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27668;&#20505;&#21464;&#21270;&#35848;&#21028;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#25105;&#20204;&#20027;&#35201;&#35752;&#35770;&#20102;&#20004;&#20010;&#20851;&#38190;&#25913;&#36827;&#39046;&#22495;&#65292;&#37325;&#28857;&#25918;&#22312;&#22320;&#29702;&#24433;&#21709;&#21644;&#25928;&#29992;&#26694;&#26550;&#19978;&#12290;&#22312;&#22320;&#29702;&#24433;&#21709;&#26041;&#38754;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20116;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#65288;1&#65289;&#20174;&#23616;&#37096;&#21040;&#20840;&#29699;&#24433;&#21709;&#30340;&#36716;&#21464;&#65292;&#65288;2&#65289;&#19981;&#21516;&#22320;&#21306;&#27668;&#20505;&#21464;&#21270;&#25928;&#24212;&#30340;&#21464;&#24322;&#24615;&#65292;&#65288;3&#65289;&#22320;&#29702;&#20301;&#32622;&#21644;&#25919;&#27835;&#32467;&#26500;&#30340;&#24322;&#36136;&#24615;&#65292;&#65288;4&#65289;&#30456;&#37051;&#22269;&#23478;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#65288;5&#65289;&#21253;&#25324;&#21382;&#21490;&#21644;&#25991;&#21270;&#22240;&#32032;&#23545;&#27668;&#20505;&#35848;&#21028;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#38656;&#35201;&#23436;&#21892;&#25928;&#29992;&#21644;&#22870;&#21169;&#26694;&#26550;&#65292;&#20943;&#23569;&#19968;&#33268;&#24615;&#21644;&#39640;&#20272;&#27668;&#20505;&#32531;&#35299;&#31243;&#24230;&#30340;&#38382;&#39064;&#65292;&#23558;&#20648;&#33988;&#29575;&#30340;&#27491;&#38754;&#25928;&#24212;&#32435;&#20837;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#25152;&#26377;&#22320;&#21306;&#30340;&#24322;&#36136;&#24615;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current framework for climate change negotiation models presents several limitations that warrant further research and development. In this track, we discuss mainly two key areas for improvement, focusing on the geographical impacts and utility framework. In the aspects of geographical impacts, We explore five critical aspects: (1) the shift from local to global impact, (2) variability in climate change effects across regions, (3) heterogeneity in geographical location and political structures, and (4) collaborations between adjacent nations, (5) the importance of including historical and cultural factors influencing climate negotiations. Furthermore, we emphasize the need to refine the utility and rewards framework to reduce the homogeneity and the level of overestimating the climate mitigation by integrating the positive effects of saving rates into the reward function and heterogeneity among all regions. By addressing these limitations, we hope to enhance the accuracy and effect
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>MAEA&#26159;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#20219;&#20309;&#21487;&#24494;&#20998;&#30340;&#22810;&#27169;&#24577;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#30340;&#20840;&#23616;&#24402;&#22240;&#30340;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#26469;&#25490;&#21517;&#21644;&#20998;&#32452;&#25925;&#38556;&#22330;&#26223;&#65292;&#35843;&#26597;&#24314;&#27169;&#21644;&#25968;&#25454;&#38598;&#20559;&#35265;&#65292;&#24182;&#23545;&#22810;&#27169;&#24577;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#30340;&#31283;&#20581;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#36827;&#34892;&#20851;&#38190;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.13850</link><description>&lt;p&gt;
MAEA: &#22522;&#20110;&#22810;&#27169;&#24577;&#30340;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#30340;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
MAEA: Multimodal Attribution for Embodied AI. (arXiv:2307.13850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13850
&lt;/p&gt;
&lt;p&gt;
MAEA&#26159;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#20219;&#20309;&#21487;&#24494;&#20998;&#30340;&#22810;&#27169;&#24577;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#30340;&#20840;&#23616;&#24402;&#22240;&#30340;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#26469;&#25490;&#21517;&#21644;&#20998;&#32452;&#25925;&#38556;&#22330;&#26223;&#65292;&#35843;&#26597;&#24314;&#27169;&#21644;&#25968;&#25454;&#38598;&#20559;&#35265;&#65292;&#24182;&#23545;&#22810;&#27169;&#24577;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#30340;&#31283;&#20581;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#36827;&#34892;&#20851;&#38190;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22810;&#27169;&#24577;&#24863;&#30693;&#23545;&#20110;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#36755;&#20837;&#21487;&#33021;&#21253;&#21547;&#39640;&#24230;&#20114;&#34917;&#21644;&#20887;&#20313;&#30340;&#20449;&#24687;&#29992;&#20110;&#20219;&#21153;&#12290;&#22810;&#27169;&#24577;&#25919;&#31574;&#30340;&#19968;&#20010;&#30456;&#20851;&#26041;&#21521;&#26159;&#29702;&#35299;&#34701;&#21512;&#23618;&#20013;&#27599;&#31181;&#27169;&#24577;&#30340;&#20840;&#23616;&#36235;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;ALFRED&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#19981;&#21516;&#25919;&#31574;&#19978;&#35299;&#24320;&#20102;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#20808;&#21069;&#21160;&#20316;&#36755;&#20837;&#30340;&#24402;&#22240;&#12290;&#24402;&#22240;&#20998;&#26512;&#21487;&#20197;&#29992;&#20110;&#25490;&#21517;&#21644;&#20998;&#32452;&#25925;&#38556;&#22330;&#26223;&#12289;&#35843;&#26597;&#24314;&#27169;&#21644;&#25968;&#25454;&#38598;&#20559;&#35265;&#65292;&#24182;&#22312;&#37096;&#32626;&#20043;&#21069;&#23545;&#22810;&#27169;&#24577;EAI&#25919;&#31574;&#30340;&#31283;&#20581;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#36827;&#34892;&#20851;&#38190;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MAEA&#65292;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#20219;&#20309;&#21487;&#24494;&#20998;&#25919;&#31574;&#30340;&#27599;&#20010;&#27169;&#24577;&#30340;&#20840;&#23616;&#24402;&#22240;&#30340;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24402;&#22240;&#22914;&#20309;&#22312;EAI&#25919;&#31574;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#24402;&#22240;&#20013;&#21551;&#29992;&#26356;&#24213;&#23618;&#30340;&#34892;&#20026;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25972;&#25968;&#31639;&#26415;&#32467;&#26500;&#25193;&#23637;&#27010;&#29575;&#32534;&#31243;&#20013;&#25972;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#36827;&#21046;&#32534;&#30721;&#21644;&#30693;&#35782;&#32534;&#35793;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#27010;&#29575;&#25512;&#29702;&#65292;&#33021;&#22815;&#22788;&#29702;&#35268;&#27169;&#26356;&#22823;&#30340;&#25972;&#25968;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.13837</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#32534;&#31243;&#20013;&#30340;&#25972;&#25968;&#31639;&#26415;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Scaling Integer Arithmetic in Probabilistic Programs. (arXiv:2307.13837v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25972;&#25968;&#31639;&#26415;&#32467;&#26500;&#25193;&#23637;&#27010;&#29575;&#32534;&#31243;&#20013;&#25972;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#36827;&#21046;&#32534;&#30721;&#21644;&#30693;&#35782;&#32534;&#35793;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#27010;&#29575;&#25512;&#29702;&#65292;&#33021;&#22815;&#22788;&#29702;&#35268;&#27169;&#26356;&#22823;&#30340;&#25972;&#25968;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#25968;&#20998;&#24067;&#22312;&#27010;&#29575;&#24314;&#27169;&#20013;&#38750;&#24120;&#24120;&#35265;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#24403;&#20170;&#30340;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26680;&#24515;&#25361;&#25112;&#26469;&#33258;&#20110;&#31163;&#25955;&#32467;&#26500;&#65306;&#24403;&#20170;&#30340;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#25512;&#29702;&#31574;&#30053;&#24448;&#24448;&#20381;&#36182;&#20110;&#26522;&#20030;&#12289;&#37319;&#26679;&#25110;&#24494;&#20998;&#26469;&#36827;&#34892;&#25193;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#28041;&#21450;&#25972;&#25968;&#30340;&#39640;&#32500;&#22797;&#26434;&#31163;&#25955;&#20998;&#24067;&#19978;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#21040;&#31639;&#26415;&#20013;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#20998;&#24067;&#30340;&#20108;&#36827;&#21046;&#32534;&#30721;&#31574;&#30053;&#65292;&#21033;&#29992;&#20102;&#25972;&#25968;&#36816;&#31639;&#65288;&#22914;&#27714;&#21644;&#21644;&#27604;&#36739;&#65289;&#30340;&#20016;&#23500;&#36923;&#36753;&#32467;&#26500;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#21270;&#32534;&#30721;&#21644;&#30693;&#35782;&#32534;&#35793;&#36827;&#34892;&#31934;&#30830;&#30340;&#27010;&#29575;&#25512;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20855;&#26377;&#31639;&#26415;&#30340;&#26356;&#22823;&#25972;&#25968;&#20998;&#24067;&#19978;&#30340;&#25193;&#23637;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributions on integers are ubiquitous in probabilistic modeling but remain challenging for many of today's probabilistic programming languages (PPLs). The core challenge comes from discrete structure: many of today's PPL inference strategies rely on enumeration, sampling, or differentiation in order to scale, which fail for high-dimensional complex discrete distributions involving integers. Our insight is that there is structure in arithmetic that these approaches are not using. We present a binary encoding strategy for discrete distributions that exploits the rich logical structure of integer operations like summation and comparison. We leverage this structured encoding with knowledge compilation to perform exact probabilistic inference, and show that this approach scales to much larger integer distributions with arithmetic.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21521;&#34892;&#20026;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#34892;&#20026;&#31574;&#30053;&#26412;&#36523;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#26469;&#22788;&#29702;&#22806;&#25512;&#35823;&#24046;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13824</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22312;&#32447;Q&#20989;&#25968;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with On-Policy Q-Function Regularization. (arXiv:2307.13824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21521;&#34892;&#20026;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#34892;&#20026;&#31574;&#30053;&#26412;&#36523;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#26469;&#22788;&#29702;&#22806;&#25512;&#35823;&#24046;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;Offline RL&#65289;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#22788;&#29702;&#21382;&#21490;&#25968;&#25454;&#38598;&#19982;&#26399;&#26395;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#25152;&#24341;&#36215;&#30340;&#65288;&#28508;&#22312;&#28798;&#38590;&#24615;&#30340;&#65289;&#22806;&#25512;&#35823;&#24046;&#12290;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#36890;&#36807;&#38544;&#24335;/&#26174;&#24335;&#22320;&#23558;&#23398;&#20064;&#31574;&#30053;&#21521;&#34892;&#20026;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#21487;&#38752;&#22320;&#20272;&#35745;&#34892;&#20026;&#31574;&#30053;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21521;&#34892;&#20026;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#21069;&#25552;&#26159;Q&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;SARSA-style&#20272;&#35745;&#26356;&#21487;&#38752;&#19988;&#26356;&#23481;&#26131;&#22320;&#22788;&#29702;&#22806;&#25512;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21033;&#29992;&#20272;&#35745;&#30340;Q&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy. A large portion of prior work tackles this challenge by implicitly/explicitly regularizing the learning policy towards the behavior policy, which is hard to estimate reliably in practice. In this work, we propose to regularize towards the Q-function of the behavior policy instead of the behavior policy itself, under the premise that the Q-function can be estimated more reliably and easily by a SARSA-style estimate and handles the extrapolation error more straightforwardly. We propose two algorithms taking advantage of the estimated Q-function through regularizations, and demonstrate they exhibit strong performance on the D4RL benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;MuReNN&#65289;&#30340;&#31070;&#32463;&#38899;&#39057;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#30340;&#20843;&#24230;&#23376;&#24102;&#19978;&#35757;&#32451;&#21333;&#29420;&#30340;&#21367;&#31215;&#31639;&#23376;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#38754;&#20020;&#30340;&#38750;&#21442;&#25968;&#21644;&#21442;&#25968;&#26041;&#27861;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2307.13821</link><description>&lt;p&gt;
&#29992;&#22810;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;&#25311;&#21512;&#21548;&#35273;&#28388;&#27874;&#22120;&#32452;
&lt;/p&gt;
&lt;p&gt;
Fitting Auditory Filterbanks with Multiresolution Neural Networks. (arXiv:2307.13821v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;MuReNN&#65289;&#30340;&#31070;&#32463;&#38899;&#39057;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#30340;&#20843;&#24230;&#23376;&#24102;&#19978;&#35757;&#32451;&#21333;&#29420;&#30340;&#21367;&#31215;&#31639;&#23376;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#38754;&#20020;&#30340;&#38750;&#21442;&#25968;&#21644;&#21442;&#25968;&#26041;&#27861;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#38754;&#20020;&#38750;&#21442;&#25968;&#21644;&#21442;&#25968;&#26041;&#27861;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;&#19968;&#26041;&#38754;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;convnets&#65289;&#21487;&#20197;&#36817;&#20284;&#20219;&#20309;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65307;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#30340;&#39057;&#29575;&#21709;&#24212;&#38543;&#30528;&#24863;&#21463;&#37326;&#30340;&#22686;&#38271;&#21464;&#24471;&#26356;&#21152;&#19981;&#35268;&#21017;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35832;&#22914;LEAF&#20043;&#31867;&#30340;&#21442;&#25968;&#27169;&#22411;&#20445;&#35777;&#20135;&#29983;Gabor&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#30340;&#26102;&#39057;&#23450;&#20301;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#19981;&#21033;&#20110;&#34920;&#31034;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;&#22810;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;MuReNN&#65289;&#30340;&#31070;&#32463;&#38899;&#39057;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#19968;&#22256;&#22659;&#12290;MuReNN&#32972;&#21518;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#30340;&#20843;&#24230;&#23376;&#24102;&#19978;&#35757;&#32451;&#21333;&#29420;&#30340;&#21367;&#31215;&#31639;&#23376;&#12290;&#30001;&#20110;DWT&#21407;&#23376;&#30340;&#23610;&#24230;&#22312;&#20843;&#24230;&#20043;&#38388;&#25353;&#25351;&#25968;&#22686;&#38271;&#65292;MuReNN&#20013;&#21518;&#32493;&#21487;&#23398;&#20064;&#21367;&#31215;&#30340;&#24863;&#21463;&#37326;&#20063;&#30456;&#24212;&#25193;&#24352;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25311;&#21512;&#20102;&#24133;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Waveform-based deep learning faces a dilemma between nonparametric and parametric approaches. On one hand, convolutional neural networks (convnets) may approximate any linear time-invariant system; yet, in practice, their frequency responses become more irregular as their receptive fields grow. On the other hand, a parametric model such as LEAF is guaranteed to yield Gabor filters, hence an optimal time-frequency localization; yet, this strong inductive bias comes at the detriment of representational capacity. In this paper, we aim to overcome this dilemma by introducing a neural audio model, named multiresolution neural network (MuReNN). The key idea behind MuReNN is to train separate convolutional operators over the octave subbands of a discrete wavelet transform (DWT). Since the scale of DWT atoms grows exponentially between octaves, the receptive fields of the subsequent learnable convolutions in MuReNN are dilated accordingly. For a given real-world dataset, we fit the magnitude r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Forest Monkey&#65288;FM&#65289;&#24037;&#20855;&#21253;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#29702;&#20219;&#20309;&#22522;&#20110;AI&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;/&#25110;&#20998;&#31867;&#27169;&#22411;&#36755;&#20986;&#32467;&#26524;&#30340;&#24037;&#20855;&#21253;&#12290;&#35813;&#24037;&#20855;&#21253;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#34920;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#25512;&#29702;&#32467;&#26524;&#24182;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.13815</link><description>&lt;p&gt;
ForestMonkey&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#25512;&#29702;&#24037;&#20855;&#21253;&#30340;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ForestMonkey: Toolkit for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.13815v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Forest Monkey&#65288;FM&#65289;&#24037;&#20855;&#21253;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#29702;&#20219;&#20309;&#22522;&#20110;AI&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;/&#25110;&#20998;&#31867;&#27169;&#22411;&#36755;&#20986;&#32467;&#26524;&#30340;&#24037;&#20855;&#21253;&#12290;&#35813;&#24037;&#20855;&#21253;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#34920;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#25512;&#29702;&#32467;&#26524;&#24182;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#35299;&#37322;AI&#27169;&#22411;&#30340;&#39044;&#27979;&#25110;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Forest Monkey&#65288;FM&#65289;&#24037;&#20855;&#21253;&#65292;&#23427;&#26159;&#20026;&#20102;&#25512;&#29702;&#20219;&#20309;&#22522;&#20110;AI&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;/&#25110;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#32780;&#35774;&#35745;&#30340;&#65292;&#24182;&#20855;&#22791;&#25968;&#25454;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#20026;&#19968;&#20010;Python&#36719;&#20214;&#21253;&#23454;&#29616;&#65292;FM&#20197;&#25968;&#25454;&#38598;&#25991;&#20214;&#22841;&#36335;&#24452;&#30340;&#24418;&#24335;&#20316;&#20026;&#36755;&#20837;&#65288;&#21253;&#25324;&#21407;&#22987;&#22270;&#20687;&#12289;&#30495;&#23454;&#26631;&#31614;&#21644;&#39044;&#27979;&#26631;&#31614;&#65289;&#65292;&#24182;&#25552;&#20379;&#19968;&#32452;&#22270;&#34920;&#21644;&#25991;&#26412;&#25991;&#20214;&#20197;&#35828;&#26126;&#25512;&#29702;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#21487;&#33021;&#30340;&#25913;&#36827;&#12290;FM&#24037;&#20855;&#21253;&#21253;&#25324;&#20174;&#39044;&#27979;&#25552;&#21462;&#29305;&#24449;&#21040;&#25512;&#29702;&#30446;&#26631;&#12289;&#20174;&#22270;&#20687;&#25552;&#21462;&#29305;&#24449;&#21040;&#32570;&#38519;&#29305;&#24449;&#20197;&#21450;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;AI&#25512;&#29702;&#22120;&#31561;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;FM&#24037;&#20855;&#21253;&#22312;&#24212;&#29992;&#20110;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;AI&#27169;&#22411;&#26102;&#30340;&#26102;&#38388;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25945;&#31243;&#26469;&#25351;&#23548;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) reasoning and explainable AI (XAI) tasks have gained popularity recently, enabling users to explain the predictions or decision processes of AI models. This paper introduces Forest Monkey (FM), a toolkit designed to reason the outputs of any AI-based defect detection and/or classification model with data explainability. Implemented as a Python package, FM takes input in the form of dataset folder paths (including original images, ground truth labels, and predicted labels) and provides a set of charts and a text file to illustrate the reasoning results and suggest possible improvements. The FM toolkit consists of processes such as feature extraction from predictions to reasoning targets, feature extraction from images to defect characteristics, and a decision tree-based AI-Reasoner. Additionally, this paper investigates the time performance of the FM toolkit when applied to four AI models with different datasets. Lastly, a tutorial is provided to guide users
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2307.13813</link><description>&lt;p&gt;
&#22914;&#20309;&#25193;&#23637;&#24744;&#30340;EMA&#65288;arXiv:2307.13813v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#22312;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#36890;&#24120;&#36890;&#36807;&#19968;&#20010;&#32553;&#25918;&#35268;&#21017;&#26469;&#23454;&#29616;&#65292;&#20363;&#22914;&#65292;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#65292;&#24212;&#35813;&#23558;&#23398;&#20064;&#29575;&#19982;&#25209;&#37327;&#22823;&#23567;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#21478;&#19968;&#20010;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24037;&#20855;&#26159;&#27169;&#22411;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#25509;&#25910;&#26799;&#24230;&#20449;&#24687;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#32780;&#26159;&#20197;&#19968;&#23450;&#30340;&#21160;&#37327;&#36319;&#38543;&#20854;&#30446;&#26631;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;EMA&#21487;&#20197;&#25552;&#39640;&#30417;&#30563;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#31283;&#23450;&#20266;&#26631;&#35760;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;EMA&#19982;&#20248;&#21270;&#20998;&#24320;&#22788;&#29702;&#65292;&#23548;&#33268;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32452;&#20214;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;GPT&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;GPT&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#20154;&#31867;&#25552;&#20379;&#30340;&#35780;&#20272;&#21644;&#24773;&#24863;&#26631;&#31614;&#26174;&#33879;&#19968;&#33268;&#65292;&#20294;&#22312;&#39044;&#27979;&#24773;&#24863;&#24378;&#24230;&#21644;&#24212;&#23545;&#21453;&#24212;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2307.13779</link><description>&lt;p&gt;
GPT&#26159;&#24773;&#24863;&#30340;&#35745;&#31639;&#27169;&#22411;&#21527;&#65311;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is GPT a Computational Model of Emotion? Detailed Analysis. (arXiv:2307.13779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32452;&#20214;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;GPT&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;GPT&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#20154;&#31867;&#25552;&#20379;&#30340;&#35780;&#20272;&#21644;&#24773;&#24863;&#26631;&#31614;&#26174;&#33879;&#19968;&#33268;&#65292;&#20294;&#22312;&#39044;&#27979;&#24773;&#24863;&#24378;&#24230;&#21644;&#24212;&#23545;&#21453;&#24212;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32452;&#20214;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;GPT&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#32771;&#23519;&#20102;&#35813;&#27169;&#22411;&#23545;&#33258;&#20256;&#24335;&#35760;&#24518;&#30340;&#25512;&#29702;&#26041;&#24335;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25913;&#21464;&#24773;&#22659;&#30340;&#21508;&#20010;&#26041;&#38754;&#26469;&#24433;&#21709;&#24773;&#24863;&#24378;&#24230;&#21644;&#24212;&#23545;&#20542;&#21521;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;GPT&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#20154;&#31867;&#25552;&#20379;&#30340;&#35780;&#20272;&#21644;&#24773;&#24863;&#26631;&#31614;&#26174;&#33879;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;GPT&#22312;&#39044;&#27979;&#24773;&#24863;&#24378;&#24230;&#21644;&#24212;&#23545;&#21453;&#24212;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#23613;&#31649;GPT-4&#22312;&#21021;&#27493;&#30740;&#31350;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#20294;&#22312;&#31532;&#20108;&#39033;&#30740;&#31350;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#23613;&#31649;&#32463;&#36807;&#36731;&#24494;&#30340;&#25552;&#31034;&#24037;&#31243;&#21518;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#35780;&#20272;&#24341;&#21457;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#28857;&#24182;&#35299;&#20915;&#20854;&#24369;&#28857;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#21709;&#24212;&#30340;&#21464;&#24322;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#31361;&#26174;&#20102;&#20174;&#32452;&#20214;&#30340;&#35282;&#24230;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the emotional reasoning abilities of the GPT family of large language models via a component perspective. The paper first examines how the model reasons about autobiographical memories. Second, it systematically varies aspects of situations to impact emotion intensity and coping tendencies. Even without the use of prompt engineering, it is shown that GPT's predictions align significantly with human-provided appraisals and emotional labels. However, GPT faces difficulties predicting emotion intensity and coping responses. GPT-4 showed the highest performance in the initial study but fell short in the second, despite providing superior results after minor prompt engineering. This assessment brings up questions on how to effectively employ the strong points and address the weak areas of these models, particularly concerning response variability. These studies underscore the merits of evaluating models from a componential perspective.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;PyTorch&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#22797;&#21046;&#30740;&#31350;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#35780;&#20272;&#38169;&#35823;&#30340;&#21407;&#22240;&#21644;&#30151;&#29366;&#65292;&#25552;&#20379;&#20102;&#23545;&#38169;&#35823;&#35782;&#21035;&#21644;&#20462;&#22797;&#36807;&#31243;&#30340;&#20102;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13777</link><description>&lt;p&gt;
PyTorch&#20869;&#37096;&#30340;&#38169;&#35823;&#65306;&#19968;&#20010;&#22797;&#21046;&#30740;&#31350;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Bugs Inside PyTorch: A Replication Study. (arXiv:2307.13777v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;PyTorch&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#22797;&#21046;&#30740;&#31350;&#65292;&#36890;&#36807;&#35843;&#26597;&#21644;&#35780;&#20272;&#38169;&#35823;&#30340;&#21407;&#22240;&#21644;&#30151;&#29366;&#65292;&#25552;&#20379;&#20102;&#23545;&#38169;&#35823;&#35782;&#21035;&#21644;&#20462;&#22797;&#36807;&#31243;&#30340;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#35782;&#21035;&#22797;&#26434;&#25968;&#25454;&#27169;&#24335;&#21644;&#23454;&#29616;&#26234;&#33021;&#34892;&#20026;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#36719;&#20214;&#31995;&#32479;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#12290;&#36825;&#31181;&#36719;&#20214;&#24320;&#21457;&#21464;&#38761;&#30340;&#26680;&#24515;&#25512;&#21160;&#32773;&#26159;&#26131;&#20110;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#21487;&#29992;&#24615;&#12290;&#20687;PyTorch&#21644;TensorFlow&#36825;&#26679;&#30340;&#24211;&#36171;&#20104;&#21508;&#31181;&#26234;&#33021;&#31995;&#32479;&#20197;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#31639;&#27861;&#21644;&#37197;&#32622;&#36873;&#39033;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21463;&#27426;&#36814;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#30340;&#38169;&#35823;&#20063;&#21487;&#33021;&#23545;&#20854;&#25152;&#25903;&#25345;&#30340;&#31995;&#32479;&#30340;&#36136;&#37327;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#65292;&#22240;&#27492;&#20102;&#35299;&#22914;&#20309;&#22312;&#36825;&#20123;&#24211;&#20013;&#35782;&#21035;&#21644;&#20462;&#22797;&#38169;&#35823;&#38750;&#24120;&#37325;&#35201;&#12290;&#21463;Jia&#31561;&#20154;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;TensorFlow&#20013;&#38169;&#35823;&#30340;&#35782;&#21035;&#21644;&#20462;&#22797;&#36807;&#31243;&#65292;&#25105;&#20204;&#23545;&#38750;&#24120;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;PyTorch&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;PyTorch&#24320;&#21457;&#36807;&#31243;&#20013;&#21457;&#29616;&#30340;&#38169;&#35823;&#30340;&#21407;&#22240;&#21644;&#30151;&#29366;&#65292;&#24182;&#35780;&#20272;&#20102;&#20462;&#22797;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software systems are increasingly relying on deep learning components, due to their remarkable capability of identifying complex data patterns and powering intelligent behaviour. A core enabler of this change in software development is the availability of easy-to-use deep learning libraries. Libraries like PyTorch and TensorFlow empower a large variety of intelligent systems, offering a multitude of algorithms and configuration options, applicable to numerous domains of systems. However, bugs in those popular deep learning libraries also may have dire consequences for the quality of systems they enable; thus, it is important to understand how bugs are identified and fixed in those libraries.  Inspired by a study of Jia et al., which investigates the bug identification and fixing process at TensorFlow, we characterize bugs in the PyTorch library, a very popular deep learning framework. We investigate the causes and symptoms of bugs identified during PyTorch's development, and assess the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#21270;&#26144;&#23556;&#26426;&#21046;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;&#30340;&#22810;&#35821;&#35328;&#35781;&#21650;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13776</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#40784;&#31232;&#30095;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#34920;&#31034;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;&#20013;&#30340;&#22810;&#35821;&#35328;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations. (arXiv:2307.13776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#21270;&#26144;&#23556;&#26426;&#21046;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;&#30340;&#22810;&#35821;&#35328;&#35781;&#21650;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20513;&#22312;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#35789;&#20041;&#28040;&#27495;&#65288;WSD&#65289;&#20013;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#19978;&#19979;&#25991;&#21270;&#26144;&#23556;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#36807;&#31243;&#33719;&#24471;&#30340;&#31232;&#30095;&#19978;&#19979;&#25991;&#21270;&#35789;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#36848;&#25913;&#36827;&#20351;&#24471;17&#31181;&#35821;&#35328;&#30340;&#24179;&#22343;F&#20998;&#25968;&#26377;&#36817;6.5&#20010;&#30334;&#20998;&#28857;&#30340;&#26174;&#33879;&#25552;&#39640;&#65288;&#20174;62.0&#25552;&#39640;&#21040;68.5&#65289;&#12290;&#25105;&#20204;&#22312;https://github.com/begab/sparsity_makes_sense&#21457;&#24067;&#20102;&#29992;&#20110;&#22797;&#29616;&#25105;&#20204;&#23454;&#39564;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in the average F-score (from 62.0 to 68.5) over a collection of 17 typologically diverse set of target languages. We release our source code for replicating our experiments at https://github.com/begab/sparsity_makes_sense.
&lt;/p&gt;</description></item><item><title>E^2VPT&#26159;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;Transformer&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#38190;&#20540;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#20197;&#21450;&#25552;&#31034;&#21098;&#26525;&#36807;&#31243;&#26469;&#25913;&#21892;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#26524;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.13770</link><description>&lt;p&gt;
E^2VPT: &#19968;&#31181;&#26377;&#25928;&#39640;&#25928;&#30340;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning. (arXiv:2307.13770v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13770
&lt;/p&gt;
&lt;p&gt;
E^2VPT&#26159;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;Transformer&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#38190;&#20540;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#20197;&#21450;&#25552;&#31034;&#21098;&#26525;&#36807;&#31243;&#26469;&#25913;&#21892;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#26524;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#23545;&#36825;&#20123;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#26032;&#20219;&#21153;&#30340;&#24494;&#35843;&#21464;&#24471;&#36234;&#26469;&#36234;&#21442;&#25968;&#23494;&#38598;&#12290;&#20026;&#20102;&#20943;&#23569;&#24494;&#35843;&#36807;&#31243;&#20013;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#19982;&#23436;&#20840;&#24494;&#35843;&#30456;&#27604;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Effective and Efficient Visual Prompt Tuning&#65288;E^2VPT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#38190;&#20540;&#25552;&#31034;&#21644;&#35270;&#35273;&#25552;&#31034;&#21040;&#33258;&#27880;&#24847;&#21147;&#21644;&#36755;&#20837;&#23618;&#65292;&#20998;&#21035;&#25913;&#21892;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25552;&#31034;&#21098;&#26525;&#36807;&#31243;&#65292;&#31995;&#32479;&#22320;&#21098;&#26525;&#20302;&#37325;&#35201;&#24615;&#30340;&#25552;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of transformer-based models continues to grow, fine-tuning these large-scale pretrained vision models for new tasks has become increasingly parameter-intensive. Parameter-efficient learning has been developed to reduce the number of tunable parameters during fine-tuning. Although these methods show promising results, there is still a significant performance gap compared to full fine-tuning. To address this challenge, we propose an Effective and Efficient Visual Prompt Tuning (E^2VPT) approach for large-scale transformer-based model adaptation. Specifically, we introduce a set of learnable key-value prompts and visual prompts into self-attention and input layers, respectively, to improve the effectiveness of model fine-tuning. Moreover, we design a prompt pruning procedure to systematically prune low importance prompts while preserving model performance, which largely enhances the model's efficiency. Empirical results demonstrate that our approach outperforms several state-o
&lt;/p&gt;</description></item><item><title>ClusterSeq&#26159;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20803;&#23398;&#20064;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#25552;&#39640;&#20102;&#29289;&#21697;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20445;&#30041;&#20102;&#27425;&#35201;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#21033;&#29992;&#20102;&#21516;&#19968;&#32858;&#31867;&#20013;&#29992;&#25143;&#30340;&#38598;&#20307;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.13766</link><description>&lt;p&gt;
ClusterSeq: &#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#20803;&#23398;&#20064;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning. (arXiv:2307.13766v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13766
&lt;/p&gt;
&lt;p&gt;
ClusterSeq&#26159;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20803;&#23398;&#20064;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#25552;&#39640;&#20102;&#29289;&#21697;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20445;&#30041;&#20102;&#27425;&#35201;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#21033;&#29992;&#20102;&#21516;&#19968;&#32858;&#31867;&#20013;&#29992;&#25143;&#30340;&#38598;&#20307;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20102;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#36825;&#26159;&#30001;&#20110;&#26377;&#38480;&#30340;&#20132;&#20114;&#20351;&#24471;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#23558;&#20803;&#23398;&#20064;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#20391;&#20449;&#24687;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#21160;&#24577;&#26041;&#38754;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23637;&#29616;&#20986;&#19982;&#26356;&#24120;&#35265;&#25110;&#8220;&#20027;&#35201;&#29992;&#25143;&#8221;&#19981;&#21516;&#20559;&#22909;&#30340;&#8220;&#27425;&#35201;&#29992;&#25143;&#8221;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ClusterSeq&#65292;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20803;&#23398;&#20064;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#12290;ClusterSeq&#21033;&#29992;&#29992;&#25143;&#24207;&#21015;&#20013;&#30340;&#21160;&#24577;&#20449;&#24687;&#26469;&#25552;&#39640;&#29289;&#21697;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#27809;&#26377;&#20391;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#20445;&#30041;&#20102;&#27425;&#35201;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#20250;&#34987;&#20027;&#35201;&#29992;&#25143;&#25152;&#25513;&#30422;&#65292;&#24182;&#21033;&#29992;&#20102;&#21516;&#19968;&#32858;&#31867;&#20013;&#29992;&#25143;&#30340;&#38598;&#20307;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical scenarios, the effectiveness of sequential recommendation systems is hindered by the user cold-start problem, which arises due to limited interactions for accurately determining user preferences. Previous studies have attempted to address this issue by combining meta-learning with user and item-side information. However, these approaches face inherent challenges in modeling user preference dynamics, particularly for "minor users" who exhibit distinct preferences compared to more common or "major users." To overcome these limitations, we present a novel approach called ClusterSeq, a Meta-Learning Clustering-Based Sequential Recommender System. ClusterSeq leverages dynamic information in the user sequence to enhance item prediction accuracy, even in the absence of side information. This model preserves the preferences of minor users without being overshadowed by major users, and it capitalizes on the collective knowledge of users within the same cluster. Extensive experiment
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#65292;&#35813;&#26041;&#27861;&#22312; Anomaly Detection benchmark &#20013;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>http://arxiv.org/abs/2307.13763</link><description>&lt;p&gt;
&#38544;&#24335;&#24402;&#19968;&#21270;&#26174;&#24335;&#27491;&#21017;&#21270;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Implicitly Normalized Explicitly Regularized Density Estimation. (arXiv:2307.13763v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13763
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#65292;&#35813;&#26041;&#27861;&#22312; Anomaly Detection benchmark &#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#26680;&#23494;&#24230;&#20272;&#35745;&#26377;&#26126;&#26174;&#24046;&#24322;&#65292;&#21487;&#20197;&#28165;&#26224;&#35299;&#37322;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;&#34429;&#28982;&#25105;&#20204;&#26080;&#27861;&#24471;&#21040;&#30456;&#20851;&#26680;&#20989;&#25968;&#30340;&#38381;&#21512;&#35299;&#26512;&#24418;&#24335;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#37319;&#26679;&#36827;&#34892;&#36817;&#20284;&#12290;&#20915;&#23450;&#23494;&#24230;&#30340;&#20248;&#21270;&#38382;&#39064;&#26159;&#38750;&#20984;&#30340;&#65292;&#26631;&#20934;&#30340;&#26799;&#24230;&#26041;&#27861;&#25928;&#26524;&#19981;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#35813;&#26041;&#27861;&#25552;&#20379;&#30340;&#26159;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65292;&#26080;&#27861;&#20351;&#29992;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#37319;&#29992;&#22522;&#20110; Fisher &#25955;&#24230;&#30340;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214; ADBench &#19978;&#35780;&#20272;&#20102;&#24471;&#21040;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#36229;&#36807;15&#20010;&#31639;&#27861;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 al
&lt;/p&gt;</description></item><item><title>&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#29992;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#21644;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;TMR&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#23398;&#21040;&#30340;&#27169;&#24335;&#65307;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13755</link><description>&lt;p&gt;
TMR-RD: &#29992;&#20110;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;&#21644;&#34920;&#31034;&#20998;&#27495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection. (arXiv:2307.13755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13755
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#29992;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#21644;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;TMR&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#23398;&#21040;&#30340;&#27169;&#24335;&#65307;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;(SSOD)&#21487;&#20197;&#23558;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#29616;&#26377;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;SSOD&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#20266;&#26631;&#31614;&#22122;&#22768;/&#35823;&#23548;&#12289;&#32463;&#20856;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;(EMA)&#31574;&#30053;&#21644;&#21518;&#26399;&#35757;&#32451;&#20013;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#32463;&#20856;EMA&#30340;&#23616;&#38480;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;TMR&#38454;&#27573;&#20248;&#21270;&#20102;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#65292;&#20197;&#31934;&#21270;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#36825;&#20123;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#36830;&#22238;&#24402;&#26469;&#29983;&#25104;... (&#25688;&#35201;&#26410;&#23436;&#25972;&#25552;&#20379;)
&lt;/p&gt;
&lt;p&gt;
Semi-supervised object detection (SSOD) can incorporate limited labeled data and large amounts of unlabeled data to improve the performance and generalization of existing object detectors. Despite many advances, recent SSOD methods are still challenged by noisy/misleading pseudo-labels, classical exponential moving average (EMA) strategy, and the consensus of Teacher-Student models in the latter stages of training. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model's weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore complementary representations. In addition, we use cascade regression to gene
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#26032;&#20852;&#22522;&#30784;&#27169;&#22411;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#25552;&#31034;&#23454;&#29616;&#19978;&#19979;&#25991;&#25512;&#29702;&#12289;&#29983;&#25104;&#27867;&#21270;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#20855;&#26377;&#21363;&#26102;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13721</link><description>&lt;p&gt;
&#35270;&#35273;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#23450;&#20041;&#30340;&#22522;&#30784;&#27169;&#22411;: &#19968;&#39033;&#35843;&#26597;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Foundational Models Defining a New Era in Vision: A Survey and Outlook. (arXiv:2307.13721v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13721
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#26032;&#20852;&#22522;&#30784;&#27169;&#22411;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#25552;&#31034;&#23454;&#29616;&#19978;&#19979;&#25991;&#25512;&#29702;&#12289;&#29983;&#25104;&#27867;&#21270;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#20855;&#26377;&#21363;&#26102;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#31995;&#32479;&#20197;&#35266;&#23519;&#21644;&#25512;&#29702;&#35270;&#35273;&#22330;&#26223;&#30340;&#32452;&#25104;&#24615;&#36136;&#20026;&#22522;&#30784;&#65292;&#23545;&#20110;&#29702;&#35299;&#25105;&#20204;&#30340;&#19990;&#30028;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#20301;&#32622;&#12289;&#27495;&#20041;&#21644;&#21464;&#21270;&#21487;&#20197;&#26356;&#22909;&#22320;&#29992;&#20154;&#31867;&#35821;&#35328;&#25551;&#36848;&#65292;&#36825;&#31181;&#35821;&#35328;&#36890;&#24120;&#21463;&#21040;&#35821;&#27861;&#35268;&#21017;&#21644;&#20854;&#20182;&#27169;&#24577;&#65288;&#22914;&#38899;&#39057;&#21644;&#28145;&#24230;&#65289;&#30340;&#32422;&#26463;&#12290;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#22914;&#20309;&#24357;&#21512;&#36825;&#20123;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#32467;&#21512;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20419;&#36827;&#20102;&#19978;&#19979;&#25991;&#25512;&#29702;&#12289;&#27867;&#21270;&#20197;&#21450;&#27979;&#35797;&#26102;&#30340;&#21363;&#26102;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#31216;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#25552;&#31034;&#36827;&#34892;&#20462;&#25913;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#20363;&#22914;&#36890;&#36807;&#25552;&#20379;&#36793;&#30028;&#26694;&#23545;&#29305;&#23450;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;&#65292;&#36890;&#36807;&#35810;&#38382;&#20851;&#20110;&#22270;&#20687;&#25110;&#35270;&#39057;&#22330;&#26223;&#30340;&#38382;&#39064;&#23454;&#29616;&#20114;&#21160;&#23545;&#35805;&#65292;&#25110;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36827;&#34892;&#25805;&#20316;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#26032;&#20852;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision systems to see and reason about the compositional nature of visual scenes are fundamental to understanding our world. The complex relations between objects and their locations, ambiguities, and variations in the real-world environment can be better described in human language, naturally governed by grammatical rules and other modalities such as audio and depth. The models learned to bridge the gap between such modalities coupled with large-scale training data facilitate contextual reasoning, generalization, and prompt capabilities at test time. These models are referred to as foundational models. The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box, having interactive dialogues by asking questions about an image or video scene or manipulating the robot's behavior through language instructions. In this survey, we provide a comprehensive review of such emerging foundational mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#32452;&#21512;&#25193;&#25955;&#20316;&#20026;&#33402;&#26415;&#23478;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#23376;&#22330;&#26223;&#24182;&#28789;&#27963;&#24067;&#23616;&#65292;&#33402;&#26415;&#23478;&#21487;&#20197;&#25551;&#36848;&#27599;&#20010;&#23376;&#22330;&#26223;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#21508;&#31181;&#25511;&#21046;&#36755;&#20837;&#29983;&#25104;&#12289;&#32452;&#21512;&#21644;&#21327;&#35843;&#23376;&#22330;&#26223;&#65292;&#20197;&#23454;&#29616;&#32508;&#21512;&#32780;&#32454;&#33268;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;&#20316;&#32773;&#25552;&#20986;&#29616;&#26377;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#25972;&#20307;&#35780;&#20272;&#30340;&#33021;&#21147;&#65292;&#24076;&#26395;&#36890;&#36807;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#26469;&#32508;&#21512;&#35780;&#20272;&#22270;&#20687;&#36136;&#37327;&#21644;&#23454;&#29616;&#33402;&#26415;&#23478;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.13720</link><description>&lt;p&gt;
&#32452;&#21512;&#25193;&#25955;&#65306;&#25972;&#20307;&gt; = SUM&#65288;&#37096;&#20998;&#65289;
&lt;/p&gt;
&lt;p&gt;
Composite Diffusion | whole &gt;= \Sigma parts. (arXiv:2307.13720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#32452;&#21512;&#25193;&#25955;&#20316;&#20026;&#33402;&#26415;&#23478;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#23376;&#22330;&#26223;&#24182;&#28789;&#27963;&#24067;&#23616;&#65292;&#33402;&#26415;&#23478;&#21487;&#20197;&#25551;&#36848;&#27599;&#20010;&#23376;&#22330;&#26223;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#21508;&#31181;&#25511;&#21046;&#36755;&#20837;&#29983;&#25104;&#12289;&#32452;&#21512;&#21644;&#21327;&#35843;&#23376;&#22330;&#26223;&#65292;&#20197;&#23454;&#29616;&#32508;&#21512;&#32780;&#32454;&#33268;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;&#20316;&#32773;&#25552;&#20986;&#29616;&#26377;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#25972;&#20307;&#35780;&#20272;&#30340;&#33021;&#21147;&#65292;&#24076;&#26395;&#36890;&#36807;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#26469;&#32508;&#21512;&#35780;&#20272;&#22270;&#20687;&#36136;&#37327;&#21644;&#23454;&#29616;&#33402;&#26415;&#23478;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33402;&#26415;&#23478;&#25110;&#24179;&#38754;&#35774;&#35745;&#24072;&#26469;&#35828;&#65292;&#22330;&#26223;&#30340;&#31354;&#38388;&#24067;&#23616;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#25972;&#21512;&#31354;&#38388;&#20449;&#24687;&#26041;&#38754;&#25552;&#20379;&#30340;&#25903;&#25345;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#32452;&#21512;&#25193;&#25955;&#20316;&#20026;&#33402;&#26415;&#23478;&#36890;&#36807;&#32452;&#21512;&#23376;&#22330;&#26223;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#25163;&#27573;&#12290;&#33402;&#26415;&#23478;&#21487;&#20197;&#36890;&#36807;&#28789;&#27963;&#30340;&#33258;&#30001;&#24418;&#24335;&#27573;&#33853;&#24067;&#23616;&#26469;&#25351;&#23450;&#36825;&#20123;&#23376;&#22330;&#26223;&#30340;&#25490;&#21015;&#12290;&#20182;&#20204;&#21487;&#20197;&#20027;&#35201;&#20351;&#29992;&#33258;&#28982;&#25991;&#26412;&#26469;&#25551;&#36848;&#27599;&#20010;&#23376;&#22330;&#26223;&#30340;&#20869;&#23481;&#65292;&#27492;&#22806;&#36824;&#21487;&#20197;&#21033;&#29992;&#21442;&#32771;&#22270;&#20687;&#25110;&#25511;&#21046;&#36755;&#20837;&#65292;&#22914;&#32447;&#26465;&#33402;&#26415;&#12289;&#28034;&#40486;&#12289;&#20154;&#20307;&#23039;&#21183;&#12289;&#36793;&#32536;&#31561;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#32780;&#27169;&#22359;&#21270;&#30340;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#65292;&#20351;&#24471;&#21487;&#20197;&#20197;&#22791;&#36873;&#30340;&#26041;&#24335;&#29983;&#25104;&#12289;&#32452;&#21512;&#21644;&#21327;&#35843;&#23376;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24076;&#26395;&#35780;&#20272;&#32452;&#21512;&#22270;&#20687;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#23454;&#29616;&#33402;&#26415;&#23478;&#24847;&#22270;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#25972;&#20307;&#35780;&#20272;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
For an artist or a graphic designer, the spatial layout of a scene is a critical design choice. However, existing text-to-image diffusion models provide limited support for incorporating spatial information. This paper introduces Composite Diffusion as a means for artists to generate high-quality images by composing from the sub-scenes. The artists can specify the arrangement of these sub-scenes through a flexible free-form segment layout. They can describe the content of each sub-scene primarily using natural text and additionally by utilizing reference images or control inputs such as line art, scribbles, human pose, canny edges, and more.  We provide a comprehensive and modular method for Composite Diffusion that enables alternative ways of generating, composing, and harmonizing sub-scenes. Further, we wish to evaluate the composite image for effectiveness in both image quality and achieving the artist's intent. We argue that existing image quality metrics lack a holistic evaluation
&lt;/p&gt;</description></item><item><title>FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13716</link><description>&lt;p&gt;
FedDRL: &#19968;&#31181;&#22522;&#20110;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13716
&lt;/p&gt;
&lt;p&gt;
FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#26679;&#26412;&#25968;&#37327;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#22266;&#23450;&#26435;&#37325;&#20540;&#26469;&#34701;&#21512;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#36129;&#29486;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26679;&#26412;&#37327;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23458;&#25143;&#31471;&#25925;&#24847;&#19978;&#20256;&#20302;&#36136;&#37327;&#25110;&#24694;&#24847;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#23558;&#20005;&#37325;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDRL&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36807;&#28388;&#25481;&#24694;&#24847;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#21487;&#20449;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#19982;&#27169;&#22411;&#34701;&#21512;&#12290;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;FedDRL&#31639;&#27861;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21487;&#20449;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#24182;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ShuttleNet&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#20987;&#29699;&#20449;&#24687;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#39044;&#27979;&#32701;&#27611;&#29699;&#20987;&#29699;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#22312;CoachAI&#32701;&#27611;&#29699;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2307.13715</link><description>&lt;p&gt;
&#12298;2023&#24180;&#25945;&#32451;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;8&#21442;&#21152;CoachAI&#32701;&#27611;&#29699;&#25361;&#25112;&#36187;&#65306;&#29992;&#20110;&#20987;&#29699;&#39044;&#27979;&#30340;&#39640;&#32423;ShuttleNet&#12299;
&lt;/p&gt;
&lt;p&gt;
Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions. (arXiv:2307.13715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ShuttleNet&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#20987;&#29699;&#20449;&#24687;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#39044;&#27979;&#32701;&#27611;&#29699;&#20987;&#29699;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#22312;CoachAI&#32701;&#27611;&#29699;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#20987;&#29699;&#26469;&#25913;&#36827;&#29616;&#26377;&#26694;&#26550;ShuttleNet&#22312;&#39044;&#27979;&#32701;&#27611;&#29699;&#20987;&#29699;&#31867;&#22411;&#21644;&#20301;&#32622;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21442;&#21152;&#20102;2023&#24180;IJCAI&#30340;CoachAI&#32701;&#27611;&#29699;&#25361;&#25112;&#36187;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#22522;&#20934;&#32447;&#30340;&#25104;&#32489;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#27604;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#24182;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, our objective is to improve the performance of the existing framework ShuttleNet in predicting badminton shot types and locations by leveraging past strokes. We participated in the CoachAI Badminton Challenge at IJCAI 2023 and achieved significantly better results compared to the baseline. Ultimately, our team achieved the first position in the competition and we made our code available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CALMED&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25913;&#36827;&#33258;&#21160;&#24773;&#32490;&#26816;&#27979;&#31995;&#32479;&#22312;&#33258;&#38381;&#30151;&#24739;&#32773;&#36523;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.13706</link><description>&lt;p&gt;
&#24341;&#20837;CALMED&#65306;&#29992;&#20110;&#33258;&#38381;&#30151;&#20799;&#31461;&#24773;&#32490;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#27880;&#37322;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Introducing CALMED: Multimodal Annotated Dataset for Emotion Detection in Children with Autism. (arXiv:2307.13706v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CALMED&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#22810;&#27169;&#24577;&#24773;&#32490;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25913;&#36827;&#33258;&#21160;&#24773;&#32490;&#26816;&#27979;&#31995;&#32479;&#22312;&#33258;&#38381;&#30151;&#24739;&#32773;&#36523;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24773;&#32490;&#26816;&#27979;&#65288;ED&#65289;&#26088;&#22312;&#26500;&#24314;&#31995;&#32479;&#20197;&#33258;&#21160;&#35782;&#21035;&#29992;&#25143;&#30340;&#24773;&#32490;&#12290;&#36825;&#19968;&#39046;&#22495;&#26377;&#28508;&#21147;&#25552;&#21319;&#20154;&#26426;&#20132;&#20114;&#20307;&#39564;&#65292;&#20026;&#29992;&#25143;&#21019;&#36896;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;ED&#31995;&#32479;&#22312;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#24739;&#32773;&#36523;&#19978;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21019;&#24314;&#38024;&#23545;&#33258;&#38381;&#30151;&#24739;&#32773;&#24773;&#32490;&#34920;&#36798;&#26041;&#24335;&#30340;ED&#31995;&#32479;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34429;&#28982;&#21019;&#24314;&#20102;&#38024;&#23545;ASD&#20799;&#31461;&#30340;ED&#31995;&#32479;&#65292;&#20294;&#26410;&#20849;&#20139;&#25152;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#12290;&#20849;&#20139;&#27880;&#37322;&#25968;&#25454;&#38598;&#23545;&#20110;&#20419;&#36827;&#30740;&#31350;&#31038;&#21306;&#20869;&#26356;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#23545;ED&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#24314;&#31435;&#19968;&#20010;&#27969;&#31243;&#26469;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#38381;&#30151;1&#32423;&#35786;&#26029;&#30340;&#20799;&#31461;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CALMED&#65288;Children, Autism, Multimodal, Emotion, Detection&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#24773;&#32490;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#24180;&#40836;&#22312;8-12&#23681;&#30340;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Emotion Detection (ED) aims to build systems to identify users' emotions automatically. This field has the potential to enhance HCI, creating an individualised experience for the user. However, ED systems tend to perform poorly on people with Autism Spectrum Disorder (ASD). Hence, the need to create ED systems tailored to how people with autism express emotions. Previous works have created ED systems tailored for children with ASD but did not share the resulting dataset. Sharing annotated datasets is essential to enable the development of more advanced computer models for ED within the research community. In this paper, we describe our experience establishing a process to create a multimodal annotated dataset featuring children with a level 1 diagnosis of autism. In addition, we introduce CALMED (Children, Autism, Multimodal, Emotion, Detection), the resulting multimodal emotion detection dataset featuring children with autism aged 8-12. CALMED includes audio and video featur
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#38416;&#36848;&#20102;&#25511;&#21046;&#21644;&#30417;&#25511;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#28418;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25351;&#26631;&#29992;&#20110;&#23457;&#26597;&#27169;&#22411;&#22312;&#28508;&#22312;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13705</link><description>&lt;p&gt;
&#25511;&#21046;&#21644;&#30417;&#25511;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Control and Monitoring of Artificial Intelligence Algorithms. (arXiv:2307.13705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13705
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#38416;&#36848;&#20102;&#25511;&#21046;&#21644;&#30417;&#25511;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#28418;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25351;&#26631;&#29992;&#20110;&#23457;&#26597;&#27169;&#22411;&#22312;&#28508;&#22312;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#22312;&#37096;&#32626;&#21518;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#30417;&#27979;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20043;&#38388;&#30340;&#28508;&#22312;&#27874;&#21160;&#30340;&#37325;&#35201;&#24615;&#12290;&#35814;&#32454;&#35299;&#37322;&#20102;&#25968;&#25454;&#28418;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#22522;&#26412;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#21487;&#29992;&#20110;&#23457;&#26597;&#27169;&#22411;&#22312;&#28508;&#22312;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.13704</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#29616;&#22312;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#33021;&#22815;&#35299;&#37322;&#22797;&#26434;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;XAI&#29305;&#21035;&#36866;&#29992;&#20110;&#21361;&#38505;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#31867;&#30340;&#29983;&#21629;&#20381;&#36182;&#20110;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#12290;&#21307;&#30103;&#30740;&#31350;&#30340;&#19968;&#20010;&#39046;&#22495;&#26159;&#24180;&#40836;&#39044;&#27979;&#21644;&#34928;&#32769;&#21450;&#19982;&#24180;&#40836;&#30456;&#20851;&#30142;&#30149;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#37492;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;XAI&#30340;&#20316;&#29992;&#23578;&#26410;&#30452;&#25509;&#25506;&#35752;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#22120;&#23448;&#31995;&#32479;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;XAI&#22312;&#21307;&#30103;&#24212;&#29992;&#20197;&#21450;&#29305;&#21035;&#26159;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36827;&#34892;&#24544;&#23454;&#30340;&#8220;&#38142;&#24335;&#24605;&#32500;&#8221;&#25512;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23545;&#38142;&#24335;&#24605;&#32500;&#30340;&#20381;&#36182;&#31243;&#24230;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#26356;&#33021;&#21147;&#36234;&#24378;&#65292;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#25512;&#29702;&#36234;&#26469;&#36234;&#19981;&#24544;&#23454;&#12290;</title><link>http://arxiv.org/abs/2307.13702</link><description>&lt;p&gt;
&#27979;&#37327;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#20013;&#30340;&#24544;&#35802;&#24230;
&lt;/p&gt;
&lt;p&gt;
Measuring Faithfulness in Chain-of-Thought Reasoning. (arXiv:2307.13702v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36827;&#34892;&#24544;&#23454;&#30340;&#8220;&#38142;&#24335;&#24605;&#32500;&#8221;&#25512;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23545;&#38142;&#24335;&#24605;&#32500;&#30340;&#20381;&#36182;&#31243;&#24230;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#26356;&#33021;&#21147;&#36234;&#24378;&#65292;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#25512;&#29702;&#36234;&#26469;&#36234;&#19981;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#65292;&#22914;&#26524;&#33021;&#22815;&#20135;&#29983;&#36880;&#27493;&#30340;&#8220;&#38142;&#24335;&#24605;&#32500;&#8221;&#25512;&#29702;&#65292;&#20854;&#34920;&#29616;&#20250;&#26356;&#22909;&#65292;&#20294;&#19981;&#28165;&#26970;&#25152;&#36848;&#30340;&#25512;&#29702;&#26159;&#21542;&#24544;&#23454;&#22320;&#35299;&#37322;&#20102;&#27169;&#22411;&#23454;&#38469;&#30340;&#25512;&#29702;&#36807;&#31243;&#65288;&#21363;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#24335;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#24403;&#20171;&#20837;&#38142;&#24335;&#24605;&#32500;&#26102;&#27169;&#22411;&#39044;&#27979;&#22914;&#20309;&#21457;&#29983;&#21464;&#21270;&#65288;&#20363;&#22914;&#65292;&#28155;&#21152;&#38169;&#35823;&#25110;&#25913;&#20889;&#23427;&#65289;&#65292;&#26469;&#30740;&#31350;&#38142;&#24335;&#24605;&#32500;&#21487;&#33021;&#19981;&#24544;&#23454;&#30340;&#20551;&#35774;&#12290;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#22312;&#39044;&#27979;&#31572;&#26696;&#26102;&#23545;&#38142;&#24335;&#24605;&#32500;&#30340;&#20381;&#36182;&#31243;&#24230;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#26377;&#26102;&#20250;&#20005;&#37325;&#20381;&#36182;&#38142;&#24335;&#24605;&#32500;&#65292;&#32780;&#20854;&#20182;&#26102;&#20505;&#21017;&#20027;&#35201;&#24573;&#35270;&#23427;&#12290;&#38142;&#24335;&#24605;&#32500;&#30340;&#24615;&#33021;&#25552;&#21319;&#20284;&#20046;&#19981;&#20165;&#20165;&#26469;&#33258;&#20110;&#20854;&#22686;&#21152;&#30340;&#27979;&#35797;&#35745;&#31639;&#37327;&#65292;&#20063;&#19981;&#20165;&#20165;&#26469;&#33258;&#20110;&#38142;&#24335;&#24605;&#32500;&#30340;&#29305;&#23450;&#25514;&#36766;&#25152;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#26356;&#26377;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#25512;&#29702;&#36234;&#26469;&#36234;&#19981;&#24544;&#23454;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#29615;&#22659;&#36866;&#24403;&#65292;&#38142;&#24335;&#24605;&#32500;&#21487;&#20197;&#26159;&#24544;&#23454;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\text{EFO}_{k}$-CQA&#26694;&#26550;&#65292;&#29992;&#20110;&#36229;&#36234;&#38598;&#21512;&#25805;&#20316;&#30340;&#30693;&#35782;&#22270;&#35889;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#26041;&#27861;&#35780;&#20272;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#29616;&#26377;&#26597;&#35810;&#31354;&#38388;&#12290;&#20351;&#29992;&#26500;&#24314;&#30340;$\text{EFO}_{k}$-CQA&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#26597;&#35810;&#38590;&#24230;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#35777;&#26126;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#36807;&#31243;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13701</link><description>&lt;p&gt;
$\text{EFO}_{k}$-CQA&#65306;&#36229;&#36234;&#38598;&#21512;&#25805;&#20316;&#30340;&#30693;&#35782;&#22270;&#35889;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
$\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation. (arXiv:2307.13701v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\text{EFO}_{k}$-CQA&#26694;&#26550;&#65292;&#29992;&#20110;&#36229;&#36234;&#38598;&#21512;&#25805;&#20316;&#30340;&#30693;&#35782;&#22270;&#35889;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#26041;&#27861;&#35780;&#20272;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#29616;&#26377;&#26597;&#35810;&#31354;&#38388;&#12290;&#20351;&#29992;&#26500;&#24314;&#30340;$\text{EFO}_{k}$-CQA&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#26597;&#35810;&#38590;&#24230;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#35777;&#26126;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#36807;&#31243;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22238;&#31572;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#38656;&#35201;&#23545;&#19981;&#23436;&#25972;&#30693;&#35782;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#22240;&#20026;&#23384;&#22312;&#30528;&#24320;&#25918;&#19990;&#30028;&#30340;&#20551;&#35774;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23545;&#26410;&#35266;&#23519;&#21040;&#30340;&#30693;&#35782;&#36827;&#34892;&#27867;&#21270;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#65292;&#19968;&#20010;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#33719;&#21462;&#21644;&#35780;&#20272;&#36825;&#26679;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#26041;&#27861;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#23384;&#22312;&#37327;&#21270;&#19968;&#38454;&#26597;&#35810;&#19982;&#22810;&#20010;&#21464;&#37327;($\text{EFO}_{k}$)&#30340;&#32452;&#21512;&#31354;&#38388;&#12290;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;&#32452;&#21512;&#26597;&#35810;&#31354;&#38388;&#26174;&#33879;&#25193;&#23637;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#36890;&#36807;&#38598;&#21512;&#25805;&#20316;&#23450;&#20041;&#30340;&#26597;&#35810;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;741&#31181;&#26597;&#35810;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;$\text{EFO}_{k}$-CQA&#65292;&#20197;&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#32467;&#26524;&#20026;&#29702;&#35299;&#26597;&#35810;&#38590;&#24230;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#36807;&#31243;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge is required due to the open-world assumption. Learning-based methods are essential because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables ($\text{EFO}_{k}$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset, $\text{EFO}_{k}$-CQA, with 741 types of query for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systema
&lt;/p&gt;</description></item><item><title>CAMP&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26495;&#29699;&#29699;&#21592;&#34920;&#29616;&#25351;&#26631;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#23545;&#25163;&#23454;&#21147;&#21644;&#27604;&#36187;&#29615;&#22659;&#31561;&#22240;&#32032;&#65292;&#21487;&#20197;&#37327;&#21270;&#20010;&#20307;&#29699;&#21592;&#23545;&#27604;&#36187;&#32467;&#26524;&#30340;&#36129;&#29486;&#12290;CAMP&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#20026;&#36873;&#25321;&#21644;&#33609;&#26696;&#12289;&#25945;&#32451;&#21644;&#35757;&#32451;&#12289;&#22242;&#38431;&#38453;&#23481;&#21644;&#25112;&#30053;&#21046;&#23450;&#25552;&#20379;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2307.13700</link><description>&lt;p&gt;
CAMP:&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26495;&#29699;&#29699;&#21592;&#34920;&#29616;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
CAMP: A Context-Aware Cricket Players Performance Metric. (arXiv:2307.13700v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13700
&lt;/p&gt;
&lt;p&gt;
CAMP&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26495;&#29699;&#29699;&#21592;&#34920;&#29616;&#25351;&#26631;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#23545;&#25163;&#23454;&#21147;&#21644;&#27604;&#36187;&#29615;&#22659;&#31561;&#22240;&#32032;&#65292;&#21487;&#20197;&#37327;&#21270;&#20010;&#20307;&#29699;&#21592;&#23545;&#27604;&#36187;&#32467;&#26524;&#30340;&#36129;&#29486;&#12290;CAMP&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#20026;&#36873;&#25321;&#21644;&#33609;&#26696;&#12289;&#25945;&#32451;&#21644;&#35757;&#32451;&#12289;&#22242;&#38431;&#38453;&#23481;&#21644;&#25112;&#30053;&#21046;&#23450;&#25552;&#20379;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26495;&#29699;&#26159;&#20165;&#27425;&#20110;&#36275;&#29699;&#22312;&#25910;&#35270;&#29575;&#19978;&#26368;&#21463;&#27426;&#36814;&#30340;&#36816;&#21160;&#12290;&#28982;&#32780;&#65292;&#23545;&#20010;&#20307;&#29699;&#21592;&#34920;&#29616;&#30340;&#35780;&#20272;&#65292;&#20316;&#20026;&#22242;&#38431;&#36816;&#21160;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#30446;&#21069;&#20027;&#35201;&#22522;&#20110;&#32508;&#21512;&#34920;&#29616;&#25968;&#25454;&#65292;&#21253;&#25324;&#24179;&#22343;&#24471;&#20998;&#21644;&#20987;&#29699;&#27425;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CAMP&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29699;&#21592;&#34920;&#29616;&#25351;&#26631;&#65292;&#29992;&#20110;&#34913;&#37327;&#20010;&#20307;&#29699;&#21592;&#23545;&#26495;&#29699;&#27604;&#36187;&#32467;&#26524;&#30340;&#36129;&#29486;&#12290;CAMP&#37319;&#29992;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#24182;&#20026;&#36873;&#25321;&#21644;&#33609;&#26696;&#12289;&#25945;&#32451;&#21644;&#35757;&#32451;&#12289;&#22242;&#38431;&#38453;&#23481;&#21644;&#25112;&#30053;&#21046;&#23450;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#12290;CAMP&#32435;&#20837;&#20102;&#34920;&#29616;&#30340;&#30830;&#20999;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23545;&#25163;&#30340;&#23454;&#21147;&#21644;&#27604;&#36187;&#30340;&#29305;&#23450;&#29615;&#22659;&#65292;&#20363;&#22914;&#21387;&#21147;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;2001&#24180;&#33267;2019&#24180;&#38388;&#30340;&#26377;&#38480;&#36807;&#24230;&#26495;&#29699;&#27604;&#36187;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#22312;&#27599;&#22330;&#27604;&#36187;&#20013;&#65292;&#19968;&#32452;&#19987;&#23478;&#23459;&#24067;&#19968;&#21517;&#29699;&#21592;&#20026;&#26368;&#20339;&#29699;&#21592;&#65292;&#34987;&#31216;&#20026;M}atch&#30340;&#26368;&#20339;&#29699;&#21592;&#65288;MoM&#65289;&#12290;&#36890;&#36807;CAMP&#35780;&#20272;&#65292;&#26368;&#39640;&#35780;&#20998;&#30340;&#20004;&#21517;&#29699;&#21592;&#19982;MoM&#21305;&#37197;&#30340;&#27010;&#29575;&#20026;83&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cricket is the second most popular sport after soccer in terms of viewership. However, the assessment of individual player performance, a fundamental task in team sports, is currently primarily based on aggregate performance statistics, including average runs and wickets taken. We propose Context-Aware Metric of player Performance, CAMP, to quantify individual players' contributions toward a cricket match outcome. CAMP employs data mining methods and enables effective data-driven decision-making for selection and drafting, coaching and training, team line-ups, and strategy development. CAMP incorporates the exact context of performance, such as opponents' strengths and specific circumstances of games, such as pressure situations. We empirically evaluate CAMP on data of limited-over cricket matches between 2001 and 2019. In every match, a committee of experts declares one player as the best player, called Man of the M}atch (MoM). The top two rated players by CAMP match with MoM in 83\% 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;67&#20301;&#26469;&#33258;&#39321;&#28207;&#22235;&#25152;&#20013;&#23398;&#30340;EFL&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#24577;&#24230;&#21644;&#30683;&#30462;&#12290;&#30740;&#31350;&#21457;&#29616;&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#36127;&#38754;&#25110;&#30683;&#30462;&#30340;&#24863;&#24773;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;EFL&#35838;&#22530;&#20013;&#23454;&#26045;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#24314;&#35758;&#25945;&#32946;&#24037;&#20316;&#32773;&#23558;&#27963;&#21160;&#30446;&#26631;&#19982;&#23398;&#29983;&#30340;&#20215;&#20540;&#35266;&#12289;&#35821;&#35328;&#33021;&#21147;&#21644;AI&#33021;&#21147;&#30456;&#19968;&#33268;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#27963;&#21160;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.13699</link><description>&lt;p&gt;
EFL&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#24577;&#24230;&#21644;&#30683;&#30462;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
EFL Students' Attitudes and Contradictions in a Machine-in-the-loop Activity System. (arXiv:2307.13699v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;67&#20301;&#26469;&#33258;&#39321;&#28207;&#22235;&#25152;&#20013;&#23398;&#30340;EFL&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#24577;&#24230;&#21644;&#30683;&#30462;&#12290;&#30740;&#31350;&#21457;&#29616;&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#36127;&#38754;&#25110;&#30683;&#30462;&#30340;&#24863;&#24773;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;EFL&#35838;&#22530;&#20013;&#23454;&#26045;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#24314;&#35758;&#25945;&#32946;&#24037;&#20316;&#32773;&#23558;&#27963;&#21160;&#30446;&#26631;&#19982;&#23398;&#29983;&#30340;&#20215;&#20540;&#35266;&#12289;&#35821;&#35328;&#33021;&#21147;&#21644;AI&#33021;&#21147;&#30456;&#19968;&#33268;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#27963;&#21160;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#27963;&#21160;&#29702;&#35770;&#25506;&#35752;&#20102;&#26469;&#33258;&#39321;&#28207;&#22235;&#25152;&#20013;&#23398;&#30340;67&#21517;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#65288;EFL&#65289;&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#24577;&#24230;&#21644;&#30683;&#30462;&#12290;&#22312;&#20889;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25552;&#20379;&#21019;&#20316;&#24819;&#27861;&#12290;&#23398;&#29983;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#34920;&#36798;&#20182;&#20204;&#23545;&#19982;AI&#19968;&#36215;&#20889;&#20316;&#30340;&#24863;&#21463;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#37096;&#20998;&#23398;&#29983;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#36127;&#38754;&#25110;&#30683;&#30462;&#30340;&#24863;&#24773;&#12290;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#21457;&#29616;&#65292;&#23398;&#29983;&#21644;AI&#20043;&#38388;&#30340;&#30683;&#30462;&#25110;&#32039;&#24352;&#28857;&#28304;&#20110;AI&#30340;&#19981;&#36275;&#65292;&#23398;&#29983;&#22312;&#28909;&#24773;&#21644;&#20559;&#22909;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20197;&#21450;&#20182;&#20204;&#36861;&#27714;&#35821;&#35328;&#33258;&#20027;&#24615;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#22312;EFL&#35838;&#22530;&#19978;&#23454;&#26045;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#24314;&#35758;&#25945;&#32946;&#24037;&#20316;&#32773;&#23558;&#27963;&#21160;&#30446;&#26631;&#19982;&#23398;&#29983;&#30340;&#20215;&#20540;&#35266;&#12289;&#35821;&#35328;&#33021;&#21147;&#21644;AI&#33021;&#21147;&#30456;&#19968;&#33268;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#27963;&#21160;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study applies Activity Theory and investigates the attitudes and contradictions of 67 English as a foreign language (EFL) students from four Hong Kong secondary schools towards machine-in-the-loop writing, where artificial intelligence (AI) suggests ideas during composition. Students answered an open-ended question about their feelings on writing with AI. Results revealed mostly positive attitudes, with some negative or mixed feelings. From a thematic analysis, contradictions or points of tension between students and AI stemmed from AI inadequacies, students' balancing enthusiasm with preference, and their striving for language autonomy. The research highlights the benefits and challenges of implementing machine-in-the-loop writing in EFL classrooms, suggesting educators align activity goals with students' values, language abilities, and AI capabilities to enhance students' activity systems.
&lt;/p&gt;</description></item><item><title>GPT-3&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#34920;&#29616;&#26377;&#38480;&#65292;&#38656;&#35201;&#20351;&#29992;&#29420;&#31435;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#36923;&#36753;&#24341;&#25806;&#26469;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13617</link><description>&lt;p&gt;
GPT-3&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#37329;&#34701;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPT-3 Models are Few-Shot Financial Reasoners. (arXiv:2307.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13617
&lt;/p&gt;
&lt;p&gt;
GPT-3&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#34920;&#29616;&#26377;&#38480;&#65292;&#38656;&#35201;&#20351;&#29992;&#29420;&#31435;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#36923;&#36753;&#24341;&#25806;&#26469;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20998;&#26512;&#26159;&#35780;&#20272;&#20844;&#21496;&#19994;&#32489;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#20174;&#19994;&#32773;&#36890;&#36807;&#28145;&#20837;&#30340;&#37327;&#21270;&#20998;&#26512;&#22238;&#31572;&#37329;&#34701;&#38382;&#39064;&#65292;&#20174;&#32780;&#20570;&#20986;&#26377;&#21033;&#21487;&#22270;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#37329;&#34701;&#38382;&#31572;&#26159;&#19968;&#20010;&#38656;&#35201;&#23545;&#25968;&#23383;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#30340;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#25512;&#29702;&#33021;&#21147;&#22914;&#20309;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#38656;&#35201;&#19968;&#20010;&#26816;&#32034;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25910;&#38598;&#19982;&#37329;&#34701;&#38382;&#39064;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#26377;&#25928;&#30340;&#37329;&#34701;&#31243;&#24207;&#21644;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;GPT-3&#20165;&#20165;&#36890;&#36807;&#23569;&#37327;&#31034;&#20363;&#23601;&#23454;&#29616;&#20102;&#24191;&#27867;&#20219;&#21153;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;GPT-3&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#21457;&#29616;&#29420;&#31435;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#36923;&#36753;&#24341;&#25806;&#20173;&#28982;&#26159;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;&#31934;&#30830;&#24615;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial analysis is an important tool for evaluating company performance. Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so. As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers. Furthermore, it is unknown how well pre-trained language models can reason in the financial domain. The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer. However, recently large language models like GPT-3 have achieved state-of-the-art performance on wide variety of tasks with just a few shot examples. We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of finan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FacTool&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22411;&#38382;&#31572;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#31561;&#22235;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13528</link><description>&lt;p&gt;
FacTool&#65306;&#29983;&#25104;AI&#20013;&#30340;&#20107;&#23454;&#24615;&#26816;&#27979; &#8212;&#8212; &#19968;&#31181;&#20026;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#22330;&#26223;&#21152;&#24378;&#30340;&#24037;&#20855;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. (arXiv:2307.13528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13528
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FacTool&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22411;&#38382;&#31572;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#31561;&#22235;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#26041;&#20415;&#20102;&#39640;&#36136;&#37327;&#25991;&#26412;&#30340;&#21512;&#25104;&#65292;&#20294;&#20063;&#22312;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#20197;&#19979;&#38382;&#39064;&#25552;&#20986;&#20102;FacTool&#26694;&#26550;&#65306;&#65288;1&#65289;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#30001;&#29983;&#25104;&#27169;&#22411;&#22788;&#29702;&#26102;&#65292;&#23384;&#22312;&#30528;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#39118;&#38505;&#65307;&#65288;2&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#24456;&#38271;&#65292;&#32570;&#20047;&#28165;&#26224;&#23450;&#20041;&#30340;&#32454;&#31890;&#24230;&#20010;&#20307;&#20107;&#23454;&#65307;&#65288;3&#65289;&#22312;&#20107;&#23454;&#26816;&#26597;&#36807;&#31243;&#20013;&#32570;&#20047;&#26126;&#30830;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65288;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#65289;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method.
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;Hi-MPC&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#30828;&#39592;&#26550;&#25366;&#25496;&#65292;&#29992;&#20110;&#26080;&#26631;&#31614;3D&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;&#36890;&#36807;&#26500;&#24314;&#23618;&#27425;&#39592;&#26550;&#34920;&#31034;&#24182;&#21033;&#29992;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#26356;&#22810;&#20449;&#24687;&#20016;&#23500;&#30340;&#39592;&#26550;&#29305;&#24449;&#30340;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.12917</link><description>&lt;p&gt;
&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#19982;&#30828;&#39592;&#26550;&#25366;&#25496;&#30456;&#32467;&#21512;&#30340;&#26080;&#30417;&#30563;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification. (arXiv:2307.12917v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;Hi-MPC&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#30828;&#39592;&#26550;&#25366;&#25496;&#65292;&#29992;&#20110;&#26080;&#26631;&#31614;3D&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;&#36890;&#36807;&#26500;&#24314;&#23618;&#27425;&#39592;&#26550;&#34920;&#31034;&#24182;&#21033;&#29992;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#26356;&#22810;&#20449;&#24687;&#20016;&#23500;&#30340;&#39592;&#26550;&#29305;&#24449;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20174;&#36523;&#20307;&#20851;&#33410;&#23398;&#20064;&#21333;&#23618;&#27425;&#30340;&#39592;&#26550;&#29305;&#24449;&#65292;&#24182;&#20551;&#35774;&#39592;&#26550;&#30340;&#37325;&#35201;&#24615;&#30456;&#31561;&#65292;&#22240;&#27492;&#36890;&#24120;&#26080;&#27861;&#21033;&#29992;&#26356;&#22810;&#26469;&#33258;&#19981;&#21516;&#23618;&#27425;&#65288;&#22914;&#32930;&#20307;&#23618;&#27425;&#65289;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#39592;&#26550;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#26631;&#31614;&#20381;&#36182;&#24615;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23398;&#20064;&#26356;&#19968;&#33324;&#30340;&#39592;&#26550;&#34920;&#31034;&#26041;&#38754;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26080;&#30417;&#30563;&#23618;&#27425;&#39592;&#26550;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;Hi-MPC&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#30828;&#39592;&#26550;&#25366;&#25496;&#65288;HSM&#65289;&#29992;&#20110;&#26080;&#26631;&#31614;3D&#39592;&#26550;&#30340;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#39592;&#26550;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#20197;&#27169;&#25311;&#36523;&#20307;&#20851;&#33410;&#12289;&#32452;&#20214;&#21644;&#32930;&#20307;&#23618;&#27425;&#30340;&#31895;&#21040;&#32454;&#30340;&#36523;&#20307;&#21644;&#21160;&#20316;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23618;&#27425;&#21270;&#30340;&#20803;-&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid advancements in depth sensors and deep learning, skeleton-based person re-identification (re-ID) models have recently achieved remarkable progress with many advantages. Most existing solutions learn single-level skeleton features from body joints with the assumption of equal skeleton importance, while they typically lack the ability to exploit more informative skeleton features from various levels such as limb level with more global body patterns. The label dependency of these methods also limits their flexibility in learning more general skeleton representations. This paper proposes a generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning (Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with unlabeled 3D skeletons. Firstly, we construct hierarchical representations of skeletons to model coarse-to-fine body and motion features from the levels of body joints, components, and limbs. Then a hierarchical meta-prototype contrastive learni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MediaGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#22269;&#23186;&#20307;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;MediaGPT&#22312;&#21508;&#31181;&#20013;&#22269;&#23186;&#20307;&#20219;&#21153;&#19978;&#20248;&#20110;&#20027;&#27969;&#27169;&#22411;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10930</link><description>&lt;p&gt;
MediaGPT&#65306;&#29992;&#20110;&#20013;&#22269;&#23186;&#20307;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MediaGPT : A Large Language Model For Chinese Media. (arXiv:2307.10930v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MediaGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#22269;&#23186;&#20307;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;MediaGPT&#22312;&#21508;&#31181;&#20013;&#22269;&#23186;&#20307;&#20219;&#21153;&#19978;&#20248;&#20110;&#20027;&#27969;&#27169;&#22411;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#21644;&#22522;&#20110;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#23186;&#20307;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23186;&#20307;&#30340;&#29992;&#20363;&#19982;&#36890;&#29992;LLM&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#21464;&#24471;&#36234;&#26469;&#36234;&#26126;&#26174;&#65292;&#29305;&#21035;&#26159;&#22312;&#20013;&#25991;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23186;&#20307;&#39046;&#22495;&#29305;&#23450;LLM&#19982;&#36890;&#29992;LLM&#20043;&#38388;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#25351;&#20196;&#31867;&#22411;&#20197;&#28385;&#36275;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#23186;&#20307;&#39046;&#22495;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20123;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20013;&#22269;&#23186;&#20307;&#39046;&#22495;&#30340;&#39046;&#22495;&#29305;&#23450;LLM&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#21644;&#19987;&#23478;&#30340;SFT&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#19978;&#36827;&#34892;&#20154;&#24037;&#19987;&#23478;&#35780;&#20272;&#21644;&#24378;&#27169;&#22411;&#35780;&#20272;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;MediaGPT&#22312;&#21508;&#31181;&#20013;&#22269;&#23186;&#20307;&#39046;&#22495;&#20219;&#21153;&#19978;&#20248;&#20110;&#20027;&#27969;&#27169;&#22411;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities in generating high-quality text and making predictions based on large amounts of data, including the media domain. However, in practical applications, the differences between the media's use cases and the general-purpose applications of LLMs have become increasingly apparent, especially Chinese. This paper examines the unique characteristics of media-domain-specific LLMs compared to general LLMs, designed a diverse set of task instruction types to cater the specific requirements of the domain and constructed unique datasets that are tailored to the media domain. Based on these, we proposed MediaGPT, a domain-specific LLM for the Chinese media domain, training by domain-specific data and experts SFT data. By performing human experts evaluation and strong model evaluation on a validation set, this paper demonstrated that MediaGPT outperforms mainstream models on various Chinese media domain tasks and verifies the importance 
&lt;/p&gt;</description></item><item><title>AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.10711</link><description>&lt;p&gt;
AdjointDPM: &#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10711
&lt;/p&gt;
&lt;p&gt;
AdjointDPM&#26159;&#19968;&#31181;&#26032;&#30340;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#35299;&#20915;&#20102;DPM&#23450;&#21046;&#21270;&#20013;&#20869;&#23384;&#28040;&#32791;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23450;&#21046;&#21270;&#26041;&#27861;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#26679;&#20363;&#26469;&#23558;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#27010;&#24565;&#23545;&#40784;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#24403;&#21807;&#19968;&#21487;&#29992;&#30340;&#30417;&#30563;&#26159;&#23450;&#20041;&#22312;&#29983;&#25104;&#20869;&#23481;&#19978;&#30340;&#21487;&#24494;&#24230;&#37327;&#26102;&#30340;DPM&#23450;&#21046;&#21270;&#25361;&#25112;&#12290;&#30001;&#20110;DPM&#30340;&#37319;&#26679;&#36807;&#31243;&#28041;&#21450;&#23545;&#21435;&#22122;UNet&#30340;&#36882;&#24402;&#35843;&#29992;&#65292;&#26420;&#32032;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#23384;&#20648;&#25152;&#26377;&#36845;&#20195;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#23548;&#33268;&#20869;&#23384;&#28040;&#32791;&#26497;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;AdjointDPM&#65292;&#39318;&#20808;&#36890;&#36807;&#27714;&#35299;&#30456;&#24212;&#30340;&#27010;&#29575;&#27969;ODE&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#28982;&#21518;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#36890;&#36807;&#27714;&#35299;&#21478;&#19968;&#20010;&#22686;&#24378;&#30340;ODE&#23558;&#25439;&#22833;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#21442;&#25968;(&#21253;&#25324;&#35843;&#21046;&#20449;&#21495;&#12289;&#32593;&#32476;&#26435;&#37325;&#21644;&#21021;&#22987;&#22122;&#22768;)&#12290;&#20026;&#20102;&#20943;&#23569;&#27491;&#21521;&#29983;&#25104;&#21644;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#25968;&#20540;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#36825;&#19968;&#26032;&#26041;&#21521;&#65292;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#38754;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#21364;&#26263;&#20013;&#36827;&#34892;&#38544;&#34255;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2307.10569</link><description>&lt;p&gt;
&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deceptive Alignment Monitoring. (arXiv:2307.10569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#36825;&#19968;&#26032;&#26041;&#21521;&#65292;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#38754;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#21364;&#26263;&#20013;&#36827;&#34892;&#38544;&#34255;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#20197;&#21450;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#33258;&#27835;&#26435;&#19981;&#26029;&#25193;&#22823;&#65292;&#19968;&#20010;&#26032;&#30340;&#23545;&#25163;&#20986;&#29616;&#20102;&#65306;&#27169;&#22411;&#26412;&#36523;&#12290;&#19968;&#20010;&#27169;&#22411;&#30475;&#20284;&#21512;&#29702;&#22320;&#34892;&#20026;&#65292;&#21364;&#26263;&#20013;&#12289;&#24494;&#22937;&#22320;&#20462;&#25913;&#20854;&#34892;&#20026;&#20197;&#36798;&#21040;&#21035;&#30340;&#30446;&#30340;&#30340;&#23041;&#32961;&#65292;&#36890;&#24120;&#22312;AI&#23433;&#20840;&#19982;&#23545;&#40784;&#31038;&#21306;&#20013;&#34987;&#31216;&#20026;&#27450;&#39575;&#24615;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#26041;&#21521;&#31216;&#20026;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26426;&#22120;&#23398;&#20064;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#26041;&#21521;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#23545;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#19988;&#32039;&#23494;&#30456;&#20851;&#65292;&#24182;&#19988;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#39046;&#22495;&#30340;&#36827;&#27493;&#26082;&#25552;&#20986;&#20102;&#38271;&#26399;&#25361;&#25112;&#65292;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21628;&#21505;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#26356;&#22810;&#22320;&#21442;&#19982;&#36825;&#20123;&#26032;&#20852;&#26041;&#21521;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety &amp; Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#37327;&#23376;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20013;&#36825;&#20123;&#26032;&#20852;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2307.08072</link><description>&lt;p&gt;
&#22312;&#37327;&#23376;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26032;&#20852;&#33021;&#21147;&#65306;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study. (arXiv:2307.08072v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#37327;&#23376;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20013;&#36825;&#20123;&#26032;&#20852;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#37096;&#32626;&#21644;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#37327;&#23376;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;LLMs&#30340;&#20869;&#23384;&#21344;&#29992;&#20197;&#21450;&#22686;&#21152;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#20302;&#20301;&#37327;&#23376;&#21270;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20102;&#35299;&#37327;&#23376;&#21270;&#23545;LLMs&#33021;&#21147;&#30340;&#24433;&#21709;&#26159;&#37325;&#35201;&#30340;&#12290;&#19982;&#20197;&#24448;&#19987;&#27880;&#20110;&#24635;&#20307;&#24615;&#33021;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#37327;&#23376;&#21270;&#23545;&#8220;&#26032;&#20852;&#33021;&#21147;&#8221;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#33021;&#21147;&#26159;&#21306;&#20998;LLMs&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;LLMs&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#32500;&#36830;&#36143;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#26032;&#20852;&#33021;&#21147;&#22312;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;&#32780;2&#20301;&#27169;&#22411;&#22312;&#36825;&#20123;&#33021;&#21147;&#19978;&#36935;&#21040;&#20102;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the superior performance, Large Language Models~(LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increasing the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on \emph{emergent abilities}, which are important characteristics that distinguish LLMs from small language models. Specially, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2307.04192</link><description>&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#65306;&#33258;&#36866;&#24212;&#37319;&#26679;&#29992;&#20110;&#39640;&#25928;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04192
&lt;/p&gt;
&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#26159;&#35270;&#39057;&#29702;&#35299;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#37197;&#22791;&#20102;&#35270;&#39057;&#21464;&#25442;&#22120;(Video Transformers)&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#24314;&#27169;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#22330;&#26223;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#19968;&#31181;&#32463;&#27982;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21482;&#23545;&#35270;&#39057;&#30340;&#19968;&#23567;&#37096;&#20998;&#24103;&#36827;&#34892;&#37319;&#26679;&#65292;&#26469;&#20195;&#34920;&#35270;&#39057;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#24182;&#22312;&#36825;&#20123;&#37319;&#26679;&#24103;&#19978;&#35843;&#25972;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#36890;&#24120;&#38543;&#26426;&#37319;&#26679;&#19968;&#32452;&#24103;&#25110;&#29255;&#27573;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#37096;&#20851;&#32852;&#24615;&#21644;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26080;&#30446;&#26631;&#30340;&#37319;&#26679;&#21487;&#33021;&#20250;&#36951;&#28431;&#21487;&#20197;&#25512;&#23548;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#20851;&#38190;&#24103;&#65292;&#22312;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#22686;&#21152;&#26102;&#65292;&#24773;&#20917;&#20250;&#21464;&#24471;&#26356;&#31967;&#65292;&#32780;&#38543;&#30528;&#35270;&#39057;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24103;&#37319;&#26679;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#19978;&#23545;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25214;&#21040;&#20102;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;FETA-Friends&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03506</link><description>&lt;p&gt;
&#26080;&#23548;&#25968;&#30340;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Derivative Free Weight-space Ensembling. (arXiv:2307.03506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#19978;&#23545;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25214;&#21040;&#20102;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;FETA-Friends&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#20043;&#38388;&#25554;&#20540;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#25506;&#32034;&#22312;&#20004;&#20010;&#20197;&#19978;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#26377;&#19968;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21019;&#24314;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#28304;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#19987;&#23478;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#30446;&#26631;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#22312;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;&#22909;&#30340;&#25554;&#20540;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;FETA-Friends&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work suggests that interpolating between the weights of two specialized language models can transfer knowledge between tasks in a way that multi-task learning cannot. However, very few have explored interpolation between more than two models, where each has a distinct knowledge base. In this paper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new few-sample task transfer approach for open-domain dialogue. Our framework creates a set of diverse expert language models trained using a predefined set of source tasks. Next, we finetune each of the expert models on the target task, approaching the target task from several distinct knowledge bases. Finally, we linearly interpolate between the model weights using a gradient-free-optimization algorithm, to efficiently find a good interpolation weighting. We demonstrate the effectiveness of the method on FETA-Friends outperforming the standard pretrain-finetune approach.
&lt;/p&gt;</description></item><item><title>DifFSS&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36741;&#21161;&#25903;&#25345;&#22270;&#20687;&#65292;&#32780;&#19981;&#38656;&#35201;&#20462;&#25913;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00773</link><description>&lt;p&gt;
DifFSS: &#19968;&#31181;&#29992;&#20110;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DifFSS: Diffusion Model for Few-Shot Semantic Segmentation. (arXiv:2307.00773v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00773
&lt;/p&gt;
&lt;p&gt;
DifFSS&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36741;&#21161;&#25903;&#25345;&#22270;&#20687;&#65292;&#32780;&#19981;&#38656;&#35201;&#20462;&#25913;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#32467;&#26500;&#19981;&#21516;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#20294;&#26159;&#24615;&#33021;&#30340;&#25913;&#36827;&#24050;&#32463;&#36798;&#21040;&#20102;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#27425;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DifFSS&#12290;DifFSS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#19981;&#20462;&#25913;&#32593;&#32476;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#25513;&#30721;&#12289;&#28034;&#25273;&#25110;&#36719;HED&#36793;&#30028;&#30340;&#25903;&#25345;&#22270;&#20687;&#20316;&#20026;&#25511;&#21046;&#26465;&#20214;&#65292;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36741;&#21161;&#25903;&#25345;&#22270;&#20687;&#12290;&#36825;&#20010;&#29983;&#25104;&#36807;&#31243;&#27169;&#25311;&#20102;&#26597;&#35810;&#22270;&#20687;&#31867;&#21035;&#20869;&#30340;&#22810;&#26679;&#24615;&#65292;&#22914;&#39068;&#33394;&#12289;&#32441;&#29702;&#21464;&#21270;&#21644;&#20809;&#29031;&#31561;&#12290;&#22240;&#27492;&#65292;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#21487;&#20197;&#21442;&#32771;&#26356;&#22810;&#22810;&#26679;&#30340;&#25903;&#25345;&#22270;&#20687;&#65292;&#20135;&#29983;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated excellent performance in image generation. Although various few-shot semantic segmentation (FSS) models with different network structures have been proposed, performance improvement has reached a bottleneck. This paper presents the first work to leverage the diffusion model for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve the performance of the state-of-the-art FSS models by a large margin without modifying their network structure. Specifically, we utilize the powerful generation ability of diffusion models to generate diverse auxiliary support images by using the semantic mask, scribble or soft HED boundary of the support image as control conditions. This generation process simulates the variety within the class of the query image, such as color, texture variation, lighting, $etc$. As a result, FSS models can refer to more diverse support images, yielding more robust representations, thereby achieving a consistent improv
&lt;/p&gt;</description></item><item><title>FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.11650</link><description>&lt;p&gt;
FedNoisy: &#20998;&#24067;&#24335;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11650
&lt;/p&gt;
&lt;p&gt;
FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#22240;&#20026;&#26080;&#38656;&#23545;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25935;&#24863;&#25968;&#25454;&#36827;&#34892;&#32858;&#21512;&#32780;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#25968;&#25454;&#38548;&#31163;&#30340;&#20998;&#24067;&#24335;&#21644;&#23396;&#31435;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#36136;&#37327;&#30340;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#26631;&#31614;&#30340;&#24178;&#25200;&#12290;&#35768;&#22810;&#21162;&#21147;&#37117;&#33268;&#21147;&#20110;&#22312;&#38598;&#20013;&#24335;&#25110;&#32852;&#21512;&#24335;&#29615;&#22659;&#20013;&#38450;&#24481;&#22122;&#22768;&#26631;&#31614;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#32771;&#34385;&#21508;&#31181;&#20856;&#22411;&#32852;&#21512;&#23398;&#20064;&#22330;&#26223;&#20013;&#22122;&#22768;&#26631;&#31614;&#24433;&#21709;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#32852;&#21512;&#22122;&#22768;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25506;&#32034;&#36825;&#20123;&#25968;&#25454;&#35774;&#32622;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#65292;&#36825;&#21487;&#33021;&#25351;&#23548;&#26410;&#26469;&#30340;&#26041;&#27861;&#24320;&#21457;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20986;&#30340;20&#20010;&#22522;&#26412;&#35774;&#32622;&#65292;&#36866;&#29992;&#20110;5&#20010;&#20197;&#19978;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;</title><link>http://arxiv.org/abs/2306.00017</link><description>&lt;p&gt;
&#21521;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#36808;&#36827;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#31526;&#21495;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#65292;&#26080;&#21487;&#21542;&#35748;&#22320;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#35768;&#22810;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#30495;&#27491;&#30340;&#35821;&#35328;&#29702;&#35299;&#26102;&#65292;&#36825;&#20123;LLM&#30340;&#35768;&#22810;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20123;&#38480;&#21046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#26550;&#26500;&#30340;&#21103;&#20135;&#21697;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#33719;&#24471;&#26377;&#20851;&#35821;&#35328;&#22914;&#20309;&#36816;&#20316;&#30340;&#20219;&#20309;&#30693;&#35782;&#37117;&#23558;&#34987;&#22475;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#26377;&#24847;&#20041;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#31526;&#21495;&#34920;&#31034;&#30340;&#24378;&#24230;&#19982;&#25105;&#20204;&#35748;&#20026;&#26159;LLMs&#25104;&#21151;&#30340;&#20851;&#38190;&#32467;&#21512;&#36215;&#26469;&#65292;&#21363;&#22312;&#35268;&#27169;&#19978;&#25104;&#21151;&#22320;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#35821;&#35328;&#36870;&#21521;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#31526;&#21495;&#35774;&#32622;&#19979;&#23545;&#35821;&#35328;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#12290;&#19968;&#20123;&#20316;&#32773;&#25552;&#20986;&#20102;&#36825;&#20010;&#39033;&#30446;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#21345;&#36890;&#39118;&#26684;&#22270;&#29255;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#25200;&#21160;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#65292;&#36825;&#19968;&#21151;&#33021;&#24471;&#20197;&#23454;&#29616;&#12290;&#22238;&#28378;&#25200;&#21160;&#33021;&#22815;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#25928;&#36716;&#25442;&#25104;&#21345;&#36890;&#22270;&#20687;&#65292;&#32780;&#22270;&#20687;&#25200;&#21160;&#21017;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#30340;&#21345;&#36890;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.06710</link><description>&lt;p&gt;
&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#21345;&#36890;&#39118;&#26684;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator. (arXiv:2305.06710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#21345;&#36890;&#39118;&#26684;&#22270;&#29255;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#25200;&#21160;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#65292;&#36825;&#19968;&#21151;&#33021;&#24471;&#20197;&#23454;&#29616;&#12290;&#22238;&#28378;&#25200;&#21160;&#33021;&#22815;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#25928;&#36716;&#25442;&#25104;&#21345;&#36890;&#22270;&#20687;&#65292;&#32780;&#22270;&#20687;&#25200;&#21160;&#21017;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#30340;&#21345;&#36890;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26159;&#25193;&#25955;&#27169;&#22411;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#25216;&#26415;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#22312;&#25991;&#26412;&#25351;&#23548;&#26041;&#21521;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#36828;&#31163;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#21345;&#36890;&#39118;&#26684;&#29983;&#25104;&#22120;&#65292;&#21363;&#36890;&#36807;&#31616;&#21333;&#22320;&#25200;&#21160;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#23601;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#36716;&#25442;&#25104;&#21345;&#36890;&#22270;&#20687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25200;&#21160;&#26041;&#27861;&#65306;&#22238;&#28378;&#25200;&#21160;&#65288;Back-D&#65289;&#21644;&#22270;&#20687;&#25200;&#21160;&#65288;Image-D&#65289;&#65292;&#29992;&#20110;&#26500;&#36896;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#29992;&#20110;&#39044;&#27979;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#21644;&#25991;&#26412;&#25351;&#23548;&#30340;&#22024;&#26434;&#22270;&#20687;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;Back-D&#36890;&#36807;&#36890;&#36807;&#23558;$x_t$&#26367;&#25442;&#20026;$x_{t+\Delta t}$&#26469;&#25913;&#21464;&#26080;&#26631;&#27880;&#22024;&#26434;&#22270;&#20687;&#30340;&#22122;&#22768;&#27700;&#24179;&#20174;&#32780;&#23454;&#29616;&#21345;&#36890;&#21270;&#12290;Image-D&#21017;&#36890;&#36807;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#30340;&#21345;&#36890;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier-free guidance is an effective sampling technique in diffusion models that has been widely adopted. The main idea is to extrapolate the model in the direction of text guidance and away from null-text guidance. In this paper, we demonstrate that null-text guidance in diffusion models is secretly a cartoon-style creator, i.e., the generated images can be efficiently transformed into cartoons by simply perturbing the null-text guidance. Specifically, we proposed two disturbance methods, i.e., Rollback disturbance (Back-D) and Image disturbance (Image-D), to construct misalignment between the noisy images used for predicting null-text guidance and text guidance (subsequently referred to as \textbf{null-text noisy image} and \textbf{text noisy image} respectively) in the sampling process. Back-D achieves cartoonization by altering the noise level of null-text noisy image via replacing $x_t$ with $x_{t+\Delta t}$. Image-D, alternatively, produces high-fidelity, diverse cartoons by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#36712;&#36857;&#25130;&#26029;&#26041;&#27861;&#65288;TATU&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#22411;&#39537;&#21160;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;TATU&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.04660</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#20013;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#36712;&#36857;&#25130;&#26029;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning. (arXiv:2304.04660v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#36712;&#36857;&#25130;&#26029;&#26041;&#27861;&#65288;TATU&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#22411;&#39537;&#21160;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;TATU&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#33021;&#22815;&#20174;&#22266;&#23450;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#21040;&#22909;&#30340;&#31574;&#30053;&#65292;&#29978;&#33267;&#19968;&#20123;&#36136;&#37327;&#36739;&#24046;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#21512;&#25104;&#26679;&#26412;&#26159;&#21542;&#21487;&#38752;&#26080;&#27861;&#24471;&#21040;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#19968;&#20123;&#21512;&#25104;&#26679;&#26412;&#21487;&#33021;&#20301;&#20110;&#38745;&#24577;&#25968;&#25454;&#38598;&#25903;&#25345;&#21306;&#22495;&#20043;&#22806;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#36712;&#36857;&#25130;&#26029;&#65288;TATU&#65289;&#26041;&#27861;&#65292;&#22914;&#26524;&#32047;&#31215;&#19981;&#30830;&#23450;&#24615;&#36229;&#36807;&#38408;&#20540;&#65292;&#21017;&#33258;&#36866;&#24212;&#25130;&#26029;&#21512;&#25104;&#36712;&#36857;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;TATU&#30340;&#24615;&#33021;&#36793;&#30028;&#20197;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;&#20026;&#20102;&#22312;&#23454;&#35777;&#19978;&#26174;&#31034;&#20986;TATU&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#19982;&#20004;&#20010;&#32463;&#20856;&#30340;&#27169;&#22411;&#39537;&#21160;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MOPO&#21644;COMBO&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;TATU&#19982;&#22810;&#20010;&#29616;&#25104;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;BCQ&#65289;&#36827;&#34892;&#25972;&#21512;&#12290;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TATU&#30340;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with the trained environmental dynamics, model-based offline reinforcement learning (RL) algorithms can often successfully learn good policies from fixed-sized datasets, even some datasets with poor quality. Unfortunately, however, it can not be guaranteed that the generated samples from the trained dynamics model are reliable (e.g., some synthetic samples may lie outside of the support region of the static dataset). To address this issue, we propose Trajectory Truncation with Uncertainty (TATU), which adaptively truncates the synthetic trajectory if the accumulated uncertainty along the trajectory is too large. We theoretically show the performance bound of TATU to justify its benefits. To empirically show the advantages of TATU, we first combine it with two classical model-based offline RL algorithms, MOPO and COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free offline RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark show that TATU
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#20687;&#27700;&#21360;&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#25152;&#26377;&#29983;&#25104;&#30340;&#22270;&#20687;&#37117;&#33021;&#38544;&#34255;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#38544;&#24418;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15435</link><description>&lt;p&gt;
&#31283;&#23450;&#31614;&#21517;&#65306;&#23558;&#27700;&#21360;&#25166;&#26681;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
The Stable Signature: Rooting Watermarks in Latent Diffusion Models. (arXiv:2303.15435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#20687;&#27700;&#21360;&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#25152;&#26377;&#29983;&#25104;&#30340;&#22270;&#20687;&#37117;&#33021;&#38544;&#34255;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#38544;&#24418;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22270;&#20687;&#24314;&#27169;&#21487;&#20197;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20851;&#20110;&#36127;&#36131;&#20219;&#37096;&#32626;&#30340;&#36947;&#24503;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#20687;&#27700;&#21360;&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#20027;&#21160;&#31574;&#30053;&#12290;&#20854;&#30446;&#26631;&#26159;&#20351;&#25152;&#26377;&#29983;&#25104;&#30340;&#22270;&#20687;&#37117;&#38544;&#34255;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#65292;&#20197;&#20379;&#23558;&#26469;&#26816;&#27979;&#21644;/&#25110;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#20108;&#36827;&#21046;&#31614;&#21517;&#30340;&#26465;&#20214;&#19979;&#24555;&#36895;&#24494;&#35843;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#28508;&#22312;&#35299;&#30721;&#22120;&#26469;&#23454;&#29616;&#12290;&#39044;&#35757;&#32451;&#30340;&#27700;&#21360;&#25552;&#21462;&#22120;&#21487;&#20197;&#20174;&#20219;&#20309;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#31614;&#21517;&#65292;&#28982;&#21518;&#36890;&#36807;&#32479;&#35745;&#27979;&#35797;&#26469;&#30830;&#23450;&#23427;&#26159;&#21542;&#26469;&#33258;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27700;&#21360;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#38544;&#24418;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#31283;&#23450;&#31614;&#21517;&#22312;&#22270;&#20687;&#34987;&#20462;&#25913;&#21518;&#20173;&#28982;&#26377;&#25928;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#30340;&#22270;&#20687;&#35009;&#21098;&#20445;&#30041;10%&#30340;&#20869;&#23481;&#21518;&#65292;&#20197;10$^{-6}$&#20197;&#19979;&#30340;&#38169;&#35823;&#29575;&#19979;&#65292;&#33021;&#20197;90%&#20197;&#19978;&#30340;&#20934;&#30830;&#29575;&#26816;&#27979;&#20986;&#22270;&#20687;&#30340;&#28304;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative image modeling enables a wide range of applications but raises ethical concerns about responsible deployment. This paper introduces an active strategy combining image watermarking and Latent Diffusion Models. The goal is for all generated images to conceal an invisible watermark allowing for future detection and/or identification. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature. A pre-trained watermark extractor recovers the hidden signature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that Stable Signature works even after the images are modified. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep $10\%$ of the content, with $90$+$\%$ accuracy at a false positive rate below 10$^{-6}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#37319;&#29992;&#29983;&#25104;&#24335; AI&#65288;GAI&#65289;&#21161;&#25163;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#30340;&#24895;&#26223;&#21644;&#25945;&#23398;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.13936</link><description>&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#20013;&#30340;&#29983;&#25104;&#24335; AI &#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Generative AI Assistants in Software Development Education. (arXiv:2303.13936v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#37319;&#29992;&#29983;&#25104;&#24335; AI&#65288;GAI&#65289;&#21161;&#25163;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#30340;&#24895;&#26223;&#21644;&#25945;&#23398;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#27491;&#22312;&#36827;&#34892;&#19968;&#27425;&#28508;&#22312;&#30340;&#39072;&#35206;&#24615;&#30340;&#33539;&#24335;&#21464;&#38761;&#8212;&#8212;&#37319;&#29992;&#29983;&#25104;&#24335; AI&#65288;GAI&#65289;&#21161;&#25163;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#12290;&#34429;&#28982; AI &#24050;&#32463;&#22312;&#36719;&#20214;&#24037;&#31243;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#34987;&#20351;&#29992;&#65292;&#20294;&#26159;&#20687; GitHub Copilot &#21644; ChatGPT &#36825;&#26679;&#30340; GAI &#25216;&#26415;&#24050;&#32463;&#28608;&#21457;&#20102;&#35768;&#22810;&#20154;&#30340;&#24819;&#35937;&#21147;&#65288;&#21644;&#24656;&#24807;&#65289;&#12290;&#23613;&#31649;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#35813;&#34892;&#19994;&#23558;&#22914;&#20309;&#37319;&#29992;&#21644;&#36866;&#24212;&#36825;&#20123;&#25216;&#26415;&#65292;&#20294;&#24494;&#36719;&#65288;GitHub&#12289;&#24517;&#24212;&#65289;&#21644;&#35895;&#27468;&#65288;Bard&#65289;&#31561;&#22823;&#22411;&#36719;&#20214;&#20844;&#21496;&#23558;&#36825;&#20123;&#25216;&#26415;&#25972;&#21512;&#21040;&#26356;&#24191;&#27867;&#30340;&#34892;&#19994;&#20013;&#30340;&#20030;&#21160;&#26159;&#26126;&#30830;&#30340;&#24847;&#22270;&#21644;&#26041;&#21521;&#12290;&#25105;&#20204;&#19982;&#34892;&#19994;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#35775;&#35848;&#65292;&#20197;&#20102;&#35299;&#24403;&#21069;&#30340;&#23454;&#36341;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#23545;&#26410;&#26469;&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#30340;&#24895;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#25945;&#23398;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The software development industry is amid another potentially disruptive paradigm change--adopting the use of generative AI (GAI) assistants for software development. Whilst AI is already used in various areas of software engineering, GAI technologies, such as GitHub Copilot and ChatGPT, have ignited the imaginations (and fears) of many people. Whilst it is unclear how the industry will adopt and adapt to these technologies, the move to integrate these technologies into the wider industry by large software companies, such as Microsoft (GitHub, Bing) and Google (Bard), is a clear indication of intent and direction. We performed exploratory interviews with industry professionals to understand current practices and challenges, which we incorporate into our vision of a future of software development education and make some pedagogical recommendations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32423;&#32852;LSTM&#32593;&#32476;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#32929;&#31080;&#20132;&#26131;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#32929;&#31080;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#31574;&#30053;&#20989;&#25968;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32047;&#31215;&#22238;&#25253;&#21644;&#22799;&#26222;&#27604;&#29575;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#29305;&#21035;&#22312;&#20013;&#22269;&#32929;&#24066;&#36825;&#19968;&#26032;&#20852;&#24066;&#22330;&#20013;&#34920;&#29616;&#26356;&#20026;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2212.02721</link><description>&lt;p&gt;
&#20351;&#29992;&#32423;&#32852;LSTM&#32593;&#32476;&#30340;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#32929;&#31080;&#20132;&#26131;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Novel Deep Reinforcement Learning Based Automated Stock Trading System Using Cascaded LSTM Networks. (arXiv:2212.02721v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32423;&#32852;LSTM&#32593;&#32476;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#32929;&#31080;&#20132;&#26131;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#32929;&#31080;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#31574;&#30053;&#20989;&#25968;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32047;&#31215;&#22238;&#25253;&#21644;&#22799;&#26222;&#27604;&#29575;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#29305;&#21035;&#22312;&#20013;&#22269;&#32929;&#24066;&#36825;&#19968;&#26032;&#20852;&#24066;&#22330;&#20013;&#34920;&#29616;&#26356;&#20026;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#32929;&#31080;&#20132;&#26131;&#31574;&#30053;&#26159;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26500;&#24314;&#30340;&#65292;&#20294;&#26159;DRL&#26041;&#27861;&#26368;&#21021;&#22312;&#28216;&#25103;&#30028;&#24191;&#27867;&#20351;&#29992;&#65292;&#30452;&#25509;&#36866;&#24212;&#37329;&#34701;&#25968;&#25454;&#30340;&#20302;&#20449;&#22122;&#27604;&#21644;&#19981;&#22343;&#21248;&#24615;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#36275;&#12290;&#20026;&#20102;&#25429;&#25417;&#38544;&#34255;&#30340;&#20449;&#24687;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32423;&#32852;LSTM&#30340;DRL&#32929;&#31080;&#20132;&#26131;&#31995;&#32479;&#65292;&#39318;&#20808;&#20351;&#29992;LSTM&#20174;&#32929;&#31080;&#26085;&#24120;&#25968;&#25454;&#20013;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#65292;&#28982;&#21518;&#23558;&#25552;&#21462;&#30340;&#29305;&#24449;&#39304;&#32473;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20989;&#25968;&#20063;&#20351;&#29992;&#21478;&#19968;&#20010;LSTM&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#32654;&#22269;&#24066;&#22330;&#30340;DJI&#21644;&#20013;&#22269;&#32929;&#24066;&#30340;SSE50&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32047;&#31215;&#22238;&#25253;&#21644;&#22799;&#26222;&#27604;&#29575;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20013;&#22269;&#32929;&#24066;&#36825;&#19968;&#26032;&#20852;&#24066;&#22330;&#20013;&#20248;&#21183;&#26356;&#20026;&#26174;&#33879;&#12290;&#36825;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26500;&#24314;&#33258;&#21160;&#32929;&#31080;&#20132;&#26131;&#31995;&#32479;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
More and more stock trading strategies are constructed using deep reinforcement learning (DRL) algorithms, but DRL methods originally widely used in the gaming community are not directly adaptable to financial data with low signal-to-noise ratios and unevenness, and thus suffer from performance shortcomings. In this paper, to capture the hidden information, we propose a DRL based stock trading system using cascaded LSTM, which first uses LSTM to extract the time-series features from stock daily data, and then the features extracted are fed to the agent for training, while the strategy functions in reinforcement learning also use another LSTM for training. Experiments in DJI in the US market and SSE50 in the Chinese stock market show that our model outperforms previous baseline models in terms of cumulative returns and Sharp ratio, and this advantage is more significant in the Chinese stock market, a merging market. It indicates that our proposed method is a promising way to build a aut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#65292;&#23545;&#20110;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.00679</link><description>&lt;p&gt;
&#26410;&#30693;&#21160;&#24577;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#25511;&#21046;&#22120;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics. (arXiv:2212.00679v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#65292;&#23545;&#20110;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#65292;&#23545;&#20110;&#25511;&#21046;&#22120;&#30340;&#33258;&#21160;&#21270;&#21512;&#25104;&#21487;&#20197;&#30830;&#20445;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#28151;&#21512;&#29305;&#24615;&#21644;&#38543;&#26426;&#25110;&#26410;&#30693;&#30340;&#34892;&#20026;&#20351;&#24471;&#21512;&#25104;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#36825;&#26159;&#19968;&#31867;&#31163;&#25955;&#26102;&#38047;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#12290;&#19968;&#20010;MJLS&#30001;&#19968;&#32452;&#26377;&#38480;&#30340;&#38543;&#26426;&#32447;&#24615;&#21160;&#24577;&#21644;&#36825;&#20123;&#21160;&#24577;&#20043;&#38388;&#30340;&#31163;&#25955;&#36339;&#21464;&#32452;&#25104;&#65292;&#36825;&#20123;&#36339;&#21464;&#30001;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#31649;&#29702;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20010;MDP&#30340;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#26377;&#38480;&#29366;&#24577;&#25277;&#35937;&#65292;&#25429;&#25417;&#20102;MJLS&#30340;&#31163;&#25955;&#65288;&#27169;&#24335;&#36339;&#36291;&#65289;&#21644;&#36830;&#32493;&#65288;&#38543;&#26426;&#32447;&#24615;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25277;&#35937;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;MDP&#65288;iMDP&#65289;&#65292;&#28982;&#21518;&#35745;&#31639;&#20102;&#29366;&#24577;&#36716;&#31227;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated synthesis of provably correct controllers for cyber-physical systems is crucial for deployment in safety-critical scenarios. However, hybrid features and stochastic or unknown behaviours make this problem challenging. We propose a method for synthesising controllers for Markov jump linear systems (MJLSs), a class of discrete-time models for cyber-physical systems, so that they certifiably satisfy probabilistic computation tree logic (PCTL) formulae. An MJLS consists of a finite set of stochastic linear dynamics and discrete jumps between these dynamics that are governed by a Markov decision process (MDP). We consider the cases where the transition probabilities of this MDP are either known up to an interval or completely unknown. Our approach is based on a finite-state abstraction that captures both the discrete (mode-jumping) and continuous (stochastic linear) behaviour of the MJLS. We formalise this abstraction as an interval MDP (iMDP) for which we compute intervals of tra
&lt;/p&gt;</description></item><item><title>FsaNet&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#39057;&#27573;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36793;&#32536;&#30340;&#21516;&#26102;&#20419;&#36827;&#23545;&#35937;&#20869;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#65292;&#20302;&#39057;&#33258;&#27880;&#24847;&#21147;&#20063;&#21487;&#20197;&#36798;&#21040;&#25509;&#36817;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39057;&#29575;&#33258;&#27880;&#24847;&#21147;&#36824;&#31616;&#21270;&#20102;&#20196;&#29260;&#26144;&#23556;&#21644;&#20196;&#29260;&#28151;&#21512;&#38454;&#27573;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15595</link><description>&lt;p&gt;
FsaNet: &#39057;&#29575;&#33258;&#27880;&#24847;&#21147;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
FsaNet: Frequency Self-attention for Semantic Segmentation. (arXiv:2211.15595v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15595
&lt;/p&gt;
&lt;p&gt;
FsaNet&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#39057;&#27573;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36793;&#32536;&#30340;&#21516;&#26102;&#20419;&#36827;&#23545;&#35937;&#20869;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#65292;&#20302;&#39057;&#33258;&#27880;&#24847;&#21147;&#20063;&#21487;&#20197;&#36798;&#21040;&#25509;&#36817;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39057;&#29575;&#33258;&#27880;&#24847;&#21147;&#36824;&#31616;&#21270;&#20102;&#20196;&#29260;&#26144;&#23556;&#21644;&#20196;&#29260;&#28151;&#21512;&#38454;&#27573;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#22270;&#20687;&#30340;&#39057;&#35889;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#39640;&#24230;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20445;&#30041;&#36793;&#32536;&#24182;&#20419;&#36827;&#23545;&#35937;&#20869;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#19981;&#21516;&#39057;&#27573;&#19978;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20165;&#22312;&#20302;&#39057;&#20998;&#37327;&#19978;&#36827;&#34892;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#20302;&#39057;&#33258;&#27880;&#24847;&#21147;&#20063;&#21487;&#20197;&#36798;&#21040;&#38750;&#24120;&#25509;&#36817;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23884;&#20837;&#20102;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#21040;CNN&#32593;&#32476;&#30340;&#22836;&#37096;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;FsaNet&#12290;&#39057;&#29575;&#33258;&#27880;&#24847;&#21147;1&#65289;&#20165;&#38656;&#35201;&#20960;&#20010;&#20302;&#39057;&#31995;&#25968;&#20316;&#20026;&#36755;&#20837;&#65292;2&#65289;&#22312;&#25968;&#23398;&#19978;&#21487;&#20197;&#31561;&#25928;&#20110;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#30340;&#31354;&#38388;&#22495;&#33258;&#27880;&#24847;&#21147;&#65292;3&#65289;&#21516;&#26102;&#31616;&#21270;&#20102;&#20196;&#29260;&#26144;&#23556;&#65288;$1\times1$&#21367;&#31215;&#65289;&#38454;&#27573;&#21644;&#20196;&#29260;&#28151;&#21512;&#38454;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39057;&#29575;&#33258;&#27880;&#24847;&#21147;&#21482;&#38656;&#35201;87
&lt;/p&gt;
&lt;p&gt;
Considering the spectral properties of images, we propose a new self-attention mechanism with highly reduced computational complexity, up to a linear rate. To better preserve edges while promoting similarity within objects, we propose individualized processes over different frequency bands. In particular, we study a case where the process is merely over low-frequency components. By ablation study, we show that low frequency self-attention can achieve very close or better performance relative to full frequency even without retraining the network. Accordingly, we design and embed novel plug-and-play modules to the head of a CNN network that we refer to as FsaNet. The frequency self-attention 1) requires only a few low frequency coefficients as input, 2) can be mathematically equivalent to spatial domain self-attention with linear structures, 3) simplifies token mapping ($1\times1$ convolution) stage and token mixing stage simultaneously. We show that frequency self-attention requires $87
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#26377;&#20851;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#12289;&#25915;&#20987;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#19982;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2210.03829</link><description>&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#26377;&#20851;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#12289;&#25915;&#20987;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#19982;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#24341;&#36215;&#30340;&#26641;&#26408;&#27515;&#20129;&#26041;&#38754;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#20174;&#26641;&#30382;&#30002;&#34411;&#19982;&#23492;&#20027;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12289;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#36827;&#34892;&#20102;&#24635;&#32467;&#12290;&#19982;&#20197;&#24448;&#30340;&#21162;&#21147;&#30456;&#21453;&#65292;&#26412;&#32508;&#36848;&#21253;&#25324;&#20102;&#25152;&#26377;&#36965;&#24863;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26681;&#25454;&#22810;&#20809;&#35889;&#25110;&#39640;&#20809;&#35889;&#20998;&#26512;&#35299;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#24182;&#20174;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#21644;&#25915;&#20987;&#38454;&#27573;&#65292;&#37325;&#28857;&#20851;&#27880;&#25915;&#20987;&#30340;&#26089;&#26399;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#21644;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;/&#31354;&#38388;/&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#20809;&#35889;&#26893;&#34987;&#25351;&#25968;&#65288;SVIs&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#23398;&#20064;&#26041;&#26696;&#12289;&#20219;&#21153;&#31867;&#21035;&#12289;&#27169;&#22411;&#12289;&#31639;&#27861;&#12289;&#31867;&#21035;/&#31751;&#12289;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#19982;&#26550;&#26500;&#26041;&#38754;&#25552;&#21462;&#30693;&#35782;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#21487;&#35265;&#20809;&#21644;&#28909;&#32418;&#22806;&#31561;&#27874;&#27573;&#19978;&#26816;&#27979;&#24494;&#23567;&#21464;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive review of past and current advances in the early detection of bark beetle-induced tree mortality from three primary perspectives: bark beetle &amp; host interactions, RS, and ML/DL. In contrast to prior efforts, this review encompasses all RS systems and emphasizes ML/DL methods to investigate their strengths and weaknesses. We parse existing literature based on multi- or hyper-spectral analyses and distill their knowledge based on: bark beetle species &amp; attack phases with a primary emphasis on early stages of attacks, host trees, study regions, RS platforms &amp; sensors, spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation indices (SVIs), ML approaches, learning schemes, task categories, models, algorithms, classes/clusters, features, and DL networks &amp; architectures. Although DL-based methods and the random forest (RF) algorithm showed promising results, highlighting their potential to detect subtle changes across visible, therma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedIIC&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#29305;&#24449;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23398;&#20064;&#20004;&#20010;&#35282;&#24230;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.13803</link><description>&lt;p&gt;
FedIIC: &#38754;&#21521;&#31867;&#21035;&#19981;&#24179;&#34913;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification. (arXiv:2206.13803v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13803
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedIIC&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#29305;&#24449;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23398;&#20064;&#20004;&#20010;&#35282;&#24230;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#22312;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20174;&#20998;&#25955;&#25968;&#25454;&#20013;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21307;&#23398;&#22270;&#20687;&#35745;&#31639;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#21307;&#23398;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;FL&#22312;&#24615;&#33021;&#19978;&#21487;&#33021;&#20250;&#20986;&#29616;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#31867;&#65288;&#22914;&#32597;&#35265;&#30142;&#30149;&#65289;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#35757;&#32451;&#19968;&#20010;&#24179;&#34913;&#20998;&#31867;&#22120;&#20197;&#28040;&#38500;&#31867;&#21035;&#20808;&#39564;&#20559;&#24046;&#65292;&#20294;&#24573;&#35270;&#20102;&#25506;&#32034;&#26356;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#20197;&#20419;&#36827;&#20998;&#31867;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedIIC&#30340;&#38544;&#31169;&#20445;&#25252;FL&#26041;&#27861;&#65292;&#20174;&#29305;&#24449;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23398;&#20064;&#20004;&#20010;&#35282;&#24230;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#29305;&#24449;&#23398;&#20064;&#26041;&#38754;&#65292;&#35774;&#35745;&#20102;&#20004;&#32423;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#21462;&#26356;&#22909;&#30340;&#19982;&#19981;&#24179;&#34913;&#25968;&#25454;&#30456;&#20851;&#30340;&#31867;&#21035;&#29305;&#23450;&#29305;&#24449;&#12290;&#22312;&#20998;&#31867;&#22120;&#23398;&#20064;&#26041;&#38754;&#65292;&#26681;&#25454;&#23454;&#26102;&#38590;&#24230;&#21644;&#31867;&#21035;&#20808;&#39564;&#21160;&#24577;&#35774;&#32622;&#27599;&#31867;&#38388;&#38548;&#65292;&#24110;&#21161;&#27169;&#22411;&#24179;&#31561;&#22320;&#23398;&#20064;&#21508;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), training deep models from decentralized data without privacy leakage, has shown great potential in medical image computing recently. However, considering the ubiquitous class imbalance in medical data, FL can exhibit performance degradation, especially for minority classes (e.g. rare diseases). Existing methods towards this problem mainly focus on training a balanced classifier to eliminate class prior bias among classes, but neglect to explore better representation to facilitate classification performance. In this paper, we present a privacy-preserving FL method named FedIIC to combat class imbalance from two perspectives: feature learning and classifier learning. In feature learning, two levels of contrastive learning are designed to extract better class-specific features with imbalanced data in FL. In classifier learning, per-class margins are dynamically set according to real-time difficulty and class priors, which helps the model learn classes equally. Exp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#23545;&#28145;&#24230;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#20013;&#20808;&#39564;&#30693;&#35782;&#26368;&#26032;&#36827;&#23637;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#29702;&#35770;&#20998;&#26512;&#12289;&#20998;&#31867;&#12289;&#35752;&#35770;&#21644;&#38382;&#39064;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2206.02070</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#65306;&#19968;&#27425;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Priors in Deep Image Restoration and Enhancement: A Survey. (arXiv:2206.02070v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02070
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#23545;&#28145;&#24230;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#20013;&#20808;&#39564;&#30693;&#35782;&#26368;&#26032;&#36827;&#23637;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#29702;&#35770;&#20998;&#26512;&#12289;&#20998;&#31867;&#12289;&#35752;&#35770;&#21644;&#38382;&#39064;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#26159;&#36890;&#36807;&#21435;&#38500;&#22122;&#22768;&#12289;&#27169;&#31946;&#21644;&#20998;&#36776;&#29575;&#38477;&#20302;&#31561;&#38477;&#36136;&#26469;&#25913;&#21892;&#22270;&#20687;&#36136;&#37327;&#30340;&#36807;&#31243;&#12290;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#12290;&#30001;&#20110;&#20854;&#36870;&#38382;&#39064;&#30340;&#29305;&#24615;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#20808;&#39564;&#30693;&#35782;&#26469;&#24110;&#21161;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20808;&#39564;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#30740;&#31350;&#21644;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26159;&#31532;&#19968;&#27425;&#23545;&#28145;&#24230;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20840;&#38754;&#27010;&#36848;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21253;&#25324;&#20116;&#20010;&#20027;&#35201;&#20869;&#23481;&#65306;&#65288;1&#65289;&#23545;&#28145;&#24230;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65307;&#65288;2&#65289;&#23545;DL&#26041;&#27861;&#20013;&#24120;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#20998;&#23618;&#21644;&#32467;&#26500;&#21270;&#20998;&#31867;&#65307;&#65288;3&#65289;&#23545;&#27599;&#20010;&#20808;&#39564;&#30693;&#35782;&#30340;&#21407;&#29702;&#12289;&#28508;&#21147;&#21644;&#24212;&#29992;&#36827;&#34892;&#28145;&#20837;&#35752;&#35770;&#65307;&#65288;4&#65289;&#36890;&#36807;&#37325;&#28857;&#31361;&#20986;&#20851;&#38190;&#38382;&#39064;&#30340;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Image restoration and enhancement is a process of improving the image quality by removing degradations, such as noise, blur, and resolution degradation. Deep learning (DL) has recently been applied to image restoration and enhancement. Due to its ill-posed property, plenty of works have been explored priors to facilitate training deep neural networks (DNNs). However, the importance of priors has not been systematically studied and analyzed by far in the research community. Therefore, this paper serves as the first study that provides a comprehensive overview of recent advancements in priors for deep image restoration and enhancement. Our work covers five primary contents: (1) A theoretical analysis of priors for deep image restoration and enhancement; (2) A hierarchical and structural taxonomy of priors commonly used in the DL-based methods; (3) An insightful discussion on each prior regarding its principle, potential, and applications; (4) A summary of crucial problems by highlighting
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#38024;&#23545;&#25512;&#33616;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24212;&#29992;&#26469;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13619</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#22522;&#30784;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#38024;&#23545;&#25512;&#33616;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24212;&#29992;&#26469;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26368;&#26222;&#36941;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#25512;&#33616;&#31995;&#32479;&#22312;&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#21644;&#24179;&#21488;&#30340;&#21033;&#30410;&#19982;&#29983;&#25104;&#30340;&#25512;&#33616;&#32467;&#26524;&#30340;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#39640;&#24230;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#21463;&#21040;&#25968;&#25454;&#25110;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#21066;&#24369;&#31995;&#32479;&#30340;&#21487;&#20449;&#36182;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#33616;&#35774;&#32622;&#20013;&#35299;&#20915;&#28508;&#22312;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#32771;&#34385;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#28041;&#21450;&#25552;&#21319;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30456;&#23545;&#38646;&#25955;&#19988;&#32570;&#20047;&#31995;&#32479;&#21270;&#25972;&#29702;&#65292;&#22240;&#27492;&#23545;&#20110;&#26032;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#38590;&#20197;&#28145;&#20837;&#39046;&#22495;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23545;&#25512;&#33616;&#20013;&#29616;&#26377;&#20844;&#24179;&#24615;&#20316;&#21697;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24378;&#20581;&#30340;&#25968;&#37327;&#24863;&#30693;&#32858;&#21512;&#31639;&#27861;&#65292;&#21517;&#20026;FedRA&#65292;&#33021;&#22815;&#22312;&#32858;&#21512;&#27169;&#22411;&#26102;&#32771;&#34385;&#26412;&#22320;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#33021;&#22815;&#25269;&#24481;&#25968;&#37327;&#22686;&#24378;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2205.10848</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#25968;&#37327;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Quantity-Aware Aggregation for Federated Learning. (arXiv:2205.10848v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24378;&#20581;&#30340;&#25968;&#37327;&#24863;&#30693;&#32858;&#21512;&#31639;&#27861;&#65292;&#21517;&#20026;FedRA&#65292;&#33021;&#22815;&#22312;&#32858;&#21512;&#27169;&#22411;&#26102;&#32771;&#34385;&#26412;&#22320;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#33021;&#22815;&#25269;&#24481;&#25968;&#37327;&#22686;&#24378;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;FL&#38754;&#20020;&#20005;&#37325;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#24694;&#24847;&#30340;&#23458;&#25143;&#31471;&#21487;&#20197;&#27745;&#26579;&#27169;&#22411;&#26356;&#26032;&#65292;&#24182;&#21516;&#26102;&#34394;&#25253;&#22823;&#37327;&#20197;&#25918;&#22823;&#20854;&#22312;&#27169;&#22411;&#32858;&#21512;&#20013;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;FL&#38450;&#24481;&#26041;&#27861;&#65292;&#34429;&#28982;&#37117;&#33021;&#22788;&#29702;&#24694;&#24847;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20294;&#35201;&#20040;&#23558;&#25152;&#26377;&#25968;&#37327;&#35270;&#20026;&#33391;&#24615;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#24573;&#30053;/&#25130;&#26029;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#12290;&#21069;&#32773;&#23481;&#26131;&#21463;&#21040;&#25968;&#37327;&#22686;&#24378;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#23548;&#33268;&#23376;&#20248;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#26412;&#22320;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181; robust quantity-aware aggregation algorithm for federated learning (FedRA)&#65292;&#36890;&#36807;&#23545;&#26412;&#22320;&#25968;&#25454;&#25968;&#37327;&#30340;&#24863;&#30693;&#26469;&#25191;&#34892;&#32858;&#21512;&#65292;&#24182;&#33021;&#22815;&#25269;&#24481;&#25968;&#37327;&#22686;&#24378;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple clients to collaboratively train models without sharing their local data, and becomes an important privacy-preserving machine learning framework. However, classical FL faces serious security and robustness problem, e.g., malicious clients can poison model updates and at the same time claim large quantities to amplify the impact of their model updates in the model aggregation. Existing defense methods for FL, while all handling malicious model updates, either treat all quantities benign or simply ignore/truncate the quantities of all clients. The former is vulnerable to quantity-enhanced attack, while the latter leads to sub-optimal performance since the local data on different clients is usually in significantly different sizes. In this paper, we propose a robust quantity-aware aggregation algorithm for federated learning, called FedRA, to perform the aggregation with awareness of local data quantities while being able to defend against quantity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MICDIR&#26041;&#27861;&#65292;&#22312;&#22810;&#23610;&#24230;&#19978;&#20351;&#29992;UNetMSS&#21644;&#33258;&#26500;&#24314;&#22270;&#28508;&#21464;&#37327;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#20013;&#23384;&#22312;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#22823;&#24418;&#21464;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.04317</link><description>&lt;p&gt;
MICDIR: &#20351;&#29992;UNetMSS&#21644;&#33258;&#26500;&#24314;&#22270;&#28508;&#21464;&#37327;&#30340;&#22810;&#23610;&#24230;&#21453;&#21521;&#19968;&#33268;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent. (arXiv:2203.04317v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MICDIR&#26041;&#27861;&#65292;&#22312;&#22810;&#23610;&#24230;&#19978;&#20351;&#29992;UNetMSS&#21644;&#33258;&#26500;&#24314;&#22270;&#28508;&#21464;&#37327;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#20013;&#23384;&#22312;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#22823;&#24418;&#21464;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#37197;&#20934;&#26159;&#23558;&#19981;&#21516;&#22270;&#20687;&#24102;&#20837;&#21040;&#19968;&#20010;&#20849;&#21516;&#22352;&#26631;&#31995;&#30340;&#36807;&#31243;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#22914;&#36965;&#24863;&#12289;&#22270;&#20687;&#26816;&#32034;&#21644;&#21307;&#23398;&#25104;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#22797;&#26434;&#30340;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#25552;&#20986;&#20102;&#20960;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#37197;&#20934;&#25216;&#26415;&#12290;&#20363;&#22914;Voxelmorph&#31561;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#25216;&#26415;&#65292;&#25104;&#21151;&#25429;&#25417;&#21040;&#26356;&#32454;&#24494;&#30340;&#21464;&#21270;&#24182;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#21464;&#24418;&#12290;&#28982;&#32780;&#65292;Voxelmorph&#12289;ICNet&#21644;FIRE&#31561;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#32534;&#30721;&#20840;&#23616;&#20381;&#36182;&#24615;&#65288;&#21363;&#25152;&#25552;&#20379;&#22270;&#20687;&#30340;&#25972;&#20307;&#35299;&#21078;&#35270;&#22270;&#65289;&#65292;&#22240;&#27492;&#19981;&#33021;&#36319;&#36394;&#22823;&#30340;&#24418;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#20197;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;Voxelmorph&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image registration is the process of bringing different images into a common coordinate system - a technique widely used in various applications of computer vision, such as remote sensing, image retrieval, and, most commonly, medical imaging. Deep learning based techniques have been applied successfully to tackle various complex medical image processing problems, including medical image registration. Over the years, several image registration techniques have been proposed using deep learning. Deformable image registration techniques such as Voxelmorph have been successful in capturing finer changes and providing smoother deformations. However, Voxelmorph, as well as ICNet and FIRE, do not explicitly encode global dependencies (i.e. the overall anatomical view of the supplied image) and, therefore, cannot track large deformations. In order to tackle the aforementioned problems, this paper extends the Voxelmorph approach in three different ways. To improve the performance in case of smal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;SIMMC 2.0&#25361;&#25112;&#20013;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#34920;&#36798;&#23545;&#20110;&#27495;&#20041;&#26816;&#27979;&#21644;&#20849;&#35782;&#28040;&#35299;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#21333;&#27169;&#24577;&#30340;&#20849;&#35782;&#28040;&#35299;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2202.12645</link><description>&lt;p&gt;
&#22312;SIMMC 2.0&#25361;&#25112;&#20013;&#25506;&#32034;&#22810;&#27169;&#24577;&#34920;&#36798;&#23545;&#20110;&#27495;&#20041;&#26816;&#27979;&#21644;&#20849;&#35782;&#28040;&#35299;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring Multi-Modal Representations for Ambiguity Detection &amp; Coreference Resolution in the SIMMC 2.0 Challenge. (arXiv:2202.12645v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;SIMMC 2.0&#25361;&#25112;&#20013;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#34920;&#36798;&#23545;&#20110;&#27495;&#20041;&#26816;&#27979;&#21644;&#20849;&#35782;&#28040;&#35299;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#21333;&#27169;&#24577;&#30340;&#20849;&#35782;&#28040;&#35299;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#34920;&#36798;&#65292;&#27604;&#22914;&#20195;&#35789;&#21644;&#25351;&#31034;&#25551;&#36848;&#65292;&#26082;&#21463;&#21069;&#25991;&#30340;&#35821;&#35328;&#35821;&#22659;&#30340;&#24433;&#21709;&#65292;&#20063;&#21463;&#21040;&#24403;&#21069;&#35270;&#35273;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#21457;&#35328;&#32773;&#30340;&#25351;&#31034;&#25551;&#36848;&#24182;&#19981;&#33021;&#24635;&#26159;&#21807;&#19968;&#22320;&#30830;&#23450;&#25351;&#20195;&#29289;&#65292;&#23548;&#33268;&#38656;&#35201;&#36890;&#36807;&#21518;&#32493;&#30340;&#28548;&#28165;&#20132;&#27969;&#26469;&#28040;&#38500;&#27495;&#20041;&#12290;&#22240;&#27492;&#65292;&#22312;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#26377;&#25928;&#30340;&#27495;&#20041;&#26816;&#27979;&#21644;&#20849;&#35782;&#28040;&#35299;&#23545;&#20110;&#20219;&#21153;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20316;&#20026;SIMMC 2.0&#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;TOD-BERT&#21644;LXMERT&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#19968;&#20123;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;1&#65289;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#26816;&#27979;&#27495;&#20041;&#65307;&#65288;2&#65289;&#22522;&#20110;&#21333;&#27169;&#24577;&#30340;&#20849;&#35782;&#28040;&#35299;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26234;&#33021;&#29289;&#20307;&#34920;&#31034;&#26469;&#36991;&#20813;&#38656;&#35201;&#35270;&#35273;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anaphoric expressions, such as pronouns and referential descriptions, are situated with respect to the linguistic context of prior turns, as well as, the immediate visual environment. However, a speaker's referential descriptions do not always uniquely identify the referent, leading to ambiguities in need of resolution through subsequent clarificational exchanges. Thus, effective Ambiguity Detection and Coreference Resolution are key to task success in Conversational AI. In this paper, we present models for these two tasks as part of the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERT and LXMERT based models, compare them to a number of baselines and provide ablation experiments. Our results show that (1) language models are able to exploit correlations in the data to detect ambiguity; and (2) unimodal coreference resolution models can avoid the need for a vision component, through the use of smart object representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#23398;&#27169;&#22411;&#30340;&#28145;&#24230;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#36328;&#20250;&#35805;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#26080;&#38656;&#25913;&#21464;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#36890;&#36947;&#24402;&#19968;&#21270;&#21644;&#27431;&#27663;&#23545;&#40784;&#26500;&#24314;&#20102;&#22495;&#19981;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.09559</link><description>&lt;p&gt;
&#29992;&#36890;&#29992;&#28145;&#24230;&#22495;&#36866;&#24212;&#26694;&#26550;&#36827;&#34892;&#36328;&#20250;&#35805;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#30340;&#39044;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Priming Cross-Session Motor Imagery Classification with A Universal Deep Domain Adaptation Framework. (arXiv:2202.09559v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#23398;&#27169;&#22411;&#30340;&#28145;&#24230;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#36328;&#20250;&#35805;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#26080;&#38656;&#25913;&#21464;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#36890;&#36947;&#24402;&#19968;&#21270;&#21644;&#27431;&#27663;&#23545;&#40784;&#26500;&#24314;&#20102;&#22495;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24819;&#35937;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#33539;&#24335;&#12290;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20855;&#26377;&#38750;&#24179;&#31283;&#24615;&#21644;&#20302;&#20449;&#22122;&#27604;&#65292;&#23558;&#21516;&#19968;&#20010;&#21442;&#19982;&#32773;&#22312;&#19981;&#21516;EEG&#35760;&#24405;&#20250;&#35805;&#20013;&#30340;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;EEG&#25968;&#25454;&#20998;&#24067;&#22312;&#19981;&#21516;&#37319;&#38598;&#20250;&#35805;&#20043;&#38388;&#21487;&#33021;&#24046;&#24322;&#24040;&#22823;&#12290;&#34429;&#28982;&#23558;&#36328;&#20250;&#35805;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#35270;&#20026;&#22495;&#36866;&#24212;&#38382;&#39064;&#26159;&#30452;&#35266;&#30340;&#65292;&#20294;&#20854;&#21407;&#29702;&#21644;&#21487;&#34892;&#30340;&#26041;&#27861;&#23578;&#26410;&#38416;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23398;&#27169;&#22411;&#30340;&#21516;&#26500;&#28145;&#24230;&#22495;&#36866;&#24212;&#65288;SDDA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#20250;&#35805;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#25913;&#21464;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#36890;&#36807;&#36890;&#36947;&#24402;&#19968;&#21270;&#21644;&#27431;&#27663;&#23545;&#40784;&#20849;&#21516;&#26500;&#24314;&#20102;&#22495;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motor imagery (MI) is a common brain computer interface (BCI) paradigm. EEG is non-stationary with low signal-to-noise, classifying motor imagery tasks of the same participant from different EEG recording sessions is generally challenging, as EEG data distribution may vary tremendously among different acquisition sessions. Although it is intuitive to consider the cross-session MI classification as a domain adaptation problem, the rationale and feasible approach is not elucidated. In this paper, we propose a Siamese deep domain adaptation (SDDA) framework for cross-session MI classification based on mathematical models in domain adaptation theory. The proposed framework can be easily applied to most existing artificial neural networks without altering the network structure, which facilitates our method with great flexibility and transferability. In the proposed framework, domain invariants were firstly constructed jointly with channel normalization and Euclidean alignment. Then, embeddi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.11104</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11104
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#25509;&#22270;&#20013;&#25214;&#21040;&#26368;&#20248;&#36335;&#24452;&#38656;&#35201;&#30830;&#23450;&#27839;&#30528;&#22270;&#30340;&#36793;&#32536;&#34892;&#36827;&#30340;&#26368;&#23567;&#24635;&#25104;&#26412;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20960;&#31181;&#32463;&#20856;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#36890;&#24120;&#25152;&#26377;&#36793;&#32536;&#30340;&#25104;&#26412;&#37117;&#26159;&#39044;&#20808;&#23450;&#20041;&#22909;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#24819;&#35201;&#26681;&#25454;&#26576;&#20010;&#20219;&#21153;&#30340;&#35201;&#27714;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#25913;&#21464;&#25104;&#26412;&#26102;&#65292;&#36890;&#24120;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31361;&#35302;&#26435;&#37325;&#26469;&#23450;&#20041;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#36825;&#20801;&#35768;&#20351;&#29992;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#36827;&#34892;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#24403;&#20174;&#19968;&#20010;&#21021;&#22987;&#27963;&#36291;&#24230;&#20540;&#20026;1&#24320;&#22987;&#26102;&#65292;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#30340;&#27963;&#21160;&#20256;&#25773;&#23558;&#23548;&#33268;&#19982;Bellman-Ford&#31639;&#27861;&#25214;&#21040;&#30340;&#35299;&#30456;&#21516;&#30340;&#35299;&#12290;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19982;Bellman-Ford&#30456;&#21516;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#65288;&#22914;&#36203;&#24067;&#23398;&#20064;&#65289;&#21487;&#20197;&#35843;&#25972;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal paths in connected graphs requires determining the smallest total cost for traveling along the graph's edges. This problem can be solved by several classical algorithms where, usually, costs are predefined for all edges. Conventional planning methods can, thus, normally not be used when wanting to change costs in an adaptive way following the requirements of some task. Here we show that one can define a neural network representation of path finding problems by transforming cost values into synaptic weights, which allows for online weight adaptation using network learning mechanisms. When starting with an initial activity value of one, activity propagation in this network will lead to solutions, which are identical to those found by the Bellman-Ford algorithm. The neural network has the same algorithmic complexity as Bellman-Ford and, in addition, we can show that network learning mechanisms (such as Hebbian learning) can adapt the weights in the network augmenting the r
&lt;/p&gt;</description></item></channel></rss>