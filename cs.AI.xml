<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;</title><link>https://rss.arxiv.org/abs/2402.01030</link><description>&lt;p&gt;
&#21487;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#33021;&#22815;&#28608;&#21457;&#26356;&#20986;&#33394;&#30340;LLM&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Executable Code Actions Elicit Better LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01030
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20307;&#20855;&#22791;&#25191;&#34892;&#24191;&#27867;&#34892;&#21160;&#30340;&#33021;&#21147;&#65292;&#22914;&#35843;&#29992;&#24037;&#20855;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#31561;&#65292;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;LLM&#26234;&#33021;&#20307;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;JSON&#25110;&#25991;&#26412;&#30340;&#39044;&#23450;&#20041;&#26684;&#24335;&#26469;&#20135;&#29983;&#34892;&#21160;&#65292;&#36825;&#36890;&#24120;&#21463;&#38480;&#20110;&#21463;&#38480;&#21046;&#30340;&#34892;&#21160;&#31354;&#38388;&#65288;&#20363;&#22914;&#65292;&#39044;&#23450;&#20041;&#24037;&#20855;&#30340;&#33539;&#22260;&#65289;&#21644;&#21463;&#38480;&#30340;&#28789;&#27963;&#24615;&#65288;&#20363;&#22914;&#65292;&#26080;&#27861;&#32452;&#21512;&#22810;&#20010;&#24037;&#20855;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#23558;LLM&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65288;CodeAct&#65289;&#12290;CodeAct&#19982;Python&#35299;&#37322;&#22120;&#38598;&#25104;&#65292;&#21487;&#20197;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#22312;&#26032;&#30340;&#35266;&#23519;&#20013;&#21160;&#24577;&#20462;&#35746;&#20808;&#21069;&#30340;&#34892;&#21160;&#25110;&#21457;&#20986;&#26032;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;17&#20010;LLM&#22312;API-Bank&#21644;&#26032;&#32534;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;CodeAct&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#65288;&#25104;&#21151;&#29575;&#39640;&#20986;20%&#65289;&#12290;CodeAct&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#28608;&#21169;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2403.12031</link><description>&lt;p&gt;
ROUTERBENCH&#65306;&#29992;&#20110;&#22810;LLM&#36335;&#30001;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ROUTERBENCH: A Benchmark for Multi-LLM Routing System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#19981;&#26029;&#25193;&#22823;&#65292;&#23545;&#26377;&#25928;&#30340;&#26381;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#22810;&#26679;&#24615;&#65292;&#20294;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#26368;&#20248;&#22320;&#35299;&#20915;&#25152;&#26377;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#24179;&#34913;&#24615;&#33021;&#21644;&#25104;&#26412;&#20043;&#38388;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#38480;&#21046;&#65292;&#21457;&#23637;&#20102;LLM&#36335;&#30001;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#32467;&#21512;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#20811;&#26381;&#21333;&#20010;LLMs&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#22120;&#24615;&#33021;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#30340;&#21151;&#25928;&#65292;&#20197;&#21450;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#20195;&#34920;&#24615;LLMs&#30340;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36335;&#30001;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12031v1 Announce Type: cross  Abstract: As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and del
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ultraman&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#19968;&#22270;&#20687;&#24555;&#36895;&#37325;&#24314;&#24102;&#26377;&#32441;&#29702;&#30340;3D&#20154;&#20307;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#30340;&#26694;&#26550;&#25552;&#39640;&#20102;&#37325;&#24314;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.12028</link><description>&lt;p&gt;
Ultraman&#65306;&#20855;&#26377;&#36229;&#39640;&#36895;&#21644;&#32454;&#33410;&#30340;&#21333;&#22270;&#20687;3D&#20154;&#20307;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Ultraman: Single Image 3D Human Reconstruction with Ultra Speed and Detail
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ultraman&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#19968;&#22270;&#20687;&#24555;&#36895;&#37325;&#24314;&#24102;&#26377;&#32441;&#29702;&#30340;3D&#20154;&#20307;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#30340;&#26694;&#26550;&#25552;&#39640;&#20102;&#37325;&#24314;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#20154;&#20307;&#37325;&#24314;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#32791;&#26102;&#19988;&#38590;&#20197;&#25429;&#25417;&#20154;&#20307;&#30340;&#35814;&#32454;&#22806;&#35266;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ultraman&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21333;&#19968;&#22270;&#20687;&#24555;&#36895;&#37325;&#24314;&#24102;&#26377;&#32441;&#29702;&#30340;3D&#20154;&#20307;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#8220;Ultraman&#8221;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#37325;&#24314;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#39640;&#36136;&#37327;&#30340;&#32441;&#29702;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#26032;&#30340;&#20154;&#20307;&#37325;&#24314;&#26694;&#26550;&#65292;&#21253;&#25324;&#20960;&#20010;&#37096;&#20998;&#65292;&#20960;&#20309;&#37325;&#24314;&#12289;&#32441;&#29702;&#29983;&#25104;&#21644;&#32441;&#29702;&#26144;&#23556;&#12290;&#39318;&#20808;&#20351;&#29992;&#20102;&#19968;&#20010;&#32593;&#26684;&#37325;&#24314;&#26694;&#26550;&#65292;&#20934;&#30830;&#22320;&#20174;&#21333;&#19968;&#22270;&#20687;&#20013;&#25552;&#21462;3D&#20154;&#20307;&#24418;&#29366;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#19968;&#22270;&#20687;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#20154;&#20307;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#32441;&#29702;&#26144;&#23556;&#26041;&#27861;&#26469;&#20248;&#21270;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12028v1 Announce Type: cross  Abstract: 3D human body reconstruction has been a challenge in the field of computer vision. Previous methods are often time-consuming and difficult to capture the detailed appearance of the human body. In this paper, we propose a new method called \emph{Ultraman} for fast reconstruction of textured 3D human models from a single image. Compared to existing techniques, \emph{Ultraman} greatly improves the reconstruction speed and accuracy while preserving high-quality texture details. We present a set of new frameworks for human reconstruction consisting of three parts, geometric reconstruction, texture generation and texture mapping. Firstly, a mesh reconstruction framework is used, which accurately extracts 3D human shapes from a single image. At the same time, we propose a method to generate a multi-view consistent image of the human body based on a single image. This is finally combined with a novel texture mapping method to optimize texture 
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12026</link><description>&lt;p&gt;
FlexCap&#65306;&#22312;&#22270;&#20687;&#20013;&#29983;&#25104;&#20016;&#23500;&#12289;&#26412;&#22320;&#21270;&#21644;&#28789;&#27963;&#30340;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
FlexCap: Generating Rich, Localized, and Flexible Captions in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12026
&lt;/p&gt;
&lt;p&gt;
FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;$\textit{&#28789;&#27963;&#23383;&#24149;}$&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#38271;&#24230;&#19981;&#21516;&#30340;&#29305;&#23450;&#21306;&#22495;&#25551;&#36848;&#12290;&#35813;&#27169;&#22411;FlexCap&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20026;&#36755;&#20837;&#30340;&#36793;&#30028;&#26694;&#29983;&#25104;&#38271;&#24230;&#26465;&#20214;&#30340;&#23383;&#24149;&#65292;&#20174;&#32780;&#21487;&#20197;&#25511;&#21046;&#20854;&#36755;&#20986;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#25551;&#36848;&#33539;&#22260;&#20174;&#31616;&#27905;&#30340;&#23545;&#35937;&#26631;&#31614;&#21040;&#35814;&#32454;&#30340;&#23383;&#24149;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#24102;&#23383;&#24149;&#30340;&#22270;&#20687;&#24320;&#22987;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#21306;&#22495;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#23383;&#24149;&#21151;&#33021;&#26377;&#20960;&#20010;&#23453;&#36149;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;FlexCap&#22312;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;FlexCap&#29983;&#25104;&#26412;&#22320;&#21270;&#25551;&#36848;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#26469;&#26500;&#24314;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#31995;&#32479;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#22312;&#35768;&#22810;VQ&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#35265;&#35299;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#23545;&#40784;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.12017</link><description>&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#20316;&#20026;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Fine-Tuning as Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#35265;&#35299;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#23545;&#40784;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#40784;&#30340;&#20027;&#27969;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#31867;&#25110;AI&#21453;&#39304;&#65292;&#24182;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#29305;&#23450;&#31867;&#22411;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#21508;&#31181;&#24773;&#26223;&#19979;&#19982;&#19987;&#23478;&#28436;&#31034;&#23545;&#40784;&#26356;&#20026;&#29616;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#26694;&#26550;&#65292;&#20197;&#28436;&#31034;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#26469;&#35268;&#21010;&#23545;&#40784;LLMs&#30340;&#38382;&#39064;&#12290;&#20511;&#37492;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;LLM&#23545;&#40784;&#20219;&#21153;&#20013;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#19981;&#21516;&#26041;&#27861;&#30340;&#35206;&#30422;&#29575;&#21644;&#23547;&#25214;&#27169;&#24335;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#32463;&#20856;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#30340;&#21033;&#24330;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19981;&#21516;&#26041;&#27861;&#34920;&#29616;&#31361;&#20986;&#30340;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12017v1 Announce Type: cross  Abstract: The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.
&lt;/p&gt;</description></item><item><title>EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12014</link><description>&lt;p&gt;
EnvGen: &#36890;&#36807;LLMs&#29983;&#25104;&#21644;&#35843;&#25972;&#29615;&#22659;&#20197;&#35757;&#32451;&#20855;&#36523;&#20307;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12014
&lt;/p&gt;
&lt;p&gt;
EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#36890;&#36807;&#20114;&#21160;&#36827;&#34892;&#20855;&#36523;&#20307;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#30452;&#25509;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#65292;&#20197;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#19979;&#19968;&#27493;&#12290;LLM&#20195;&#29702;&#30001;&#20110;&#20854;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20197;&#24448;&#36739;&#23567;&#30340;&#20195;&#29702;&#34920;&#29616;&#26356;&#24378;&#65307;&#20294;&#39057;&#32321;&#35843;&#29992;LLMs&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;EnvGen&#65292;&#19968;&#20010;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#31034;&#19968;&#20010;LLM&#29983;&#25104;&#35757;&#32451;&#29615;&#22659;&#65292;&#20351;&#20195;&#29702;&#21487;&#20197;&#24555;&#36895;&#24182;&#34892;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#33719;&#24471;&#20219;&#21153;&#25551;&#36848;&#21644;&#27169;&#25311;&#22120;&#30446;&#26631;&#65292;&#28982;&#21518;&#34987;&#35201;&#27714;&#29983;&#25104;&#19968;&#32452;&#29615;&#22659;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12014v1 Announce Type: cross  Abstract: Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22810;&#35270;&#35282;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#24182;&#24341;&#20837;3D&#24863;&#30693;&#21435;&#22122;&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25968;&#25454;&#35757;&#32451;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12010</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22810;&#35270;&#35282;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12010
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22810;&#35270;&#35282;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#24182;&#24341;&#20837;3D&#24863;&#30693;&#21435;&#22122;&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#35270;&#35282;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25968;&#25454;&#35757;&#32451;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22522;&#20110;&#25991;&#26412;&#25110;&#21333;&#22270;&#20687;&#25552;&#31034;&#30340;&#22810;&#35270;&#35282;&#22270;&#20687;&#26159;&#21019;&#24314;3D&#20869;&#23481;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23545;&#20110;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#21644;&#22914;&#20309;&#30830;&#20445;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#36825;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#37117;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#19982;&#21033;&#29992;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#30340;&#19968;&#33268;&#24615;&#22810;&#35270;&#35282;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#20174;&#29616;&#25104;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#24494;&#35843;&#32780;&#26469;&#30340;&#12290;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#26356;&#36866;&#21512;&#20110;&#22810;&#35270;&#35282;&#29983;&#25104;&#65292;&#22240;&#20026;&#29983;&#25104;&#36825;&#20123;&#22270;&#20687;&#30340;&#22522;&#30784;&#32593;&#32476;&#26550;&#26500;&#37319;&#29992;&#20102;&#26102;&#38388;&#27169;&#22359;&#26469;&#24378;&#21046;&#24103;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#20016;&#23500;&#22810;&#26679;&#65292;&#23548;&#33268;&#20943;&#23569;&#20102;&#35757;&#32451;&#24494;&#35843;&#39046;&#22495;&#24046;&#36317;&#12290;&#20026;&#20102;&#22686;&#24378;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;3D&#24863;&#30693;&#21435;&#22122;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12010v1 Announce Type: cross  Abstract: Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first empl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#33014;&#22218;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#30382;&#32932;&#30284;&#35786;&#26029;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12009</link><description>&lt;p&gt;
&#21033;&#29992;&#33014;&#22218;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#30382;&#32932;&#30284;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#33014;&#22218;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#30382;&#32932;&#30284;&#35786;&#26029;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30382;&#32932;&#30149;&#21464;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#65292;&#22797;&#26434;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23545;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26041;&#27861;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#24615;&#21152;&#21095;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#26377;&#25928;&#23398;&#20064;&#23569;&#25968;&#31867;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#19982;&#33014;&#22218;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#20998;&#31867;&#24615;&#33021;&#12290;GNNs&#25797;&#38271;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#20026;&#25429;&#33719;&#22797;&#26434;&#27169;&#24335;&#21644;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#32423;&#26426;&#21046;&#65292;&#36828;&#36229;&#20256;&#32479;CNN&#30340;&#33021;&#21147;&#12290;&#33014;&#22218;&#32593;&#32476;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20379;&#23545;&#31354;&#38388;&#23618;&#27425;&#30340;&#20248;&#36234;&#35782;&#21035;&#33021;&#21147;&#26469;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12009v1 Announce Type: cross  Abstract: In the realm of skin lesion image classification, the intricate spatial and semantic features pose significant challenges for conventional Convolutional Neural Network (CNN)-based methodologies. These challenges are compounded by the imbalanced nature of skin lesion datasets, which hampers the ability of models to learn minority class features effectively. Despite augmentation strategies, such as those using Generative Adversarial Networks (GANs), previous attempts have not fully addressed these complexities. This study introduces an innovative approach by integrating Graph Neural Networks (GNNs) with Capsule Networks to enhance classification performance. GNNs, known for their proficiency in handling graph-structured data, offer an advanced mechanism for capturing complex patterns and relationships beyond the capabilities of traditional CNNs. Capsule Networks further contribute by providing superior recognition of spatial hierarchies 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#21407;&#22987;&#35270;&#39057;&#21644;&#32534;&#36753;&#35270;&#39057;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#65292;&#22312;&#20998;&#25968;&#33976;&#39311;&#36807;&#31243;&#20013;&#35299;&#20915;&#20102;&#26032;&#20869;&#23481;&#24341;&#20837;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#32467;&#26500;&#21644;&#36816;&#21160;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12002</link><description>&lt;p&gt;
DreamMotion&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#20998;&#25968;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12002
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#21407;&#22987;&#35270;&#39057;&#21644;&#32534;&#36753;&#35270;&#39057;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#65292;&#22312;&#20998;&#25968;&#33976;&#39311;&#36807;&#31243;&#20013;&#35299;&#20915;&#20102;&#26032;&#20869;&#23481;&#24341;&#20837;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#32467;&#26500;&#21644;&#36816;&#21160;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12002v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#25193;&#25955;&#24335;&#35270;&#39057;&#32534;&#36753;&#22312;&#22270;&#20687;&#32534;&#36753;&#25991;&#29486;&#20013;&#26174;&#29616;&#20102;&#19968;&#39033;&#29420;&#29305;&#25361;&#25112;&#65306;&#24314;&#31435;&#30495;&#23454;&#19990;&#30028;&#36816;&#21160;&#12290;&#19982;&#29616;&#26377;&#30340;&#35270;&#39057;&#32534;&#36753;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#19987;&#27880;&#20110;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65292;&#20197;&#35268;&#36991;&#26631;&#20934;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#20174;&#24050;&#23637;&#29616;&#33258;&#28982;&#36816;&#21160;&#30340;&#35270;&#39057;&#20013;&#21551;&#21160;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#35270;&#39057;&#20998;&#25968;&#33976;&#39311;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#20837;&#30446;&#26631;&#25991;&#26412;&#25351;&#31034;&#30340;&#26032;&#20869;&#23481;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#32467;&#26500;&#21644;&#36816;&#21160;&#20559;&#24046;&#12290;&#20026;&#20102;&#25269;&#28040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#20998;&#25968;&#33976;&#39311;&#36807;&#31243;&#20013;&#21305;&#37197;&#21407;&#22987;&#35270;&#39057;&#21644;&#32534;&#36753;&#35270;&#39057;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#20998;&#25968;&#33976;&#39311;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#24212;&#29992;&#20110;&#32423;&#32852;&#21644;&#38750;&#32423;&#32852;&#35270;&#39057;&#25193;&#25955;&#26694;&#26550;&#12290;&#36890;&#36807;&#19982;&#39046;&#20808;&#26041;&#27861;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#35270;&#39057;&#32534;&#36753;&#20013;&#30340;&#21331;&#36234;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12002v1 Announce Type: cross  Abstract: Text-driven diffusion-based video editing presents a unique challenge not encountered in image editing literature: establishing real-world motion. Unlike existing video editing approaches, here we focus on score distillation sampling to circumvent the standard reverse diffusion process and initiate optimization from videos that already exhibit natural motion. Our analysis reveals that while video score distillation can effectively introduce new content indicated by target text, it can also cause significant structure and motion deviation. To counteract this, we propose to match space-time self-similarities of the original video and the edited video during the score distillation. Thanks to the use of score distillation, our approach is model-agnostic, which can be applied for both cascaded and non-cascaded video diffusion frameworks. Through extensive comparisons with leading methods, our approach demonstrates its superiority in alterin
&lt;/p&gt;</description></item><item><title>Notochord&#26159;&#19968;&#31181;&#28789;&#27963;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;MIDI&#34920;&#28436;&#20013;&#29983;&#25104;&#22810;&#38899;&#36712;MIDI&#24182;&#20197;&#20302;&#20110;10&#27627;&#31186;&#30340;&#24310;&#36831;&#21709;&#24212;&#36755;&#20837;&#65292;&#25903;&#25345;&#22810;&#31181;&#20132;&#20114;&#38899;&#20048;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12000</link><description>&lt;p&gt;
Notochord&#65306;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;MIDI&#34920;&#28436;&#30340;&#28789;&#27963;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12000
&lt;/p&gt;
&lt;p&gt;
Notochord&#26159;&#19968;&#31181;&#28789;&#27963;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;MIDI&#34920;&#28436;&#20013;&#29983;&#25104;&#22810;&#38899;&#36712;MIDI&#24182;&#20197;&#20302;&#20110;10&#27627;&#31186;&#30340;&#24310;&#36831;&#21709;&#24212;&#36755;&#20837;&#65292;&#25903;&#25345;&#22810;&#31181;&#20132;&#20114;&#38899;&#20048;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#25968;&#25454;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#36924;&#30495;&#30340;&#32467;&#26524;&#65292;&#24182;&#26377;&#26395;&#36827;&#20837;&#21508;&#31181;&#21019;&#20316;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#34920;&#28436;&#29615;&#22659;&#20013;&#23545;&#20854;&#23454;&#38469;&#25928;&#26524;&#36827;&#34892;&#30740;&#31350;&#36824;&#24456;&#23569;&#65292;&#29992;&#25143;&#34892;&#20026;&#30340;&#32467;&#26524;&#36890;&#24120;&#24212;&#35813;&#21363;&#26102;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Notochord&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#32467;&#26500;&#21270;&#20107;&#20214;&#24207;&#21015;&#30340;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#24182;&#22312;Lakh MIDI&#25968;&#25454;&#38598;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27010;&#29575;&#24418;&#24335;&#20801;&#35768;&#22312;&#23376;&#20107;&#20214;&#32423;&#21035;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#24178;&#39044;&#65292;&#36825;&#20351;&#24471;&#19968;&#20010;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#22810;&#26679;&#20132;&#20114;&#38899;&#20048;&#21151;&#33021;&#30340;&#22522;&#30784;&#65292;&#21253;&#25324;&#21487;&#25805;&#25511;&#30340;&#29983;&#25104;&#12289;&#21644;&#22768;&#12289;&#26426;&#22120;&#21363;&#20852;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#30028;&#38754;&#12290;Notochord&#21487;&#20197;&#29983;&#25104;&#22810;&#38899;&#36712;MIDI&#24182;&#22312;&#21313;&#27627;&#31186;&#20197;&#19979;&#30340;&#24310;&#36831;&#20869;&#21709;&#24212;&#36755;&#20837;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35757;&#32451;&#20195;&#30721;&#65292;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#20132;&#20114;&#31034;&#20363;&#20316;&#20026;&#24320;&#28304;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12000v1 Announce Type: cross  Abstract: Deep learning-based probabilistic models of musical data are producing increasingly realistic results and promise to enter creative workflows of many kinds. Yet they have been little-studied in a performance setting, where the results of user actions typically ought to feel instantaneous. To enable such study, we designed Notochord, a deep probabilistic model for sequences of structured events, and trained an instance of it on the Lakh MIDI dataset. Our probabilistic formulation allows interpretable interventions at a sub-event level, which enables one model to act as a backbone for diverse interactive musical functions including steerable generation, harmonization, machine improvisation, and likelihood-based interfaces. Notochord can generate polyphonic and multi-track MIDI, and respond to inputs with latency below ten milliseconds. Training code, model checkpoints and interactive examples are provided as open source software.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.11996</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#30693;&#35782;&#25552;&#21462;&#12289;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#26234;&#33021;&#22270;&#25512;&#29702;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11996
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#23558;&#19968;&#32452;&#28041;&#21450;&#29983;&#29289;&#26448;&#26009;&#39046;&#22495;&#30340;1,000&#31687;&#31185;&#23398;&#35770;&#25991;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26412;&#20307;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24615;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#32452;&#21512;&#25490;&#21517;&#65292;&#25506;&#27979;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#22270;&#36941;&#21382;&#36335;&#24452;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#20837;&#30340;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26597;&#35810;&#65292;&#35782;&#21035;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#26448;&#26009;&#35774;&#35745;&#21450;&#20854;&#34892;&#20026;&#12290;&#19968;&#39033;&#27604;&#36739;&#25581;&#31034;&#20102;&#29983;&#29289;&#26448;&#26009;&#21644;&#36125;&#22810;&#33452;&#31532;&#20061;&#20132;&#21709;&#26354;&#20043;&#38388;&#30340;&#35814;&#32454;&#32467;&#26500;&#30456;&#20284;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#36890;&#36807;&#21516;&#26500;&#26144;&#23556;&#20849;&#20139;&#22797;&#26434;&#24615;&#27169;&#24335;&#12290;&#35813;&#31639;&#27861;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20998;&#32423;&#33740;&#19997;&#20307;&#30340;&#22797;&#21512;&#26448;&#26009;&#65292;&#23558;&#22270;&#37319;&#26679;&#30340;&#32852;&#21512;&#21512;&#25104;&#21407;&#29702;&#19982;&#24247;&#23450;&#26031;&#22522;&#12298;&#31532;&#19971;&#32452;&#25104;&#12299;&#20013;&#25552;&#21462;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#25945;&#23398;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#21453;&#39304;&#20013;&#25552;&#21462;&#12289;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#24635;&#32467;SETs&#12290;</title><link>https://arxiv.org/abs/2403.11984</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#20026;&#25945;&#23398;&#35780;&#20272;&#21019;&#24314;&#23450;&#24615;&#20195;&#30721;&#25163;&#20876;
&lt;/p&gt;
&lt;p&gt;
Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#25945;&#23398;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#21453;&#39304;&#20013;&#25552;&#21462;&#12289;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#24635;&#32467;SETs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#39304;&#26159;&#25913;&#36827;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#26377;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#22823;&#37327;&#21453;&#39304;&#26102;&#65292;&#23558;&#20449;&#24687;&#25552;&#28860;&#25104;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20998;&#26512;&#25945;&#23398;&#35780;&#20272;&#65288;SETs&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#25152;&#22823;&#22411;&#20844;&#31435;&#22823;&#23398;&#30340;5000&#20010;SETs&#35821;&#26009;&#24211;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11984v1 Announce Type: cross  Abstract: Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs). We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university. We show that the method can be used to extract, embed, cluster, and summarize the SETs to id
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#30693;&#24335;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.11966</link><description>&lt;p&gt;
&#36890;&#30693;&#35889;&#24402;&#19968;&#21270;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Informed Spectral Normalized Gaussian Processes for Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11966
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#30693;&#24335;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#21442;&#25968;&#20998;&#24067;&#20026;&#36890;&#30693;&#24335;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#24335;&#65292;&#20197;&#34920;&#31034;&#20808;&#39564;&#19987;&#23478;&#21644;&#19990;&#30028;&#30693;&#35782;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#26679;&#30340;&#20449;&#24687;&#20808;&#39564;&#26469;&#35268;&#33539;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20250;&#22686;&#21152;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#29575;DL&#27169;&#22411;&#30340;&#24120;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36817;&#20284;&#26041;&#27861;&#21487;&#33021;&#35745;&#31639;&#26114;&#36149;&#65292;&#38656;&#35201;&#22810;&#27425;&#25512;&#29702;&#21644;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#35745;&#31639;&#39640;&#25928;&#30340;&#26368;&#21518;&#19968;&#23618;&#26680;&#36924;&#36817;&#22914;&#35889;&#24402;&#19968;&#21270;&#39640;&#26031;&#36807;&#31243;&#65288;SNGPs&#65289;&#26159;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;SNGPs&#30340;&#22522;&#20110;&#26032;&#39062;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#20195;&#34920;&#20808;&#21069;&#20219;&#21153;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#36890;&#30693;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#24314;&#31435;&#22312;&#25104;&#29087;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#19981;&#38656;&#35201;&#35760;&#24518;&#25110;&#21442;&#25968;&#25193;&#23637;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#36890;&#30693;SNGP&#27169;&#22411;&#24212;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11966v1 Announce Type: cross  Abstract: Prior parameter distributions provide an elegant way to represent prior expert and world knowledge for informed learning. Previous work has shown that using such informative priors to regularize probabilistic deep learning (DL) models increases their performance and data-efficiency. However, commonly used sampling-based approximations for probabilistic DL models can be computationally expensive, requiring multiple inference passes and longer training times. Promising alternatives are compute-efficient last layer kernel approximations like spectral normalized Gaussian processes (SNGPs). We propose a novel regularization-based continual learning method for SNGPs, which enables the use of informative priors that represent prior knowledge learned from previous tasks. Our proposal builds upon well-established methods and requires no rehearsal memory or parameter expansion. We apply our informed SNGP model to the trajectory prediction proble
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36816;&#21160;&#34917;&#20607;&#26469;&#22686;&#24378;&#37325;&#24314;&#36136;&#37327;&#65292;&#25552;&#20986;&#23558;&#36755;&#20837;&#24103;&#21644;&#31232;&#30095;&#32534;&#30721;&#36827;&#34892;&#21464;&#25442;&#65292;&#24182;&#23558;&#27969;&#32593;&#32476;&#19982;CISTA-LSTC&#38598;&#25104;&#65292;&#24418;&#25104;CISTA-Flow&#32593;&#32476;&#65292;&#20351;&#31995;&#32479;&#20165;&#20381;&#36182;&#20107;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.11961</link><description>&lt;p&gt;
&#36890;&#36807;&#36816;&#21160;&#34917;&#20607;&#22686;&#24378;&#20107;&#20214;&#39537;&#21160;&#35270;&#39057;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Enhanced Event-Based Video Reconstruction with Motion Compensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11961
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36816;&#21160;&#34917;&#20607;&#26469;&#22686;&#24378;&#37325;&#24314;&#36136;&#37327;&#65292;&#25552;&#20986;&#23558;&#36755;&#20837;&#24103;&#21644;&#31232;&#30095;&#32534;&#30721;&#36827;&#34892;&#21464;&#25442;&#65292;&#24182;&#23558;&#27969;&#32593;&#32476;&#19982;CISTA-LSTC&#38598;&#25104;&#65292;&#24418;&#25104;CISTA-Flow&#32593;&#32476;&#65292;&#20351;&#31995;&#32479;&#20165;&#20381;&#36182;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#35270;&#39057;&#37325;&#24314;&#36890;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#20869;&#23384;&#38656;&#27714;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#32593;&#32476;CISTA-LSTC&#65292;&#34920;&#26126;&#36890;&#36807;&#31995;&#32479;&#35774;&#35745;&#20854;&#26550;&#26500;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#20854;&#24314;&#27169;&#20551;&#35774;&#36755;&#20837;&#20449;&#21495;&#21644;&#36755;&#20986;&#37325;&#24314;&#24103;&#20849;&#20139;&#30456;&#21516;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#24573;&#35270;&#20102;&#36816;&#21160;&#23548;&#33268;&#30340;&#20301;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23545;&#36755;&#20837;&#24378;&#24230;&#24103;&#21644;&#31232;&#30095;&#32534;&#30721;&#36827;&#34892;&#21464;&#25442;&#20197;&#22686;&#24378;&#37325;&#24314;&#36136;&#37327;&#12290;&#36890;&#36807;&#23558;&#27969;&#32593;&#32476;&#19982;CISTA-LSTC&#38598;&#25104;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;CISTA-Flow&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#34917;&#20607;&#12290;&#31995;&#32479;&#20165;&#20381;&#36182;&#20107;&#20214;&#65292;&#20854;&#20013;&#39044;&#27979;&#30340;&#27969;&#26377;&#21161;&#20110;&#37325;&#24314;&#65292;&#28982;&#21518;&#37325;&#24314;&#30340;&#24103;&#29992;&#20110;&#20419;&#36827;&#27969;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#20026;&#35813;&#32452;&#21512;&#31995;&#32479;&#24341;&#20837;&#20102;&#19968;&#20010;&#36845;&#20195;&#35757;&#32451;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11961v1 Announce Type: cross  Abstract: Deep neural networks for event-based video reconstruction often suffer from a lack of interpretability and have high memory demands. A lightweight network called CISTA-LSTC has recently been introduced showing that high-quality reconstruction can be achieved through the systematic design of its architecture. However, its modelling assumption that input signals and output reconstructed frame share the same sparse representation neglects the displacement caused by motion. To address this, we propose warping the input intensity frames and sparse codes to enhance reconstruction quality. A CISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC for motion compensation. The system relies solely on events, in which predicted flow aids in reconstruction and then reconstructed frames are used to facilitate flow estimation. We also introduce an iterative training framework for this combined system. Results demonstrate tha
&lt;/p&gt;</description></item><item><title>IVAC-P2L&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#39057;&#21160;&#20316;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19981;&#35268;&#21017;&#37325;&#22797;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#39057;&#20013;&#20934;&#30830;&#35745;&#25968;&#21160;&#20316;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2403.11959</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#35268;&#21017;&#37325;&#22797;&#20808;&#39564;&#20248;&#21270;&#35270;&#39057;&#21160;&#20316;&#35745;&#25968;&#30340;IVAC-P2L
&lt;/p&gt;
&lt;p&gt;
IVAC-P2L: Enhancing Video Action Counting through Irregular Repetition Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11959
&lt;/p&gt;
&lt;p&gt;
IVAC-P2L&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#39057;&#21160;&#20316;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19981;&#35268;&#21017;&#37325;&#22797;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#39057;&#20013;&#20934;&#30830;&#35745;&#25968;&#21160;&#20316;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21160;&#20316;&#35745;&#25968;&#65288;VAC&#65289;&#22312;&#20998;&#26512;&#36816;&#21160;&#12289;&#20581;&#36523;&#21644;&#26085;&#24120;&#27963;&#21160;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#36890;&#36807;&#37327;&#21270;&#35270;&#39057;&#20013;&#30340;&#37325;&#22797;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;VAC&#26041;&#27861;&#24573;&#30053;&#20102;&#21160;&#20316;&#37325;&#22797;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#20013;&#26029;&#21644;&#21608;&#26399;&#25345;&#32493;&#26102;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;VAC&#26041;&#27861;&#65292;&#31216;&#20026;Irregular Video Action Counting&#65288;IVAC&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#12290;IVAC&#20248;&#20808;&#32771;&#34385;&#24314;&#27169;&#35270;&#39057;&#20013;&#30340;&#19981;&#35268;&#21017;&#37325;&#22797;&#27169;&#24335;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#23545;&#20854;&#36827;&#34892;&#23450;&#20041;&#65306;&#36328;&#21608;&#26399;&#19968;&#33268;&#24615;&#21644;&#21608;&#26399;&#38388;&#38548;&#19981;&#19968;&#33268;&#24615;&#12290;&#36328;&#21608;&#26399;&#19968;&#33268;&#24615;&#30830;&#20445;&#21608;&#26399;&#27573;&#30340;&#26102;&#31354;&#34920;&#31034;&#30340;&#21516;&#36136;&#24615;&#65292;&#34920;&#31034;&#21608;&#26399;&#20869;&#21160;&#20316;&#30340;&#19968;&#33268;&#24615;&#12290;&#21608;&#26399;&#38388;&#38548;&#19981;&#19968;&#33268;&#24615;&#24378;&#35843;&#22522;&#20110;&#22266;&#26377;&#20869;&#23481;&#24046;&#24322;&#21306;&#20998;&#21608;&#26399;&#27573;&#21644;&#38388;&#38548;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#27010;&#25324;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11959v1 Announce Type: cross  Abstract: Video Action Counting (VAC) is crucial in analyzing sports, fitness, and everyday activities by quantifying repetitive actions in videos. However, traditional VAC methods have overlooked the complexity of action repetitions, such as interruptions and the variability in cycle duration. Our research addresses the shortfall by introducing a novel approach to VAC, called Irregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular repetition patterns in videos, which we define through two primary aspects: Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle Consistency ensures homogeneity in the spatial-temporal representations of cycle segments, signifying action uniformity within cycles. Cycle-interval inconsistency highlights the importance of distinguishing between cycle segments and intervals based on their inherent content differences. To encapsulate these principles, we propose a new methodology that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#29983;&#25104;&#34920;&#24773;&#31867;&#21035;&#20266;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#38754;&#20020;&#25968;&#25454;&#38598;&#38480;&#21046;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11942</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#26102;&#38388;&#24314;&#27169;&#25506;&#32034;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11942
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#29983;&#25104;&#34920;&#24773;&#31867;&#21035;&#20266;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#38754;&#20020;&#25968;&#25454;&#38598;&#38480;&#21046;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;FER&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#20171;&#32461;&#25105;&#20204;&#38024;&#23545;&#21363;&#23558;&#22312;CVPR2024&#20030;&#34892;&#30340;&#31532;6&#23626;&#37326;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#65288;ABAW&#65289;&#27604;&#36187;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20026;&#26410;&#26631;&#35760;&#30340;&#38754;&#37096;&#25968;&#25454;&#29983;&#25104;&#34920;&#24773;&#31867;&#21035;&#20266;&#26631;&#31614;&#65292;&#36890;&#36807;&#22343;&#21248;&#37319;&#26679;&#26631;&#35760;&#30340;&#38754;&#37096;&#34920;&#24773;&#26679;&#26412;&#65292;&#24182;&#23454;&#26045;&#21435;&#20559;&#21453;&#39304;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11942v1 Announce Type: cross  Abstract: Facial Expression Recognition (FER) plays a crucial role in computer vision and finds extensive applications across various fields. This paper aims to present our approach for the upcoming 6th Affective Behavior Analysis in-the-Wild (ABAW) competition, scheduled to be held at CVPR2024.. In the facial expression recognition task, The limited size of the FER dataset poses a challenge to the expression recognition model's generalization ability, resulting in subpar recognition performance. To address this problem, we employ a semi-supervised learning technique to generate expression category pseudo-labels for unlabeled face data. At the same time, we uniformly sampled the labeled facial expression samples and implemented a debiased feedback learning strategy to address the problem of category imbalance in the dataset and the possible data bias in semi-supervised learning. Moreover, , to further compensate for the limitation and bias of fe
&lt;/p&gt;</description></item><item><title>Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11905</link><description>&lt;p&gt;
Tur[k]ingBench&#65306;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#30340;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench: A Challenge Benchmark for Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11905
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#21407;&#22987;&#25991;&#26412;&#24418;&#24335;&#19979;&#29702;&#35299;&#21644;&#20132;&#27969;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#19981;&#20165;&#20165;&#26159;&#21407;&#22987;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#22312;&#32593;&#39029;&#19978;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22312;&#36825;&#20123;&#32593;&#39029;&#19978;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#24418;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#20197;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#30340;&#24418;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#39046;&#22495;&#21602;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TurkingBench&#65292;&#19968;&#20010;&#30001;&#21253;&#21547;&#22810;&#27169;&#24577;&#32972;&#26223;&#30340;&#25991;&#26412;&#35828;&#26126;&#21046;&#23450;&#30340;&#20219;&#21153;&#22522;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#32593;&#39029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21508;&#31181;&#27880;&#37322;&#30446;&#30340;&#30340;&#33258;&#28982;HTML&#39029;&#38754;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;HTML&#35828;&#26126;&#20063;&#34987;&#23454;&#20363;&#21270;&#20026;&#21508;&#31181;&#20540;&#65288;&#20174;&#20247;&#21253;&#20219;&#21153;&#33719;&#24471;&#65289;&#20197;&#24418;&#25104;&#20219;&#21153;&#30340;&#26032;&#23454;&#20363;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;32.2K&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
&lt;/p&gt;</description></item><item><title>Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.11901</link><description>&lt;p&gt;
Larimar: &#20855;&#26377;&#24773;&#33410;&#35760;&#24518;&#25511;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Larimar: Large Language Models with Episodic Memory Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11901
&lt;/p&gt;
&lt;p&gt;
Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Larimar - &#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#12290; Larimar&#30340;&#35760;&#24518;&#20801;&#35768;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#26356;&#26032;&#30693;&#35782;&#65292;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#20107;&#23454;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Larimar&#22312;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322; - &#26681;&#25454;&#22522;&#30784;LLM&#30340;&#19981;&#21516;&#65292;&#36895;&#24230;&#25552;&#21319;&#20026;4-10&#20493;&#65292;&#24182;&#19988;&#30001;&#20110;&#25552;&#20986;&#30340;&#26550;&#26500;&#31616;&#21333;&#12289;&#19981;&#20381;&#36182;&#20110;LLM&#65292;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#20107;&#23454;&#36951;&#24536;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#27010;&#25324;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11901v1 Announce Type: cross  Abstract: Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.11894</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#21040;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#23454;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DL&#30340;NLP&#26041;&#27861;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#36879;&#26126;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#25110;&#33267;&#23569;&#26159;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#23545;&#21307;&#30103;&#20581;&#24247;NLP&#20013;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;DL&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#33539;&#22260;&#23457;&#26597;&#12290;&#24341;&#20837;&#20102;&#26415;&#35821;&#8220;XIAI&#8221;&#65288;eXplainable&#21644;Interpretable Artificial Intelligence&#65289;&#20197;&#21306;&#20998;XAI&#21644;IAI&#12290;&#26041;&#27861;&#26681;&#25454;&#20854;&#21151;&#33021;&#65288;&#27169;&#22411;&#12289;&#36755;&#20837;&#12289;&#36755;&#20986;&#20026;&#22522;&#30784;&#65289;&#21644;&#33539;&#22260;&#65288;&#23616;&#37096;&#12289;&#20840;&#23616;&#65289;&#36827;&#19968;&#27493;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27880;&#24847;&#26426;&#21046;&#26159;&#26368;&#20027;&#35201;&#30340;&#26032;&#20852;IAI&#12290;&#27492;&#22806;&#65292;IAI&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#23545;&#25239;XAI&#12290;&#30830;&#23450;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;XIAI&#19981;&#25506;&#32034;&#8220;&#20840;&#23616;&#8221;&#24314;&#27169;&#36807;&#31243;&#65292;&#32570;&#20047;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#19988;&#38656;&#35201;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
&lt;/p&gt;</description></item><item><title>SuperLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#19988;&#39640;&#24230;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25216;&#24039;&#25193;&#23637;&#20102;&#19981;&#21516;&#30340;LoRA&#21464;&#20307;&#65292;&#22312;&#26497;&#23569;&#21442;&#25968;&#24773;&#20917;&#19979;&#29305;&#21035;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.11887</link><description>&lt;p&gt;
SuperLoRA: &#22810;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#21442;&#25968;&#39640;&#25928;&#32479;&#19968;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11887
&lt;/p&gt;
&lt;p&gt;
SuperLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#19988;&#39640;&#24230;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25216;&#24039;&#25193;&#23637;&#20102;&#19981;&#21516;&#30340;LoRA&#21464;&#20307;&#65292;&#22312;&#26497;&#23569;&#21442;&#25968;&#24773;&#20917;&#19979;&#29305;&#21035;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#21450;&#20854;&#21464;&#20307;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24494;&#35843;&#22823;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SuperLoRA&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#32479;&#19968;&#24182;&#25193;&#23637;&#20102;&#19981;&#21516;&#30340;LoRA&#21464;&#20307;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#23454;&#29616;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#32452;&#12289;&#25240;&#21472;&#12289;&#27927;&#29260;&#12289;&#25237;&#24433;&#21644;&#24352;&#37327;&#22240;&#23376;&#21270;&#65292;SuperLoRA&#30456;&#27604;&#20854;&#20182;LoRA&#21464;&#20307;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#22312;&#38750;&#24120;&#23569;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#29305;&#21035;&#22312;&#20256;&#36882;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11887v1 Announce Type: cross  Abstract: Low-rank adaptation (LoRA) and its variants are widely employed in fine-tuning large models, including large language models for natural language processing and diffusion models for computer vision. This paper proposes a generalized framework called SuperLoRA that unifies and extends different LoRA variants, which can be realized under different hyper-parameter settings. Introducing grouping, folding, shuffling, projecting, and tensor factoring, SuperLoRA offers high flexibility compared with other LoRA variants and demonstrates superior performance for transfer learning tasks especially in the extremely few-parameter regimes.
&lt;/p&gt;</description></item><item><title>QueryAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#65292;&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#39640;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.11886</link><description>&lt;p&gt;
QueryAgent&#65306;&#19968;&#31181;&#20855;&#26377;&#29615;&#22659;&#21453;&#39304;&#30340;&#21487;&#38752;&#39640;&#25928;&#25512;&#29702;&#26694;&#26550;&#21450;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11886
&lt;/p&gt;
&lt;p&gt;
QueryAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#65292;&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#22312;&#36935;&#21040;&#24187;&#35273;&#26102;&#29616;&#26377;&#26041;&#27861;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;QueryAgent&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#24182;&#36827;&#34892;&#36880;&#27493;&#33258;&#25105;&#26657;&#27491;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ERASER&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#20013;&#30340;&#20016;&#23500;&#29615;&#22659;&#21453;&#39304;&#65292;&#22312;&#24517;&#35201;&#26102;&#20165;&#36827;&#34892;&#36873;&#25321;&#24615;&#21644;&#24046;&#24322;&#21270;&#30340;&#33258;&#25105;&#26657;&#27491;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QueryAgent&#22312;GrailQA&#21644;GraphQ&#19978;&#20165;&#20351;&#29992;&#19968;&#20010;&#20363;&#23376;&#23601;&#27604;&#25152;&#26377;&#20808;&#21069;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#21462;&#24471;&#20102;7.0&#21644;15.0&#30340;F1&#20540;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#21253;&#25324;&#36816;&#34892;&#26102;&#38388;&#12289;&#26597;&#35810;&#24320;&#38144;&#21644;API&#35843;&#29992;&#25104;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;ERASER&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11886v1 Announce Type: cross  Abstract: Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20154;&#38469;&#20114;&#21160;&#30340;&#38750;&#23545;&#31216;&#12289;&#21160;&#24577;&#12289;&#21516;&#27493;&#21644;&#35814;&#32454;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#22330;&#26223;&#20154;&#31867;&#34892;&#21160;-&#21453;&#24212;&#32508;&#21512;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.11882</link><description>&lt;p&gt;
ReGenNet&#65306;&#36208;&#21521;&#20154;&#31867;&#34892;&#21160;-&#21453;&#24212;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
ReGenNet: Towards Human Action-Reaction Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20154;&#38469;&#20114;&#21160;&#30340;&#38750;&#23545;&#31216;&#12289;&#21160;&#24577;&#12289;&#21516;&#27493;&#21644;&#35814;&#32454;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#22330;&#26223;&#20154;&#31867;&#34892;&#21160;-&#21453;&#24212;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19981;&#26029;&#19982;&#21608;&#22260;&#29615;&#22659;&#20114;&#21160;&#12290;&#24403;&#21069;&#20197;&#20154;&#31867;&#20026;&#20013;&#24515;&#30340;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#21512;&#25104;&#20154;&#31867;&#19982;&#38745;&#24577;&#22330;&#26223;&#21644;&#23545;&#35937;&#36827;&#34892;&#21487;&#20449;&#20114;&#21160;&#65292;&#32780;&#23545;&#20110;&#26222;&#36941;&#22240;&#26524;&#20154;&#38469;&#20114;&#21160;&#30340;&#21160;&#24577;&#20154;&#31867;&#34892;&#21160;-&#21453;&#24212;&#32508;&#21512;&#36827;&#34892;&#20102;&#36739;&#23569;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#20154;&#38469;&#20114;&#21160;&#30340;&#38750;&#23545;&#31216;&#12289;&#21160;&#24577;&#12289;&#21516;&#27493;&#21644;&#35814;&#32454;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#22330;&#26223;&#20154;&#31867;&#34892;&#21160;-&#21453;&#24212;&#32508;&#21512;&#22522;&#20934;&#65292;&#20197;&#22312;&#32473;&#23450;&#20154;&#31867;&#34892;&#21160;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#20154;&#31867;&#21453;&#24212;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20026;NTU120&#12289;InterHuman&#21644;Chi3D&#25968;&#25454;&#38598;&#27880;&#37322;&#20132;&#20114;&#24207;&#21015;&#30340;actor-reactor&#27425;&#24207;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;Transformer&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#21517;&#20026;ReGenNet&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11882v1 Announce Type: cross  Abstract: Humans constantly interact with their surrounding environments. Current human-centric generative models mainly focus on synthesizing humans plausibly interacting with static scenes and objects, while the dynamic human action-reaction synthesis for ubiquitous causal human-human interactions is less explored. Human-human interactions can be regarded as asymmetric with actors and reactors in atomic interaction periods. In this paper, we comprehensively analyze the asymmetric, dynamic, synchronous, and detailed nature of human-human interactions and propose the first multi-setting human action-reaction synthesis benchmark to generate human reactions conditioned on given human actions. To begin with, we propose to annotate the actor-reactor order of the interaction sequences for the NTU120, InterHuman, and Chi3D datasets. Based on them, a diffusion-based generative model with a Transformer decoder architecture called ReGenNet together with 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#25216;&#26415;&#25972;&#21512;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21644;&#37319;&#29992;LSTM&#26550;&#26500;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.11879</link><description>&lt;p&gt;
&#21333;&#27169;&#24577;&#22810;&#20219;&#21153;&#34701;&#21512;&#29992;&#20110;&#24773;&#24863;&#27169;&#20223;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11879
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#25216;&#26415;&#25972;&#21512;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21644;&#37319;&#29992;LSTM&#26550;&#26500;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31532;&#20845;&#23626;&#25143;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#30740;&#35752;&#20250;&#21644;&#31454;&#36187;&#20013;&#36827;&#34892;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#65288;EMI&#65289;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Wav2Vec 2.0&#26694;&#26550;&#65292;&#22312;&#19968;&#20010;&#20840;&#38754;&#30340;&#25773;&#23458;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#21462;&#28085;&#30422;&#35821;&#35328;&#21644;&#35821;&#22806;&#20803;&#32032;&#30340;&#24191;&#27867;&#38899;&#39057;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#34701;&#21512;&#25216;&#26415;&#22686;&#24378;&#20102;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#25216;&#26415;&#23558;&#20010;&#20307;&#29305;&#24449;&#19982;&#20840;&#23616;&#22343;&#20540;&#21521;&#37327;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21040;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;Wav2Vec 2.0&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;valence-arousal-dominance&#65288;VAD&#65289;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#34701;&#21512;&#37319;&#29992;&#20102;&#19968;&#31181;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#23545;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#30340;&#26102;&#38388;&#20998;&#26512;&#12290;&#20165;&#21033;&#29992;&#25152;&#25552;&#20379;&#30340;&#38899;&#39057;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#32447;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11879v1 Announce Type: cross  Abstract: In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence- arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#20197;&#23558;&#31532;&#20108;&#27169;&#24577;&#65288;&#38750;RGB&#65289;&#32435;&#20837;NeRFs&#20013;&#65292;&#36890;&#36807;&#36873;&#25321;&#28909;&#25104;&#20687;&#20316;&#20026;&#31532;&#20108;&#27169;&#24577;&#26469;&#25361;&#25112;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#30340;&#25972;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.11865</link><description>&lt;p&gt;
&#21033;&#29992;&#28909;&#25104;&#20687;&#25506;&#32034;&#22810;&#27169;&#24577;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#24182;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#20197;&#23558;&#31532;&#20108;&#27169;&#24577;&#65288;&#38750;RGB&#65289;&#32435;&#20837;NeRFs&#20013;&#65292;&#36890;&#36807;&#36873;&#25321;&#28909;&#25104;&#20687;&#20316;&#20026;&#31532;&#20108;&#27169;&#24577;&#26469;&#25361;&#25112;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#22312;&#19968;&#32452;RGB&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#36805;&#36895;&#21457;&#23637;&#20026;&#26032;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#23545;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#65288;&#22914;NeRFs&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22914;&#20309;&#23558;&#31532;&#20108;&#27169;&#24577;&#65288;&#38750;RGB&#65289;&#32435;&#20837;NeRFs&#20013;&#65306;&#65288;1&#65289;&#29420;&#31435;&#22320;&#20174;&#22836;&#35757;&#32451;&#27599;&#31181;&#27169;&#24577;&#65307;&#65288;2&#65289;&#22312;RGB&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#27169;&#24577;&#19978;&#36827;&#34892;&#24494;&#35843;&#65307;&#65288;3&#65289;&#28155;&#21152;&#31532;&#20108;&#20998;&#25903;&#65307;&#65288;4&#65289;&#28155;&#21152;&#19968;&#20010;&#21333;&#29420;&#30340;&#32452;&#20214;&#26469;&#39044;&#27979;&#65288;&#39068;&#33394;&#65289;&#39069;&#22806;&#27169;&#24577;&#30340;&#20540;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#28909;&#25104;&#20687;&#20316;&#20026;&#31532;&#20108;&#27169;&#24577;&#65292;&#22240;&#20026;&#20174;&#36752;&#23556;&#24230;&#26469;&#30475;&#65292;&#23427;&#19982;RGB&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#23558;&#20854;&#25972;&#21512;&#21040;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11865v1 Announce Type: cross  Abstract: Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#65292;&#22312;&#27809;&#26377;&#35814;&#32454;&#20102;&#35299;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#22312;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11852</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#65292;&#22312;&#27809;&#26377;&#35814;&#32454;&#20102;&#35299;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#22312;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#26080;&#32541;&#22320;&#34701;&#20837;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36710;&#27969;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS)&#20195;&#29702;&#65292;&#26088;&#22312;&#22312;&#27809;&#26377;&#20851;&#20110;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#20840;&#38754;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35813;&#20195;&#29702;&#30340;&#22686;&#24378;&#29256;AL3IS&#65292;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#20855;&#26377;&#36710;&#36742;&#38388;&#36890;&#20449;&#24310;&#36831;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20570;&#20986;&#26356;&#31283;&#20581;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#36890;&#36807;&#28508;&#22312;&#29366;&#24577;&#24314;&#27169;&#29615;&#22659;&#20013;&#30340;&#19981;&#21487;&#35266;&#23519;&#26041;&#38754;&#65292;&#22914;&#20854;&#20182;&#39550;&#39542;&#21592;&#30340;&#24847;&#22270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#20195;&#29702;&#36866;&#24212;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#12289;&#20248;&#21270;&#21512;&#24182;&#25805;&#20316;&#24182;&#30830;&#20445;&#19982;&#20854;&#20182;&#36710;&#36742;&#36827;&#34892;&#23433;&#20840;&#20114;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11852v1 Announce Type: cross  Abstract: This paper presents a novel approach to address the challenging problem of autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly integrate into a flow of vehicles on a multi-lane highway. We introduce the Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS) agent, designed to perform the on-ramp merging task safely without comprehensive knowledge about surrounding vehicles' intents or driving styles. We also present an augmentation of this agent called AL3IS that accounts for observation delays, allowing the agent to make more robust decisions in real-world environments with vehicle-to-vehicle (V2V) communication delays. By modeling the unobservable aspects of the environment through latent states, such as other drivers' intents, our approach enhances the agent's ability to adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure safe interactions with other vehi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#31895;&#31961;&#38598;&#21512;&#24230;&#37327;&#30340;&#26032;&#22411;Choquet&#36317;&#31163;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20351;&#24471;&#36317;&#31163;&#24230;&#37327;&#26356;&#21152;&#28789;&#27963;&#19982;&#20934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2403.11843</link><description>&lt;p&gt;
&#27169;&#31946;&#31895;&#31961;Choquet&#36317;&#31163;&#29992;&#20110;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Rough Choquet Distances for Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#31895;&#31961;&#38598;&#21512;&#24230;&#37327;&#30340;&#26032;&#22411;Choquet&#36317;&#31163;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20351;&#24471;&#36317;&#31163;&#24230;&#37327;&#26356;&#21152;&#28789;&#27963;&#19982;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#31895;&#31961;&#38598;&#21512;&#24230;&#37327;&#30340;&#26032;&#22411;Choquet&#36317;&#31163;&#12290;&#25152;&#25552;&#20986;&#30340;&#36317;&#31163;&#24230;&#37327;&#32467;&#21512;&#20102;&#20174;&#27169;&#31946;&#31895;&#31961;&#38598;&#29702;&#35770;&#20013;&#33719;&#24471;&#30340;&#23646;&#24615;&#20449;&#24687;&#21644;Choquet&#31215;&#20998;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#36866;&#24212;&#24615;&#22320;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#35748;&#35782;&#21040;&#26465;&#20214;&#23646;&#24615;&#19982;&#20915;&#31574;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23454;&#29616;&#26356;&#28789;&#27963;&#19982;&#20934;&#30830;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#24378;&#35843;&#22522;&#20110;&#36317;&#31163;&#30340;&#20998;&#31867;&#26041;&#27861;&#65288;&#20363;&#22914;k&#26368;&#36817;&#37051;&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#22522;&#20110;&#27491;&#21306;&#22495;&#30340;&#27169;&#31946;&#31895;&#31961;&#38598;&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#20004;&#31181;&#26681;&#25454;&#27169;&#31946;&#31895;&#31961;&#38598;&#29702;&#35770;&#23548;&#20986;&#30340;&#29992;&#20110;&#36866;&#29992;Choquet&#31215;&#20998;&#30340;&#21333;&#35843;&#21270;&#31243;&#24207;&#65292;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11843v1 Announce Type: cross  Abstract: This paper introduces a novel Choquet distance using fuzzy rough set based measures. The proposed distance measure combines the attribute information received from fuzzy rough set theory with the flexibility of the Choquet integral. This approach is designed to adeptly capture non-linear relationships within the data, acknowledging the interplay of the conditional attributes towards the decision attribute and resulting in a more flexible and accurate distance. We explore its application in the context of machine learning, with a specific emphasis on distance-based classification approaches (e.g. k-nearest neighbours). The paper examines two fuzzy rough set based measures that are based on the positive region. Moreover, we explore two procedures for monotonizing the measures derived from fuzzy rough set theory, making them suitable for use with the Choquet integral, and investigate their differences.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;PESCAL&#65292;&#21033;&#29992;&#22522;&#20110;&#21069;&#38376;&#26631;&#20934;&#30340;&#20013;&#20171;&#21464;&#37327;&#28040;&#38500;&#28151;&#26434;&#20559;&#24046;&#65292;&#24182;&#37319;&#29992;&#24754;&#35266;&#21407;&#21017;&#22788;&#29702;&#20505;&#36873;&#31574;&#30053;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.11841</link><description>&lt;p&gt;
&#22522;&#20110;&#20013;&#20171;&#22240;&#32032;&#30340;&#24754;&#35266;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28151;&#26434;&#30340;&#31163;&#32447;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11841
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;PESCAL&#65292;&#21033;&#29992;&#22522;&#20110;&#21069;&#38376;&#26631;&#20934;&#30340;&#20013;&#20171;&#21464;&#37327;&#28040;&#38500;&#28151;&#26434;&#20559;&#24046;&#65292;&#24182;&#37319;&#29992;&#24754;&#35266;&#21407;&#21017;&#22788;&#29702;&#20505;&#36873;&#31574;&#30053;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#38543;&#26426;&#23454;&#39564;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#21463;&#21040;&#26102;&#38388;&#21644;&#39044;&#31639;&#38480;&#21046;&#32780;&#35268;&#27169;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#25104;&#20026;&#23454;&#29616;&#39640;&#36136;&#37327;&#31574;&#30053;&#23398;&#20064;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#20851;&#38190;&#20551;&#35774;-- &#38750;&#28151;&#26434;&#24615;&#21644;&#27491;&#24615;-- &#36825;&#20004;&#20010;&#20551;&#35774;&#22312;&#35266;&#27979;&#25968;&#25454;&#29615;&#22659;&#20013;&#32463;&#24120;&#19981;&#25104;&#31435;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#24754;&#35266;&#22240;&#26524;&#23398;&#20064;&#65288;PESCAL&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#21069;&#38376;&#26631;&#20934;&#30340;&#20013;&#20171;&#21464;&#37327;&#26469;&#28040;&#38500;&#28151;&#26434;&#20559;&#24046;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#24754;&#35266;&#21407;&#21017;&#26469;&#35299;&#20915;&#30001;&#20505;&#36873;&#31574;&#30053;&#24341;&#36215;&#30340;&#21160;&#20316;&#20998;&#24067;&#19982;&#29983;&#25104;&#35266;&#27979;&#25968;&#25454;&#30340;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#36890;&#36807;&#34701;&#21512;&#36741;&#21161;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11841v1 Announce Type: cross  Abstract: In real-world scenarios, datasets collected from randomized experiments are often constrained by size, due to limitations in time and budget. As a result, leveraging large observational datasets becomes a more attractive option for achieving high-quality policy learning. However, most existing offline reinforcement learning (RL) methods depend on two key assumptions--unconfoundedness and positivity--which frequently do not hold in observational data contexts. Recognizing these challenges, we propose a novel policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the mediator variable based on front-door criterion to remove the confounding bias; additionally, we adopt the pessimistic principle to address the distributional shift between the action distributions induced by candidate policies, and the behavior policy that generates the observational data. Our key observation is that, by incorporating auxiliary variable
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Guide-Align&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21046;&#23450;&#29305;&#23450;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20840;&#38754;&#30340;&#25351;&#23548;&#24211;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#29983;&#25104;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.11838</link><description>&lt;p&gt;
&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#65306;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#24211;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11838
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Guide-Align&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21046;&#23450;&#29305;&#23450;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20840;&#38754;&#30340;&#25351;&#23548;&#24211;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#29983;&#25104;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#20559;&#35265;&#20869;&#23481;&#29983;&#25104;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#39118;&#38505;&#12290;&#24403;&#21069;&#30340;&#23545;&#40784;&#25216;&#26415;&#20043;&#19968;&#21253;&#25324;&#22522;&#20110;&#21407;&#21017;&#30340;&#38598;&#25104;&#65292;&#20294;&#38754;&#20020;&#30001;&#20110;&#25163;&#24037;&#21046;&#23450;&#35268;&#21017;&#30340;&#19981;&#31934;&#30830;&#24615;&#21644;&#26410;&#32463;&#23433;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#39118;&#38505;&#24863;&#30693;&#19981;&#36275;&#32780;&#20135;&#29983;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Guide-Align&#65292;&#36825;&#26159;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#26368;&#21021;&#65292;&#19968;&#20010;&#32463;&#36807;&#23433;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#20026;&#21508;&#31181;&#36755;&#20837;&#21046;&#23450;&#20855;&#20307;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#25351;&#21335;&#24211;&#21644;&#29992;&#20110;&#36755;&#20837;&#25351;&#21335;&#26816;&#32034;&#30340;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#26816;&#32034;&#27169;&#22411;&#23558;&#26032;&#36755;&#20837;&#19982;&#30456;&#20851;&#25351;&#21335;&#30456;&#20851;&#32852;&#65292;&#24341;&#23548;LLMs&#22312;&#21709;&#24212;&#29983;&#25104;&#20013;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#65292;&#20174;&#32780;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#12290;&#21478;&#19968;&#20010;&#39069;&#22806;&#21487;&#36873;&#38454;&#27573;&#28041;&#21450;&#20351;&#29992;&#32463;&#36807;&#32454;&#33268;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11838v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#21487;&#34892;&#32467;&#26500;&#25915;&#20987;&#25152;&#38656;&#32771;&#34385;&#30340;&#38382;&#39064;&#31354;&#38388;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.11830</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#38382;&#39064;&#31354;&#38388;&#32467;&#26500;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#21487;&#34892;&#32467;&#26500;&#25915;&#20987;&#25152;&#38656;&#32771;&#34385;&#30340;&#38382;&#39064;&#31354;&#38388;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#22320;&#29992;&#20110;&#25903;&#25345;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#24494;&#23567;&#25200;&#21160;&#65292;&#26088;&#22312;&#21361;&#23475;&#20854;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#25552;&#35758;&#26377;&#25928;&#22320;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#20135;&#29983;&#22522;&#20110;&#20837;&#20405;&#23637;&#31034;&#30340;&#32467;&#26500;&#27169;&#24335;&#30340;&#39044;&#27979;&#65292;&#20197;&#22686;&#24378;&#26816;&#27979;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#22522;&#20110;GNN&#30340;NIDS&#24341;&#20837;&#20102;&#26032;&#31867;&#22411;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19987;&#38376;&#38024;&#23545;GNN&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#20013;&#24418;&#25104;&#23545;&#25239;&#25915;&#20987;&#30340;&#24418;&#24335;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#24182;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#38656;&#35201;&#32771;&#34385;&#30340;&#38382;&#39064;&#31354;&#38388;&#32422;&#26463;&#65292;&#20197;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#21487;&#34892;&#30340;&#32467;&#26500;&#25915;&#20987;&#12290;&#20316;&#20026;&#26368;&#32456;&#36129;&#29486;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11830v1 Announce Type: cross  Abstract: Machine Learning (ML) algorithms have become increasingly popular for supporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensive research has shown their vulnerability to adversarial attacks, which involve subtle perturbations to the inputs of the models aimed at compromising their performance. Recent proposals have effectively leveraged Graph Neural Networks (GNN) to produce predictions based also on the structural patterns exhibited by intrusions to enhance the detection robustness. However, the adoption of GNN-based NIDS introduces new types of risks. In this paper, we propose the first formalization of adversarial attacks specifically tailored for GNN in network intrusion detection. Moreover, we outline and model the problem space constraints that attackers need to consider to carry out feasible structural attacks in real-world scenarios. As a final contribution, we conduct an extensive experimental campaign in 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;</title><link>https://arxiv.org/abs/2403.11821</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65306;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11821
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32467;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20114;&#32852;&#32593;&#25110;&#20854;&#20182;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#30340;&#28023;&#37327;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#38543;&#30528;&#23545;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#30340;&#38656;&#27714;&#36716;&#21521;&#30830;&#20445;&#25991;&#26412;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#23545;&#40784;&#65292;&#24050;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#21028;&#26029;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25910;&#38598;&#20855;&#26377;&#36234;&#26469;&#36234;&#22797;&#26434;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#25104;&#24615;&#21450;&#20854;&#20316;&#20026;&#25991;&#26412;&#19982;&#22270;&#20687;&#20869;&#23481;&#32452;&#25104;&#23545;&#40784;&#36136;&#37327;&#24230;&#37327;&#30340;&#20854;&#32435;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102;&#32463;&#24120;&#37319;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11807</link><description>&lt;p&gt;
LLM&#30340;&#20915;&#31574;&#27700;&#24179;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#31350;&#31455;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21508;&#31181;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#22909;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#25506;&#31350;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25903;&#25345;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#21442;&#19982;&#30340;&#28216;&#25103;&#65292;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;GAMA-Bench&#65292;&#21253;&#25324;&#20843;&#20010;&#32463;&#20856;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20998;&#26041;&#26696;&#65292;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;GAMA-Bench&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#22686;&#24378;&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;GPT-3.5&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19968;&#20123;&#26041;&#27861;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#20986;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.11793</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#23545;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11793
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#20986;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#33021;&#21147;&#30340;&#29616;&#26377;&#26041;&#27861;&#20197;&#32467;&#26524;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#35780;&#20272;&#25512;&#29702;&#36807;&#31243;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#20197;&#36807;&#31243;&#20026;&#20013;&#24515;&#30340;&#26041;&#24335;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;ARC&#35201;&#27714;&#35299;&#20915;&#38382;&#39064;&#26102;&#20855;&#26377;&#20005;&#35880;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#30340;&#22522;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#26174;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11793v1 Announce Type: cross  Abstract: The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#28145;&#24230;&#20013;&#36724;&#20307;&#65292;&#19968;&#31181;&#21322;&#38544;&#24335;&#34920;&#31034;&#65292;&#36890;&#36807;&#21367;&#31215;&#34920;&#38754;&#23454;&#29616;&#24418;&#29366;&#37325;&#24314;&#65292;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11790</link><description>&lt;p&gt;
&#28145;&#24230;&#20013;&#36724;&#20307;&#65306;&#23398;&#20064;&#30340;&#35299;&#21078;&#24418;&#29366;&#24314;&#27169;&#20013;&#36724;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical Shape Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11790
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#28145;&#24230;&#20013;&#36724;&#20307;&#65292;&#19968;&#31181;&#21322;&#38544;&#24335;&#34920;&#31034;&#65292;&#36890;&#36807;&#21367;&#31215;&#34920;&#38754;&#23454;&#29616;&#24418;&#29366;&#37325;&#24314;&#65292;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25104;&#20687;&#20307;&#31215;&#20013;&#37325;&#24314;&#24418;&#29366;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#32463;&#24120;&#38656;&#35201;&#30340;&#25805;&#20316;&#12290;&#24120;&#35265;&#30340;&#24037;&#20316;&#27969;&#31243;&#20174;&#20998;&#21106;&#27493;&#39588;&#24320;&#22987;&#65292;&#28982;&#21518;&#36827;&#34892;&#20180;&#32454;&#30340;&#21518;&#22788;&#29702;&#65292;&#26368;&#21518;&#20351;&#29992;&#20020;&#26102;&#30340;&#32593;&#26684;&#21270;&#31639;&#27861;&#12290;&#30001;&#20110;&#36825;&#20010;&#24207;&#21015;&#21487;&#33021;&#32791;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#29992;&#20110;&#36890;&#36807;&#27169;&#26495;&#21464;&#24418;&#37325;&#24314;&#24418;&#29366;&#12290;&#36825;&#20123;&#32593;&#32476;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#23427;&#20204;&#20027;&#35201;&#22312;&#35299;&#21078;&#24418;&#29366;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#20010;&#20307;&#20043;&#38388;&#30340;&#25299;&#25169;&#21464;&#21270;&#24456;&#23567;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20854;&#20182;&#20316;&#21697;&#20559;&#21521;&#20110;&#23398;&#20064;&#38544;&#24335;&#24418;&#29366;&#27169;&#22411;&#65292;&#23545;&#20110;&#32593;&#26684;&#21270;&#21644;&#21487;&#35270;&#21270;&#26377;&#22810;&#37325;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36981;&#24490;&#36825;&#20010;&#26041;&#21521;&#65292;&#24341;&#20837;&#20102;&#28145;&#24230;&#20013;&#36724;&#20307;&#65292;&#19968;&#31181;&#24544;&#23454;&#22320;&#20174;&#25104;&#20687;&#20307;&#31215;&#20013;&#36817;&#20284;&#34920;&#36798;&#25299;&#25169;&#39592;&#26550;&#30340;&#21322;&#38544;&#24335;&#34920;&#31034;&#65292;&#26368;&#32456;&#36890;&#36807;&#21367;&#31215;&#34920;&#38754;&#23454;&#29616;&#24418;&#29366;&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#37325;&#24314;&#25216;&#26415;&#26174;&#31034;&#20986;&#22312;&#24418;&#29366;&#37325;&#24314;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11790v1 Announce Type: cross  Abstract: Shape reconstruction from imaging volumes is a recurring need in medical image analysis. Common workflows start with a segmentation step, followed by careful post-processing and,finally, ad hoc meshing algorithms. As this sequence can be timeconsuming, neural networks are trained to reconstruct shapes through template deformation. These networks deliver state-ofthe-art results without manual intervention, but, so far, they have primarily been evaluated on anatomical shapes with little topological variety between individuals. In contrast, other works favor learning implicit shape models, which have multiple benefits for meshing and visualization. Our work follows this direction by introducing deep medial voxels, a semi-implicit representation that faithfully approximates the topological skeleton from imaging volumes and eventually leads to shape reconstruction via convolution surfaces. Our reconstruction technique shows potential for bo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25104;&#21151;&#25552;&#21462;&#20102;&#36229;&#20851;&#32852;&#30693;&#35782;&#65292;&#23613;&#31649;&#31934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20294;&#32467;&#26524;&#26174;&#31034;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.11786</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#36229;&#20851;&#32852;&#30693;&#35782;&#22270;
&lt;/p&gt;
&lt;p&gt;
Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11786
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25104;&#21151;&#25552;&#21462;&#20102;&#36229;&#20851;&#32852;&#30693;&#35782;&#65292;&#23613;&#31649;&#31934;&#30830;&#29575;&#36739;&#20302;&#65292;&#20294;&#32467;&#26524;&#26174;&#31034;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#36229;&#20851;&#31995;&#23545;&#20110;&#26500;&#24314;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#30417;&#30563;&#26041;&#27861;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;OpenAI&#30340;GPT-3.5&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#36229;&#20851;&#32852;&#30693;&#35782;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#21484;&#22238;&#29575;&#36798;&#21040;&#20102;0.77&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#31934;&#30830;&#29575;&#30446;&#21069;&#36739;&#20302;&#65292;&#20294;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#35814;&#32454;&#20998;&#26512;&#25581;&#31034;&#20102;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11786v1 Announce Type: cross  Abstract: Extracting hyper-relations is crucial for constructing comprehensive knowledge graphs, but there are limited supervised methods available for this task. To address this gap, we introduce a zero-shot prompt-based method using OpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text. Comparing our model with a baseline, we achieved promising results, with a recall of 0.77. Although our precision is currently lower, a detailed analysis of the model outputs has uncovered potential pathways for future research in this area.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11780</link><description>&lt;p&gt;
Prompt-Singer: &#24102;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#21487;&#25511;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;(SVS)&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#33258;&#28982;&#24230;&#65292;&#28982;&#32780;&#23427;&#20204;&#32570;&#20047;&#26174;&#24335;&#25511;&#21046;&#21512;&#25104;&#21809;&#27468;&#39118;&#26684;&#23646;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;SVS&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20165;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#20855;&#26377;&#22810;&#23610;&#24230;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#31163;&#38899;&#39640;&#34920;&#31034;&#30340;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#22495;&#25511;&#21046;&#21516;&#26102;&#20445;&#25345;&#20102;&#26059;&#24459;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#24494;&#35843;&#65292;&#20197;&#21450;&#24341;&#20837;&#35821;&#38899;&#25968;&#25454;&#20197;&#20943;&#36731;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26088;&#22312;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#38899;&#39057;&#36136;&#37327;&#12290;&#38899;&#39057;&#31034;&#20363;&#21487;&#35775;&#38382; http://prompt-singer.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#23454;&#29616;&#33041;&#30005;&#20449;&#21495;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;Signal-JEPA&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31934;&#30830;&#19979;&#28216;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11772</link><description>&lt;p&gt;
S-JEPA&#65306;&#36890;&#36807;&#21160;&#24577;&#31354;&#38388;&#27880;&#24847;&#21147;&#23454;&#29616;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#23454;&#29616;&#33041;&#30005;&#20449;&#21495;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;Signal-JEPA&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31934;&#30830;&#19979;&#28216;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#33041;&#30005;&#20449;&#21495;&#22788;&#29702;&#20013;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#36801;&#31227;&#23398;&#20064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#33041;&#30005;&#20449;&#21495;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#30340;Signal-JEPA&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#29305;&#23450;&#31354;&#38388;&#22359;&#25513;&#34109;&#31574;&#30053;&#21644;&#19977;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#19979;&#28216;&#20998;&#31867;&#30340;&#26550;&#26500;&#12290;&#35813;&#30740;&#31350;&#22312;&#19968;&#20010;54&#20010;&#21463;&#35797;&#32773;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;BCI&#33539;&#24335;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;&#36816;&#21160;&#24819;&#35937;&#12289;ERP&#21644;SSVEP&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;JEPAs&#22312;&#33041;&#30005;&#20449;&#21495;&#32534;&#30721;&#20013;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#31354;&#38388;&#28388;&#27874;&#23545;&#20934;&#30830;&#19979;&#28216;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11772v1 Announce Type: cross  Abstract: Motivated by the challenge of seamless cross-dataset transfer in EEG signal processing, this article presents an exploratory study on the use of Joint Embedding Predictive Architectures (JEPAs). In recent years, self-supervised learning has emerged as a promising approach for transfer learning in various domains. However, its application to EEG signals remains largely unexplored. In this article, we introduce Signal-JEPA for representing EEG recordings which includes a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification. The study is conducted on a 54~subjects dataset and the downstream performance of the models is evaluated on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study provides preliminary evidence for the potential of JEPAs in EEG signal encoding. Notably, our results highlight the importance of spatial filtering for accurate downstream classific
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11755</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#25552;&#31034;&#33258;&#21160;&#21270;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#30340;&#25552;&#31034;&#38598;&#25104;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#20026;LLMs&#35774;&#35745;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;VLM&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#25163;&#21160;&#32534;&#20889;&#36825;&#20123;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65292;&#32780;&#19988;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#19982;&#24863;&#20852;&#36259;&#31867;&#21035;&#30456;&#20851;&#30340;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#20219;&#21153;&#29305;&#23450;&#39118;&#26684;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#25490;&#38500;&#22312;&#24490;&#29615;&#20043;&#22806;&#65292;&#24182;&#23436;&#20840;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#20803;&#25552;&#31034;&#65288;MPVR&#65289;&#12290;&#20165;&#20197;&#30446;&#26631;&#20219;&#21153;&#30340;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24418;&#24335;&#20197;&#21450;&#19968;&#31995;&#21015;&#30456;&#20851;&#31867;&#21035;&#26631;&#31614;&#20316;&#20026;&#36755;&#20837;&#65292;MPVR&#33258;&#21160;&#20135;&#29983;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#31867;&#21035;&#25552;&#31034;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#29256;&#26412;&#30340;&#20851;&#31995;GNNs&#65292;&#36890;&#36807;&#22312;$t$&#20026;&#26080;&#31351;&#22823;&#26102;&#20165;&#20351;&#29992;&#20108;&#27425;&#31354;&#38388;&#30340;&#23884;&#20837;&#26469;&#36817;&#20284;$3$-GNNs&#65292;&#23545;&#20110;&#36739;&#20302;&#30340;$t$&#20540;&#65292;&#36890;&#36807;&#20132;&#25442;&#36739;&#23569;&#30340;&#28040;&#24687;&#23454;&#29616;&#24369;&#30340;&#36817;&#20284;&#65292;&#21516;&#26102;&#36890;&#24120;&#20135;&#29983;&#20102;&#20960;&#20010;&#35268;&#21010;&#39046;&#22495;&#20013;&#25152;&#38656;&#30340;$C_3$&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.11734</link><description>&lt;p&gt;
&#23398;&#20064;&#21476;&#20856;&#35268;&#21010;&#39046;&#22495;&#30340;&#36890;&#29992;&#31574;&#30053;&#65306;&#36229;&#36234;$C_2$
&lt;/p&gt;
&lt;p&gt;
Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#29256;&#26412;&#30340;&#20851;&#31995;GNNs&#65292;&#36890;&#36807;&#22312;$t$&#20026;&#26080;&#31351;&#22823;&#26102;&#20165;&#20351;&#29992;&#20108;&#27425;&#31354;&#38388;&#30340;&#23884;&#20837;&#26469;&#36817;&#20284;$3$-GNNs&#65292;&#23545;&#20110;&#36739;&#20302;&#30340;$t$&#20540;&#65292;&#36890;&#36807;&#20132;&#25442;&#36739;&#23569;&#30340;&#28040;&#24687;&#23454;&#29616;&#24369;&#30340;&#36817;&#20284;&#65292;&#21516;&#26102;&#36890;&#24120;&#20135;&#29983;&#20102;&#20960;&#20010;&#35268;&#21010;&#39046;&#22495;&#20013;&#25152;&#38656;&#30340;$C_3$&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#36328;&#35268;&#21010;&#39046;&#22495;&#30340;&#36890;&#29992;&#31574;&#30053;&#21463;&#21040;$C_2$&#34920;&#36798;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#21363;&#19968;&#38454;&#36923;&#36753;&#21482;&#33021;&#21253;&#21547;&#20004;&#20010;&#21464;&#37327;&#21644;&#35745;&#25968;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#20197;&#36890;&#36807;&#36716;&#21521;$k$-GNNs&#65292;&#20854;&#20013;$k=3$&#65292;&#20854;&#20013;&#29289;&#20307;&#23884;&#20837;&#34987;&#19977;&#20803;&#32452;&#23884;&#20837;&#25152;&#26367;&#25442;&#65292;&#26469;&#20811;&#26381;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;$3$-GNNs&#20855;&#26377;$C_3$&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#19981;&#21516;&#20110;&#21463;&#38480;&#20110;$C_2$&#30340;$1$-&#21644;$2$-GNNs&#65292;&#23427;&#20204;&#38656;&#35201;&#22235;&#27425;&#26102;&#38388;&#36827;&#34892;&#28040;&#24687;&#20132;&#25442;&#21644;&#19977;&#27425;&#31354;&#38388;&#36827;&#34892;&#23884;&#20837;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#29256;&#26412;&#30340;&#20851;&#31995;GNNs&#12290;&#24403;$t$&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;R-GNN[$t$]&#20165;&#20351;&#29992;&#20108;&#27425;&#31354;&#38388;&#30340;&#23884;&#20837;&#26469;&#36817;&#20284;$3$-GNNs&#12290;&#23545;&#20110;&#36739;&#20302;&#30340;$t$&#20540;&#65292;&#20363;&#22914;$t=1$&#21644;$t=2$&#65292;R-GNN[$t$]&#36890;&#36807;&#20132;&#25442;&#36739;&#23569;&#30340;&#28040;&#24687;&#23454;&#29616;&#20102;&#26356;&#24369;&#30340;&#36817;&#20284;&#65292;&#20294;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#24120;&#20135;&#29983;&#20102;&#22312;&#20960;&#20010;&#35268;&#21010;&#39046;&#22495;&#20013;&#25152;&#38656;&#30340;$C_3$&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;R-GNN[$t$] ar
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11734v1 Announce Type: new  Abstract: GNN-based approaches for learning general policies across planning domains are limited by the expressive power of $C_2$, namely; first-order logic with two variables and counting. This limitation can be overcomed by transitioning to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet embeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$- and $2$-GNNs that are confined to $C_2$, they require quartic time for message exchange and cubic space for embeddings, rendering them impractical. In this work, we introduce a parameterized version of relational GNNs. When $t$ is infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for embeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by exchanging fewer messages, yet interestingly, often yield the $C_3$ features required in several planning domains. Furthermore, the new R-GNN[$t$] ar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLaVA-UHD&#65292;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#20687;&#27169;&#22359;&#21270;&#31574;&#30053;&#12289;&#21387;&#32553;&#27169;&#22359;&#21644;&#31354;&#38388;&#27169;&#24335;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#24863;&#30693;&#20219;&#24847;&#38271;&#23485;&#27604;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;</title><link>https://arxiv.org/abs/2403.11703</link><description>&lt;p&gt;
LLaVA-UHD&#65306;&#19968;&#20010;&#33021;&#24863;&#30693;&#20219;&#24847;&#38271;&#23485;&#27604;&#21644;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;LMM
&lt;/p&gt;
&lt;p&gt;
LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11703
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLaVA-UHD&#65292;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#20687;&#27169;&#22359;&#21270;&#31574;&#30053;&#12289;&#21387;&#32553;&#27169;&#22359;&#21644;&#31354;&#38388;&#27169;&#24335;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#24863;&#30693;&#20219;&#24847;&#38271;&#23485;&#27604;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11703v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#25991;&#25688;&#35201;&#65306;&#35270;&#35273;&#32534;&#30721;&#26500;&#25104;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#29702;&#35299;&#35270;&#35273;&#19990;&#30028;&#30340;&#22522;&#30784;&#12290;&#20256;&#32479;&#30340; LMM &#22788;&#29702;&#22266;&#23450;&#22823;&#23567;&#21644;&#26377;&#38480;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#32780;&#26368;&#36817;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#25506;&#32034;&#22312;&#36866;&#24212;&#24615;&#12289;&#25928;&#29575;&#29978;&#33267;&#27491;&#30830;&#24615;&#26041;&#38754;&#26377;&#25152;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20197; GPT-4V &#21644; LLaVA-1.5 &#20026;&#20195;&#34920;&#20363;&#65292;&#24182;&#25581;&#31034;&#20102;&#26681;&#26893;&#20110;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#31574;&#30053;&#20013;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLaVA-UHD&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24863;&#30693;&#20219;&#24847;&#38271;&#23485;&#27604;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;LLaVA-UHD &#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;(1)&#19968;&#31181;&#22270;&#20687;&#27169;&#22359;&#21270;&#31574;&#30053;&#65292;&#23558;&#21407;&#22987;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#20998;&#25104;&#26356;&#23567;&#30340;&#21487;&#21464;&#22823;&#23567;&#29255;&#27573;&#65292;&#20197;&#20415;&#36827;&#34892;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#32534;&#30721;&#65292;(2)&#19968;&#20010;&#21387;&#32553;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#21387;&#32553;&#26469;&#33258;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#22270;&#20687;&#20196;&#29260;&#65292;&#20197;&#21450;(3)&#19968;&#20010;&#29992;&#20110;&#20026; LLMs &#32452;&#32455;&#29255;&#27573;&#20196;&#29260;&#30340;&#31354;&#38388;&#27169;&#24335;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;LLaVA-UHD
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11703v1 Announce Type: cross  Abstract: Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36741;&#21161;&#30340;HDL&#35843;&#35797;&#26694;&#26550; HDLdebugger&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#29983;&#25104;HDL&#35843;&#35797;&#25968;&#25454;&#65292;&#25552;&#20379;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#25628;&#32034;&#24341;&#25806;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;LLM&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;HDL&#35843;&#35797;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.11671</link><description>&lt;p&gt;
HDLdebugger: &#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;HDL&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;
HDLdebugger: Streamlining HDL debugging with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36741;&#21161;&#30340;HDL&#35843;&#35797;&#26694;&#26550; HDLdebugger&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#29983;&#25104;HDL&#35843;&#35797;&#25968;&#25454;&#65292;&#25552;&#20379;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#25628;&#32034;&#24341;&#25806;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;LLM&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;HDL&#35843;&#35797;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#65292;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65288;HDLs&#65289;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;HDL&#30340;&#22797;&#26434;&#35821;&#27861;&#21644;&#22312;&#32447;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#21363;&#20351;&#26159;&#32463;&#39564;&#20016;&#23500;&#30340;&#24037;&#31243;&#24072;&#65292;&#35843;&#35797;HDL&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#39033;&#22256;&#38590;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#21270;HDL&#20195;&#30721;&#35843;&#35797;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#30828;&#20214;&#24037;&#31243;&#24072;&#30340;&#36127;&#25285;&#12290;&#23613;&#31649;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#12289;&#23436;&#25104;&#21644;&#35843;&#35797;&#36719;&#20214;&#20195;&#30721;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#19987;&#38376;&#39046;&#22495;&#30340;HDL&#35843;&#35797;&#20013;&#30340;&#21033;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#65292;&#24182;&#19988;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36741;&#21161;&#30340;HDL&#35843;&#35797;&#26694;&#26550;&#65292;&#21363;HDLdebugger&#65292;&#23427;&#21253;&#25324;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#29983;&#25104;HDL&#35843;&#35797;&#25968;&#25454;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;&#20197;&#21450;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;LLM&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11671v1 Announce Type: cross  Abstract: In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#36816;&#34892;&#26102;&#30340;&#26102;&#38388;&#32422;&#26463;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#25972;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#25216;&#26415;&#65292;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#20013;&#24341;&#20837;&#20102;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.11642</link><description>&lt;p&gt;
&#24341;&#23548;&#22522;&#20110;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#21487;&#35299;&#37322;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#36827;&#34892;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#36816;&#34892;&#26102;&#30340;&#26102;&#38388;&#32422;&#26463;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#25972;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#25216;&#26415;&#65292;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#20013;&#24341;&#20837;&#20102;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#25351;&#20986;&#20102;&#20462;&#25913;&#36755;&#20837;&#23454;&#20363;&#20197;&#25913;&#21464;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#32467;&#26524;&#24212;&#35813;&#26377;&#20160;&#20040;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#39046;&#22495;&#22788;&#29702;&#21453;&#20107;&#23454;&#35299;&#37322;&#26102;&#65292;&#24517;&#39035;&#20180;&#32454;&#32771;&#34385;&#20107;&#20214;&#20043;&#38388;&#30340;&#25511;&#21046;&#27969;&#20851;&#31995;&#12290;&#30830;&#23454;&#65292;&#19968;&#20010;&#21453;&#20107;&#23454;&#19981;&#24212;&#36829;&#21453;&#27963;&#21160;&#20043;&#38388;&#30340;&#25511;&#21046;&#27969;&#20851;&#31995;&#65288;&#21363;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#65289;&#12290;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#30340;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#26377;&#19968;&#31995;&#21015;&#20851;&#20110;&#22522;&#20110;&#32467;&#26524;&#30340;&#39044;&#27979;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20316;&#21697;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#22312;&#29983;&#25104;&#36825;&#20123;&#21453;&#20107;&#23454;&#26102;&#32771;&#34385;&#20102;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;&#30340;&#21253;&#21547;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20197;&#32771;&#34385;&#19968;&#31995;&#21015;&#36816;&#34892;&#26102;&#30340;&#26102;&#38388;&#32422;&#26463;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#20551;&#23450;&#36825;&#31181;&#26102;&#38388;&#32972;&#26223;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11642v1 Announce Type: new  Abstract: Counterfactual explanations suggest what should be different in the input instance to change the outcome of an AI system. When dealing with counterfactual explanations in the field of Predictive Process Monitoring, however, control flow relationships among events have to be carefully considered. A counterfactual, indeed, should not violate control flow relationships among activities (temporal background knowledege). Within the field of Explainability in Predictive Process Monitoring, there have been a series of works regarding counterfactual explanations for outcome-based predictions. However, none of them consider the inclusion of temporal background knowledge when generating these counterfactuals. In this work, we adapt state-of-the-art techniques for counterfactual generation in the domain of XAI that are based on genetic algorithms to consider a series of temporal constraints at runtime. We assume that this temporal background knowle
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22235;&#20803;&#25968;&#35270;&#35282;&#36827;&#34892;&#35270;&#35273;&#33310;&#36424;&#29983;&#25104;&#30340;&#22235;&#20803;&#25968;&#22686;&#24378;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QEAN&#65289;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#36816;&#21160;&#24207;&#21015;&#21644;&#38899;&#39057;&#24207;&#21015;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.11626</link><description>&lt;p&gt;
QEAN&#65306;&#22235;&#20803;&#25968;&#22686;&#24378;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#33310;&#36424;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11626
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22235;&#20803;&#25968;&#35270;&#35282;&#36827;&#34892;&#35270;&#35273;&#33310;&#36424;&#29983;&#25104;&#30340;&#22235;&#20803;&#25968;&#22686;&#24378;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QEAN&#65289;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#36816;&#21160;&#24207;&#21015;&#21644;&#38899;&#39057;&#24207;&#21015;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#21019;&#29983;&#30340;&#33310;&#36424;&#30740;&#31350;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22235;&#20803;&#25968;&#30340;Quaternion-Enhanced Attention Network (QEAN)&#29992;&#20110;&#20174;&#22235;&#20803;&#25968;&#35270;&#35282;&#21512;&#25104;&#35270;&#35273;&#33310;&#36424;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#26059;&#20301;&#32622;&#23884;&#20837;&#65288;SPE&#65289;&#27169;&#22359;&#21644;&#22235;&#20803;&#25968;&#26059;&#36716;&#27880;&#24847;&#21147;&#65288;QRA&#65289;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11626v1 Announce Type: cross  Abstract: The study of music-generated dance is a novel and challenging Image generation task. It aims to input a piece of music and seed motions, then generate natural dance movements for the subsequent music. Transformer-based methods face challenges in time series prediction tasks related to human movements and music due to their struggle in capturing the nonlinear relationship and temporal aspects. This can lead to issues like joint deformation, role deviation, floating, and inconsistencies in dance movements generated in response to the music. In this paper, we propose a Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a quaternion perspective, which consists of a Spin Position Embedding (SPE) module and a Quaternion Rotary Attention (QRA) module. First, SPE embeds position information into self-attention in a rotational manner, leading to better learning of features of movement sequences and audio sequences, and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;&#35745;&#21010;&#30340;SAT&#32534;&#30721;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#24212;&#29992;1&#20010;SWAP&#21644;&#19968;&#32452;CNOT&#12290;&#21033;&#29992;&#29305;&#23450;&#39046;&#22495;&#20449;&#24687;&#65292;&#22312;&#24182;&#34892;&#35745;&#21010;&#20013;&#20445;&#25345;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25193;&#23637;&#21040;&#22823;&#22411;&#28145;&#24230;&#30005;&#36335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#22810;&#39033;&#27604;&#39046;&#20808;&#30340;&#31934;&#30830;&#21644;&#25509;&#36817;&#26368;&#20248;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65288;&#39640;&#36798;100&#20493;&#65289;&#12290;&#25105;&#20204;&#39318;&#27425;&#33021;&#22815;&#26368;&#20248;&#22320;&#26144;&#23556;&#22810;&#20010;8&#12289;14&#21644;16&#37327;&#23376;&#27604;&#29305;</title><link>https://arxiv.org/abs/2403.11598</link><description>&lt;p&gt;
&#38754;&#21521;&#20855;&#26377;100&#22810;&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;NISQ&#22788;&#29702;&#22120;&#30340;&#28145;&#24230;&#37327;&#23376;&#30005;&#36335;&#26368;&#20339;&#24067;&#23616;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors with 100+ Qubits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;&#35745;&#21010;&#30340;SAT&#32534;&#30721;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#24212;&#29992;1&#20010;SWAP&#21644;&#19968;&#32452;CNOT&#12290;&#21033;&#29992;&#29305;&#23450;&#39046;&#22495;&#20449;&#24687;&#65292;&#22312;&#24182;&#34892;&#35745;&#21010;&#20013;&#20445;&#25345;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25193;&#23637;&#21040;&#22823;&#22411;&#28145;&#24230;&#30005;&#36335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#22810;&#39033;&#27604;&#39046;&#20808;&#30340;&#31934;&#30830;&#21644;&#25509;&#36817;&#26368;&#20248;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65288;&#39640;&#36798;100&#20493;&#65289;&#12290;&#25105;&#20204;&#39318;&#27425;&#33021;&#22815;&#26368;&#20248;&#22320;&#26144;&#23556;&#22810;&#20010;8&#12289;14&#21644;16&#37327;&#23376;&#27604;&#29305;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23616;&#32508;&#21512;&#26159;&#23558;&#37327;&#23376;&#30005;&#36335;&#26144;&#23556;&#21040;&#37327;&#23376;&#22788;&#29702;&#22120;&#12290;&#38656;&#35201;&#20026;&#20165;&#22312;&#36830;&#25509;&#30340;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#19978;&#23433;&#25490;2&#27604;&#29305;&#38376;&#30340;&#35843;&#24230;&#36827;&#34892;SWAP&#38376;&#25554;&#20837;&#12290;&#38543;&#30528;NISQ&#22788;&#29702;&#22120;&#20013;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#21487;&#25193;&#23637;&#30340;&#24067;&#23616;&#32508;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21551;&#21457;&#24335;&#26041;&#27861;&#20013;&#35266;&#23519;&#21040;&#30340;&#20248;&#21270;&#38388;&#38553;&#36739;&#22823;&#65292;&#38656;&#35201;&#21487;&#25193;&#23637;&#30340;&#31934;&#30830;&#26041;&#27861;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#31934;&#30830;&#21644;&#25509;&#36817;&#26368;&#20248;&#26041;&#27861;&#36866;&#29992;&#20110;&#20013;&#31561;&#35268;&#27169;&#30005;&#36335;&#65292;&#20294;&#22823;&#22411;&#28145;&#24230;&#30005;&#36335;&#20173;&#36229;&#20986;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11598v1 Announce Type: cross  Abstract: Layout synthesis is mapping a quantum circuit to a quantum processor. SWAP gate insertions are needed for scheduling 2-qubit gates only on connected physical qubits. With the ever-increasing number of qubits in NISQ processors, scalable layout synthesis is of utmost importance. With large optimality gaps observed in heuristic approaches, scalable exact methods are needed. While recent exact and near-optimal approaches scale to moderate circuits, large deep circuits are still out of scope.   In this work, we propose a SAT encoding based on parallel plans that apply 1 SWAP and a group of CNOTs at each time step. Using domain-specific information, we maintain optimality in parallel plans while scaling to large and deep circuits. From our results, we show the scalability of our approach which significantly outperforms leading exact and near-optimal approaches (up to 100x). For the first time, we can optimally map several 8, 14, and 16 qubi
&lt;/p&gt;</description></item><item><title>Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11585</link><description>&lt;p&gt;
Linguacodus&#65306;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#36827;&#34892;&#21464;&#38761;&#24615;&#20195;&#30721;&#29983;&#25104;&#30340;&#21327;&#21516;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11585
&lt;/p&gt;
&lt;p&gt;
Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26080;&#32541;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Linguacodus&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#21160;&#24577;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39640;&#32423;&#25968;&#25454;&#22609;&#24418;&#25351;&#20196;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36845;&#20195;&#22320;&#36716;&#25442;&#20026;&#20195;&#30721;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;Linguacodus&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#35780;&#20272;&#21508;&#31181;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#29305;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#20195;&#30721;&#12290;Linguacodus&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#20219;&#21153;&#25551;&#36848;&#21644;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#23545;&#25512;&#36827;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TOLE&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#32423;&#21035;&#22870;&#21169;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;"&#20808;&#37327;&#23376;&#21270;&#65292;&#28982;&#21518;&#21152;&#22122;&#22768;"&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#21487;&#20197;&#22312;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#19979;&#28789;&#27963;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#36807;&#25311;&#21512;&#21644;&#35821;&#20041;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11558</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#32423;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Token-level Feedback for Controllable Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11558
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TOLE&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#32423;&#21035;&#22870;&#21169;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;"&#20808;&#37327;&#23376;&#21270;&#65292;&#28982;&#21518;&#21152;&#22122;&#22768;"&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#21487;&#20197;&#22312;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#19979;&#28789;&#27963;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#36807;&#25311;&#21512;&#21644;&#35821;&#20041;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24341;&#20837;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65288;&#24494;&#35843;&#22522;&#30784;&#26041;&#27861;&#65289;&#25110;&#35821;&#20041;&#23849;&#28291;&#65288;&#21518;&#22788;&#29702;&#26041;&#27861;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;RL&#26041;&#27861;&#36890;&#24120;&#26159;&#30001;&#31895;&#31890;&#24230;&#65288;&#21477;&#23376;/&#27573;&#33853;&#32423;&#21035;&#65289;&#30340;&#21453;&#39304;&#24341;&#23548;&#30340;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21477;&#23376;&#20869;&#35821;&#20041;&#25197;&#26354;&#25110;&#36827;&#23637;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLE&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21046;&#23450;&#20102;TOken-LEvel&#22870;&#21169;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#37319;&#29992;&#8220;&#20808;&#37327;&#23376;&#21270;&#65292;&#28982;&#21518;&#21152;&#22122;&#22768;&#8221;&#33539;&#24335;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;TOLE&#21487;&#20197;&#28789;&#27963;&#22320;&#25193;&#23637;&#21040;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#65292;&#35745;&#31639;&#25104;&#26412;&#24456;&#20302;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;al
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11558v1 Announce Type: cross  Abstract: To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a "first-quantize-then-noise" paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our al
&lt;/p&gt;</description></item><item><title>LLM^3&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#20855;&#22791;&#24378;&#22823;&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#36890;&#36807;&#25509;&#21475;&#25552;&#20986;&#31526;&#21495;&#21160;&#20316;&#24207;&#21015;&#21644;&#36873;&#25321;&#36830;&#32493;&#21160;&#20316;&#21442;&#25968;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#36816;&#21160;&#35268;&#21010;&#30340;&#21453;&#39304;&#26469;&#36845;&#20195;&#20248;&#21270;&#25552;&#35758;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#28040;&#24687;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11552</link><description>&lt;p&gt;
LLM^3:&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#20197;&#21450;&#36816;&#21160;&#22833;&#36133;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM^3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11552
&lt;/p&gt;
&lt;p&gt;
LLM^3&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#20855;&#22791;&#24378;&#22823;&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#36890;&#36807;&#25509;&#21475;&#25552;&#20986;&#31526;&#21495;&#21160;&#20316;&#24207;&#21015;&#21644;&#36873;&#25321;&#36830;&#32493;&#21160;&#20316;&#21442;&#25968;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#36816;&#21160;&#35268;&#21010;&#30340;&#21453;&#39304;&#26469;&#36845;&#20195;&#20248;&#21270;&#25552;&#35758;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#28040;&#24687;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#30028;&#38754;&#65292;&#23558;&#31526;&#21495;&#20219;&#21153;&#35268;&#21010;&#19982;&#36830;&#32493;&#36816;&#21160;&#29983;&#25104;&#36830;&#25509;&#36215;&#26469;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#27169;&#22359;&#22312;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#26032;&#20219;&#21153;&#26041;&#38754;&#26377;&#38480;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM^3&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;TAMP&#26694;&#26550;&#65292;&#20855;&#26377;&#39046;&#22495;&#26080;&#20851;&#30340;&#25509;&#21475;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#30340;&#24378;&#22823;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#26469;&#25552;&#20986;&#31526;&#21495;&#21160;&#20316;&#24207;&#21015;&#65292;&#24182;&#36873;&#25321;&#36830;&#32493;&#21160;&#20316;&#21442;&#25968;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#12290;&#20851;&#38190;&#26159;&#65292;LLM^3&#36890;&#36807;&#25552;&#31034;&#23558;&#36816;&#21160;&#35268;&#21010;&#21453;&#39304;&#21040;&#20854;&#20013;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#36890;&#36807;&#23545;&#36816;&#21160;&#22833;&#36133;&#36827;&#34892;&#25512;&#29702;&#26469;&#36845;&#20195;&#22320;&#20248;&#21270;&#20854;&#25552;&#35758;&#12290;&#22240;&#27492;&#65292;LLM^3&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#36816;&#21160;&#35268;&#21010;&#20043;&#38388;&#24314;&#31435;&#25509;&#21475;&#65292;&#20943;&#36731;&#20102;&#22788;&#29702;&#23427;&#20204;&#20043;&#38388;&#29305;&#23450;&#39046;&#22495;&#28040;&#24687;&#30340;&#22797;&#26434;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11552v1 Announce Type: cross  Abstract: Conventional Task and Motion Planning (TAMP) approaches rely on manually crafted interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM^3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM^3 incorporates motion planning feed- back through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure. Consequently, LLM^3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain- specific messages between them. Through a series of simulat
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#22806;&#37096;&#27169;&#24577;&#24341;&#23548;&#30340;&#25968;&#25454;&#25366;&#25496;&#26694;&#26550;&#20197;&#35299;&#20915;&#33258;&#21160;&#20809;&#23398;&#26816;&#39564;&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#38754;&#20020;&#30340;&#27169;&#22411;&#37096;&#32626;&#25361;&#25112;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11536</link><description>&lt;p&gt;
OCR&#21363;&#20026;&#25152;&#38656;&#65306;&#23558;&#22810;&#27169;&#24577;&#24615;&#24341;&#20837;&#22522;&#20110;&#22270;&#20687;&#30340;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11536
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22806;&#37096;&#27169;&#24577;&#24341;&#23548;&#30340;&#25968;&#25454;&#25366;&#25496;&#26694;&#26550;&#20197;&#35299;&#20915;&#33258;&#21160;&#20809;&#23398;&#26816;&#39564;&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#38754;&#20020;&#30340;&#27169;&#22411;&#37096;&#32626;&#25361;&#25112;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11536v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#21160;&#20809;&#23398;&#26816;&#39564;&#65288;AOI&#65289;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20027;&#35201;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#20202;&#22120;&#36827;&#34892;&#25195;&#25551;&#12290;&#23427;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#32441;&#29702;&#25110;&#22270;&#26696;&#26469;&#26816;&#27979;&#24322;&#24120;&#65292;&#20174;&#32780;&#25104;&#20026;&#24037;&#19994;&#21046;&#36896;&#21644;&#36136;&#37327;&#25511;&#21046;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23613;&#31649;&#23427;&#30340;&#37325;&#35201;&#24615;&#65292;&#29992;&#20110;AOI&#30340;&#27169;&#22411;&#37096;&#32626;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#26679;&#26412;&#37327;&#38459;&#30861;&#20102;&#26377;&#25928;&#29305;&#24449;&#23398;&#20064;&#65292;&#28304;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#22312;&#25104;&#20687;&#36807;&#31243;&#20013;&#20809;&#29031;&#21644;&#25668;&#20687;&#26426;&#20301;&#32622;&#21464;&#21270;&#23545;&#20854;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#22240;&#32032;&#20849;&#21516;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;AOI&#36890;&#24120;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#26469;&#33258;&#26426;&#22120;&#25110;&#22270;&#20687;&#20869;&#37096;&#30340;&#20016;&#23500;&#26426;&#21046;&#21442;&#25968;&#20449;&#24687;&#65292;&#21253;&#25324;&#32463;&#24120;&#26377;&#30410;&#20110;AOI&#20998;&#31867;&#30340;&#32479;&#35745;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#22806;&#37096;&#27169;&#24577;&#24341;&#23548;&#30340;&#25968;&#25454;&#25366;&#25496;&#26694;&#26550;&#65292;&#20027;&#35201;&#26681;&#26893;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11536v1 Announce Type: cross  Abstract: Automatic optical inspection (AOI) plays a pivotal role in the manufacturing process, predominantly leveraging high-resolution imaging instruments for scanning purposes. It detects anomalies by analyzing image textures or patterns, making it an essential tool in industrial manufacturing and quality control. Despite its importance, the deployment of models for AOI often faces challenges. These include limited sample sizes, which hinder effective feature learning, variations among source domains, and sensitivities to changes in lighting and camera positions during imaging. These factors collectively compromise the accuracy of model predictions. Traditional AOI often fails to capitalize on the rich mechanism-parameter information from machines or inside images, including statistical parameters, which typically benefit AOI classification. To address this, we introduce an external modality-guided data mining framework, primarily rooted in o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#27700;&#19979;&#35270;&#39057;&#22686;&#24378;&#27169;&#22411;UVENet&#65292;&#21033;&#29992;&#24103;&#38388;&#20851;&#31995;&#26469;&#25552;&#39640;&#27700;&#19979;&#35270;&#39057;&#30340;&#21487;&#35265;&#24615;&#21644;&#24103;&#36136;&#37327;&#65292;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;UVE&#20219;&#21153;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;SUVE&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#32570;&#20047;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11506</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#27700;&#19979;&#35270;&#39057;&#22686;&#24378;&#65306;&#25968;&#25454;&#38598;&#19982;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
End-To-End Underwater Video Enhancement: Dataset and Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#27700;&#19979;&#35270;&#39057;&#22686;&#24378;&#27169;&#22411;UVENet&#65292;&#21033;&#29992;&#24103;&#38388;&#20851;&#31995;&#26469;&#25552;&#39640;&#27700;&#19979;&#35270;&#39057;&#30340;&#21487;&#35265;&#24615;&#21644;&#24103;&#36136;&#37327;&#65292;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;UVE&#20219;&#21153;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;SUVE&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#32570;&#20047;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#35270;&#39057;&#22686;&#24378;&#65288;UVE&#65289;&#26088;&#22312;&#25552;&#39640;&#27700;&#19979;&#35270;&#39057;&#30340;&#21487;&#35265;&#24615;&#21644;&#24103;&#36136;&#37327;&#65292;&#23545;&#28023;&#27915;&#30740;&#31350;&#21644;&#25506;&#32034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#22270;&#20687;&#22686;&#24378;&#31639;&#27861;&#65292;&#20197;&#29420;&#31435;&#22686;&#24378;&#27599;&#19968;&#24103;&#12290;&#30446;&#21069;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;UVE&#20219;&#21153;&#23450;&#21046;&#30340;&#21463;&#30417;&#30563;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21512;&#25104;&#27700;&#19979;&#35270;&#39057;&#22686;&#24378;&#65288;SUVE&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;840&#20010;&#22810;&#26679;&#21270;&#30340;&#27700;&#19979;&#39118;&#26684;&#35270;&#39057;&#65292;&#37197;&#23545;&#30495;&#23454;&#35270;&#39057;&#20316;&#20026;&#21442;&#32771;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#19979;&#35270;&#39057;&#22686;&#24378;&#27169;&#22411;UVENet&#65292;&#21033;&#29992;&#24103;&#38388;&#20851;&#31995;&#23454;&#29616;&#26356;&#22909;&#30340;&#22686;&#24378;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#27700;&#19979;&#35270;&#39057;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#30740;&#31350;&#20195;&#34920;&#20102;&#23545;UVE&#30340;&#39318;&#27425;&#20840;&#38754;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11506v1 Announce Type: cross  Abstract: Underwater video enhancement (UVE) aims to improve the visibility and frame quality of underwater videos, which has significant implications for marine research and exploration. However, existing methods primarily focus on developing image enhancement algorithms to enhance each frame independently. There is a lack of supervised datasets and models specifically tailored for UVE tasks. To fill this gap, we construct the Synthetic Underwater Video Enhancement (SUVE) dataset, comprising 840 diverse underwater-style videos paired with ground-truth reference videos. Based on this dataset, we train a novel underwater video enhancement model, UVENet, which utilizes inter-frame relationships to achieve better enhancement performance. Through extensive experiments on both synthetic and real underwater videos, we demonstrate the effectiveness of our approach. This study represents the first comprehensive exploration of UVE to our knowledge. The c
&lt;/p&gt;</description></item><item><title>MLVICX&#26159;&#19968;&#31181;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#26041;&#24046;&#21644;&#21327;&#26041;&#24046;&#25506;&#32034;&#31574;&#30053;&#26469;&#25429;&#33719;&#20016;&#23500;&#30340;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.11504</link><description>&lt;p&gt;
MLVICX: &#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#22810;&#32423;&#26041;&#24046;-&#21327;&#26041;&#24046;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11504
&lt;/p&gt;
&lt;p&gt;
MLVICX&#26159;&#19968;&#31181;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#26041;&#24046;&#21644;&#21327;&#26041;&#24046;&#25506;&#32034;&#31574;&#30053;&#26469;&#25429;&#33719;&#20016;&#23500;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#20943;&#23569;&#25163;&#21160;&#27880;&#37322;&#38656;&#27714;&#12289;&#20351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#29992;&#36884;&#12290;&#36890;&#36807;&#21033;&#29992;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#26080;&#38656;&#25110;&#21482;&#38656;&#23569;&#37327;&#24494;&#35843;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#22797;&#26434;&#35299;&#21078;&#32467;&#26500;&#21644;&#22810;&#26679;&#20020;&#24202;&#30149;&#24773;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#22914;&#33016;&#37096;X&#23556;&#32447;&#65292;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#32534;&#30721;&#32454;&#31890;&#24230;&#32454;&#33410;&#24182;&#20445;&#30041;&#26356;&#24191;&#27867;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#34920;&#24449;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MLVICX&#65288;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#22810;&#32423;&#26041;&#24046;-&#21327;&#26041;&#24046;&#25506;&#32034;&#65289;&#65292;&#19968;&#31181;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25429;&#33719;&#23500;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32423;&#26041;&#24046;&#21644;&#21327;&#26041;&#24046;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#20854;&#26356;&#26377;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11504v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) is potentially useful in reducing the need for manual annotation and making deep learning models accessible for medical image analysis tasks. By leveraging the representations learned from unlabeled data, self-supervised models perform well on tasks that require little to no fine-tuning. However, for medical images, like chest X-rays, which are characterized by complex anatomical structures and diverse clinical conditions, there arises a need for representation learning techniques that can encode fine-grained details while preserving the broader contextual information. In this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning), an approach to capture rich representations in the form of embeddings from chest X-ray images. Central to our approach is a novel multi-level variance and covariance exploration strategy that empowers t
&lt;/p&gt;</description></item><item><title>MCD&#25968;&#25454;&#38598;&#20026;&#26426;&#22120;&#20154;&#24863;&#30693;&#39046;&#22495;&#24102;&#26469;&#20102;&#20840;&#26032;&#25361;&#25112;&#65292;&#21253;&#21547;&#22810;&#31181;&#20256;&#24863;&#27169;&#24335;&#12289;&#39640;&#31934;&#24230;&#22320;&#38754;&#30495;&#20540;&#21644;&#36328;&#26657;&#22253;&#30340;&#22810;&#26679;&#21270;&#25361;&#25112;&#29615;&#22659;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#23545;&#22823;&#35268;&#27169;NRE&#28608;&#20809;&#25195;&#25551;&#36827;&#34892;&#30340;&#35821;&#20041;&#27880;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.11496</link><description>&lt;p&gt;
MCD: &#29992;&#20110;&#26426;&#22120;&#20154;&#24863;&#30693;&#30340;&#22810;&#26679;&#24615;&#22823;&#35268;&#27169;&#22810;&#26657;&#21306;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11496
&lt;/p&gt;
&lt;p&gt;
MCD&#25968;&#25454;&#38598;&#20026;&#26426;&#22120;&#20154;&#24863;&#30693;&#39046;&#22495;&#24102;&#26469;&#20102;&#20840;&#26032;&#25361;&#25112;&#65292;&#21253;&#21547;&#22810;&#31181;&#20256;&#24863;&#27169;&#24335;&#12289;&#39640;&#31934;&#24230;&#22320;&#38754;&#30495;&#20540;&#21644;&#36328;&#26657;&#22253;&#30340;&#22810;&#26679;&#21270;&#25361;&#25112;&#29615;&#22659;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#23545;&#22823;&#35268;&#27169;NRE&#28608;&#20809;&#25195;&#25551;&#36827;&#34892;&#30340;&#35821;&#20041;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#23545;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#23384;&#22312;&#20559;&#35265;&#65292;&#32780;&#26410;&#26631;&#35760;&#30340;SLAM&#25968;&#25454;&#38598;&#24456;&#24555;&#34987;&#36807;&#24230;&#25311;&#21512;&#65292;&#36890;&#24120;&#32570;&#20047;&#29615;&#22659;&#21644;&#39046;&#22495;&#21464;&#21270;&#12290;&#20026;&#20102;&#25299;&#23637;&#36825;&#20123;&#39046;&#22495;&#30340;&#36793;&#30028;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#21517;&#20026;MCD&#65288;&#22810;&#26657;&#21306;&#25968;&#25454;&#38598;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20256;&#24863;&#27169;&#24335;&#12289;&#39640;&#31934;&#24230;&#22320;&#38754;&#30495;&#20540;&#20197;&#21450;&#36328;&#36234;&#19977;&#20010;&#27431;&#20122;&#22823;&#23398;&#26657;&#22253;&#30340;&#22810;&#26679;&#21270;&#25361;&#25112;&#29615;&#22659;&#12290;MCD&#21253;&#25324;&#32463;&#20856;&#22278;&#26609;&#26059;&#36716;&#65288;CCS&#65289;&#21644;&#38750;&#37325;&#22797;&#22238;&#29615;&#65288;NRE&#65289;&#28608;&#20809;&#38647;&#36798;&#12289;&#39640;&#36136;&#37327;IMU&#65288;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65289;&#12289;&#25668;&#20687;&#22836;&#21644;UWB&#65288;&#36229;&#23485;&#24102;&#65289;&#20256;&#24863;&#22120;&#12290;&#27492;&#22806;&#65292;&#22312;&#24320;&#21019;&#24615;&#30340;&#21162;&#21147;&#19979;&#65292;&#25105;&#20204;&#38024;&#23545;&#19977;&#20010;&#39046;&#22495;&#30340;59k&#20010;&#31232;&#30095;NRE&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#24341;&#20837;&#20102;29&#31867;&#30340;&#35821;&#20041;&#27880;&#37322;&#65292;&#20174;&#32780;&#20026;&#29616;&#26377;&#30340;&#35821;&#20041;&#20998;&#21106;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11496v1 Announce Type: cross  Abstract: Perception plays a crucial role in various robot applications. However, existing well-annotated datasets are biased towards autonomous driving scenarios, while unlabelled SLAM datasets are quickly over-fitted, and often lack environment and domain variations. To expand the frontier of these fields, we introduce a comprehensive dataset named MCD (Multi-Campus Dataset), featuring a wide range of sensing modalities, high-accuracy ground truth, and diverse challenging environments across three Eurasian university campuses. MCD comprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive Epicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and UWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce semantic annotations of 29 classes over 59k sparse NRE lidar scans across three domains, thus providing a novel challenge to existing semantic segmentation research upon this largely u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Toast&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#21450;&#22686;&#24378;&#29256;DyToast&#65292;&#29992;&#20110;&#23398;&#20064;&#36335;&#32593;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#24182;&#22686;&#24378;&#20102;&#26102;&#38388;&#21160;&#24577;&#30340;&#25972;&#21512;&#65292;&#20197;&#25552;&#39640;&#21508;&#31181;&#26102;&#38388;&#25935;&#24863;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11495</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#38388;&#21160;&#24577;&#30340;&#36335;&#32593;&#35821;&#20041;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11495
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Toast&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#21450;&#22686;&#24378;&#29256;DyToast&#65292;&#29992;&#20110;&#23398;&#20064;&#36335;&#32593;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#24182;&#22686;&#24378;&#20102;&#26102;&#38388;&#21160;&#24577;&#30340;&#25972;&#21512;&#65292;&#20197;&#25552;&#39640;&#21508;&#31181;&#26102;&#38388;&#25935;&#24863;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Toast&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36335;&#32593;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#21450;&#20854;&#22686;&#24378;&#29256;DyToast&#65292;&#26088;&#22312;&#22686;&#24378;&#26102;&#38388;&#21160;&#24577;&#30340;&#25972;&#21512;&#65292;&#25552;&#39640;&#21508;&#31181;&#26102;&#38388;&#25935;&#24863;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#32534;&#30721;&#36335;&#32593;&#22266;&#26377;&#30340;&#20004;&#20010;&#20851;&#38190;&#35821;&#20041;&#29305;&#24449;&#65306;&#20132;&#36890;&#27169;&#24335;&#21644;&#34892;&#39542;&#35821;&#20041;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#32435;&#20837;&#26088;&#22312;&#39044;&#27979;&#19982;&#30446;&#26631;&#36335;&#27573;&#30456;&#20851;&#30340;&#20132;&#36890;&#19978;&#19979;&#25991;&#30340;&#36741;&#21161;&#30446;&#26631;&#65292;&#25913;&#36827;&#20102;skip-gram&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36712;&#36857;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#22312;&#36335;&#32593;&#19978;&#25552;&#28860;&#34892;&#39542;&#35821;&#20041;&#12290;DyToast&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#30410;&#22788;&#29305;&#24615;&#30340;&#32479;&#19968;&#19977;&#35282;&#20989;&#25968;&#36827;&#19968;&#27493;&#22686;&#36827;&#20102;&#35813;&#26694;&#26550;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#36335;&#32593;&#30340;&#26102;&#38388;&#28436;&#21464;&#21644;&#21160;&#24577;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11495v1 Announce Type: cross  Abstract: In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road netwo
&lt;/p&gt;</description></item><item><title>SmartRefine&#25552;&#20986;&#20102;&#19968;&#31181;&#22330;&#26223;&#33258;&#36866;&#24212;&#32454;&#21270;&#31574;&#30053;&#65292;&#21487;&#22312;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#37327;&#19979;&#23545;&#36816;&#21160;&#39044;&#27979;&#36827;&#34892;&#32454;&#21270;</title><link>https://arxiv.org/abs/2403.11492</link><description>&lt;p&gt;
SmartRefine: &#19968;&#31181;&#22330;&#26223;&#33258;&#36866;&#24212;&#32454;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient Motion Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11492
&lt;/p&gt;
&lt;p&gt;
SmartRefine&#25552;&#20986;&#20102;&#19968;&#31181;&#22330;&#26223;&#33258;&#36866;&#24212;&#32454;&#21270;&#31574;&#30053;&#65292;&#21487;&#22312;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#37327;&#19979;&#23545;&#36816;&#21160;&#39044;&#27979;&#36827;&#34892;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21608;&#22260;&#20195;&#29702;&#30340;&#26410;&#26469;&#36816;&#21160;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#22312;&#21160;&#24577;&#30340;&#12289;&#20154;&#26426;&#28151;&#21512;&#29615;&#22659;&#20013;&#23433;&#20840;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#36947;&#36335;&#22320;&#22270;&#21644;&#21608;&#22260;&#20195;&#29702;&#30340;&#29366;&#24577;&#65292;&#20026;&#36816;&#21160;&#34892;&#20026;&#39044;&#27979;&#25552;&#20379;&#20851;&#38190;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22330;&#26223;&#33258;&#36866;&#24212;&#32454;&#21270;&#31574;&#30053;&#65292;&#31216;&#20026;SmartRefine&#65292;&#20197;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#37327;&#23545;&#39044;&#27979;&#36827;&#34892;&#32454;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SmartRefine&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#22330;&#26223;&#30340;&#29305;&#24615;&#20840;&#38754;&#35843;&#25972;&#32454;&#21270;&#37197;&#32622;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36136;&#37327;&#26469;&#26234;&#33021;&#22320;&#36873;&#25321;&#32454;&#21270;&#36845;&#20195;&#30340;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11492v1 Announce Type: cross  Abstract: Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction. To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement. However, they either incur a large amount of computation or bring limited improvement, if not both. In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation. Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a qualit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#20855;&#36523;&#26426;&#22120;&#20154;&#30340;&#8220;&#34892;&#36335;&#25351;&#31034;&#8221;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#27169;&#25311;&#24179;&#21488;&#19978;&#23637;&#31034;&#20986;&#36328;&#24179;&#21488;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11487</link><description>&lt;p&gt;
LLM&#33021;&#29983;&#25104;&#31867;&#20154;&#34892;&#36335;&#25351;&#31034;&#21527;&#65311;&#36208;&#21521;&#36328;&#24179;&#21488;&#30340;&#20855;&#36523;&#25351;&#20196;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#20855;&#36523;&#26426;&#22120;&#20154;&#30340;&#8220;&#34892;&#36335;&#25351;&#31034;&#8221;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#27169;&#25311;&#24179;&#21488;&#19978;&#23637;&#31034;&#20986;&#36328;&#24179;&#21488;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;&#8220;&#34892;&#36335;&#25351;&#31034;&#8221;&#20197;&#25351;&#23548;&#20855;&#36523;&#26426;&#22120;&#20154;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20877;&#20381;&#36182;&#20110;&#20165;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#27169;&#25311;&#24179;&#21488;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#26159;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35843;&#33410;LLM&#65292;&#20197;&#20351;&#29992;&#23569;&#37327;&#21442;&#32771;&#29983;&#25104;&#25351;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#35270;&#35273;&#38382;&#31572;&#31574;&#30053;&#25910;&#38598;&#29615;&#22659;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;LLM&#29992;&#20110;&#25351;&#20196;&#21512;&#25104;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#22312;&#22810;&#20010;&#27169;&#25311;&#24179;&#21488;&#19978;&#65292;&#21253;&#25324;Matterport3D&#12289;AI Habitat&#21644;ThreeDWorld&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#20854;&#36328;&#24179;&#21488;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#20027;&#35266;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35266;&#23519;&#21040;83.3%&#30340;&#29992;&#25143;&#35748;&#20026;&#21512;&#25104;&#30340;&#25351;&#31034;&#20934;&#30830;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32454;&#33410;&#65292;&#24182;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25351;&#31034;&#31867;&#20284;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11487v1 Announce Type: cross  Abstract: We present a novel approach to automatically synthesize "wayfinding instructions" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we con
&lt;/p&gt;</description></item><item><title>&#26041;&#24046;&#19981;&#24179;&#34913;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25552;&#20986;&#19968;&#31181;&#19981;&#20381;&#36182;&#39044;&#35757;&#32451;&#22270;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.11483</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Open-World Semi-Supervised Learning for Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11483
&lt;/p&gt;
&lt;p&gt;
&#26041;&#24046;&#19981;&#24179;&#34913;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25552;&#20986;&#19968;&#31181;&#19981;&#20381;&#36182;&#39044;&#35757;&#32451;&#22270;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#23398;&#20064; (Open-world SSL) &#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#65292;&#22312;&#22270;&#24418;&#31038;&#21306;&#20013;&#26159;&#19968;&#20010;&#23454;&#29992;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#23427;&#23558;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#31867;&#20026;&#24050;&#35265;&#31867;&#25110;&#22810;&#20010;&#26032;&#39062;&#31867;&#12290;&#26681;&#25454;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26041;&#24046;&#19981;&#24179;&#34913;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#39044;&#35757;&#32451;&#29305;&#24449;&#32534;&#30721;&#22120;&#21487;&#20197;&#36890;&#36807;&#20026;&#26032;&#39062;&#31867;&#29983;&#25104;&#32039;&#20945;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;&#21508;&#31181;&#31867;&#22411;&#30340;&#22270;&#24418;&#25968;&#25454;&#21019;&#24314;&#36890;&#29992;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#19981;&#20381;&#36182;&#39044;&#35757;&#32451;&#22270;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11483v1 Announce Type: cross  Abstract: Open-world semi-supervised learning (Open-world SSL) for node classification, that classifies unlabeled nodes into seen classes or multiple novel classes, is a practical but under-explored problem in the graph community. As only seen classes have human labels, they are usually better learned than novel classes, and thus exhibit smaller intra-class variances within the embedding space (named as imbalance of intra-class variances between seen and novel classes). Based on empirical and theoretical analysis, we find the variance imbalance can negatively impact the model performance. Pre-trained feature encoders can alleviate this issue via producing compact representations for novel classes. However, creating general pre-trained encoders for various types of graph data has been proven to be challenging. As such, there is a demand for an effective method that does not rely on pre-trained graph encoders. In this paper, we propose an IMbalanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39034;&#24207;&#37325;&#24314;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#35789;&#24207;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#33539;&#22260;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#20381;&#36182;&#20110;&#35789;&#24207;&#36827;&#34892;&#25512;&#26029;&#65292;&#20294;&#19981;&#33021;&#26126;&#30830;&#25903;&#25345;&#25110;&#21542;&#23450;&#35789;&#24207;&#19982;&#35789;&#27719;&#35821;&#20041;&#20043;&#38388;&#30340;&#20887;&#20313;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.11473</link><description>&lt;p&gt;
&#35789;&#24207;&#30340;&#24433;&#21709;&#65306;&#37325;&#26032;&#25490;&#24207;&#21644;&#29983;&#25104;&#20998;&#26512;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Word Order's Impacts: Insights from Reordering and Generation Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39034;&#24207;&#37325;&#24314;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#35789;&#24207;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#33539;&#22260;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#20381;&#36182;&#20110;&#35789;&#24207;&#36827;&#34892;&#25512;&#26029;&#65292;&#20294;&#19981;&#33021;&#26126;&#30830;&#25903;&#25345;&#25110;&#21542;&#23450;&#35789;&#24207;&#19982;&#35789;&#27719;&#35821;&#20041;&#20043;&#38388;&#30340;&#20887;&#20313;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#35789;&#35821;&#39034;&#24207;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#25171;&#20081;&#21407;&#22987;&#35789;&#24207;&#26469;&#21019;&#24314;&#19968;&#20010;&#28151;&#20081;&#30340;&#24207;&#21015;&#36827;&#34892;&#20998;&#26512;&#65292;&#28982;&#21518;&#27604;&#36739;&#27169;&#22411;&#22312;&#21407;&#22987;&#24207;&#21015;&#21644;&#28151;&#20081;&#24207;&#21015;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#19968;&#20123;&#23567;&#24133;&#19979;&#38477;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#20851;&#20110;&#35789;&#24207;&#30340;&#20551;&#35774;&#65292;&#21253;&#25324;&#8220;&#35789;&#24207;&#19982;&#35789;&#27719;&#35821;&#20041;&#37325;&#22797;&#8221;&#21644;&#8220;&#27169;&#22411;&#19981;&#20381;&#36182;&#20110;&#35789;&#24207;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#39034;&#24207;&#37325;&#24314;&#30340;&#35270;&#35282;&#65292;&#36873;&#25321;&#19981;&#21516;&#33539;&#22260;&#30340;&#25968;&#25454;&#38598;&#26469;&#37325;&#26032;&#23457;&#35270;&#19978;&#36848;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36873;&#25321;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#39034;&#24207;&#37325;&#24314;&#21644;&#36830;&#32493;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;ChatGPT&#20381;&#36182;&#20110;&#35789;&#24207;&#36827;&#34892;&#25512;&#26029;&#65292;&#20294;&#26080;&#27861;&#25903;&#25345;&#25110;&#21542;&#23450;&#35789;&#24207;&#19982;&#35789;&#27719;&#35821;&#20041;&#20043;&#38388;&#30340;&#20887;&#20313;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11473v1 Announce Type: cross  Abstract: Existing works have studied the impacts of the order of words within natural text. They usually analyze it by destroying the original order of words to create a scrambled sequence, and then comparing the models' performance between the original and scrambled sequences. The experimental results demonstrate marginal drops. Considering this findings, different hypothesis about word order is proposed, including ``the order of words is redundant with lexical semantics'', and ``models do not rely on word order''. In this paper, we revisit the aforementioned hypotheses by adding a order reconstruction perspective, and selecting datasets of different spectrum. Specifically, we first select four different datasets, and then design order reconstruction and continuing generation tasks. Empirical findings support that ChatGPT relies on word order to infer, but cannot support or negate the redundancy relations between word order lexical semantics.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Collage Prompting&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;GPT-4V&#21512;&#20316;&#30340;&#32463;&#27982;&#21487;&#34892;&#30340;&#35270;&#35273;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22270;&#20687;&#25490;&#21015;&#39034;&#24207;&#33719;&#24471;&#26368;&#22823;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11468</link><description>&lt;p&gt;
Collage Prompting: &#19982;GPT-4V&#21512;&#20316;&#30340;&#32463;&#27982;&#21487;&#34892;&#30340;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11468
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Collage Prompting&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;GPT-4V&#21512;&#20316;&#30340;&#32463;&#27982;&#21487;&#34892;&#30340;&#35270;&#35273;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22270;&#20687;&#25490;&#21015;&#39034;&#24207;&#33719;&#24471;&#26368;&#22823;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#36890;&#36807;&#37319;&#29992;&#35270;&#35273;&#25552;&#31034;&#65292;GPT-4V&#21487;&#20197;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24230;&#12290;&#23613;&#31649;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#19982;GPT-4V&#30340;&#25512;&#26029;&#30456;&#20851;&#30340;&#36130;&#21153;&#25104;&#26412;&#26500;&#25104;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;Collage Prompting&#65292;&#36825;&#26159;&#19968;&#31181;&#32463;&#27982;&#23454;&#24800;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#22270;&#20687;&#36830;&#25509;&#25104;&#21333;&#20010;&#35270;&#35273;&#36755;&#20837;&#12290;&#20511;&#21161;&#25340;&#36148;&#25552;&#31034;&#65292;GPT-4V&#21487;&#20197;&#21516;&#26102;&#22312;&#22810;&#24133;&#22270;&#20687;&#19978;&#25191;&#34892;&#22270;&#20687;&#35782;&#21035;&#12290;&#22522;&#20110;GPT-4V&#30340;&#22270;&#20687;&#35782;&#21035;&#20934;&#30830;&#24615;&#19982;&#25340;&#36148;&#25552;&#31034;&#20013;&#22270;&#20687;&#39034;&#24207;&#26126;&#26174;&#21464;&#21270;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#23398;&#20064;&#20248;&#21270;&#22270;&#20687;&#23433;&#25490;&#20197;&#33719;&#24471;&#26368;&#22823;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#35757;&#32451;&#20102;&#19968;&#20010;&#22270;&#39044;&#27979;&#22120;&#26469;&#25351;&#31034;&#27599;&#20010;&#25340;&#36148;&#25552;&#31034;&#30340;&#20934;&#30830;&#24615;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#26469;&#23548;&#33322;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11468v1 Announce Type: cross  Abstract: Recent advancements in generative AI have suggested that by taking visual prompt, GPT-4V can demonstrate significant proficiency in image recognition task. Despite its impressive capabilities, the financial cost associated with GPT-4V's inference presents a substantial barrier for its wide use. To address this challenge, our work introduces Collage Prompting, a budget-friendly prompting approach that concatenates multiple images into a single visual input. With collage prompt, GPT-4V is able to perform image recognition on several images simultaneously. Based on the observation that the accuracy of GPT-4V's image recognition varies significantly with the order of images within the collage prompt, our method further learns to optimize the arrangement of images for maximum recognition accuracy. A graph predictor is trained to indicate the accuracy of each collage prompt, then we propose an optimization method to navigate the search space
&lt;/p&gt;</description></item><item><title>HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11456</link><description>&lt;p&gt;
HateCOT&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27867;&#21270;&#25915;&#20987;&#24615;&#35328;&#35770;&#26816;&#27979;&#30340;&#35299;&#37322;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11456
&lt;/p&gt;
&lt;p&gt;
HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#21487;&#38752;&#39640;&#25928;&#26816;&#27979;&#30340;&#38656;&#27714;&#65292;&#20026;&#20102;&#38480;&#21046;&#20854;&#26377;&#23475;&#24433;&#21709;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#19982;&#26816;&#27979;&#25915;&#20987;&#24615;&#20869;&#23481;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HateCOT&#65292;&#36825;&#26159;&#20174;&#22810;&#26679;&#21270;&#29616;&#26377;&#26469;&#28304;&#20013;&#25277;&#21462;&#30340;5.2&#19975;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;GPT-3.5-Turbo&#21644;&#20154;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;HateCOT&#19978;&#20026;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#38646;-shot&#21644;few-shot&#35774;&#32622;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#28155;&#21152;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11432</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#30340;&#25581;&#31192;
&lt;/p&gt;
&lt;p&gt;
Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#28155;&#21152;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20986;&#29616;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#25968;&#37327;&#28608;&#22686;&#12290;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#24050;&#25104;&#20026;&#20854;&#20013;&#19968;&#39033;&#20027;&#35201;&#24212;&#29992;&#65292;&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#25110;&#39640;&#38454;&#36816;&#21160;&#23398;&#21464;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#31163;&#25955;&#36873;&#25321;&#25110;&#36830;&#32493;&#25511;&#21046;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;DRL&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#20102;&#22522;&#20110;&#36830;&#32493;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;DRL&#31639;&#27861;&#20316;&#20026;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#20998;&#26512;&#25216;&#26415;&#26469;&#35752;&#35770;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11432v1 Announce Type: cross  Abstract: With the advent of universal function approximators in the domain of reinforcement learning, the number of practical applications leveraging deep reinforcement learning (DRL) has exploded. Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output. However, the black-box nature of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs). Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework. We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV simulation environment. We provide some analytical techniques for discussing the interpretability of the trained mode
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26144;&#23556;&#26041;&#27861;&#65292;&#23558;&#24191;&#27867;&#31867;&#30340;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#65292;&#20174;&#32780;&#21487;&#20197;&#23545;Feynman&#36335;&#24452;&#31215;&#20998;&#20013;&#30340;&#20219;&#24847;&#36335;&#24452;&#36827;&#34892;&#32479;&#35745;&#27714;&#21644;&#12290;</title><link>https://arxiv.org/abs/2403.11420</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#37327;&#23376;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Neural network representation of quantum systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11420
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26144;&#23556;&#26041;&#27861;&#65292;&#23558;&#24191;&#27867;&#31867;&#30340;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#65292;&#20174;&#32780;&#21487;&#20197;&#23545;Feynman&#36335;&#24452;&#31215;&#20998;&#20013;&#30340;&#20219;&#24847;&#36335;&#24452;&#36827;&#34892;&#32479;&#35745;&#27714;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#65292;&#25509;&#36817;&#39640;&#26031;&#36807;&#31243;&#30340;&#38543;&#26426;&#23485;&#31070;&#32463;&#32593;&#32476;&#26159;&#22260;&#32469;&#39640;&#26031;&#22266;&#23450;&#28857;&#30340;&#37327;&#23376;&#22330;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26144;&#23556;&#65292;&#36890;&#36807;&#35813;&#26144;&#23556;&#65292;&#21487;&#20197;&#23558;&#19968;&#22823;&#31867;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#34920;&#36798;&#20026;&#20855;&#26377;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#32479;&#35745;&#27714;&#21644;&#24418;&#24335;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;&#24605;&#24819;&#26159;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#29983;&#25104;&#36153;&#26364;&#36335;&#24452;&#31215;&#20998;&#20013;&#30340;&#20219;&#24847;&#36335;&#24452;&#12290;&#36825;&#31181;&#26144;&#23556;&#21487;&#20197;&#24212;&#29992;&#20110;&#30456;&#20114;&#20316;&#29992;&#30340;&#37327;&#23376;&#31995;&#32479;/&#22330;&#35770;&#65292;&#21363;&#20351;&#36828;&#31163;&#39640;&#26031;&#26497;&#38480;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20351;&#26426;&#22120;&#23398;&#20064;&#19982;&#37327;&#23376;&#19990;&#30028;&#26356;&#21152;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11420v1 Announce Type: cross  Abstract: It has been proposed that random wide neural networks near Gaussian process are quantum field theories around Gaussian fixed points. In this paper, we provide a novel map with which a wide class of quantum mechanical systems can be cast into the form of a neural network with a statistical summation over network parameters. Our simple idea is to use the universal approximation theorem of neural networks to generate arbitrary paths in the Feynman's path integral. The map can be applied to interacting quantum systems / field theories, even away from the Gaussian limit. Our findings bring machine learning closer to the quantum world.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26174;&#24335;&#21442;&#25968;&#21270;&#36807;&#28193;&#20989;&#25968;&#26469;&#23398;&#20064;&#36712;&#36857;&#20998;&#24067;&#30340;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#36712;&#36857;&#30340;&#39640;&#25928;&#21512;&#25104;&#21644;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.11418</link><description>&lt;p&gt;
&#26102;&#38388;&#36712;&#36857;&#30340;&#21464;&#20998;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Variational Sampling of Temporal Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26174;&#24335;&#21442;&#25968;&#21270;&#36807;&#28193;&#20989;&#25968;&#26469;&#23398;&#20064;&#36712;&#36857;&#20998;&#24067;&#30340;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#36712;&#36857;&#30340;&#39640;&#25928;&#21512;&#25104;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#30830;&#23450;&#24615;&#30340;&#26102;&#38388;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#20854;&#36712;&#36857;&#26469;&#30830;&#23450;&#65292;&#35813;&#36712;&#36857;&#26159;(a)&#21021;&#22987;&#26465;&#20214;$z_0 \in \mathcal{Z}$&#21644;(b)&#36807;&#28193;&#20989;&#25968;$f:(\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$&#30340;&#20056;&#31215;&#31354;&#38388;&#20013;&#30340;&#20803;&#32032;&#65292;&#24448;&#24448;&#21463;&#24213;&#23618;&#21160;&#21147;&#31995;&#32479;&#25511;&#21046;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#36807;&#28193;&#20989;&#25968;&#24314;&#27169;&#20026;&#24494;&#20998;&#26041;&#31243;&#25110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#39044;&#27979;&#26410;&#26469;&#27979;&#37327;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#20294;&#24456;&#23569;&#26377;&#32467;&#26524;&#25104;&#21151;&#24314;&#31435;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36712;&#36857;&#37319;&#26679;&#21644;&#32479;&#35745;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#21442;&#25968;&#21270;&#26041;&#38754;&#30340;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#36807;&#28193;&#20989;&#25968;$f$&#26174;&#24335;&#22320;&#21442;&#25968;&#21270;&#20026;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#20803;&#32032;&#26469;&#23398;&#20064;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#26377;&#25928;&#22320;&#21512;&#25104;&#26032;&#39062;&#30340;&#36712;&#36857;&#65292;&#21516;&#26102;&#36824;&#30452;&#25509;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#20415;&#30340;&#24037;&#20855;&#26469;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11418v1 Announce Type: cross  Abstract: A deterministic temporal process can be determined by its trajectory, an element in the product space of (a) initial condition $z_0 \in \mathcal{Z}$ and (b) transition function $f: (\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$ often influenced by the control of the underlying dynamical system. Existing methods often model the transition function as a differential equation or as a recurrent neural network. Despite their effectiveness in predicting future measurements, few results have successfully established a method for sampling and statistical inference of trajectories using neural networks, partially due to constraints in the parameterization. In this work, we introduce a mechanism to learn the distribution of trajectories by parameterizing the transition function $f$ explicitly as an element in a function space. Our framework allows efficient synthesis of novel trajectories, while also directly providing a convenient tool for inferen
&lt;/p&gt;</description></item><item><title>DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11415</link><description>&lt;p&gt;
DreamSampler&#65306;&#32479;&#19968;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#20197;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11415
&lt;/p&gt;
&lt;p&gt;
DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#24050;&#25104;&#20026;&#26368;&#36817;&#20960;&#24180;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LDM&#26550;&#26500;&#25110;&#29305;&#24449;&#24037;&#31243;&#65292;&#20998;&#25968;&#33976;&#39311;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20294;&#24448;&#24448;&#23481;&#26131;&#21457;&#29983;&#27169;&#24335;&#23849;&#28291;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#24182;&#21033;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;DreamSampler&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#28508;&#22312;&#20248;&#21270;&#30340;&#35270;&#35282;&#26080;&#32541;&#22320;&#25972;&#21512;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#20998;&#25968;&#33976;&#39311;&#65292;DreamSampler&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;LDM&#26550;&#26500;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20294;&#23427;&#20801;&#35768;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#36827;&#34892;&#33976;&#39311;&#21644;&#21453;&#21521;&#37319;&#26679;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#28041;&#21450;&#22270;&#20687;&#32534;&#36753;&#12289;SVG&#37325;&#26500;&#31561;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11415v1 Announce Type: cross  Abstract: Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent diffusion models (LDMs). While reverse diffusion sampling often requires adjustments of LDM architecture or feature engineering, score distillation offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing. To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization. Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both distillation and reverse sampling with additional guidance for image editing and reconstruction. Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive pe
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;AI&#25216;&#26415;&#22914;ChatGPT&#23545;&#32593;&#32476;&#23433;&#20840;&#25945;&#32946;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#30740;&#31350;&#24378;&#35843;&#20102;&#22823;&#23398;&#24212;&#23545;&#35838;&#31243;&#36827;&#34892;&#35843;&#25972;&#20197;&#36866;&#24212;&#34892;&#19994;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.11402</link><description>&lt;p&gt;
&#25317;&#25265;&#29983;&#25104;AI&#38761;&#21629;&#65306;&#29992;GPT&#25512;&#36827;&#32593;&#32476;&#23433;&#20840;&#30340;&#39640;&#31561;&#25945;&#32946;
&lt;/p&gt;
&lt;p&gt;
Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11402
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#25216;&#26415;&#22914;ChatGPT&#23545;&#32593;&#32476;&#23433;&#20840;&#25945;&#32946;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#30740;&#31350;&#24378;&#35843;&#20102;&#22823;&#23398;&#24212;&#23545;&#35838;&#31243;&#36827;&#34892;&#35843;&#25972;&#20197;&#36866;&#24212;&#34892;&#19994;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22914;ChatGPT&#31561;Generative Pre-trained Transformer&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#23558;&#23545;&#32593;&#32476;&#23433;&#20840;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65292;&#29305;&#21035;&#26159;ChatGPT&#22312;&#32593;&#32476;&#23433;&#20840;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#23601;&#22823;&#23398;&#24212;&#22914;&#20309;&#35843;&#25972;&#35838;&#31243;&#20869;&#23481;&#20197;&#28385;&#36275;&#34892;&#19994;&#19981;&#26029;&#21457;&#23637;&#30340;&#38656;&#27714;&#25552;&#20379;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;GPT&#30340;&#8220;&#24515;&#26234;&#27169;&#22411;&#8221;&#19982;&#20154;&#31867;&#35748;&#30693;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#25552;&#39640;GPT&#33021;&#21147;&#20197;&#31526;&#21512;&#24067;&#40065;&#22982;&#20998;&#31867;&#27861;&#20013;&#30340;&#20154;&#31867;&#25216;&#33021;&#12290;&#36890;&#36807;&#20998;&#26512;&#24403;&#21069;&#30340;&#25945;&#32946;&#23454;&#36341;&#20197;&#21450;&#35838;&#31243;&#20869;&#23481;&#19982;&#34892;&#19994;&#38656;&#27714;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#25552;&#20379;&#32593;&#32476;&#23433;&#20840;&#31561;&#23454;&#29992;&#23398;&#20301;&#30340;&#22823;&#23398;&#24212;&#19982;&#34892;&#19994;&#38656;&#27714;&#23494;&#20999;&#23545;&#25509;&#65292;&#25317;&#25265;&#19981;&#21487;&#36991;&#20813;&#30340;&#29983;&#25104;AI&#38761;&#21629;&#65292;&#24182;&#24212;&#29992;strik
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11402v1 Announce Type: cross  Abstract: The rapid advancement of generative Artificial Intelligence (AI) technologies, particularly Generative Pre-trained Transformer (GPT) models such as ChatGPT, has the potential to significantly impact cybersecurity. In this study, we investigated the impact of GPTs, specifically ChatGPT, on tertiary education in cybersecurity, and provided recommendations for universities to adapt their curricula to meet the evolving needs of the industry. Our research highlighted the importance of understanding the alignment between GPT's ``mental model'' and human cognition, as well as the enhancement of GPT capabilities to human skills based on Bloom's taxonomy. By analyzing current educational practices and the alignment of curricula with industry requirements, we concluded that universities providing practical degrees like cybersecurity should align closely with industry demand and embrace the inevitable generative AI revolution, while applying stri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Scene-LLM&#65292;&#19968;&#31181;3D&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#22312;&#20132;&#20114;&#24335;3D&#23460;&#20869;&#29615;&#22659;&#20013;&#20855;&#36523;&#20307;&#23384;&#22312;&#30340;&#20195;&#29702;&#32773;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11401</link><description>&lt;p&gt;
Scene-LLM&#65306;&#25193;&#23637;&#29992;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Scene-LLM&#65292;&#19968;&#31181;3D&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#22312;&#20132;&#20114;&#24335;3D&#23460;&#20869;&#29615;&#22659;&#20013;&#20855;&#36523;&#20307;&#23384;&#22312;&#30340;&#20195;&#29702;&#32773;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Scene-LLM&#65292;&#36825;&#26159;&#19968;&#31181;3D&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#22312;&#20132;&#20114;&#24335;3D&#23460;&#20869;&#29615;&#22659;&#20013;&#20855;&#36523;&#20307;&#23384;&#22312;&#30340;&#20195;&#29702;&#32773;&#33021;&#21147;&#12290;Scene-LLM&#37319;&#29992;&#20102;&#28151;&#21512;&#30340;3D&#35270;&#35273;&#29305;&#24449;&#34920;&#31034;&#65292;&#21253;&#25324;&#20102;&#23494;&#38598;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#25903;&#25345;&#22330;&#26223;&#29366;&#24577;&#30340;&#26356;&#26032;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#25237;&#24433;&#23618;&#23558;&#36825;&#20123;&#29305;&#24449;&#39640;&#25928;&#22320;&#25237;&#24433;&#21040;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#37322;3D&#35270;&#35273;&#20449;&#24687;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#25972;&#21512;&#20102;&#22330;&#26223;&#32423;&#21644;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;3D&#20449;&#24687;&#12290;&#36825;&#31181;&#32452;&#21512;&#23545;&#20110;&#20132;&#20114;&#24335;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#22330;&#26223;&#32423;&#25968;&#25454;&#25903;&#25345;&#20840;&#23616;&#35268;&#21010;&#65292;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#23545;&#20110;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;3D&#24103;&#29305;&#24449;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#23545;&#23567;&#29289;&#20307;&#29305;&#24449;&#23545;&#40784;&#33021;&#21147;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11401v1 Announce Type: cross  Abstract: This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a hybrid 3D visual feature representation, that incorporates dense spatial information and supports scene state updates. The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space, enabling effective interpretation of 3D visual information. Unique to our approach is the integration of both scene-level and ego-centric 3D information. This combination is pivotal for interactive planning, where scene-level data supports global planning and ego-centric data is important for localization. Notably, we use ego-centric 3D frame features for feature alignment, an efficient technique that enhances the model's ability to align features of small objects within the s
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#20852;&#36215;&#39537;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#25968;&#25454;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11395</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automated data processing and feature engineering for deep learning and big data applications: a survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11395
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#20852;&#36215;&#39537;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#25968;&#25454;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#24182;&#22312;AI&#30340;&#21457;&#23637;&#20013;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#29305;&#21035;&#26159;&#22312;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#12290;&#23427;&#20063;&#31616;&#21270;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#22240;&#20026;&#23398;&#20064;&#36807;&#31243;&#26159;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#37117;&#24050;&#33258;&#21160;&#21270;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#24517;&#39035;&#22312;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20043;&#21069;&#32463;&#36807;&#25163;&#21160;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#36827;&#19968;&#27493;&#25193;&#23637;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#29992;&#20110;&#33258;&#21160;&#21270;&#36825;&#20123;&#20219;&#21153;&#30340;&#29305;&#27530;&#25216;&#26415;&#12290;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#39537;&#21160;&#21147;&#26159;&#21033;&#29992;&#22823;&#37327;&#22797;&#26434;&#12289;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#12290;&#22914;&#20170;&#65292;&#22522;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;A
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11395v1 Announce Type: cross  Abstract: Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (A
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#22312;&#21512;&#20316;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#26174;&#31034;&#20986;&#21512;&#20316;&#20542;&#21521;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#21512;&#20316;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#20581;&#22766;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.11381</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#21512;&#20316;&#21527;&#65311;&#36890;&#36807;&#34701;&#21512;&#30406;&#35780;&#20272;&#23427;&#20204;&#30340;&#21512;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#22312;&#21512;&#20316;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#26174;&#31034;&#20986;&#21512;&#20316;&#20542;&#21521;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#21512;&#20316;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#20581;&#22766;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#32500;&#24230;&#26159;&#21457;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#28508;&#21147;&#22686;&#24378;&#22810;&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#33879;&#21517;&#30340;Meltin Pot&#29615;&#22659;&#20197;&#21450;&#21442;&#32771;&#27169;&#22411;&#22914;GPT4&#21644;GPT3.5&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#33258;&#20027;&#20195;&#29702;(LAAs)&#30340;&#21512;&#20316;&#33021;&#21147;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#21512;&#20316;&#30340;&#20542;&#21521;&#65292;&#20294;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#24378;&#35843;&#20102;&#26356;&#24378;&#22823;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#36866;&#24212;LLM&#30340;Melting Pot&#28216;&#25103;&#22330;&#26223;&#30340;&#25277;&#35937;&#21270;&#23618;&#65292;&#19968;&#20010;&#29992;&#20110;LLM&#20013;&#20171;&#20195;&#29702;&#24320;&#21457;&#30340;&#21487;&#37325;&#29992;&#26550;&#26500;-&#21253;&#25324;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20197;&#21450;&#19981;&#21516;&#30340;&#35748;&#30693;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#26041;&#27861;&#35780;&#20272;&#21512;&#20316;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11381v1 Announce Type: new  Abstract: As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#28436;&#31034;&#21644;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23545;&#40784;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#39550;&#39542;&#20195;&#29702;&#19982;&#20154;&#31867;&#39550;&#39542;&#39118;&#26684;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.11368</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#39550;&#39542;&#21592;&#20195;&#29702;&#30340;&#39550;&#39542;&#39118;&#26684;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Driving Style Alignment for LLM-powered Driver Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11368
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#28436;&#31034;&#21644;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23545;&#40784;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#39550;&#39542;&#20195;&#29702;&#19982;&#20154;&#31867;&#39550;&#39542;&#39118;&#26684;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;LLM&#25216;&#26415;&#39537;&#21160;&#30340;&#39550;&#39542;&#20195;&#29702;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20351;&#39550;&#39542;&#20195;&#29702;&#34892;&#20026;&#19982;&#20154;&#31867;&#39550;&#39542;&#39118;&#26684;&#23545;&#40784;&#30340;&#24403;&#21069;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#32570;&#20047;&#26469;&#33258;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#30340;&#39640;&#36136;&#37327;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23545;&#40784;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#28436;&#31034;&#21644;&#21453;&#39304;&#23558;&#39550;&#39542;&#20195;&#29702;&#19982;&#20154;&#31867;&#39550;&#39542;&#39118;&#26684;&#23545;&#40784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#39550;&#39542;&#23454;&#39564;&#21644;&#39550;&#39542;&#21518;&#35775;&#35848;&#26500;&#24314;&#20102;&#19968;&#20010;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20026;LLM&#30340;&#23545;&#40784;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#28436;&#31034;&#12290;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#22312;CARLA&#22478;&#24066;&#20132;&#36890;&#27169;&#25311;&#22120;&#20013;&#30340;&#20223;&#30495;&#23454;&#39564;&#24471;&#20197;&#39564;&#35777;&#65292;&#24182;&#24471;&#21040;&#20102;&#20154;&#31867;&#35780;&#20272;&#30340;&#36827;&#19968;&#27493;&#35777;&#23454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#21487;&#20197;&#25351;&#23548;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11368v1 Announce Type: cross  Abstract: Recently, LLM-powered driver agents have demonstrated considerable potential in the field of autonomous driving, showcasing human-like reasoning and decision-making abilities.However, current research on aligning driver agent behaviors with human driving styles remains limited, partly due to the scarcity of high-quality natural language data from human driving behaviors.To address this research gap, we propose a multi-alignment framework designed to align driver agents with human driving styles through demonstrations and feedback. Notably, we construct a natural language dataset of human driver behaviors through naturalistic driving experiments and post-driving interviews, offering high-quality human demonstrations for LLM alignment. The framework's effectiveness is validated through simulation experiments in the CARLA urban traffic simulator and further corroborated by human evaluations. Our research offers valuable insights into desi
&lt;/p&gt;</description></item><item><title>IGANN Sparse&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#36873;&#25321;&#20419;&#36827;&#31232;&#30095;&#24615;&#65292;&#30830;&#20445;&#22312;&#19981;&#24433;&#21709;&#39044;&#27979;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11363</link><description>&lt;p&gt;
IGANN Sparse: &#29992;&#38750;&#32447;&#24615;&#27934;&#23519;&#21147;&#36830;&#25509;&#31232;&#30095;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear Insight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11363
&lt;/p&gt;
&lt;p&gt;
IGANN Sparse&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#36873;&#25321;&#20419;&#36827;&#31232;&#30095;&#24615;&#65292;&#30830;&#20445;&#22312;&#19981;&#24433;&#21709;&#39044;&#27979;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#39044;&#27979;&#24615;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;IGANN Sparse&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#26469;&#33258;&#24191;&#20041;&#21152;&#27861;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#36873;&#25321;&#20419;&#36827;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#30830;&#20445;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#31232;&#30095;&#24615;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#32780;&#19981;&#24433;&#21709;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11363v1 Announce Type: cross  Abstract: Feature selection is a critical component in predictive analytics that significantly affects the prediction accuracy and interpretability of models. Intrinsic methods for feature selection are built directly into model learning, providing a fast and attractive option for large amounts of data. Machine learning algorithms, such as penalized regression models (e.g., lasso) are the most common choice when it comes to in-built feature selection. However, they fail to capture non-linear relationships, which ultimately affects their ability to predict outcomes in intricate datasets. In this paper, we propose IGANN Sparse, a novel machine learning model from the family of generalized additive models, which promotes sparsity through a non-linear feature selection process during training. This ensures interpretability through improved model sparsity without sacrificing predictive performance. Moreover, IGANN Sparse serves as an exploratory tool
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.11353</link><description>&lt;p&gt;
&#28342;&#21058;&#24863;&#30693;&#30340;2D&#26680;&#30913;&#20849;&#25391;&#39044;&#27979;&#65306;&#21033;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#36845;&#20195;&#33258;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#20998;&#23376;&#30340;&#32467;&#26500;&#20449;&#24687;&#12289;&#30005;&#23376;&#24615;&#36136;&#21644;&#21160;&#24577;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20934;&#30830;&#30340;NMR&#20809;&#35889;&#39044;&#27979;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20505;&#36873;&#20998;&#23376;&#65292;&#20351;&#21270;&#23398;&#23478;&#33021;&#22815;&#23558;&#23427;&#20204;&#19982;&#23454;&#38469;&#23454;&#39564;&#20809;&#35889;&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#35748;&#20998;&#23376;&#32467;&#26500;&#25110;&#25351;&#20986;&#24046;&#24322;&#65292;&#24341;&#23548;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20998;&#23376;&#32467;&#26500;&#39044;&#27979;&#20998;&#23376;&#30340;&#21407;&#23376;NMR&#21270;&#23398;&#20301;&#31227;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#19968;&#32500;&#65288;1D&#65289;NMR&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20108;&#32500;&#65288;2D&#65289;NMR&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#27880;&#30340;NMR&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33258;&#35757;&#32451;&#65288;IST&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21407;&#23376;2DNMR&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#65292;&#25552;&#20986;&#20102;COLEP&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#23454;&#40065;&#26834;&#23398;&#20064;&#25512;&#29702;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#20854;&#29305;&#28857;&#22312;&#20110;&#35757;&#32451;&#32479;&#35745;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#27010;&#29575;&#30005;&#36335;&#23454;&#29616;&#31934;&#30830;&#39640;&#25928;&#25512;&#29702;</title><link>https://arxiv.org/abs/2403.11348</link><description>&lt;p&gt;
COLEP: &#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23454;&#29616;&#21487;&#35777;&#23454;&#40065;&#26834;&#23398;&#20064;&#25512;&#29702;&#19968;&#33268;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#65292;&#25552;&#20986;&#20102;COLEP&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#23454;&#40065;&#26834;&#23398;&#20064;&#25512;&#29702;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#20854;&#29305;&#28857;&#22312;&#20110;&#35757;&#32451;&#32479;&#35745;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#27010;&#29575;&#30005;&#36335;&#23454;&#29616;&#31934;&#30830;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#19968;&#33268;&#24615;&#39044;&#27979;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20026;&#20219;&#24847;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#32479;&#35745;&#20005;&#35880;&#30340;&#39044;&#27979;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20551;&#35774;&#25968;&#25454;&#26159;&#21487;&#20132;&#25442;&#30340;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#24494;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#20063;&#21487;&#33021;&#36829;&#21453;&#21487;&#20132;&#25442;&#24615;&#20551;&#35774;&#65292;&#25361;&#25112;&#35206;&#30422;&#29575;&#20445;&#35777;&#65292;&#24182;&#23548;&#33268;&#21518;&#32493;&#23454;&#35777;&#35206;&#30422;&#29575;&#30340;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23454;&#29616;&#21487;&#35777;&#23454;&#40065;&#26834;&#23398;&#20064;&#25512;&#29702;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#26694;&#26550;&#65288;COLEP&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#32452;&#20214;&#65292;&#29992;&#20110;&#35757;&#32451;&#32479;&#35745;&#27169;&#22411;&#20197;&#23398;&#20064;&#19981;&#21516;&#30340;&#35821;&#20041;&#27010;&#24565;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#32534;&#30721;&#30693;&#35782;&#24182;&#34920;&#24449;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#25512;&#29702;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#22312;&#25512;&#29702;&#32452;&#20214;&#20869;&#37096;&#20351;&#29992;&#20102;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#39044;&#27979;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11348v1 Announce Type: cross  Abstract: Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary black-box machine learning models, assuming the data is exchangeable. However, even small adversarial perturbations during the inference can violate the exchangeability assumption, challenge the coverage guarantees, and result in a subsequent decline in empirical coverage. In this work, we propose a certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits, which comprise a data-driven learning component that trains statistical models to learn different semantic concepts, and a reasoning component that encodes knowledge and characterizes the relationships among the trained models for logic reasoning. To achieve exact and efficient reasoning, we employ probabilistic circuits (PCs) within the reasoning component. Theoretically, we provide end-to-end certification of predict
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;</title><link>https://arxiv.org/abs/2403.11346</link><description>&lt;p&gt;
CantonMT: &#27721;&#33521;NMT&#24179;&#21488;&#65292;&#20351;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 &#28040;&#24687;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#21453;&#21521;&#32763;&#35793;&#65292;&#24212;&#29992;&#21040;&#20102;&#26032;&#30340;&#35821;&#35328;&#32763;&#35793;&#26041;&#21521;&#31908;&#35821;&#33267;&#33521;&#35821;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;(&#21253;&#25324;OpusMT, NLLB,&#21644;mBART)&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#25351;&#26631;&#21253;&#25324;&#22522;&#20110;&#35789;&#27719;&#21644;&#23884;&#20837;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#39033;\textsc{CantonMT}&#30740;&#31350;&#39033;&#30446;&#20013;&#21253;&#21547;&#30340;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#23454;&#29616;&#31908;&#35821;&#33267;&#33521;&#35821;MT&#30740;&#31350;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#24320;&#28304;\textsc{CantonMT}&#24037;&#20855;&#21253;\url{https://github.com/kenrickkung/CantoneseTranslation}&#21521;&#24179;&#21488;&#28155;&#21152;&#26356;&#22810;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11345</link><description>&lt;p&gt;
&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#20316;&#31454;&#20105;Agent&#65306;&#22343;&#22330;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#25104;&#22242;&#38431;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#27599;&#20010;&#22242;&#38431;&#20869;&#37096;&#23384;&#22312;&#21512;&#20316;&#65292;&#20294;&#19981;&#21516;&#22242;&#38431;&#20043;&#38388;&#23384;&#22312;&#38750;&#38646;&#21644;&#30340;&#31454;&#20105;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#31181;&#21487;&#20197;&#26126;&#30830;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30001;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24341;&#36215;&#30340;&#38750;&#31283;&#24577;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#27599;&#20010;&#22242;&#38431;&#20869;&#20195;&#29702;&#25968;&#37327;&#26080;&#38480;&#30340;&#24773;&#20917;&#65292;&#21363;&#22343;&#22330;&#35774;&#32622;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#30340;LQ&#22343;&#22330;&#31867;&#22411;&#21338;&#24328;&#65288;GS-MFTGs&#65289;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#36870;&#21487;&#36870;&#26465;&#20214;&#19979;&#34920;&#24449;&#20102;GS-MFTG&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#12290;&#28982;&#21518;&#35777;&#26126;&#20102;&#36825;&#20010;MFTG NE&#22312;&#26377;&#38480;&#20154;&#21475;&#21338;&#24328;&#20013;&#20026;$\mathcal{O}(1/M)$-NE&#65292;&#20854;&#20013;$M$&#26159;&#27599;&#20010;&#22242;&#38431;&#20013;&#20195;&#29702;&#25968;&#37327;&#30340;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26500;&#24615;&#32467;&#26524;&#25512;&#21160;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#29609;&#23478;&#36882;&#36827;&#24335;&#33258;&#28982;Pol&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11345v1 Announce Type: cross  Abstract: We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Pol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#28857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20851;&#38190;&#28857;&#39044;&#27979;&#21644;&#35270;&#39057;&#24103;&#21512;&#25104;&#32467;&#21512;&#24212;&#29992;&#22312;&#35270;&#39057;&#36816;&#21160;&#20256;&#36755;&#20013;&#65292;&#25552;&#21319;&#20102;&#24102;&#23485;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.11337</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#28857;&#39044;&#27979;&#25552;&#21319;&#35270;&#39057;&#36816;&#21160;&#20256;&#36755;&#24212;&#29992;&#30340;&#24102;&#23485;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Enhancing Bandwidth Efficiency for Video Motion Transfer Applications using Deep Learning Based Keypoint Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#28857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20851;&#38190;&#28857;&#39044;&#27979;&#21644;&#35270;&#39057;&#24103;&#21512;&#25104;&#32467;&#21512;&#24212;&#29992;&#22312;&#35270;&#39057;&#36816;&#21160;&#20256;&#36755;&#20013;&#65292;&#25552;&#21319;&#20102;&#24102;&#23485;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#36816;&#21160;&#36716;&#31227;&#35270;&#39057;&#24212;&#29992;&#65288;&#22914;&#35270;&#39057;&#20250;&#35758;&#12289;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#21644;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#38544;&#31169;&#20445;&#25252;&#65289;&#20013;&#30340;&#24102;&#23485;&#20943;&#23569;&#12290;&#20026;&#20102;&#24314;&#27169;&#22797;&#26434;&#36816;&#21160;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#34920;&#21160;&#24577;&#23545;&#35937;&#30340;&#31532;&#19968;&#38454;&#21160;&#20316;&#27169;&#22411;&#65288;FOMM&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#20851;&#38190;&#28857;&#21450;&#20854;&#23616;&#37096;&#20223;&#23556;&#21464;&#25442;&#12290;&#20851;&#38190;&#28857;&#30001;&#33258;&#30417;&#30563;&#20851;&#38190;&#28857;&#26816;&#27979;&#22120;&#25552;&#21462;&#65292;&#24182;&#25353;&#29031;&#35270;&#39057;&#24103;&#23545;&#24212;&#30340;&#26102;&#38388;&#24207;&#21015;&#32452;&#32455;&#12290;&#20851;&#38190;&#28857;&#30340;&#39044;&#27979;&#26159;&#36890;&#36807;&#21464;&#20998;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;VRNN&#65289;&#36827;&#34892;&#30340;&#65292;&#20197;&#20415;&#22312;&#28304;&#35774;&#22791;&#19978;&#20351;&#29992;&#26356;&#20302;&#30340;&#24103;&#29575;&#36827;&#34892;&#20256;&#36755;&#12290;&#28982;&#21518;&#21033;&#29992;&#20809;&#27969;&#20272;&#35745;&#22120;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#65292;&#23558;&#39044;&#27979;&#30340;&#20851;&#38190;&#28857;&#21512;&#25104;&#20026;&#35270;&#39057;&#24103;&#12290;&#22312;&#35270;&#39057;&#21160;&#30011;&#20256;&#36755;&#20013;&#65292;&#21033;&#29992;&#20851;&#38190;&#28857;&#34920;&#31034;&#21644;&#22522;&#20110;VRNN&#30340;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11337v1 Announce Type: cross  Abstract: We propose a deep learning based novel prediction framework for enhanced bandwidth reduction in motion transfer enabled video applications such as video conferencing, virtual reality gaming and privacy preservation for patient health monitoring. To model complex motion, we use the First Order Motion Model (FOMM) that represents dynamic objects using learned keypoints along with their local affine transformations. Keypoints are extracted by a self-supervised keypoint detector and organized in a time series corresponding to the video frames. Prediction of keypoints, to enable transmission using lower frames per second on the source device, is performed using a Variational Recurrent Neural Network (VRNN). The predicted keypoints are then synthesized to video frames using an optical flow estimator and a generator network. This efficacy of leveraging keypoint based representations in conjunction with VRNN based prediction for both video ani
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.11330</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#19968;&#20010;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#26469;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11330
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#65288;&#21363;&#65292;&#23545;&#35805;&#32423;&#65289;&#22870;&#21169;&#23545;&#40784;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#33258;&#28982;&#21457;&#29983;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#21517;&#20026;GELI&#65289;&#36890;&#36807;&#23558;&#20154;&#31867;&#25552;&#20379;&#30340;&#20840;&#23616;&#26126;&#30830;&#65288;GE&#65289;&#20250;&#35805;&#32423;&#22870;&#21169;&#25286;&#20998;&#65292;&#21033;&#29992;&#26412;&#22320;&#38544;&#24335;&#65288;LI&#65289;&#22810;&#27169;&#24577;&#22870;&#21169;&#20449;&#21495;&#26469;&#36328;&#27169;&#24577;&#22320;&#22609;&#36896;&#22870;&#21169;&#20998;&#35299;&#27493;&#39588;&#12290;&#28982;&#21518;&#23558;&#36825;&#31181;&#20998;&#35299;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#26631;&#20934;RHLF&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#26469;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;GELI&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#24341;&#23548;&#36974;&#32617;&#31574;&#30053;d-MAE&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#36816;&#21160;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#29699;&#21592;&#35782;&#21035;&#30340;&#31283;&#20581;&#29305;&#24449;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2403.11328</link><description>&lt;p&gt;
&#39046;&#22495;&#24341;&#23548;&#30340;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#29420;&#29305;&#29699;&#21592;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Domain-Guided Masked Autoencoders for Unique Player Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11328
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#24341;&#23548;&#36974;&#32617;&#31574;&#30053;d-MAE&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#36816;&#21160;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#29699;&#21592;&#35782;&#21035;&#30340;&#31283;&#20581;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#29305;&#29699;&#21592;&#35782;&#21035;&#26159;&#35270;&#35273;&#39537;&#21160;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#27169;&#22359;&#12290;&#20174;&#24191;&#25773;&#35270;&#39057;&#20013;&#35782;&#21035;&#29699;&#21592;&#21487;&#20197;&#24110;&#21161;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#29699;&#21592;&#35780;&#20272;&#12289;&#27604;&#36187;&#20998;&#26512;&#21644;&#36716;&#25773;&#21046;&#20316;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#28145;&#24230;&#29305;&#24449;&#33258;&#21160;&#26816;&#27979;&#29699;&#34915;&#21495;&#30721;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#21407;&#22240;&#21253;&#25324;&#65306;a&#65289;&#36816;&#21160;&#27169;&#31946;&#65292;b&#65289;&#20302;&#20998;&#36776;&#29575;&#35270;&#39057;&#20449;&#21495;&#65292;&#21644;c&#65289;&#36974;&#25377;&#12290;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#20197;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#32780;&#33073;&#39062;&#32780;&#20986;&#65292;&#25104;&#20026;&#20256;&#32479;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#20248;&#36234;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;MAEs&#21482;&#26159;&#31616;&#21333;&#22320;&#23558;&#22270;&#20687;&#34917;&#19969;&#32622;&#38646;&#65292;&#35201;&#20040;&#38543;&#26426;&#35201;&#20040;&#20851;&#27880;&#20110;&#22312;&#21738;&#37324;&#36974;&#32617;&#65292;&#32780;&#19981;&#26159;&#22914;&#20309;&#36974;&#32617;&#12290;&#21463;&#20154;&#31867;&#35270;&#35273;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#24341;&#23548;&#36974;&#32617;&#31574;&#30053;&#65292;&#29992;&#20110;&#20419;&#36827;d-MAE&#20197;&#25552;&#39640;&#22312;&#36816;&#21160;&#27169;&#31946;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#29699;&#21592;&#35782;&#21035;&#30340;&#31283;&#20581;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11328v1 Announce Type: cross  Abstract: Unique player identification is a fundamental module in vision-driven sports analytics. Identifying players from broadcast videos can aid with various downstream tasks such as player assessment, in-game analysis, and broadcast production. However, automatic detection of jersey numbers using deep features is challenging primarily due to: a) motion blur, b) low resolution video feed, and c) occlusions. With their recent success in various vision tasks, masked autoencoders (MAEs) have emerged as a superior alternative to conventional feature extractors. However, most MAEs simply zero-out image patches either randomly or focus on where to mask rather than how to mask. Motivated by human vision, we devise a novel domain-guided masking policy for MAEs termed d-MAE to facilitate robust feature extraction in the presence of motion blur for player identification. We further introduce a new spatio-temporal network leveraging our novel d-MAE for 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11322</link><description>&lt;p&gt;
&#20351;&#29992;StateFlow&#22686;&#24378;LLM&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#36890;&#36807;&#29366;&#24577;&#39537;&#21160;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#65292;&#20363;&#22914;&#38656;&#35201;&#19968;&#31995;&#21015;&#25805;&#20316;&#21644;&#19982;&#24037;&#20855;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;StateFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#27714;&#35299;&#33539;&#24335;&#65292;&#23558;&#30001;LLM&#25903;&#25345;&#30340;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#12290;&#36890;&#36807;&#27491;&#30830;&#26500;&#24314;&#29366;&#24577;&#21644;&#23450;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;StateFlow&#30830;&#23450;&#20102;&#20219;&#21153;&#27714;&#35299;&#30340;&#36827;&#23637;&#65292;&#30830;&#20445;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;LLM&#22312;&#25972;&#20010;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#30340;&#21709;&#24212;&#12290;&#22312;&#27599;&#20010;&#29366;&#24577;&#20013;&#65292;StateFlow&#20801;&#35768;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#19981;&#20165;&#21253;&#25324;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#25351;&#23548;&#29983;&#25104;LLM&#21709;&#24212;&#65292;&#36824;&#21253;&#25324;&#26681;&#25454;&#38656;&#35201;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#12290;&#29366;&#24577;&#36716;&#25442;&#30001;LLM&#20570;&#20986;&#30340;&#29305;&#23450;&#35268;&#21017;&#25110;&#20915;&#31574;&#25511;&#21046;&#65292;&#20801;&#35768;&#36890;&#36807;&#20219;&#21153;&#30340;&#39044;&#23450;&#20041;StateFlow&#27169;&#22411;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#31561;&#21464;&#35268;&#21010;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36755;&#20837;&#31354;&#38388;&#30340;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#21516;&#26102;&#65292;&#23558;&#36816;&#21160;&#39044;&#27979;&#21644;&#36712;&#36857;&#35268;&#21010;&#32467;&#21512;&#21040;&#19968;&#20010;&#27493;&#39588;&#20013;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#32423;&#36335;&#32447;&#24341;&#23548;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11304</link><description>&lt;p&gt;
&#24320;&#21019;&#24615;SE(2)-&#31561;&#21464;&#36712;&#36857;&#35268;&#21010;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#31561;&#21464;&#35268;&#21010;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36755;&#20837;&#31354;&#38388;&#30340;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#21516;&#26102;&#65292;&#23558;&#36816;&#21160;&#39044;&#27979;&#21644;&#36712;&#36857;&#35268;&#21010;&#32467;&#21512;&#21040;&#19968;&#20010;&#27493;&#39588;&#20013;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#32423;&#36335;&#32447;&#24341;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#21010;&#25511;&#21046;&#33258;&#36710;&#36712;&#36857;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#21033;&#29992;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#26469;&#21033;&#29992;&#22330;&#26223;&#20013;&#30340;&#20960;&#20309;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#35777;&#36755;&#20837;&#31354;&#38388;&#30340;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#21516;&#26102;&#65292;&#23558;&#36816;&#21160;&#39044;&#27979;&#21644;&#36712;&#36857;&#35268;&#21010;&#32467;&#21512;&#21040;&#19968;&#20010;&#27493;&#39588;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#36731;&#37327;&#32423;&#31561;&#21464;&#35268;&#21010;&#27169;&#22411;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#25152;&#26377;&#36710;&#36742;&#30340;&#22810;&#27169;&#24577;&#32852;&#21512;&#39044;&#27979;&#65292;&#24182;&#36873;&#25321;&#19968;&#20010;&#27169;&#24577;&#20316;&#20026;&#33258;&#36710;&#35745;&#21010;&#12290;&#31561;&#21464;&#32593;&#32476;&#35774;&#35745;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#20445;&#35777;&#20102;&#36755;&#20986;&#31283;&#23450;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#31561;&#21464;&#36335;&#32447;&#21560;&#24341;&#21147;&#65292;&#20197;&#24341;&#23548;&#33258;&#36710;&#27839;&#30528;&#19968;&#20010;&#30001;&#29616;&#25104;GPS&#23548;&#33322;&#31995;&#32479;&#25552;&#20379;&#30340;&#39640;&#32423;&#36335;&#32447;&#21069;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11304v1 Announce Type: cross  Abstract: Planning the trajectory of the controlled ego vehicle is a key challenge in automated driving. As for human drivers, predicting the motions of surrounding vehicles is important to plan the own actions. Recent motion prediction methods utilize equivariant neural networks to exploit geometric symmetries in the scene. However, no existing method combines motion prediction and trajectory planning in a joint step while guaranteeing equivariance under roto-translations of the input space. We address this gap by proposing a lightweight equivariant planning model that generates multi-modal joint predictions for all vehicles and selects one mode as the ego plan. The equivariant network design improves sample efficiency, guarantees output stability, and reduces model parameters. We further propose equivariant route attraction to guide the ego vehicle along a high-level route provided by an off-the-shelf GPS navigation system. This module creates
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11299</link><description>&lt;p&gt;
SQ-LLaVA&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#30528;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#40511;&#27807;&#25104;&#20026;&#25972;&#20010;&#32593;&#32476;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#25913;&#21892;&#36328;&#27169;&#24577;&#23545;&#40784;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#32771;&#34385;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35270;&#35273;&#20219;&#21153;&#33539;&#22260;&#30340;&#26356;&#22810;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38382;&#31572;&#65292;&#20294;&#36825;&#31181;&#25805;&#20316;&#25104;&#26412;&#36739;&#39640;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#21253;&#21547;&#22823;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#19968;&#30452;&#40092;&#26377;&#20154;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20869;&#37096;&#34987;&#24573;&#35270;&#30340;&#19978;&#19979;&#25991;&#65292;&#35757;&#32451;&#27169;&#22411;&#33258;&#25105;&#35757;&#32451;'&#23398;&#20064;'&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;&#12290;SQ-LLaVA&#22312;&#29983;&#25104;&#28789;&#27963;&#19988;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOOD&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#39046;&#22495;&#22806;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20851;&#31995;&#23884;&#20837;&#32858;&#21512;&#35774;&#35745;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.11292</link><description>&lt;p&gt;
&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39046;&#22495;&#22806;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOOD&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#39046;&#22495;&#22806;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20851;&#31995;&#23884;&#20837;&#32858;&#21512;&#35774;&#35745;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22810;&#20851;&#31995;&#22270;&#26159;&#19968;&#31181;&#29992;&#20110;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#25968;&#25454;&#30340;&#34920;&#36798;&#24418;&#24335;&#65292;&#22312;&#36825;&#31181;&#25968;&#25454;&#20013;&#65292;&#20851;&#31995;&#20801;&#35768;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#35299;&#20915;&#36825;&#31867;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20219;&#21153;&#38656;&#35201;&#33021;&#22815;&#25214;&#21040;&#25429;&#25417;&#25152;&#28041;&#21450;&#20851;&#31995;&#22810;&#26679;&#24615;&#20197;&#21450;&#21160;&#24577;&#28436;&#21464;&#30340;&#32467;&#26500;&#23884;&#20837;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#21160;&#24577;&#22810;&#20851;&#31995;&#22270;&#24314;&#31435;&#20102;&#19968;&#31867;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#39046;&#22495;&#22806;&#38142;&#25509;&#39044;&#27979;&#65292;&#20854;&#20013;&#24453;&#39044;&#27979;&#30340;&#20851;&#31995;&#22312;&#36755;&#20837;&#22270;&#20013;&#19981;&#21487;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;GOOD&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#39046;&#22495;&#22806;&#27867;&#21270;&#38382;&#39064;&#12290;GOOD&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20851;&#31995;&#23884;&#20837;&#32858;&#21512;&#35774;&#35745;&#27010;&#24565;&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#24403;&#33021;&#22815;&#35299;&#24320;&#19981;&#21516;&#20851;&#31995;&#28151;&#21512;&#27604;&#20363;&#26102;&#65292;&#23884;&#20837;&#25165;&#26159;&#22909;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11292v1 Announce Type: cross  Abstract: Dynamic multi-relational graphs are an expressive relational representation for data enclosing entities and relations of different types, and where relationships are allowed to vary in time. Addressing predictive tasks over such data requires the ability to find structure embeddings that capture the diversity of the relationships involved, as well as their dynamic evolution. In this work, we establish a novel class of challenging tasks for dynamic multi-relational graphs involving out-of-domain link prediction, where the relationship being predicted is not available in the input graph. We then introduce a novel Graph Neural Network model, named GOOD, designed specifically to tackle the out-of-domain generalization problem. GOOD introduces a novel design concept for multi-relation embedding aggregation, based on the idea that good representations are such when it is possible to disentangle the mixing proportions of the different relatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.11265</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#21892;&#20316;&#32773;&#39564;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11265
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20851;&#27880;&#30340;&#26159;&#25512;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#19968;&#20010;&#29305;&#23450;&#20316;&#32773;&#25776;&#20889;&#36824;&#26159;&#30001;&#20854;&#20182;&#20154;&#25776;&#20889;&#12290;&#24050;&#32463;&#26174;&#31034;&#35768;&#22810;AV&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#24694;&#24847;&#20316;&#32773;&#31215;&#26497;&#23581;&#35797;&#27450;&#39575;&#20998;&#31867;&#22120;&#65292;&#26041;&#27861;&#26159;&#38544;&#34255;&#20182;&#20204;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#25110;&#32773;&#27169;&#20223;&#21478;&#19968;&#20301;&#20316;&#32773;&#30340;&#39118;&#26684;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20998;&#31867;&#22120;&#35757;&#32451;&#38598;&#19982;&#65288;&#36127;&#38754;&#30340;&#65289;&#21512;&#25104;&#31034;&#20363;&#36827;&#34892;&#22686;&#24378;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#36825;&#20123;&#21512;&#25104;&#31034;&#20363;&#26159;&#20026;&#20102;&#27169;&#20223;&#24863;&#20852;&#36259;&#30340;&#20316;&#32773;&#30340;&#39118;&#26684;&#32780;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22686;&#24378;&#23545;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#30340;AV&#20219;&#21153;&#20013;&#24102;&#26469;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#25913;&#36827;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65288;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#23567;&#35268;&#27169;transformers&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#27969;&#34892;&#30340;GPT&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11265v1 Announce Type: cross  Abstract: Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting. In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36153;&#26364;&#30340;&#36335;&#24452;&#31215;&#20998;&#24341;&#20837;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#24418;&#24335;&#65292;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20840;&#38754;&#25551;&#36848;&#65292;&#24182;&#35782;&#21035;&#20986;&#36830;&#25509;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#37319;&#26679;&#26041;&#26696;&#30340;&#25554;&#20540;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.11262</link><description>&lt;p&gt;
&#36890;&#36807;&#36153;&#26364;&#30340;&#36335;&#24452;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Understanding Diffusion Models by Feynman's Path Integral
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11262
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36153;&#26364;&#30340;&#36335;&#24452;&#31215;&#20998;&#24341;&#20837;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#24418;&#24335;&#65292;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20840;&#38754;&#25551;&#36848;&#65292;&#24182;&#35782;&#21035;&#20986;&#36830;&#25509;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#37319;&#26679;&#26041;&#26696;&#30340;&#25554;&#20540;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65307;&#28982;&#32780;&#65292;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#65288;&#21363;&#27010;&#29575;&#27969;ODEs&#65289;&#37319;&#26679;&#26041;&#26696;&#20043;&#38388;&#24615;&#33021;&#24046;&#24322;&#30340;&#28508;&#22312;&#22240;&#32032;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#36153;&#26364;&#30340;&#36335;&#24452;&#31215;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#24418;&#24335;&#65292;&#36825;&#26159;&#26368;&#21021;&#20026;&#37327;&#23376;&#29289;&#29702;&#23398;&#24320;&#21457;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#24418;&#24335;&#25552;&#20379;&#20102;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20840;&#38754;&#25551;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#25512;&#23548;&#12290;&#36825;&#31181;&#24418;&#24335;&#23481;&#32435;&#20102;&#19968;&#20010;&#25554;&#20540;&#21442;&#25968;&#65292;&#36830;&#25509;&#20102;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#37319;&#26679;&#26041;&#26696;&#65292;&#25105;&#20204;&#30830;&#23450;&#36825;&#20010;&#21442;&#25968;&#23601;&#20687;&#26159;&#37327;&#23376;&#29289;&#29702;&#23398;&#20013;&#30340;&#26222;&#26391;&#20811;&#24120;&#25968;&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#31181;&#31867;&#27604;&#20351;&#25105;&#20204;&#33021;&#22815;&#24212;&#29992;&#28201;&#31574;-&#20811;&#25289;&#22696;-&#24067;&#37324;&#28170;&#65288;WKB&#65289;&#23637;&#24320;&#65292;&#36825;&#26159;&#37327;&#23376;&#29289;&#29702;&#23398;&#20013;&#19968;&#20010;&#25104;&#29087;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11262v1 Announce Type: cross  Abstract: Score-based diffusion models have proven effective in image generation and have gained widespread usage; however, the underlying factors contributing to the performance disparity between stochastic and deterministic (i.e., the probability flow ODEs) sampling schemes remain unclear. We introduce a novel formulation of diffusion models using Feynman's path integral, which is a formulation originally developed for quantum physics. We find this formulation providing comprehensive descriptions of score-based generative models, and demonstrate the derivation of backward stochastic differential equations and loss functions.The formulation accommodates an interpolating parameter connecting stochastic and deterministic sampling schemes, and we identify this parameter as a counterpart of Planck's constant in quantum physics. This analogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a well-established technique in quantum ph
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;Lie&#32676;&#19978;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20026;&#40654;&#26364;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;RBN&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;Lie&#32676;&#21040;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#19977;&#31867;&#21442;&#25968;&#21270;Lie&#32676;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.11261</link><description>&lt;p&gt;
&#19968;&#31181;Lie&#32676;&#26041;&#27861;&#24212;&#29992;&#20110;&#40654;&#26364;&#25209;&#37327;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Lie Group Approach to Riemannian Batch Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;Lie&#32676;&#19978;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20026;&#40654;&#26364;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;RBN&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;Lie&#32676;&#21040;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#19977;&#31867;&#21442;&#25968;&#21270;Lie&#32676;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#23384;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#27969;&#24418;&#20540;&#27979;&#37327;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25193;&#23637;&#21040;&#27969;&#24418;&#65292;&#24182;&#19988;&#21516;&#26102;&#65292;&#24402;&#19968;&#21270;&#25216;&#26415;&#20063;&#24050;&#32463;&#34987;&#24212;&#29992;&#21040;&#20960;&#20010;&#27969;&#24418;&#65292;&#31216;&#20026;&#40654;&#26364;&#24402;&#19968;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#40654;&#26364;&#24402;&#19968;&#21270;&#26041;&#27861;&#26159;&#20197;&#20020;&#26102;&#26041;&#24335;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#27969;&#24418;&#12290;&#26412;&#25991;&#22312;Lie&#32676;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#40654;&#26364;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;RBN&#65289;&#25216;&#26415;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#25511;&#21046;&#40654;&#26364;&#24179;&#22343;&#20540;&#21644;&#26041;&#24046;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;Lie&#32676;&#32467;&#26500;&#30340;&#23545;&#31216;&#27491;&#23450;(SPD)&#27969;&#24418;&#12290;&#21033;&#29992;&#21464;&#24418;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;SPD&#27969;&#24418;&#19978;&#30340;Lie&#32676;&#25512;&#24191;&#25104;&#19977;&#31867;&#21442;&#25968;&#21270;Lie&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11261v1 Announce Type: cross  Abstract: Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups. Specific normalization l
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#65292;&#20197;&#35299;&#20915;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.11259</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11259
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#65292;&#20197;&#35299;&#20915;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#26381;&#21153;&#22120;&#20013;&#25918;&#32622;&#24212;&#29992;&#31243;&#24207;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#35768;&#22810;&#26381;&#21153;&#22120;&#12289;&#29992;&#25143;&#21450;&#20854;&#35831;&#27714;&#12290;&#29616;&#26377;&#31639;&#27861;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#35299;&#20915;&#20855;&#26377;&#37325;&#22823;&#19981;&#30830;&#23450;&#24615;&#24773;&#26223;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26368;&#22823;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#21516;&#26102;&#32771;&#34385;&#25152;&#26377;&#25216;&#26415;&#32422;&#26463;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#65292;&#23427;&#27169;&#25311;&#20102;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#20013;&#37096;&#32626;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#35745;&#23558;&#23398;&#20064;&#22914;&#20309;&#26681;&#25454;&#29992;&#25143;&#21644;&#26381;&#21153;&#22120;&#30340;&#31354;&#38388;&#20301;&#32622;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#12290;&#26412;&#30740;&#31350;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#36890;&#36807;&#21464;&#21270;&#21442;&#25968;&#22914;&#29992;&#25143;&#20301;&#32622;&#12289;&#35831;&#27714;&#36895;&#29575;&#21644;&#35299;&#20915;&#20248;&#21270;&#27169;&#22411;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#35760;&#24405;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#36317;&#31163;&#21487;&#29992;&#26381;&#21153;&#22120;&#30340;&#36317;&#31163;&#29305;&#24449;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11259v1 Announce Type: cross  Abstract: Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests. Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios. Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints. One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers. Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers. In this study, the problem is formulated as a two-stage stochastic programming. A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model. Then, based on the distance features of each user from the available servers and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.11220</link><description>&lt;p&gt;
CPA-Enhancer&#65306;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#21333;&#19968;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#26469;&#30830;&#23450;&#36864;&#21270;&#31867;&#22411;&#65292;&#24182;&#20026;&#27599;&#31181;&#31867;&#22411;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CPA-Enhancer&#22312;CoT&#25552;&#31034;&#30340;&#36880;&#27493;&#25351;&#23548;&#19979;&#36880;&#27493;&#35843;&#25972;&#20854;&#22686;&#24378;&#31574;&#30053;&#65292;&#36825;&#20123;&#25552;&#31034;&#32534;&#30721;&#20102;&#19982;&#36864;&#21270;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#21033;&#29992;CoT&#25552;&#31034;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CPA-Enhancer&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#22312;&#19981;&#20107;&#20808;&#30693;&#36947;&#36864;&#21270;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21463;&#25439;&#22270;&#20687;&#19978;&#23454;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CPA-E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#36807;&#21435;&#20116;&#21313;&#24180;&#20013;&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#20854;&#19982;&#20154;&#24037;&#26234;&#33021;&#31561;&#26032;&#26041;&#27861;&#30340;&#20114;&#21160;&#65292;&#30740;&#31350;&#20102;&#22240;&#26524;&#20851;&#31995;&#23545;&#21508;&#39046;&#22495;&#30340;&#24433;&#21709;&#21644;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.11219</link><description>&lt;p&gt;
&#33258;&#24213;&#21521;&#19978;&#30340;&#22240;&#26524;&#20851;&#31995;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Causality from Bottom to Top: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#36807;&#21435;&#20116;&#21313;&#24180;&#20013;&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#20854;&#19982;&#20154;&#24037;&#26234;&#33021;&#31561;&#26032;&#26041;&#27861;&#30340;&#20114;&#21160;&#65292;&#30740;&#31350;&#20102;&#22240;&#26524;&#20851;&#31995;&#23545;&#21508;&#39046;&#22495;&#30340;&#24433;&#21709;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#24050;&#32463;&#25104;&#20026;&#35299;&#37322;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#20107;&#20214;&#12289;&#29616;&#35937;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#22522;&#26412;&#26041;&#27861;&#12290;&#23427;&#24050;&#32463;&#28183;&#36879;&#21040;&#21307;&#23398;&#12289;&#20445;&#20581;&#12289;&#32463;&#27982;&#23398;&#12289;&#37329;&#34701;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#25945;&#32946;&#12289;&#20844;&#20849;&#25919;&#31574;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#26426;&#22120;&#20154;&#23398;&#12289;&#25511;&#21046;&#12289;&#31038;&#20250;&#23398;&#12289;&#33829;&#38144;&#21644;&#24191;&#21578;&#31561;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#36807;&#21435;&#20116;&#21313;&#24180;&#20013;&#30340;&#21457;&#23637;&#65292;&#25581;&#31034;&#20102;&#22240;&#26524;&#20851;&#31995;&#19982;&#20854;&#20182;&#26041;&#27861;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#20351;&#29992;&#23427;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;&#22240;&#26524;&#20851;&#31995;&#22914;&#20309;&#19982;&#26032;&#26041;&#27861;&#22914;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#31946;&#36923;&#36753;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#20851;&#31995;&#23545;&#21508;&#39046;&#22495;&#30340;&#24433;&#21709;&#12289;&#20854;&#36129;&#29486;&#20197;&#21450;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20114;&#21160;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#20030;&#20363;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11219v1 Announce Type: new  Abstract: Causality has become a fundamental approach for explaining the relationships between events, phenomena, and outcomes in various fields of study. It has invaded various fields and applications, such as medicine, healthcare, economics, finance, fraud detection, cybersecurity, education, public policy, recommender systems, anomaly detection, robotics, control, sociology, marketing, and advertising. In this paper, we survey its development over the past five decades, shedding light on the differences between causality and other approaches, as well as the preconditions for using it. Furthermore, the paper illustrates how causality interacts with new approaches such as Artificial Intelligence (AI), Generative AI (GAI), Machine and Deep Learning, Reinforcement Learning (RL), and Fuzzy Logic. We study the impact of causality on various fields, its contribution, and its interaction with state-of-the-art approaches. Additionally, the paper exempli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#33539;&#30068;&#35770;&#24341;&#20837;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#23450;&#20041;&#65292;&#28436;&#31034;&#20102;&#22240;&#26524;&#25512;&#26029;&#20013;&#25351;&#26631;&#32508;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11217</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#20010;&#20154;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Research on Personal Credit Risk Assessment Methods Based on Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#33539;&#30068;&#35770;&#24341;&#20837;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#23450;&#20041;&#65292;&#28436;&#31034;&#20102;&#22240;&#26524;&#25512;&#26029;&#20013;&#25351;&#26631;&#32508;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21382;&#21490;&#19978;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#35752;&#35770;&#21487;&#20197;&#36861;&#28335;&#21040;&#21476;&#24076;&#33098;&#65292;&#28982;&#32780;&#30452;&#33267;&#20170;&#26085;&#20173;&#27809;&#26377;&#20849;&#35782;&#12290;&#20174;&#26681;&#26412;&#19978;&#35762;&#65292;&#36825;&#28304;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#26412;&#36136;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#38656;&#35201;&#36229;&#36234;&#20154;&#31867;&#35748;&#30693;&#23616;&#38480;&#30340;&#25277;&#35937;&#24037;&#20855;&#12290;&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#25968;&#23398;&#21644;&#35745;&#31639;&#24037;&#20855;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#25506;&#32034;&#22240;&#26524;&#20851;&#31995;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#21644;&#25216;&#26415;&#25163;&#27573;&#65292;&#20026;&#30740;&#31350;&#24320;&#36767;&#20102;&#26356;&#22810;&#36884;&#24452;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#30001;Samuel Eilenberg&#21644;Saunders Mac Lane&#20110;1945&#24180;&#25552;&#20986;&#30340;&#21033;&#29992;&#33539;&#30068;&#35770;&#23450;&#20041;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#27010;&#24565;&#65292;&#20197;&#36991;&#20813;&#38598;&#21512;&#35770;&#20013;&#30340;&#33258;&#25351;&#30683;&#30462;&#65292;&#29305;&#21035;&#26159;&#32599;&#32032;&#24726;&#35770;&#12290;&#22312;&#36825;&#19968;&#26694;&#26550;&#20869;&#65292;&#28436;&#31034;&#20102;&#22240;&#26524;&#25512;&#26029;&#20013;&#25351;&#26631;&#32508;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;&#30001;&#20110;&#33539;&#30068;&#35770;&#30456;&#20851;&#25216;&#26415;&#24037;&#20855;&#30340;&#21457;&#23637;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#27010;&#29575;&#35770;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11217v1 Announce Type: new  Abstract: The discussion on causality in human history dates back to ancient Greece, yet to this day, there is still no consensus. Fundamentally, this stems from the nature of human cognition, as understanding causality requires abstract tools to transcend the limitations of human cognition. In recent decades, the rapid development of mathematical and computational tools has provided new theoretical and technical means for exploring causality, creating more avenues for investigation.   Based on this, this paper introduces a new definition of causality using category theory, proposed by Samuel Eilenberg and Saunders Mac Lane in 1945 to avoid the self-referential contradictions in set theory, notably the Russell paradox. Within this framework, the feasibility of indicator synthesis in causal inference is demonstrated. Due to the limitations in the development of category theory-related technical tools, this paper adopts the widely-used probabilistic
&lt;/p&gt;</description></item><item><title>MindEye2&#20351;&#29992;&#20849;&#20139;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;1&#23567;&#26102;&#30340;fMRI&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;fMRI&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.11207</link><description>&lt;p&gt;
MindEye2&#65306;&#20849;&#20139;&#20027;&#39064;&#27169;&#22411;&#20351;&#24471;&#21482;&#38656;1&#23567;&#26102;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;fMRI&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11207
&lt;/p&gt;
&lt;p&gt;
MindEye2&#20351;&#29992;&#20849;&#20139;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;1&#23567;&#26102;&#30340;fMRI&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;fMRI&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24863;&#30693;&#30340;&#37325;&#24314;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#26159;&#38024;&#23545;&#27599;&#20010;&#21463;&#35797;&#32773;&#29420;&#31435;&#35757;&#32451;&#30340;&#65292;&#27599;&#20010;&#21463;&#35797;&#32773;&#38656;&#35201;&#25968;&#21313;&#23567;&#26102;&#26114;&#36149;&#30340;fMRI&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;1&#23567;&#26102;fMRI&#35757;&#32451;&#25968;&#25454;&#23454;&#29616;&#39640;&#36136;&#37327;&#37325;&#24314;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;7&#21517;&#21463;&#35797;&#32773;&#20013;&#39044;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#26032;&#21463;&#35797;&#32773;&#30340;&#37096;&#20998;&#25968;&#25454;&#19978;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#21151;&#33021;&#23545;&#40784;&#31243;&#24207;&#23558;&#25152;&#26377;&#33041;&#25968;&#25454;&#32447;&#24615;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#20027;&#39064;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#36890;&#36807;&#20849;&#20139;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#21040;CLIP&#22270;&#20687;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;XL&#26469;&#25509;&#21463;CLIP&#28508;&#22312;&#20316;&#20026;&#36755;&#20837;&#32780;&#19981;&#26159;&#25991;&#26412;&#65292;&#23558;CLIP&#31354;&#38388;&#26144;&#23556;&#21040;&#20687;&#32032;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#36328;&#21463;&#35797;&#32773;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#20063;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#37325;&#24314;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11207v1 Announce Type: cross  Abstract: Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruct
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040;&#19981;&#21516;GPU&#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11204</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#36827;&#34892;&#20998;&#21306;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Partitioned Neural Network Training via Synthetic Intermediate Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040;&#19981;&#21516;GPU&#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26222;&#21450;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290; GPU &#20869;&#23384;&#32422;&#26463;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#36825;&#20123;&#24222;&#22823;&#27169;&#22411;&#30340;&#19968;&#20010;&#26126;&#26174;&#29942;&#39048;&#12290;&#29616;&#26377;&#31574;&#30053;&#65292;&#21253;&#25324;&#25968;&#25454;&#24182;&#34892;&#12289;&#27169;&#22411;&#24182;&#34892;&#12289;&#27969;&#27700;&#32447;&#24182;&#34892;&#21644;&#23436;&#20840;&#20998;&#29255;&#25968;&#25454;&#24182;&#34892;&#65292;&#25552;&#20379;&#20102;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#12290; &#29305;&#21035;&#26159;&#27169;&#22411;&#24182;&#34892;&#20801;&#35768;&#23558;&#25972;&#20010;&#27169;&#22411;&#20998;&#24067;&#22312;&#22810;&#20010; GPU &#19978;&#65292;&#20294;&#38543;&#21518;&#30340;&#36825;&#20123;&#20998;&#21306;&#20043;&#38388;&#30340;&#25968;&#25454;&#36890;&#20449;&#20943;&#24930;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#20026;&#22312;&#27599;&#20010; GPU &#19978;&#23384;&#20648;&#36741;&#21161;&#21442;&#25968;&#25152;&#38656;&#30340;&#22823;&#37327;&#20869;&#23384;&#24320;&#38144;&#22686;&#21152;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290; &#26412;&#30740;&#31350;&#20027;&#24352;&#19981;&#20351;&#29992;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26159;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040; GPU &#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#12290; &#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#36825;&#20123;&#26631;&#31614;&#20943;&#32531;&#20102;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11204v1 Announce Type: cross  Abstract: The proliferation of extensive neural network architectures, particularly deep learning models, presents a challenge in terms of resource-intensive training. GPU memory constraints have become a notable bottleneck in training such sizable models. Existing strategies, including data parallelism, model parallelism, pipeline parallelism, and fully sharded data parallelism, offer partial solutions. Model parallelism, in particular, enables the distribution of the entire model across multiple GPUs, yet the ensuing data communication between these partitions slows down training. Additionally, the substantial memory overhead required to store auxiliary parameters on each GPU compounds computational demands. Instead of using the entire model for training, this study advocates partitioning the model across GPUs and generating synthetic intermediate labels to train individual segments. These labels, produced through a random process, mitigate me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;Verilog&#25968;&#25454;&#21294;&#20047;&#21644;&#35757;&#32451;&#25968;&#25454;&#20934;&#22791;&#26102;&#38388;&#38271;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.11202</link><description>&lt;p&gt;
&#25968;&#25454;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#23545;LLM&#36827;&#34892;&#33455;&#29255;&#35774;&#35745;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;Verilog&#25968;&#25454;&#21294;&#20047;&#21644;&#35757;&#32451;&#25968;&#25454;&#20934;&#22791;&#26102;&#38388;&#38271;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#39640;&#23618;&#25552;&#31034;&#19979;&#33258;&#21160;&#29983;&#25104;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65288;HDL&#65289;&#20195;&#30721;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#24494;&#35843;&#26469;&#22686;&#24378;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;Verilog&#25968;&#25454;&#38459;&#30861;&#20102;LLMs&#22312;Verilog&#29983;&#25104;&#36136;&#37327;&#19978;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#32570;&#23569;Verilog&#21644;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#33050;&#26412;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26174;&#30528;&#22686;&#21152;&#20102;&#20026;LLM&#35757;&#32451;&#22120;&#20934;&#22791;&#35757;&#32451;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#23427;&#29983;&#25104;&#19982;Verilog&#21644;EDA&#33050;&#26412;&#23545;&#40784;&#30340;&#22823;&#37327;&#39640;&#36136;&#37327;&#33258;&#28982;&#35821;&#35328;&#12290;&#23545;&#20110;Verilog&#29983;&#25104;&#65292;&#23427;&#23558;Verilog&#25991;&#20214;&#36716;&#25442;&#20026;&#25277;&#35937;&#35821;&#27861;&#26641;&#65292;&#28982;&#21518;&#23558;&#33410;&#28857;&#26144;&#23556;&#21040;&#20855;&#26377;&#39044;&#23450;&#20041;&#27169;&#26495;&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#23545;&#20110;Verilog&#20462;&#22797;&#65292;&#23427;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11202v1 Announce Type: cross  Abstract: Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts. Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design. However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers. This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts. For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template. For Verilog repair, it 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUMP&#30340;&#22270;&#21333;&#20803;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#26469;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11199</link><description>&lt;p&gt;
&#22270;&#21333;&#20803;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Graph Unitary Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUMP&#30340;&#22270;&#21333;&#20803;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#26469;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25913;&#21892;&#22270;&#35889;&#30340;&#37325;&#36830;&#25216;&#26415;&#12289;&#30772;&#22351;&#22270;&#20013;&#30340;&#32467;&#26500;&#20559;&#35265;&#26469;&#25269;&#21046;&#36807;&#24230;&#21387;&#32553;&#65292;&#28982;&#32780;&#22312;&#36807;&#24230;&#21387;&#32553;&#24230;&#37327;&#26041;&#38754;&#23545;&#36807;&#24230;&#21387;&#32553;&#30340;&#25913;&#36827;&#26377;&#25152;&#38480;&#21046;&#12290;&#21463;&#21040;&#21333;&#20803;RNN&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#21333;&#20803;&#28040;&#24687;&#20256;&#36882;&#65288;GUMP&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#26469;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#20026;&#35774;&#35745;GUMP&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#26222;&#36890;&#22270;&#20855;&#26377;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#24182;&#20445;&#25345;&#20854;&#32467;&#26500;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#30340;&#22266;&#26377;&#32467;&#26500;&#23454;&#29616;&#21333;&#20301;&#21270;&#25237;&#24433;&#31639;&#27861;&#33719;&#24471;&#21333;&#20803;&#37051;&#25509;&#30697;&#38453;&#65292;&#24182;&#20801;&#35768;GUMP&#26159;&#32622;&#25442;&#31561;&#21464;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;GUMP&#22312;&#25913;&#21892;&#21508;&#31181;&#24212;&#29992;&#20219;&#21153;&#19978;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11199v1 Announce Type: cross  Abstract: Message passing mechanism contributes to the success of GNNs in various applications, but also brings the oversquashing problem. Recent works combat oversquashing by improving the graph spectrums with rewiring techniques, disrupting the structural bias in graphs, and having limited improvement on oversquashing in terms of oversquashing measure. Motivated by unitary RNN, we propose Graph Unitary Message Passing (GUMP) to alleviate oversquashing in GNNs by applying unitary adjacency matrix for message passing. To design GUMP, a transformation is first proposed to make general graphs have unitary adjacency matrix and keep its structural bias. Then, unitary adjacency matrix is obtained with a unitary projection algorithm, which is implemented by utilizing the intrinsic structure of unitary adjacency matrix and allows GUMP to be permutation-equivariant. Experimental results show the effectiveness of GUMP in improving the performance on vari
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20808;&#39564;&#20381;&#36182;&#24615;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#23545;&#21518;&#39564;&#25277;&#26679;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#25913;&#36827;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#22522;&#20934;&#30340;&#26041;&#27861;&#35770;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.11175</link><description>&lt;p&gt;
&#20808;&#39564;&#20381;&#36182;&#24615;&#20998;&#26512;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#30340;&#21518;&#39564;&#25277;&#26679;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prior-dependent analysis of posterior sampling reinforcement learning with function approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11175
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20808;&#39564;&#20381;&#36182;&#24615;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#23545;&#21518;&#39564;&#25277;&#26679;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#25913;&#36827;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#22522;&#20934;&#30340;&#26041;&#27861;&#35770;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#23545;&#32447;&#24615;&#28151;&#21512;MDPs&#24314;&#27169;&#30340;&#20989;&#25968;&#36924;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#25512;&#36827;&#20102;&#38543;&#26426;&#25506;&#32034;&#12290;&#25105;&#20204;&#20026;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;RL&#24314;&#31435;&#20102;&#39318;&#20010;&#20808;&#39564;&#20381;&#36182;&#24615;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#65307;&#24182;&#19988;&#25913;&#36827;&#20102;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#24378;&#21270;&#23398;&#20064;&#65288;PSRL&#65289;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#30028;&#20026;${\mathcal{O}}(d\sqrt{H^3 T \log T})$&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;$d$&#34920;&#31034;&#36716;&#31227;&#26680;&#30340;&#32500;&#24230;&#65292;$H$&#34920;&#31034;&#35268;&#21010;&#35270;&#37326;&#65292;$T$&#34920;&#31034;&#24635;&#20132;&#20114;&#27425;&#25968;&#12290; &#36825;&#34920;&#31034;&#36890;&#36807;&#20248;&#21270;$\mathcal{O}(\sqrt{\log T})$&#22240;&#23376;&#65292;&#25105;&#20204;&#22312;&#20043;&#21069;&#38024;&#23545;&#32447;&#24615;&#28151;&#21512;MDPs&#30340;&#22522;&#20934;&#65288;Osband&#21644;Van Roy&#65292;2014&#65289;&#19978;&#21462;&#24471;&#20102;&#26041;&#27861;&#35770;&#19978;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20215;&#20540;&#23450;&#21521;&#27169;&#22411;&#23398;&#20064;&#30340;&#35270;&#35282;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#35770;&#35777;&#21644;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#20998;&#26512;&#20381;&#36182;&#20110;&#32622;&#20449;&#21306;&#38388;&#21644;&#38598;&#20013;&#19981;&#31561;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11175v1 Announce Type: cross  Abstract: This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs. We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of ${\mathcal{O}}(d\sqrt{H^3 T \log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\mathcal{O}(\sqrt{\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11169</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Correcting misinformation on social media with a large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20449;&#24687;&#20250;&#30772;&#22351;&#20844;&#20247;&#23545;&#31185;&#23398;&#21644;&#27665;&#20027;&#30340;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#19981;&#20934;&#30830;&#20449;&#24687;&#20250;&#36805;&#36895;&#20256;&#25773;&#12290;&#19987;&#23478;&#21644;&#26222;&#36890;&#20154;&#36890;&#36807;&#25163;&#21160;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#20934;&#30830;&#20449;&#24687;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#32416;&#27491;&#35823;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#25285;&#24551;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#25216;&#26415;&#20351;&#35823;&#20449;&#24687;&#26356;&#23481;&#26131;&#29983;&#25104;&#12290;LLMs&#36824;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#65292;&#21487;&#20197;&#21152;&#36895;&#32416;&#27491;&#35823;&#20449;&#24687;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30001;&#20110;&#32570;&#20047;&#26368;&#26032;&#20449;&#24687;&#12289;&#20542;&#21521;&#20110;&#29983;&#25104;&#20284;&#26159;&#32780;&#38750;&#30340;&#20869;&#23481;&#21644;&#24341;&#29992;&#20197;&#21450;&#26080;&#27861;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#32780;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUSE&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#26368;&#26032;&#20449;&#24687;&#35775;&#38382;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;LLM&#12290;&#36890;&#36807;&#26816;&#32034;&#19978;&#19979;&#25991;&#35777;&#25454;&#21644;&#21453;&#39539;&#65292;MUSE&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#21487;&#20449;&#30340;&#35299;&#37322;&#21644;&#21442;&#32771;&#12290;&#23427;&#36824;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#26799;&#24230;&#21453;&#36716;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#23383;&#29256;&#26435;&#35748;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#20043;&#38388;&#30340;&#27010;&#24565;&#24046;&#24322;&#26469;&#24674;&#22797;&#22270;&#20687;&#30340;&#32570;&#22833;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2403.11162</link><description>&lt;p&gt;
CGI-DM&#65306;&#36890;&#36807;&#23545;&#27604;&#26799;&#24230;&#21453;&#36716;&#36827;&#34892;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#23383;&#29256;&#26435;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#26799;&#24230;&#21453;&#36716;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#23383;&#29256;&#26435;&#35748;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#20043;&#38388;&#30340;&#27010;&#24565;&#24046;&#24322;&#26469;&#24674;&#22797;&#22270;&#20687;&#30340;&#32570;&#22833;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#24050;&#32463;&#21457;&#23637;&#25104;&#20026;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#26679;&#26412;&#29983;&#25104;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#24494;&#35843;&#21040;&#19968;&#23567;&#32452;&#22270;&#20687;&#19978;&#20197;&#25429;&#25417;&#29305;&#23450;&#39118;&#26684;&#25110;&#23545;&#35937;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#23545;&#28508;&#22312;&#30340;&#29256;&#26435;&#20405;&#29359;&#38382;&#39064;&#34920;&#31034;&#25285;&#24551;&#65292;&#36825;&#20027;&#35201;&#28304;&#20110;&#22312;&#27492;&#36807;&#31243;&#20013;&#20351;&#29992;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23545;&#27604;&#26799;&#24230;&#21453;&#36716;&#36827;&#34892;&#25193;&#25955;&#27169;&#22411;&#65288;CGI-DM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#29983;&#21160;&#30340;&#35270;&#35273;&#22270;&#20687;&#65292;&#29992;&#20110;&#25968;&#23383;&#29256;&#26435;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21024;&#38500;&#22270;&#20687;&#30340;&#37096;&#20998;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#20043;&#38388;&#30340;&#27010;&#24565;&#24046;&#24322;&#24674;&#22797;&#32570;&#22833;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#27169;&#22411;&#30340;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#26500;&#24314;&#20026;&#32473;&#23450;&#30456;&#21516;&#36755;&#20837;&#22270;&#20687;&#26102;&#30340;KL&#25955;&#24230;&#65292;&#21487;&#20197;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#21644;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#36827;&#34892;&#26368;&#22823;&#21270;&#12290;&#21407;&#22270;&#20687;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11162v1 Announce Type: cross  Abstract: Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20262;&#29702;&#23618;&#38754;&#30340;&#37325;&#35201;&#24615;&#20197;&#30830;&#20445;&#20854;&#26377;&#25928;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#26032;&#39062;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#35821;&#35328;&#33021;&#21147;&#12289;&#19987;&#19994;&#27861;&#24459;&#30693;&#35782;&#21644;&#27861;&#24459;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.11152</link><description>&lt;p&gt;
&#27861;&#24459;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#35780;&#20272;&#20262;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evaluation Ethics of LLMs in Legal Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11152
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20262;&#29702;&#23618;&#38754;&#30340;&#37325;&#35201;&#24615;&#20197;&#30830;&#20445;&#20854;&#26377;&#25928;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#26032;&#39062;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#35821;&#35328;&#33021;&#21147;&#12289;&#19987;&#19994;&#27861;&#24459;&#30693;&#35782;&#21644;&#27861;&#24459;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#24212;&#23545;&#35832;&#22914;&#27861;&#24459;&#31561;&#19987;&#19994;&#39046;&#22495;&#29305;&#23450;&#25361;&#25112;&#26041;&#38754;&#30340;&#36890;&#29992;&#33021;&#21147;&#20173;&#21463;&#21040;&#36136;&#30097;&#12290;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#23558;&#27861;&#24459;&#20262;&#29702;&#32435;&#20837;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20027;&#24352;&#20005;&#26684;&#30340;&#20262;&#29702;&#35780;&#20272;&#23545;&#20110;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#26377;&#25928;&#25972;&#21512;&#33267;&#20851;&#37325;&#35201;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#21644;&#39046;&#22495;&#29305;&#23450;&#20262;&#29702;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#30495;&#23454;&#27861;&#24459;&#26696;&#20363;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#22522;&#26412;&#35821;&#35328;&#33021;&#21147;&#12289;&#19987;&#19994;&#27861;&#24459;&#30693;&#35782;&#21644;&#27861;&#24459;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#32467;&#26524;&#23545;&#23398;&#26415;&#30028;&#22260;&#32469;&#27861;&#24459;&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11152v1 Announce Type: cross  Abstract: In recent years, the utilization of large language models for natural language dialogue has gained momentum, leading to their widespread adoption across various domains. However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny. The incorporation of legal ethics into the model has been overlooked by researchers. We asserts that rigorous ethic evaluation is essential to ensure the effective integration of large language models in legal domains, emphasizing the need to assess domain-specific proficiency and domain-specific ethic. To address this, we propose a novelty evaluation methodology, utilizing authentic legal cases to evaluate the fundamental language abilities, specialized legal knowledge and legal robustness of large language models (LLMs). The findings from our comprehensive evaluation contribute significantly to the academic discourse surrounding the s
&lt;/p&gt;</description></item><item><title>&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26356;&#23569;&#30340;&#25552;&#31034;&#21487;&#20197;&#26356;&#22909;&#22320;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#65292;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#22810;&#26679;&#24615;&#20844;&#24335;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#24433;&#21709;LLMs&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11124</link><description>&lt;p&gt;
&#22312;&#20154;&#31867;&#23545;&#40784;&#20013;&#25193;&#23637;&#25968;&#25454;&#22810;&#26679;&#24615;&#20197;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11124
&lt;/p&gt;
&lt;p&gt;
&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26356;&#23569;&#30340;&#25552;&#31034;&#21487;&#20197;&#26356;&#22909;&#22320;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#65292;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#22810;&#26679;&#24615;&#20844;&#24335;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#24433;&#21709;LLMs&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#21487;&#20197;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#35823;&#23548;&#24615;&#25110;&#26377;&#27602;&#20869;&#23481;&#65292;&#21516;&#26102;&#38656;&#35201;&#39640;&#25104;&#26412;&#30340;&#20154;&#31867;&#21453;&#39304;&#12290;&#20551;&#35774;&#20154;&#24037;&#27880;&#37322;&#36164;&#28304;&#26377;&#38480;&#65292;&#21017;&#21487;&#20197;&#32771;&#34385;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#37197;&#26041;&#24335;&#65306;&#26356;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#25110;&#26356;&#22810;&#26679;&#21270;&#30340;&#24453;&#26631;&#35760;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#30340;&#30452;&#25509;&#27604;&#36739;&#23578;&#19981;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#24494;&#35843;&#26679;&#26412;&#25968;&#37327;&#25511;&#21046;&#21452;&#26041;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#20197;&#30452;&#25509;&#21453;&#26144;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22823;&#37327;&#25552;&#31034;&#19981;&#21516;&#65292;&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26159;&#26356;&#23569;&#30340;&#25552;&#31034;&#26356;&#33021;&#28608;&#21457;LLMs&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#21487;&#33021;&#27604;&#36890;&#24120;&#30001;&#21333;&#20010;&#25968;&#23383;&#37327;&#21270;&#30340;&#21709;&#24212;&#26356;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#25552;&#31034;&#22810;&#26679;&#24615;&#30340;&#26032;&#20844;&#24335;&#65292;&#36827;&#19968;&#27493;&#26263;&#31034;&#19982;&#24494;&#35843;&#21518;LLMs&#26368;&#32456;&#24615;&#33021;&#30340;&#32447;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;</title><link>https://arxiv.org/abs/2403.11116</link><description>&lt;p&gt;
&#21338;&#22763;&#35770;&#25991;&#65306;&#19968;&#20010;&#25552;&#31034;&#30340;&#35270;&#35273;&#24187;&#35273;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PhD: A Prompted Visual Hallucination Evaluation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#25512;&#21160;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#22312;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24187;&#35273;&#25361;&#25112;&#20063;&#20986;&#29616;&#22312;LVLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;LVLM&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;&#19978;&#65292;&#24573;&#30053;&#20102;LVLM&#24187;&#35273;&#30340;&#22810;&#26679;&#21270;&#31867;&#22411;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22266;&#26377;&#35270;&#35273;&#35821;&#35328;&#24187;&#35273;&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#65292;&#23545;&#23548;&#33268;&#24187;&#35273;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;IVL-Hallu&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#65288;a&#65289;&#23545;&#35937;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#35937;&#30340;&#35823;&#35782;&#21035;&#32780;&#20135;&#29983;&#65292;&#65288;b&#65289;&#23646;&#24615;&#24187;&#35273;&#65292;&#30001;&#20110;&#23646;&#24615;&#30340;&#35823;&#35782;&#21035;&#32780;&#24341;&#36215;&#65292;&#65288;c&#65289;&#22810;&#27169;&#24577;&#20914;&#31361;&#24187;&#35273;&#65292;&#28304;&#33258;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20197;&#21450;&#65288;d&#65289;&#21453;&#24120;&#35782;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#31435;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11116v1 Announce Type: cross  Abstract: The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions betwee
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Phasic Diversity Optimization (PDO)&#31639;&#27861;&#65292;&#37319;&#29992;&#31181;&#32676;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#22870;&#21169;&#21644;&#22810;&#26679;&#24615;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#38454;&#27573;&#65292;&#24182;&#22312;&#36741;&#21161;&#38454;&#27573;&#23454;&#29616;&#28608;&#36827;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.11114</link><description>&lt;p&gt;
&#31181;&#32676;&#24378;&#21270;&#23398;&#20064;&#30340;&#30456;&#20301;&#22810;&#26679;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Phasic Diversity Optimization for Population-Based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11114
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Phasic Diversity Optimization (PDO)&#31639;&#27861;&#65292;&#37319;&#29992;&#31181;&#32676;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#22870;&#21169;&#21644;&#22810;&#26679;&#24615;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#38454;&#27573;&#65292;&#24182;&#22312;&#36741;&#21161;&#38454;&#27573;&#23454;&#29616;&#28608;&#36827;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Phasic Diversity Optimization (PDO)&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#31181;&#32676;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#22870;&#21169;&#21644;&#22810;&#26679;&#24615;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#32780;&#19981;&#26159;&#20248;&#21270;&#22810;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#36741;&#21161;&#38454;&#27573;&#65292;&#34920;&#29616;&#36739;&#24046;&#30340;agent&#36890;&#36807;&#20915;&#31574;&#32773;&#36827;&#34892;&#22810;&#26679;&#21270;&#65292;&#19981;&#20250;&#21462;&#20195;&#23384;&#26723;&#20013;&#26356;&#22909;&#30340;agent&#12290;&#22870;&#21169;&#21644;&#22810;&#26679;&#24615;&#30340;&#35299;&#32806;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#36741;&#21161;&#38454;&#27573;&#20351;&#29992;&#28608;&#36827;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11114v1 Announce Type: cross  Abstract: Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.Generally,diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;(SQAKD)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20840;&#31934;&#24230;&#21644;&#20302;&#27604;&#29305;&#27169;&#22411;&#20043;&#38388;&#30340;KL&#25439;&#22833;&#20197;&#21450;&#37327;&#21270;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#22797;&#26434;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11106</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Quantization-Aware Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11106
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;(SQAKD)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20840;&#31934;&#24230;&#21644;&#20302;&#27604;&#29305;&#27169;&#22411;&#20043;&#38388;&#30340;KL&#25439;&#22833;&#20197;&#21450;&#37327;&#21270;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#22797;&#26434;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#25022;&#22320;&#65292;&#29616;&#26377;&#24037;&#20316;&#23558;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#38656;&#35201;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26469;&#24179;&#34913;&#19981;&#21516;&#25439;&#22833;&#39033;&#30340;&#26435;&#37325;&#65292;&#20551;&#23450;&#26377;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#65292;&#24182;&#19988;&#38656;&#35201;&#22797;&#26434;&#12289;&#35745;&#31639;&#23494;&#38598;&#30340;&#35757;&#32451;&#31243;&#24207;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;(SQAKD)&#26694;&#26550;&#12290;SQAKD&#39318;&#20808;&#32479;&#19968;&#20102;&#21508;&#31181;&#37327;&#21270;&#20989;&#25968;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#21160;&#24577;&#65292;&#20351;&#20854;&#21487;&#20197;&#28789;&#27963;&#22320;&#25972;&#21512;&#21508;&#31181;QAT&#24037;&#20316;&#12290;&#28982;&#21518;&#65292;&#23427;&#23558;QAT&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#29992;&#20110;KD&#30340;&#20840;&#31934;&#24230;&#27169;&#22411;&#21644;&#20302;&#27604;&#29305;&#27169;&#22411;&#20043;&#38388;&#30340;KL&#25439;&#22833;&#65292;&#20197;&#21450;&#29992;&#20110;&#37327;&#21270;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#32780;&#26080;&#38656;&#26469;&#33258;&#26631;&#31614;&#30340;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11106v1 Announce Type: cross  Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models. However, existing works applying KD to QAT require tedious hyper-parameter tuning to balance the weights of different loss terms, assume the availability of labeled training data, and require complex, computationally intensive training procedures for good performance. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and backward dynamics of various quantization functions, making it flexible for incorporating various QAT works. Then it formulates QAT as a co-optimization problem that simultaneously minimizes the KL-Loss between the full-precision and low-bit models for KD and the discretization error for quantization, without supervision from labels. A comprehensive e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22522;&#20934;&#20013;&#23384;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#26085;&#35821;&#21644;&#20013;&#25991;&#20013;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#38169;&#35823;&#65292;&#25552;&#20379;&#20102;&#32416;&#27491;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.11092</link><description>&lt;p&gt;
&#32763;&#35793;&#22256;&#22659;&#65311;&#36328;&#35821;&#35328;&#27010;&#24565;&#19978;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20844;&#24179;&#35780;&#20272;&#30340;&#32763;&#35793;&#38169;&#35823;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22522;&#20934;&#20013;&#23384;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#26085;&#35821;&#21644;&#20013;&#25991;&#20013;&#19981;&#21516;&#31243;&#24230;&#30340;&#32763;&#35793;&#38169;&#35823;&#65292;&#25552;&#20379;&#20102;&#32416;&#27491;&#65292;&#24182;&#20998;&#26512;&#20102;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#27010;&#24565;&#21015;&#34920;&#32763;&#35793;&#25104;&#19971;&#31181;&#35821;&#35328;&#65292;&#23558;&#21050;&#28608;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#39044;&#26399;&#30340;&#22270;&#20687;&#20998;&#24067;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#22522;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#22522;&#20934;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#26085;&#35821;&#21644;&#20013;&#25991;&#20013;&#23384;&#22312;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#32763;&#35793;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#38169;&#35823;&#30340;&#26356;&#27491;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#23545;CoCo-CroLa&#20316;&#20026;&#22522;&#20934;&#30340;&#25928;&#29992;&#21644;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#35746;&#21518;&#37325;&#26032;&#35780;&#20272;&#20102;&#22810;&#20010;&#22522;&#32447;T2I&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#22312;&#26032;&#32763;&#35793;&#19979;&#24341;&#21457;&#30340;&#36755;&#20986;&#19982;&#22312;&#26087;&#32763;&#35793;&#19979;&#24341;&#21457;&#30340;&#36755;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#20462;&#27491;&#23545;&#22270;&#20687;&#39046;&#22495;&#22522;&#20934;&#32467;&#26524;&#30340;&#24433;&#21709;&#31243;&#24230;&#26159;&#21487;&#20197;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11092v1 Announce Type: cross  Abstract: Benchmarks of the multilingual capabilities of text-to-image (T2I) models compare generated images prompted in a test language to an expected image distribution over a concept set. One such benchmark, "Conceptual Coverage Across Languages" (CoCo-CroLa), assesses the tangible noun inventory of T2I models by prompting them to generate pictures from a concept list translated to seven languages and comparing the output image populations. Unfortunately, we find that this benchmark contains translation errors of varying severity in Spanish, Japanese, and Chinese. We provide corrections for these errors and analyze how impactful they are on the utility and validity of CoCo-CroLa as a benchmark. We reassess multiple baseline T2I models with the revisions, compare the outputs elicited under the new translations to those conditioned on the old, and show that a correction's impactfulness on the image-domain benchmark results can be predicted in t
&lt;/p&gt;</description></item><item><title>RobustSentEmbed&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#25991;&#26412;&#34920;&#31034;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.11082</link><description>&lt;p&gt;
RobustSentEmbed&#65306;&#20351;&#29992;&#23545;&#25239;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#31283;&#20581;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11082
&lt;/p&gt;
&lt;p&gt;
RobustSentEmbed&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#25991;&#26412;&#34920;&#31034;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#22522;&#20110;PLM&#30340;&#34920;&#31034;&#36890;&#24120;&#22312;&#23545;&#25239;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;RobustSentEmbed&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#20197;&#21450;&#23545;&#25239;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#29983;&#25104;&#39640;&#39118;&#38505;&#30340;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#29992;&#20110;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;RobustSentEmbed&#24039;&#22937;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#21644;&#31283;&#20581;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;RobustSentEmbed&#20248;&#20110;&#26368;&#20808;&#36827;&#34920;&#31034;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#38477;&#20302;&#20102;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#38477;&#20302;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11082v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing
&lt;/p&gt;</description></item><item><title>GOMA&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#30340;&#24515;&#26234;&#23545;&#40784;&#30340;&#21512;&#20316;&#27807;&#36890;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26234;&#33021;&#20307;&#24515;&#26234;&#29366;&#24577;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#24110;&#21161;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.11075</link><description>&lt;p&gt;
GOMA&#65306;&#36890;&#36807;&#38754;&#21521;&#30446;&#26631;&#30340;&#24515;&#26234;&#23545;&#40784;&#23454;&#29616;&#20027;&#21160;&#21512;&#20316;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11075
&lt;/p&gt;
&lt;p&gt;
GOMA&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#30340;&#24515;&#26234;&#23545;&#40784;&#30340;&#21512;&#20316;&#27807;&#36890;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26234;&#33021;&#20307;&#24515;&#26234;&#29366;&#24577;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#24110;&#21161;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#22836;&#20132;&#27969;&#22312;&#20154;&#31867;&#21512;&#20316;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#21512;&#20316;&#20249;&#20276;&#21482;&#23545;&#20219;&#21153;&#12289;&#29615;&#22659;&#21644;&#24444;&#27492;&#30340;&#24515;&#29702;&#29366;&#24577;&#20855;&#26377;&#19981;&#23436;&#25972;&#30340;&#20449;&#24687;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#20316;&#27807;&#36890;&#26694;&#26550;&#65292;&#21363;&#38754;&#21521;&#30446;&#26631;&#30340;&#24515;&#26234;&#23545;&#40784;&#65288;GOMA&#65289;&#12290;GOMA&#23558;&#21475;&#22836;&#20132;&#27969;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#35268;&#21010;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#26234;&#33021;&#20307;&#24515;&#26234;&#29366;&#24577;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#20419;&#36827;&#21512;&#20316;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#19968;&#20010;&#20855;&#26377;&#36523;&#20307;&#30340;&#21161;&#25163;&#33021;&#22815;&#25512;&#29702;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20197;&#33258;&#28982;&#35821;&#35328;&#20027;&#21160;&#24320;&#22987;&#19982;&#20154;&#31867;&#30340;&#21475;&#22836;&#27807;&#36890;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#20316;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;Overcooked&#65288;&#19968;&#27454;&#22810;&#20154;&#28216;&#25103;&#65289;&#21644;VirtualHome&#65288;&#19968;&#20010;&#23478;&#24237;&#27169;&#25311;&#22120;&#65289;&#20013;&#65292;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;&#35821;&#22659;&#30340;&#26377;&#24847;&#20041;&#27807;&#36890;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11075v1 Announce Type: cross  Abstract: Verbal communication plays a crucial role in human cooperation, particularly when the partners only have incomplete information about the task, environment, and each other's mental state. In this paper, we propose a novel cooperative communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates verbal communication as a planning problem that minimizes the misalignment between the parts of agents' mental states that are relevant to the goals. This approach enables an embodied assistant to reason about when and how to proactively initialize communication with humans verbally using natural language to help achieve better cooperation. We evaluate our approach against strong baselines in two challenging environments, Overcooked (a multiplayer game) and VirtualHome (a household simulator). Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#37051;&#36817;&#24103;&#21644;&#36828;&#36317;&#24103;&#26469;&#23454;&#29616;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;</title><link>https://arxiv.org/abs/2403.11074</link><description>&lt;p&gt;
&#36890;&#36807;&#26410;&#26631;&#35760;&#24103;&#21033;&#29992;&#23454;&#29616;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Segmentation via Unlabeled Frame Exploitation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11074
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#37051;&#36817;&#24103;&#21644;&#36828;&#36317;&#24103;&#26469;&#23454;&#29616;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#65288;AVS&#65289;&#26088;&#22312;&#20998;&#21106;&#35270;&#39057;&#24103;&#20013;&#30340;&#21457;&#20986;&#22768;&#38899;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#24403;&#21069;&#26041;&#27861;&#22312;&#20351;&#29992;&#26410;&#26631;&#35760;&#24103;&#26102;&#21482;&#23454;&#29616;&#20102;&#36793;&#38469;&#24615;&#33021;&#25552;&#21319;&#65292;&#23548;&#33268;&#20102;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20805;&#20998;&#25366;&#25496;&#26410;&#26631;&#35760;&#24103;&#22312;AVS&#20013;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#26126;&#30830;&#23558;&#23427;&#20204;&#22522;&#20110;&#20854;&#26102;&#38388;&#29305;&#24449;&#20998;&#20026;&#20004;&#31867;&#65292;&#21363;&#37051;&#36817;&#24103;&#65288;NF&#65289;&#21644;&#36828;&#36317;&#24103;&#65288;DF&#65289;&#12290;NF&#65292;&#19982;&#26631;&#35760;&#24103;&#22312;&#26102;&#38388;&#19978;&#30456;&#37051;&#65292;&#36890;&#24120;&#21253;&#21547;&#20016;&#23500;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#20934;&#30830;&#23450;&#20301;&#21457;&#22768;&#23545;&#35937;&#12290;&#19982;NF&#30456;&#21453;&#65292;DF&#19982;&#26631;&#35760;&#24103;&#20043;&#38388;&#23384;&#22312;&#24456;&#38271;&#30340;&#26102;&#38388;&#38388;&#38548;&#65292;&#20854;&#20855;&#26377;&#22806;&#35266;&#21464;&#21270;&#20294;&#20849;&#20139;&#35821;&#20041;&#30456;&#20284;&#23545;&#35937;&#12290;&#32771;&#34385;&#21040;&#23427;&#20204;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#26469;&#22788;&#29702;AVS&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;NF&#65292;&#25105;&#20204;&#21033;&#29992;&#36816;&#21160;&#32447;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11074v1 Announce Type: cross  Abstract: Audio-visual segmentation (AVS) aims to segment the sounding objects in video frames. Although great progress has been witnessed, we experimentally reveal that current methods reach marginal performance gain within the use of the unlabeled frames, leading to the underutilization issue. To fully explore the potential of the unlabeled frames for AVS, we explicitly divide them into two categories based on their temporal characteristics, i.e., neighboring frame (NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame, often contain rich motion information that assists in the accurate localization of sounding objects. Contrary to NFs, DFs have long temporal distances from the labeled frame, which share semantic-similar objects with appearance variations. Considering their unique characteristics, we propose a versatile framework that effectively leverages them to tackle AVS. Specifically, for NFs, we exploit the motion cues
&lt;/p&gt;</description></item><item><title>Tokensome&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26579;&#33394;&#20307;&#26631;&#35760;&#30340;&#21019;&#26032;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22788;&#29702;&#26680;&#22411;&#38382;&#39064;&#20174;&#20256;&#32479;&#30340;&#35270;&#35273;&#24863;&#30693;&#23618;&#21319;&#32423;&#21040;&#35748;&#30693;&#20915;&#31574;&#23618;&#65292;&#36890;&#36807;&#25972;&#21512;&#30693;&#35782;&#22270;&#21644;LLM&#65292;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20419;&#36827;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.11073</link><description>&lt;p&gt;
Tokensome&#65306;&#38754;&#21521;&#21487;&#35299;&#37322;&#21644;&#35748;&#30693;&#26680;&#22411;&#30340;&#22522;&#22240;&#35270;&#35273;&#35821;&#35328;GPT
&lt;/p&gt;
&lt;p&gt;
Tokensome: Towards a Genetic Vision-Language GPT for Explainable and Cognitive Karyotyping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11073
&lt;/p&gt;
&lt;p&gt;
Tokensome&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26579;&#33394;&#20307;&#26631;&#35760;&#30340;&#21019;&#26032;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22788;&#29702;&#26680;&#22411;&#38382;&#39064;&#20174;&#20256;&#32479;&#30340;&#35270;&#35273;&#24863;&#30693;&#23618;&#21319;&#32423;&#21040;&#35748;&#30693;&#20915;&#31574;&#23618;&#65292;&#36890;&#36807;&#25972;&#21512;&#30693;&#35782;&#22270;&#21644;LLM&#65292;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20419;&#36827;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26680;&#22411;&#20998;&#26512;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#19968;&#39033;&#19987;&#27880;&#20110;&#26579;&#33394;&#20307;&#23545;&#35937;&#32423;&#24314;&#27169;&#30340;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#65292;&#35813;&#23450;&#20041;&#23548;&#33268;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#32452;&#20214;&#21644;&#25972;&#20307;&#20449;&#24687;&#65292;&#26174;&#33879;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#25216;&#26415;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#20020;&#24202;&#30340;&#37319;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Tokensome&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26579;&#33394;&#20307;&#26631;&#35760;&#30340;&#26032;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#35748;&#30693;&#26680;&#22411;&#12290;Tokensome&#23558;&#26041;&#27861;&#20174;&#20256;&#32479;&#30340;&#35270;&#35273;&#24863;&#30693;&#23618;&#25552;&#21319;&#21040;&#35748;&#30693;&#20915;&#31574;&#23618;&#12290;&#36825;&#31181;&#25552;&#21319;&#36890;&#36807;&#30693;&#35782;&#22270;&#21644;LLM&#65292;&#20351;&#39046;&#22495;&#30693;&#35782;&#21644;&#35748;&#30693;&#25512;&#29702;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#24182;&#20419;&#36827;&#20102;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11073v1 Announce Type: cross  Abstract: Automatic karyotype analysis is often defined as a visual perception task focused solely on chromosomal object-level modeling. This definition has led most existing methods to overlook componential and holistic information, significantly constraining model performance. Moreover, the lack of interpretability in current technologies hinders clinical adoption. In this paper, we introduce Tokensome, a novel vision-language model based on chromosome tokenization for explainable and cognitive karyotyping. Tokensome elevates the method from the conventional visual perception layer to the cognitive decision-making layer. This elevation enables the integration of domain knowledge and cognitive reasoning via knowledge graphs and LLMs, markedly enhancing model's explainability and facilitating abnormality detection.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#35889;&#22270;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.11047</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#39044;&#27979;&#65306;&#35889;&#22270;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#35889;&#22270;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#20063;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#36890;&#24120;&#37319;&#29992;&#32447;&#22270;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#26102;&#38388;-&#39057;&#29575;&#35889;&#22270;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#32479;&#35745;&#22522;&#32447;&#65288;EMA&#21644;ARIMA&#65289;&#12289;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;DeepAR&#65289;&#12289;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35270;&#35273;&#34920;&#31034;&#65288;&#32447;&#22270;&#20687;&#65289;&#20197;&#21450;&#20165;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#30340;&#28040;&#34701;&#30740;&#31350;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#21033;&#29992;&#35889;&#22270;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#37325;&#35201;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11047v1 Announce Type: cross  Abstract: Time series forecasting plays a crucial role in decision-making across various domains, but it presents significant challenges. Recent studies have explored image-driven approaches using computer vision models to address these challenges, often employing lineplots as the visual representation of time series data. In this paper, we propose a novel approach that uses time-frequency spectrograms as the visual representation of time series data. We introduce the use of a vision transformer for multimodal learning, showcasing the advantages of our approach across diverse datasets from different domains. To evaluate its effectiveness, we compare our method against statistical baselines (EMA and ARIMA), a state-of-the-art deep learning-based approach (DeepAR), other visual representations of time series data (lineplot images), and an ablation study on using only the time series as input. Our experiments demonstrate the benefits of utilizing s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#20449;&#24687;&#38388;&#31454;&#20105;&#65292;&#25552;&#20986;&#21033;&#29992;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#20316;&#20026;&#26377;&#25928;&#20943;&#36731;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#24230;&#35880;&#24910;&#65292;&#38656;&#37325;&#26032;&#35780;&#20272;&#30417;&#31649;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11046</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#38388;&#31454;&#20105;&#35843;&#25511;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Regulating Chatbot Output via Inter-Informational Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#20449;&#24687;&#38388;&#31454;&#20105;&#65292;&#25552;&#20986;&#21033;&#29992;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#20316;&#20026;&#26377;&#25928;&#20943;&#36731;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#24230;&#35880;&#24910;&#65292;&#38656;&#37325;&#26032;&#35780;&#20272;&#30417;&#31649;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#19968;&#24180;&#22810;&#30340;&#30417;&#31649;&#29378;&#28526;&#12290;&#28982;&#32780;&#65292;&#23569;&#25968;&#29616;&#26377;&#30740;&#31350;&#20005;&#26684;&#36136;&#30097;&#20102;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65306;&#22914;&#26524;&#19981;&#32463;&#35268;&#33539;&#65292;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#36755;&#20986;&#20250;&#23545;&#20154;&#31867;&#20107;&#21153;&#36896;&#25104;&#23454;&#36136;&#19988;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#20851;&#38190;&#21487;&#33021;&#24615;&#65292;&#24182;&#22240;&#27492;&#20542;&#21521;&#20110;&#20351;&#29992;&#30417;&#31649;&#24037;&#20855;&#30452;&#25509;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;&#21508;&#31181;&#28192;&#36947;&#20043;&#38388;&#30340;&#20449;&#24687;&#31454;&#20105;&#65292;&#21457;&#23637;&#20102;&#19968;&#20010;&#37325;&#26032;&#35780;&#20272;AI&#30456;&#20851;&#20869;&#23481;&#39118;&#38505;&#21644;&#30456;&#24212;&#30417;&#31649;&#25552;&#35758;&#30340;&#26631;&#20934;&#12290;&#38271;&#36798;&#25968;&#21313;&#24180;&#30340;&#20449;&#24687;&#21644;&#36890;&#20449;&#25216;&#26415;&#30417;&#31649;&#21490;&#34920;&#26126;&#65292;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#20110;&#35880;&#24910;&#65292;&#24182;&#22312;&#25552;&#20986;&#36807;&#24230;&#30340;&#30417;&#31649;&#25514;&#26045;&#26102;&#29359;&#38169;&#35823;&#12290;&#20107;&#23454;&#19978;&#65292;&#20016;&#23500;&#30340;&#32463;&#39564;&#25968;&#25454;&#25903;&#25345;&#20102;&#20449;&#24687;&#24066;&#22330;&#26426;&#21046;&#22312;&#20449;&#24687;&#30417;&#31649;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11046v1 Announce Type: cross  Abstract: The advent of ChatGPT has sparked over a year of regulatory frenzy. However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI chatbot's output would inflict tangible, severe real harm on human affairs. Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly. This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets. The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies. In fact, a trove of empiric
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24341;&#23548;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LCD&#36807;&#31243;&#20013;&#25972;&#21512;&#22870;&#21169;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#39640;&#20445;&#30495;&#22270;&#20687;&#29983;&#25104;&#26102;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.11027</link><description>&lt;p&gt;
&#22870;&#21169;&#24341;&#23548;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Reward Guided Latent Consistency Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24341;&#23548;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LCD&#36807;&#31243;&#20013;&#25972;&#21512;&#22870;&#21169;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#39640;&#20445;&#30495;&#22270;&#20687;&#29983;&#25104;&#26102;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;(LCD)&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#33539;&#24335;&#12290;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#20013;&#33976;&#39311;&#20986;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;(LCM)&#65292;LCD&#22312;&#20165;&#38656;2&#21040;4&#20010;&#25512;&#29702;&#27493;&#39588;&#20869;&#20419;&#36827;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;LCM&#30340;&#39640;&#25928;&#25512;&#29702;&#26159;&#20197;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;LCM&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#26469;&#34917;&#20607;&#36136;&#37327;&#25439;&#22833;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#22870;&#21169;&#24341;&#23548;&#30340;LCD(RG-LCD)&#65292;&#36890;&#36807;&#23558;&#22870;&#21169;&#27169;&#22411;(RM)&#30340;&#21453;&#39304;&#25972;&#21512;&#21040;LCD&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;LCD&#25439;&#22833;&#19982;&#26368;&#22823;&#21270;&#19982;LCM&#21333;&#27493;&#29983;&#25104;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#30340;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#26469;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#65292;&#24403;&#20351;&#29992;&#33391;&#22909;RM&#30340;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;RG-LCM&#30340;2&#27493;&#29983;&#25104;&#34987;&#20154;&#31867;&#38738;&#30544;&#65292;&#36229;&#36807;&#20102;50&#27493;DDIM&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11027v1 Announce Type: cross  Abstract: Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficient text-to-image synthesis. By distilling a latent consistency model (LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates the generation of high-fidelity images within merely 2 to 4 inference steps. However, the LCM's efficient inference is obtained at the cost of the sample quality. In this paper, we propose compensating the quality loss by aligning LCM's output with human preference during training. Specifically, we introduce Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) into the LCD process by augmenting the original LCD loss with the objective of maximizing the reward associated with LCM's single-step generation. As validated through human evaluation, when trained with the feedback of a good RM, the 2-step generations from our RG-LCM are favored by humans over the 50-step DDIM samples from 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.11021</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Video Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#35270;&#39057;&#25968;&#25454;&#29983;&#20135;&#30340;&#31354;&#21069;&#28608;&#22686;&#38656;&#27714;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#24103;&#20379;&#19979;&#28216;&#20219;&#21153;&#20351;&#29992;&#12290; &#38271;&#26399;&#26102;&#38388;&#25512;&#29702;&#26159;&#24103;&#26816;&#32034;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#12290; &#34429;&#28982; VideoLLaMA &#21644; ViCLIP &#31561;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#30701;&#26399;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#24103;&#30340;&#38271;&#26399;&#25512;&#29702;&#26041;&#38754;&#21364;&#20196;&#20154;&#24778;&#35766;&#22320;&#22833;&#36133;&#12290; &#36825;&#31181;&#22833;&#36133;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#23427;&#20204;&#23558;&#36880;&#24103;&#24863;&#30693;&#21644;&#26102;&#38388;&#25512;&#29702;&#20132;&#32455;&#25104;&#21333;&#20010;&#28145;&#24230;&#32593;&#32476;&#12290; &#22240;&#27492;&#65292;&#35299;&#32806;&#20294;&#20849;&#21516;&#35774;&#35745;&#35821;&#20041;&#29702;&#35299;&#21644;&#26102;&#38388;&#25512;&#29702;&#23545;&#20110;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#20010;&#24103;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#20294;&#26377;&#25928;&#22320;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#65288;TL&#65289;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20123;&#20844;&#24335;&#22312;&#26412;&#36136;&#19978;&#25429;&#25417;&#20102;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11021v1 Announce Type: cross  Abstract: The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal reasoning is a key desideratum for frame retrieval systems. While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames. A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based reas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Zadeh&#35745;&#31639;&#19982;&#35789;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#23384;&#22312;&#24040;&#22823;&#19981;&#30830;&#23450;&#24615;&#26102;&#20174;&#26102;&#38388;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#35782;&#21035;&#21560;&#24341;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.11015</link><description>&lt;p&gt;
&#20174;&#34920;&#36798;&#25968;&#25454;&#20013;&#35782;&#21035;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#21560;&#24341;&#23376;&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying the Attractors of Gene Regulatory Networks from Expression Data under Uncertainty: An Interpretable Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Zadeh&#35745;&#31639;&#19982;&#35789;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#23384;&#22312;&#24040;&#22823;&#19981;&#30830;&#23450;&#24615;&#26102;&#20174;&#26102;&#38388;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#35782;&#21035;&#21560;&#24341;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31995;&#32479;&#29983;&#29289;&#23398;&#20013;&#65292;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#21560;&#24341;&#23376;&#26223;&#35266;&#20998;&#26512;&#34987;&#35748;&#20026;&#26159;&#30740;&#31350;&#20174;&#22686;&#27542;&#21644;&#20998;&#21270;&#21040;&#34928;&#32769;&#21644;&#20939;&#20129;&#31561;&#21508;&#31181;&#32454;&#32990;&#29366;&#24577;&#30340;&#24378;&#22823;&#35745;&#31639;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#35782;&#21035;&#21560;&#24341;&#23376;&#22312;&#30830;&#23450;&#32454;&#32990;&#21629;&#36816;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Zadeh&#35745;&#31639;&#19982;&#35789;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11015v1 Announce Type: cross  Abstract: In systems biology, attractor landscape analysis of gene regulatory networks is recognized as a powerful computational tool for studying various cellular states from proliferation and differentiation to senescence and apoptosis. Therefore, accurate identification of attractors plays a critical role in determination of the cell fates. On the other hand, in a real biological circuit, genetic/epigenetic alterations as well as varying environmental factors drastically take effect on the location, characteristics, and even the number of attractors. The central question is: Given a temporal gene expression profile of a real gene regulatory network, how can the attractors be robustly identified in the presence of huge amount of uncertainty? This paper addresses this question using a novel approach based on Zadeh Computing with Words. The proposed scheme could effectively identify the attractors from temporal gene expression data in terms of b
&lt;/p&gt;</description></item><item><title>DIALECTBENCH&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.11009</link><description>&lt;p&gt;
DIALECTBENCH&#65306;&#19968;&#20010;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11009
&lt;/p&gt;
&lt;p&gt;
DIALECTBENCH&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11009v1 &#35821;&#31181;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#35821;&#35328;&#25216;&#26415;&#24212;&#35813;&#26681;&#25454;&#20854;&#22312;&#23454;&#38469;&#29992;&#20363;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#21028;&#26029;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#21644;&#35780;&#20272;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#38750;&#26631;&#20934;&#26041;&#35328;&#25110;&#35821;&#35328;&#21464;&#20307;&#65288;&#20197;&#19979;&#31616;&#31216;&#20026;&#21464;&#20307;&#65289;&#24418;&#24335;&#30340;&#35821;&#35328;&#21464;&#20307;. &#22823;&#22810;&#25968;NLP&#22522;&#20934;&#27979;&#35797;&#20165;&#38480;&#20110;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIALECTBENCH&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#35821;&#35328;&#21464;&#20307;&#30340;&#22823;&#35268;&#27169;NLP&#22522;&#20934;&#27979;&#35797;&#65292;&#27719;&#24635;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#20219;&#21153;&#26679;&#26412;&#30340;&#21464;&#20307;&#25968;&#25454;&#38598;&#65288;&#28085;&#30422;281&#31181;&#21464;&#20307;&#30340;10&#20010;&#25991;&#26412;&#32423;&#20219;&#21153;&#65289;&#12290;&#36825;&#20801;&#35768;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#19982;&#38750;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#20043;&#38388;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#22312;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22823;&#37327;&#24615;&#33021;&#24046;&#36317;&#30340;&#35821;&#35328;&#31867;&#32676;&#12290;&#25105;&#20204;&#35748;&#20026;DIALECTBENCH&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#35821;&#35328;NLP&#29366;&#24577;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11009v1 Announce Type: cross  Abstract: Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for langua
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.10997</link><description>&lt;p&gt;
&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#23618;&#27425;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10997
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29702;&#35299;&#22810;&#23618;&#25277;&#35937;&#30340;&#22797;&#26434;&#22330;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330; (N2F2)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#30417;&#30563;&#26469;&#23398;&#20064;&#21333;&#20010;&#29305;&#24449;&#22330;&#65292;&#22312;&#21516;&#19968;&#39640;&#32500;&#29305;&#24449;&#20013;&#30340;&#19981;&#21516;&#32500;&#24230;&#32534;&#30721;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#28789;&#27963;&#23450;&#20041;&#23618;&#27425;&#65292;&#21487;&#20197;&#26681;&#25454;&#29289;&#29702;&#32500;&#24230;&#12289;&#35821;&#20041;&#32500;&#24230;&#25110;&#20004;&#32773;&#22343;&#21305;&#37197;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22330;&#26223;&#30340;&#20840;&#38754;&#21644;&#32454;&#33268;&#29702;&#35299;&#12290;&#25105;&#20204;&#21033;&#29992;2D&#31867;&#21035;&#26080;&#20851;&#20998;&#21106;&#27169;&#22411;&#22312;&#22270;&#20687;&#31354;&#38388;&#30340;&#20219;&#24847;&#23610;&#24230;&#25552;&#20379;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20687;&#32032;&#20998;&#32452;&#65292;&#24182;&#26597;&#35810;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#20026;&#36825;&#20123;&#27573;&#33853;&#20013;&#30340;&#27599;&#20010;&#37096;&#20998;&#33719;&#24471;&#19982;&#35821;&#35328;&#23545;&#40784;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#23618;&#30417;&#30563;&#26041;&#27861;&#23558;&#19981;&#21516;&#30340;&#23884;&#22871;&#29305;&#24449;&#22330;&#32500;&#24230;&#20998;&#37197;&#32473;&#25552;&#21462;C
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10997v1 Announce Type: cross  Abstract: Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the C
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;GNN&#35757;&#32451;&#31639;&#27861;Eclipse&#65292;&#36890;&#36807;&#35266;&#23519;&#22270;&#32467;&#26500;&#20013;&#37051;&#25509;&#30697;&#38453;&#30340;&#20302;&#31209;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25252;&#36793;&#32536;&#38544;&#31169;&#30340;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33391;&#22909;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10995</link><description>&lt;p&gt;
&#20855;&#26377;&#22855;&#24322;&#20540;&#25200;&#21160;&#30340;&#36793;&#32536;&#31169;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Edge Private Graph Neural Networks with Singular Value Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;GNN&#35757;&#32451;&#31639;&#27861;Eclipse&#65292;&#36890;&#36807;&#35266;&#23519;&#22270;&#32467;&#26500;&#20013;&#37051;&#25509;&#30697;&#38453;&#30340;&#20302;&#31209;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25252;&#36793;&#32536;&#38544;&#31169;&#30340;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33391;&#22909;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#20174;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;GNN&#35757;&#32451;&#27969;&#31243;&#24050;&#34987;&#26174;&#31034;&#23481;&#26131;&#21463;&#21040;&#33410;&#28857;&#29305;&#24449;&#27844;&#28431;&#21644;&#36793;&#25552;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24773;&#26223;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#20174;&#35757;&#32451;&#36807;&#30340;GNN&#27169;&#22411;&#20013;&#24674;&#22797;&#31169;&#26377;&#36793;&#32536;&#20449;&#24687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30452;&#25509;&#21521;&#37051;&#25509;&#30697;&#38453;&#25110;&#32039;&#20945;&#22270;&#34920;&#31034;&#28155;&#21152;&#22122;&#22768;&#12290;&#28155;&#21152;&#30340;&#25200;&#21160;&#23548;&#33268;&#22270;&#32467;&#26500;&#34987;&#22823;&#24133;&#25913;&#21464;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;GNN&#35757;&#32451;&#31639;&#27861;Eclipse&#65292;&#35813;&#31639;&#27861;&#22312;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;Eclipse&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#12290;&#31532;&#19968;&#65292;&#22270;&#32467;&#26500;&#20013;&#30340;&#37051;&#25509;&#30697;&#38453;&#34920;&#29616;&#20986;&#20302;&#31209;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;Eclipse&#20351;&#29992;&#20302;&#31209;&#30340;&#26041;&#24335;&#35757;&#32451;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10995v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) play a key role in learning representations from graph-structured data and are demonstrated to be useful in many applications. However, the GNN training pipeline has been shown to be vulnerable to node feature leakage and edge extraction attacks. This paper investigates a scenario where an attacker aims to recover private edge information from a trained GNN model. Previous studies have employed differential privacy (DP) to add noise directly to the adjacency matrix or a compact graph representation. The added perturbations cause the graph structure to be substantially morphed, reducing the model utility. We propose a new privacy-preserving GNN training algorithm, Eclipse, that maintains good model utility while providing strong privacy protection on edges. Eclipse is based on two key observations. First, adjacency matrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains GNNs with a low-r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;&#36229;&#20998;&#36776;&#27169;&#22411;&#20013;&#30340;&#32593;&#26684;&#20266;&#24433;&#12289;&#36870;&#30697;&#38453;&#29190;&#28856;&#21644;&#27425;&#20248;&#32467;&#26524;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10988</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#25552;&#21319;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#36229;&#20998;&#36776;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Boosting Flow-based Generative Super-Resolution Models via Learned Prior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10988
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;&#36229;&#20998;&#36776;&#27169;&#22411;&#20013;&#30340;&#32593;&#26684;&#20266;&#24433;&#12289;&#36870;&#30697;&#38453;&#29190;&#28856;&#21644;&#27425;&#20248;&#32467;&#26524;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#36229;&#20998;&#36776;&#65288;SR&#65289;&#27169;&#22411;&#23637;&#29616;&#20102;&#22312;&#29983;&#25104;&#21151;&#33021;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#36935;&#21040;&#20102;&#20960;&#20010;&#25361;&#25112;&#65292;&#27604;&#22914;&#32593;&#26684;&#20266;&#24433;&#12289;&#36870;&#30697;&#38453;&#29190;&#28856;&#21644;&#30001;&#20110;&#22266;&#23450;&#30340;&#37319;&#26679;&#28201;&#24230;&#23548;&#33268;&#30340;&#27425;&#20248;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#22522;&#20110;&#27969;&#30340;SR&#27169;&#22411;&#30340;&#25512;&#26029;&#38454;&#27573;&#24341;&#20837;&#20102;&#26465;&#20214;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20010;&#20808;&#39564;&#30693;&#35782;&#26159;&#30001;&#25105;&#20204;&#25552;&#20986;&#30340;&#28508;&#22312;&#27169;&#22359;&#26681;&#25454;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#39044;&#27979;&#20986;&#30340;&#28508;&#22312;&#20195;&#30721;&#65292;&#28982;&#21518;&#36890;&#36807;&#27969;&#27169;&#22411;&#36716;&#25442;&#20026;SR&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#26080;&#32541;&#22320;&#19982;&#20219;&#20309;&#29616;&#20195;&#22522;&#20110;&#27969;&#30340;SR&#27169;&#22411;&#38598;&#25104;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#20854;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#26435;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#20998;&#26512;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;SR&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#22266;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10988v1 Announce Type: cross  Abstract: Flow-based super-resolution (SR) models have demonstrated astonishing capabilities in generating high-quality images. However, these methods encounter several challenges during image generation, such as grid artifacts, exploding inverses, and suboptimal results due to a fixed sampling temperature. To overcome these issues, this work introduces a conditional learned prior to the inference phase of a flow-based SR model. This prior is a latent code predicted by our proposed latent module conditioned on the low-resolution image, which is then transformed by the flow model into an SR image. Our framework is designed to seamlessly integrate with any contemporary flow-based SR model without modifying its architecture or pre-trained weights. We evaluate the effectiveness of our proposed framework through extensive experiments and ablation analyses. The proposed framework successfully addresses all the inherent issues in flow-based SR models a
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\carb &#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;-&#21551;&#29992;&#28145;&#24230;&#23398;&#20064;&#20013;&#31934;&#30830;&#20272;&#31639;&#30899;&#36275;&#36857;&#65292;&#23637;&#31034;&#20102;&#19982;&#23454;&#38469;&#27979;&#37327;&#20540;&#30456;&#27604;&#26368;&#22823;$\pm21\%$&#30340;&#30899;&#36275;&#36857;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.10984</link><description>&lt;p&gt;
IoTCO2&#65306;&#35780;&#20272;&#29289;&#32852;&#32593;-&#21551;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10984
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\carb &#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;-&#21551;&#29992;&#28145;&#24230;&#23398;&#20064;&#20013;&#31934;&#30830;&#20272;&#31639;&#30899;&#36275;&#36857;&#65292;&#23637;&#31034;&#20102;&#19982;&#23454;&#38469;&#27979;&#37327;&#20540;&#30456;&#27604;&#26368;&#22823;$\pm21\%$&#30340;&#30899;&#36275;&#36857;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#38544;&#31169;&#24615;&#21644;&#30830;&#20445;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#19978;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#65292;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#19982;IoT&#19978;DL&#30456;&#20851;&#30340;&#30899;&#36275;&#36857;&#65292;&#28085;&#30422;&#20102;&#25805;&#20316;&#21644;&#23454;&#20307;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#25805;&#20316;&#33021;&#37327;&#39044;&#27979;&#22120;&#32463;&#24120;&#24573;&#30053;&#20102;&#37327;&#21270;&#30340;DL&#27169;&#22411;&#21644;&#26032;&#20852;&#30340;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#65288;NPUs&#65289;&#65292;&#32780;&#23454;&#20307;&#30899;&#36275;&#36857;&#24314;&#27169;&#24037;&#20855;&#24573;&#30053;&#20102;IoT&#35774;&#22791;&#20013;&#24120;&#35265;&#30340;&#38750;&#35745;&#31639;&#30828;&#20214;&#32452;&#20214;&#65292;&#23548;&#33268;&#20102;&#29289;&#32852;&#32593;DL&#20934;&#30830;&#30899;&#36275;&#36857;&#24314;&#27169;&#24037;&#20855;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;\textit{\carb}&#65292;&#19968;&#31181;&#29992;&#20110;&#31934;&#30830;&#20272;&#31639;&#29289;&#32852;&#32593;DL&#20013;&#30899;&#36275;&#36857;&#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#24037;&#20855;&#65292;&#23637;&#31034;&#20102;&#19982;&#21508;&#31181;DL&#27169;&#22411;&#30340;&#23454;&#38469;&#27979;&#37327;&#20540;&#30456;&#27604;&#26368;&#22823;$\pm21\%$&#30340;&#30899;&#36275;&#36857;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;\carb&#30340;&#23454;&#38469;&#24212;&#29992;&#36890;&#36807;&#22810;&#20010;&#29992;&#25143;&#26696;&#20363;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10984v1 Announce Type: cross  Abstract: To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook quantized DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \textit{\carb}, an end-to-end modeling tool for precise carbon footprint estimation in IoT-enabled DL, demonstrating a maximum $\pm21\%$ deviation in carbon footprint values compared to actual measurements across various DL models. Additionally, practical applications of \carb are showcased through multiple user case studies.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38598;&#20307;&#26234;&#24935;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#22686;&#24378;&#29289;&#32852;&#32593;&#23545;&#25239;DDoS&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10968</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22686;&#24378;&#23545;&#25239; DDoS &#25915;&#20987;&#30340;&#29289;&#32852;&#32593;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Enhancing IoT Security Against DDoS Attacks through Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10968
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38598;&#20307;&#26234;&#24935;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#22686;&#24378;&#29289;&#32852;&#32593;&#23545;&#25239;DDoS&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#36805;&#36895;&#26222;&#21450;&#24341;&#20837;&#20102;&#29289;&#29702;&#35774;&#22791;&#19982;&#25968;&#23383;&#19990;&#30028;&#20043;&#38388;&#30340;&#21464;&#38761;&#24615;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#19981;&#26029;&#21319;&#32423;&#65292;&#21361;&#21450;&#29289;&#32852;&#32593;&#32593;&#32476;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#20256;&#32479;&#30340;DDoS&#32531;&#35299;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#33021; compromise &#25968;&#25454;&#38544;&#31169;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#22686;&#24378;&#29289;&#32852;&#32593;&#32593;&#32476;&#23545;&#25239;DDoS&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#65292;&#20801;&#35768;&#22810;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#25110;&#36793;&#32536;&#33410;&#28857;&#21327;&#20316;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#32852;&#37030;&#23398;&#20064;&#22312;&#26816;&#27979;&#21644;&#32531;&#35299;&#29289;&#32852;&#32593;&#20013;DDoS&#25915;&#20987;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38598;&#20307;&#26234;&#24935;&#36827;&#34892;&#23454;&#26102;&#25915;&#20987;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10968v1 Announce Type: cross  Abstract: The rapid proliferation of the Internet of Things (IoT) has ushered in transformative connectivity between physical devices and the digital realm. Nonetheless, the escalating threat of Distributed Denial of Service (DDoS) attacks jeopardizes the integrity and reliability of IoT networks. Conventional DDoS mitigation approaches are ill-equipped to handle the intricacies of IoT ecosystems, potentially compromising data privacy. This paper introduces an innovative strategy to bolster the security of IoT networks against DDoS attacks by harnessing the power of Federated Learning that allows multiple IoT devices or edge nodes to collaboratively build a global model while preserving data privacy and minimizing communication overhead. The research aims to investigate Federated Learning's effectiveness in detecting and mitigating DDoS attacks in IoT. Our proposed framework leverages IoT devices' collective intelligence for real-time attack det
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#19979;&#30340;&#38646;&#26679;&#28857;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10967</link><description>&lt;p&gt;
&#26790;&#24819;&#20013;&#30340;&#35768;&#22810;&#19990;&#30028;&#65306;&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#38646;&#26679;&#28857;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10967
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#19979;&#30340;&#38646;&#26679;&#28857;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#28857;&#27867;&#21270;&#65288;Zero-shot generalization&#65292;ZSG&#65289;&#21040;&#26410;&#35265;&#36807;&#30340;&#21160;&#24577;&#23545;&#20110;&#21019;&#24314;&#20855;&#26377;&#26222;&#36941;&#33021;&#21147;&#30340;&#20307;&#31995;&#20195;&#29702;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;contextual reinforcement learning&#65292;cRL&#65289;&#30340;&#31616;&#21333;&#35774;&#32622;&#24320;&#22987;&#65292;&#20551;&#35774;&#21487;&#35266;&#23519;&#21040;&#21442;&#25968;&#21270;&#31995;&#32479;&#21160;&#24577;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#20540;&#65292;&#22914;&#26426;&#22120;&#20154;&#30340;&#36136;&#37327;&#25110;&#23610;&#23544;&#65292;&#32780;&#19981;&#23545;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#30340;&#21487;&#35266;&#23519;&#24615;&#20570;&#36827;&#19968;&#27493;&#31616;&#21270;&#20551;&#35774;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;ZSG&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24490;&#29615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;contextual recurrent state-space model&#65292;cRSSM&#65289;&#65292;&#23427;&#23545;Dreamer&#65288;v3&#65289;&#65288;Hafner&#31561;&#20154;&#65292;2023&#24180;&#65289;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20351;&#24471;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#34701;&#20837;&#19978;&#19979;&#25991;&#20197;&#20174;&#35266;&#23519;&#20013;&#25512;&#26029;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24182;&#24314;&#27169;&#28508;&#22312;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#31995;&#32479;&#24615;&#22320;&#23558;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#20013;&#25552;&#39640;&#20102;&#22312;&#8220;&#26790;&#22659;&#8221;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;ZSG&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10967v1 Announce Type: cross  Abstract: Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ``dreams
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.10949</link><description>&lt;p&gt;
SelfIE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SelfIE: Self-Interpretation of Large Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#33719;&#24471;&#31572;&#26696;&#65311;&#35299;&#37322;&#21644;&#25511;&#21046;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#26410;&#26469;&#27169;&#22411;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SelfIE&#65288;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;LLMs&#21709;&#24212;&#20851;&#20110;&#32473;&#23450;&#27573;&#33853;&#30340;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#33258;&#24049;&#30340;&#23884;&#20837;&#12290;SelfIE&#33021;&#22815;&#35299;&#37322;&#38544;&#34255;&#23884;&#20837;&#20013;&#30340;&#24320;&#25918;&#19990;&#30028;&#27010;&#24565;&#65292;&#22312;&#26696;&#20363;&#20013;&#25581;&#31034;LLM&#30340;&#20869;&#37096;&#25512;&#29702;&#65292;&#22914;&#20570;&#20986;&#36947;&#24503;&#20915;&#31574;&#12289;&#20869;&#21270;&#25552;&#31034;&#27880;&#20837;&#21644;&#22238;&#24819;&#26377;&#23475;&#30693;&#35782;&#12290;SelfIE&#23545;&#38544;&#34255;&#23884;&#20837;&#30340;&#25991;&#26412;&#25551;&#36848;&#20063;&#24320;&#36767;&#20102;&#25511;&#21046;LLM&#25512;&#29702;&#30340;&#26032;&#36884;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#25511;&#21046;&#65292;&#23427;&#20801;&#35768;&#32534;&#36753;&#24320;&#25918;&#24335;&#27010;&#24565;&#65292;&#32780;&#21482;&#38656;&#35201;&#35745;&#31639;&#21333;&#20010;&#23618;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23558;RLHF&#25193;&#23637;&#21040;&#38544;&#34255;&#30340;&#23884;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#24378;&#21270;&#25511;&#21046;&#26469;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27861;&#24459;&#25991;&#26412;&#20998;&#26512;&#20013;&#25972;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22797;&#21512;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.10944</link><description>&lt;p&gt;
&#38754;&#21521;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#20998;&#26512;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Human Centered AI for Indian Legal Text Analytics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27861;&#24459;&#25991;&#26412;&#20998;&#26512;&#20013;&#25972;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22797;&#21512;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#30740;&#31350;&#22312;&#27861;&#24459;&#23454;&#36341;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#38656;&#35201;&#24378;&#28872;&#30340;&#20154;&#21147;&#21644;&#26234;&#21147;&#35880;&#24910;&#26469;&#30740;&#31350;&#27861;&#24459;&#26696;&#20363;&#24182;&#20934;&#22791;&#35770;&#28857;&#12290;&#36817;&#24180;&#26469;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#32321;&#33635;&#24182;&#27809;&#26377;&#23548;&#33268;&#23545;&#24433;&#21709;&#28145;&#36828;&#30340;&#27861;&#24459;&#24212;&#29992;&#30340;&#25104;&#27604;&#20363;&#22686;&#38271;&#65292;&#22240;&#20026;&#20854;&#21487;&#20449;&#24230;&#20302;&#20197;&#21450;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#19987;&#19994;&#25968;&#25454;&#38598;&#31232;&#32570;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#27861;&#24459;&#25991;&#26412;&#20998;&#26512;&#65288;LTA&#65289;&#20013;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25972;&#21512;&#65292;&#22914;&#20309;&#26174;&#33879;&#22686;&#24378;&#20854;&#24615;&#33021;&#65292;&#19982;&#19987;&#23478;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22797;&#21512;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20027;&#35201;&#32467;&#21512;&#20154;&#31867;&#36755;&#20837;&#65292;&#20197;&#22312;LLMs&#20013;&#25191;&#34892;LTA&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10944v1 Announce Type: cross  Abstract: Legal research is a crucial task in the practice of law. It requires intense human effort and intellectual prudence to research a legal case and prepare arguments. Recent boom in generative AI has not translated to proportionate rise in impactful legal applications, because of low trustworthiness and and the scarcity of specialized datasets for training Large Language Models (LLMs). This position paper explores the potential of LLMs within Legal Text Analytics (LTA), highlighting specific areas where the integration of human expertise can significantly enhance their performance to match that of experts. We introduce a novel dataset and describe a human centered, compound AI system that principally incorporates human inputs for performing LTA tasks with LLMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#21516;&#24577;POMDP&#65288;H-POMDP&#65289;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#31181;&#35748;&#30693;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#26500;&#24314;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#35825;&#23548;&#26356;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.10930</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#24577;POMDP&#35825;&#23548;&#20010;&#21035;&#23398;&#29983;&#30340;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Inducing Individual Students' Learning Strategies through Homomorphic POMDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10930
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#21516;&#24577;POMDP&#65288;H-POMDP&#65289;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#31181;&#35748;&#30693;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#26500;&#24314;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#35825;&#23548;&#26356;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#23398;&#29983;&#30340;&#23398;&#20064;&#31574;&#30053;&#26159;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#36890;&#36807;&#27169;&#25311;&#23398;&#29983;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20026;&#23398;&#29983;&#21046;&#23450;&#20010;&#24615;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#30740;&#31350;&#20551;&#35774;&#23398;&#29983;&#20154;&#32676;&#36981;&#24490;&#32479;&#19968;&#30340;&#35748;&#30693;&#27169;&#24335;&#12290;&#34429;&#28982;&#36825;&#31181;&#20551;&#35774;&#31616;&#21270;&#20102;POMDP&#24314;&#27169;&#36807;&#31243;&#65292;&#20294;&#26174;&#28982;&#20559;&#31163;&#20102;&#30495;&#23454;&#22330;&#26223;&#65292;&#22240;&#27492;&#38477;&#20302;&#20102;&#35825;&#23548;&#20010;&#21035;&#23398;&#29983;&#23398;&#20064;&#31574;&#30053;&#30340;&#31934;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#24577;POMDP&#65288;H-POMDP&#65289;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#31181;&#35748;&#30693;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#26500;&#24314;H-POMDP&#27169;&#22411;&#12290;&#22522;&#20110;H-POMDP&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#34920;&#31034;&#19981;&#21516;&#30340;&#35748;&#30693;&#27169;&#24335;&#24182;&#35825;&#23548;&#26356;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10930v1 Announce Type: new  Abstract: Optimizing students' learning strategies is a crucial component in intelligent tutoring systems. Previous research has demonstrated the effectiveness of devising personalized learning strategies for students by modelling their learning processes through partially observable Markov decision process (POMDP). However, the research holds the assumption that the student population adheres to a uniform cognitive pattern. While this assumption simplifies the POMDP modelling process, it evidently deviates from a real-world scenario, thus reducing the precision of inducing individual students' learning strategies. In this article, we propose the homomorphic POMDP (H-POMDP) model to accommodate multiple cognitive patterns and present the parameter learning approach to automatically construct the H-POMDP model. Based on the H-POMDP model, we are able to represent different cognitive patterns from the data and induce more personalized learning strat
&lt;/p&gt;</description></item><item><title>TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.10923</link><description>&lt;p&gt;
TabPFN&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for TabPFN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10923
&lt;/p&gt;
&lt;p&gt;
TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;Prior-Data Fitted Networks&#65288;PFNs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#20855;&#26377;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#32467;&#26524;&#12290;TabPFN&#27169;&#22411;&#26159;PFN&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#22312;&#19981;&#38656;&#35201;&#23398;&#20064;&#21442;&#25968;&#25110;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#22312;&#30701;&#30701;&#20960;&#31186;&#38047;&#20869;&#23454;&#29616;&#22810;&#31181;&#20998;&#31867;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;TabPFN&#22240;&#27492;&#25104;&#20026;&#20102;&#35768;&#22810;&#39046;&#22495;&#24212;&#29992;&#20013;&#38750;&#24120;&#21560;&#24341;&#20154;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;TabPFN&#19987;&#38376;&#35774;&#35745;&#30340;&#27969;&#34892;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#21033;&#29992;&#35813;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#25913;&#36827;&#20801;&#35768;&#27604;&#29616;&#26377;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36991;&#20813;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10923v1 Announce Type: cross  Abstract: The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes. The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by in-context learning without the need for learning parameters or hyperparameter tuning. This makes TabPFN a very attractive option for a wide range of domain applications. However, a major drawback of the method is its lack of interpretability. Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN. By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations. In particular, we show how in-context learning facilitates the estimation of Shapley values by avoid
&lt;/p&gt;</description></item><item><title>DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.10903</link><description>&lt;p&gt;
DTOR&#65306;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#29992;&#20110;&#35299;&#37322;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
DTOR: Decision Tree Outlier Regressor to explain anomalies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10903
&lt;/p&gt;
&lt;p&gt;
DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#20197;&#21450;&#20854;&#20135;&#29983;&#26426;&#21046;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#25925;&#38556;&#12289;&#27450;&#35784;&#12289;&#23041;&#32961;&#31561;&#38382;&#39064;&#65292;&#38500;&#20102;&#34987;&#27491;&#30830;&#35782;&#21035;&#20043;&#22806;&#65292;&#36890;&#24120;&#38656;&#35201;&#26377;&#25928;&#30340;&#35299;&#37322;&#20197;&#26377;&#25928;&#25191;&#34892;&#21487;&#25805;&#20316;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#20351;&#24471;&#36825;&#26679;&#30340;&#35299;&#37322;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65288;DTOR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#36825;&#26159;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#26469;&#35745;&#31639;&#20272;&#35745;&#20998;&#25968;&#65292;&#28982;&#21518;&#25552;&#21462;&#19982;&#25968;&#25454;&#28857;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#30456;&#23545;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;DTOR&#30340;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25193;&#23637;&#35789;&#27719;&#12289;&#21452;&#35821;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.10882</link><description>&lt;p&gt;
&#20248;&#21270;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22686;&#24378;&#65306;&#20197;&#38889;&#35821;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25193;&#23637;&#35789;&#27719;&#12289;&#21452;&#35821;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#39044;&#35757;&#32451;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25193;&#23637;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#35768;&#22810;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#21644;&#30740;&#31350;&#26426;&#26500;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;LLMs&#65288;MLLMs&#65289;&#20197;&#28385;&#36275;&#24403;&#21069;&#38656;&#27714;&#65292;&#20294;&#24573;&#35270;&#20102;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65288;LRLs&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;LRLs&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25193;&#23637;LRLs&#30340;MLLM&#35789;&#27719;&#20197;&#22686;&#24378;&#34920;&#36798;&#24615;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#21452;&#35821;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#23545;&#40784;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#31532;&#19977;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#23567;&#35268;&#27169;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#20197;&#22686;&#24378;LRL&#12290;&#23454;&#39564;&#37319;&#29992;&#20102;Llama2&#27169;&#22411;&#65292;&#20197;&#38889;&#35821;&#20316;&#20026;LRL&#65292;&#24182;&#22312;&#20843;&#39033;&#20219;&#21153;&#20013;&#23545;&#20854;&#19982;&#20854;&#20182;&#24050;&#24320;&#21457;&#30340;LLMs&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20154;&#31867;&#35780;&#20272;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10882v1 Announce Type: cross  Abstract: Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human eva
&lt;/p&gt;</description></item><item><title>stMCDI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23450;&#20301;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#26469;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.10863</link><description>&lt;p&gt;
stMCDI: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#25513;&#30721;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22635;&#34917;
&lt;/p&gt;
&lt;p&gt;
stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10863
&lt;/p&gt;
&lt;p&gt;
stMCDI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23450;&#20301;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#26469;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#23450;&#20301;&#36716;&#24405;&#32452;&#23398;&#36890;&#36807;&#25552;&#20379;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#21450;&#20854;&#30456;&#24212;&#30340;&#29289;&#29702;&#20301;&#32622;&#65292;&#20026;&#21333;&#32454;&#32990;&#20998;&#26512;&#24102;&#26469;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39640;&#24230;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#21364;&#24102;&#26469;&#20102;&#19968;&#20010;&#32570;&#28857;&#65292;&#21363;&#30001;&#20110;&#32570;&#22833;&#20540;&#30340;&#39640;&#21457;&#29983;&#29575;&#65292;&#23548;&#33268;&#20102;&#32454;&#32990;&#27700;&#24179;&#19978;&#30340;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#26126;&#26174;&#21463;&#21040;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22635;&#34917;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#19981;&#21516;&#28857;&#20043;&#38388;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#35201;&#20040;&#29306;&#29298;&#25972;&#20307;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#26377;&#25928;&#21033;&#29992;&#31354;&#38388;&#23450;&#20301;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#26469;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;stMCDI&#65292;&#21033;&#29992;&#20351;&#29992;&#38543;&#26426;&#25513;&#30721;&#25968;&#25454;&#37096;&#20998;&#20316;&#20026;&#25351;&#23548;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10863v1 Announce Type: cross  Abstract: Spatially resolved transcriptomics represents a significant advancement in single-cell analysis by offering both gene expression data and their corresponding physical locations. However, this high degree of spatial resolution entails a drawback, as the resulting spatial transcriptomic data at the cellular level is notably plagued by a high incidence of missing values. Furthermore, most existing imputation methods either overlook the spatial information between spots or compromise the overall gene expression data distribution. To address these challenges, our primary focus is on effectively utilizing the spatial location information within spatial transcriptomic data to impute missing values, while preserving the overall data distribution. We introduce \textbf{stMCDI}, a novel conditional diffusion model for spatial transcriptomics data imputation, which employs a denoising network trained using randomly masked data portions as guidance
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#65292;&#23558;&#20174;&#26415;&#21069;&#35268;&#21010;&#21040;&#27979;&#35797;&#38454;&#27573;&#30340;&#26102;&#38388;&#32553;&#30701;&#33267;&#19981;&#21040;&#20116;&#20998;&#38047;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30495;&#23454;&#22270;&#20687;&#21644;&#26415;&#21069;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21450;&#24341;&#20837;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#20943;&#23567;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20809;&#29031;&#26465;&#20214;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.10860</link><description>&lt;p&gt;
&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#30340;&#39640;&#25928;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation for Endoscopic Visual Odometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10860
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#65292;&#23558;&#20174;&#26415;&#21069;&#35268;&#21010;&#21040;&#27979;&#35797;&#38454;&#27573;&#30340;&#26102;&#38388;&#32553;&#30701;&#33267;&#19981;&#21040;&#20116;&#20998;&#38047;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30495;&#23454;&#22270;&#20687;&#21644;&#26415;&#21069;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21450;&#24341;&#20837;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#20943;&#23567;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20809;&#29031;&#26465;&#20214;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10860v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;&#37324;&#31243;&#35745;&#22312;&#20869;&#31397;&#38236;&#25104;&#20687;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#32570;&#20047;&#20855;&#26377;&#22320;&#38754;&#30495;&#23454;&#24615;&#30340;&#22270;&#20687;&#23545;&#20110;&#23398;&#20064;&#37324;&#31243;&#35745;&#20449;&#24687;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#20026;&#36830;&#25509;&#26415;&#21069;&#35268;&#21010;&#39046;&#22495;&#21644;&#26415;&#20013;&#23454;&#38469;&#39046;&#22495;&#23398;&#20064;&#37324;&#31243;&#35745;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#38388;&#19978;&#23384;&#22312;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#30340;&#39640;&#25928;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#65292;&#23558;&#20174;&#26415;&#21069;&#35268;&#21010;&#21040;&#27979;&#35797;&#38454;&#27573;&#30340;&#26102;&#38388;&#32553;&#30701;&#33267;&#19981;&#21040;&#20116;&#20998;&#38047;&#12290;&#20026;&#20102;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30495;&#23454;&#22270;&#20687;&#35757;&#32451;&#27169;&#22359;&#65292;&#24182;&#21033;&#29992;&#26415;&#21069;&#20808;&#39564;&#20449;&#24687;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#26469;&#28040;&#38500;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20809;&#29031;&#26465;&#20214;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10860v1 Announce Type: cross  Abstract: Visual odometry plays a crucial role in endoscopic imaging, yet the scarcity of realistic images with ground truth poses poses a significant challenge. Therefore, domain adaptation offers a promising approach to bridge the pre-operative planning domain with the intra-operative real domain for learning odometry information. However, existing methodologies suffer from inefficiencies in the training time. In this work, an efficient neural style transfer framework for endoscopic visual odometry is proposed, which compresses the time from pre-operative planning to testing phase to less than five minutes. For efficient traing, this work focuses on training modules with only a limited number of real images and we exploit pre-operative prior information to dramatically reduce training duration. Moreover, during the testing phase, we propose a novel Test Time Adaptation (TTA) method to mitigate the gap in lighting conditions between training an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10853</link><description>&lt;p&gt;
&#21482;&#35828;&#21517;&#31216;&#65306;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#23454;&#29616;&#20165;&#21033;&#29992;&#31867;&#21035;&#21517;&#31216;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Just Say the Name: Online Continual Learning with Category Names Only via Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#22823;&#35268;&#27169;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#35832;&#22914;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#20351;&#29992;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#32493;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550; - &#20165;&#20351;&#29992;&#21517;&#31216;&#30340;&#29983;&#25104;&#24335;&#36830;&#32493;&#23398;&#20064;&#65288;G-NoCL&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;G-NoCL&#20351;&#29992;&#19968;&#32452;&#29983;&#25104;&#22120;G&#20197;&#21450;&#23398;&#20064;&#32773;&#12290;&#24403;&#36935;&#21040;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#31867;&#21035;&#65289;&#26102;&#65292;G-NoCL&#37319;&#29992;&#26032;&#39062;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;DIverSity and COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#20174;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#26368;&#20248;&#25277;&#26679;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DISCOBER&#22312;G-NoCL&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#28085;&#30422;&#20102;In-Distributi&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
&lt;/p&gt;</description></item><item><title>GAgent &#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21018;&#26580;&#25569;&#25345;Agent&#22312;&#22797;&#26434;&#29031;&#26126;&#29615;&#22659;&#19979;&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21253;&#25324;Prompt Engineer&#27169;&#22359;&#12289;VLM&#26680;&#24515;&#21644;Workflow&#27169;&#22359;&#65292;&#21516;&#26102;&#20855;&#22791;&#21487;&#21464;&#30828;&#24230;&#36719;&#25163;&#25345;&#22120;&#65292;&#23545;&#29289;&#20307;&#21644;&#26448;&#26009;&#36827;&#34892;&#35782;&#21035;&#21644;&#20934;&#30830;&#25235;&#21462;&#65292;&#20026;&#26080;&#20154;&#26426;&#31561;&#22330;&#26223;&#24102;&#26469;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.10850</link><description>&lt;p&gt;
GAgent&#65306;&#19968;&#31181;&#36866;&#24212;&#22797;&#26434;&#29031;&#26126;&#29615;&#22659;&#30340;&#33258;&#36866;&#24212;&#21018;&#26580;&#25569;&#25345;Agent
&lt;/p&gt;
&lt;p&gt;
GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language Models for Complex Lighting Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10850
&lt;/p&gt;
&lt;p&gt;
GAgent &#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21018;&#26580;&#25569;&#25345;Agent&#22312;&#22797;&#26434;&#29031;&#26126;&#29615;&#22659;&#19979;&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21253;&#25324;Prompt Engineer&#27169;&#22359;&#12289;VLM&#26680;&#24515;&#21644;Workflow&#27169;&#22359;&#65292;&#21516;&#26102;&#20855;&#22791;&#21487;&#21464;&#30828;&#24230;&#36719;&#25163;&#25345;&#22120;&#65292;&#23545;&#29289;&#20307;&#21644;&#26448;&#26009;&#36827;&#34892;&#35782;&#21035;&#21644;&#20934;&#30830;&#25235;&#21462;&#65292;&#20026;&#26080;&#20154;&#26426;&#31561;&#22330;&#26223;&#24102;&#26469;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;GAgent&#65306;&#19968;&#31181;&#20026;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#35774;&#35745;&#30340;&#25569;&#25345;Agent&#65292;&#36890;&#36807;VLM&#20195;&#29702;&#25552;&#20379;&#20808;&#36827;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#20855;&#22791;&#21487;&#21464;&#30828;&#24230;&#36719;&#25163;&#25345;&#22120;&#30340;&#28789;&#27963;&#25235;&#21462;&#33021;&#21147;&#12290;GAgent&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214; - Prompt Engineer&#27169;&#22359;&#65292;Visual-Language Model&#65288;VLM&#65289;&#26680;&#24515;&#21644;Workflow&#27169;&#22359;&#12290;&#36825;&#19977;&#20010;&#27169;&#22359;&#36890;&#36807;&#35782;&#21035;&#29289;&#20307;&#21644;&#26448;&#26009;&#65292;&#21363;&#20351;&#22312;&#25361;&#25112;&#24615;&#30340;&#20809;&#29031;&#26465;&#20214;&#19979;&#20063;&#33021;&#20934;&#30830;&#20272;&#35745;&#25235;&#21462;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#22841;&#29226;&#25104;&#21151;&#29575;&#12290;&#20316;&#20026;&#21019;&#36896;&#24615;&#30340;&#19968;&#37096;&#20998;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#21019;&#24314;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#21464;&#30828;&#24230;&#30340;&#20223;&#29983;&#28151;&#21512;&#36719;&#25163;&#25569;&#22120;&#65292;&#33021;&#22815;&#22841;&#25345;&#37325;&#29289;&#21516;&#26102;&#36731;&#26580;&#25509;&#35302;&#29289;&#20307;&#12290;&#36825;&#31181;&#29305;&#24449;VLM&#22522;&#30784;&#35748;&#30693;&#22788;&#29702;&#21644;&#20223;&#29983;&#35774;&#35745;&#30340;&#26234;&#33021;Agent&#26174;&#31034;&#20102;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#26377;&#21161;&#20110;&#21508;&#31181;&#24773;&#26223;&#20013;&#30340;&#26080;&#20154;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10850v1 Announce Type: cross  Abstract: This paper introduces GAgent: an Gripping Agent designed for open-world environments that provides advanced cognitive abilities via VLM agents and flexible grasping abilities with variable stiffness soft grippers. GAgent comprises three primary components - Prompt Engineer module, Visual-Language Model (VLM) core and Workflow module. These three modules enhance gripper success rates by recognizing objects and materials and accurately estimating grasp area even under challenging lighting conditions. As part of creativity, researchers also created a bionic hybrid soft gripper with variable stiffness capable of gripping heavy loads while still gently engaging objects. This intelligent agent, featuring VLM-based cognitive processing with bionic design, shows promise as it could potentially benefit UAVs in various scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.10842</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#30340;&#21452;Transformer&#22312;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65288;FDD&#65289;&#23545;&#20110;&#30830;&#20445;&#24037;&#19994;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FDD&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#65288;TEP&#65289;&#65292;&#36825;&#26159;&#21270;&#24037;&#36807;&#31243;&#25511;&#21046;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;Transformer&#20998;&#25903;&#65292;&#33021;&#22815;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#65288;GDLAttention&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#38376;&#25511;&#26426;&#21046;&#21644;&#21160;&#24577;&#23398;&#20064;&#33021;&#21147;&#12290;&#38376;&#25511;&#26426;&#21046;&#35843;&#33410;&#27880;&#24847;&#26435;&#37325;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#36755;&#20837;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#12290;&#21160;&#24577;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;&#27880;&#24847;&#26426;&#21046;&#20351;&#29992;&#21452;&#32447;&#24615;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#26469;&#25429;&#25417;&#26597;&#35810;&#21644;&#36755;&#20837;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
&lt;/p&gt;</description></item><item><title>SF(DA)$^2$&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26500;&#24314;&#22686;&#24378;&#22270;&#24182;&#21033;&#29992;&#35889;&#37051;&#22495;&#32858;&#31867;&#26469;&#35782;&#21035;&#20998;&#21306;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#32780;&#36991;&#20813;&#20102;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.10834</link><description>&lt;p&gt;
SF(DA)$^2$: &#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#22312;&#25968;&#25454;&#22686;&#24378;&#30340;&#35270;&#35282;&#19979;
&lt;/p&gt;
&lt;p&gt;
SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10834
&lt;/p&gt;
&lt;p&gt;
SF(DA)$^2$&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26500;&#24314;&#22686;&#24378;&#22270;&#24182;&#21033;&#29992;&#35889;&#37051;&#22495;&#32858;&#31867;&#26469;&#35782;&#21035;&#20998;&#21306;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#32780;&#36991;&#20813;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#23545;&#22495;&#20559;&#31227;&#30340;&#33030;&#24369;&#24615;&#26102;&#65292;&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#30446;&#26631;&#22495;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#28304;&#22495;&#25968;&#25454;&#12290;&#23613;&#31649;&#23558;&#25968;&#25454;&#22686;&#24378;&#24212;&#29992;&#20110;SFDA&#30340;&#28508;&#22312;&#22909;&#22788;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#20063;&#20250;&#24102;&#26469;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#20381;&#36182;&#20110;&#20445;&#23384;&#31867;&#21035;&#21464;&#25442;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#21450;&#22686;&#21152;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SF(DA)$^2$&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#24182;&#36991;&#20813;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#26500;&#24314;&#19968;&#20010;&#22686;&#24378;&#22270;&#65292;&#21033;&#29992;&#30446;&#26631;&#29305;&#24449;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#35889;&#37051;&#22495;&#32858;&#31867;&#26469;&#35782;&#21035;&#39044;&#27979;&#31354;&#38388;&#20013;&#30340;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10834v1 Announce Type: cross  Abstract: In the face of the deep learning model's vulnerability to domain shift, source-free domain adaptation (SFDA) methods have been proposed to adapt models to new, unseen target domains without requiring access to source domain data. Although the potential benefits of applying data augmentation to SFDA are attractive, several challenges arise such as the dependence on prior knowledge of class-preserving transformations and the increase in memory and computational requirements. In this paper, we propose Source-free Domain Adaptation Through the Lens of Data Augmentation (SF(DA)$^2$), a novel approach that leverages the benefits of data augmentation without suffering from these challenges. We construct an augmentation graph in the feature space of the pretrained model using the neighbor relationships between target features and propose spectral neighborhood clustering to identify partitions in the prediction space. Furthermore, we propose im
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LLM&#20195;&#29702;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#29616;&#23454;&#26102;&#30340;&#20154;&#31867;&#35282;&#33394;&#25198;&#28436;&#65292;&#20445;&#30041;&#29420;&#29305;&#19978;&#19979;&#25991;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#25110;&#39044;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.10824</link><description>&lt;p&gt;
LookALike: &#22522;&#20110;&#20154;&#31867;&#27169;&#20223;&#30340;&#21327;&#20316;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LookALike: Human Mimicry based collaborative decision making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10824
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LLM&#20195;&#29702;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#29616;&#23454;&#26102;&#30340;&#20154;&#31867;&#35282;&#33394;&#25198;&#28436;&#65292;&#20445;&#30041;&#29420;&#29305;&#19978;&#19979;&#25991;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#25110;&#39044;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#22312;&#21521;&#20854;&#20182;&#31995;&#32479;&#20256;&#36798;&#29305;&#23450;&#35282;&#33394;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26500;&#24314;&#33021;&#22815;&#30456;&#20114;&#27807;&#36890;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#33258;&#20027;LLM&#20195;&#29702;&#26102;&#65292;&#36825;&#19968;&#19981;&#36275;&#26356;&#20026;&#26126;&#26174;&#12290;&#20154;&#31867;&#33021;&#22815;&#20256;&#36798;&#19978;&#19979;&#25991;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#24494;&#23567;&#24046;&#21035;&#20197;&#21450;&#30693;&#35782;&#65292;&#36825;&#23548;&#33268;&#20102;&#25216;&#33021;&#30340;&#36827;&#19968;&#27493;&#23436;&#21892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20445;&#30041;&#29420;&#29305;&#19978;&#19979;&#25991;&#32780;&#26080;&#38656;&#20381;&#36182;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#25110;&#39044;&#35757;&#32451;&#30340;&#23454;&#26102;&#20154;&#31867;&#35282;&#33394;&#25198;&#28436;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#27169;&#25311;&#23454;&#38469;&#20219;&#21153;&#20013;&#30456;&#27604;&#26368;&#20808;&#36827;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10824v1 Announce Type: cross  Abstract: Artificial General Intelligence falls short when communicating role specific nuances to other systems. This is more pronounced when building autonomous LLM agents capable and designed to communicate with each other for real world problem solving. Humans can communicate context and domain specific nuances along with knowledge, and that has led to refinement of skills. In this work we propose and evaluate a novel method that leads to knowledge distillation among LLM agents leading to realtime human role play preserving unique contexts without relying on any stored data or pretraining. We also evaluate how our system performs better in simulated real world tasks compared to state of the art.
&lt;/p&gt;</description></item><item><title>VisionCLIP&#21033;&#29992;Med-AIGC&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#20262;&#29702;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;&#65292;&#22312;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.10823</link><description>&lt;p&gt;
VisionCLIP&#65306;&#19968;&#31181;&#22522;&#20110;Med-AIGC&#30340;&#20262;&#29702;&#35821;&#35328;-&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#29992;&#24615;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10823
&lt;/p&gt;
&lt;p&gt;
VisionCLIP&#21033;&#29992;Med-AIGC&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#20262;&#29702;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;&#65292;&#22312;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#24341;&#20837;&#20102;&#26032;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;&#27880;&#37322;&#25968;&#25454;&#38656;&#27714;&#19982;&#24739;&#32773;&#38544;&#31169;&#20043;&#38388;&#30340;&#30683;&#30462;&#25345;&#32493;&#21152;&#21095;&#12290;&#21033;&#29992;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;Med-AIGC&#65289;&#20316;&#20026;&#19968;&#20010;&#28304;&#28304;&#19981;&#26029;&#30340;&#36164;&#28304;&#24211;&#65292;&#25104;&#20026;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#21033;&#29992;100&#19975;&#20010;&#24320;&#28304;&#21512;&#25104;&#24213;&#29255;&#22270;&#20687;&#19982;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#31574;&#21010;&#20102;&#19968;&#31181;&#21517;&#20026;VisionCLIP&#30340;&#20262;&#29702;&#35821;&#35328;-&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#12290;VisionCLIP&#20197;&#38646;-shot&#26041;&#24335;&#22312;&#19977;&#20010;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#25991;&#26412;&#25968;&#25454;&#20351;&#24471;&#36825;&#20010;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#25104;&#21151;&#22320;&#21560;&#25910;&#20102;&#30142;&#30149;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10823v1 Announce Type: cross  Abstract: Generalist foundation model has ushered in newfound capabilities in medical domain. However, the contradiction between the growing demand for high-quality annotated data with patient privacy continues to intensify. The utilization of medical artificial intelligence generated content (Med-AIGC) as an inexhaustible resource repository arises as a potential solution to address the aforementioned challenge. Here we harness 1 million open-source synthetic fundus images paired with natural language descriptions, to curate an ethical language-image foundation model for retina image analysis named VisionCLIP. VisionCLIP achieves competitive performance on three external datasets compared with the existing method pre-trained on real-world data in a zero-shot fashion. The employment of artificially synthetic images alongside corresponding textual data for training enables the medical foundation model to successfully assimilate knowledge of disea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#38543;&#26426;&#36172;&#21338;&#26426;&#30340;&#28608;&#21169;&#25506;&#32034;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#38543;&#26102;&#38388;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#21644;&#34917;&#20607;</title><link>https://arxiv.org/abs/2403.10819</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#38543;&#26426;&#36172;&#21338;&#26426;&#30340;&#28608;&#21169;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Incentivized Exploration of Non-Stationary Stochastic Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#38543;&#26426;&#36172;&#21338;&#26426;&#30340;&#28608;&#21169;&#25506;&#32034;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#38543;&#26102;&#38388;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#21644;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#38382;&#39064;&#20013;&#30340;&#28608;&#21169;&#25506;&#32034;&#65292;&#20854;&#20013;&#29609;&#23478;&#36890;&#36807;&#25506;&#32034;&#38500;&#20102;&#36138;&#23146;&#36873;&#25321;&#20043;&#22806;&#30340;&#33218;&#33719;&#24471;&#34917;&#20607;&#65292;&#24182;&#19988;&#21487;&#33021;&#23545;&#22870;&#21169;&#25552;&#20379;&#20559;&#20506;&#21453;&#39304;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#65306;&#31361;&#21464;&#21644;&#25345;&#32493;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#28608;&#21169;&#25506;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#38543;&#26102;&#38388;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#21644;&#34917;&#20607;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#28608;&#21169;&#20102;&#25506;&#32034;&#65292;&#23613;&#31649;&#23384;&#22312;&#38750;&#24179;&#31283;&#24615;&#21644;&#20559;&#20506;&#25110;&#28418;&#31227;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10819v1 Announce Type: cross  Abstract: We study incentivized exploration for the multi-armed bandit (MAB) problem with non-stationary reward distributions, where players receive compensation for exploring arms other than the greedy choice and may provide biased feedback on the reward. We consider two different non-stationary environments: abruptly-changing and continuously-changing, and propose respective incentivized exploration algorithms. We show that the proposed algorithms achieve sublinear regret and compensation over time, thus effectively incentivizing exploration despite the nonstationarity and the biased or drifted feedback.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#27169;&#22411; Persona-Gestor&#65292;&#36890;&#36807;&#27169;&#31946;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#38750;&#33258;&#22238;&#24402;&#33258;&#36866;&#24212;&#23618;&#24402;&#19968;&#21270;&#21464;&#21387;&#22120;&#25193;&#25955;&#26550;&#26500;&#65292;&#20165;&#20381;&#36182;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#39640;&#24230;&#20010;&#24615;&#21270;&#30340;3D&#20840;&#36523;&#25163;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.10805</link><description>&lt;p&gt;
&#30001;&#35821;&#38899;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#25163;&#21183;&#21512;&#25104;&#65306;&#21033;&#29992;&#33258;&#21160;&#27169;&#31946;&#29305;&#24449;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#27169;&#22411; Persona-Gestor&#65292;&#36890;&#36807;&#27169;&#31946;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#38750;&#33258;&#22238;&#24402;&#33258;&#36866;&#24212;&#23618;&#24402;&#19968;&#21270;&#21464;&#21387;&#22120;&#25193;&#25955;&#26550;&#26500;&#65292;&#20165;&#20381;&#36182;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#39640;&#24230;&#20010;&#24615;&#21270;&#30340;3D&#20840;&#36523;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#29983;&#25104;&#26159;&#34394;&#25311;&#20154;&#31867;&#21019;&#36896;&#20013;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#20934;&#30830;&#30830;&#23450;&#21644;&#22788;&#29702;&#22810;&#31181;&#36755;&#20837;&#29305;&#24449;&#65288;&#22914;&#22768;&#23398;&#12289;&#35821;&#20041;&#12289;&#24773;&#24863;&#12289;&#20010;&#24615;&#65292;&#29978;&#33267;&#24494;&#22937;&#30340;&#26410;&#30693;&#29305;&#24449;&#65289;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#21508;&#31181;&#26174;&#24335;&#29305;&#24449;&#36755;&#20837;&#21644;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#65292;&#38480;&#21046;&#20102;&#29983;&#25104;&#25163;&#21183;&#30340;&#34920;&#29616;&#21147;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;Persona-Gestor&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20165;&#20381;&#36182;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#39640;&#24230;&#20010;&#24615;&#21270;&#30340;3D&#20840;&#36523;&#25163;&#21183;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#27169;&#31946;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#38750;&#33258;&#22238;&#24402;&#33258;&#36866;&#24212;&#23618;&#24402;&#19968;&#21270;&#65288;AdaLN&#65289;&#21464;&#21387;&#22120;&#25193;&#25955;&#26550;&#26500;&#12290;&#27169;&#31946;&#29305;&#24449;&#25552;&#21462;&#22120;&#21033;&#29992;&#27169;&#31946;&#25512;&#26029;&#31574;&#30053;&#65292;&#33258;&#21160;&#25512;&#26029;&#20986;&#38544;&#21547;&#30340;&#36830;&#32493;&#27169;&#31946;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10805v1 Announce Type: cross  Abstract: Speech-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and complex multimodal processing, constrain the expressiveness of resulting gestures and limit their applicability. To address these challenges, we present Persona-Gestor, a novel end-to-end generative model designed to generate highly personalized 3D full-body gestures solely relying on raw speech audio. The model combines a fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization (AdaLN) transformer diffusion architecture. The fuzzy feature extractor harnesses a fuzzy inference strategy that automatically infers implicit, continuous fuzzy features. These fuzzy feature
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLOD&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27979;&#35797;&#36807;&#31243;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#29305;&#24449;&#20013;&#35782;&#21035;&#27979;&#35797;&#26679;&#26412;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#26080;&#38656;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.10803</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#22810;&#27979;&#35797;&#30340;&#36880;&#23618;&#29305;&#24449;&#34701;&#21512;&#22686;&#24378;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10803
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLOD&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27979;&#35797;&#36807;&#31243;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#29305;&#24449;&#20013;&#35782;&#21035;&#27979;&#35797;&#26679;&#26412;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#26080;&#38656;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#20250;&#36935;&#21040;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;&#36935;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#21508;&#31181;&#27979;&#35797;&#36755;&#20837;&#65292;&#36825;&#20123;&#36229;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#21487;&#33021;&#22312;&#23616;&#37096;&#25110;&#20840;&#23616;&#29305;&#24449;&#19978;&#19982;&#35757;&#32451;&#20998;&#24067;&#26377;&#25152;&#20559;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#22810;&#27979;&#35797;&#30340;&#36880;&#23618;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#65288;MLOD&#65289;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#22810;&#20010;&#27979;&#35797;&#36807;&#31243;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#29305;&#24449;&#20013;&#37492;&#21035;&#27979;&#35797;&#26679;&#26412;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20462;&#25913;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10803v1 Announce Type: cross  Abstract: Deploying machine learning in open environments presents the challenge of encountering diverse test inputs that differ significantly from the training data. These out-of-distribution samples may exhibit shifts in local or global features compared to the training distribution. The machine learning (ML) community has responded with a number of methods aimed at distinguishing anomalous inputs from original training data. However, the majority of previous studies have primarily focused on the output layer or penultimate layer of pre-trained deep neural networks. In this paper, we propose a novel framework, Multitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), to identify distributional shifts in test samples at different levels of features through rigorous multiple testing procedure. Our approach distinguishes itself from existing methods as it does not require modifying the structure or fine-tuning of the pre-trained c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.10795</link><description>&lt;p&gt;
&#20174;&#21333;&#35789;&#21040;&#36335;&#24452;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
From Words to Routes: Applying Large Language Models to Vehicle Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65288;&#20363;&#22914;&#25805;&#20316;&#21644;&#23548;&#33322;&#65289;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20854;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#12290;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#35753;&#25105;&#20204;&#24605;&#32771;&#65306;LLMs&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;VRPs&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#19977;&#27493;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;21&#31181;&#21333;&#36710;&#25110;&#22810;&#36710;&#36335;&#24452;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22235;&#31181;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#27599;&#31181;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20174;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#20195;&#30721;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#23545;&#20110;GPT-4&#25928;&#26524;&#26368;&#20339;&#65292;&#23454;&#29616;&#20102;56%&#30340;&#21487;&#34892;&#24615;&#65292;40%&#30340;&#20248;&#21270;&#24615;&#21644;53%&#30340;&#25928;&#29575;&#12290;&#31532;&#19977;&#65292;&#22522;&#20110;&#35266;&#23519;&#21040;LLMs&#21487;&#33021;&#26080;&#27861;&#22312;&#21021;&#22987;&#23581;&#35797;&#20013;&#25552;&#20379;&#27491;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10795v1 Announce Type: cross  Abstract: LLMs have shown impressive progress in robotics (e.g., manipulation and navigation) with natural language task descriptions. The success of LLMs in these tasks leads us to wonder: What is the ability of LLMs to solve vehicle routing problems (VRPs) with natural language task descriptions? In this work, we study this question in three steps. First, we construct a dataset with 21 types of single- or multi-vehicle routing problems. Second, we evaluate the performance of LLMs across four basic prompt paradigms of text-to-code generation, each involving different types of text input. We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency. Third, based on the observation that LLMs may not be able to provide correct solutions at the initial attempt, we propose a framework that enables LLMs to refin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCOTT&#30340;&#20855;&#26377;&#30417;&#30563;&#23545;&#27604;&#21464;&#25442;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;Temporal Convolutional Networks&#20197;&#23398;&#20064;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#31616;&#21270;&#20102;&#29992;&#20110;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.10787</link><description>&lt;p&gt;
&#20855;&#26377;&#30417;&#30563;&#23545;&#27604;&#26102;&#38388;&#21464;&#25442;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time Series Representation Learning with Supervised Contrastive Temporal Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10787
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCOTT&#30340;&#20855;&#26377;&#30417;&#30563;&#23545;&#27604;&#21464;&#25442;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;Temporal Convolutional Networks&#20197;&#23398;&#20064;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#31616;&#21270;&#20102;&#29992;&#20110;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26377;&#25928;&#34920;&#31034;&#26159;&#19968;&#39033;&#26377;&#29992;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26377;&#20123;&#24037;&#20316;&#21033;&#29992;&#33258;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#21487;&#29992;&#30340;&#26631;&#31614;&#20449;&#24687;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#31216;&#20026;&#65306;\textbf{S}upervised \textbf{CO}ntrastive \textbf{T}emporal \textbf{T}ransformer (SCOTT)&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21512;&#36866;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#23398;&#20064;&#20855;&#26377;&#21464;&#21270;&#19981;&#21464;&#24615;&#30340;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20197;&#31616;&#21333;&#30340;&#26041;&#24335;&#32467;&#21512;&#20102;Transformer&#21644;Temporal Convolutional Networks&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;&#29992;&#20110;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10787v1 Announce Type: cross  Abstract: Finding effective representations for time series data is a useful but challenging task. Several works utilize self-supervised or unsupervised learning methods to address this. However, there still remains the open question of how to leverage available label information for better representations. To answer this question, we exploit pre-existing techniques in time series and representation learning domains and develop a simple, yet novel fusion model, called: \textbf{S}upervised \textbf{CO}ntrastive \textbf{T}emporal \textbf{T}ransformer (SCOTT). We first investigate suitable augmentation methods for various types of time series data to assist with learning change-invariant representations. Secondly, we combine Transformer and Temporal Convolutional Networks in a simple way to efficiently learn both global and local features. Finally, we simplify Supervised Contrastive Loss for representation learning of labelled time series data. We p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#22269;&#24189;&#40664;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#20856;&#25925;&#35866;&#35821;&#12290;&#20182;&#20204;&#37319;&#29992;&#26032;&#39062;&#30340;fine-tuning&#26041;&#27861;&#65292;&#21253;&#21547;&#34701;&#21512;&#25340;&#38899;&#23884;&#20837;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#20174;&#32780;&#25104;&#21151;&#29983;&#25104;&#24189;&#40664;&#24615;&#20856;&#25925;&#35866;&#35821;&#12290;</title><link>https://arxiv.org/abs/2403.10781</link><description>&lt;p&gt;
&#25506;&#32034;&#20013;&#22269;&#24189;&#40664;&#29983;&#25104;&#65306;&#20851;&#20110;&#20004;&#21477;&#20856;&#25925;&#35866;&#35821;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#22269;&#24189;&#40664;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#20856;&#25925;&#35866;&#35821;&#12290;&#20182;&#20204;&#37319;&#29992;&#26032;&#39062;&#30340;fine-tuning&#26041;&#27861;&#65292;&#21253;&#21547;&#34701;&#21512;&#25340;&#38899;&#23884;&#20837;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#20174;&#32780;&#25104;&#21151;&#29983;&#25104;&#24189;&#40664;&#24615;&#20856;&#25925;&#35866;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#35821;&#35328;&#20013;&#23500;&#26377;&#25991;&#21270;&#20869;&#28085;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#23545;&#20110;&#35745;&#31639;&#26426;&#29702;&#35299;&#21644;&#29983;&#25104;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#30456;&#23545;&#26410;&#34987;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25506;&#32034;&#30340;&#20013;&#22269;&#24189;&#40664;&#39046;&#22495;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#22269;&#24189;&#40664;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#32858;&#28966;&#20110;&#35757;&#32451;&#23427;&#20204;&#21019;&#36896;&#20856;&#25925;&#35866;&#35821;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#26174;&#33879;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#35843;&#25972;&#19968;&#20010;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#19968;&#20010;&#22823;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#35843;&#25972;&#26041;&#27861;&#34701;&#21512;&#20102;&#25340;&#38899;&#23884;&#20837;&#20197;&#32771;&#34385;&#21516;&#38899;&#24322;&#20041;&#35789;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#19982;&#21512;&#25104;&#22256;&#38590;&#24615;&#36127;&#20363;&#20197;&#21306;&#20998;&#24189;&#40664;&#20803;&#32032;&#12290;&#20154;&#31867;&#27880;&#37322;&#30340;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24189;&#40664;&#30340;&#20856;&#25925;&#35866;&#35821;&#65292;&#25552;&#31034;&#27861;&#35777;&#26126;&#26159;&#19968;&#20010;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#19982;&#20154;&#31867;&#21019;&#36896;&#21147;&#21305;&#37197;&#30340;&#20856;&#25925;&#35866;&#35821;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10781v1 Announce Type: cross  Abstract: Humor, a culturally nuanced aspect of human language, poses challenges for computational understanding and generation, especially in Chinese humor, which remains relatively unexplored in the NLP community. This paper investigates the capability of state-of-the-art language models to comprehend and generate Chinese humor, specifically focusing on training them to create allegorical sayings. We employ two prominent training methods: fine-tuning a medium-sized language model and prompting a large one. Our novel fine-tuning approach incorporates fused Pinyin embeddings to consider homophones and employs contrastive learning with synthetic hard negatives to distinguish humor elements. Human-annotated results show that these models can generate humorous allegorical sayings, with prompting proving to be a practical and effective method. However, there is still room for improvement in generating allegorical sayings that match human creativity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#19981;&#21464;&#30340;Real-to-Simulation&#24494;&#35843;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22810;&#31867;&#22810;&#23454;&#20363;&#20998;&#21106;&#20013;Segment Any Object Model&#65288;SAOM&#65289;&#30340;&#8220;&#19968;&#20999;&#8221;&#27169;&#24335;&#24037;&#20316;&#65292;&#24182;&#22312;&#23460;&#20869;&#22330;&#26223;&#29702;&#35299;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10780</link><description>&lt;p&gt;
&#29289;&#20307;&#20219;&#24847;&#27169;&#22411;&#65288;SAOM&#65289;&#65306;&#29992;&#20110;&#22810;&#31867;&#22810;&#23454;&#20363;&#20998;&#21106;&#30340;&#23454;&#38469;&#21040;&#20223;&#30495;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy for Multi-Class Multi-Instance Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#19981;&#21464;&#30340;Real-to-Simulation&#24494;&#35843;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22810;&#31867;&#22810;&#23454;&#20363;&#20998;&#21106;&#20013;Segment Any Object Model&#65288;SAOM&#65289;&#30340;&#8220;&#19968;&#20999;&#8221;&#27169;&#24335;&#24037;&#20316;&#65292;&#24182;&#22312;&#23460;&#20869;&#22330;&#26223;&#29702;&#35299;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31867;&#22810;&#23454;&#20363;&#20998;&#21106;&#26159;&#35782;&#21035;&#22270;&#20687;&#20013;&#22810;&#20010;&#23545;&#35937;&#31867;&#21035;&#21644;&#21516;&#19968;&#31867;&#21035;&#30340;&#22810;&#20010;&#23454;&#20363;&#30340;&#20219;&#21153;&#12290;&#22522;&#30784;&#30340;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22810;&#31867;&#22810;&#23454;&#20363;&#20998;&#21106;&#65292;&#20294;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#24448;&#24448;&#20250;&#20197;&#8220;&#19968;&#20999;&#8221;&#27169;&#24335;&#36755;&#20986;&#37096;&#20998;&#25110;&#23376;&#37096;&#20998;&#25513;&#27169;&#12290;&#25972;&#20307;&#29289;&#20307;&#20998;&#21106;&#25513;&#27169;&#22312;&#23460;&#20869;&#22330;&#26223;&#29702;&#35299;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#19981;&#21464;&#30340;Real-to-Simulation&#65288;Real-Sim&#65289;&#24494;&#35843;&#31574;&#30053;&#29992;&#20110;SAM&#12290;&#25105;&#20204;&#22312;&#24494;&#35843;&#65288;&#30495;&#23454;&#21040;&#20223;&#30495;&#65289;&#26399;&#38388;&#20351;&#29992;&#20174;Ai2Thor&#27169;&#25311;&#22120;&#25910;&#38598;&#30340;&#29289;&#20307;&#22270;&#20687;&#21644;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20026;&#20102;&#20801;&#35768;&#25105;&#20204;&#30340;Segment Any Object Model&#65288;SAOM&#65289;&#22312;&#8220;&#19968;&#20999;&#8221;&#27169;&#24335;&#19979;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#36817;&#37051;&#20998;&#37197;&#26041;&#27861;&#65292;&#26356;&#26032;&#27599;&#20010;&#22320;&#38754;&#30495;&#23454;&#25513;&#27169;&#30340;&#28857;&#23884;&#20837;&#12290;SAOM&#22312;&#25105;&#20204;&#20174;Ai2T&#25910;&#38598;&#30340;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10780v1 Announce Type: cross  Abstract: Multi-class multi-instance segmentation is the task of identifying masks for multiple object classes and multiple instances of the same class within an image. The foundational Segment Anything Model (SAM) is designed for promptable multi-class multi-instance segmentation but tends to output part or sub-part masks in the "everything" mode for various real-world applications. Whole object segmentation masks play a crucial role for indoor scene understanding, especially in robotics applications. We propose a new domain invariant Real-to-Simulation (Real-Sim) fine-tuning strategy for SAM. We use object images and ground truth data collected from Ai2Thor simulator during fine-tuning (real-to-sim). To allow our Segment Any Object Model (SAOM) to work in the "everything" mode, we propose the novel nearest neighbour assignment method, updating point embeddings for each ground-truth mask. SAOM is evaluated on our own dataset collected from Ai2T
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#29983;&#25104;&#24335;AI&#20013;&#30340;&#31038;&#20250;&#26377;&#23475;&#24433;&#21709;&#65292;&#25552;&#20986;&#23545;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#20851;&#20999;&#65292;&#24182;&#24341;&#39046;&#35752;&#35770;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.10776</link><description>&lt;p&gt;
&#20174;&#22823;&#29076;&#28809;&#21040;&#35823;&#20256;&#65306;&#25506;&#35752;&#29983;&#25104;&#24335;AI&#20013;&#30340;&#26377;&#23475;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
From Melting Pots to Misrepresentations: Exploring Harms in Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10776
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#29983;&#25104;&#24335;AI&#20013;&#30340;&#31038;&#20250;&#26377;&#23475;&#24433;&#21709;&#65292;&#25552;&#20986;&#23545;&#22810;&#26679;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#20851;&#20999;&#65292;&#24182;&#24341;&#39046;&#35752;&#35770;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Gemini&#21644;GPT&#31561;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#32435;&#20837;AI&#20316;&#20026;&#26381;&#21153;&#65288;AIaaS&#65289;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#65292;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#31038;&#20250;&#20154;&#21475;&#32500;&#24230;&#19978;&#30340;&#27495;&#35270;&#20542;&#21521;&#20173;&#23384;&#22312;&#20851;&#20999;&#65292;&#23588;&#20854;&#26159;&#20542;&#21521;&#20110;&#26576;&#20123;&#8220;&#22810;&#25968;&#32676;&#20307;&#8221;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#12290;&#23613;&#31649;&#26377;&#20851;&#23186;&#20307;&#21628;&#21505;&#22810;&#26679;&#21270;&#20195;&#34920;&#30340;&#24191;&#27867;&#65292;&#20294;&#22312;AIaaS&#32972;&#26223;&#19979;&#65292;&#36793;&#32536;&#21270;&#31181;&#26063;&#21644;&#27665;&#26063;&#32676;&#20307;&#20173;&#28982;&#38754;&#20020;&#25345;&#32493;&#25197;&#26354;&#12289;&#21051;&#26495;&#21360;&#35937;&#21644;&#24573;&#35270;&#12290;&#26412;&#25991;&#23545;&#31038;&#20250;&#26377;&#23475;&#30740;&#31350;&#29616;&#29366;&#36827;&#34892;&#20102;&#20851;&#38190;&#24635;&#32467;&#65292;&#24341;&#39046;&#35752;&#35770;&#37325;&#28857;&#25918;&#22312;&#23427;&#20204;&#30340;&#24433;&#21709;&#19978;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26412;&#35752;&#35770;&#25351;&#24341;&#19979;&#30340;&#24320;&#25918;&#24335;&#30740;&#31350;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;&#30830;&#23450;&#26410;&#26469;&#30740;&#31350;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10776v1 Announce Type: cross  Abstract: With the widespread adoption of advanced generative models such as Gemini and GPT, there has been a notable increase in the incorporation of such models into sociotechnical systems, categorized under AI-as-a-Service (AIaaS). Despite their versatility across diverse sectors, concerns persist regarding discriminatory tendencies within these models, particularly favoring selected `majority' demographics across various sociodemographic dimensions. Despite widespread calls for diversification of media representations, marginalized racial and ethnic groups continue to face persistent distortion, stereotyping, and neglect within the AIaaS context. In this work, we provide a critical summary of the state of research in the context of social harms to lead the conversation to focus on their implications. We also present open-ended research questions, guided by our discussion, to help define future research pathways.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ECRC&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#35789;&#32423;&#21644;&#21477;&#23376;&#32423;&#23884;&#20837;&#65292;&#20197;&#21450;&#22522;&#20110;&#26032;&#39062;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#22312;&#38889;&#22269;&#20250;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24773;&#32490;&#22240;&#26524;&#35782;&#21035;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.10764</link><description>&lt;p&gt;
&#38889;&#22269;&#20250;&#35805;&#20013;&#24773;&#32490;&#22240;&#26524;&#35782;&#21035;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ECRC: Emotion-Causality Recognition in Korean Conversation for GCN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ECRC&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#35789;&#32423;&#21644;&#21477;&#23376;&#32423;&#23884;&#20837;&#65292;&#20197;&#21450;&#22522;&#20110;&#26032;&#39062;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#22312;&#38889;&#22269;&#20250;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24773;&#32490;&#22240;&#26524;&#35782;&#21035;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#22810;&#20219;&#21153;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21516;&#26102;&#20998;&#26512;&#20102;&#20250;&#35805;&#29615;&#22659;&#20013;&#30340;&#24773;&#24863;&#21450;&#20854;&#28508;&#22312;&#21407;&#22240;&#65292;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26377;&#25928;&#22788;&#29702;&#21644;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#20197;&#24448;&#23884;&#20837;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21033;&#29992;&#20102;&#21333;&#35789;&#32423;&#21644;&#21477;&#23376;&#32423;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26032;&#39062;&#22270;&#32467;&#26500;&#30340;&#24773;&#32490;&#22240;&#26524;&#35782;&#21035;&#27169;&#22411;&#65288;ECRC&#65289;&#65292;&#20174;&#32780;&#20511;&#37492;&#20102;&#20004;&#31181;&#23884;&#20837;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#35813;&#27169;&#22411;&#29420;&#29305;&#22320;&#38598;&#25104;&#20102;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;Bi-LSTM&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10764v1 Announce Type: cross  Abstract: In this multi-task learning study on simultaneous analysis of emotions and their underlying causes in conversational contexts, deep neural network methods were employed to effectively process and train large labeled datasets. However, these approaches are typically limited to conducting context analyses across the entire corpus because they rely on one of the two methods: word- or sentence-level embedding. The former struggles with polysemy and homonyms, whereas the latter causes information loss when processing long sentences. In this study, we overcome the limitations of previous embeddings by utilizing both word- and sentence-level embeddings. Furthermore, we propose the emotion-causality recognition in conversation (ECRC) model, which is based on a novel graph structure, thereby leveraging the strengths of both embedding methods. This model uniquely integrates the bidirectional long short-term memory (Bi-LSTM) and graph neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23545;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;&#36827;&#34892;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#20154;&#26426;&#22312;&#35266;&#23519;&#20219;&#21153;&#20013;&#30340;&#33322;&#32447;&#35268;&#21010;&#21644;&#20805;&#30005;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10761</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;
&lt;/p&gt;
&lt;p&gt;
Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23545;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;&#36827;&#34892;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#20154;&#26426;&#22312;&#35266;&#23519;&#20219;&#21153;&#20013;&#30340;&#33322;&#32447;&#35268;&#21010;&#21644;&#20805;&#30005;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#23545;&#20351;&#29992;&#26080;&#32447;&#20805;&#30005;&#22120;&#24310;&#38271;&#26080;&#20154;&#26426;&#25805;&#20316;&#23551;&#21629;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#21463;&#21040;&#20805;&#30005;&#22120;&#36741;&#21161;&#30340;&#26080;&#20154;&#26426;&#24212;&#29992;&#65306;&#26080;&#20154;&#26426;&#37096;&#32626;&#21040;&#35266;&#23519;&#19968;&#32452;&#20852;&#36259;&#28857;&#65292;&#32780;&#20805;&#30005;&#22120;&#21487;&#20197;&#31227;&#21160;&#20197;&#20026;&#26080;&#20154;&#26426;&#20805;&#30005;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;&#30340;&#33322;&#32447;&#21644;&#20805;&#30005;&#35745;&#21010;&#65292;&#20197;&#22312;&#23613;&#21487;&#33021;&#30701;&#30340;&#26102;&#38388;&#20869;&#33719;&#24471;&#39640;&#35266;&#23519;&#25928;&#29992;&#65292;&#21516;&#26102;&#30830;&#20445;&#26080;&#20154;&#26426;&#22312;&#20219;&#21153;&#25191;&#34892;&#26399;&#38388;&#20445;&#25345;&#21487;&#25805;&#20316;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26080;&#20154;&#26426;-&#20805;&#30005;&#22120;&#35843;&#24230;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#65292;&#22312;&#35813;&#36807;&#31243;&#20013;&#65292;&#26080;&#20154;&#26426;&#21644;&#31227;&#21160;&#20805;&#30005;&#22120;&#20805;&#24403;&#20004;&#20010;&#20195;&#29702;&#21830;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#12290;&#36825;&#20004;&#20010;&#20195;&#29702;&#21830;&#30340;&#31163;&#25955;&#36830;&#32493;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#22312;&#25105;&#20204;&#30340;&#38382;&#39064;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#21160;&#20316;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10761v1 Announce Type: new  Abstract: Recently there has been a growing interest in industry and academia, regarding the use of wireless chargers to prolong the operational longevity of unmanned aerial vehicles (commonly knowns as drones). In this paper we consider a charger-assisted drone application: a drone is deployed to observe a set points of interest, while a charger can move to recharge the drone's battery. We focus on the route and charging schedule of the drone and the mobile charger, to obtain high observation utility with the shortest possible time, while ensuring the drone remains operational during task execution. Essentially, this proposed drone-charger scheduling problem is a multi-stage decision-making process, in which the drone and the mobile charger act as two agents who cooperate to finish a task. The discrete-continuous hybrid action space of the two agents poses a significant challenge in our problem. To address this issue, we present a hybrid-action d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LIGHTCODE&#36731;&#37327;&#32423;&#31070;&#32463;&#32534;&#30721;&#26041;&#26696;&#65292;&#22312;&#20855;&#22791;&#35299;&#37322;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;&#21306;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10751</link><description>&lt;p&gt;
LIGHTCODE&#65306;&#20855;&#26377;&#21453;&#39304;&#36890;&#36947;&#30340;&#20809;&#35299;&#26512;&#21644;&#31070;&#32463;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LIGHTCODE: Light Analytical and Neural Codes for Channels with Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LIGHTCODE&#36731;&#37327;&#32423;&#31070;&#32463;&#32534;&#30721;&#26041;&#26696;&#65292;&#22312;&#20855;&#22791;&#35299;&#37322;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;&#21306;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36947;&#21453;&#39304;&#20013;&#21487;&#38752;&#19988;&#39640;&#25928;&#30340;&#32534;&#30721;&#26041;&#26696;&#35774;&#35745;&#19968;&#30452;&#26159;&#36890;&#20449;&#29702;&#35770;&#20013;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#31070;&#32463;&#32534;&#30721;&#24448;&#24448;&#38754;&#20020;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#26377;&#38480;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#35299;&#37322;&#24615;&#24378;&#19988;&#26356;&#36866;&#29992;&#20110;&#36890;&#20449;&#31995;&#32479;&#30340;&#20302;&#22797;&#26434;&#24230;&#32534;&#30721;&#26041;&#26696;&#12290;&#25105;&#20204;&#20808;&#36827;&#20102;&#35299;&#26512;&#32534;&#30721;&#21644;&#31070;&#32463;&#32534;&#30721;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;POWERBLAST&#65292;&#19968;&#31181;&#21463;Schalkwijk-Kailath&#65288;SK&#65289;&#21644;Gallager-Nakiboglu&#65288;GN&#65289;&#26041;&#26696;&#21551;&#21457;&#30340;&#35299;&#26512;&#32534;&#30721;&#26041;&#26696;&#65292;&#22312;&#39640;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#21306;&#22495;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#21487;&#38752;&#24615;&#25913;&#36827;&#65292;&#32988;&#36807;&#31070;&#32463;&#32534;&#30721;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#22686;&#24378;&#20302;SNR&#21306;&#22495;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LIGHTCODE&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10751v1 Announce Type: cross  Abstract: The design of reliable and efficient codes for channels with feedback remains a longstanding challenge in communication theory. While significant improvements have been achieved by leveraging deep learning techniques, neural codes often suffer from high computational costs, a lack of interpretability, and limited practicality in resource-constrained settings. We focus on designing low-complexity coding schemes that are interpretable and more suitable for communication systems. We advance both analytical and neural codes. First, we demonstrate that POWERBLAST, an analytical coding scheme inspired by Schalkwijk-Kailath (SK) and Gallager-Nakiboglu (GN) schemes, achieves notable reliability improvements over both SK and GN schemes, outperforming neural codes in high signal-to-noise ratio (SNR) regions. Next, to enhance reliability in low-SNR regions, we propose LIGHTCODE, a lightweight neural code that achieves state-of-the-art reliability
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21517;&#20026;DORIS&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20998;&#26512;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24086;&#23376;&#21382;&#21490;&#35760;&#24405;&#26469;&#30830;&#23450;&#25233;&#37057;&#30151;&#24739;&#32773;&#65292;&#20197;&#25552;&#39640;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.10750</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Depression Detection on Social Media with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10750
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21517;&#20026;DORIS&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20998;&#26512;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24086;&#23376;&#21382;&#21490;&#35760;&#24405;&#26469;&#30830;&#23450;&#25233;&#37057;&#30151;&#24739;&#32773;&#65292;&#20197;&#25552;&#39640;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#36896;&#25104;&#21361;&#23475;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#24515;&#29702;&#20581;&#24247;&#24847;&#35782;&#21644;&#23545;&#30149;&#30151;&#32827;&#36785;&#24863;&#30340;&#24656;&#24807;&#65292;&#35768;&#22810;&#24739;&#32773;&#24182;&#26410;&#31215;&#26497;&#23547;&#27714;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#23548;&#33268;&#19981;&#21033;&#21518;&#26524;&#12290;&#25233;&#37057;&#30151;&#26816;&#27979;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20010;&#20154;&#24086;&#23376;&#30340;&#21382;&#21490;&#35760;&#24405;&#26469;&#30830;&#23450;&#20010;&#20307;&#26159;&#21542;&#24739;&#26377;&#25233;&#37057;&#30151;&#65292;&#36825;&#21487;&#26174;&#33879;&#26377;&#21161;&#20110;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;&#23427;&#20027;&#35201;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#38656;&#35201;&#19987;&#19994;&#21307;&#23398;&#30693;&#35782;&#65292;2&#65289;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DORIS&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#21307;&#23398;&#30693;&#35782;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#39318;&#20808;&#23545;&#39640;&#21361;&#25991;&#26412;&#36827;&#34892;&#26631;&#27880;&#20197;&#30830;&#23450;&#26159;&#21542;&#31526;&#21512;&#21307;&#23398;&#35786;&#26029;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10750v1 Announce Type: cross  Abstract: Depression harms. However, due to a lack of mental health awareness and fear of stigma, many patients do not actively seek diagnosis and treatment, leading to detrimental outcomes. Depression detection aims to determine whether an individual suffers from depression by analyzing their history of posts on social media, which can significantly aid in early detection and intervention. It mainly faces two key challenges: 1) it requires professional medical knowledge, and 2) it necessitates both high accuracy and explainability. To address it, we propose a novel depression detection system called DORIS, combining medical knowledge and the recent advances in large language models (LLMs). Specifically, to tackle the first challenge, we proposed an LLM-based solution to first annotate whether high-risk texts meet medical diagnostic criteria. Further, we retrieve texts with high emotional intensity and summarize critical information from the his
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#34892;&#30149;&#25919;&#31574;&#21046;&#23450;&#27169;&#22411;-&#25919;&#31574;&#32452;&#21512;&#32508;&#21512;&#65288;PCS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#23398;&#20064;&#26469;&#36991;&#20813;&#26497;&#31471;&#20915;&#31574;&#65292;&#25552;&#39640;&#25919;&#31574;&#30340;&#20154;&#24615;&#21270;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#27425;&#20248;&#21382;&#21490;&#25919;&#31574;&#23545;&#20915;&#31574;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.10744</link><description>&lt;p&gt;
&#28216;&#25103;&#19982;&#21442;&#32771;&#65306;&#27969;&#34892;&#30149;&#38450;&#25511;&#25919;&#31574;&#32452;&#21512;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Game and Reference: Policy Combination Synthesis for Epidemic Prevention and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#34892;&#30149;&#25919;&#31574;&#21046;&#23450;&#27169;&#22411;-&#25919;&#31574;&#32452;&#21512;&#32508;&#21512;&#65288;PCS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#23398;&#20064;&#26469;&#36991;&#20813;&#26497;&#31471;&#20915;&#31574;&#65292;&#25552;&#39640;&#25919;&#31574;&#30340;&#20154;&#24615;&#21270;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#27425;&#20248;&#21382;&#21490;&#25919;&#31574;&#23545;&#20915;&#31574;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#27969;&#34892;&#30149;&#21046;&#23450;&#25919;&#31574;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#29992;&#26469;&#20026;&#21508;&#32423;&#25919;&#24220;&#25552;&#20379;&#21046;&#23450;&#38450;&#25511;SARS&#12289;H1N1&#21644;COVID-19&#31561;&#28798;&#38590;&#24615;&#27969;&#34892;&#30149;&#25919;&#31574;&#30340;&#21442;&#32771;&#12290;&#29616;&#26377;&#30740;&#31350;&#30446;&#21069;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#26082;&#26377;&#26041;&#27861;&#22522;&#20110;&#25928;&#26524;&#35780;&#20272;&#21046;&#23450;&#25919;&#31574;&#65292;&#30001;&#20110;&#30495;&#23454;&#20915;&#31574;&#20013;&#24456;&#23569;&#33021;&#34987;&#24314;&#27169;&#30340;&#22240;&#32032;&#65292;&#36755;&#20986;&#30340;&#25919;&#31574;&#24456;&#23481;&#26131;&#21464;&#24471;&#26497;&#31471;&#12290;&#20854;&#27425;&#65292;&#20154;&#31867;&#30340;&#20027;&#35266;&#24615;&#21644;&#35748;&#30693;&#23616;&#38480;&#24615;&#20351;&#24471;&#21382;&#21490;&#25919;&#31574;&#24182;&#19981;&#24635;&#26159;&#26368;&#20248;&#30340;&#20915;&#31574;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#34892;&#30149;&#25919;&#31574;&#21046;&#23450;&#27169;&#22411;-&#25919;&#31574;&#32452;&#21512;&#32508;&#21512;&#65288;PCS&#65289;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#20026;&#39044;&#38450;&#26497;&#31471;&#20915;&#31574;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#21046;&#23450;&#25919;&#31574;&#21644;&#23454;&#38469;&#25919;&#31574;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#65292;&#20197;&#36843;&#20351;&#36755;&#20986;&#30340;&#25919;&#31574;&#26356;&#31526;&#21512;&#20154;&#31867;&#21916;&#22909;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20026;&#20102;&#20943;&#23567;&#27425;&#20248;&#21382;&#21490;&#25919;&#31574;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10744v1 Announce Type: new  Abstract: In recent years, epidemic policy-making models are increasingly being used to provide reference for governors on prevention and control policies against catastrophic epidemics such as SARS, H1N1 and COVID-19. Existing studies are currently constrained by two issues: First, previous methods develop policies based on effect evaluation, since few of factors in real-world decision-making can be modeled, the output policies will then easily become extreme. Second, the subjectivity and cognitive limitation of human make the historical policies not always optimal for the training of decision models. To these ends, we present a novel Policy Combination Synthesis (PCS) model for epidemic policy-making. Specially, to prevent extreme decisions, we introduce adversarial learning between the model-made policies and the real policies to force the output policies to be more human-liked. On the other hand, to minimize the impact of sub-optimal historica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#21033;&#29992;&#22870;&#21169;&#20998;&#24067;&#26041;&#24046;&#21644;&#21464;&#21270;&#39044;&#31639;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#19978;&#38480;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.10732</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#24179;&#31283;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Variance-Dependent Regret Bounds for Non-stationary Linear Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#21033;&#29992;&#22870;&#21169;&#20998;&#24067;&#26041;&#24046;&#21644;&#21464;&#21270;&#39044;&#31639;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#19978;&#38480;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#24179;&#31283;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#20998;&#24067;&#27599;&#19968;&#36718;&#37117;&#22312;&#28436;&#21464;&#12290;&#29616;&#26377;&#31639;&#27861;&#36890;&#36807;&#24635;&#21464;&#21270;&#39044;&#31639;$B_K$&#26469;&#34920;&#24449;&#38750;&#24179;&#31283;&#24615;&#65292;&#35813;&#39044;&#31639;&#26159;&#32447;&#24615;&#36172;&#21338;&#26426;&#27599;$K$&#36718;&#36830;&#32493;&#29305;&#24449;&#21521;&#37327;&#21464;&#21270;&#30340;&#24635;&#21644;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#37327;&#21482;&#34913;&#37327;&#20102;&#30456;&#23545;&#20110;&#22870;&#21169;&#20998;&#24067;&#26399;&#26395;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#36825;&#20351;&#24471;&#29616;&#26377;&#31639;&#27861;&#22312;&#19968;&#33324;&#38750;&#24179;&#31283;&#20998;&#24067;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#22870;&#21169;&#20998;&#24067;&#26041;&#24046;&#20197;&#21450;$B_K$&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#19978;&#38480;&#30028;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;: &#37325;&#26032;&#21551;&#21160;&#30340;&#21152;&#26435;$\text{OFUL}^+$&#21644;&#37325;&#26032;&#21551;&#21160;&#30340;$\text{SAVE}^+$&#12290;&#36825;&#20123;&#31639;&#27861;&#20998;&#21035;&#22788;&#29702;&#20102;&#22870;&#21169;&#26041;&#24046;&#20449;&#24687;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10732v1 Announce Type: cross  Abstract: We investigate the non-stationary stochastic linear bandit problem where the reward distribution evolves each round. Existing algorithms characterize the non-stationarity by the total variation budget $B_K$, which is the summation of the change of the consecutive feature vectors of the linear bandits over $K$ rounds. However, such a quantity only measures the non-stationarity with respect to the expectation of the reward distribution, which makes existing algorithms sub-optimal under the general non-stationary distribution setting. In this work, we propose algorithms that utilize the variance of the reward distribution as well as the $B_K$, and show that they can achieve tighter regret upper bounds. Specifically, we introduce two novel algorithms: Restarted Weighted$\text{OFUL}^+$ and Restarted $\text{SAVE}^+$. These algorithms address cases where the variance information of the rewards is known and unknown, respectively. Notably, when
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20005;&#26684;&#20998;&#21306;&#35843;&#24230;&#31574;&#30053;&#65292;&#29992;&#20110;&#38646;&#26143;&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#21644;&#22788;&#29702;&#22120;&#20998;&#21306;&#65292;&#24182;&#23581;&#35797;&#23558;&#30456;&#20284;&#23481;&#37327;&#30340;&#20219;&#21153;&#20998;&#37197;&#32473;&#21516;&#19968;&#20998;&#21306;&#65292;&#20197;&#20943;&#23569;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.10726</link><description>&lt;p&gt;
&#38024;&#23545;&#38646;&#26143;&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#30340;&#20005;&#26684;&#20998;&#21306;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Strict Partitioning for Sporadic Rigid Gang Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10726
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20005;&#26684;&#20998;&#21306;&#35843;&#24230;&#31574;&#30053;&#65292;&#29992;&#20110;&#38646;&#26143;&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#21644;&#22788;&#29702;&#22120;&#20998;&#21306;&#65292;&#24182;&#23581;&#35797;&#23558;&#30456;&#20284;&#23481;&#37327;&#30340;&#20219;&#21153;&#20998;&#37197;&#32473;&#21516;&#19968;&#20998;&#21306;&#65292;&#20197;&#20943;&#23569;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#27169;&#22411;&#22522;&#20110;&#22312;&#22266;&#23450;&#25968;&#37327;&#30340;&#22788;&#29702;&#22120;&#19978;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#32447;&#31243;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#24605;&#24819;&#12290;&#34429;&#28982;&#20840;&#23616;&#21018;&#24615;&#27969;&#24335;&#35843;&#24230;&#26377;&#22823;&#37327;&#25991;&#29486;&#65292;&#20294;&#20998;&#21306;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#23454;&#38469;&#20248;&#21183;&#65288;&#20363;&#22914;&#20219;&#21153;&#38548;&#31163;&#21644;&#20943;&#23569;&#35843;&#24230;&#24320;&#38144;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#30340;&#20998;&#21306;&#35843;&#24230;&#31574;&#30053;&#65292;&#31216;&#20026;&#20005;&#26684;&#20998;&#21306;&#12290;&#35813;&#26041;&#27861;&#21019;&#24314;&#20219;&#21153;&#21644;&#22788;&#29702;&#22120;&#30340;&#19981;&#30456;&#20132;&#20998;&#21306;&#65292;&#20197;&#36991;&#20813;&#20998;&#21306;&#38388;&#24178;&#25200;&#12290;&#27492;&#22806;&#65292;&#23427;&#23581;&#35797;&#23558;&#20855;&#26377;&#30456;&#20284;&#23481;&#37327;&#65288;&#21363;&#24182;&#34892;&#24615;&#65289;&#30340;&#20219;&#21153;&#20998;&#37197;&#32473;&#21516;&#19968;&#20998;&#21306;&#65292;&#20197;&#20943;&#23569;&#20998;&#21306;&#20869;&#24178;&#25200;&#12290;&#22312;&#27599;&#20010;&#20998;&#21306;&#20869;&#65292;&#20219;&#21153;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#31867;&#22411;&#30340;&#35843;&#24230;&#22120;&#36827;&#34892;&#35843;&#24230;&#65292;&#36825;&#20801;&#35768;&#20351;&#29992;&#19981;&#37027;&#20040;&#24754;&#35266;&#30340;&#21487;&#35843;&#24230;&#27979;&#35797;&#12290;&#22823;&#37327;&#30340;&#21512;&#25104;&#23454;&#39564;&#35777;&#26126;&#21644;&#22522;&#20110;Edge TPU&#22522;&#20934;&#30340;&#26696;&#20363;&#30740;&#31350;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10726v1 Announce Type: cross  Abstract: The rigid gang task model is based on the idea of executing multiple threads simultaneously on a fixed number of processors to increase efficiency and performance. Although there is extensive literature on global rigid gang scheduling, partitioned approaches have several practical advantages (e.g., task isolation and reduced scheduling overheads). In this paper, we propose a new partitioned scheduling strategy for rigid gang tasks, named strict partitioning. The method creates disjoint partitions of tasks and processors to avoid inter-partition interference. Moreover, it tries to assign tasks with similar volumes (i.e., parallelisms) to the same partition so that the intra-partition interference can be reduced. Within each partition, the tasks can be scheduled using any type of scheduler, which allows the use of a less pessimistic schedulability test. Extensive synthetic experiments and a case study based on Edge TPU benchmarks show th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#21644;&#35780;&#20272;&#20004;&#20010;MCTS&#31639;&#27861;&#21464;&#20307;&#65292;&#21457;&#29616;&#36798;&#33452;&#22855;&#23494;&#30721;&#26827;&#30424;&#28216;&#25103;&#20013;&#30340;&#20998;&#25903;&#20998;&#27495;&#26126;&#26174;&#38459;&#30861;&#20102;GPU&#25191;&#34892;&#26102;&#30340;&#24182;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10720</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#24212;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#27169;&#25311;&#36798;&#33452;&#22855;&#23494;&#30721;&#28216;&#25103;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#21644;&#35780;&#20272;&#20004;&#20010;MCTS&#31639;&#27861;&#21464;&#20307;&#65292;&#21457;&#29616;&#36798;&#33452;&#22855;&#23494;&#30721;&#26827;&#30424;&#28216;&#25103;&#20013;&#30340;&#20998;&#25903;&#20998;&#27495;&#26126;&#26174;&#38459;&#30861;&#20102;GPU&#25191;&#34892;&#26102;&#30340;&#24182;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#25928;&#29575;&#65292;&#36825;&#26159;&#19968;&#31181;&#33879;&#21517;&#30340;&#20915;&#31574;&#31639;&#27861;&#65292;&#20197;&#20854;&#22312;&#22797;&#26434;&#20915;&#31574;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#38395;&#21517;&#65292;&#21462;&#20915;&#20110;&#36827;&#34892;&#30340;&#27169;&#25311;&#25968;&#37327;&#12290;&#23613;&#31649;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#20294;&#22312;&#26576;&#20123;&#22330;&#26223;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#28216;&#25103;&#31574;&#30053;&#24320;&#21457;&#39046;&#22495;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#36798;&#33452;&#22855;&#23494;&#30721;&#26827;&#30424;&#28216;&#25103;&#20013;&#22266;&#26377;&#30340;&#20998;&#25903;&#20998;&#27495;&#26174;&#33879;&#38459;&#30861;&#20102;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#19978;&#25191;&#34892;&#26102;&#30340;&#24182;&#34892;&#24615;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#32454;&#33268;&#35780;&#20272;&#20102;&#20004;&#20010;&#21464;&#20307;&#30340;MCTS&#31639;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#20998;&#25903;&#20998;&#27495;&#23545;&#35745;&#31639;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#65292;&#19982;&#22522;&#20110;CPU&#30340;&#23454;&#29616;&#30456;&#27604;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#32447;&#24615;&#25913;&#36827;&#65292;&#19982;GPU&#23454;&#29616;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10720v1 Announce Type: new  Abstract: In this study, we explore the efficiency of the Monte Carlo Tree Search (MCTS), a prominent decision-making algorithm renowned for its effectiveness in complex decision environments, contingent upon the volume of simulations conducted. Notwithstanding its broad applicability, the algorithm's performance can be adversely impacted in certain scenarios, particularly within the domain of game strategy development. This research posits that the inherent branch divergence within the Da Vinci Code board game significantly impedes parallelism when executed on Graphics Processing Units (GPUs). To investigate this hypothesis, we implemented and meticulously evaluated two variants of the MCTS algorithm, specifically designed to assess the impact of branch divergence on computational performance. Our comparative analysis reveals a linear improvement in performance with the CPU-based implementation, in stark contrast to the GPU implementation, which 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#27604;&#20363;&#39044;&#27979;&#19968;&#33268;&#24615;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#34987;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#35782;&#21035;&#21518;&#38376;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#24178;&#20928;&#25968;&#25454;&#25110;&#25163;&#21160;&#23450;&#20041;&#21518;&#38376;&#26816;&#27979;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.10717</link><description>&lt;p&gt;
&#25581;&#31034;&#21518;&#38376;&#31192;&#23494;&#65306;&#21033;&#29992;&#20248;&#21270;&#30340;&#27604;&#20363;&#39044;&#27979;&#19968;&#33268;&#24615;&#35782;&#21035;&#21518;&#38376;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10717
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#27604;&#20363;&#39044;&#27979;&#19968;&#33268;&#24615;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#34987;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#35782;&#21035;&#21518;&#38376;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#24178;&#20928;&#25968;&#25454;&#25110;&#25163;&#21160;&#23450;&#20041;&#21518;&#38376;&#26816;&#27979;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#24120;&#38656;&#35201;&#20511;&#21161;&#22806;&#37096;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20570;&#27861;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#27602;&#21270;&#25915;&#20987;&#12290;&#20808;&#21069;&#30340;&#21518;&#38376;&#38450;&#24481;&#31574;&#30053;&#20027;&#35201;&#38598;&#20013;&#22312;&#35782;&#21035;&#34987;&#26893;&#20837;&#21518;&#38376;&#30340;&#27169;&#22411;&#25110;&#34987;&#27602;&#21270;&#25968;&#25454;&#30340;&#29305;&#24449;&#19978;&#65292;&#36890;&#24120;&#22312;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#24178;&#20928;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36816;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#30456;&#23545;&#19981;&#22826;&#34987;&#25506;&#32034;&#30340;&#25361;&#25112;&#65306;&#22312;&#34987;&#27602;&#21270;&#30340;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#35782;&#21035;&#21518;&#38376;&#25968;&#25454;&#65292;&#32780;&#19988;&#22312;&#29616;&#23454;&#26465;&#20214;&#19979;&#65292;&#21363;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24178;&#20928;&#25968;&#25454;&#25110;&#19981;&#38656;&#35201;&#25163;&#21160;&#23450;&#20041;&#21518;&#38376;&#26816;&#27979;&#30340;&#38408;&#20540;&#12290;&#25105;&#20204;&#20174;&#27604;&#20363;&#39044;&#27979;&#19968;&#33268;&#24615;&#65288;SPC&#65289;&#25216;&#26415;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#34987;&#27602;&#21270;&#25968;&#25454;&#23545;&#36755;&#20837;&#32553;&#25918;&#22240;&#23376;&#30340;&#39044;&#27979;&#19981;&#21464;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#23558;&#21518;&#38376;&#25968;&#25454;&#35782;&#21035;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#23618;&#25968;&#25454;&#21010;&#20998;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10717v1 Announce Type: cross  Abstract: Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, i.e., without the need for additional clean data or without manually defining a threshold for backdoor detection. We draw an inspiration from the scaled prediction consistency (SPC) technique, which exploits the prediction invariance of poisoned data to an input scaling factor. Based on this, we pose the backdoor data identification problem as a hierarchical data splitting optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10707</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#38598;&#25104;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#30340;&#28508;&#22312;&#20027;&#39064;&#65306;&#27668;&#20505;&#36816;&#21160;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25581;&#31034;&#21644;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#37492;&#20110;&#20256;&#32479;&#20027;&#39064;&#32423;&#20998;&#26512;&#30340;&#23616;&#38480;&#24615;&#65292;&#24448;&#24448;&#21482;&#25429;&#25417;&#21040;&#25972;&#20307;&#27169;&#24335;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#26356;&#31934;&#32454;&#12289;&#20027;&#39064;&#32858;&#28966;&#30340;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#28041;&#21450;&#25163;&#21160;&#27969;&#31243;&#21644;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#22312;&#20280;&#32553;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36164;&#28304;&#24378;&#24230;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20808;&#36827;&#21151;&#33021;&#30340;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#26356;&#28145;&#20837;&#22320;&#35843;&#26597;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#30340;&#20027;&#39064;&#26041;&#38754;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#22810;&#26679;&#30340;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#26356;&#24191;&#27867;&#20027;&#39064;&#20869;&#26377;&#30340;&#24494;&#22937;&#32454;&#33410;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;PERL&#65289;&#65292;&#33021;&#22815;&#22312;&#19982;&#20256;&#32479;RLHF&#35774;&#32622;&#30456;&#24403;&#30340;&#24615;&#33021;&#19979;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10704</link><description>&lt;p&gt;
PERL: &#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PERL: Parameter Efficient Reinforcement Learning from Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10704
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;PERL&#65289;&#65292;&#33021;&#22815;&#22312;&#19982;&#20256;&#32479;RLHF&#35774;&#32622;&#30456;&#24403;&#30340;&#24615;&#33021;&#19979;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;RLHF&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#19988;&#25972;&#20010;&#36807;&#31243;&#22797;&#26434;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#65292;&#20854;&#20013;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#32993;&#31561;&#20154;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#8220;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#8221;&#65288;PERL&#65289;&#30340;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;LoRA&#36827;&#34892;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;PERL&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#65288;&#20840;&#35843;&#65289;&#22312;&#21253;&#25324;2&#20010;&#26032;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;7&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#22870;&#21169;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#21508;&#31181;&#37197;&#32622;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;PERL&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#30340;RLHF&#35774;&#32622;&#30456;&#24403;&#65292;&#21516;&#26102;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23569;&#12290;&#36825;&#20351;&#24471;RLHF&#20855;&#26377;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10704v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of "Parameter Efficient Reinforcement Learning" (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;</title><link>https://arxiv.org/abs/2403.10700</link><description>&lt;p&gt;
&#27880;&#24847;&#38169;&#35823;&#65281;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25351;&#20196;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation in Continuous Environments (VLN-CE) &#26159;&#19968;&#39033;&#30452;&#35266;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#39564;&#26234;&#33021;&#20219;&#21153;&#12290;&#20195;&#29702;&#20154;&#34987;&#35201;&#27714;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#20302;&#32423;&#21160;&#20316;&#12289;&#36981;&#24490;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#23548;&#33322;&#21040;&#30446;&#26631;&#30446;&#26631;&#12290;&#25152;&#26377;&#25991;&#29486;&#20013;&#30340; VLN-CE &#26041;&#27861;&#37117;&#20551;&#35774;&#35821;&#35328;&#25351;&#20196;&#26159;&#20934;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20154;&#31867;&#32473;&#20986;&#30340;&#25351;&#20196;&#21487;&#33021;&#30001;&#20110;&#19981;&#20934;&#30830;&#30340;&#35760;&#24518;&#25110;&#28151;&#28102;&#32780;&#21253;&#21547;&#31354;&#38388;&#29615;&#22659;&#25551;&#36848;&#20013;&#30340;&#38169;&#35823;&#12290;&#24403;&#21069; VLN-CE &#22522;&#20934;&#27809;&#26377;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#65292;&#20351;&#24471; VLN-CE &#20013;&#30340;&#26368;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#26469;&#33258;&#20154;&#31867;&#29992;&#25143;&#30340;&#38169;&#35823;&#25351;&#20196;&#26102;&#21464;&#24471;&#33030;&#24369;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#20837;&#21508;&#31181;&#31867;&#22411;&#25351;&#20196;&#38169;&#35823;&#32771;&#34385;&#28508;&#22312;&#20154;&#31867;&#21407;&#22240;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#20026;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040; noticeable...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10700v1 Announce Type: cross  Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#22122;&#22768;MRI&#22270;&#20687;&#36827;&#34892;&#33041;&#37096;&#32959;&#30244;&#20998;&#31867;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10698</link><description>&lt;p&gt;
&#38024;&#23545;&#22122;&#22768;&#33041;&#37096;MRI&#30340;&#31283;&#20581;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Influence-based Training Methods for Noisy Brain MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10698
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#22122;&#22768;MRI&#22270;&#20687;&#36827;&#34892;&#33041;&#37096;&#32959;&#30244;&#20998;&#31867;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30830;&#20998;&#31867;&#33041;&#37096;&#32959;&#30244;&#23545;&#21450;&#26102;&#21644;&#20934;&#30830;&#27835;&#30103;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#20960;&#31181;&#22522;&#20110;&#32463;&#20856;&#22270;&#20687;&#22788;&#29702;&#25110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20998;&#31867;&#31639;&#27861;&#34987;&#25552;&#20986;&#26469;&#24555;&#36895;&#20998;&#31867;MR&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#65292;&#20294;&#22823;&#22810;&#25968;&#20551;&#23450;&#20102;&#35757;&#32451;&#25968;&#25454;&#26159;&#26080;&#22122;&#22768;&#30340;&#19981;&#20999;&#23454;&#38469;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22122;&#22768;&#30340;MR&#22270;&#20687;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#20998;&#31867;&#33041;&#37096;&#32959;&#30244;&#30340;&#22256;&#38590;&#20294;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31283;&#20581;&#20110;&#22122;&#22768;MRI&#35757;&#32451;&#25968;&#25454;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#65288;ISR&#65289;&#21644;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#26679;&#26412;&#25200;&#21160;&#65288;ISP&#65289;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#22522;&#20110;&#40065;&#26834;&#32479;&#35745;&#20013;&#30340;&#24433;&#21709;&#20989;&#25968;&#12290;&#22312;ISR&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#35757;&#32451;&#36807;&#31243;&#20013;&#26679;&#26412;&#23545;&#35757;&#32451;&#30340;&#24110;&#21161;/&#21361;&#23475;&#31243;&#24230;&#33258;&#36866;&#24212;&#22320;&#37325;&#26032;&#21152;&#26435;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#22312;ISP&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#24433;&#21709;&#24471;&#20998;&#37327;&#36523;&#23450;&#21046;&#24182;&#27880;&#20837;&#26377;&#24110;&#21161;&#30340;&#25200;&#21160;&#12290;ISR&#21644;ISP&#22343;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10698v1 Announce Type: cross  Abstract: Correctly classifying brain tumors is imperative to the prompt and accurate treatment of a patient. While several classification algorithms based on classical image processing or deep learning methods have been proposed to rapidly classify tumors in MR images, most assume the unrealistic setting of noise-free training data. In this work, we study a difficult but realistic setting of training a deep learning model on noisy MR images to classify brain tumors. We propose two training methods that are robust to noisy MRI training data, Influence-based Sample Reweighing (ISR) and Influence-based Sample Perturbation (ISP), which are based on influence functions from robust statistics. Using the influence functions, in ISR, we adaptively reweigh training examples according to how helpful/harmful they are to the training process, while in ISP, we craft and inject helpful perturbation proportional to the influence score. Both ISR and ISP harden
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXPLORER&#30340;&#25506;&#32034;&#24341;&#23548;&#25512;&#29702;&#20195;&#29702;&#65292;&#29992;&#20110;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#27867;&#21270;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#23545;&#35937;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.10692</link><description>&lt;p&gt;
EXPLORER&#65306;&#25506;&#32034;&#24341;&#23548;&#30340;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10692
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXPLORER&#30340;&#25506;&#32034;&#24341;&#23548;&#25512;&#29702;&#20195;&#29702;&#65292;&#29992;&#20110;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#27867;&#21270;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#23545;&#35937;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#28216;&#25103;&#65288;TBGs&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#19968;&#20010;&#37325;&#35201;&#38598;&#21512;&#65292;&#35201;&#27714;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXPLORER&#30340;&#25506;&#32034;&#24341;&#23548;&#25512;&#29702;&#20195;&#29702;&#65292;&#29992;&#20110;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;&#65292;&#26088;&#22312;&#35299;&#20915;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#27867;&#21270;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#23545;&#35937;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10692v1 Announce Type: cross  Abstract: Text-based games (TBGs) have emerged as an important collection of NLP tasks, requiring reinforcement learning (RL) agents to combine natural language understanding with reasoning. A key challenge for agents attempting to solve such tasks is to generalize across multiple games and demonstrate good performance on both seen and unseen objects. Purely deep-RL-based approaches may perform well on seen objects; however, they fail to showcase the same performance on unseen objects. Commonsense-infused deep-RL agents may work better on unseen data; unfortunately, their policies are often not interpretable or easily transferable. To tackle these issues, in this paper, we present EXPLORER which is an exploration-guided reasoning agent for textual reinforcement learning. EXPLORER is neurosymbolic in nature, as it relies on a neural module for exploration and a symbolic module for exploitation. It can also learn generalized symbolic policies and 
&lt;/p&gt;</description></item><item><title>MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2403.10691</link><description>&lt;p&gt;
MYTE&#65306;&#24418;&#24577;&#23398;&#39537;&#21160;&#30340;&#23383;&#33410;&#32534;&#30721;&#65292;&#29992;&#20110;&#26356;&#22909;&#12289;&#26356;&#20844;&#24179;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10691
&lt;/p&gt;
&lt;p&gt;
MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#22914;&#20309;&#26368;&#22909;&#22320;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#35789;&#27719;&#21644;&#25991;&#23383;&#30340;&#35821;&#35328;&#12290;&#23613;&#31649;&#24403;&#20195;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#19990;&#30028;&#25991;&#23383;&#31995;&#32479;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20559;&#21521;&#20110;&#20840;&#29699;&#35199;&#26041;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23569;&#25968;&#35821;&#35328;&#30340;&#25991;&#26412;&#24448;&#24448;&#34987;&#20998;&#21106;&#20026;&#19968;&#38271;&#20018;&#22312;&#35821;&#35328;&#23398;&#19978;&#27627;&#26080;&#24847;&#20041;&#30340;&#21333;&#20803;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#19981;&#24179;&#31561;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#36328;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#32534;&#30721;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#32422;&#23450;&#65288;MYTE&#65289;&#22522;&#20110;&#24418;&#24577;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24211;&#23384;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#27604;&#23383;&#31526;&#26356;&#24179;&#34913;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#23383;&#31526;&#12290;&#25105;&#20204;&#23637;&#31034;MYTE&#20026;&#25152;&#26377;99&#31181;&#20998;&#26512;&#35821;&#35328;&#20135;&#29983;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#20854;&#20013;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;&#36825;&#36827;&#32780;&#25913;&#21892;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
&lt;/p&gt;</description></item><item><title>AutoHLS&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;HLS&#30828;&#20214;&#35774;&#35745;&#20248;&#21270;&#65292;&#23454;&#29616;&#39640;&#36798;70&#20493;&#30340;&#25506;&#32034;&#26102;&#38388;&#21152;&#36895;</title><link>https://arxiv.org/abs/2403.10686</link><description>&lt;p&gt;
AutoHLS&#65306;&#23398;&#20064;&#21152;&#36895;HLS&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10686
&lt;/p&gt;
&lt;p&gt;
AutoHLS&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;HLS&#30828;&#20214;&#35774;&#35745;&#20248;&#21270;&#65292;&#23454;&#29616;&#39640;&#36798;70&#20493;&#30340;&#25506;&#32034;&#26102;&#38388;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#32508;&#21512;&#65288;HLS&#65289;&#26159;&#19968;&#31181;&#35774;&#35745;&#27969;&#31243;&#65292;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#29305;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22914;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#65292;&#32487;&#25215;&#65292;&#27169;&#26495;&#31561;&#65292;&#24555;&#36895;&#21407;&#22411;&#21270;&#30828;&#20214;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#25506;&#32034;&#21508;&#31181;&#35774;&#35745;&#31354;&#38388;&#21442;&#25968;&#21487;&#33021;&#38656;&#35201;&#30828;&#20214;&#24037;&#31243;&#24072;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#35774;&#35745;&#35268;&#26684;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AutoHLS&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#38598;&#25104;&#65292;&#21152;&#36895;HLS&#30828;&#20214;&#35774;&#35745;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#19987;&#27880;&#20110;HLS pragma&#25506;&#32034;&#21644;&#25805;&#20316;&#36716;&#25442;&#12290;&#23427;&#21033;&#29992;&#38598;&#25104;&#30340;DNN&#26469;&#39044;&#27979;&#22312;&#32473;&#23450;FPGA&#36164;&#28304;&#39044;&#31639;&#19979;&#30340;&#32508;&#21512;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#30340;&#28508;&#21147;&#65292;&#21462;&#20195;AutoHLS&#27969;&#27700;&#32447;&#20013;&#30340;&#32463;&#20856;DNNs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25506;&#32034;&#26102;&#38388;&#19978;&#21152;&#36895;&#20102;&#22810;&#36798;70&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10686v1 Announce Type: cross  Abstract: High-level synthesis (HLS) is a design flow that leverages modern language features and flexibility, such as complex data structures, inheritance, templates, etc., to prototype hardware designs rapidly. However, exploring various design space parameters can take much time and effort for hardware engineers to meet specific design specifications. This paper proposes a novel framework called AutoHLS, which integrates a deep neural network (DNN) with Bayesian optimization (BO) to accelerate HLS hardware design optimization. Our tool focuses on HLS pragma exploration and operation transformation. It utilizes integrated DNNs to predict synthesizability within a given FPGA resource budget. We also investigate the potential of emerging quantum neural networks (QNNs) instead of classical DNNs for the AutoHLS pipeline. Our experimental results demonstrate up to a 70-fold speedup in exploration time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#34562;&#32676;&#31639;&#27861;&#21644;&#22810;&#29238;&#20195;&#20132;&#21449;&#30340;&#31163;&#25955;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35266;&#23519;&#32773;&#34588;&#34562;&#36827;&#34892;&#29420;&#31435;&#19988;&#23494;&#38598;&#30340;&#37051;&#22495;&#25628;&#32034;&#26469;&#25913;&#36827;&#31639;&#27861;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10684</link><description>&lt;p&gt;
&#20351;&#29992;&#34562;&#32676;&#31639;&#27861;&#21644;&#22810;&#29238;&#20195;&#20132;&#21449;&#26041;&#27861;&#25913;&#36827;&#30340;&#31163;&#25955;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;&#26696;&#20363;&#30740;&#31350;&#65306;&#20998;&#37197;&#38382;&#39064;&#21644;&#22522;&#20934;&#20989;&#25968;&#65289;
&lt;/p&gt;
&lt;p&gt;
Improved discrete particle swarm optimization using Bee Algorithm and multi-parent crossover method (Case study: Allocation problem and benchmark functions)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#34562;&#32676;&#31639;&#27861;&#21644;&#22810;&#29238;&#20195;&#20132;&#21449;&#30340;&#31163;&#25955;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35266;&#23519;&#32773;&#34588;&#34562;&#36827;&#34892;&#29420;&#31435;&#19988;&#23494;&#38598;&#30340;&#37051;&#22495;&#25628;&#32034;&#26469;&#25913;&#36827;&#31639;&#27861;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#25216;&#26415;&#30456;&#27604;&#65292;&#31890;&#23376;&#32676;&#20248;&#21270;&#26356;&#24120;&#34987;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#20351;&#29992;&#19988;&#21464;&#24322;&#24615;&#20302;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#20248;&#21270;&#38382;&#39064;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#25214;&#21040;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#26159;&#22797;&#26434;&#30340;&#12290;&#27492;&#22806;&#65292;&#25913;&#21464;&#31639;&#27861;&#21464;&#37327;&#24182;&#19981;&#20250;&#23545;&#31639;&#27861;&#30340;&#25910;&#25947;&#20135;&#29983;&#24456;&#22823;&#24433;&#21709;&#12290;PSO&#31639;&#27861;&#21487;&#20197;&#19982;&#20854;&#20182;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#36816;&#31639;&#31526;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35266;&#23519;&#32773;&#22810;&#29238;&#20195;&#20132;&#21449;&#31163;&#25955;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;OMPCDPSO&#65289;&#12290;&#20026;&#20102;&#25913;&#36827;DPSO&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#22312;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#19978;&#20351;&#29992;&#20102;&#22810;&#29238;&#20195;&#20132;&#21449;&#12290;&#25105;&#20204;&#21033;&#29992;&#34562;&#32676;&#31639;&#27861;&#30340;&#35266;&#23519;&#32773;&#34588;&#34562;&#36827;&#34892;&#29420;&#31435;&#19988;&#23494;&#38598;&#30340;&#37051;&#22495;&#25628;&#32034;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#35266;&#23519;&#32773;&#34588;&#34562;&#21644;&#20132;&#21449;&#12290;&#23427;&#20204;&#36827;&#34892;&#23616;&#37096;&#25628;&#32034;&#65288;&#24320;&#21457;&#65289;&#21644;&#20840;&#23616;&#25628;&#32034;&#65288;&#25506;&#32034;&#65289;&#12290;&#36825;&#20123;&#25628;&#32034;&#20013;&#30340;&#27599;&#19968;&#20010;&#37117;&#26159;&#26368;&#20339;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10684v1 Announce Type: cross  Abstract: Compared to other techniques, particle swarm optimization is more frequently utilized because of its ease of use and low variability. However, it is complicated to find the best possible solution in the search space in large-scale optimization problems. Moreover, changing algorithm variables does not influence algorithm convergence much. The PSO algorithm can be combined with other algorithms. It can use their advantages and operators to solve this problem. Therefore, this paper proposes the onlooker multi-parent crossover discrete particle swarm optimization (OMPCDPSO). To improve the efficiency of the DPSO algorithm, we utilized multi-parent crossover on the best solutions. We performed an independent and intensive neighborhood search using the onlooker bees of the bee algorithm. The algorithm uses onlooker bees and crossover. They do local search (exploitation) and global search (exploration). Each of these searches is among the bes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10667</link><description>&lt;p&gt;
&#36890;&#21521;&#32479;&#19968;&#22810;&#27169;&#24335;&#20010;&#24615;&#21270;&#65306;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#36164;&#28304;&#24182;&#28385;&#36275;&#21508;&#31181;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#36890;&#29992;&#27169;&#22411;&#19968;&#30452;&#26159;&#31038;&#21306;&#28212;&#26395;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26085;&#24120;&#30340;&#36873;&#25321;&#65292;&#23588;&#20854;&#26159;&#22312;&#26102;&#23578;&#21644;&#38646;&#21806;&#31561;&#39046;&#22495;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#36825;&#20123;&#27169;&#24577;&#19981;&#20165;&#25552;&#20379;&#30452;&#35266;&#30340;&#25351;&#23548;&#65292;&#36824;&#36814;&#21512;&#20010;&#24615;&#21270;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20027;&#27969;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#20027;&#35201;&#32858;&#28966;&#20110;&#22522;&#20110;ID&#25110;&#25991;&#26412;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#26410;&#33021;&#29702;&#35299;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#25110;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#30784;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10667v1 Announce Type: cross  Abstract: Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided
&lt;/p&gt;</description></item><item><title>&#20272;&#35745;&#20013;&#20301;&#25968;&#24046;&#24322;&#27604;&#20272;&#35745;&#20013;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#26356;&#23481;&#26131;&#30340;&#38382;&#39064;&#26159;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10618</link><description>&lt;p&gt;
&#36817;&#20284;&#20013;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Limits of Approximating the Median Treatment Effect
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10618
&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#20013;&#20301;&#25968;&#24046;&#24322;&#27604;&#20272;&#35745;&#20013;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#26356;&#23481;&#26131;&#30340;&#38382;&#39064;&#26159;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#30340;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#19968;&#20010;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#24182;&#19981;&#19968;&#23450;&#33021;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#20272;&#35745;&#20998;&#20301;&#22788;&#29702;&#25928;&#24212;&#12290;&#22312;&#21253;&#21547;$n$&#20010;&#20010;&#20307;&#30340;&#26377;&#38480;&#20154;&#32676;&#35774;&#32622;&#20013;&#65292;&#27835;&#30103;&#21644;&#23545;&#29031;&#20540;&#29992;&#28508;&#22312;&#32467;&#26524;&#21521;&#37327;$\mathbf{a}, \mathbf{b}$&#34920;&#31034;&#65292;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#20272;&#35745;median$(\mathbf{a}) -$ median$(\mathbf{b})$&#65292;&#20854;&#20013;median($\mathbf x$)&#34920;&#31034;&#21521;&#37327;$\mathbf x$&#20013;&#25152;&#26377;&#20540;&#25490;&#24207;&#21518;&#30340;&#20013;&#20301;&#25968;&#20540;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20272;&#35745;&#20013;&#20301;&#25968;&#24046;&#24322;&#27604;&#20272;&#35745;&#20013;&#20301;&#25968;$(\mathbf{a-b})$&#26356;&#23481;&#26131;&#65292;&#21363;&#25152;&#35859;&#30340;&#20013;&#20301;&#25968;&#22788;&#29702;&#25928;&#24212;&#65288;MTE&#65289;&#12290;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#23545;&#20110;&#27599;&#20010;&#20010;&#20307;$i$&#65292;&#25105;&#20204;&#21482;&#33021;&#35266;&#27979;&#21040;&#28508;&#22312;&#32467;&#26524;&#20540;&#20043;&#19968;&#65292;&#21363;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10618v1 Announce Type: cross  Abstract: Average Treatment Effect (ATE) estimation is a well-studied problem in causal inference. However, it does not necessarily capture the heterogeneity in the data, and several approaches have been proposed to tackle the issue, including estimating the Quantile Treatment Effects. In the finite population setting containing $n$ individuals, with treatment and control values denoted by the potential outcome vectors $\mathbf{a}, \mathbf{b}$, much of the prior work focused on estimating median$(\mathbf{a}) -$ median$(\mathbf{b})$, where median($\mathbf x$) denotes the median value in the sorted ordering of all the values in vector $\mathbf x$. It is known that estimating the difference of medians is easier than the desired estimand of median$(\mathbf{a-b})$, called the Median Treatment Effect (MTE). The fundamental problem of causal inference -- for every individual $i$, we can only observe one of the potential outcome values, i.e., either the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SurvRNC&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#26469;&#33719;&#24471;&#22522;&#20110;&#29983;&#23384;&#26102;&#38388;&#30340;&#26377;&#24207;&#34920;&#31034;&#65292;&#33021;&#22788;&#29702;&#34987;&#25130;&#23614;&#25968;&#25454;&#65292;&#24182;&#21487;&#25972;&#21512;&#21040;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.10603</link><description>&lt;p&gt;
SurvRNC&#65306;&#20351;&#29992;Rank-N-Contrast&#23398;&#20064;&#26377;&#24207;&#34920;&#31034;&#26469;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SurvRNC: Learning Ordered Representations for Survival Prediction using Rank-N-Contrast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SurvRNC&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#26469;&#33719;&#24471;&#22522;&#20110;&#29983;&#23384;&#26102;&#38388;&#30340;&#26377;&#24207;&#34920;&#31034;&#65292;&#33021;&#22788;&#29702;&#34987;&#25130;&#23614;&#25968;&#25454;&#65292;&#24182;&#21487;&#25972;&#21512;&#21040;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#29983;&#23384;&#30340;&#21487;&#33021;&#24615;&#23545;&#20110;&#34987;&#35786;&#26029;&#20026;&#30284;&#30151;&#30340;&#20010;&#20307;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#20851;&#20110;&#26089;&#26399;&#39044;&#21518;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#36825;&#31181;&#30693;&#35782;&#20351;&#24471;&#21046;&#23450;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#24102;&#26469;&#25913;&#21892;&#24739;&#32773;&#32467;&#23616;&#30340;&#32467;&#26524;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20026;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#20197;&#20272;&#35745;&#30284;&#30151;&#39118;&#38505;&#20998;&#25968;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#23398;&#20064;&#20855;&#26377;&#22238;&#24402;&#24847;&#35782;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Survival Rank-N Contrast&#65288;SurvRNC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#26681;&#25454;&#29983;&#23384;&#26102;&#38388;&#33719;&#24471;&#19968;&#20010;&#26377;&#24207;&#34920;&#31034;&#12290;&#35813;&#20989;&#25968;&#21487;&#20197;&#22788;&#29702;&#34987;&#25130;&#23614;&#30340;&#25968;&#25454;&#65292;&#24182;&#21487;&#20197;&#34987;&#25972;&#21512;&#21040;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#20013;&#65292;&#20197;&#30830;&#20445;&#25152;&#23398;&#24471;&#30340;&#34920;&#31034;&#26159;&#39034;&#24207;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10603v1 Announce Type: cross  Abstract: Predicting the likelihood of survival is of paramount importance for individuals diagnosed with cancer as it provides invaluable information regarding prognosis at an early stage. This knowledge enables the formulation of effective treatment plans that lead to improved patient outcomes. In the past few years, deep learning models have provided a feasible solution for assessing medical images, electronic health records, and genomic data to estimate cancer risk scores. However, these models often fall short of their potential because they struggle to learn regression-aware feature representations. In this study, we propose Survival Rank-N Contrast (SurvRNC) method, which introduces a loss function as a regularizer to obtain an ordered representation based on the survival times. This function can handle censored data and can be incorporated into any survival model to ensure that the learned representation is ordinal. The model was extensi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#27169;&#25311;&#21463;&#25511;&#31070;&#32463;&#36864;&#34892;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LLMs&#20013;&#28155;&#21152;&#22122;&#38899;&#25110;&#20999;&#38500;&#31070;&#32463;&#20803;&#26469;&#36880;&#28176;&#38477;&#20302;&#20854;&#24615;&#33021;&#65292;&#22312;&#26234;&#21830;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#31070;&#32463;&#36864;&#34892;&#30340;&#36807;&#31243;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#19981;&#21516;&#65292;&#26159;&#39318;&#20010;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#27169;&#25311;&#31070;&#32463;&#36864;&#34892;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.10596</link><description>&lt;p&gt;
&#31070;&#32463;&#20405;&#34432;&#65306;&#22312;AI&#31995;&#32479;&#20013;&#27169;&#25311;&#21463;&#25511;&#31070;&#32463;&#36864;&#34892;&#21644;&#34928;&#32769;
&lt;/p&gt;
&lt;p&gt;
Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#27169;&#25311;&#21463;&#25511;&#31070;&#32463;&#36864;&#34892;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LLMs&#20013;&#28155;&#21152;&#22122;&#38899;&#25110;&#20999;&#38500;&#31070;&#32463;&#20803;&#26469;&#36880;&#28176;&#38477;&#20302;&#20854;&#24615;&#33021;&#65292;&#22312;&#26234;&#21830;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#31070;&#32463;&#36864;&#34892;&#30340;&#36807;&#31243;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#19981;&#21516;&#65292;&#26159;&#39318;&#20010;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#27169;&#25311;&#31070;&#32463;&#36864;&#34892;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#25511;&#21046;&#26041;&#27861;&#20197;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#30340;&#31070;&#32463;&#36864;&#34892;&#23545;&#20110;&#27169;&#25311;&#22823;&#33041;&#21151;&#33021;&#19979;&#38477;&#21644;&#35748;&#30693;&#38556;&#30861;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25191;&#34892;&#30340;&#26234;&#21830;&#27979;&#35797;&#65292;&#29305;&#21035;&#26159;LLaMA 2&#65292;&#24341;&#20837;&#20102;&#8220;&#31070;&#32463;&#20405;&#34432;&#8221;&#30340;&#27010;&#24565;&#12290;&#36825;&#31181;&#26377;&#24847;&#30340;&#20405;&#34432;&#28041;&#21450;&#22312;&#35757;&#32451;&#26399;&#38388;&#25110;&#20043;&#21518;&#20999;&#38500;&#31361;&#35302;&#25110;&#31070;&#32463;&#20803;&#65292;&#25110;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#65292;&#23548;&#33268;LLMs&#34920;&#29616;&#30340;&#36880;&#28176;&#19979;&#38477;&#12290;&#25105;&#20204;&#33021;&#22815;&#25551;&#36848;&#26234;&#21830;&#27979;&#35797;&#20013;&#30340;&#31070;&#32463;&#36864;&#34892;&#65292;&#24182;&#26174;&#31034;LLM&#39318;&#20808;&#22833;&#21435;&#20854;&#25968;&#23398;&#33021;&#21147;&#65292;&#28982;&#21518;&#22833;&#21435;&#20854;&#35821;&#35328;&#33021;&#21147;&#65292;&#21516;&#26102;&#36827;&#19968;&#27493;&#22833;&#21435;&#29702;&#35299;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#24314;&#27169;&#31070;&#32463;&#36864;&#34892;&#30340;&#24037;&#20316;&#65292;&#19982;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#36827;&#34892;&#25805;&#20316;&#30340;&#20854;&#20182;&#24037;&#20316;&#30456;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#19982;&#35748;&#30693;&#36827;&#34892;&#30456;&#20284;&#24615;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10596v1 Announce Type: cross  Abstract: Creating controlled methods to simulate neurodegeneration in artificial intelligence (AI) is crucial for applications that emulate brain function decline and cognitive disorders. We use IQ tests performed by Large Language Models (LLMs) and, more specifically, the LLaMA 2 to introduce the concept of ``neural erosion." This deliberate erosion involves ablating synapses or neurons, or adding Gaussian noise during or after training, resulting in a controlled progressive decline in the LLMs' performance. We are able to describe the neurodegeneration in the IQ tests and show that the LLM first loses its mathematical abilities and then its linguistic abilities, while further losing its ability to understand the questions. To the best of our knowledge, this is the first work that models neurodegeneration with text data, compared to other works that operate in the computer vision domain. Finally, we draw similarities between our study and cogn
&lt;/p&gt;</description></item><item><title>S3LLM&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#20197;&#20132;&#20114;&#24335;&#12289;&#23545;&#35805;&#24335;&#30340;&#26041;&#24335;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26816;&#26597;&#22823;&#35268;&#27169;&#31185;&#23398;&#36719;&#20214;&#30340;&#28304;&#20195;&#30721;&#12289;&#20195;&#30721;&#20803;&#25968;&#25454;&#21644;&#25688;&#35201;&#20449;&#24687;&#20197;&#21450;&#25991;&#26412;&#25216;&#26415;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#36716;&#25442;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20026;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#26597;&#35810;&#26469;&#22686;&#24378;&#20195;&#30721;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.10588</link><description>&lt;p&gt;
S3LLM&#65306;&#20351;&#29992;&#28304;&#20195;&#30721;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26723;&#30340;LLMs&#36827;&#34892;&#22823;&#35268;&#27169;&#31185;&#23398;&#36719;&#20214;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10588
&lt;/p&gt;
&lt;p&gt;
S3LLM&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#20197;&#20132;&#20114;&#24335;&#12289;&#23545;&#35805;&#24335;&#30340;&#26041;&#24335;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26816;&#26597;&#22823;&#35268;&#27169;&#31185;&#23398;&#36719;&#20214;&#30340;&#28304;&#20195;&#30721;&#12289;&#20195;&#30721;&#20803;&#25968;&#25454;&#21644;&#25688;&#35201;&#20449;&#24687;&#20197;&#21450;&#25991;&#26412;&#25216;&#26415;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#36716;&#25442;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20026;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#26597;&#35810;&#26469;&#22686;&#24378;&#20195;&#30721;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22810;&#26679;&#30340;&#20195;&#30721;&#24211;&#12289;&#24222;&#22823;&#30340;&#20195;&#30721;&#38271;&#24230;&#21644;&#30446;&#26631;&#35745;&#31639;&#26550;&#26500;&#65292;&#22823;&#35268;&#27169;&#31185;&#23398;&#36719;&#20214;&#30340;&#29702;&#35299;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20026;&#29702;&#35299;&#36825;&#31181;&#22797;&#26434;&#31185;&#23398;&#20195;&#30721;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;S3LLM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#20197;&#20132;&#20114;&#24335;&#12289;&#23545;&#35805;&#24335;&#30340;&#26041;&#24335;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26816;&#26597;&#28304;&#20195;&#30721;&#12289;&#20195;&#30721;&#20803;&#25968;&#25454;&#21644;&#25688;&#35201;&#20449;&#24687;&#20197;&#21450;&#25991;&#26412;&#25216;&#26415;&#25253;&#21578;&#12290;S3LLM&#21033;&#29992;&#24320;&#28304;&#30340;LLaMA-2&#27169;&#22411;&#22686;&#24378;&#20102;&#20195;&#30721;&#20998;&#26512;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#33258;&#21160;&#36716;&#25442;&#20026;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#35821;&#35328;&#65288;DSL&#65289;&#26597;&#35810;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#23558;&#36825;&#20123;&#26597;&#35810;&#36716;&#25442;&#20026;&#29305;&#24449;&#26597;&#35810;&#35821;&#35328;&#65288;FQL&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25972;&#20010;&#20195;&#30721;&#23384;&#20648;&#24211;&#30340;&#39640;&#25928;&#25195;&#25551;&#21644;&#35299;&#26512;&#12290;&#27492;&#22806;&#65292;S3LLM&#36824;&#33021;&#22815;&#22788;&#29702;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10588v1 Announce Type: cross  Abstract: The understanding of large-scale scientific software poses significant challenges due to its diverse codebase, extensive code length, and target computing architectures. The emergence of generative AI, specifically large language models (LLMs), provides novel pathways for understanding such complex scientific codes. This paper presents S3LLM, an LLM-based framework designed to enable the examination of source code, code metadata, and summarized information in conjunction with textual technical reports in an interactive, conversational manner through a user-friendly interface. S3LLM leverages open-source LLaMA-2 models to enhance code analysis through the automatic transformation of natural language queries into domain-specific language (DSL) queries. Specifically, it translates these queries into Feature Query Language (FQL), enabling efficient scanning and parsing of entire code repositories. In addition, S3LLM is equipped to handle d
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.10586</link><description>&lt;p&gt;
&#20174;&#31639;&#27861;&#21040;&#32467;&#26524;&#65306;&#23457;&#35270;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10586
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33152;&#33009;&#30284;&#26159;&#33521;&#22269;&#27599;&#22825;&#36896;&#25104;15&#20154;&#27515;&#20129;&#30340;&#39046;&#20808;&#27852;&#23615;&#36947;&#30284;&#30151;&#12290;&#36825;&#31181;&#30284;&#30151;&#20027;&#35201;&#34920;&#29616;&#20026;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#65288;NMIBC&#65289;&#65292;&#20854;&#29305;&#28857;&#26159;&#32959;&#30244;&#36824;&#26410;&#28183;&#36879;&#21040;&#33152;&#33009;&#22721;&#30340;&#32908;&#32905;&#23618;&#12290; NMIBC&#30340;&#22797;&#21457;&#29575;&#38750;&#24120;&#39640;&#65292;&#36798;&#21040;70-80&#65285;&#65292;&#22240;&#27492;&#27835;&#30103;&#25104;&#26412;&#26368;&#39640;&#12290;&#30446;&#21069;&#29992;&#20110;&#39044;&#27979;&#22797;&#21457;&#30340;&#24037;&#20855;&#20351;&#29992;&#35780;&#20998;&#31995;&#32479;&#26469;&#39640;&#20272;&#39118;&#38505;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#23545;&#22797;&#21457;&#30340;&#19981;&#20934;&#30830;&#21644;&#24310;&#36831;&#39044;&#27979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27515;&#20129;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#22797;&#21457;&#23545;&#20110;&#25104;&#26412;&#25928;&#30410;&#30340;&#31649;&#29702;&#21644;&#27835;&#30103;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#23601;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#20986;&#29616;&#30340;&#22320;&#26041;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#23376;&#21644;&#20020;&#24202;&#25968;&#25454;&#39044;&#27979;NMIBC&#22797;&#21457;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#27425;&#23457;&#26597;&#23545;&#39044;&#27979;NMIBC&#22797;&#21457;&#30340;ML&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35780;&#20272;&#20351;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10586v1 Announce Type: cross  Abstract: Bladder cancer, the leading urinary tract cancer, is responsible for 15 deaths daily in the UK. This cancer predominantly manifests as non-muscle-invasive bladder cancer (NMIBC), characterised by tumours not yet penetrating the muscle layer of the bladder wall. NMIBC is plagued by a very high recurrence rate of 70-80% and hence the costliest treatments. Current tools for predicting recurrence use scoring systems that overestimate risk and have poor accuracy. Inaccurate and delayed prediction of recurrence significantly elevates the likelihood of mortality. Accurate prediction of recurrence is hence vital for cost-effective management and treatment planning. This is where Machine learning (ML) techniques have emerged as a promising approach for predicting NMIBC recurrence by leveraging molecular and clinical data. This review provides a comprehensive analysis of ML approaches for predicting NMIBC recurrence. Our systematic evaluation de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#39564;&#25277;&#26679;&#35299;&#20915;&#19968;&#33324;&#24615;&#22122;&#22768;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#31574;&#30053;&#26799;&#24230;&#35270;&#35282;&#65292;&#36890;&#36807;&#25193;&#25955;&#31574;&#30053;&#26799;&#24230;&#65288;DPG&#65289;&#31934;&#30830;&#20272;&#35745;&#25351;&#23548;&#35780;&#20998;&#20989;&#25968;&#65292;&#23454;&#29616;&#23545;&#22810;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36870;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#35299;&#20915;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#24674;&#22797;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.10585</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#39564;&#25277;&#26679;&#35299;&#20915;&#19968;&#33324;&#24615;&#22122;&#22768;&#36870;&#38382;&#39064;&#65306;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10585
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#39564;&#25277;&#26679;&#35299;&#20915;&#19968;&#33324;&#24615;&#22122;&#22768;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#31574;&#30053;&#26799;&#24230;&#35270;&#35282;&#65292;&#36890;&#36807;&#25193;&#25955;&#31574;&#30053;&#26799;&#24230;&#65288;DPG&#65289;&#31934;&#30830;&#20272;&#35745;&#25351;&#23548;&#35780;&#20998;&#20989;&#25968;&#65292;&#23454;&#29616;&#23545;&#22810;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36870;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#35299;&#20915;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#24674;&#22797;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22270;&#20687;&#36870;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#65289;&#38656;&#35201;&#29983;&#25104;&#19968;&#20010;&#19982;&#32473;&#23450;&#36755;&#20837;&#65288;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#25110;&#36974;&#25377;&#22270;&#20687;&#65289;&#30456;&#21305;&#37197;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#22270;&#20687;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26469;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;&#36870;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#12290;&#20026;&#20102;&#31934;&#30830;&#20272;&#35745;&#36755;&#20837;&#22270;&#20687;&#30340;&#25351;&#23548;&#35780;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#31574;&#30053;&#26799;&#24230;&#65288;DPG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20013;&#38388;&#22122;&#22768;&#22270;&#20687;&#35270;&#20026;&#31574;&#30053;&#65292;&#23558;&#30446;&#26631;&#22270;&#20687;&#35270;&#20026;&#31574;&#30053;&#36873;&#25321;&#30340;&#29366;&#24577;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#22810;&#20010;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36870;&#20219;&#21153;&#19978;&#30340;&#39640;&#26031;&#21644;&#27850;&#26494;&#22122;&#22768;&#36864;&#21270;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#22312;FFHQ&#12289;ImageNet&#21644;LSUN&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#39640;&#30340;&#22270;&#20687;&#24674;&#22797;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10585v1 Announce Type: cross  Abstract: Solving image inverse problems (e.g., super-resolution and inpainting) requires generating a high fidelity image that matches the given input (the low-resolution image or the masked image). By using the input image as guidance, we can leverage a pretrained diffusion generative model to solve a wide range of image inverse tasks without task specific model fine-tuning. To precisely estimate the guidance score function of the input image, we propose Diffusion Policy Gradient (DPG), a tractable computation method by viewing the intermediate noisy images as policies and the target image as the states selected by the policy. Experiments show that our method is robust to both Gaussian and Poisson noise degradation on multiple linear and non-linear inverse tasks, resulting into a higher image restoration quality on FFHQ, ImageNet and LSUN datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.10581</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#30340;ECG&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#30001;&#20110;&#20840;&#29699;&#27515;&#20129;&#29575;&#19981;&#26029;&#19978;&#21319;&#32780;&#26500;&#25104;&#37325;&#22823;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#36890;&#36807;&#26089;&#26399;&#35786;&#26029;&#21644;&#39044;&#38450;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#21487;&#26174;&#33879;&#20943;&#23569;&#30142;&#30149;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#20020;&#24202;&#33719;&#21462;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#36827;&#34892;HF&#39118;&#38505;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#26088;&#22312;&#25429;&#25417;&#23545;&#26089;&#26399;HF&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#22797;&#26434;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#23613;&#31649;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#24179;&#34913;&#12290;&#35813;&#32593;&#32476;&#20855;&#26377;&#19968;&#20010;&#36328;&#23548;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;12&#20010;&#23548;&#32852;&#29305;&#23450;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#25429;&#25417;&#20132;&#21449;&#23548;&#32852;&#20132;&#20114;&#20316;&#29992;&#21644;&#27599;&#20010;&#23548;&#32852;&#20869;&#30340;&#23616;&#37096;&#26102;&#38388;&#21160;&#24577;&#12290;&#20026;&#20102;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20844;&#20849;ECG-Report&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#36827;&#34892;ECG-&#25253;&#21578;&#23545;&#40784;&#20219;&#21153;&#12290;&#28982;&#21518;&#23545;&#32593;&#32476;&#36827;&#34892;fine-tune&#20197;&#29992;&#20110;HF&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#26368;&#36817;&#20195;&#30721;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#36741;&#21161;&#20989;&#25968;&#21033;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#39118;&#26684;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27169;&#22411;&#21033;&#29992;&#36741;&#21161;&#20989;&#25968;&#30340;&#24456;&#26377;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.10575</link><description>&lt;p&gt;
&#29992;&#36741;&#21161;&#21151;&#33021;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring Language Model's Code Generation Ability with Auxiliary Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10575
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#26368;&#36817;&#20195;&#30721;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#36741;&#21161;&#20989;&#25968;&#21033;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#39118;&#26684;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27169;&#22411;&#21033;&#29992;&#36741;&#21161;&#20989;&#25968;&#30340;&#24456;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#20989;&#25968;&#26159;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#26377;&#29992;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#23578;&#26410;&#23436;&#25104;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#26368;&#36817;&#30340;&#20195;&#30721;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#36741;&#21161;&#20989;&#25968;&#21033;&#29992;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20154;&#24037;&#35774;&#35745;&#30340;&#35780;&#20272;&#38598;&#65292;&#31216;&#20026;HumanExtension&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#20989;&#25968;&#24110;&#21161;&#21478;&#19968;&#20010;&#20989;&#25968;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;HumanExtension&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20960;&#20010;&#23454;&#39564;&#20197;&#22810;&#26041;&#38754;&#22320;&#26816;&#39564;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#36807;&#31243;&#20351;&#24471;&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#21253;&#25324;&#36741;&#21161;&#20989;&#25968;&#22312;&#25552;&#31034;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#23454;&#29616;&#39118;&#26684;&#20998;&#26512;&#25429;&#25417;&#21040;&#27169;&#22411;&#22312;&#35775;&#38382;&#36741;&#21161;&#20989;&#25968;&#26102;&#30340;&#21508;&#31181;&#23454;&#29616;&#27169;&#24335;&#12290;&#36890;&#36807;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21033;&#29992;&#36741;&#21161;&#20989;&#25968;&#30340;&#33021;&#21147;&#24456;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10575v1 Announce Type: cross  Abstract: Auxiliary function is a helpful component to improve language model's code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other. With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models' various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models' promising ability to utilize auxiliary functions includi
&lt;/p&gt;</description></item><item><title>&#21338;&#24328;&#35770;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;&#32593;&#32476;&#27450;&#39575;&#31574;&#30053;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#25552;&#21319;&#20027;&#21160;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.10570</link><description>&lt;p&gt;
&#25112;&#30053;&#32593;&#32476;&#25112;&#20013;&#30340;&#29983;&#29289;&#20849;&#29983;&#28216;&#25103;&#21644;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10570
&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;&#32593;&#32476;&#27450;&#39575;&#31574;&#30053;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#25552;&#21319;&#20027;&#21160;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#20020;&#30528;&#32593;&#32476;&#25112;&#25112;&#26415;&#30340;&#24555;&#36895;&#28436;&#21464;&#12289;&#24773;&#25253;&#19981;&#23545;&#31216;&#24615;&#22686;&#21152;&#21644;&#40657;&#23458;&#24037;&#20855;&#30340;&#26085;&#30410;&#26131;&#24471;&#65292;&#25105;&#20204;&#27491;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#32593;&#32476;&#25112;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#32593;&#32476;&#27450;&#39575;&#20316;&#20026;&#25105;&#20204;&#38450;&#24481;&#31574;&#30053;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23853;&#38706;&#22836;&#35282;&#65292;&#26088;&#22312;&#24212;&#23545;&#26085;&#30410;&#22797;&#26434;&#30340;&#25915;&#20987;&#12290;&#26412;&#31456;&#26088;&#22312;&#24378;&#35843;&#21338;&#24328;&#35770;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#22312;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;&#32593;&#32476;&#27450;&#39575;&#31574;&#30053;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#21338;&#24328;&#27169;&#22411;&#65288;GMs&#65289;&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#26679;&#30340;&#23545;&#25239;&#24615;&#20132;&#20114;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21253;&#23481;&#23545;&#25239;&#24615;&#30693;&#35782;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35265;&#35299;&#12290;&#21516;&#26102;&#65292;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21019;&#24314;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#23450;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26500;&#24314;&#22359;&#12290;&#36890;&#36807;&#21033;&#29992;&#21338;&#24328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#19981;&#20165;&#20445;&#25252;&#25105;&#20204;&#30340;&#32593;&#32476;&#20813;&#21463;&#25915;&#20987;&#65292;&#32780;&#19988;&#25552;&#39640;&#20027;&#21160;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10570v1 Announce Type: cross  Abstract: We are currently facing unprecedented cyber warfare with the rapid evolution of tactics, increasing asymmetry of intelligence, and the growing accessibility of hacking tools. In this landscape, cyber deception emerges as a critical component of our defense strategy against increasingly sophisticated attacks. This chapter aims to highlight the pivotal role of game-theoretic models and foundation models (FMs) in analyzing, designing, and implementing cyber deception tactics. Game models (GMs) serve as a foundational framework for modeling diverse adversarial interactions, allowing us to encapsulate both adversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the building blocks for creating tailored machine learning models suited to given applications. By leveraging the synergy between GMs and FMs, we can advance proactive and automated cyber defense mechanisms by not only securing our networks against attacks but als
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;Xception&#19978;&#23454;&#26045;&#39640;&#25928;&#30340;&#21442;&#25968;&#32553;&#20943;&#31574;&#30053;&#65292;&#35813;&#30740;&#31350;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;DNN&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#21033;&#29992;&#65292;&#19988;&#22312;Caltech-101&#22270;&#20687;&#20998;&#31867;&#20013;&#34920;&#29616;&#20248;&#20110;&#21407;&#22987;Xception&#21644;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.10569</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#21033;&#29992;&#39640;&#25928;&#21442;&#25968;&#32553;&#20943;&#23454;&#29616;DNN&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10569
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;Xception&#19978;&#23454;&#26045;&#39640;&#25928;&#30340;&#21442;&#25968;&#32553;&#20943;&#31574;&#30053;&#65292;&#35813;&#30740;&#31350;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;DNN&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#21033;&#29992;&#65292;&#19988;&#22312;Caltech-101&#22270;&#20687;&#20998;&#31867;&#20013;&#34920;&#29616;&#20248;&#20110;&#21407;&#22987;Xception&#21644;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#25913;&#21892;&#20854;&#30828;&#20214;&#21033;&#29992;&#29575;&#65292;&#24182;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#25552;&#20379;&#20415;&#20110;&#35774;&#22791;&#19978;&#35757;&#32451;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;Xception&#19978;&#23454;&#26045;&#20102;&#39640;&#25928;&#30340;&#21442;&#25968;&#32553;&#20943;&#31574;&#30053;&#65292;&#32553;&#23567;&#27169;&#22411;&#22823;&#23567;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65306;Caltech-101&#22270;&#20687;&#20998;&#31867;&#21644;PCB&#32570;&#38519;&#26816;&#27979;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#21407;&#22987;&#30340;Xception&#21644;&#36731;&#37327;&#32423;&#27169;&#22411;EfficientNetV2B1&#21644;MobileNetV2&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;Caltech-101&#22270;&#20687;&#20998;&#31867;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65288;76.21%&#65289;&#65292;&#27604;Xception&#65288;75.89%&#65289;&#24179;&#22343;&#20351;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#65288;847.9MB&#27604;Xception&#30340;874.6MB&#65289;&#65292;&#24182;&#19988;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#26356;&#24555;&#12290;&#36731;&#37327;&#32423;&#27169;&#22411;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;EfficientNetV2B1&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20026;30.52%&#65292;MobileNetV2&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20026;58.11%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10569v1 Announce Type: cross  Abstract: This paper proposes an optimization of an existing Deep Neural Network (DNN) that improves its hardware utilization and facilitates on-device training for resource-constrained edge environments. We implement efficient parameter reduction strategies on Xception that shrink the model size without sacrificing accuracy, thus decreasing memory utilization during training. We evaluate our model in two experiments: Caltech-101 image classification and PCB defect detection and compare its performance against the original Xception and lightweight models, EfficientNetV2B1 and MobileNetV2. The results of the Caltech-101 image classification show that our model has a better test accuracy (76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than Xception (874.6MB), and has faster training and inference times. The lightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy and MobileNetV2 having a 58.11% test acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#23548;&#20837;&#20919;&#21364;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;AI&#26041;&#27861;&#20248;&#21270;&#20102;&#30005;&#27744;&#21333;&#20803;&#24067;&#23616;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#21333;&#20803;&#30340;&#26368;&#39640;&#28201;&#24230;&#65292;&#24182;&#22312;&#20919;&#21364;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.10566</link><description>&lt;p&gt;
&#30005;&#27744;&#21333;&#20803;&#24067;&#23616;&#30340;&#20919;&#21364;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cooling-Guide Diffusion Model for Battery Cell Arrangement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10566
&lt;/p&gt;
&lt;p&gt;
&#23548;&#20837;&#20919;&#21364;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;AI&#26041;&#27861;&#20248;&#21270;&#20102;&#30005;&#27744;&#21333;&#20803;&#24067;&#23616;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#21333;&#20803;&#30340;&#26368;&#39640;&#28201;&#24230;&#65292;&#24182;&#22312;&#20919;&#21364;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#21033;&#29992;&#20919;&#21364;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#26469;&#20248;&#21270;&#30005;&#27744;&#21333;&#20803;&#30340;&#24067;&#23616;&#65292;&#36825;&#26159;&#22686;&#24378;&#30005;&#27744;&#28909;&#31649;&#29702;&#31995;&#32479;&#20919;&#21364;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#19982;&#20256;&#32479;&#35774;&#35745;&#27969;&#31243;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#20351;&#29992;&#21442;&#25968;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#19982;&#20998;&#31867;&#22120;&#21644;&#20919;&#21364;&#24341;&#23548;&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#20855;&#26377;&#22686;&#24378;&#20919;&#21364;&#36335;&#24452;&#30340;&#20248;&#21270;&#21333;&#20803;&#24067;&#23616;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#21333;&#20803;&#30340;&#26368;&#39640;&#28201;&#24230;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#20301;&#32622;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#65292;&#25105;&#20204;&#30830;&#20445;&#29983;&#25104;&#30340;&#24067;&#23616;&#30340;&#21487;&#34892;&#24615;&#12290;&#21516;&#26102;&#65292;&#20919;&#21364;&#24341;&#23548;&#30452;&#25509;&#20248;&#21270;&#20919;&#21364;&#25928;&#29575;&#65292;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#20855;&#25928;&#26524;&#12290;&#19982;&#20004;&#31181;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#29420;&#29305;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10566v1 Announce Type: cross  Abstract: Our study introduces a Generative AI method that employs a cooling-guided diffusion model to optimize the layout of battery cells, a crucial step for enhancing the cooling performance and efficiency of battery thermal management systems. Traditional design processes, which rely heavily on iterative optimization and extensive guesswork, are notoriously slow and inefficient, often leading to suboptimal solutions. In contrast, our innovative method uses a parametric denoising diffusion probabilistic model (DDPM) with classifier and cooling guidance to generate optimized cell layouts with enhanced cooling paths, significantly lowering the maximum temperature of the cells. By incorporating position-based classifier guidance, we ensure the feasibility of generated layouts. Meanwhile, cooling guidance directly optimizes cooling-efficiency, making our approach uniquely effective. When compared to two advanced models, the Tabular Denoising Diff
&lt;/p&gt;</description></item><item><title>PTSD-MDNN&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#23458;&#35266;&#12289;&#26356;&#24555;&#36895;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.10565</link><description>&lt;p&gt;
PTSD-MDNN&#65306;&#34701;&#21512;&#22810;&#27169;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#24555;&#36895;&#22320;&#26816;&#27979;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
PTSD-MDNN : Fusion tardive de r\'eseaux de neurones profonds multimodaux pour la d\'etection du trouble de stress post-traumatique
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10565
&lt;/p&gt;
&lt;p&gt;
PTSD-MDNN&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#23458;&#35266;&#12289;&#26356;&#24555;&#36895;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#20379;&#19968;&#31181;&#26356;&#23458;&#35266;&#12289;&#26356;&#24555;&#36895;&#30340;&#35786;&#26029;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#65288;PTSD&#65289;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PTSD-MDNN&#65292;&#23427;&#21512;&#24182;&#20102;&#20004;&#20010;&#21333;&#27169;&#24577;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26816;&#27979;&#38169;&#35823;&#29575;&#12290;&#35813;&#27169;&#22411;&#20165;&#20351;&#29992;&#35270;&#39057;&#21644;&#38899;&#39057;&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#29992;&#20110;&#36828;&#31243;&#20250;&#35786;&#12289;&#20248;&#21270;&#24739;&#32773;&#23601;&#35786;&#36807;&#31243;&#25110;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10565v1 Announce Type: cross  Abstract: In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two unimodal convolutional neural networks and which gives low detection error rate. By taking only videos and audios as inputs, the model could be used in the configuration of teleconsultation sessions, in the optimization of patient journeys or for human-robot interaction.
&lt;/p&gt;</description></item><item><title>&#26080;&#29366;&#24577;&#31574;&#30053;&#36890;&#36807;&#35780;&#20272;&#21453;&#26679;&#26412;&#23545;&#25239;&#40657;&#30418;&#26597;&#35810;&#65292;&#26377;&#25928;&#24341;&#20837;&#20102;&#38450;&#24481;&#32773;&#26377;&#21033;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#38450;&#24481;&#26041;&#27861;&#33021;&#22815;&#27450;&#39575;&#25915;&#20987;&#32773;&#23547;&#25214;&#23545;&#25239;&#24615;&#31034;&#20363;&#12289;&#20445;&#25345;&#27169;&#22411;&#22312;&#21512;&#27861;&#36755;&#20837;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#25915;&#20987;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.10562</link><description>&lt;p&gt;
Counter-Samples: &#19968;&#31181;&#25269;&#24481;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#30340;&#26080;&#29366;&#24577;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10562
&lt;/p&gt;
&lt;p&gt;
&#26080;&#29366;&#24577;&#31574;&#30053;&#36890;&#36807;&#35780;&#20272;&#21453;&#26679;&#26412;&#23545;&#25239;&#40657;&#30418;&#26597;&#35810;&#65292;&#26377;&#25928;&#24341;&#20837;&#20102;&#38450;&#24481;&#32773;&#26377;&#21033;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#38450;&#24481;&#26041;&#27861;&#33021;&#22815;&#27450;&#39575;&#25915;&#20987;&#32773;&#23547;&#25214;&#23545;&#25239;&#24615;&#31034;&#20363;&#12289;&#20445;&#25345;&#27169;&#22411;&#22312;&#21512;&#27861;&#36755;&#20837;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#25915;&#20987;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25269;&#24481;&#40657;&#30418;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#21463;&#23475;&#32773;&#27169;&#22411;&#20316;&#20026; Oracle &#26469;&#26500;&#24314;&#20182;&#20204;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#19982;&#20256;&#32479;&#30340;&#39044;&#22788;&#29702;&#38450;&#24481;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26080;&#29366;&#24577;&#31574;&#30053;&#23454;&#38469;&#19978;&#23545;&#25239;&#25915;&#20987;&#36807;&#31243;&#26412;&#36523;&#12290;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#26597;&#35810;&#37117;&#35780;&#20272;&#19968;&#20010;&#21453;&#26679;&#26412;&#65292;&#20854;&#20013;&#21453;&#26679;&#26412;&#26159;&#38024;&#23545;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#21407;&#22987;&#26679;&#26412;&#12290;&#36890;&#36807;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#30333;&#30418;&#20248;&#21270;&#26469;&#23545;&#25239;&#27599;&#19968;&#20010;&#40657;&#30418;&#26597;&#35810;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#26377;&#25928;&#22320;&#20026;&#38450;&#24481;&#32773;&#24341;&#20837;&#20102;&#23545;&#31216;&#24615;&#65292;&#20351;&#20854;&#22788;&#20110;&#26377;&#21033;&#22320;&#20301;&#12290;&#36825;&#31181;&#38450;&#24481;&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22320;&#35823;&#23548;&#25915;&#20987;&#32773;&#23547;&#25214;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36824;&#33021;&#20445;&#30041;&#22312;&#21512;&#27861;&#36755;&#20837;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#26368;&#20808;&#36827;&#30340;&#40657;&#30418;&#25915;&#20987;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312; C &#19978;&#32988;&#36807;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10562v1 Announce Type: cross  Abstract: Our paper presents a novel defence against black box attacks, where attackers use the victim model as an oracle to craft their adversarial examples. Unlike traditional preprocessing defences that rely on sanitizing input samples, our stateless strategy counters the attack process itself. For every query we evaluate a counter-sample instead, where the counter-sample is the original sample optimized against the attacker's objective. By countering every black box query with a targeted white box optimization, our strategy effectively introduces an asymmetry to the game to the defender's advantage. This defence not only effectively misleads the attacker's search for an adversarial example, it also preserves the model's accuracy on legitimate inputs and is generic to multiple types of attacks.   We demonstrate that our approach is remarkably effective against state-of-the-art black box attacks and outperforms existing defences for both the C
&lt;/p&gt;</description></item><item><title>AAAI 2024&#24180;&#20154;&#26412;&#20027;&#20041;&#34920;&#31034;&#23398;&#20064;&#30740;&#35752;&#20250;&#30340;&#34987;&#25509;&#21463;&#35770;&#25991;&#38598;&#21512;&#12290;&#37096;&#20998;&#35770;&#25991;&#36873;&#25321;&#36864;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.10561</link><description>&lt;p&gt;
AAAI 2024&#24180;&#20154;&#26412;&#20027;&#20041;&#34920;&#31034;&#23398;&#20064;&#30740;&#35752;&#20250;&#30340;&#34987;&#25509;&#21463;&#35770;&#25991;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10561
&lt;/p&gt;
&lt;p&gt;
AAAI 2024&#24180;&#20154;&#26412;&#20027;&#20041;&#34920;&#31034;&#23398;&#20064;&#30740;&#35752;&#20250;&#30340;&#34987;&#25509;&#21463;&#35770;&#25991;&#38598;&#21512;&#12290;&#37096;&#20998;&#35770;&#25991;&#36873;&#25321;&#36864;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10561v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39033;  &#25688;&#35201;: &#36825;&#20221;&#38750;&#23384;&#26723;&#32034;&#24341;&#24182;&#19981;&#23436;&#25972;&#65292;&#22240;&#20026;&#19968;&#20123;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#36873;&#25321;&#20102;&#36864;&#20986;&#12290;&#25152;&#26377;&#34987;&#25509;&#21463;&#35770;&#25991;&#30340;&#21015;&#34920;&#21487;&#22312;&#30740;&#35752;&#20250;&#32593;&#31449;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10561v1 Announce Type: cross  Abstract: This non-archival index is not complete, as some accepted papers chose to opt-out of inclusion. The list of all accepted papers is available on the workshop website.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#25972;&#21512;&#26377;&#26395;&#25552;&#21319;&#33258;&#21160;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#65292;&#23545;&#20132;&#36890;&#34892;&#19994;&#30340;&#23433;&#20840;&#21644;&#21019;&#26032;&#20855;&#26377;&#28508;&#22312;&#25512;&#21160;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10559</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65306;&#25506;&#32034;&#20132;&#36890;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10559
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#25972;&#21512;&#26377;&#26395;&#25552;&#21319;&#33258;&#21160;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#65292;&#23545;&#20132;&#36890;&#34892;&#19994;&#30340;&#23433;&#20840;&#21644;&#21019;&#26032;&#20855;&#26377;&#28508;&#22312;&#25512;&#21160;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#35843;&#26597;&#20102;&#29983;&#25104;&#27169;&#22411;&#21644;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#20004;&#31181;&#25512;&#21160;&#25216;&#26415;&#21644;&#20132;&#36890;&#36827;&#27493;&#30340;&#31361;&#30772;&#24615;&#21147;&#37327;&#30340;&#21382;&#21490;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#20851;&#27880;&#29983;&#25104;&#27169;&#22411;&#22312;CAVs&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#36825;&#31181;&#25972;&#21512;&#22914;&#20309;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#20132;&#36890;&#39046;&#22495;&#25972;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;CAV&#25216;&#26415;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#26088;&#22312;&#24378;&#35843;&#21462;&#24471;&#30340;&#36827;&#23637;&#12289;&#21097;&#20313;&#30340;&#38556;&#30861;&#20197;&#21450;&#22312;&#23433;&#20840;&#21644;&#21019;&#26032;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10559v1 Announce Type: cross  Abstract: This report investigates the history and impact of Generative Models and Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing progress in technology and transportation. By focusing on the application of generative models within the context of CAVs, the study aims to unravel how this integration could enhance predictive modeling, simulation accuracy, and decision-making processes in autonomous vehicles. This thesis discusses the benefits and challenges of integrating generative models and CAV technology in transportation. It aims to highlight the progress made, the remaining obstacles, and the potential for advancements in safety and innovation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20108;&#38454;&#20449;&#24687;&#65288;Hessian&#65289;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10557</link><description>&lt;p&gt;
&#20108;&#38454;&#20449;&#24687;&#24456;&#37325;&#35201;&#65306;&#37325;&#35775;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20108;&#38454;&#20449;&#24687;&#65288;Hessian&#65289;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;ChatGPT&#12289;LLaMa&#21644;Gemini&#31561;&#20027;&#35201;LLM&#20135;&#21697;&#20043;&#38388;&#30340;&#28608;&#28872;&#31454;&#20105;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#21508;&#31181;&#38382;&#39064;&#65288;&#22914;&#38544;&#31169;&#27844;&#38706;&#21644;&#29256;&#26435;&#20405;&#29359;&#65289;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#20197;LLM&#20174;&#19994;&#32773;&#30340;&#35270;&#35282;&#26469;&#30475;&#65292;&#22788;&#29702;&#36825;&#20123;&#24847;&#22806;&#30340;&#38544;&#31169;&#20405;&#29359;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#35299;&#20915;&#20102;LLMs&#30340;&#8220;&#36951;&#24536;&#8221;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#24320;&#38144;&#65292;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#25110;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#19982;&#22522;&#20110;&#19968;&#38454;&#20449;&#24687;&#30340;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#20108;&#38454;&#20449;&#24687;&#65288;Hessian&#65289;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#36951;&#24536;&#38382;&#39064;&#12290;&#21463;&#32463;&#20856;&#29275;&#39039;&#26356;&#26032;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#36951;&#24536;&#31639;&#27861;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10557v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), we have witnessed intense competition among the major LLM products like ChatGPT, LLaMa, and Gemini. However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of LLM practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning" problem of LLMs using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are 
&lt;/p&gt;</description></item><item><title>KARINA&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;ConvNext&#12289;SENet&#21644;Geocyclic&#22635;&#20805;&#65292;&#22312;2.5&#176;&#20998;&#36776;&#29575;&#19979;&#25552;&#21319;&#22825;&#27668;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21482;&#38656;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23637;&#31034;&#20986;&#19982;&#26356;&#39640;&#20998;&#36776;&#29575;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.10555</link><description>&lt;p&gt;
KARINA&#65306;&#19968;&#31181;&#29992;&#20110;&#20840;&#29699;&#22825;&#27668;&#39044;&#27979;&#30340;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KARINA: An Efficient Deep Learning Model for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10555
&lt;/p&gt;
&lt;p&gt;
KARINA&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;ConvNext&#12289;SENet&#21644;Geocyclic&#22635;&#20805;&#65292;&#22312;2.5&#176;&#20998;&#36776;&#29575;&#19979;&#25552;&#21319;&#22825;&#27668;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21482;&#38656;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23637;&#31034;&#20986;&#19982;&#26356;&#39640;&#20998;&#36776;&#29575;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#27668;&#20505;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#29305;&#21035;&#26159;&#22312;&#20840;&#29699;&#22825;&#27668;&#39044;&#27979;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20197;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#20840;&#29699;&#22825;&#27668;&#25968;&#25454;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KARINA&#30340;&#26032;&#27169;&#22411;&#65292;&#20197;&#20811;&#26381;&#35813;&#39046;&#22495;&#20856;&#22411;&#30340;&#24040;&#22823;&#35745;&#31639;&#38656;&#27714;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21482;&#38656;4&#20010;NVIDIA A100 GPU&#21644;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#21363;&#21487;&#23454;&#29616;&#19982;&#26356;&#39640;&#20998;&#36776;&#29575;&#23545;&#25163;&#30456;&#23218;&#32654;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#12290;KARINA&#32467;&#21512;&#20102;ConvNext&#12289;SENet&#21644;Geocyclic&#22635;&#20805;&#20197;&#22686;&#24378;&#20197;2.5&#176;&#20998;&#36776;&#29575;&#36827;&#34892;&#30340;&#22825;&#27668;&#39044;&#27979;&#65292;&#21487;&#20197;&#36807;&#28388;&#25481;&#39640;&#39057;&#22122;&#22768;&#12290;&#22320;&#29702;&#21608;&#26399;&#22635;&#20805;&#20445;&#30041;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20391;&#36793;&#30028;&#20687;&#32032;&#65292;&#20174;&#32780;&#22312;&#29699;&#24418;&#22320;&#29699;&#20013;&#20445;&#25345;&#22823;&#27668;&#27969;&#30340;&#36830;&#32493;&#24615;&#12290;SENet&#21160;&#24577;&#25913;&#21892;&#29305;&#24449;&#21709;&#24212;&#65292;&#25512;&#36827;&#20102;&#22823;&#27668;&#36807;&#31243;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10555v1 Announce Type: cross  Abstract: Deep learning-based, data-driven models are gaining prevalence in climate research, particularly for global weather prediction. However, training the global weather data at high resolution requires massive computational resources. Therefore, we present a new model named KARINA to overcome the substantial computational demands typical of this field. This model achieves forecasting accuracy comparable to higher-resolution counterparts with significantly less computational resources, requiring only 4 NVIDIA A100 GPUs and less than 12 hours of training. KARINA combines ConvNext, SENet, and Geocyclic Padding to enhance weather forecasting at a 2.5{\deg} resolution, which could filter out high-frequency noise. Geocyclic Padding preserves pixels at the lateral boundary of the input image, thereby maintaining atmospheric flow continuity in the spherical Earth. SENet dynamically improves feature response, advancing atmospheric process modeling,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20449;&#21495;&#23884;&#20837;LLM&#30340;&#26435;&#37325;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#19968;&#31181;&#27169;&#22411;&#32423;&#27700;&#21360;&#65292;&#26377;&#25928;&#36861;&#36394;&#29983;&#25104;&#25991;&#26412;&#30340;&#28389;&#29992;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#27700;&#21360;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#19988;&#26356;&#36866;&#24212;&#26032;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.10553</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#32473;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#28155;&#21152;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Learning to Watermark LLM-generated Text via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10553
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20449;&#21495;&#23884;&#20837;LLM&#30340;&#26435;&#37325;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#19968;&#31181;&#27169;&#22411;&#32423;&#27700;&#21360;&#65292;&#26377;&#25928;&#36861;&#36394;&#29983;&#25104;&#25991;&#26412;&#30340;&#28389;&#29992;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#27700;&#21360;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#19988;&#26356;&#36866;&#24212;&#26032;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#32473;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#28155;&#21152;&#27700;&#21360;&#65292;&#21363;&#23558;&#21487;&#20197;&#36890;&#36807;&#31639;&#27861;&#26816;&#27979;&#21040;&#30340;&#20449;&#21495;&#23884;&#20837;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20197;&#36861;&#36394;&#28389;&#29992;&#24773;&#20917;&#12290;&#19982;&#24403;&#21069;&#20027;&#27969;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;LLM&#35843;&#20248;&#38454;&#27573;&#32435;&#20837;&#27700;&#21360;&#27969;&#31243;&#65292;&#25193;&#23637;&#20102;&#27700;&#21360;&#35774;&#35745;&#31354;&#38388;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#22312;&#36755;&#20986;&#20013;&#23884;&#20837;&#20449;&#21495;&#30340;&#26631;&#35760;&#32423;&#27700;&#21360;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27169;&#22411;&#32423;&#27700;&#21360;&#65292;&#23558;&#20449;&#21495;&#23884;&#20837;LLM&#30340;&#26435;&#37325;&#20013;&#65292;&#21487;&#20197;&#34987;&#37197;&#23545;&#30340;&#26816;&#27979;&#22120;&#26816;&#27979;&#21040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#36845;&#20195;&#22320;&#65288;1&#65289;&#35757;&#32451;&#19968;&#20010;&#26816;&#27979;&#22120;&#26469;&#26816;&#27979;&#29983;&#25104;&#30340;&#24102;&#27700;&#21360;&#25991;&#26412;&#65292;&#24182;&#65288;2&#65289;&#35843;&#25972;LLM&#20197;&#29983;&#25104;&#26816;&#27979;&#22120;&#36731;&#26494;&#26816;&#27979;&#21040;&#20294;&#20445;&#25345;&#27491;&#24120;&#25928;&#29992;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27700;&#21360;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#19988;&#26356;&#36866;&#24212;&#65288;&#24212;&#23545;&#26032;&#25915;&#20987;&#65289;&#12290;&#23427;&#36824;&#20801;&#35768;&#27700;&#21360;&#27169;&#22411;&#24320;&#28304;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#19982;al&#19968;&#36215;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10553v1 Announce Type: cross  Abstract: We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse. Unlike the current mainstream methods that work with a fixed LLM, we expand the watermark design space by including the LLM tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the LLM weights, and such signals can be detected by a paired detector. We propose a co-training framework based on reinforcement learning that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the LLM to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks). It also allows watermarked model open-sourcing. In addition, if used together with al
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24072;&#29983;&#26080;&#25968;&#25454;&#30693;&#35782;&#20256;&#36755;&#65292;&#23454;&#29616;&#22312;&#26410;&#30693;&#38476;&#29983;&#22320;&#28857;&#19978;&#35757;&#32451;&#33258;&#23450;&#20301;&#27169;&#22411;&#65292;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#32769;&#24072;&#65292;&#36824;&#26377;&#25928;&#36991;&#20813;&#20381;&#36182;&#20110;&#24072;&#29983;&#31169;&#20154;&#25968;&#25454;&#21487;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10552</link><description>&lt;p&gt;
&#36890;&#36807;&#24072;&#29983;&#26080;&#25968;&#25454;&#30693;&#35782;&#20256;&#36755;&#22312;&#26410;&#30693;&#38476;&#29983;&#22320;&#28857;&#19978;&#35757;&#32451;&#33258;&#23450;&#20301;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10552
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24072;&#29983;&#26080;&#25968;&#25454;&#30693;&#35782;&#20256;&#36755;&#65292;&#23454;&#29616;&#22312;&#26410;&#30693;&#38476;&#29983;&#22320;&#28857;&#19978;&#35757;&#32451;&#33258;&#23450;&#20301;&#27169;&#22411;&#65292;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#32769;&#24072;&#65292;&#36824;&#26377;&#25928;&#36991;&#20813;&#20381;&#36182;&#20110;&#24072;&#29983;&#31169;&#20154;&#25968;&#25454;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19968;&#20010;&#26426;&#22120;&#20154;&#22312;&#19968;&#20010;&#19968;&#33324;&#30340;&#24320;&#25918;&#19990;&#30028;&#20013;&#36816;&#34892;&#26102;&#65292;&#30446;&#21069;&#33258;&#23450;&#20301;&#27169;&#22411;&#30340;&#19968;&#20010;&#20856;&#22411;&#20551;&#35774;&#26159;&#30446;&#26631;&#24037;&#20316;&#31354;&#38388;&#20013;&#26377;&#19968;&#20010;&#24102;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#33324;&#24320;&#25918;&#19990;&#30028;&#20013;&#65292;&#36825;&#31181;&#24773;&#20917;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#19990;&#30028;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#26696;&#20013;&#65292;&#19968;&#20010;&#26426;&#22120;&#20154;&#65288;"&#23398;&#29983;"&#65289;&#21487;&#20197;&#21521;&#23427;&#22312;&#38476;&#29983;&#22320;&#28857;&#36935;&#21040;&#30340;&#20854;&#20182;&#26426;&#22120;&#20154;&#65288;"&#32769;&#24072;"&#65289;&#23547;&#27714;&#25351;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20174;&#32769;&#24072;&#27169;&#22411;&#37325;&#24314;&#19968;&#20010;&#20266;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#29992;&#20110;&#23398;&#29983;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#19982;&#20856;&#22411;&#30340;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#32769;&#24072;&#27169;&#22411;&#19978;&#21482;&#24341;&#20837;&#20102;&#26368;&#23567;&#30340;&#20551;&#35774;&#65292;&#20351;&#20854;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#32769;&#24072;&#65292;&#21253;&#25324;&#19981;&#21512;&#20316;&#30340;&#12289;&#26080;&#27861;&#35757;&#32451;&#30340;&#65288;&#22914;&#22270;&#20687;&#26816;&#32034;&#24341;&#25806;&#65289;&#21644;&#40657;&#21283;&#23376;&#32769;&#24072;&#65288;&#21363;&#25968;&#25454;&#38544;&#31169;&#65289;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20013;&#20381;&#36182;&#20110;&#32769;&#24072;&#30340;&#31169;&#20154;&#25968;&#25454;&#21487;&#29992;&#24615;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10552v1 Announce Type: cross  Abstract: A typical assumption in state-of-the-art self-localization models is that an annotated training dataset is available in the target workspace. However, this does not always hold when a robot travels in a general open-world. This study introduces a novel training scheme for open-world distributed robot systems. In our scheme, a robot ("student") can ask the other robots it meets at unfamiliar places ("teachers") for guidance. Specifically, a pseudo-training dataset is reconstructed from the teacher model and thereafter used for continual learning of the student model. Unlike typical knowledge transfer schemes, our scheme introduces only minimal assumptions on the teacher model, such that it can handle various types of open-set teachers, including uncooperative, untrainable (e.g., image retrieval engines), and blackbox teachers (i.e., data privacy). Rather than relying on the availability of private data of teachers as in existing methods
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#20266;&#24322;&#24120;&#26679;&#26412;&#21644;&#20351;&#29992;&#21452;&#21521;&#24402;&#19968;&#21270;&#27969;&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#24322;&#24120;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.10550</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24322;&#24120;&#27969;&#37327;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10550
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#20266;&#24322;&#24120;&#26679;&#26412;&#21644;&#20351;&#29992;&#21452;&#21521;&#24402;&#19968;&#21270;&#27969;&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#24322;&#24120;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21508;&#31181;&#31867;&#22411;&#30340;&#24322;&#24120;&#27969;&#37327;&#23041;&#32961;&#30528;&#32593;&#32476;&#23433;&#20840;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#24322;&#24120;&#32593;&#32476;&#27969;&#37327;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20165;&#20351;&#29992;&#27491;&#24120;&#27969;&#37327;&#30340;&#19977;&#38454;&#27573;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20266;&#24322;&#24120;&#26679;&#26412;&#65292;&#26080;&#38656;&#20808;&#21069;&#23545;&#24322;&#24120;&#36827;&#34892;&#20102;&#35299;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24322;&#24120;&#25968;&#25454;&#30340;&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#37325;&#26500;&#26041;&#27861;&#26469;&#23398;&#20064;&#27491;&#24120;&#26679;&#26412;&#30340;&#28145;&#23618;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#21452;&#21521;&#27969;&#27169;&#22359;&#23558;&#36825;&#20123;&#34920;&#31034;&#24402;&#19968;&#21270;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#20026;&#20102;&#27169;&#25311;&#24322;&#24120;&#26679;&#26412;&#65292;&#25105;&#20204;&#21521;&#24402;&#19968;&#21270;&#21518;&#30340;&#34920;&#31034;&#28155;&#21152;&#22122;&#22768;&#65292;&#28982;&#21518;&#23558;&#20854;&#36890;&#36807;&#21452;&#21521;&#27969;&#27169;&#22359;&#30340;&#29983;&#25104;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#22120;&#26469;&#21306;&#20998;&#27491;&#24120;&#26679;&#26412;&#21644;&#20266;&#24322;&#24120;&#26679;&#26412;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20165;&#38656;&#35201;&#20004;&#20010;&#27169;&#22359;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10550v1 Announce Type: cross  Abstract: With the rapid development of the Internet, various types of anomaly traffic are threatening network security. We consider the problem of anomaly network traffic detection and propose a three-stage anomaly detection framework using only normal traffic. Our framework can generate pseudo anomaly samples without prior knowledge of anomalies to achieve the detection of anomaly data. Firstly, we employ a reconstruction method to learn the deep representation of normal samples. Secondly, these representations are normalized to a standard normal distribution using a bidirectional flow module. To simulate anomaly samples, we add noises to the normalized representations which are then passed through the generation direction of the bidirectional flow module. Finally, a simple classifier is trained to differentiate the normal samples and pseudo anomaly samples in the latent space. During inference, our framework requires only two modules to detec
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#20013;&#23547;&#25214;SOSP&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#20197;\emph{&#29420;&#31435;&#20110;&#32500;&#24230;}&#30340;&#31934;&#24230;&#20445;&#35777;&#39640;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;SOSP&#65292;&#20855;&#26377;&#23545;&#25239;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#21457;&#23637;&#20102;&#33021;&#22815;&#23481;&#24525;&#25968;&#25454;&#30772;&#22351;&#30340;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10547</link><description>&lt;p&gt;
&#22362;&#38887;&#30340;&#20108;&#38454;&#38750;&#20984;&#20248;&#21270;&#21450;&#20854;&#22312;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10547
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#20013;&#23547;&#25214;SOSP&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#20197;\emph{&#29420;&#31435;&#20110;&#32500;&#24230;}&#30340;&#31934;&#24230;&#20445;&#35777;&#39640;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;SOSP&#65292;&#20855;&#26377;&#23545;&#25239;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#21457;&#23637;&#20102;&#33021;&#22815;&#23481;&#24525;&#25968;&#25454;&#30772;&#22351;&#30340;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#36817;&#20284;&#30340;&#20108;&#38454;&#31283;&#23450;&#28857;&#65288;SOSP&#65289;&#26159;&#38543;&#26426;&#38750;&#20984;&#20248;&#21270;&#20013;&#19968;&#20010;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#29702;&#35299;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#38750;&#20984;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#20013;&#23547;&#25214;SOSPs&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#19968;&#23450;&#37096;&#20998;&#30340;&#25968;&#25454;&#28857;&#34987;&#20219;&#24847;&#30772;&#22351;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23547;&#25214;&#36817;&#20284;SOSP&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#20855;&#26377;\emph{&#29420;&#31435;&#20110;&#32500;&#24230;}&#30340;&#31934;&#24230;&#20445;&#35777;&#65292;&#20351;&#29992;$\widetilde{O}({D^2}/{\epsilon})$&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;$D$&#26159;&#29615;&#22659;&#32500;&#24230;&#65292;$\epsilon$&#26159;&#34987;&#30772;&#22351;&#25968;&#25454;&#28857;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10547v1 Announce Type: cross  Abstract: Finding an approximate second-order stationary point (SOSP) is a well-studied and fundamental problem in stochastic nonconvex optimization with many applications in machine learning. However, this problem is poorly understood in the presence of outliers, limiting the use of existing nonconvex algorithms in adversarial settings.   In this paper, we study the problem of finding SOSPs in the strong contamination model, where a constant fraction of datapoints are arbitrarily corrupted. We introduce a general framework for efficiently finding an approximate SOSP with \emph{dimension-independent} accuracy guarantees, using $\widetilde{O}({D^2}/{\epsilon})$ samples where $D$ is the ambient dimension and $\epsilon$ is the fraction of corrupted datapoints.   As a concrete application of our framework, we apply it to the problem of low rank matrix sensing, developing efficient and provably robust algorithms that can tolerate corruptions in both 
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#27835;&#30103;&#36335;&#24452;&#65292;&#21033;&#29992;&#27969;&#31243;&#25366;&#25496;&#25216;&#26415;&#23545;&#31232;&#30095;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25506;&#31350;&#26159;&#21542;&#33021;&#22815;&#22238;&#31572;&#22810;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10544</link><description>&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#27835;&#30103;&#36335;&#24452;&#30340;&#27969;&#31243;&#24863;&#30693;&#20998;&#26512;&#65306;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Process-Aware Analysis of Treatment Paths in Heart Failure Patients: A Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10544
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#27835;&#30103;&#36335;&#24452;&#65292;&#21033;&#29992;&#27969;&#31243;&#25366;&#25496;&#25216;&#26415;&#23545;&#31232;&#30095;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25506;&#31350;&#26159;&#21542;&#33021;&#22815;&#22238;&#31572;&#22810;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#36827;&#34892;&#27969;&#31243;&#25366;&#25496;&#26102;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#22240;&#20026;&#21307;&#30103;&#36807;&#31243;&#20013;&#28041;&#21450;&#30340;&#25968;&#25454;&#31867;&#22411;&#21508;&#24322;&#12290;&#36890;&#36807;&#20174;&#21307;&#30103;&#36807;&#31243;&#20013;&#25910;&#38598;&#30340;&#21508;&#31181;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#39640;&#24230;&#22810;&#26679;&#24615;&#65306;&#30001;&#32034;&#36180;&#25968;&#25454;&#25552;&#20379;&#30340;&#36816;&#33829;&#36807;&#31243;&#65292;&#25163;&#26415;&#26399;&#38388;&#21457;&#29983;&#30340;&#20107;&#20214;&#38598;&#21512;&#65292;&#19982;&#26415;&#21069;&#21644;&#26415;&#21518;&#25252;&#29702;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#22522;&#20110;&#23450;&#26399;&#38376;&#35786;&#35775;&#38382;&#30340;&#39640;&#32423;&#25968;&#25454;&#25910;&#38598;&#65292;&#20854;&#20013;&#24182;&#27809;&#26377;&#26126;&#26174;&#30340;&#20107;&#20214;&#12290;&#26412;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#20102;&#26469;&#33258;&#26368;&#21518;&#19968;&#31867;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#31232;&#30095;&#30340;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#25968;&#25454;&#24212;&#29992;&#27969;&#31243;&#25366;&#25496;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#23545;&#22810;&#20010;&#30740;&#31350;&#38382;&#39064;&#30340;&#20449;&#24687;&#22686;&#30410;&#12290;&#22312;&#36825;&#37324;&#65292;&#21487;&#29992;&#25968;&#25454;&#34987;&#36716;&#25442;&#25104;&#20107;&#20214;&#26085;&#24535;&#26684;&#24335;&#65292;&#24182;&#24212;&#29992;&#27969;&#31243;&#21457;&#29616;&#21644;&#31526;&#21512;&#24615;&#26816;&#26597;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#21512;&#24182;&#30151;&#65292;&#22914;&#31958;&#23615;&#30149;&#21644;&#24930;&#24615;&#32958;&#30149;&#65292;&#24739;&#32773;&#34987;&#20998;&#25104;&#19981;&#21516;&#30340;&#38431;&#21015;&#65292;&#24182;&#23545;&#22810;&#20010;&#23454;&#39564;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10544v1 Announce Type: cross  Abstract: Process mining in healthcare presents a range of challenges when working with different types of data within the healthcare domain. There is high diversity considering the variety of data collected from healthcare processes: operational processes given by claims data, a collection of events during surgery, data related to pre-operative and post-operative care, and high-level data collections based on regular ambulant visits with no apparent events. In this case study, a data set from the last category is analyzed. We apply process-mining techniques on sparse patient heart failure data and investigate whether an information gain towards several research questions is achievable. Here, available data are transformed into an event log format, and process discovery and conformance checking are applied. Additionally, patients are split into different cohorts based on comorbidities, such as diabetes and chronic kidney disease, and multiple st
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MATADOR&#65292;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#29255;&#19978;Tsetlin&#26426;&#35774;&#35745;&#29983;&#25104;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#23558;ML&#27169;&#22411;&#36716;&#25442;&#20026;SoC-FPGA&#35299;&#20915;&#26041;&#26696;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10538</link><description>&lt;p&gt;
MATADOR&#65306;&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#29255;&#19978;Tsetlin&#26426;&#35774;&#35745;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for Edge Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MATADOR&#65292;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;&#33258;&#21160;&#21270;&#29255;&#19978;Tsetlin&#26426;&#35774;&#35745;&#29983;&#25104;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#23558;ML&#27169;&#22411;&#36716;&#25442;&#20026;SoC-FPGA&#35299;&#20915;&#26041;&#26696;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
System-on-Chip Field-Programmable Gate Array&#65288;SoC-FPGA&#65289;&#36890;&#36807;&#35774;&#35745;&#21327;&#22788;&#29702;&#22120;&#21152;&#36895;&#22120;&#31995;&#32479;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36793;&#32536;&#25512;&#26029;&#24212;&#29992;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#21534;&#21520;&#37327;&#22686;&#30410;&#12290;&#28982;&#32780;&#65292;&#23558;ML&#27169;&#22411;&#35757;&#32451;&#24182;&#32763;&#35793;&#20026;SoC-FPGA&#35299;&#20915;&#26041;&#26696;&#30340;&#35774;&#35745;&#24037;&#20316;&#21487;&#33021;&#26159;&#30456;&#24403;&#22823;&#30340;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#26469;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#12289;&#21151;&#32791;&#12289;&#24310;&#36831;&#21644;&#36164;&#28304;&#21033;&#29992;&#29575;&#20043;&#38388;&#30340; trade-offs&#12290;&#19982;&#20854;&#20182;ML&#31639;&#27861;&#30456;&#21453;&#65292;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#36890;&#36807;&#20174;Tsetlin&#33258;&#21160;&#26426;&#65288;&#23398;&#20064;&#20803;&#32032;&#65289;&#21644;&#24067;&#23572;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#24418;&#25104;&#36923;&#36753;&#21629;&#39064;&#26469;&#25191;&#34892;&#20998;&#31867;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;TM&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#24456;&#39640;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#19988;&#36825;&#20123;&#36923;&#36753;&#21629;&#39064;&#22312;&#31867;&#21035;&#20869;&#37096;&#21644;&#31867;&#21035;&#20043;&#38388;&#37117;&#26377;&#30456;&#24403;&#22823;&#30340;&#37325;&#21472;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#26497;&#23569;&#37327;&#30340;AND&#21644;NOT&#38376;&#36716;&#25442;&#20026;RTL&#32423;&#30340;&#35774;&#35745;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MATADOR&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24067;&#23572;&#21040;si
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10538v1 Announce Type: cross  Abstract: System-on-Chip Field-Programmable Gate Arrays (SoC-FPGAs) offer significant throughput gains for machine learning (ML) edge inference applications via the design of co-processor accelerator systems. However, the design effort for training and translating ML models into SoC-FPGA solutions can be substantial and requires specialist knowledge aware trade-offs between model performance, power consumption, latency and resource utilization. Contrary to other ML algorithms, Tsetlin Machine (TM) performs classification by forming logic proposition between boolean actions from the Tsetlin Automata (the learning elements) and boolean input features. A trained TM model, usually, exhibits high sparsity and considerable overlapping of these logic propositions both within and among the classes. The model, thus, can be translated to RTL-level design using a miniscule number of AND and NOT gates. This paper presents MATADOR, an automated boolean-to-si
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26032;&#30340;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;VISREAS&#65292;&#36890;&#36807;&#39564;&#35777;&#38382;&#39064;&#22312;&#22238;&#31572;&#20043;&#21069;&#30340;&#26377;&#25928;&#24615;&#65292;&#35299;&#20915;&#20102;&#29992;&#25143;&#25552;&#20379;&#19981;&#23436;&#32654;&#25351;&#31034;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.10534</link><description>&lt;p&gt;
VISREAS: &#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#19982;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
VISREAS: Complex Visual Reasoning with Unanswerable Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26032;&#30340;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;VISREAS&#65292;&#36890;&#36807;&#39564;&#35777;&#38382;&#39064;&#22312;&#22238;&#31572;&#20043;&#21069;&#30340;&#26377;&#25928;&#24615;&#65292;&#35299;&#20915;&#20102;&#29992;&#25143;&#25552;&#20379;&#19981;&#23436;&#32654;&#25351;&#31034;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#39564;&#35777;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#22312;&#22238;&#31572;&#20043;&#21069;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#29992;&#25143;&#21487;&#33021;&#25552;&#20379;&#19981;&#23436;&#32654;&#30340;&#25351;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;VISREAS&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36890;&#36807;&#36941;&#21382;&#21644;&#25200;&#21160;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#32780;&#26500;&#25104;&#30340;&#21487;&#22238;&#31572;&#21644;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10534v1 Announce Type: cross  Abstract: Verifying a question's validity before answering is crucial in real-world applications, where users may provide imperfect instructions. In this scenario, an ideal model should address the discrepancies in the query and convey them to the users rather than generating the best possible answer. Addressing this requirement, we introduce a new compositional visual question-answering dataset, VISREAS, that consists of answerable and unanswerable visual queries formulated by traversing and perturbing commonalities and differences among objects, attributes, and relations. VISREAS contains 2.07M semantically diverse queries generated automatically using Visual Genome scene graphs. The unique feature of this task, validating question answerability with respect to an image before answering, and the poor performance of state-of-the-art models inspired the design of a new modular baseline, LOGIC2VISION that reasons by producing and executing pseudo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09732</link><description>&lt;p&gt;
PET-SQL&#65306;&#19968;&#20010;&#24102;&#26377;&#20132;&#21449;&#19968;&#33268;&#24615;&#30340;&#22686;&#24378;&#25552;&#31034;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;SQL&#65288;Text2SQL&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24378;&#35843;&#21050;&#28608;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#30340;&#29992;&#25143;&#24847;&#22270;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#34920;&#31034;&#65292;&#31216;&#20026;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#24335;&#20449;&#24687;&#21644;&#20174;&#34920;&#26684;&#38543;&#26426;&#25277;&#26679;&#30340;&#21333;&#20803;&#26684;&#20540;&#65292;&#20197;&#25351;&#23548;LLM&#29983;&#25104;SQL&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#26816;&#32034;&#38382;&#39064;-SQL&#23545;&#20316;&#20026;&#23569;&#37327;&#28436;&#31034;&#65292;&#20419;&#20351;LLM&#29983;&#25104;&#21021;&#27493;SQL&#65288;PreSQL&#65289;&#12290;&#20043;&#21518;&#65292;&#35299;&#26512;PreSQL&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#36827;&#34892;&#27169;&#24335;&#38142;&#25509;&#65292;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26377;&#29992;&#20449;&#24687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#38142;&#25509;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
&lt;/p&gt;</description></item><item><title>Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09629</link><description>&lt;p&gt;
Quiet-STaR: &#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#24049;&#23398;&#20250;&#24605;&#32771;&#21518;&#20877;&#35828;&#35805;
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09629
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#21644;&#20132;&#35848;&#26102;&#65292;&#20154;&#20204;&#26377;&#26102;&#20250;&#20572;&#19979;&#26469;&#24605;&#32771;&#12290;&#23613;&#31649;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#25512;&#29702;&#26694;&#23450;&#20026;&#22238;&#31572;&#38382;&#39064;&#25110;&#23436;&#25104;&#20195;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20294;&#25512;&#29702;&#20960;&#20046;&#37117;&#38544;&#21547;&#22312;&#25152;&#26377;&#20070;&#38754;&#25991;&#26412;&#20013;&#12290;&#20363;&#22914;&#65292;&#36825;&#36866;&#29992;&#20110;&#35777;&#26126;&#20013;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#27493;&#39588;&#65292;&#20197;&#21450;&#25903;&#25745;&#23545;&#35805;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#22312;&#33258;&#23398;&#20064;&#25512;&#29702;&#32773;&#65288;STaR&#65292;Zelikman&#31561;&#65292;2022&#65289;&#20013;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#25512;&#26029;&#26469;&#33258;&#38382;&#31572;&#20013;&#26377;&#29992;&#30340;&#24605;&#32771;&#65292;&#24182;&#23398;&#20064;&#37027;&#20123;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#24605;&#32771;&#12290;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;--&#29702;&#24819;&#24773;&#20917;&#19979;, &#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20250;&#20174;&#20219;&#24847;&#25991;&#26412;&#20013;&#25512;&#26029;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#24605;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;Quiet-STaR&#65292;&#36825;&#26159;STaR&#30340;&#19968;&#20010;&#27867;&#21270;&#29256;&#26412;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#39044;&#27979;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;1&#65289;&#29983;&#25104;&#36830;&#32493;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.09603</link><description>&lt;p&gt;
&#25511;&#21046;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#36827;&#34892;&#20048;&#35266;&#21487;&#39564;&#35777;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Optimistic Verifiable Training by Controlling Hardware Nondeterminism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#26085;&#30410;&#22686;&#21152;&#30340;&#35745;&#31639;&#38656;&#27714;&#23548;&#33268;&#20102;&#20026;&#32570;&#20047;&#24517;&#35201;&#36164;&#28304;&#30340;&#23458;&#25143;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#26381;&#21153;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#35757;&#32451;&#30340;&#27491;&#30830;&#24615;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#35757;&#32451;&#26102;&#25915;&#20987;&#65292;&#20363;&#22914;&#25968;&#25454;&#27602;&#21270;&#65292;&#37117;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#21487;&#39564;&#35777;&#35757;&#32451;&#30340;&#24037;&#20316;&#20027;&#35201;&#20998;&#20026;&#20004;&#31867;&#65306;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#38656;&#35201;&#21152;&#23494;&#25216;&#26415;&#32780;&#38590;&#20197;&#25193;&#23637;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#19968;&#20010;&#21487;&#20449;&#31532;&#19977;&#26041;&#23457;&#35745;&#21592;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#30340;&#8220;&#20048;&#35266;&#8221;&#26041;&#27861;&#12290; &#21518;&#32773;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;GPU&#31867;&#22411;&#20043;&#38388;&#30340;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#38459;&#27490;&#23457;&#35745;&#21592;&#31934;&#30830;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#26041;&#26696;&#19981;&#22815;&#20581;&#22766;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#19979;&#36827;&#34892;&#65292;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#22235;&#33293;&#20116;&#20837;&#65292;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09603v1 Announce Type: cross  Abstract: The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#26032;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#31934;&#24230;&#26368;&#22823;&#21270;&#21644;&#21162;&#21147;&#20943;&#23569;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#22312;&#35814;&#23613;&#30340;&#36923;&#36753;&#22788;&#29702;&#21644;&#20351;&#29992;&#35748;&#30693;&#24555;&#25463;&#26041;&#24335;&#65288;&#21551;&#21457;&#24335;&#65289;&#20043;&#38388;&#36716;&#25442;&#30340;&#26465;&#20214;&#65292;&#24182;&#21306;&#20998;&#20102;&#21551;&#21457;&#24335;&#30340;&#24037;&#20855;&#24615;&#20351;&#29992;&#21644;&#27169;&#20223;&#21560;&#25910;&#20004;&#31181;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.09404</link><description>&lt;p&gt;
AI&#20013;&#30340;&#21551;&#21457;&#24335;&#25512;&#29702;: &#24037;&#20855;&#24615;&#20351;&#29992;&#19982;&#27169;&#20223;&#21560;&#25910;
&lt;/p&gt;
&lt;p&gt;
Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#26032;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#31934;&#24230;&#26368;&#22823;&#21270;&#21644;&#21162;&#21147;&#20943;&#23569;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#22312;&#35814;&#23613;&#30340;&#36923;&#36753;&#22788;&#29702;&#21644;&#20351;&#29992;&#35748;&#30693;&#24555;&#25463;&#26041;&#24335;&#65288;&#21551;&#21457;&#24335;&#65289;&#20043;&#38388;&#36716;&#25442;&#30340;&#26465;&#20214;&#65292;&#24182;&#21306;&#20998;&#20102;&#21551;&#21457;&#24335;&#30340;&#24037;&#20855;&#24615;&#20351;&#29992;&#21644;&#27169;&#20223;&#21560;&#25910;&#20004;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#25512;&#29702;&#26041;&#26696;&#65292;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#21019;&#26032;&#24615;&#23454;&#39564;&#65292;&#21253;&#25324;&#23545;&#32463;&#20856;&#29747;&#36798;&#38382;&#39064;&#30340;&#21464;&#20307;&#21644;&#23545;&#32654;&#20029;&#27604;&#36187;&#28216;&#25103;&#30340;&#26032;&#24212;&#29992;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#31934;&#24230;&#26368;&#22823;&#21270;&#21644;&#21162;&#21147;&#20943;&#23569;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#22609;&#36896;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#22312;&#35814;&#23613;&#30340;&#36923;&#36753;&#22788;&#29702;&#21644;&#20351;&#29992;&#35748;&#30693;&#24555;&#25463;&#26041;&#24335;&#65288;&#21551;&#21457;&#24335;&#65289;&#20043;&#38388;&#36716;&#25442;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#21551;&#21457;&#24335;&#30340;'&#24037;&#20855;&#24615;'&#20351;&#29992;&#65292;&#20197;&#21305;&#37197;&#36164;&#28304;&#19982;&#30446;&#26631;&#65292;&#20197;&#21450;'&#27169;&#20223;&#21560;&#25910;'&#65292;&#21363;&#20174;&#20154;&#31867;&#37027;&#37324;&#23398;&#21040;&#30340;&#21551;&#21457;&#24335;&#65292;&#24182;&#34920;&#29616;&#20026;&#38543;&#26426;&#19988;&#26222;&#36941;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#32570;&#20047;&#20869;&#22312;&#30446;&#26631;&#25110;&#33258;&#25105;&#24847;&#35782;&#65292;&#20154;&#24037;&#26234;&#33021;&#34920;&#29616;&#20986;&#31934;&#24230;&#21644;&#25928;&#29575;&#30340;&#33258;&#36866;&#24212;&#24179;&#34913;&#65292;&#31526;&#21512;&#36164;&#28304;&#29702;&#24615;&#20154;&#31867;&#35748;&#30693;&#30340;&#21407;&#21017;&#65292;&#36825;&#26159;&#21463;&#38480;&#29702;&#24615;&#21644;&#21452;&#31995;&#32479;&#29702;&#35770;&#32463;&#20856;&#29702;&#35770;&#30340;&#26126;&#25991;&#38416;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09404v1 Announce Type: new  Abstract: We propose a novel program of heuristic reasoning within artificial intelligence (AI) systems. Through a series of innovative experiments, including variations of the classic Linda problem and a novel application of the Beauty Contest game, we uncover trade-offs between accuracy maximization and effort reduction that shape the conditions under which AIs transition between exhaustive logical processing and the use of cognitive shortcuts (heuristics). We distinguish between the 'instrumental' use of heuristics to match resources with objectives, and 'mimetic absorption,' whereby heuristics are learned from humans, and manifest randomly and universally. We provide evidence that AI, despite lacking intrinsic goals or self-awareness, manifests an adaptive balancing of precision and efficiency, consistent with principles of resource-rational human cognition as explicated in classical theories of bounded rationality and dual-process theory.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LAN&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#27963;&#21160;&#32423;&#21035;&#36827;&#34892;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65292;&#24182;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.09209</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#36866;&#24212;&#37051;&#23621;&#20197;&#23454;&#26102;&#26816;&#27979;&#20869;&#37096;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LAN&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#27963;&#21160;&#32423;&#21035;&#36827;&#34892;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65292;&#24182;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#21644;&#32452;&#32455;&#38754;&#20020;&#26469;&#33258;&#20869;&#37096;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;&#20808;&#21069;&#20851;&#20110;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65288;ITD&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26816;&#27979;&#24322;&#24120;&#29992;&#25143;&#25110;&#24322;&#24120;&#26102;&#38388;&#27573;&#65288;&#20363;&#22914;&#65292;&#19968;&#21608;&#25110;&#19968;&#22825;&#65289;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21487;&#33021;&#22312;&#26085;&#24535;&#20013;&#26377;&#25968;&#21313;&#19975;&#26465;&#27963;&#21160;&#65292;&#21363;&#20351;&#22312;&#19968;&#22825;&#20869;&#65292;&#19968;&#20010;&#29992;&#25143;&#20063;&#21487;&#33021;&#23384;&#22312;&#25968;&#21315;&#26465;&#27963;&#21160;&#65292;&#36825;&#38656;&#35201;&#39640;&#26114;&#30340;&#35843;&#26597;&#39044;&#31639;&#26469;&#39564;&#35777;&#24322;&#24120;&#29992;&#25143;&#25110;&#27963;&#21160;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#26159;&#20107;&#21518;&#26041;&#27861;&#32780;&#19981;&#26159;&#23454;&#26102;&#26816;&#27979;&#65292;&#26080;&#27861;&#21450;&#26102;&#25253;&#21578;&#20869;&#37096;&#23041;&#32961;&#22312;&#24341;&#36215;&#25439;&#22833;&#20043;&#21069;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38024;&#23545;&#27963;&#21160;&#32423;&#21035;&#23454;&#26102;ITD&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#21644;&#39640;&#25928;&#30340;&#26694;&#26550;LAN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LAN&#21516;&#26102;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09209v1 Announce Type: cross  Abstract: Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between ac
&lt;/p&gt;</description></item><item><title>AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09113</link><description>&lt;p&gt;
AutoLoRA&#65306;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#33258;&#21160;&#35843;&#25972;&#30697;&#38453;&#31209;&#22312;&#20302;&#31209;&#36866;&#24212;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09113
&lt;/p&gt;
&lt;p&gt;
AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20043;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#23384;&#22312;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#21457;&#20102;&#20960;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#36890;&#36807;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#20043;&#19978;&#24494;&#35843;&#20302;&#31209;&#22686;&#37327;&#26356;&#26032;&#30697;&#38453;&#65292;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;LoRA&#22312;&#25152;&#26377;&#23618;&#20013;&#22343;&#21248;&#20998;&#37197;&#31209;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#31351;&#20030;&#25628;&#32034;&#26469;&#25214;&#21040;&#26368;&#20339;&#31209;&#65292;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24494;&#35843;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoLoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#12290;AutoLoRA&#23558;&#20302;&#31209;&#26356;&#26032;&#30697;&#38453;&#20013;&#30340;&#27599;&#20010;&#31209;&#20026;1&#30340;&#30697;&#38453;&#19982;&#36873;&#25321;&#21464;&#37327;&#30456;&#20851;&#32852;&#65292;&#35813;&#21464;&#37327;&#20915;&#23450;&#20102;&#31209;&#20026;1&#30340;&#30697;&#38453;&#26159;&#21542;&#24212;&#35813;&#34987;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19977;&#31181;&#20027;&#35201;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#25910;&#38598;&#30340;333&#20010;&#32570;&#38519;&#26679;&#26412;&#65292;&#24182;&#35782;&#21035;&#20102;10&#31181;&#29420;&#29305;&#30340;&#38169;&#35823;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.08937</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Bugs in Large Language Models Generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19977;&#31181;&#20027;&#35201;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#25910;&#38598;&#30340;333&#20010;&#32570;&#38519;&#26679;&#26412;&#65292;&#24182;&#35782;&#21035;&#20102;10&#31181;&#29420;&#29305;&#30340;&#38169;&#35823;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#23427;&#20204;&#21487;&#20197;&#22522;&#20110;&#25552;&#20379;&#30340;&#25552;&#31034;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#29983;&#25104;&#20195;&#30721;&#65292;&#23454;&#29616;&#20102;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#19968;&#20010;&#26790;&#24819;&#65292;&#21363;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#12290;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#20195;&#30721;&#31867;&#20284;&#65292;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#20294;&#31038;&#21306;&#23578;&#26410;&#23545;&#36825;&#20123;&#38169;&#35823;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#37492;&#20110;&#36719;&#20214;&#24037;&#31243;&#27963;&#21160;&#20013;LLM-based&#20195;&#30721;&#29983;&#25104;&#24037;&#20855;&#65288;&#20363;&#22914;GitHub Copilot&#65289;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#20102;&#35299;LLMs&#29983;&#25104;&#20195;&#30721;&#20013;&#21253;&#21547;&#30340;&#32570;&#38519;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19977;&#31181;&#20027;&#35201;LLM&#65288;&#21363;CodeGen&#12289;PanGu-Coder&#21644;Codex&#65289;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#25910;&#38598;&#30340;333&#20010;&#32570;&#38519;&#26679;&#26412;&#65292;&#24182;&#35782;&#21035;&#20102;&#20197;&#19979;10&#31181;&#29420;&#29305;&#30340;&#38169;&#35823;&#27169;&#24335;&#65306;&#35823;&#35299;&#12289;&#35821;&#27861;&#38169;&#35823;&#12289;&#24858;&#34850;&#38169;&#35823;&#12289;&#25552;&#31034;&#20559;&#21521;&#20195;&#30721;&#12289;&#36951;&#28431;&#35282;&#33853;&#26696;&#20363;&#12289;&#38169;&#35823;&#30340;&#36755;&#20837;&#31867;&#22411;&#12289;&#20135;&#29983;&#24187;&#35937;&#23545;&#35937;&#12289;&#38169;&#35823;&#30340;&#23646;&#24615;&#12289;&#19981;&#23436;&#25972;&#65288;&#26410;&#23436;&#32467;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08937v1 Announce Type: cross  Abstract: Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomple
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.08333</link><description>&lt;p&gt;
&#24555;&#36895;&#25512;&#26029;&#22522;&#20110;&#31227;&#38500;&#30340;&#33410;&#28857;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fast Inference of Removal-Based Node Influence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25429;&#33719;&#22270;&#20013;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#36235;&#21183;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#35757;&#32451;&#22909;&#30340;GNN&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#19968;&#20010;&#30495;&#23454;&#24212;&#29992;&#26159;&#65292;&#8220;&#22312;&#39044;&#27979;Twitter&#36134;&#25143;&#26497;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#26524;&#31227;&#38500;&#29305;&#23450;&#36134;&#25143;&#65292;&#20854;&#20182;&#36134;&#25143;&#30340;&#26497;&#24615;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#8221;&#25105;&#20204;&#23558;GNN&#20316;&#20026;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#21487;&#20197;&#27169;&#25311;&#31227;&#38500;&#33410;&#28857;&#24341;&#36215;&#30340;&#33410;&#28857;&#25110;&#36793;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#24433;&#21709;&#65292;&#19968;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#20132;&#26367;&#31227;&#38500;&#27599;&#20010;&#33410;&#28857;&#65292;&#24182;&#22312;&#20462;&#25913;&#21518;&#30340;&#22270;&#19978;&#24212;&#29992;&#35757;&#32451;&#22909;&#30340;GNN&#12290;&#36825;&#26159;&#21487;&#38752;&#30340;&#20294;&#32791;&#26102;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
&lt;/p&gt;</description></item><item><title>GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;</title><link>https://arxiv.org/abs/2403.08293</link><description>&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65306;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08293
&lt;/p&gt;
&lt;p&gt;
GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#20197;&#20174;&#24038;&#21040;&#21491;&#30340;&#26041;&#24335;&#36880;&#27493;&#29983;&#25104;&#24102;&#26377;&#20854;&#21477;&#27861;&#26641;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65288;GPST&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;SLM&#65292;&#33021;&#22815;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#12290;GPST&#35268;&#36991;&#20102;&#20043;&#21069;SLM&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#12290;&#23427;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#19968;&#20010;&#36890;&#24120;&#30340;SLM&#21463;&#21333;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#39069;&#22806;&#30340;&#32452;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#23548;&#21477;&#27861;&#35299;&#26512;&#26641;&#24182;&#35745;&#31639;&#25104;&#20998;&#34920;&#31034;&#65292;&#21463;&#21452;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20004;&#20010;&#27169;&#22411;&#30340;&#32852;&#21512;&#24182;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#30828;EM&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;OpenWebText&#19978;&#23545;GPST&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;90&#20159;&#20010;token&#65292;&#24182;&#23637;&#31034;&#20102;GPST&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#28085;&#30422;&#20102;&#19982;GPT-2&#30456;&#24403;&#35268;&#27169;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08293v1 Announce Type: cross  Abstract: A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering bot
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08281</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25484;&#25569;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08281
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#12289;&#32534;&#31243;&#20195;&#30721;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#24040;&#22823;&#65292;&#23545;&#20110;&#37027;&#20123;&#21162;&#21147;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#20102;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#34701;&#21512;&#24050;&#32463;&#39640;&#24230;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#26694;&#26550;UltraFuser&#21253;&#25324;&#19977;&#20010;&#24050;&#32463;&#22312;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#25968;&#23398;&#19978;&#24471;&#21040;&#20805;&#20998;&#35757;&#32451;&#30340;&#19987;&#23478;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#26469;&#28151;&#21512;&#19987;&#23478;&#30340;&#36755;&#20986;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20276;&#38543;&#24179;&#34913;&#37319;&#26679;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20197;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#34701;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#23376;&#27169;&#20989;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08199</link><description>&lt;p&gt;
&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Submodular Peripteral Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08199
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#23376;&#27169;&#20989;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#20989;&#25968;&#23545;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23454;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#33719;&#21462;&#23427;&#20204;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23376;&#27169;&#20989;&#25968;&#21442;&#25968;&#21270;&#26063;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#36830;&#25509;&#24182;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08199v1 Announce Type: cross  Abstract: Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style "peripteral" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07440</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#65306;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LPLMs&#65289;&#30340;&#24494;&#35843;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#25511;&#21046;LPLMs&#30340;&#36755;&#20986;&#34892;&#20026;&#12290;&#26412;&#25991;&#21463;&#22823;&#33041;&#21151;&#33021;&#21463;&#20854;&#20960;&#20309;&#32467;&#26500;&#22609;&#36896;&#30340;&#21551;&#21457;&#65292;&#23558;&#36825;&#19968;&#24605;&#24819;&#34701;&#20837;LoRA&#25216;&#26415;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30452;&#35273;&#27169;&#31946;&#20915;&#31574;&#26641;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#38543;&#26426;&#24615;&#12289;&#27169;&#31946;&#36923;&#36753;&#21644;&#27169;&#31946;&#38598;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#21450;&#22810;&#20998;&#31867;&#22120;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2403.07363</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#30452;&#35273;&#27169;&#31946;&#20915;&#31574;&#26641;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07363
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30452;&#35273;&#27169;&#31946;&#20915;&#31574;&#26641;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#38543;&#26426;&#24615;&#12289;&#27169;&#31946;&#36923;&#36753;&#21644;&#27169;&#31946;&#38598;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#21450;&#22810;&#20998;&#31867;&#22120;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#23545;&#20110;&#25968;&#25454;&#25366;&#25496;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#25925;&#38556;&#26816;&#27979;&#39046;&#22495;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24320;&#21457;&#20934;&#30830;&#12289;&#36866;&#29992;&#19988;&#39640;&#25928;&#30340;&#20998;&#31867;&#26041;&#27861;&#21644;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#24378;&#28872;&#38656;&#27714;&#12290;&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#24120;&#29992;&#20110;&#22797;&#26434;&#26465;&#20214;&#19979;&#20998;&#31867;&#30340;&#36890;&#29992;&#31639;&#27861;&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#20854;&#19982;&#19981;&#21516;&#27169;&#31946;&#29702;&#35770;&#30340;&#32467;&#21512;&#20173;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#30452;&#35273;&#27169;&#31946;&#38543;&#26426;&#26862;&#26519;&#65288;IFRF&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#30001;&#30452;&#35273;&#27169;&#31946;&#20915;&#31574;&#26641;&#65288;IFDT&#65289;&#32452;&#25104;&#30340;&#26032;&#30340;&#38543;&#26426;&#26862;&#26519;&#38598;&#25104;&#12290;&#26862;&#26519;&#20013;&#30340;&#36825;&#20123;&#26641;&#20351;&#29992;&#30452;&#35273;&#27169;&#31946;&#20449;&#24687;&#22686;&#30410;&#26469;&#36873;&#25321;&#29305;&#24449;&#65292;&#24182;&#32771;&#34385;&#20449;&#24687;&#20256;&#36755;&#20013;&#30340;&#29369;&#35947;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34701;&#21512;&#20102;&#26469;&#33258;&#33258;&#21161;&#25277;&#26679;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#38543;&#26426;&#24615;&#12289;&#27169;&#31946;&#36923;&#36753;&#21644;&#27169;&#31946;&#38598;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#21450;&#22810;&#20998;&#31867;&#22120;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07363v1 Announce Type: new  Abstract: Classification is essential to the applications in the field of data mining, artificial intelligence, and fault detection. There exists a strong need in developing accurate, suitable, and efficient classification methods and algorithms with broad applicability. Random forest is a general algorithm that is often used for classification under complex conditions. Although it has been widely adopted, its combination with diverse fuzzy theory is still worth exploring. In this paper, we propose the intuitionistic fuzzy random forest (IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees (IFDT). Such trees in forest use intuitionistic fuzzy information gain to select features and consider hesitation in information transmission. The proposed method enjoys the power of the randomness from bootstrapped sampling and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the robustness of multiple classifier syste
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22914;&#20309;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#21457;&#29616;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06880</link><description>&lt;p&gt;
&#25581;&#31034;&#21463;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22312;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06880
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22914;&#20309;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#21457;&#29616;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24188;&#20799;&#20174;&#31232;&#30095;&#21453;&#39304;&#30340;&#33258;&#30001;&#25506;&#32034;&#36880;&#28176;&#21457;&#23637;&#20026;&#21033;&#29992;&#20808;&#21069;&#32463;&#39564;&#36827;&#34892;&#20197;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#23398;&#20064;&#65292;&#33719;&#24471;&#26356;&#23494;&#38598;&#22870;&#21169;&#12290;&#21463;&#27492;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#19981;&#21516;&#22870;&#21169;&#36716;&#25442;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#24847;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#20174;&#31232;&#30095;&#21040;&#22522;&#20110;&#28508;&#22312;&#30340;&#23494;&#38598;&#22870;&#21169;&#30340;&#36716;&#25442;&#65292;&#36825;&#20004;&#32773;&#20849;&#20139;&#26080;&#35770;&#22870;&#21169;&#21464;&#21270;&#22343;&#20026;&#26368;&#20339;&#31574;&#30053;&#12290;&#36890;&#36807;&#21253;&#25324;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#23548;&#33322;&#21644;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#22312;&#20869;&#30340;&#21508;&#31181;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22870;&#21169;&#36716;&#25442;&#26174;&#33879;&#24433;&#21709;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#21463;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#36825;&#20123;&#24615;&#33021;&#25351;&#26631;&#22806;&#65292;&#20351;&#29992;&#20132;&#21449;&#23494;&#24230;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36716;&#25442;&#65292;&#29305;&#21035;&#26159;S2D&#65292;&#20351;&#31574;&#30053;&#25439;&#22833;&#26223;&#35266;&#26356;&#21152;&#24179;&#28369;&#65292;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06880v1 Announce Type: cross  Abstract: Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting
&lt;/p&gt;</description></item><item><title>ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.06754</link><description>&lt;p&gt;
ALaRM: &#36890;&#36807;&#20998;&#23618;&#22870;&#21169;&#24314;&#27169;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ALaRM: Align Language Models via Hierarchical Rewards Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06754
&lt;/p&gt;
&lt;p&gt;
ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ALaRM&#65292;&#31532;&#19968;&#20010;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20174;&#20154;&#31867;&#21453;&#39304;&#27169;&#22411;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#40784;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#20154;&#31867;&#30417;&#30563;&#20449;&#21495;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36890;&#36807;&#23558;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#21644;&#19968;&#33268;&#22320;&#25351;&#23548;&#26397;&#30528;&#26399;&#26395;&#30340;&#32467;&#26524;&#21069;&#36827;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#21644;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26426;&#21046;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#20351;&#29992;gpt-3.5-turbo&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
&lt;/p&gt;</description></item><item><title>RepoHyper&#25552;&#20986;&#20102;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#21644;Expand&#21644;Refine&#26816;&#32034;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#28385;&#36275;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.06095</link><description>&lt;p&gt;
RepoHyper&#65306;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#26159;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06095
&lt;/p&gt;
&lt;p&gt;
RepoHyper&#25552;&#20986;&#20102;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#21644;Expand&#21644;Refine&#26816;&#32034;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#28385;&#36275;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06095v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CodeLLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#39033;&#30446;&#20179;&#24211;&#30340;&#24191;&#27867;&#19978;&#19979;&#25991;&#65292;&#27604;&#22914;&#30456;&#20851;&#25991;&#20214;&#21644;&#31867;&#23618;&#27425;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#34917;&#20840;&#19981;&#22815;&#31934;&#30830;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepoHyper&#65292;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#19982;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30456;&#20851;&#30340;&#22797;&#26434;&#25361;&#25112;&#30340;&#22810;&#26041;&#38754;&#26694;&#26550;&#12290;RepoHyper&#30340;&#26680;&#24515;&#26159;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#65292;&#19968;&#31181;&#23553;&#35013;&#20195;&#30721;&#20179;&#24211;&#24191;&#27867;&#19978;&#19979;&#25991;&#30340;&#26032;&#39062;&#35821;&#20041;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;RepoHyper&#21033;&#29992;&#25193;&#23637;&#21644;&#32454;&#21270;&#26816;&#32034;&#26041;&#27861;&#65292;&#21253;&#25324;&#24212;&#29992;&#20110;RSG&#30340;&#22270;&#25193;&#23637;&#21644;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#30340;&#26377;&#25928;&#26816;&#32034;&#21644;&#20248;&#20808;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RepoHyper&#22312;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06095v1 Announce Type: cross  Abstract: Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHyper is the Repo-level Semantic Graph (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that RepoHyper markedly outperforms existing techniques in re
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#24037;&#20316;&#27969;&#26550;&#26500;QCQ&#23454;&#29616;&#20102;&#37327;&#23376;&#27169;&#25311;&#20013;&#30340;&#21019;&#26032;&#65292;&#36890;&#36807;&#22312;QPUs&#19978;&#36816;&#34892;VQE&#31639;&#27861;&#21644;&#22312;&#32463;&#20856;&#30828;&#20214;&#19978;&#36827;&#34892;&#37327;&#23376;&#24577;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;cuQuantum SDK&#21644;PennyLane's Lightning plugin&#65292;&#20026;&#26448;&#26009;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#39046;&#22495;&#30340;&#35745;&#31639;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.05828</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;GPU&#21551;&#29992;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#24037;&#20316;&#27969;&#30340;Quantum-HPC&#26694;&#26550;&#65306;&#22312;&#37327;&#23376;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical Workflow: Applications in Quantum Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05828
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#24037;&#20316;&#27969;&#26550;&#26500;QCQ&#23454;&#29616;&#20102;&#37327;&#23376;&#27169;&#25311;&#20013;&#30340;&#21019;&#26032;&#65292;&#36890;&#36807;&#22312;QPUs&#19978;&#36816;&#34892;VQE&#31639;&#27861;&#21644;&#22312;&#32463;&#20856;&#30828;&#20214;&#19978;&#36827;&#34892;&#37327;&#23376;&#24577;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;cuQuantum SDK&#21644;PennyLane's Lightning plugin&#65292;&#20026;&#26448;&#26009;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#39046;&#22495;&#30340;&#35745;&#31639;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#23376;&#31995;&#32479;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#35745;&#31639;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#24357;&#21512;&#37327;&#23376;&#30828;&#20214;&#21644;&#32463;&#20856;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;Quantum-Classical-Quantum (QCQ)&#26550;&#26500;&#65292;&#23558;&#21069;&#27839;&#30340;&#37327;&#23376;&#36719;&#20214;&#26694;&#26550;&#19982;&#39640;&#24615;&#33021;&#32463;&#20856;&#35745;&#31639;&#36164;&#28304;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#39046;&#22495;&#30340;&#37327;&#23376;&#27169;&#25311;&#25361;&#25112;&#12290;&#35813;&#26550;&#26500;&#30340;&#26680;&#24515;&#26159;&#22312;QPUs&#19978;&#36816;&#34892;VQE&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#37327;&#23376;&#24577;&#20934;&#22791;&#65292;&#22312;&#32463;&#20856;&#30828;&#20214;&#19978;&#36827;&#34892;&#37327;&#23376;&#24577;&#20998;&#31867;&#30340;Tensor Network states&#21644;QCNNs&#30340;&#26080;&#32541;&#38598;&#25104;&#12290; &#20026;&#20102;&#23545;&#37327;&#23376;&#27169;&#25311;&#22120;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;QCQ&#26550;&#26500;&#21033;&#29992;cuQuantum SDK&#21033;&#29992;&#22810;GPU&#21152;&#36895;&#65292;&#38598;&#25104;&#20102;PennyLane&#30340;Lightning&#25554;&#20214;&#65292;&#23637;&#31034;&#20102;&#35745;&#31639;&#36895;&#24230;&#22686;&#21152;&#22810;&#36798;&#21313;&#20493;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05828v1 Announce Type: cross  Abstract: Achieving high-performance computation on quantum systems presents a formidable challenge that necessitates bridging the capabilities between quantum hardware and classical computing resources. This study introduces an innovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture, which integrates cutting-edge quantum software framework works with high-performance classical computing resources to address challenges in quantum simulation for materials and condensed matter physics. At the heart of this architecture is the seamless integration of VQE algorithms running on QPUs for efficient quantum state preparation, Tensor Network states, and QCNNs for classifying quantum states on classical hardware.   For benchmarking quantum simulators, the QCQ architecture utilizes the cuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's Lightning plugin, demonstrating up to tenfold increases in computational spe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MG-TSD&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#30446;&#26631;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;-of-the-art&#30340;&#39044;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.05751</link><description>&lt;p&gt;
MG-TSD&#65306;&#20855;&#26377;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#30340;&#22810;&#31890;&#24230;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MG-TSD&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#30446;&#26631;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;-of-the-art&#30340;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30001;&#20110;&#20854;&#29983;&#25104;&#39640;&#20445;&#30495;&#26679;&#26412;&#30340;&#26174;&#33879;&#33021;&#21147;&#32780;&#22312;&#29983;&#25104;&#24335;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24341;&#36215;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#29305;&#24615;&#24102;&#26469;&#30340;&#19981;&#31283;&#23450;&#24615;&#25361;&#25112;&#65292;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#22312;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24378;&#22823;&#24314;&#27169;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#31890;&#24230;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#65288;MG-TSD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#30340;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#20013;&#38388;&#25193;&#25955;&#27493;&#39588;&#30340;&#32473;&#23450;&#30446;&#26631;&#26469;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05751v1 Announce Type: cross  Abstract: Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate diffusion steps to guide the learning process of diffusion models. The way to construct the targets is motivated by the observation that the forward process of the diffusion model, which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the p
&lt;/p&gt;</description></item><item><title>DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;</title><link>https://arxiv.org/abs/2403.05050</link><description>&lt;p&gt;
DyRoNet&#65306;&#19968;&#31181;&#20302;&#31209;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65292;&#29992;&#20110;&#27969;&#23186;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05050
&lt;/p&gt;
&lt;p&gt;
DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#24863;&#30693;&#26469;&#24212;&#23545;&#22797;&#26434;&#29615;&#22659;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65288;DyRoNet&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#20197;&#22686;&#24378;&#27969;&#23186;&#20307;&#24863;&#30693;&#12290;&#36890;&#36807;&#38598;&#25104;&#19987;&#38376;&#39044;&#35757;&#32451;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#38024;&#23545;&#21508;&#31181;&#29615;&#22659;&#26465;&#20214;&#36827;&#34892;&#24494;&#35843;&#65292;DyRoNet&#22312;&#24310;&#36831;&#21644;&#31934;&#24230;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#20854;&#26680;&#24515;&#29305;&#24449;&#26159;&#36895;&#24230;&#36335;&#30001;&#27169;&#22359;&#65292;&#26234;&#33021;&#22320;&#23558;&#36755;&#20837;&#25968;&#25454;&#24341;&#23548;&#21040;&#26368;&#36866;&#21512;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#20248;&#21270;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;DyRoNet&#26377;&#25928;&#22320;&#36866;&#24212;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#65292;&#20026;&#21508;&#31181;&#22330;&#26223;&#24615;&#33021;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#26438;&#12290;DyRoNet&#19981;&#20165;&#20026;&#27969;&#23186;&#20307;&#24863;&#30693;&#24314;&#31435;&#20102;&#26032;&#30340;&#26631;&#26438;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24037;&#31243;&#27934;&#35265;&#12290;&#26377;&#20851;&#26356;&#22810;&#39033;&#30446;&#20449;&#24687;&#65292;&#35831;&#35775;&#38382; https://tastevision.github.io/DyRoNet/
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05050v1 Announce Type: cross  Abstract: Autonomous driving systems demand real-time, accurate perception to navigate complex environments. Addressing this, we introduce the Dynamic Router Network (DyRoNet), a framework that innovates with low-rank dynamic routing for enhanced streaming perception. By integrating specialized pre-trained branch networks, fine-tuned for various environmental conditions, DyRoNet achieves a balance between latency and precision. Its core feature, the speed router module, intelligently directs input data to the best-suited branch network, optimizing performance. The extensive evaluations reveal that DyRoNet adapts effectively to multiple branch selection strategies, setting a new benchmark in performance across a range of scenarios. DyRoNet not only establishes a new benchmark for streaming perception but also provides valuable engineering insights for future work. More project information is available at https://tastevision.github.io/DyRoNet/
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04325</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20998;&#25968;&#27979;&#37327;&#20154;&#33041;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04325
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21547;&#20041;&#21512;&#25104;&#30340;&#36807;&#31243;&#26159;&#25351;&#26356;&#23567;&#30340;&#21333;&#20301;&#22914;&#35821;&#32032;&#25110;&#21333;&#35789;&#32452;&#21512;&#24418;&#25104;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#23545;&#20110;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#35821;&#35328;&#23398;&#23545;&#28041;&#21450;&#21547;&#20041;&#21512;&#25104;&#30340;&#22823;&#33041;&#21306;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20173;&#32570;&#20047;&#19968;&#31181;&#35745;&#31639;&#24230;&#37327;&#26469;&#37327;&#21270;&#21512;&#25104;&#30340;&#31243;&#24230;&#12290;&#20511;&#37492;&#21464;&#21387;&#22120;&#21069;&#39304;&#32593;&#32476;&#22359;&#30340;&#38190;&#20540;&#20869;&#23384;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#21512;&#20998;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#31751;&#30456;&#20851;&#32852;&#65292;&#36825;&#20123;&#22823;&#33041;&#31751;&#19982;&#35789;&#39057;&#29575;&#12289;&#32467;&#26500;&#22788;&#29702;&#21644;&#23545;&#21333;&#35789;&#30340;&#19968;&#33324;&#25935;&#24863;&#24615;&#26377;&#20851;&#65292;&#36825;&#34920;&#26126;&#20102;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#21547;&#20041;&#21512;&#25104;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04325v1 Announce Type: cross  Abstract: The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04164</link><description>&lt;p&gt;
ProMISe: &#20351;&#29992;SAM&#36827;&#34892;&#21487;&#25552;&#31034;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ProMISe: Promptable Medical Image Segmentation using SAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25552;&#20986;&#20102;Segment Anything Model (SAM)&#30340;&#24314;&#35758;&#65292;&#23545;SAM&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;(MIS)&#30340;&#24494;&#35843;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SAM&#27169;&#22411;&#30340;&#35268;&#27169;&#36739;&#22823;&#65292;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#22522;&#20110;&#24494;&#35843;&#30340;&#31574;&#30053;&#25104;&#26412;&#39640;&#65292;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#12289;&#29305;&#24449;&#25439;&#20260;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#23558;SAM&#36716;&#31227;&#21040;&#29305;&#23450;&#39046;&#22495;MIS&#30340;&#26041;&#27861;&#31105;&#29992;&#20102;&#27169;&#22411;&#30340;&#25552;&#31034;&#33021;&#21147;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#20351;&#29992;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#22495;&#20013;&#25552;&#20379;&#20102;&#20855;&#26377;&#27431;&#20960;&#37324;&#24503;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;MIS&#20013;&#38750;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#27169;&#24335;&#31227;&#20301;&#65288;IPS&#65289;&#30340;&#26032;&#22411;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;SAM&#35843;&#25972;&#21040;&#29305;&#23450;&#21307;&#30103;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04164v1 Announce Type: cross  Abstract: With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for medical image segmentation (MIS) has become popular. However, due to the large size of the SAM model and the significant domain gap between natural and medical images, fine-tuning-based strategies are costly with potential risk of instability, feature damage and catastrophic forgetting. Furthermore, some methods of transferring SAM to a domain-specific MIS through fine-tuning strategies disable the model's prompting capability, severely limiting its utilization scenarios. In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based foundation model with Euclidean adaptive prompts in the target domain. Our experiments demonstrate that such adaptive prompts significantly improve SAM's non-fine-tuned performance in MIS. In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medica
&lt;/p&gt;</description></item><item><title>Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.03835</link><description>&lt;p&gt;
Cobweb&#65306;&#19968;&#31181;&#22686;&#37327;&#21644;&#20998;&#23618;&#24335;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03835
&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#19982;&#20854;&#20182;&#22686;&#37327;&#20998;&#31867;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21033;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Cobweb&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#65292;&#22914;&#22522;&#26412;&#27700;&#24179;&#12289;&#20856;&#22411;&#24615;&#21644;&#25159;&#24418;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#23545;Cobweb&#20316;&#20026;&#20154;&#31867;&#20998;&#31867;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#23427;&#30830;&#23450;&#20102;Cobweb&#19982;&#32463;&#20856;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25928;&#24212;&#30340;&#19968;&#33268;&#24615;&#12290;&#36824;&#25506;&#35752;&#20102;Cobweb&#23637;&#29616;&#20986;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#26082;&#26377;&#23454;&#20363;&#21448;&#26377;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23558;&#26469;&#30740;&#31350;Cobweb&#20316;&#20026;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#30340;&#32508;&#21512;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
&lt;/p&gt;</description></item><item><title>DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.03768</link><description>&lt;p&gt;
DeepCRE&#65306;&#21033;&#29992;&#23574;&#31471;&#35745;&#31639;&#27169;&#22411;&#25913;&#38761;&#33647;&#29289;&#30740;&#21457;
&lt;/p&gt;
&lt;p&gt;
DeepCRE: Revolutionizing Drug R&amp;D with Cutting-Edge Computational Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03768
&lt;/p&gt;
&lt;p&gt;
DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#21644;&#27835;&#30103;&#24212;&#29992;&#39046;&#22495;&#37117;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#27835;&#30103;&#39046;&#22495;&#38656;&#35201;&#26356;&#22810;&#30340;&#27835;&#30103;&#36873;&#25321;&#65292;&#21516;&#26102;&#22823;&#37327;&#26377;&#21069;&#26223;&#30340;&#20020;&#24202;&#21069;&#33647;&#29289;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#22833;&#36133;&#12290;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;&#33647;&#29289;&#24320;&#21457;&#30340;&#21518;&#26399;&#38454;&#27573;&#20132;&#21449;&#33647;&#29289;&#21453;&#24212;&#35780;&#20272;&#65288;CRE&#65289;&#30340;&#19981;&#36275;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;CRE&#27169;&#22411;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23398;&#35201;&#20040;&#23616;&#38480;&#20110;&#26089;&#26399;&#24320;&#21457;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#23545;&#20840;&#38754;CRE&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepCRE&#30340;&#26032;&#22411;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;DeepCRE&#22312;&#25512;&#21160;&#27835;&#30103;&#21457;&#29616;&#21644;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;DeepCRE&#36890;&#36807;&#23454;&#29616;&#24739;&#32773;&#32423;&#21035;CRE&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;17.7\%&#65292;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepCRE&#24050;&#32463;&#30830;&#23450;&#20102;&#20845;&#20010;&#26174;&#31034;&#20986;&#26126;&#26174;&#26356;&#22823;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23384;&#20648;&#22120; RC &#31995;&#32479;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340; memristor&#65292;&#24182;&#22312;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.01827</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#22522;&#20110;&#20840; memristor &#30340;&#26102;&#38388;&#25968;&#25454;&#20998;&#31867;&#30340;&#20648;&#23618;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23384;&#20648;&#22120; RC &#31995;&#32479;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340; memristor&#65292;&#24182;&#22312;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01827v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#20648;&#23618;&#35745;&#31639;&#65288;RC&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#26102;&#31354;&#20449;&#21495;&#30340;&#31070;&#32463;&#24418;&#24577;&#23398;&#26694;&#26550;&#12290;RC&#20197;&#20854;&#26102;&#38388;&#22788;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#19982;&#20256;&#32479;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#20854;&#30828;&#20214;&#37096;&#32626;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#33021;&#22815;&#29983;&#25104;&#21160;&#24577;&#20648;&#23618;&#29366;&#24577;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23384;&#20648;&#22120; RC &#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#19968;&#31181;&#22522;&#20110; WOx &#30340; memristor &#30340;&#30701;&#26399;&#23384;&#20648;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616; 16 &#20010;&#19981;&#21516;&#29366;&#24577;&#30340;&#32534;&#30721;&#36229;&#36807; 4 &#20010;&#27604;&#29305;&#65292;&#24182;&#22312;&#35835;&#20986;&#23618;&#20013;&#20351;&#29992; TiOx-based memristor &#30340;&#38271;&#26399;&#23384;&#20648;&#22120;&#32452;&#20214;&#12290;&#25105;&#20204;&#24443;&#24213;&#30740;&#31350;&#20102;&#20004;&#31181; memristor &#31867;&#22411;&#65292;&#24182;&#21033;&#29992; RC &#31995;&#32479;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340; RC &#31995;&#32479;&#30340;&#24615;&#33021;&#36890;&#36807;&#20004;&#20010;&#22522;&#20934;&#20219;&#21153;&#36827;&#34892;&#20102;&#39564;&#35777;: &#23545;&#20855;&#26377;&#19981;&#23436;&#25972;&#36755;&#20837;&#30340;&#23396;&#31435;&#21475;&#36848;&#25968;&#23383;&#35782;&#21035;&#21644; Mackey-Glass &#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340; 98.84% &#20934;&#30830;&#29575;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01827v1 Announce Type: cross  Abstract: Reservoir computing (RC) offers a neuromorphic framework that is particularly effective for processing spatiotemporal signals. Known for its temporal processing prowess, RC significantly lowers training costs compared to conventional recurrent neural networks. A key component in its hardware deployment is the ability to generate dynamic reservoir states. Our research introduces a novel dual-memory RC system, integrating a short-term memory via a WOx-based memristor, capable of achieving 16 distinct states encoded over 4 bits, and a long-term memory component using a TiOx-based memristor within the readout layer. We thoroughly examine both memristor types and leverage the RC system to process temporal data sets. The performance of the proposed RC system is validated through two benchmark tasks: isolated spoken digit recognition with incomplete inputs and Mackey-Glass time series prediction. The system delivered an impressive 98.84% accu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20302;&#21442;&#25968;&#35270;&#39057;&#27963;&#21160;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#33021;&#20934;&#30830;&#26816;&#27979;&#24182;&#20851;&#32852;&#23398;&#29983;&#22312;&#29616;&#23454;&#25945;&#23460;&#35270;&#39057;&#20013;&#25191;&#34892;&#30340;&#27963;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.01281</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#23398;&#20064;&#29615;&#22659;&#20013;&#24555;&#36895;&#20302;&#21442;&#25968;&#35270;&#39057;&#27963;&#21160;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Fast Low-parameter Video Activity Localization in Collaborative Learning Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20302;&#21442;&#25968;&#35270;&#39057;&#27963;&#21160;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#33021;&#20934;&#30830;&#26816;&#27979;&#24182;&#20851;&#32852;&#23398;&#29983;&#22312;&#29616;&#23454;&#25945;&#23460;&#35270;&#39057;&#20013;&#25191;&#34892;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#27963;&#21160;&#26816;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35782;&#21035;&#30701;&#35270;&#39057;&#29255;&#27573;&#20013;&#26126;&#30830;&#23450;&#20041;&#30340;&#20154;&#31867;&#27963;&#21160;&#12290;&#35270;&#39057;&#27963;&#21160;&#35782;&#21035;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#24320;&#21457;&#38656;&#35201;&#22312;&#22823;&#22411;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#21442;&#25968;&#31995;&#32479;&#19978;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20302;&#21442;&#25968;&#12289;&#27169;&#22359;&#21270;&#31995;&#32479;&#65292;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#23436;&#20840;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20174;&#22823;&#21442;&#25968;&#31995;&#32479;&#20013;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#20934;&#30830;&#26816;&#27979;&#21644;&#23558;&#29305;&#23450;&#27963;&#21160;&#19982;&#22312;&#29616;&#23454;&#25945;&#23460;&#35270;&#39057;&#20013;&#25191;&#34892;&#27963;&#21160;&#30340;&#23398;&#29983;&#20851;&#32852;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#22312;&#38271;&#26102;&#38388;&#30340;&#29616;&#23454;&#25945;&#23460;&#35270;&#39057;&#19978;&#21487;&#35270;&#21270;&#20154;&#31867;&#27963;&#21160;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01281v1 Announce Type: cross  Abstract: Research on video activity detection has primarily focused on identifying well-defined human activities in short video segments. The majority of the research on video activity recognition is focused on the development of large parameter systems that require training on large video datasets. This paper develops a low-parameter, modular system with rapid inferencing capabilities that can be trained entirely on limited datasets without requiring transfer learning from large-parameter systems. The system can accurately detect and associate specific activities with the students who perform the activities in real-life classroom videos. Additionally, the paper develops an interactive web-based application to visualize human activity maps over long real-life classroom videos.
&lt;/p&gt;</description></item><item><title>Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01232</link><description>&lt;p&gt;
Polynormer: &#22810;&#39033;&#24335;&#34920;&#36798;&#30340;&#32447;&#24615;&#26102;&#38388;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Polynormer: Polynomial-Expressive Graph Transformer in Linear Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01232
&lt;/p&gt;
&lt;p&gt;
Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36716;&#25442;&#22120;&#65288;GTs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26550;&#26500;&#65292;&#29702;&#35770;&#19978;&#23427;&#27604;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26356;&#20855;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;GT&#27169;&#22411;&#33267;&#23569;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#32447;&#24615;GTs&#65292;&#20294;&#23427;&#20204;&#22312;&#20960;&#20010;&#28909;&#38376;&#22270;&#25968;&#25454;&#38598;&#19978;&#20173;&#33853;&#21518;&#20110;GNN&#23545;&#24212;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#21147;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#20102;&#24179;&#34913;GTs&#30340;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Polynormer&#65292;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#12290;Polynormer&#26500;&#24314;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#36755;&#20837;&#29305;&#24449;&#19978;&#23398;&#20064;&#39640;&#27425;&#22810;&#39033;&#24335;&#12290;&#20026;&#20102;&#20351;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#20998;&#24320;&#38598;&#25104;&#65292;&#20174;&#32780;&#20135;&#29983;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#20851;&#27880;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;Polynormer&#37319;&#29992;&#20102;&#32447;&#24615;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#20851;&#27880;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01139</link><description>&lt;p&gt;
ParallelPARC: &#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31867;&#27604;&#30340;&#21487;&#25193;&#23637;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01139
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Analogy-making&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;--&#36825;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#30340;&#33021;&#21147;&#12290;&#22823;&#22810;&#25968;&#31867;&#27604;&#25968;&#25454;&#38598;&#20170;&#22825;&#20851;&#27880;&#31616;&#21333;&#30340;&#31867;&#27604;&#65288;&#20363;&#22914;&#65292;&#35789;&#31867;&#27604;&#65289;&#65307;&#21253;&#21547;&#22797;&#26434;&#31867;&#22411;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#24182;&#19988;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#31867;&#27604;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;ParallelPARC&#65288;Parallel Paragraph Creator&#65289;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21019;&#24314;&#22522;&#20110;&#27573;&#33853;&#30340;&#22797;&#26434;&#31867;&#27604;&#65292;&#20197;&#21450;&#31616;&#21333;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#65292;&#24182;&#21019;&#24314;&#20102;ProPara-Logy&#65292;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#36807;&#31243;&#38388;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#39564;&#35777;&#36807;&#30340;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#20108;&#36827;&#21046;&#21644;&#22810;&#36873;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;LLMs&#21644;&#20154;&#31867;&#23545;&#31867;&#27604;&#30340;&#35782;&#21035;&#65292;&#21457;&#29616;&#20154;&#31867;&#32988;&#36807;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.00781</link><description>&lt;p&gt;
ChatDiet&#65306;&#36890;&#36807;LLM&#22686;&#24378;&#26694;&#26550;&#36171;&#33021;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#23545;&#20581;&#24247;&#30340;&#28145;&#36828;&#24433;&#21709;&#20351;&#24471;&#20808;&#36827;&#30340;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#26381;&#21153;&#25104;&#20026;&#24517;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#31561;&#20851;&#38190;&#20803;&#32032;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#21333;&#29420;&#30340;&#20351;&#29992;&#26410;&#33021;&#23454;&#29616;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#39537;&#21160;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChatDiet&#38598;&#25104;&#20102;&#20010;&#20154;&#21644;&#20154;&#32676;&#27169;&#22411;&#65292;&#36741;&#20197;&#19968;&#20010;&#21327;&#35843;&#22120;&#65292;&#26080;&#32541;&#26816;&#32034;&#21644;&#22788;&#29702;&#30456;&#20851;&#20449;&#24687;&#12290;&#20854;&#32467;&#26524;&#26159;&#21160;&#24577;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#39135;&#21697;&#25512;&#33616;&#65292;&#26681;&#25454;&#20010;&#20154;&#29992;&#25143;&#21916;&#22909;&#23450;&#21046;&#12290;&#25105;&#20204;&#23545;ChatDiet&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20010;&#20154;&#27169;&#22411;&#26469;&#20272;&#35745;&#20010;&#20154;&#33829;&#20859;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26465;&#20214;&#35299;&#30721;&#22120;&#21644;NeRV-like&#27169;&#22359;&#30340;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#35299;&#30721;&#22120;&#22686;&#24378;&#35270;&#39057;&#30340;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Boosting Neural Representations for Videos with a Conditional Decoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18152
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26465;&#20214;&#35299;&#30721;&#22120;&#21644;NeRV-like&#27169;&#22359;&#30340;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#24050;&#32463;&#25104;&#20026;&#35270;&#39057;&#23384;&#20648;&#21644;&#22788;&#29702;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#35270;&#39057;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#26631;&#24103;&#35299;&#30721;&#36807;&#31243;&#20013;&#20013;&#38388;&#29305;&#24449;&#30340;&#19981;&#36275;&#23545;&#40784;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#34920;&#31034;&#33021;&#21147;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22686;&#24378;&#26694;&#26550;&#26469;&#21152;&#24378;&#24403;&#21069;&#30340;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#20223;&#23556;&#21464;&#25442;&#27169;&#22359;&#30340;&#26465;&#20214;&#35299;&#30721;&#22120;&#65292;&#35813;&#27169;&#22359;&#20351;&#29992;&#24103;&#32034;&#24341;&#20316;&#20026;&#20808;&#39564;&#26465;&#20214;&#65292;&#26377;&#25928;&#22320;&#23558;&#20013;&#38388;&#29305;&#24449;&#19982;&#30446;&#26631;&#24103;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24358;NeRV-like&#27169;&#22359;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#21442;&#25968;&#20998;&#24067;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#20511;&#21161;&#39640;&#39057;&#20449;&#24687;&#20445;&#30041;&#30340;&#37325;&#26500;&#25439;&#22833;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22686;&#24378;&#20102;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18152v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#39318;&#27425;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.17736</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Algorithms for Graph Searching Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#39318;&#27425;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;Banerjee&#31561;&#20154;&#65288;2022&#24180;&#65289;&#26368;&#36817;&#25552;&#20986;&#30340;&#20855;&#26377;&#39044;&#27979;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20174;&#26576;&#20010;&#39030;&#28857;$r$&#20986;&#21457;&#30340;&#20195;&#29702;&#32773;&#24517;&#39035;&#22312;&#26368;&#23567;&#21270;&#24635;&#34892;&#31243;&#30340;&#21516;&#26102;&#36941;&#21382;&#19968;&#20010;&#65288;&#28508;&#22312;&#26410;&#30693;&#30340;&#65289;&#22270;$G$&#20197;&#25214;&#21040;&#38544;&#34255;&#30340;&#30446;&#26631;&#33410;&#28857;$g$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#22312;&#20219;&#24847;&#33410;&#28857;$v$&#22788;&#65292;&#20195;&#29702;&#32773;&#20250;&#25509;&#25910;&#21040;&#21040;$g$&#30340;&#36317;&#31163;&#30340;&#22122;&#22768;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#36825;&#31181;&#25628;&#32034;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#31532;&#19968;&#27425;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#38500;&#20102;&#23545;&#25239;&#24615;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#22806;&#65292;&#36824;&#22312;&#35823;&#24046;&#26159;&#38543;&#26426;&#30340;&#20856;&#22411;&#23454;&#20363;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;Banerjee&#31561;&#20154;&#31639;&#27861;&#30340;&#26367;&#20195;&#31616;&#21270;&#24615;&#33021;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17736v1 Announce Type: cross  Abstract: We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26102;&#65292;&#26159;&#21542;&#33021;&#22815;&#22797;&#29616;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;</title><link>https://arxiv.org/abs/2402.17527</link><description>&lt;p&gt;
&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#65306;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;_____
&lt;/p&gt;
&lt;p&gt;
Predict the Next Word: &lt;Humans exhibit uncertainty in this task and language models _____&gt;
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17527
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26102;&#65292;&#26159;&#21542;&#33021;&#22815;&#22797;&#29616;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#35757;&#32451;&#29992;&#20110;&#20026;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#20998;&#37197;&#27010;&#29575;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#36136;&#30097;&#23427;&#20204;&#26159;&#21542;&#24456;&#22909;&#22320;&#36817;&#20284;&#20154;&#31867;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;&#12290;&#36825;&#31181;&#24418;&#24335;&#30340;&#32479;&#35745;&#35780;&#20272;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#24456;&#38590;&#25191;&#34892;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#65288;&#21363;&#65292;&#20154;&#31867;&#35780;&#20272;&#65289;&#25110;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#21160;&#20195;&#29702;&#65288;&#36825;&#26159;&#19981;&#24179;&#20961;&#30340;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#65292;&#36890;&#36807;&#32473;&#23450;&#19968;&#20123;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#19968;&#20010;&#39044;&#20808;&#35760;&#24405;&#30340;&#26367;&#20195;&#21333;&#35789;&#36830;&#32493;&#25968;&#25454;&#38598;&#30340;&#31934;&#30830;&#21305;&#37197;&#26469;&#35780;&#20272;LM&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20107;&#23454;&#65292;&#24182;&#35780;&#20272;LM&#37325;&#26032;&#29983;&#25104;&#20154;&#31867;&#65288;&#29305;&#21035;&#26159;&#19968;&#32676;&#33521;&#35821;&#20351;&#29992;&#32773;&#65289;&#22312;&#8220;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#8221;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#26657;&#20934;&#35780;&#20272;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65292;Baan&#31561;&#20154;&#65288;2022&#24180;&#65289;&#23558;&#20854;&#31216;&#20026;&#23545;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17527v1 Announce Type: cross  Abstract: Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertaint
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#21333;&#24212;&#24615;&#20272;&#35745;&#32593;&#32476;&#65292;&#29992;&#20110;&#24555;&#36895;&#21644;&#21487;&#23398;&#20064;&#30340;&#20960;&#20309;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.16086</link><description>&lt;p&gt;
&#29992;&#20110;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#28145;&#24230;&#21333;&#24212;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Homography Estimation for Visual Place Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16086
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#21333;&#24212;&#24615;&#20272;&#35745;&#32593;&#32476;&#65292;&#29992;&#20110;&#24555;&#36895;&#21644;&#21487;&#23398;&#20064;&#30340;&#20960;&#20309;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;(VPR)&#26159;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22914;&#26426;&#22120;&#20154;&#23450;&#20301;&#21644;&#22686;&#24378;&#29616;&#23454;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20998;&#23618;VPR&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#20204;&#36890;&#24120;&#39318;&#20808;&#20351;&#29992;&#20840;&#23616;&#29305;&#24449;&#26469;&#26816;&#32034;&#20505;&#36873;&#22270;&#20687;&#65292;&#28982;&#21518;&#39564;&#35777;&#21305;&#37197;&#30340;&#23616;&#37096;&#29305;&#24449;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#20197;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#21518;&#32773;&#36890;&#24120;&#20381;&#36182;&#20110;RANSAC&#31639;&#27861;&#36827;&#34892;&#21333;&#24212;&#24615;&#25311;&#21512;&#65292;&#36825;&#26159;&#32791;&#26102;&#19988;&#19981;&#21487;&#24494;&#20998;&#30340;&#12290;&#36825;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#22312;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#20013;&#35757;&#32451;&#32593;&#32476;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#21333;&#24212;&#24615;&#20272;&#35745;(DHE)&#32593;&#32476;&#65292;&#20854;&#20197;&#30001;&#20027;&#24178;&#32593;&#32476;&#25552;&#21462;&#30340;&#23494;&#38598;&#29305;&#24449;&#22270;&#20026;&#36755;&#20837;&#65292;&#24182;&#36866;&#21512;&#20110;&#24555;&#36895;&#21644;&#21487;&#23398;&#20064;&#30340;&#20960;&#20309;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20869;&#28857;&#37325;&#25237;&#24433;&#35823;&#24046;&#25439;&#22833;&#26469;&#35757;&#32451;DHE&#32593;&#32476;&#65292;&#26080;&#38656;&#28155;&#21152;&#39069;&#22806;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16086v1 Announce Type: cross  Abstract: Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable. This makes existing methods compromise to train the network only in global feature extraction. Here, we propose a transformer-based deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification. Moreover, we design a re-projection error of inliers loss to train the DHE network without addit
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;OpenSUN3D&#30740;&#35752;&#20250;&#19978;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#21253;&#25324;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;</title><link>https://arxiv.org/abs/2402.15321</link><description>&lt;p&gt;
OpenSUN3D: &#24320;&#25918;&#35789;&#27719;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#31532;&#19968;&#27425;&#30740;&#35752;&#20250;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15321
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;OpenSUN3D&#30740;&#35752;&#20250;&#19978;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#21253;&#25324;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#27010;&#36848;&#20102;&#22312;2023&#24180;ICCV&#20250;&#35758;&#19978;&#20030;&#21150;&#30340;OpenSUN3D Workshop&#20851;&#20110;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#35752;&#20250;&#31995;&#21015;&#30340;&#30446;&#26631;&#26159;&#20026;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#25552;&#20379;&#25506;&#32034;&#21644;&#35752;&#35770;&#24179;&#21488;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#20998;&#21106;&#12289;&#26816;&#27979;&#21644;&#26144;&#23556;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30740;&#35752;&#20250;&#19978;&#20030;&#21150;&#30340;&#25361;&#25112;&#27010;&#36848;&#65292;&#23637;&#31034;&#20102;&#25361;&#25112;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#20197;&#21450;&#33719;&#32988;&#26041;&#27861;&#30340;&#31616;&#35201;&#25551;&#36848;&#12290;&#26356;&#22810;&#35814;&#24773;&#35831;&#21442;&#38405;https://opensun3d.github.io/index_iccv23.html&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15321v1 Announce Type: cross  Abstract: This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.14899</link><description>&lt;p&gt;
&#20572;&#27490;&#25512;&#29702;&#65281;&#24403;&#22810;&#27169;&#24577;LLMs&#19982;&#20018;&#32852;&#25512;&#29702;&#36935;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#29702;&#35299;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20687;&#20256;&#32479;&#35270;&#35273;&#27169;&#22411;&#19968;&#26679;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;&#30340;&#25915;&#20987;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20018;&#32852;&#25512;&#29702;&#65288;CoT&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;MLLMs&#19978;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#25552;&#20379;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#20851;&#20110;MLLMs&#22312;CoT&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;MLLMs&#29992;&#23545;&#25239;&#24615;&#22270;&#20687;&#25512;&#26029;&#38169;&#35823;&#31572;&#26696;&#26102;&#25512;&#29702;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#37319;&#29992;CoT&#25512;&#29702;&#26102;MLLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;CoT&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25269;&#25239;&#20102;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;CoT&#24341;&#36215;&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoT&#25512;&#29702;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#26080;&#32541;&#36866;&#24212;</title><link>https://arxiv.org/abs/2402.14505</link><description>&lt;p&gt;
&#20026;&#23454;&#29616;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26080;&#32541;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#26080;&#32541;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#29992;&#36890;&#29992;&#30340;&#35270;&#35273;&#23398;&#20064;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#21487;&#20197;&#20026;&#21508;&#31181;&#35270;&#35273;&#24863;&#30693;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#23581;&#35797;&#21033;&#29992;&#22312;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#65288;VPR&#65289;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#30001;&#20110;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;VPR&#20219;&#21153;&#20043;&#38388;&#22312;&#35757;&#32451;&#30446;&#26631;&#21644;&#25968;&#25454;&#26041;&#38754;&#30340;&#22266;&#26377;&#24046;&#24322;&#65292;&#22914;&#20309;&#24357;&#21512;&#24046;&#36317;&#24182;&#20805;&#20998;&#21457;&#25381;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;VPR&#20013;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;VPR&#30340;&#26080;&#32541;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28151;&#21512;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#23616;&#21644;&#23616;&#37096;&#36866;&#24212;&#65292;&#20174;&#32780;&#33719;&#24471;&#26082;&#20851;&#27880;&#26174;&#33879;&#22320;&#26631;&#29992;&#20110;&#21306;&#20998;&#22320;&#28857;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14505v1 Announce Type: cross  Abstract: Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to gu
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#22270;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#24515;&#29702;&#23398;&#20551;&#35774;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#32852;&#21512;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.14424</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#24515;&#29702;&#23398;&#20551;&#35774;&#29983;&#25104;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14424
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#22270;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#24515;&#29702;&#23398;&#20551;&#35774;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#32852;&#21512;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#29983;&#25104;&#24515;&#29702;&#23398;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;LLM&#20998;&#26512;&#20102;43,312&#31687;&#24515;&#29702;&#23398;&#25991;&#31456;&#65292;&#25552;&#21462;&#20102;&#22240;&#26524;&#20851;&#31995;&#23545;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#24515;&#29702;&#23398;&#30340;&#22240;&#26524;&#22270;&#12290;&#24212;&#29992;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;130&#20010;&#20851;&#27880;&#8220;&#24184;&#31119;&#8221;&#30340;&#28508;&#22312;&#24515;&#29702;&#23398;&#20551;&#35774;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21338;&#22763;&#23398;&#32773;&#26500;&#24605;&#30340;&#30740;&#31350;&#24819;&#27861;&#21644;&#20165;&#30001;LLM&#20135;&#29983;&#30340;&#24819;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;LLM&#21644;&#22240;&#26524;&#22270;&#30340;&#32852;&#21512;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#19982;&#19987;&#23478;&#27700;&#24179;&#30340;&#27934;&#23519;&#21147;&#20445;&#25345;&#19968;&#33268;&#65292;&#26126;&#26174;&#20248;&#20110;&#20165;LLM&#30340;&#20551;&#35774;&#65288;&#20998;&#21035;&#20026;t(59)=3.34&#65292;p=0.007&#21644;t(59)=4.32&#65292;p&lt;0.001&#65289;&#12290;&#36825;&#31181;&#19968;&#33268;&#24615;&#36827;&#19968;&#27493;&#36890;&#36807;&#28145;&#24230;&#35821;&#20041;&#20998;&#26512;&#24471;&#21040;&#35777;&#23454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;LLM&#19982;&#22240;&#26524;&#22270;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#24515;&#29702;&#23398;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14424v1 Announce Type: new  Abstract: Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs. This analysis produced a specialized causal graph for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on `well-being', then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM. Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) = 4.32, p&lt;0.001, respectively). This alignment was further corroborated using deep semantic analysis. Our results show that combining LLM with machine learning techniques such as causal k
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13602</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25512;&#29702;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13602
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12289;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#20197;&#21450;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#36825;&#31181;&#39640;&#32423;&#25512;&#29702;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30456;&#32467;&#21512;&#20197;&#29992;&#20110;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#20915;&#31574;&#30340;&#27867;&#21270;&#33021;&#21147;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#28151;&#21512;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#24212;&#29992;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#20551;&#35774;LLMs&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20351;&#23427;&#20204;&#20998;&#26512;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#29702;&#35299;&#39550;&#39542;&#35268;&#23450;&#21644;&#29289;&#29702;&#27861;&#21017;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#35821;&#22659;&#26469;&#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#12290;&#36825;&#35299;&#20915;&#20102;&#22797;&#26434;&#24773;&#26223;&#65292;&#22914;&#20302;&#33021;&#35265;&#24230;&#65288;&#30001;&#20110;&#22825;&#27668;&#26465;&#20214;&#65289;&#19979;&#30340;&#20915;&#31574;&#65292;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#19981;&#36275;&#20197;&#32988;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20934;&#30830;&#24615;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13602v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CiMNet&#65292;&#19968;&#20010;&#26088;&#22312;&#32852;&#21512;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#37197;&#32622;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#23384;&#20648;&#30828;&#20214;&#26500;&#24314;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11780</link><description>&lt;p&gt;
&#38754;&#21521;&#35745;&#31639;&#23384;&#20648;&#30828;&#20214;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#37197;&#32622;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CiMNet&#65292;&#19968;&#20010;&#26088;&#22312;&#32852;&#21512;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#37197;&#32622;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#23384;&#20648;&#30828;&#20214;&#26500;&#24314;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#27714;&#30340;&#22686;&#38271;&#65292;&#35745;&#31639;&#23384;&#20648;&#65288;CiM&#65289;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#32531;&#35299;&#32422;&#26463;&#20911;&#183;&#35834;&#20381;&#26364;&#20307;&#31995;&#32467;&#26500;&#30340;&#24102;&#23485;&#21644;&#33455;&#29255;&#20869;&#37096;&#36830;&#25509;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;CiM&#30828;&#20214;&#30340;&#26500;&#24314;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#19981;&#21516;&#25509;&#21475;&#30340;&#32531;&#23384;&#22823;&#23567;&#21644;&#20869;&#23384;&#24102;&#23485;&#26041;&#38754;&#30340;&#20219;&#20309;&#29305;&#23450;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#21487;&#33021;&#19981;&#29702;&#24819;&#22320;&#21305;&#37197;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#23646;&#24615;&#65292;&#22914;&#24352;&#37327;&#32500;&#24230;&#21644;&#31639;&#26415;&#24378;&#24230;&#65292;&#22240;&#27492;&#23548;&#33268;&#27425;&#20248;&#21644;&#34920;&#29616;&#19981;&#20339;&#30340;&#31995;&#32479;&#12290; &#23613;&#31649;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25216;&#26415;&#22312;&#20026;&#32473;&#23450;&#30828;&#20214;&#24230;&#37327;&#39044;&#31639;&#65288;&#20363;&#22914;DNN&#25191;&#34892;&#26102;&#38388;&#25110;&#24310;&#36831;&#65289;&#20135;&#29983;&#39640;&#25928;&#23376;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#23427;&#20551;&#35774;&#30828;&#20214;&#37197;&#32622;&#34987;&#20923;&#32467;&#65292;&#36890;&#24120;&#20026;&#32473;&#23450;&#39044;&#31639;&#20135;&#29983;&#27425;&#20248;&#23376;&#32593;&#32476;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;CiMNet&#65292;&#19968;&#20010;&#32852;&#21512;&#25628;&#32034;&#26368;&#20248;&#23376;&#32593;&#32476;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11780v1 Announce Type: cross  Abstract: With the recent growth in demand for large-scale deep neural networks, compute in-memory (CiM) has come up as a prominent solution to alleviate bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman architectures. However, the construction of CiM hardware poses a challenge as any specific memory hierarchy in terms of cache sizes and memory bandwidth at different interfaces may not be ideally matched to any neural network's attributes such as tensor dimension and arithmetic intensity, thus leading to suboptimal and under-performing systems. Despite the success of neural architecture search (NAS) techniques in yielding efficient sub-networks for a given hardware metric budget (e.g., DNN execution time or latency), it assumes the hardware configuration to be frozen, often yielding sub-optimal sub-networks for a given budget. In this paper, we present CiMNet, a framework that jointly searches for optimal sub-networks and 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#20027;&#35201;&#29992;&#20110;&#23637;&#31034;&#27010;&#24565;&#25110;&#25552;&#20379;&#31034;&#20363;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#25165;&#33021;&#23454;&#29616;&#29983;&#20135;&#23601;&#32490;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.11702</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#25903;&#25345;&#24320;&#21457;&#32773;&#65311;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11702
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#20027;&#35201;&#29992;&#20110;&#23637;&#31034;&#27010;&#24565;&#25110;&#25552;&#20379;&#31034;&#20363;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#25165;&#33021;&#23454;&#29616;&#29983;&#20135;&#23601;&#32490;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#23427;&#20204;&#22312;&#21508;&#31181;&#24320;&#21457;&#22330;&#26223;&#20013;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#25552;&#20379;&#20102;&#22312;&#30740;&#31350;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#65292;&#36825;&#22312;&#29702;&#35299;LLM&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#33021;&#26377;&#25928;&#25903;&#25345;&#24320;&#21457;&#32773;&#26041;&#38754;&#30041;&#19979;&#20102;&#26174;&#33879;&#30340;&#31354;&#30333;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;DevGPT&#20013;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36825;&#26159;&#20174;&#24320;&#21457;&#32773;&#19982;ChatGPT&#30340;&#23545;&#35805;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65288;&#36890;&#36807;GitHub&#31561;&#24179;&#21488;&#19978;&#30340;Share Link&#21151;&#33021;&#25429;&#33719;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23454;&#36341;&#36890;&#24120;&#20165;&#38480;&#20110;&#23637;&#31034;&#39640;&#23618;&#27010;&#24565;&#25110;&#25552;&#20379;&#25991;&#26723;&#20013;&#30340;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#21487;&#29992;&#20110;&#29983;&#20135;&#30340;&#20195;&#30721;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;LLM&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#36824;&#38656;&#35201;&#22823;&#37327;&#26410;&#26469;&#24037;&#20316;&#25165;&#33021;&#20351;&#20854;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11702v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts o
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09900</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Recurrent Reinforcement Learning with Memory Monoids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20687;RNN&#21644;transformers&#36825;&#26679;&#30340;&#35760;&#24518;&#27169;&#22411;&#36890;&#36807;&#23558;&#36712;&#36857;&#26144;&#23556;&#21040;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#26469;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38271;&#24207;&#21015;&#30340;&#35268;&#27169;&#21270;&#22788;&#29702;&#33021;&#21147;&#24182;&#19981;&#29305;&#21035;&#22909;&#65292;&#23588;&#20854;&#26159;&#19982;&#19968;&#31867;&#26032;&#20852;&#30340;&#35760;&#24518;&#27169;&#22411;&#65288;&#26377;&#26102;&#31216;&#20026;&#32447;&#24615;&#24490;&#29615;&#27169;&#22411;&#65289;&#30456;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#24490;&#29615;&#26356;&#26032;&#26159;&#19968;&#20010;&#21333;&#23376;&#65292;&#22240;&#27492;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20256;&#32479;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#21033;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#22686;&#21152;&#20102;&#22238;&#25253;&#65292;&#24182;&#31616;&#21270;&#20102;&#24490;&#29615;&#20002;&#22833;&#20989;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09900v1 Announce Type: cross  Abstract: In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.05808</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#24182;&#24314;&#31435;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#32467;&#26524;&#30417;&#30563;&#21644;&#36807;&#31243;&#30417;&#30563;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;R$^3$&#65306;&#36890;&#36807;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#25512;&#29702;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21482;&#20351;&#29992;&#32467;&#26524;&#30417;&#30563;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#23558;RL&#24212;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#30830;&#23450;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#20197;&#33719;&#24471;&#27491;&#21521;&#22870;&#21169;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#20248;&#21270;&#30417;&#30563;&#12290;&#32467;&#26524;&#30417;&#30563;&#20026;&#26368;&#32456;&#32467;&#26524;&#25552;&#20379;&#20102;&#31232;&#30095;&#22870;&#21169;&#65292;&#32780;&#19981;&#35782;&#21035;&#38169;&#35823;&#20301;&#32622;&#65292;&#32780;&#36807;&#31243;&#30417;&#30563;&#25552;&#20379;&#20102;&#36880;&#27493;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#12290;R$^3$&#36890;&#36807;&#23398;&#20064;&#27491;&#30830;&#28436;&#31034;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;R$^3$&#23558;&#25512;&#29702;&#30340;&#36215;&#22987;&#29366;&#24577;&#20174;&#28436;&#31034;&#30340;&#32467;&#26463;&#28369;&#21160;&#21040;&#24320;&#22987;&#65292;&#20174;&#32780;&#22312;&#25152;&#26377;&#38454;&#27573;&#37117;&#20419;&#36827;&#20102;&#26356;&#23481;&#26131;&#30340;&#27169;&#22411;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;R$^3$&#24314;&#31435;&#20102;&#19968;&#20010;&#36880;&#27493;&#30340;&#35838;&#31243;&#65292;&#20351;&#32467;&#26524;&#30417;&#30563;&#33021;&#22815;&#25552;&#20379;&#38454;&#27573;&#32423;&#20449;&#21495;&#24182;&#31934;&#30830;&#23450;&#20301;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05713</link><description>&lt;p&gt;
&#26126;&#26126;&#23601;&#22312;&#30524;&#21069;&#65306;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#36827;&#34892;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#21095;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#30340;&#20020;&#24202;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#20559;&#35265;&#30340;&#37327;&#21270;&#65292;&#20294;&#38024;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#20197;&#21450;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38024;&#23545;&#20154;&#21475;&#32479;&#35745;&#23398;&#26631;&#31614;&#30340;&#27602;&#21270;&#25915;&#20987;&#21487;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#23545;&#34987;&#20302;&#20272;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#21644;&#20154;&#21475;&#32676;&#20307;&#65288;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#20197;&#21450;&#20854;&#20132;&#21449;&#23376;&#32676;&#65289;&#19978;&#34920;&#26126;&#65292;&#32676;&#20307;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#19982;&#20854;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#24449;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
&lt;/p&gt;</description></item><item><title>S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.04578</link><description>&lt;p&gt;
S-Agents: &#33258;&#32452;&#32455;&#20195;&#29702;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
S-Agents: self-organizing agents in open-ended environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04578
&lt;/p&gt;
&lt;p&gt;
S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#65292;&#20855;&#22791;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#20248;&#21270;&#21327;&#20316;&#38656;&#35201;&#28789;&#27963;&#30340;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#22266;&#23450;&#30340;&#12289;&#20219;&#21153;&#23548;&#21521;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#24573;&#35270;&#20102;&#20197;&#20195;&#29702;&#20026;&#20013;&#24515;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;&#21463;&#20154;&#31867;&#32452;&#32455;&#34892;&#20026;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65288;S-Agents&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#21160;&#24577;&#24037;&#20316;&#27969;&#31243;&#30340;&#8220;&#20195;&#29702;&#26641;&#8221;&#32467;&#26500;&#12289;&#24179;&#34913;&#20449;&#24687;&#20248;&#20808;&#32423;&#30340;&#8220;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#8221;&#20197;&#21450;&#20801;&#35768;&#20195;&#29702;&#20043;&#38388;&#24322;&#27493;&#25191;&#34892;&#20219;&#21153;&#30340;&#8220;&#38750;&#38459;&#22622;&#21327;&#20316;&#8221;&#26041;&#27861;&#12290;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#33258;&#20027;&#21327;&#35843;&#19968;&#32452;&#20195;&#29702;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#26080;&#38480;&#19988;&#21160;&#24577;&#30340;&#29615;&#22659;&#25361;&#25112;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;S-Agents&#33021;&#22815;&#29087;&#32451;&#22320;&#25191;&#34892;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#21644;&#36164;&#28304;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a "tree of agents" structure for dynamic workflow, an "hourglass agent architecture" for balancing information priorities, and a "non-obstructive collaboration" method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without human intervention. Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection i
&lt;/p&gt;</description></item><item><title>LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02544</link><description>&lt;p&gt;
LHRS-Bot&#65306;&#21033;&#29992;VGI&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36965;&#24863;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02544
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38761;&#21629;&#24615;&#33021;&#21147;&#24320;&#21019;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24182;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#39046;&#22495;&#20013;&#65292;&#36817;&#26399;&#30340;MLLM&#21162;&#21147;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#36965;&#24863;&#22270;&#20687;&#20013;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LHRS-Align&#65292;&#20197;&#21450;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;RS&#29305;&#23450;&#25351;&#23548;&#25968;&#25454;&#38598;LHRS-Instruct&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#21644;&#20840;&#29699;&#21487;&#29992;&#30340;RS&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LHRS-Bot&#65292;&#19968;&#31181;&#38024;&#23545;RS&#22270;&#20687;&#29702;&#35299;&#30340;MLLM&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
&lt;/p&gt;</description></item><item><title>LeTO &#26159;&#19968;&#31181;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#23454;&#29616;&#21463;&#38480;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#20248;&#21270;&#23618;&#34920;&#31034;&#20026;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#20197;&#23433;&#20840;&#21487;&#25511;&#30340;&#26041;&#24335;&#31471;&#21040;&#31471;&#29983;&#25104;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#32422;&#26463;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#24179;&#34913;&#28385;&#36275;&#32422;&#26463;&#12289;&#24179;&#28369;&#36712;&#36857;&#21644;&#26368;&#23567;&#21270;&#28436;&#31034;&#35823;&#24046;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#26126;LeTO&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2401.17500</link><description>&lt;p&gt;
LeTO&#65306;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#23398;&#20064;&#21463;&#38480;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
LeTO: Learning Constrained Visuomotor Policy with Differentiable Trajectory Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17500
&lt;/p&gt;
&lt;p&gt;
LeTO &#26159;&#19968;&#31181;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#23454;&#29616;&#21463;&#38480;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#20248;&#21270;&#23618;&#34920;&#31034;&#20026;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#20197;&#23433;&#20840;&#21487;&#25511;&#30340;&#26041;&#24335;&#31471;&#21040;&#31471;&#29983;&#25104;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#32422;&#26463;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#24179;&#34913;&#28385;&#36275;&#32422;&#26463;&#12289;&#24179;&#28369;&#36712;&#36857;&#21644;&#26368;&#23567;&#21270;&#28436;&#31034;&#35823;&#24046;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#26126;LeTO&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LeTO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#23454;&#29616;&#21463;&#38480;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23558;&#19968;&#20010;&#21487;&#24494;&#20998;&#20248;&#21270;&#23618;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#23558;&#20248;&#21270;&#23618;&#34920;&#31034;&#20026;&#19968;&#20010;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#27169;&#22411;&#20197;&#23433;&#20840;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#31471;&#21040;&#31471;&#29983;&#25104;&#21160;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#32422;&#26463;&#20449;&#24687;&#65292;&#20174;&#32780;&#24179;&#34913;&#28385;&#36275;&#32422;&#26463;&#12289;&#24179;&#28369;&#36712;&#36857;&#21644;&#26368;&#23567;&#21270;&#28436;&#31034;&#35823;&#24046;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#8220;&#28784;&#30418;&#8221;&#26041;&#27861;&#23558;&#22522;&#20110;&#20248;&#21270;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#34920;&#36798;&#33021;&#21147;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#23545;LeTO&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#22312;&#20223;&#30495;&#20013;&#65292;LeTO&#30340;&#25104;&#21151;&#29575;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#65292;&#20294;&#29983;&#25104;&#30340;&#36712;&#36857;&#30340;&#19981;&#19968;&#33268;&#24615;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces LeTO, a method for learning constrained visuomotor policy via differentiable trajectory optimization. Our approach uniquely integrates a differentiable optimization layer into the neural network. By formulating the optimization layer as a trajectory optimization problem, we enable the model to end-to-end generate actions in a safe and controlled fashion without extra modules. Our method allows for the introduction of constraints information during the training process, thereby balancing the training objectives of satisfying constraints, smoothing the trajectories, and minimizing errors with demonstrations. This "gray box" method marries the optimization-based safety and interpretability with the powerful representational abilities of neural networks. We quantitatively evaluate LeTO in simulation and on the real robot. In simulation, LeTO achieves a success rate comparable to state-of-the-art imitation learning methods, but the generated trajectories are of less un
&lt;/p&gt;</description></item><item><title>CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2401.11944</link><description>&lt;p&gt;
CMMMU&#65306;&#19968;&#20010;&#20013;&#22269;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11944
&lt;/p&gt;
&lt;p&gt;
CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#35780;&#20272;LMMs&#30340;&#34920;&#29616;&#26085;&#30410;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;LMMs&#22312;&#20013;&#25991;&#31561;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26356;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CMMMU&#65292;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LMMs&#22312;&#38656;&#35201;&#22823;&#23398;&#27700;&#24179;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;CMMMU&#21463;&#21040;&#20102;MMMUs&#30340;&#26631;&#27880;&#21644;&#20998;&#26512;&#27169;&#24335;&#30340;&#21551;&#21457;&#24182;&#20005;&#26684;&#36981;&#24490;&#12290;CMMMU&#21253;&#25324;&#26469;&#33258;&#22823;&#23398;&#32771;&#35797;&#12289;&#27979;&#39564;&#21644;&#25945;&#31185;&#20070;&#30340;1.2&#19975;&#20010;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#28085;&#30422;&#20845;&#20010;&#26680;&#24515;&#23398;&#31185;&#65306;&#33402;&#26415;&#19982;&#35774;&#35745;&#12289;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#20581;&#24247;&#19982;&#21307;&#23398;&#12289;&#20154;&#25991;&#31038;&#31185;&#20197;&#21450;&#25216;&#26415;&#19982;&#24037;&#31243;&#65292;&#23601;&#20687;&#20854;&#20249;&#20276;MMMMU&#19968;&#26679;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;30&#20010;&#23398;&#31185;&#65292;&#21253;&#25324;39&#20010;&#39640;&#24230;&#24322;&#36136;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp; Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
&lt;/p&gt;</description></item><item><title>FaceTalk&#26159;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#21442;&#25968;&#22836;&#37096;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#20174;&#36755;&#20837;&#38899;&#39057;&#20449;&#21495;&#20013;&#21512;&#25104;&#39640;&#20445;&#30495;3D&#36816;&#21160;&#24207;&#21015;&#65292;&#26159;&#39318;&#20010;&#25552;&#20986;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#21512;&#25104;&#36924;&#30495;&#21644;&#39640;&#36136;&#37327;&#36816;&#21160;&#24207;&#21015;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2312.08459</link><description>&lt;p&gt;
FaceTalk: &#38754;&#37096;&#29305;&#24449;&#39537;&#21160;&#30340;&#31070;&#32463;&#21442;&#25968;&#22836;&#37096;&#27169;&#22411;&#30340;&#36816;&#21160;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08459
&lt;/p&gt;
&lt;p&gt;
FaceTalk&#26159;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#21442;&#25968;&#22836;&#37096;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#20174;&#36755;&#20837;&#38899;&#39057;&#20449;&#21495;&#20013;&#21512;&#25104;&#39640;&#20445;&#30495;3D&#36816;&#21160;&#24207;&#21015;&#65292;&#26159;&#39318;&#20010;&#25552;&#20986;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#21512;&#25104;&#36924;&#30495;&#21644;&#39640;&#36136;&#37327;&#36816;&#21160;&#24207;&#21015;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;FaceTalk&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#36755;&#20837;&#38899;&#39057;&#20449;&#21495;&#21512;&#25104;&#35828;&#35805;&#20154;&#22836;&#37096;&#30340;&#39640;&#20445;&#30495;3D&#36816;&#21160;&#24207;&#21015;&#12290;&#20026;&#20102;&#25429;&#25417;&#20154;&#22836;&#37096;&#30340;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#32454;&#33410;&#24615;&#36136;&#65292;&#21253;&#25324;&#22836;&#21457;&#12289;&#32819;&#26421;&#21644;&#26356;&#32454;&#24494;&#30340;&#30524;&#30555;&#36816;&#21160;&#65292;&#25105;&#20204;&#25552;&#35758;&#23558;&#35821;&#38899;&#20449;&#21495;&#19982;&#31070;&#32463;&#21442;&#25968;&#22836;&#37096;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#30456;&#32467;&#21512;&#65292;&#20197;&#21019;&#24314;&#39640;&#20445;&#30495;&#12289;&#26102;&#38388;&#36830;&#36143;&#30340;&#36816;&#21160;&#24207;&#21015;&#12290;&#25105;&#20204;&#20026;&#36825;&#19968;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#22312;&#31070;&#32463;&#21442;&#25968;&#22836;&#37096;&#27169;&#22411;&#30340;&#34920;&#36798;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#20197;&#21512;&#25104;&#21463;&#38899;&#39057;&#39537;&#21160;&#30340;&#36924;&#30495;&#22836;&#37096;&#24207;&#21015;&#12290;&#37492;&#20110;&#32570;&#20047;&#20855;&#26377;&#30456;&#24212;NPHM&#34920;&#36798;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20248;&#21270;&#36825;&#20123;&#23545;&#24212;&#20851;&#31995;&#65292;&#20197;&#29983;&#25104;&#36866;&#21512;&#38899;&#35270;&#39057;&#35760;&#24405;&#20013;&#35828;&#35805;&#20154;&#30340;&#26102;&#38388;&#20248;&#21270;NPHM&#34920;&#36798;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#25552;&#20986;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#19988;&#39640;&#36136;&#37327;&#36816;&#21160;&#24207;&#21015;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08459v2 Announce Type: replace-cross  Abstract: We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#24320;&#21457;&#30340;DevAssistLlama&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36719;&#20214;&#30456;&#20851;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26102;&#34920;&#29616;&#20986;&#20248;&#24322;&#33021;&#21147;&#65292;&#31361;&#20986;&#20102;&#19987;&#38376;LLM&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.05626</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#23450;&#20041;&#24320;&#21457;&#32773;&#25588;&#21161;&#65306;&#22312;&#36719;&#20214;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Redefining Developer Assistance: Through Large Language Models in Software Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#24320;&#21457;&#30340;DevAssistLlama&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36719;&#20214;&#30456;&#20851;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26102;&#34920;&#29616;&#20986;&#20248;&#24322;&#33021;&#21147;&#65292;&#31361;&#20986;&#20102;&#19987;&#38376;LLM&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;DevAssistLlama&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#24320;&#21457;&#30340;&#27169;&#22411;&#65292;&#21487;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#22788;&#29702;&#19982;&#36719;&#20214;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#12290;&#36825;&#20010;&#27169;&#22411;&#65292;&#20316;&#20026;&#25351;&#23548;&#35843;&#25972;&#30340;LLM&#21464;&#20307;&#65292;&#29305;&#21035;&#25797;&#38271;&#22788;&#29702;&#22797;&#26434;&#30340;&#25216;&#26415;&#25991;&#26723;&#65292;&#22686;&#24378;&#20102;&#24320;&#21457;&#20154;&#21592;&#22312;&#36719;&#20214;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;DevAssistLlama&#30340;&#21019;&#24314;&#28041;&#21450;&#20174;&#21508;&#31181;&#36719;&#20214;&#31995;&#32479;&#26500;&#24314;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26377;&#25928;&#22788;&#29702;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#21644;&#38142;&#25509;&#39044;&#27979;&#65288;LP&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;DevAssistLlama&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#65288;&#21253;&#25324;ChatGPT&#65289;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#31361;&#20986;&#20102;&#19987;&#38376;LLM&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05626v2 Announce Type: replace-cross  Abstract: In this paper, we delve into the advancement of domain-specific Large Language Models (LLMs) with a focus on their application in software development. We introduce DevAssistLlama, a model developed through instruction tuning, to assist developers in processing software-related natural language queries. This model, a variant of instruction tuned LLM, is particularly adept at handling intricate technical documentation, enhancing developer capability in software specific tasks. The creation of DevAssistLlama involved constructing an extensive instruction dataset from various software systems, enabling effective handling of Named Entity Recognition (NER), Relation Extraction (RE), and Link Prediction (LP). Our results demonstrate DevAssistLlama's superior capabilities in these tasks, in comparison with other models including ChatGPT. This research not only highlights the potential of specialized LLMs in software development also t
&lt;/p&gt;</description></item><item><title>LLMs&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#33539;&#24335;&#30340;&#36716;&#21464;&#65292;&#20197;&#22823;&#22823;&#25552;&#39640;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.03740</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting in Autoregressive Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03740
&lt;/p&gt;
&lt;p&gt;
LLMs&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#33539;&#24335;&#30340;&#36716;&#21464;&#65292;&#20197;&#22823;&#22823;&#25552;&#39640;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26684;&#23616;&#12290;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#33539;&#24335;&#24050;&#32463;&#21462;&#20195;&#20102;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#24120;&#35268;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#12290;&#36825;&#31181;&#36716;&#21464;&#20027;&#35201;&#24471;&#30410;&#20110;LLMs&#21644;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#12290;LLMs&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#22823;&#37327;&#21442;&#25968;&#21644;&#24222;&#22823;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#65292;&#24517;&#39035;&#24341;&#23548;&#23427;&#20204;&#30340;&#36755;&#20986;&#26397;&#30528;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#25552;&#31034;&#65292;&#21363;&#25552;&#20379;&#29305;&#23450;&#30340;&#36755;&#20837;&#25110;&#25351;&#20196;&#26469;&#24341;&#23548;LLMs&#26397;&#30528;&#39044;&#26399;&#36755;&#20986;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#24050;&#25104;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#24050;&#34987;&#24212;&#29992;&#26469;&#20805;&#20998;&#21033;&#29992;LLMs&#28508;&#21147;&#30340;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#25552;&#31034;&#25216;&#26415;&#30340;&#20998;&#31867;&#65292;&#24182;&#26681;&#25454;&#20854;&#36827;&#34892;&#20102;&#31616;&#26126;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03740v1 Announce Type: cross  Abstract: Autoregressive Large Language Models have transformed the landscape of Natural Language Processing. Pre-train and prompt paradigm has replaced the conventional approach of pre-training and fine-tuning for many downstream NLP tasks. This shift has been possible largely due to LLMs and innovative prompting techniques. LLMs have shown great promise for a variety of downstream tasks owing to their vast parameters and huge datasets that they are pre-trained on. However, in order to fully realize their potential, their outputs must be guided towards the desired outcomes. Prompting, in which a specific input or instruction is provided to guide the LLMs toward the intended output, has become a tool for achieving this goal. In this paper, we discuss the various prompting techniques that have been applied to fully harness the power of LLMs. We present a taxonomy of existing literature on prompting techniques and provide a concise survey based on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24046;&#20998;&#31169;&#23494;&#31163;&#32447;&#25552;&#31034;&#35843;&#25972;&#65288;DP-OPT&#65289;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#25552;&#31034;&#26102;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#35843;&#25972;&#25552;&#31034;&#24182;&#24212;&#29992;&#20110;&#20113;&#27169;&#22411;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20256;&#36755;</title><link>https://arxiv.org/abs/2312.03724</link><description>&lt;p&gt;
DP-OPT&#65306;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#24744;&#30340;&#38544;&#31169;&#20445;&#25252;&#25552;&#31034;&#24037;&#31243;&#24072;
&lt;/p&gt;
&lt;p&gt;
DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24046;&#20998;&#31169;&#23494;&#31163;&#32447;&#25552;&#31034;&#35843;&#25972;&#65288;DP-OPT&#65289;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#25552;&#31034;&#26102;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#35843;&#25972;&#25552;&#31034;&#24182;&#24212;&#29992;&#20110;&#20113;&#27169;&#22411;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#20027;&#35201;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#38024;&#23545;&#29305;&#23450;&#30446;&#26631;&#26102;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35843;&#25972;&#30340;&#25552;&#31034;&#20381;&#36182;&#20110;&#25935;&#24863;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#22260;&#32469;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#25552;&#20986;&#20102;&#38556;&#30861;&#12290;&#19968;&#20010;&#23454;&#38469;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25176;&#31649;&#19968;&#20010;&#26412;&#22320;&#30340;LLM&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#31169;&#19979;&#20248;&#21270;&#19968;&#20010;&#36719;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#27169;&#22411;&#25152;&#26377;&#26435;&#21463;&#21040;&#20445;&#25252;&#26102;&#65292;&#25176;&#31649;&#19968;&#20010;&#26412;&#22320;&#27169;&#22411;&#23601;&#21464;&#24471;&#26377;&#38382;&#39064;&#12290;&#23558;&#25968;&#25454;&#21457;&#36865;&#32473;&#27169;&#22411;&#25552;&#20379;&#31243;&#24207;&#36827;&#34892;&#22521;&#35757;&#31561;&#26367;&#20195;&#26041;&#27861;&#21152;&#21095;&#20102;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#65292;&#38754;&#23545;&#19968;&#20010;&#19981;&#21463;&#20449;&#20219;&#30340;&#25552;&#20379;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24046;&#20998;&#31169;&#23494;&#31163;&#32447;&#25552;&#31034;&#35843;&#25972;&#65288;DP-OPT&#65289;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#23458;&#25143;&#31471;&#35843;&#25972;&#31163;&#25955;&#25552;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#25152;&#38656;&#30340;&#20113;&#27169;&#22411;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;LLMs&#26412;&#36523;&#24314;&#35758;&#30340;&#25552;&#31034;&#21487;&#20197;&#22312;&#19981;&#26292;&#38706;p&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03724v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising p
&lt;/p&gt;</description></item><item><title>SoftMAC&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#36719;&#20307;&#12289;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#39044;&#27979;&#30340;&#25509;&#35302;&#27169;&#22411;&#21644;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#31359;&#36879;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2312.03297</link><description>&lt;p&gt;
SoftMAC&#65306;&#22522;&#20110;&#39044;&#27979;&#25509;&#35302;&#27169;&#22411;&#21644;&#19982;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#21452;&#21521;&#32806;&#21512;&#30340;&#21487;&#24494;&#36719;&#20307;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03297
&lt;/p&gt;
&lt;p&gt;
SoftMAC&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#36719;&#20307;&#12289;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#39044;&#27979;&#30340;&#25509;&#35302;&#27169;&#22411;&#21644;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#31359;&#36879;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#29289;&#29702;&#20223;&#30495;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#20915;&#26426;&#22120;&#20154;&#30456;&#20851;&#38382;&#39064;&#30340;&#25928;&#29575;&#12290;&#20026;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#32437;&#22330;&#26223;&#20013;&#24212;&#29992;&#21487;&#24494;&#20223;&#30495;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#21508;&#31181;&#26448;&#26009;&#38598;&#25104;&#21040;&#32479;&#19968;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SoftMAC&#65292;&#19968;&#20010;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#23558;&#36719;&#20307;&#19982;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#12290;SoftMAC&#20351;&#29992;&#22522;&#20110;&#36830;&#32493;&#21147;&#23398;&#30340;&#26448;&#26009;&#28857;&#27861;&#26469;&#27169;&#25311;&#36719;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#27979;&#30340;MPM&#25509;&#35302;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#31359;&#36879;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#20854;&#20182;&#24322;&#24120;&#29616;&#35937;&#65292;&#22914;&#19981;&#33258;&#28982;&#30340;&#21453;&#24377;&#12290;&#20026;&#20102;&#23558;MPM&#31890;&#23376;&#19982;&#21487;&#21464;&#24418;&#21644;&#38750;&#20307;&#31215;&#34915;&#29289;&#32593;&#26684;&#32806;&#21512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#37325;&#24314;&#23616;&#37096;&#21306;&#22495;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03297v2 Announce Type: replace-cross  Abstract: Differentiable physics simulation provides an avenue to tackle previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework that couples soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a novel forecast-based contact model for MPM, which effectively reduces penetration without introducing other artifacts like unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Diverging from prev
&lt;/p&gt;</description></item><item><title>MobileGPT&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#31867;&#20154;&#24212;&#29992;&#35760;&#24518;&#27169;&#25311;&#20154;&#31867;&#19982;&#31227;&#21160;&#24212;&#29992;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23454;&#29616;&#20219;&#21153;&#31243;&#24207;&#30340;&#31934;&#30830;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2312.03003</link><description>&lt;p&gt;
&#25506;&#32034;&#12289;&#36873;&#25321;&#12289;&#25512;&#23548;&#21644;&#22238;&#24518;&#65306;&#20026;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#22686;&#21152;&#31867;&#20154;&#35760;&#24518;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03003
&lt;/p&gt;
&lt;p&gt;
MobileGPT&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#31867;&#20154;&#24212;&#29992;&#35760;&#24518;&#27169;&#25311;&#20154;&#31867;&#19982;&#31227;&#21160;&#24212;&#29992;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23454;&#29616;&#20219;&#21153;&#31243;&#24207;&#30340;&#31934;&#30830;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#23427;&#20204;&#20248;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#20351;&#29992;&#25143;&#33021;&#22815;&#33258;&#21160;&#25191;&#34892;&#22797;&#26434;&#21644;&#37325;&#22797;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#22266;&#26377;&#30340;&#19981;&#21487;&#38752;&#24615;&#21644;&#39640;&#36816;&#34892;&#25104;&#26412;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#30456;&#24403;&#26377;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;MobileGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#37197;&#22791;&#20102;&#31867;&#20154;&#24212;&#29992;&#35760;&#24518;&#12290;MobileGPT&#27169;&#25311;&#20102;&#20154;&#31867;&#19982;&#31227;&#21160;&#24212;&#29992;&#20132;&#20114;&#30340;&#35748;&#30693;&#36807;&#31243;--&#25506;&#32034;&#12289;&#36873;&#25321;&#12289;&#25512;&#23548;&#21644;&#22238;&#24518;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#20219;&#21153;&#31243;&#24207;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#27169;&#22359;&#21270;&#30340;&#23376;&#20219;&#21153;&#65292;&#20801;&#35768;&#26356;&#31934;&#30830;&#12289;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#21153;&#27969;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#23376;&#20219;&#21153;&#30340;&#37325;&#22797;&#20351;&#29992;&#12289;&#37325;&#26032;&#25490;&#21015;&#21644;&#36866;&#24212;&#21508;&#31181;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;LLM&#26381;&#21153;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#23454;&#29616;&#20102;MobileGPT&#65292;&#24182;&#22312;&#19968;&#32452;&#25968;&#25454;&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03003v2 Announce Type: replace-cross  Abstract: The advent of large language models (LLMs) has opened up new opportunities in the field of mobile task automation. Their superior language understanding and reasoning capabilities allow users to automate complex and repetitive tasks. However, due to the inherent unreliability and high operational cost of LLMs, their practical applicability is quite limited. To address these issues, this paper introduces MobileGPT, an innovative LLM-based mobile task automator equipped with a human-like app memory. MobileGPT emulates the cognitive process of humans interacting with a mobile app -- explore, select, derive, and recall. This approach allows for a more precise and efficient learning of a task's procedure by breaking it down into smaller, modular sub-tasks that can be re-used, re-arranged, and adapted for various objectives. We implement MobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its performance on a datase
&lt;/p&gt;</description></item><item><title>Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2312.00029</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#20934;&#26694;&#26550;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00029
&lt;/p&gt;
&lt;p&gt;
Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#20195;&#23545;&#40784;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#38450;&#27490;&#22312;&#27169;&#22411;&#34987;&#33988;&#24847;&#25915;&#20987;&#26102;&#20135;&#29983;&#26377;&#23475;&#24212;&#23545;&#12290;&#20026;&#20102;&#24110;&#21161;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bergeron&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;LLMs&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#21442;&#25968;&#24494;&#35843;&#12290;Bergeron&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#65307;&#27425;&#35201;LLM&#27169;&#25311;&#21463;&#20445;&#25252;&#30340;&#20027;&#35201;LLM&#30340;&#33391;&#30693;&#12290;&#35813;&#26694;&#26550;&#22312;&#30417;&#35270;&#36755;&#20986;&#20197;&#26816;&#27979;&#20219;&#20309;&#26377;&#23475;&#20869;&#23481;&#30340;&#21516;&#26102;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#20027;&#35201;&#27169;&#22411;&#20813;&#21463;&#20837;&#20405;&#25915;&#20987;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#20351;&#29992;Bergeron&#26469;&#34917;&#20805;&#29616;&#26377;&#23545;&#40784;&#35757;&#32451;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00029v2 Announce Type: replace-cross  Abstract: Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM emulating the conscience of a protected, primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis shows that, by using Bergeron to complement models with existing alignment traini
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Wasserstein&#36317;&#31163;&#21450;&#20854;&#37325;&#24515;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20998;&#24067;&#21305;&#37197;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18531</link><description>&lt;p&gt;
&#36890;&#36807;Wasserstein&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation via the Wasserstein Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18531
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Wasserstein&#36317;&#31163;&#21450;&#20854;&#37325;&#24515;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20998;&#24067;&#21305;&#37197;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#20449;&#24687;&#23553;&#35013;&#20026;&#26126;&#26174;&#26356;&#23567;&#30340;&#21512;&#25104;&#31561;&#20215;&#29289;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#30041;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#36317;&#31163;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#22686;&#24378;DD&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Wasserstein&#37325;&#24515;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#37327;&#21270;&#20998;&#24067;&#24046;&#24322;&#21644;&#39640;&#25928;&#25429;&#33719;&#20998;&#24067;&#38598;&#21512;&#20013;&#24515;&#30340;&#20960;&#20309;&#24847;&#20041;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#20419;&#36827;&#20102;&#26377;&#25928;&#30340;&#20998;&#24067;&#21305;&#37197;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#25345;&#20102;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#25216;&#26415;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#32780;&#19988;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18531v2 Announce Type: replace-cross  Abstract: Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the expansive information of large datasets into significantly smaller, synthetic equivalents, thereby preserving model performance with reduced computational overhead. Pursuing this objective, we introduce the Wasserstein distance, a metric grounded in optimal transport theory, to enhance distribution matching in DD. Our approach employs the Wasserstein barycenter to provide a geometrically meaningful method for quantifying distribution differences and capturing the centroid of distribution sets efficiently. By embedding synthetic data in the feature spaces of pretrained classification models, we facilitate effective distribution matching that leverages prior knowledge inherent in these models. Our method not only maintains the computational advantages of distribution matching-based techniques but also achieves new state-of-the-art performance across a ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#21305;&#37197;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;Generalized Various Backbone and Statistical Matching (G-VBSM)&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#20016;&#23500;&#20449;&#24687;&#21644;&#26356;&#22909;&#27010;&#25324;&#33021;&#21147;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2311.17950</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21516;&#30340;&#20027;&#24178;&#21644;&#32479;&#35745;&#21305;&#37197;&#36827;&#34892;&#24191;&#20041;&#22823;&#35268;&#27169;&#25968;&#25454;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#21305;&#37197;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;Generalized Various Backbone and Statistical Matching (G-VBSM)&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#20016;&#23500;&#20449;&#24687;&#21644;&#26356;&#22909;&#27010;&#25324;&#33021;&#21147;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SRe2L&#24341;&#20837;&#30340;&#36731;&#37327;&#32423;&#8220;&#23616;&#37096;&#21305;&#37197;-&#20840;&#23616;&#21305;&#37197;&#8221;&#25104;&#21151;&#22320;&#21019;&#36896;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#23436;&#25972;&#30340;224x224 ImageNet-1k&#30340;&#20840;&#38754;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21333;&#36793;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#20027;&#24178;&#12289;&#23618;&#21644;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#31934;&#31616;&#25968;&#25454;&#38598;&#27010;&#25324;&#33021;&#21147;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#24314;&#35758;&#20805;&#20998;&#32780;&#21508;&#24322;&#30340;&#8220;&#23616;&#37096;&#21305;&#37197;-&#20840;&#23616;&#21305;&#37197;&#8221;&#27604;&#21333;&#19968;&#21305;&#37197;&#26356;&#21152;&#31934;&#30830;&#21644;&#26377;&#25928;&#65292;&#24182;&#33021;&#22815;&#21019;&#36896;&#20986;&#26356;&#20016;&#23500;&#20449;&#24687;&#21644;&#26356;&#22909;&#27010;&#25324;&#33021;&#21147;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24191;&#20041;&#21305;&#37197;&#8221;&#35266;&#28857;&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#24191;&#20041;&#19981;&#21516;&#20027;&#24178;&#21644;&#32479;&#35745;&#21305;&#37197; (G-VBSM)&#65292;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#23494;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;&#20027;&#24178;&#12289;&#23618;&#21644;&#32479;&#35745;&#25968;&#25454;&#19978;&#19982;&#23436;&#25972;&#25968;&#25454;&#38598;&#20445;&#25345;&#19968;&#33268;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;G-VBSM&#26159;&#31532;&#19968;&#20010;&#33719;&#24471;&#24378;&#22823;&#24615;&#33021;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17950v2 Announce Type: replace-cross  Abstract: The lightweight "local-match-global" matching introduced by SRe2L successfully creates a distilled dataset with comprehensive information on the full 224x224 ImageNet-1k. However, this one-sided approach is limited to a particular backbone, layer, and statistics, which limits the improvement of the generalization of a distilled dataset. We suggest that sufficient and various "local-match-global" matching are more precise and effective than a single one and has the ability to create a distilled dataset with richer information and better generalization. We call this perspective "generalized matching" and propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this work, which aims to create a synthetic dataset with densities, ensuring consistency with the complete dataset across various backbones, layers, and statistics. As experimentally demonstrated, G-VBSM is the first algorithm to obtain strong performance a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#20801;&#35768;&#22312;&#22797;&#21512;&#39640;&#26031;&#20998;&#24067;&#31867;&#20013;&#36827;&#34892;&#38382;&#39064;&#29305;&#23450;&#32479;&#35745;&#20808;&#39564;&#36873;&#25321;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#26032;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#36845;&#20195;&#31639;&#27861;&#24191;&#20041;&#22797;&#21512;&#39640;&#26031;&#26368;&#23567;&#20108;&#20056;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#23637;&#24320;&#24471;&#21040;&#30340;&#26032;&#39062;&#28145;&#24230;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;DR-CG-Net&#12290;</title><link>https://arxiv.org/abs/2311.17248</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#28145;&#24230;&#27491;&#21017;&#21270;&#22797;&#21512;&#39640;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Regularized Compound Gaussian Network for Solving Linear Inverse Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17248
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#20801;&#35768;&#22312;&#22797;&#21512;&#39640;&#26031;&#20998;&#24067;&#31867;&#20013;&#36827;&#34892;&#38382;&#39064;&#29305;&#23450;&#32479;&#35745;&#20808;&#39564;&#36873;&#25321;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#26032;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#36845;&#20195;&#31639;&#27861;&#24191;&#20041;&#22797;&#21512;&#39640;&#26031;&#26368;&#23567;&#20108;&#20056;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#23637;&#24320;&#24471;&#21040;&#30340;&#26032;&#39062;&#28145;&#24230;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;DR-CG-Net&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20808;&#39564;&#20449;&#24687;&#32435;&#20837;&#36870;&#38382;&#39064;&#65292;&#20363;&#22914;&#36890;&#36807;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#65292;&#26159;&#20419;&#36827;&#31283;&#20581;&#36870;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#25991;&#20026;&#20801;&#35768;&#22312;&#22797;&#21512;&#39640;&#26031;&#65288;CG&#65289;&#20998;&#24067;&#31867;&#20013;&#36827;&#34892;&#38382;&#39064;&#29305;&#23450;&#32479;&#35745;&#20808;&#39564;&#36873;&#25321;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#31216;&#20026;&#24191;&#20041;&#22797;&#21512;&#39640;&#26031;&#26368;&#23567;&#20108;&#20056;&#65288;G-CG-LS&#65289;&#65292;&#23427;&#26368;&#23567;&#21270;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#24378;&#21046;&#25191;&#34892;&#19968;&#20010;CG&#20808;&#39564;&#12290;&#28982;&#21518;&#23558;G-CG-LS&#23637;&#24320;&#65292;&#25552;&#20379;&#25105;&#20204;&#30340;&#31532;&#20108;&#31181;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#27491;&#21017;&#21270;&#65288;DR&#65289;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;DR-CG-Net&#65292;&#21487;&#20197;&#23398;&#20064;&#20808;&#39564;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17248v2 Announce Type: replace-cross  Abstract: Incorporating prior information into inverse problems, e.g. via maximum-a-posteriori estimation, is an important technique for facilitating robust inverse problem solutions. In this paper, we devise two novel approaches for linear inverse problems that permit problem-specific statistical prior selections within the compound Gaussian (CG) class of distributions. The CG class subsumes many commonly used priors in signal and image reconstruction methods including those of sparsity-based approaches. The first method developed is an iterative algorithm, called generalized compound Gaussian least squares (G-CG-LS), that minimizes a regularized least squares objective function where the regularization enforces a CG prior. G-CG-LS is then unrolled, or unfolded, to furnish our second method, which is a novel deep regularized (DR) neural network, called DR-CG-Net, that learns the prior information. A detailed computational theory on conv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2311.16834</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#20351;&#29992;&#27880;&#24847;&#21147;&#36827;&#34892;&#35299;&#37322;&#24615;&#21644;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#27668;&#35937;&#23398;&#21644;&#29983;&#21629;&#31185;&#23398;&#31561;&#39046;&#22495;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#34987;&#25209;&#35780;&#20026;&#8220;&#40657;&#30418;&#8221;&#25110;&#26080;&#27861;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#26500;&#36896;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#25233;&#21046;&#22312;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#24615;&#20013;&#20351;&#29992;&#30340;&#20887;&#20313;&#29305;&#24449;&#12290;&#20174;&#36873;&#25321;&#30340;&#29305;&#24449;&#29420;&#31435;&#35757;&#32451;&#27169;&#22359;&#21270;&#28145;&#24230;&#32593;&#32476;&#65292;&#21521;&#29992;&#25143;&#23637;&#31034;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16834v3 Announce Type: replace-cross  Abstract: Multivariate time series have many applications, from healthcare and meteorology to life science. Although deep learning models have shown excellent predictive performance for time series, they have been criticised for being "black-boxes" or non-interpretable. This paper proposes a novel modular neural network model for multivariate time series prediction that is interpretable by construction. A recurrent neural network learns the temporal dependencies in the data while an attention-based feature selection component selects the most relevant features and suppresses redundant features used in the learning of the temporal dependencies. A modular deep network is trained from the selected features independently to show the users how features influence outcomes, making the model interpretable. Experimental results show that this approach can outperform state-of-the-art interpretable Neural Additive Models (NAM) and variations thereo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22235;&#39033;&#29992;&#20110;&#35774;&#35745;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#25351;&#21335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20855;&#26377;&#36890;&#29992;&#24863;&#30693;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.15599</link><description>&lt;p&gt;
UniRepLKNet&#65306;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#28857;&#20113;&#12289;&#26102;&#38388;&#24207;&#21015;&#21644;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#24863;&#30693;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15599
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22235;&#39033;&#29992;&#20110;&#35774;&#35745;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#25351;&#21335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20855;&#26377;&#36890;&#29992;&#24863;&#30693;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNets)&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20294;&#20173;&#26377;&#20004;&#20010;&#26410;&#35299;&#20915;&#30340;&#37325;&#35201;&#38382;&#39064;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;1) &#29616;&#26377;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#36981;&#24490;&#20256;&#32479;ConvNets&#25110;transformers&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#32780;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#35774;&#35745;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;2) &#38543;&#30528;transformers&#20027;&#23548;&#20102;&#22810;&#31181;&#27169;&#24577;&#65292;&#20173;&#38656;&#25506;&#35752;ConvNets&#22312;&#35270;&#35273;&#20197;&#22806;&#39046;&#22495;&#26159;&#21542;&#20855;&#26377;&#24378;&#22823;&#30340;&#36890;&#29992;&#24863;&#30693;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;1) &#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#39033;&#29992;&#20110;&#35774;&#35745;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#25351;&#21335;&#65292;&#20854;&#20013;&#30340;&#26680;&#24515;&#26159;&#21033;&#29992;&#22823;&#21367;&#31215;&#26680;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#21306;&#21035;&#20110;&#23567;&#21367;&#31215;&#26680;-&#23427;&#20204;&#21487;&#20197;&#24191;&#27867;&#22320;&#35266;&#23519;&#32780;&#26080;&#38656;&#28145;&#20837;&#12290;&#36981;&#24490;&#36825;&#20123;&#25351;&#21335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22823;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15599v2 Announce Type: replace-cross  Abstract: Large-kernel convolutional neural networks (ConvNets) have recently received extensive research attention, but two unresolved and critical issues demand further investigation. 1) The architectures of existing large-kernel ConvNets largely follow the design principles of conventional ConvNets or transformers, while the architectural design for large-kernel ConvNets remains under-addressed. 2) As transformers have dominated multiple modalities, it remains to be investigated whether ConvNets also have a strong universal perception ability in domains beyond vision. In this paper, we contribute from two aspects. 1) We propose four architectural guidelines for designing large-kernel ConvNets, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep. Following such guidelines, our proposed large-kernel ConvNet shows leading performanc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.15487</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15487
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#26368;&#23567;&#21270;$\mathcal{L}^2$&#20195;&#20215;&#20989;&#25968;&#30340;&#26799;&#24230;&#19979;&#38477;&#27969;&#65292;&#24182;&#24341;&#20837;&#20004;&#20010;&#25913;&#36827;&#29256;&#26412;&#65307;&#19968;&#20010;&#36866;&#29992;&#20110;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#65292;&#21478;&#19968;&#20010;&#36866;&#29992;&#20110;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#12290;&#36825;&#20004;&#20010;&#29256;&#26412;&#37117;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#65292;&#32771;&#34385;&#21040;&#22312;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25289;&#22238;&#21521;&#37327;&#19995;&#32467;&#26500;&#21644;&#22312;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25512;&#21069;&#21521;&#37327;&#19995;&#32467;&#26500;&#12290;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#28385;&#36275;&#31209;&#26465;&#20214;&#65292;&#25913;&#36827;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#25152;&#26377;&#36712;&#36947;&#23558;&#20197;&#22343;&#21248;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#23558;$\mathcal{L}^2$&#20195;&#20215;&#39537;&#21160;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#65307;&#22240;&#27492;&#65292;&#23545;&#20110;&#20219;&#20309;&#39044;&#20808;&#25351;&#23450;&#30340;&#25509;&#36817;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#20808;&#39564;&#20572;&#27490;&#26102;&#38388;&#12290;&#25105;&#20204;&#25351;&#20986;&#21518;&#32773;&#19982;&#27425;Riemann&#20960;&#20309;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15487v3 Announce Type: replace-cross  Abstract: We consider the gradient descent flow widely used for the minimization of the $\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate; one thereby obtains an a priori stopping time for any prescribed proximity to the global minimum. We point out relations of the latter to sub-Riemannian geometry.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;S4MI&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#33021;&#22815;&#31616;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#26426;&#22120;&#30417;&#30563;&#36807;&#31243;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.10319</link><description>&lt;p&gt;
&#36716;&#21521;&#26426;&#22120;&#30417;&#30563;&#65306;&#29992;&#20110;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#26631;&#27880;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10319
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;S4MI&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#33021;&#22815;&#31616;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#26426;&#22120;&#30417;&#30563;&#36807;&#31243;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#27835;&#30103;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#21463;&#21040;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#25216;&#26415;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#12290;&#26631;&#27880;&#36807;&#31243;&#19981;&#20165;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#23478;&#22823;&#37327;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;S4MI&#65288;&#21307;&#23398;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#65289;&#27969;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21457;&#23637;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#25216;&#26415;&#21442;&#19982;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26426;&#22120;&#30417;&#30563;&#30340;&#25193;&#23637;&#65292;&#30456;&#27604;&#23436;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10319v3 Announce Type: replace-cross  Abstract: Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages the advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self-supervised learning significantly surpassed the performance of supervised methods in the classificati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20363;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;MILLET&#65292;&#20351;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;&#21464;&#24471;&#20869;&#22312;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#29978;&#33267;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2311.10049</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20363;&#23398;&#20064;&#23454;&#29616;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Inherently Interpretable Time Series Classification via Multiple Instance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10049
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20363;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;MILLET&#65292;&#20351;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;&#21464;&#24471;&#20869;&#22312;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#29978;&#33267;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#26041;&#27861;&#36890;&#24120;&#26159;&#40657;&#21283;&#23376;&#65292;&#38590;&#20197;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MILLET&#30340;&#26032;&#26694;&#26550;&#65306;&#29992;&#20110;&#26412;&#22320;&#21487;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#22810;&#20363;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;MILLET&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;TSC&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22914;&#20309;&#21464;&#24471;&#20869;&#22312;&#21487;&#35299;&#37322;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#65288;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#25552;&#39640;&#65289;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;85&#20010;UCR TSC&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;MILLET&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#26032;&#39062;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;MILLET&#33021;&#22815;&#24555;&#36895;&#20135;&#29983;&#27604;&#20854;&#20182;&#20247;&#25152;&#21608;&#30693;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26356;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10049v3 Announce Type: replace-cross  Abstract: Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/J
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#29992;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#25163;&#20381;&#36182;&#20110;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#36801;&#31227;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21338;&#24328;&#35770;&#35270;&#35282;&#30340;&#19987;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#27169;&#22411;&#21644;&#25915;&#20987;&#31639;&#27861;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2310.17645</link><description>&lt;p&gt;
PubDef&#65306;&#38450;&#24481;&#26469;&#33258;&#20844;&#20849;&#27169;&#22411;&#30340;&#36801;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PubDef: Defending Against Transfer Attacks From Public Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.17645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#29992;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#25163;&#20381;&#36182;&#20110;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#36801;&#31227;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21338;&#24328;&#35770;&#35270;&#35282;&#30340;&#19987;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#27169;&#22411;&#21644;&#25915;&#20987;&#31639;&#27861;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#19968;&#30452;&#26159;&#34892;&#19994;&#20013;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#19988;&#26410;&#35299;&#20915;&#30340;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21313;&#24180;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25991;&#29486;&#21382;&#21490;&#65292;&#25105;&#20204;&#20102;&#35299;&#21040;&#21457;&#36215;&#24378;&#22823;&#25110;&#26368;&#20248;&#25915;&#20987;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23427;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36807;&#21435;&#22823;&#22810;&#25968;&#25991;&#29486;&#22362;&#23450;&#20551;&#35774;&#30340;&#30333;&#30418;&#23041;&#32961;&#27169;&#22411;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#29992;&#23041;&#32961;&#27169;&#22411;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#23545;&#25163;&#20381;&#36182;&#20110;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#36801;&#31227;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#35774;&#32622;&#23558;&#25104;&#20026;&#26410;&#26469;&#23433;&#20840;&#25935;&#24863;&#24212;&#29992;&#20013;&#26368;&#26222;&#36941;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#36801;&#31227;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21338;&#24328;&#35770;&#35270;&#35282;&#30340;&#19987;&#38376;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#22312;24&#20010;&#20844;&#20849;&#27169;&#22411;&#21644;11&#31181;&#25915;&#20987;&#31639;&#27861;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#12289;CIFAR-100 &#21644; ImageNet&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.17645v2 Announce Type: replace-cross  Abstract: Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this paper, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. The defenses are evaluated under 24 public models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and ImageNet). Under thi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21161;&#20215;&#20540;&#65288;VOA&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#29305;&#23450;&#35266;&#27979;&#23545;&#26426;&#22120;&#20154;&#23436;&#25104;&#25235;&#21462;&#20219;&#21153;&#30340;&#39044;&#26399;&#24433;&#21709;&#65292;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#39564;&#35777;&#20102;&#35813;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.14402</link><description>&lt;p&gt;
&#25235;&#21462;&#36807;&#31243;&#20013;&#21327;&#21161;&#20215;&#20540;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Value of Assistance for Grasping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21161;&#20215;&#20540;&#65288;VOA&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#29305;&#23450;&#35266;&#27979;&#23545;&#26426;&#22120;&#20154;&#23436;&#25104;&#25235;&#21462;&#20219;&#21153;&#30340;&#39044;&#26399;&#24433;&#21709;&#65292;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#39564;&#35777;&#20102;&#35813;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#31181;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#26426;&#22120;&#20154;&#34987;&#36171;&#20104;&#25235;&#21462;&#29289;&#20307;&#30340;&#20219;&#21153;&#65292;&#20294;&#19981;&#30693;&#36947;&#29289;&#20307;&#30340;&#30830;&#20999;&#23039;&#21183;&#65292;&#20381;&#36182;&#23545;&#23039;&#21183;&#30340;&#27010;&#29575;&#20272;&#35745;&#26469;&#20915;&#23450;&#22914;&#20309;&#23581;&#35797;&#25235;&#21462;&#12290;&#25105;&#20204;&#25903;&#25345;&#22312;&#23581;&#35797;&#25235;&#21462;&#20043;&#21069;&#21521;&#26426;&#22120;&#20154;&#25552;&#20379;&#29289;&#20307;&#35266;&#27979;&#30340;&#35774;&#32622;&#65292;&#20294;&#36825;&#31181;&#21487;&#33021;&#24615;&#26377;&#38480;&#65292;&#38656;&#35201;&#20915;&#23450;&#21738;&#31181;&#24863;&#30693;&#34892;&#20026;&#26368;&#26377;&#30410;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21161;&#20215;&#20540;&#65288;Value of Assistance, VOA&#65289;&#24230;&#37327;&#26469;&#25903;&#25345;&#36825;&#31181;&#20915;&#31574;&#65292;&#36825;&#31181;&#24230;&#37327;&#29992;&#20110;&#35780;&#20272;&#29305;&#23450;&#35266;&#27979;&#23558;&#23545;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#20135;&#29983;&#30340;&#39044;&#26399;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#21327;&#20316;&#25235;&#21462;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14402v2 Announce Type: replace-cross  Abstract: In multiple realistic settings, a robot is tasked with grasping an object without knowing its exact pose and relies on a probabilistic estimation of the pose to decide how to attempt the grasp. We support settings in which it is possible to provide the robot with an observation of the object before a grasp is attempted but this possibility is limited and there is a need to decide which sensing action would be most beneficial. We support this decision by offering a novel Value of Assistance (VOA) measure for assessing the expected effect a specific observation will have on the robot's ability to complete its task. We evaluate our suggested measure in simulated and real-world collaborative grasping settings.
&lt;/p&gt;</description></item><item><title>Llemma&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#23398;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;MATH&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#21644;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2310.10631</link><description>&lt;p&gt;
Llemma: &#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Llemma: An Open Language Model For Mathematics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10631
&lt;/p&gt;
&lt;p&gt;
Llemma&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#23398;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;MATH&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#21644;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Llemma&#65292;&#19968;&#20010;&#29992;&#20110;&#25968;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#32487;&#32493;&#22312;Proof-Pile-2&#19978;&#23545;Code Llama&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;Proof-Pile-2&#21253;&#21547;&#31185;&#23398;&#35770;&#25991;&#12289;&#21253;&#21547;&#25968;&#23398;&#20869;&#23481;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#25968;&#23398;&#20195;&#30721;&#65292;&#26368;&#32456;&#29983;&#25104;&#20102;Llemma&#12290;&#22312;MATH&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Llemma&#22312;&#21516;&#31561;&#21442;&#25968;&#22522;&#30784;&#19978;&#32988;&#36807;&#25152;&#26377;&#24050;&#30693;&#30340;&#24320;&#28304;&#22522;&#20934;&#27169;&#22411;&#65292;&#20197;&#21450;&#23578;&#26410;&#21457;&#24067;&#30340;Minerva&#27169;&#22411;&#22871;&#20214;&#12290;&#27492;&#22806;&#65292;Llemma&#33021;&#22815;&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#21644;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#25152;&#26377;&#24037;&#20214;&#65292;&#21253;&#25324;70&#20159;&#21644;340&#20159;&#21442;&#25968;&#27169;&#22411;&#12289;Proof-Pile-2&#20197;&#21450;&#29992;&#20110;&#22797;&#21046;&#25105;&#20204;&#23454;&#39564;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10631v3 Announce Type: replace-cross  Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33033;&#20914;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20316;&#20026;&#23545;&#20170;&#22825;&#20027;&#23548;&#35270;&#35273;&#39046;&#22495;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#26367;&#20195;&#65292;&#25215;&#35834;&#33021;&#22815;&#23454;&#29616;&#25968;&#37327;&#32423;&#30340;&#33021;&#37327;&#25928;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2310.09692</link><description>&lt;p&gt;
&#38754;&#21521;&#19979;&#19968;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Spike-based Neuromorphic Computing for Next-Generation Computer Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09692
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20316;&#20026;&#23545;&#20170;&#22825;&#20027;&#23548;&#35270;&#35273;&#39046;&#22495;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#26367;&#20195;&#65292;&#25215;&#35834;&#33021;&#22815;&#23454;&#29616;&#25968;&#37327;&#32423;&#30340;&#33021;&#37327;&#25928;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#25215;&#35834;&#22312;&#33021;&#37327;&#25928;&#29575;&#19978;&#27604;&#20256;&#32479;&#30340;&#20911;&#183;&#35834;&#20381;&#26364;&#35745;&#31639;&#33539;&#24335;&#26377;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#12290;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#23398;&#20064;&#21644;&#27169;&#25311;&#22823;&#33041;&#21151;&#33021;&#26469;&#24320;&#21457;&#19968;&#31181;&#33258;&#36866;&#24212;&#12289;&#23481;&#38169;&#12289;&#20302;&#21344;&#22320;&#38754;&#31215;&#12289;&#24555;&#36895;&#12289;&#20302;&#33021;&#32791;&#30340;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#21019;&#26032;&#26469;&#23454;&#29616;&#65292;&#21253;&#25324;&#26448;&#26009;&#12289;&#22120;&#20214;&#12289;&#30005;&#36335;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#12290;&#30001;&#20110;&#22312;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#22240;&#25968;&#25454;&#38598;&#22686;&#22823;&#32780;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#22522;&#20110;&#33033;&#20914;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20170;&#22825;&#20027;&#23548;&#35270;&#35273;&#39046;&#22495;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#26412;&#20070;&#31456;&#33410;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#65292;&#27010;&#36848;&#20102;&#35774;&#35745;&#22534;&#26632;&#30340;&#19981;&#21516;&#23618;&#38754;&#65288;&#22120;&#20214;&#12289;&#30005;&#36335;&#21644;&#31639;&#27861;&#65289;&#20013;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#31034;&#20363;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09692v2 Announce Type: replace-cross  Abstract: Neuromorphic Computing promises orders of magnitude improvement in energy efficiency compared to traditional von Neumann computing paradigm. The goal is to develop an adaptive, fault-tolerant, low-footprint, fast, low-energy intelligent system by learning and emulating brain functionality which can be realized through innovation in different abstraction layers including material, device, circuit, architecture and algorithm. As the energy consumption in complex vision tasks keep increasing exponentially due to larger data set and resource-constrained edge devices become increasingly ubiquitous, spike-based neuromorphic computing approaches can be viable alternative to deep convolutional neural network that is dominating the vision field today. In this book chapter, we introduce neuromorphic computing, outline a few representative examples from different layers of the design stack (devices, circuits and algorithms) and conclude w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;WeatherDepth&#27169;&#22411;&#65292;&#36890;&#36807;&#35838;&#31243;&#23545;&#27604;&#23398;&#20064;&#65292;&#36880;&#27493;&#36866;&#24212;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#28145;&#24230;&#20272;&#35745;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#25235;&#20303;&#26377;&#30410;&#28145;&#24230;&#32447;&#32034;&#23545;&#25239;&#22825;&#27668;&#24433;&#21709;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2310.05556</link><description>&lt;p&gt;
WeatherDepth: &#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#33258;&#30417;&#30563;&#28145;&#24230;&#20272;&#35745;&#30340;&#35838;&#31243;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05556
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;WeatherDepth&#27169;&#22411;&#65292;&#36890;&#36807;&#35838;&#31243;&#23545;&#27604;&#23398;&#20064;&#65292;&#36880;&#27493;&#36866;&#24212;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#28145;&#24230;&#20272;&#35745;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#25235;&#20303;&#26377;&#30410;&#28145;&#24230;&#32447;&#32034;&#23545;&#25239;&#22825;&#27668;&#24433;&#21709;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#22312;&#28165;&#26224;&#22330;&#26223;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20809;&#29031;&#21464;&#21270;&#12289;&#22825;&#27668;&#39063;&#31890;&#31561;&#26080;&#27861;&#25512;&#24191;&#21040;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;WeatherDepth&#65292;&#19968;&#31181;&#20855;&#26377;&#35838;&#31243;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#31283;&#20581;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#22312;&#22797;&#26434;&#22825;&#27668;&#26465;&#20214;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#35838;&#31243;&#23398;&#20064;&#26041;&#26696;&#65292;&#20351;&#29992;&#19977;&#31181;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#35838;&#31243;&#36880;&#28176;&#20351;&#27169;&#22411;&#20174;&#28165;&#26224;&#36866;&#24212;&#21040;&#30456;&#23545;&#24694;&#21155;&#65292;&#28982;&#21518;&#21040;&#24694;&#21155;&#22825;&#27668;&#22330;&#26223;&#12290;&#36825;&#40723;&#21169;&#27169;&#22411;&#36880;&#28176;&#25235;&#20303;&#26377;&#30410;&#30340;&#28145;&#24230;&#32447;&#32034;&#23545;&#25239;&#22825;&#27668;&#24433;&#21709;&#65292;&#20135;&#29983;&#26356;&#24179;&#28369;&#12289;&#26356;&#22909;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#38450;&#27490;&#27169;&#22411;&#24536;&#35760;&#20043;&#21069;&#30340;&#35838;&#31243;&#65292;&#25105;&#20204;&#23558;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#21040;&#19981;&#21516;&#30340;&#35838;&#31243;&#20013;&#12290;&#36890;&#36807;&#20174;&#20197;&#24448;&#35838;&#31243;&#20013;&#27762;&#21462;&#21442;&#32771;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#24314;&#31435;&#20102;&#19968;&#31181;&#28145;&#24230;&#30340;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05556v2 Announce Type: replace-cross  Abstract: Depth estimation models have shown promising performance on clear scenes but fail to generalize to adverse weather conditions due to illumination variations, weather particles, etc. In this paper, we propose WeatherDepth, a self-supervised robust depth estimation model with curriculum contrastive learning, to tackle performance degradation in complex weather conditions. Concretely, we first present a progressive curriculum learning scheme with three simple-to-complex curricula to gradually adapt the model from clear to relative adverse, and then to adverse weather scenes. It encourages the model to gradually grasp beneficial depth cues against the weather effect, yielding smoother and better domain adaption. Meanwhile, to prevent the model from forgetting previous curricula, we integrate contrastive learning into different curricula. By drawing reference knowledge from the previous course, our strategy establishes a depth consi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Toolink&#65292;&#19968;&#20010;&#36890;&#36807;&#38142;&#24335;&#35299;&#20915;&#26041;&#27861;&#39318;&#20808;&#21019;&#24314;&#24037;&#20855;&#21253;&#65292;&#20877;&#38598;&#25104;&#24037;&#20855;&#35268;&#21010;&#21644;&#35843;&#29992;&#30340;&#26694;&#26550;&#65292;&#25104;&#21151;&#36890;&#36807;&#23545;ChatGPT&#21644;CoS-GPT&#30340;&#23454;&#39564;&#65292;&#25171;&#36896;&#20102;LLaMA-CoS&#65292;&#19968;&#20010;&#20855;&#26377;&#20808;&#36827;&#24037;&#20855;&#35268;&#21010;&#21644;&#35843;&#29992;&#33021;&#21147;&#30340;&#24378;&#22823;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2310.05155</link><description>&lt;p&gt;
Toolink: &#38142;&#25509;&#24037;&#20855;&#21253;&#21019;&#24314;&#21644;&#20351;&#29992;&#30340;&#38142;&#24335;&#35299;&#20915;&#24320;&#28304;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05155
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Toolink&#65292;&#19968;&#20010;&#36890;&#36807;&#38142;&#24335;&#35299;&#20915;&#26041;&#27861;&#39318;&#20808;&#21019;&#24314;&#24037;&#20855;&#21253;&#65292;&#20877;&#38598;&#25104;&#24037;&#20855;&#35268;&#21010;&#21644;&#35843;&#29992;&#30340;&#26694;&#26550;&#65292;&#25104;&#21151;&#36890;&#36807;&#23545;ChatGPT&#21644;CoS-GPT&#30340;&#23454;&#39564;&#65292;&#25171;&#36896;&#20102;LLaMA-CoS&#65292;&#19968;&#20010;&#20855;&#26377;&#20808;&#36827;&#24037;&#20855;&#35268;&#21010;&#21644;&#35843;&#29992;&#33021;&#21147;&#30340;&#24378;&#22823;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21033;&#29992;&#24037;&#20855;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20854;&#38381;&#28304;&#24615;&#21644;&#39640;&#25512;&#29702;&#25104;&#26412;&#23545;&#20854;&#36866;&#24212;&#24615;&#36896;&#25104;&#20102;&#38480;&#21046;&#65292;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36739;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Toolink&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38142;&#24335;&#35299;&#20915;&#65288;CoS&#65289;&#26041;&#27861;&#39318;&#20808;&#21019;&#24314;&#24037;&#20855;&#21253;&#65292;&#28982;&#21518;&#38598;&#25104;&#24037;&#20855;&#30340;&#35268;&#21010;&#21644;&#35843;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;Toolink&#22312;&#21033;&#29992;&#27169;&#22411;&#30340;&#21019;&#36896;&#21147;&#21644;CoS&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;CoS-GPT&#65292;&#19968;&#20010;&#19987;&#20026;&#24037;&#20855;&#20351;&#29992;&#32780;&#35774;&#35745;&#30340;&#38142;&#24335;&#35299;&#20915;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;LLaMA-7B&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#32467;&#26524;&#26159;LLaMA-CoS&#65292;&#19968;&#20010;&#20855;&#26377;&#20808;&#36827;&#24037;&#20855;&#35268;&#21010;&#21644;&#24037;&#20855;&#35843;&#29992;&#33021;&#21147;&#30340;&#24378;&#22823;&#24320;&#28304;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;BIG-bench&#30340;&#22810;&#26679;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;&#20854;CoS&#33021;&#21147;&#19982;ChatGPT&#30456;&#21305;&#37197;&#65292;&#32780;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05155v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#40784;&#25490;&#21015;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#25972;&#29702;&#29289;&#21697;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04566</link><description>&lt;p&gt;
Knolling Bot: &#20174;&#25972;&#27905;&#30340;&#31034;&#33539;&#20013;&#23398;&#20064;&#26426;&#22120;&#20154;&#23545;&#35937;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#40784;&#25490;&#21015;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#25972;&#29702;&#29289;&#21697;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#22336;&#65306;arXiv:2310.04566v2  &#20844;&#21578;&#31867;&#22411;&#65306;replace-cross  &#25688;&#35201;&#65306;&#35299;&#20915;&#23478;&#24237;&#31354;&#38388;&#20013;&#25955;&#20081;&#29289;&#21697;&#30340;&#25972;&#29702;&#25361;&#25112;&#21463;&#21040;&#25972;&#27905;&#24615;&#30340;&#22810;&#26679;&#24615;&#21644;&#20027;&#35266;&#24615;&#30340;&#22797;&#26434;&#24615;&#24433;&#21709;&#12290;&#27491;&#22914;&#20154;&#31867;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#20801;&#35768;&#21516;&#19968;&#29702;&#24565;&#30340;&#22810;&#31181;&#34920;&#36798;&#19968;&#26679;&#65292;&#23478;&#24237;&#25972;&#27905;&#20559;&#22909;&#21644;&#32452;&#32455;&#27169;&#24335;&#21464;&#21270;&#24191;&#27867;&#65292;&#22240;&#27492;&#39044;&#35774;&#29289;&#20307;&#20301;&#32622;&#23558;&#38480;&#21046;&#23545;&#26032;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#27905;&#24067;&#23616;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#31867;&#20284;&#20110;&#20351;&#29992;&#20250;&#35805;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;Transformer&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#21518;&#32493;&#29289;&#20307;&#30340;&#25670;&#25918;&#20301;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#8220;&#25972;&#29702;&#8221;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#26800;&#33218;&#21644;RGB&#30456;&#26426;&#22312;&#26700;&#23376;&#19978;&#32452;&#32455;&#19981;&#21516;&#22823;&#23567;&#21644;&#25968;&#37327;&#30340;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04566v2 Announce Type: replace-cross  Abstract: Addressing the challenge of organizing scattered items in domestic spaces is complicated by the diversity and subjective nature of tidiness. Just as the complexity of human language allows for multiple expressions of the same idea, household tidiness preferences and organizational patterns vary widely, so presetting object locations would limit the adaptability to new objects and environments. Inspired by advancements in natural language processing (NLP), this paper introduces a self-supervised learning framework that allows robots to understand and replicate the concept of tidiness from demonstrations of well-organized layouts, akin to using conversational datasets to train Large Language Models(LLM). We leverage a transformer neural network to predict the placement of subsequent objects. We demonstrate a ``knolling'' system with a robotic arm and an RGB camera to organize items of varying sizes and quantities on a table. Our 
&lt;/p&gt;</description></item><item><title>LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2309.13788</link><description>&lt;p&gt;
&#33021;&#22815;&#26816;&#27979;&#21040;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLM-Generated Misinformation Be Detected?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13788
&lt;/p&gt;
&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;LLMs&#65288;&#22914;ChatGPT&#65289;&#21487;&#33021;&#34987;&#21033;&#29992;&#26469;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#32473;&#22312;&#32447;&#23433;&#20840;&#21644;&#20844;&#20247;&#20449;&#20219;&#24102;&#26469;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#20250;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#36896;&#25104;&#26356;&#22823;&#21361;&#23475;?&#25105;&#20204;&#25552;&#20986;&#20174;&#26816;&#27979;&#38590;&#24230;&#30340;&#35282;&#24230;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21033;&#29992;LLMs&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#28508;&#22312;&#30495;&#23454;&#19990;&#30028;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#30456;&#27604;&#65292;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#23545;&#20154;&#31867;&#21644;&#26816;&#27979;&#22120;&#26469;&#35828;&#26356;&#38590;&#26816;&#27979;&#65292;&#36825;&#34920;&#26126;&#23427;&#21487;&#33021;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#28508;&#22312;&#22320;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
&lt;/p&gt;</description></item><item><title>LeBenchmark 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#26500;&#24314;&#27861;&#35821;&#35821;&#38899;&#25216;&#26415;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;SSL&#27169;&#22411;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2309.05472</link><description>&lt;p&gt;
LeBenchmark 2.0&#65306;&#29992;&#20110;&#33258;&#30417;&#30563;&#27861;&#34920;&#31034;&#27861;&#35821;&#35821;&#38899;&#30340;&#26631;&#20934;&#21270;&#12289;&#21487;&#22797;&#21046;&#21644;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05472
&lt;/p&gt;
&lt;p&gt;
LeBenchmark 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#26500;&#24314;&#27861;&#35821;&#35821;&#38899;&#25216;&#26415;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;SSL&#27169;&#22411;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#12290;&#35821;&#38899;&#22788;&#29702;&#26497;&#22823;&#21463;&#30410;&#20110;SSL&#65292;&#22240;&#20026;&#24403;&#21069;&#22823;&#37096;&#20998;&#39046;&#22495;&#30456;&#20851;&#20219;&#21153;&#29616;&#22312;&#37117;&#26159;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LeBenchmark 2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#26500;&#24314;&#37197;&#22791;SSL&#30340;&#27861;&#35821;&#35821;&#38899;&#25216;&#26415;&#30340;&#24320;&#28304;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#26377;&#25991;&#26723;&#35760;&#24405;&#12289;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#38271;&#36798;14,000&#23567;&#26102;&#30340;&#24322;&#26500;&#35821;&#38899;&#65292;&#21313;&#20010;&#39044;&#35757;&#32451;&#30340;SSL wav2vec 2.0 &#27169;&#22411;&#65292;&#21253;&#21547;&#20174;2600&#19975;&#21040;10&#20159;&#21487;&#23398;&#20064;&#21442;&#25968;&#19982;&#31038;&#21306;&#20849;&#20139;&#65292;&#24182;&#19988;&#21253;&#21547;&#30001;&#20845;&#20010;&#19979;&#28216;&#20219;&#21153;&#32452;&#25104;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#34917;&#20805;&#29616;&#26377;&#22522;&#20934;&#12290;LeBenchmark 2.0 &#36824;&#23545;&#20110;&#35821;&#38899;&#30340;&#39044;&#35757;&#32451;SSL&#27169;&#22411;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#20923;&#32467;&#19982;&#24494;&#35843;&#19979;&#28216;&#27169;&#22411;&#20197;&#21450;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05472v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) is at the origin of unprecedented improvements in many different domains including computer vision and natural language processing. Speech processing drastically benefitted from SSL as most of the current domain-related tasks are now being approached with pre-trained models. This work introduces LeBenchmark 2.0 an open-source framework for assessing and building SSL-equipped French speech technologies. It includes documented, large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to one billion learnable parameters shared with the community, and an evaluation protocol made of six downstream tasks to complement existing benchmarks. LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for speech with the investigation of frozen versus fine-tuned downstream models, task-agnostic ve
&lt;/p&gt;</description></item><item><title>MotionGPT&#26159;&#19968;&#27454;&#33021;&#22815;&#21033;&#29992;&#22810;&#27169;&#24577;&#25511;&#21046;&#20449;&#21495;&#29983;&#25104;&#36830;&#32493;&#20154;&#31867;&#21160;&#20316;&#30340;&#36890;&#29992;&#36816;&#21160;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2306.10900</link><description>&lt;p&gt;
MotionGPT: &#24494;&#35843;&#30340;LLM&#26159;&#36890;&#29992;&#30340;&#36816;&#21160;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10900
&lt;/p&gt;
&lt;p&gt;
MotionGPT&#26159;&#19968;&#27454;&#33021;&#22815;&#21033;&#29992;&#22810;&#27169;&#24577;&#25511;&#21046;&#20449;&#21495;&#29983;&#25104;&#36830;&#32493;&#20154;&#31867;&#21160;&#20316;&#30340;&#36890;&#29992;&#36816;&#21160;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#23383;&#20154;&#31867;&#30340;&#26032;&#20852;&#38656;&#27714;&#65292;&#20174;&#32473;&#23450;&#21160;&#20316;&#25551;&#36848;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#31867;&#21160;&#20316;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#20316;&#21697;&#22312;&#30452;&#25509;&#20174;&#25991;&#26412;&#21160;&#20316;&#25551;&#36848;&#29983;&#25104;&#21160;&#20316;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#36890;&#24120;&#21482;&#25903;&#25345;&#21333;&#19968;&#25511;&#21046;&#20449;&#21495;&#27169;&#24577;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#25968;&#23383;&#20154;&#31867;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Motion General-Purpose generaTor (MotionGPT)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#22810;&#27169;&#24577;&#25511;&#21046;&#20449;&#21495;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#21333;&#24103;&#23039;&#21183;&#65289;&#26469;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#20449;&#21495;&#35270;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#29305;&#27530;&#36755;&#20837;&#26631;&#35760;&#65292;&#29983;&#25104;&#36830;&#32493;&#30340;&#20154;&#31867;&#21160;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22810;&#27169;&#24577;&#25511;&#21046;&#20449;&#21495;&#37327;&#21270;&#20026;&#31163;&#25955;&#30721;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21046;&#23450;&#20026;&#32479;&#19968;&#30340;&#25552;&#31034;&#25351;&#20196;&#65292;&#35201;&#27714;LLMs&#29983;&#25104;&#21160;&#20316;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340; MotionGPT &#23637;&#31034;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20154;&#31867;&#21160;&#20316;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#22810;&#27169;&#24577;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.10900v2 Announce Type: replace-cross  Abstract: Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multim
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;</title><link>https://arxiv.org/abs/2305.16877</link><description>&lt;p&gt;
&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning with Dual Expectile-Quantile Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16877
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36817;&#20284;&#25972;&#20010;&#22238;&#25253;&#20998;&#24067;&#65292;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#29615;&#22659;&#26679;&#26412;&#12290;&#24120;&#29992;&#30340;&#22522;&#20110;&#19981;&#23545;&#31216;$L_1$&#25439;&#22833;&#30340;&#20998;&#24067;&#24335;RL&#30340;&#20998;&#20301;&#22238;&#24402;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;&#30340;&#26041;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#25928;&#30340;&#28151;&#21512;&#19981;&#23545;&#31216;$L_1$-$L_2$ Huber&#25439;&#22833;&#26469;&#25913;&#36827;&#24448;&#24448;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#20998;&#24067;&#20272;&#35745;&#20445;&#35777;&#28040;&#22833;&#20102;&#65292;&#25105;&#20204;&#23454;&#35777;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#20998;&#24067;&#20250;&#36805;&#36895;&#25910;&#25947;&#21040;&#20854;&#22343;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#26399;&#26395;&#22238;&#24402;&#30456;&#23545;&#24212;&#30340;&#19981;&#23545;&#31216;$L_2$&#25439;&#22833;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#12290;&#21463;&#21040;$L_2$&#20026;&#22522;&#30784;&#23398;&#20064;&#25928;&#29575;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#21644;&#20998;&#20301;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16877v2 Announce Type: replace-cross  Abstract: Distributional reinforcement learning (RL) has proven useful in multiple benchmarks as it enables approximating the full distribution of returns and makes a better use of environment samples. The commonly used quantile regression approach to distributional RL -- based on asymmetric $L_1$ losses -- provides a flexible and effective way of learning arbitrary return distributions. In practice, it is often improved by using a more efficient, hybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by doing so, distributional estimation guarantees vanish, and we empirically observe that the estimated distribution rapidly collapses to its mean. Indeed, asymmetric $L_2$ losses, corresponding to expectile regression, cannot be readily used for distributional temporal difference learning. Motivated by the efficiency of $L_2$-based learning, we propose to jointly learn expectiles and quantiles of the return distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;14&#20010;&#19981;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#35299;&#20915;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#21547;45&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20026;NLP&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2305.12544</link><description>&lt;p&gt;
&#19968;&#20999;&#37117;&#24050;&#35299;&#20915;&#20102;&#21527;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26410;&#35299;&#20915;&#30340;&#24320;&#25918;&#24615;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;14&#20010;&#19981;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#35299;&#20915;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#21547;45&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20026;NLP&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#29983;&#25104;&#24335;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#24471;&#20197;&#37096;&#32626;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#19968;&#20010;&#35823;&#23548;&#24615;&#30340;&#20844;&#20247;&#35805;&#35821;&#65292;&#8220;&#19968;&#20999;&#37117;&#24050;&#35299;&#20915;&#8221;&#12290;&#19981;&#36275;&#20026;&#22855;&#30340;&#26159;&#65292;&#36825;&#21453;&#36807;&#26469;&#20351;&#24471;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21018;&#24320;&#22987;&#32844;&#19994;&#29983;&#28079;&#30340;&#20154;&#65292;&#25285;&#24515;&#20182;&#20204;&#24212;&#35813;&#19987;&#27880;&#20110;&#21738;&#20123;&#30740;&#31350;&#39046;&#22495;&#12290;&#19968;&#20999;&#37117;&#24050;&#35299;&#20915;&#20102;&#21527;&#65292;&#25110;&#32773;&#19981;&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;&#20309;&#65292;&#25105;&#20204;&#21487;&#20197;&#32487;&#32493;&#30740;&#31350;&#21738;&#20123;&#38382;&#39064;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25972;&#29702;&#20102;&#36866;&#21512;&#28145;&#20837;&#25506;&#31350;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21253;&#21547;45&#20010;&#30740;&#31350;&#26041;&#21521;&#30340;14&#20010;&#19981;&#38656;&#35201;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#35299;&#20915;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#34429;&#28982;&#25105;&#20204;&#30830;&#23450;&#20102;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#36824;&#26377;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#23384;&#22312;&#65307;&#25105;&#20204;&#26410;&#28085;&#30422;&#30446;&#21069;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22788;&#29702;&#30340;&#39046;&#22495;&#65292;&#20294;&#22312;&#24615;&#33021;&#19978;&#33853;&#21518;&#25110;&#32773;&#19987;&#27880;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#27426;&#36814;&#23545;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12544v2 Announce Type: replace-cross  Abstract: Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that ``it's all been solved.'' Not surprisingly, this has, in turn, made many NLP researchers -- especially those at the beginning of their careers -- worry about what NLP research area they should focus on. Has it all been solved, or what remaining questions can we work on regardless of LLMs? To address this question, this paper compiles NLP research directions rich for exploration. We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs. While we identify many research areas, many others exist; we do not cover areas currently addressed by LLMs, but where LLMs lag behind in performance or those focused on LLM development. We welcome suggestions for other research
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;INVPROP&#31639;&#27861;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#38598;&#30340;&#21069;&#20687;&#19978;&#30340;&#23646;&#24615;&#65292;&#32467;&#21512;&#20998;&#25903;&#30028;&#38480;&#20197;&#22686;&#21152;&#31934;&#24230;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;GPU&#21152;&#36895;&#65292;&#36991;&#20813;&#20102;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2302.01404</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#36793;&#30028;&#31070;&#32463;&#32593;&#32476;&#21069;&#20687;
&lt;/p&gt;
&lt;p&gt;
Provably Bounding Neural Network Preimages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.01404
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;INVPROP&#31639;&#27861;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#38598;&#30340;&#21069;&#20687;&#19978;&#30340;&#23646;&#24615;&#65292;&#32467;&#21512;&#20998;&#25903;&#30028;&#38480;&#20197;&#22686;&#21152;&#31934;&#24230;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;GPU&#21152;&#36895;&#65292;&#36991;&#20813;&#20102;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#39564;&#35777;&#24037;&#20316;&#20391;&#37325;&#20110;&#38480;&#23450;&#32473;&#23450;&#36755;&#20837;&#38598;&#23545;&#24212;&#30340;&#36755;&#20986;&#38598;&#65288;&#20363;&#22914;&#65292;&#26631;&#20934;&#36755;&#20837;&#30340;&#26377;&#30028;&#25200;&#21160;&#65289;&#12290;&#20294;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#35768;&#22810;&#24212;&#29992;&#24773;&#26223;&#38656;&#35201;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#25110;&#32773;&#23545;&#23548;&#33268;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#38598;&#36827;&#34892;&#36807;&#24230;&#36817;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;INVPROP&#31639;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22312;&#32447;&#24615;&#32422;&#26463;&#36755;&#20986;&#38598;&#30340;&#21069;&#20687;&#19978;&#30340;&#23646;&#24615;&#65292;&#21487;&#20197;&#19982;&#20998;&#25903;&#30028;&#38480;&#32467;&#21512;&#20197;&#22686;&#21152;&#31934;&#24230;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#39640;&#25928;&#31639;&#27861;&#26159;GPU&#21152;&#36895;&#30340;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#29992;&#20110;&#36890;&#36807;&#21518;&#21521;&#21487;&#36798;&#24615;&#20998;&#26512;&#35782;&#21035;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#25511;&#21046;&#21306;&#22495;&#65292;&#39564;&#35777;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.01404v4 Announce Type: replace-cross  Abstract: Most work on the formal verification of neural networks has focused on bounding the set of outputs that correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However, many use cases of neural network verification require solving the inverse problem, or over-approximating the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set, which can be combined with branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for identifying safe control regions for a dynamical system via backward reachability analysis, verifying adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show that in certain settings, we find over-a
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36328;&#22495;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21307;&#23398;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#30142;&#30149;&#39044;&#21518;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.08559</link><description>&lt;p&gt;
&#20351;&#29992;&#36328;&#22495;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.08559
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36328;&#22495;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21307;&#23398;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#30142;&#30149;&#39044;&#21518;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23454;&#36341;&#19978;&#21462;&#20915;&#20110;&#40065;&#26834;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#33719;&#21462;&#24182;&#26631;&#35760;&#25968;&#25454;&#21487;&#33021;&#22826;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#20363;&#22914;&#20020;&#24202;&#30142;&#30149;&#27169;&#22411;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#23567;&#25968;&#25454;&#24773;&#20917;&#19979;&#22686;&#21152;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#21307;&#23398;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#30340;&#36328;&#22495;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#30142;&#30149;&#39044;&#21518;&#24314;&#27169;&#20316;&#20026;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.08559v2 Announce Type: cross  Abstract: Developing successful artificial intelligence systems in practice depends on both robust deep learning models and large, high-quality data. However, acquiring and labeling data can be prohibitively expensive and time-consuming in many real-world applications, such as clinical disease models. Self-supervised learning has demonstrated great potential in increasing model accuracy and robustness in small data regimes. In addition, many clinical imaging and disease modeling applications rely heavily on regression of continuous quantities. However, the applicability of self-supervised learning for these medical-imaging regression tasks has not been extensively studied. In this study, we develop a cross-domain self-supervised learning approach for disease prognostic modeling as a regression problem using medical images as input. We demonstrate that self-supervised pretraining can improve the prediction of Alzheimer's Disease progression from 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#35745;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2210.06891</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#39537;&#21160;&#29305;&#24449;&#36873;&#25321;&#30340;&#22810;&#36890;&#36947;&#25104;&#20687;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#35745;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#23454;&#39564;&#35774;&#35745;&#33539;&#24335;&#65292;&#26088;&#22312;&#32553;&#30701;&#37319;&#38598;&#26102;&#38388;&#12289;&#38477;&#20302;&#25104;&#26412;&#12289;&#21152;&#36895;&#25104;&#20687;&#35774;&#22791;&#30340;&#37096;&#32626;&#12290;&#24403;&#21069;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#19978;&#65292;&#24182;&#35201;&#27714;&#23545;&#29305;&#23450;&#27169;&#22411;&#36827;&#34892;&#35268;&#33539;&#65292;&#32780;&#22312;&#25104;&#20687;&#39046;&#22495;&#65292;&#20854;&#20182;&#20219;&#21153;&#21487;&#33021;&#39537;&#21160;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25104;&#20687;&#24212;&#29992;&#20013;&#23548;&#33268;&#38590;&#20197;&#27714;&#35299;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#35774;&#35745;&#33539;&#24335;&#65292;&#21516;&#26102;&#20248;&#21270;&#35774;&#35745;&#65288;&#22270;&#20687;&#36890;&#36947;&#38598;&#65289;&#24182;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25191;&#34892;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#31354;&#38388;&#19978;&#23494;&#38598;&#37319;&#26679;&#25968;&#25454;&#65288;&#35768;&#22810;&#22270;&#20687;&#36890;&#36947;&#65289;&#36827;&#34892;&#20102;&#23569;&#37327;&#37319;&#38598;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#39044;&#20808;&#25351;&#23450;&#23610;&#23544;&#30340;&#26368;&#20339;&#25903;&#25345;&#20219;&#21153;&#30340;&#36890;&#36947;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06891v3 Announce Type: replace-cross  Abstract: This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experime
&lt;/p&gt;</description></item><item><title>&#23454;&#39564;&#23460;&#33258;&#21160;&#21270;&#26426;&#22120;&#20154;&#36890;&#36807;&#23398;&#20064;&#25216;&#33021;&#21152;&#36895;&#23454;&#39564;&#23460;&#33258;&#21160;&#21270;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26679;&#21697;&#21038;&#21462;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#23454;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2209.14875</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#21152;&#36895;&#23454;&#39564;&#23460;&#33258;&#21160;&#21270;&#36827;&#34892;&#26679;&#21697;&#21038;&#21462;
&lt;/p&gt;
&lt;p&gt;
Accelerating Laboratory Automation Through Robot Skill Learning For Sample Scraping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.14875
&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#23460;&#33258;&#21160;&#21270;&#26426;&#22120;&#20154;&#36890;&#36807;&#23398;&#20064;&#25216;&#33021;&#21152;&#36895;&#23454;&#39564;&#23460;&#33258;&#21160;&#21270;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26679;&#21697;&#21038;&#21462;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#23454;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#23460;&#33258;&#21160;&#21270;&#26426;&#22120;&#20154;&#22312;&#33258;&#20027;&#23454;&#39564;&#20013;&#30340;&#20351;&#29992;&#20026;&#20943;&#36731;&#31185;&#23398;&#23478;&#32321;&#29712;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#26465;&#21560;&#24341;&#20154;&#30340;&#36884;&#24452;&#65292;&#21516;&#26102;&#21152;&#24555;&#20102;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#21046;&#33647;&#31561;&#28909;&#28857;&#38382;&#39064;&#30340;&#26448;&#26009;&#21457;&#29616;&#36807;&#31243;&#12290;&#23613;&#31649;&#19968;&#20123;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#24050;&#32463;&#21487;&#20197;&#20174;&#33258;&#21160;&#21270;&#20013;&#21463;&#30410;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#19981;&#21516;&#24037;&#20855;&#12289;&#21270;&#23398;&#21697;&#21644;&#29627;&#29827;&#22120;&#30399;&#26102;&#38656;&#35201;&#39640;&#27700;&#24179;&#30340;&#21160;&#20316;&#21151;&#33021;&#21644;&#28789;&#24039;&#24615;&#65292;&#26679;&#21697;&#21046;&#22791;&#20173;&#28982;&#26159;&#25163;&#21160;&#36827;&#34892;&#30340;&#12290;&#21270;&#23398;&#39046;&#22495;&#30340;&#22522;&#26412;&#24037;&#20316;&#27969;&#31243;&#20043;&#19968;&#26159;&#32467;&#26230;&#65292;&#20854;&#20013;&#19968;&#20010;&#24212;&#29992;&#26159;&#22810;&#24577;&#31579;&#36873;&#65292;&#21363;&#20174;&#26230;&#20307;&#20013;&#33719;&#24471;&#19977;&#32500;&#20998;&#23376;&#32467;&#26500;&#12290;&#23545;&#20110;&#36825;&#19968;&#36807;&#31243;&#65292;&#23613;&#21487;&#33021;&#22810;&#22320;&#24674;&#22797;&#26679;&#21697;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#21512;&#25104;&#20998;&#23376;&#22312;&#26102;&#38388;&#21644;&#37329;&#38065;&#19978;&#37117;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#21270;&#23398;&#23478;&#22312;&#25104;&#20687;&#26495;&#20256;&#36865;&#20043;&#21069;&#29992;&#21038;&#29942;&#22120;&#21038;&#38500;&#26679;&#21697;&#20869;&#23481;&#12290;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.14875v2 Announce Type: replace-cross  Abstract: The use of laboratory robotics for autonomous experiments offers an attractive route to alleviate scientists from tedious tasks while accelerating material discovery for topical issues such as climate change and pharmaceuticals. While some experimental workflows can already benefit from automation, sample preparation is still carried out manually due to the high level of motor function and dexterity required when dealing with different tools, chemicals, and glassware. A fundamental workflow in chemical fields is crystallisation, where one application is polymorph screening, i.e., obtaining a three dimensional molecular structure from a crystal. For this process, it is of utmost importance to recover as much of the sample as possible since synthesising molecules is both costly in time and money. To this aim, chemists scrape vials to retrieve sample contents prior to imaging plate transfer. Automating this process is challenging 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#26426;&#26800;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#26469;&#33258;90&#22810;&#31687;&#25991;&#31456;&#21644;140&#20010;&#25968;&#25454;&#34920;&#30340;&#22823;&#37327;&#23454;&#39564;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2209.12605</link><description>&lt;p&gt;
MechProNet&#65306;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26426;&#26800;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MechProNet: Machine Learning Prediction of Mechanical Properties in Metal Additive Manufacturing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.12605
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#26426;&#26800;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#26469;&#33258;90&#22810;&#31687;&#25991;&#31456;&#21644;140&#20010;&#25968;&#25454;&#34920;&#30340;&#22823;&#37327;&#23454;&#39564;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#65288;MAM&#65289;&#20013;&#30340;&#26426;&#26800;&#24615;&#33021;&#23545;&#20110;&#30830;&#20445;&#21360;&#21046;&#38646;&#37096;&#20214;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#20197;&#21450;&#20854;&#36866;&#29992;&#20110;&#29305;&#23450;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#27979;&#26426;&#26800;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;90&#22810;&#31687;MAM&#25991;&#31456;&#21644;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#34920;&#20013;&#25910;&#38598;&#20102;&#22823;&#37327;&#23454;&#39564;&#25968;&#25454;&#65292;&#21253;&#25324;140&#20010;&#19981;&#21516;&#30340;MAM&#25968;&#25454;&#34920;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#25324;MAM&#22788;&#29702;&#26465;&#20214;&#12289;&#26426;&#22120;&#12289;&#26448;&#26009;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.12605v2 Announce Type: replace-cross  Abstract: Predicting mechanical properties in metal additive manufacturing (MAM) is essential for ensuring the performance and reliability of printed parts, as well as their suitability for specific applications. However, conducting experiments to estimate mechanical properties in MAM processes can be laborious and expensive, and they are often limited to specific materials and processes. Machine learning (ML) methods offer a more flexible and cost-effective approach to predicting mechanical properties based on processing parameters and material properties. In this study, we introduce a comprehensive framework for benchmarking ML models for predicting mechanical properties. We compiled an extensive experimental dataset from over 90 MAM articles and data sheets from a diverse range of sources, encompassing 140 different MAM data sheets. This dataset includes information on MAM processing conditions, machines, materials, and resulting mech
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#65292;&#22788;&#29702;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2209.04589</link><description>&lt;p&gt;
&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#22240;&#26524;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Causal Intervention for Fairness in Multi-behavior Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.04589
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#65292;&#22788;&#29702;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#20174;&#21508;&#31181;&#29992;&#25143;&#34892;&#20026;&#20013;&#23398;&#20064;&#29992;&#25143;&#20852;&#36259;&#65292;&#21253;&#25324;&#28857;&#20987;&#21644;&#28857;&#20987;&#21518;&#30340;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#28857;&#36190;&#21644;&#25910;&#34255;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34892;&#20026;&#19981;&#21487;&#36991;&#20813;&#22320;&#34920;&#29616;&#20986;&#27969;&#34892;&#24230;&#20559;&#24046;&#65292;&#23548;&#33268;&#19968;&#20123;&#19981;&#20844;&#24179;&#38382;&#39064;&#65306;1&#65289;&#23545;&#20110;&#30456;&#20284;&#36136;&#37327;&#30340;&#29289;&#21697;&#65292;&#26356;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#20250;&#33719;&#24471;&#26356;&#22810;&#26333;&#20809;&#65307;2&#65289;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#27969;&#34892;&#24230;&#36739;&#20302;&#30340;&#21463;&#27426;&#36814;&#29289;&#21697;&#21487;&#33021;&#20250;&#33719;&#24471;&#26356;&#22810;&#26333;&#20809;&#12290;&#29616;&#26377;&#24037;&#20316;&#22312;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#24046;&#26041;&#38754;&#30450;&#30446;&#28040;&#38500;&#20559;&#35265;&#65292;&#36890;&#24120;&#24573;&#30053;&#29289;&#21697;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#21516;&#29992;&#25143;&#34892;&#20026;&#20043;&#38388;&#65288;&#20363;&#22914;&#36716;&#21270;&#29575;&#65289;&#30340;&#20851;&#31995;&#23454;&#38469;&#19978;&#21453;&#26144;&#20102;&#29289;&#21697;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22788;&#29702;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#20132;&#20114;&#29983;&#25104;&#36807;&#31243;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.04589v2 Announce Type: replace-cross  Abstract: Recommender systems usually learn user interests from various user behaviors, including clicks and post-click behaviors (e.g., like and favorite). However, these behaviors inevitably exhibit popularity bias, leading to some unfairness issues: 1) for items with similar quality, more popular ones get more exposure; and 2) even worse the popular items with lower popularity might receive more exposure. Existing work on mitigating popularity bias blindly eliminates the bias and usually ignores the effect of item quality. We argue that the relationships between different user behaviors (e.g., conversion rate) actually reflect the item quality. Therefore, to handle the unfairness issues, we propose to mitigate the popularity bias by considering multiple user behaviors.   In this work, we examine causal relationships behind the interaction generation procedure in multi-behavior recommendation. Specifically, we find that: 1) item popula
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#23618;&#34507;&#31957;&#30340;&#20844;&#24179;&#20998;&#37197;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#8220;&#19968;&#23545;&#20992;&#8221;&#35745;&#31639;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#38024;&#23545;&#19981;&#21516;&#20195;&#29702;&#21644;&#23618;&#25968;&#30340;&#21487;&#34892;&#27604;&#20363;&#20998;&#37197;&#35745;&#31639;&#36807;&#31243;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#25216;&#26415;&#36866;&#29992;&#20110;&#20219;&#24847;&#25968;&#37327;&#30340;&#20195;&#29702;&#21644;&#23618;&#25968;&#12290;</title><link>https://arxiv.org/abs/2208.00726</link><description>&lt;p&gt;
&#22810;&#23618;&#34507;&#31957;&#30340;&#20844;&#24179;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Fair Division of Multi-layered Cakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.00726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#23618;&#34507;&#31957;&#30340;&#20844;&#24179;&#20998;&#37197;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#8220;&#19968;&#23545;&#20992;&#8221;&#35745;&#31639;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#38024;&#23545;&#19981;&#21516;&#20195;&#29702;&#21644;&#23618;&#25968;&#30340;&#21487;&#34892;&#27604;&#20363;&#20998;&#37197;&#35745;&#31639;&#36807;&#31243;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#25216;&#26415;&#36866;&#29992;&#20110;&#20219;&#24847;&#25968;&#37327;&#30340;&#20195;&#29702;&#21644;&#23618;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#23618;&#34507;&#31957;&#20999;&#21106;&#65292;&#26088;&#22312;&#22312;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#19979;&#65288;&#36830;&#32493;&#24615;&#21644;&#21487;&#34892;&#24615;&#65289;&#22312;&#19968;&#32452;&#20195;&#29702;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#22810;&#20010;&#21487;&#20998;&#36164;&#28304;&#65288;&#34507;&#31957;&#23618;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#22810;&#23618;&#34507;&#31957;&#20013;&#30340;&#26032;&#35745;&#31639;&#27169;&#22411;&#8220;&#19968;&#23545;&#20992;&#8221;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26032;&#35745;&#31639;&#27169;&#22411;&#20013;&#20026;&#20004;&#20301;&#20195;&#29702;&#21644;&#20004;&#23618;&#23384;&#22312;&#30830;&#20999;&#30340;&#22810;&#37325;&#20998;&#37197;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#19977;&#23618;&#34507;&#31957;&#19978;&#20026;&#36229;&#36807;&#19977;&#20010;&#20195;&#29702;&#36827;&#34892;&#21487;&#34892;&#19988;&#36830;&#32493;&#30340;&#25104;&#27604;&#20363;&#22810;&#37325;&#20998;&#37197;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#20219;&#20309;&#25968;&#37327;&#20026;$n\geq 2^a3$&#20010;&#20195;&#29702;&#21644;$2^a3$&#23618;&#30340;&#27604;&#20363;&#20998;&#37197;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;$a$&#26159;&#20219;&#24847;&#27491;&#25972;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.00726v2 Announce Type: replace  Abstract: We consider multi-layered cake cutting in order to fairly allocate numerous divisible resources (layers of cake) among a group of agents under two constraints: contiguity and feasibility. We first introduce a new computational model in a multi-layered cake named ``a pair of knives''. Then, we show the existence of an exact multi-allocation for two agents and two layers using the new computational model. We demonstrate the computation procedure of a feasible and contiguous proportional multi-allocation over a three-layered cake for more than three agents. Finally, we develop a technique for computing proportional allocations for any number $n\geq 2^a3$ of agents and $2^a3$ layers, where $a$ is any positive integer.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#20250;&#24615;&#31227;&#21160;&#20013;&#32487;&#65292;&#25552;&#20986;&#20102;FedMobile&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#20026;$O(\frac{1}{\sqrt{NT}})$&#12290;</title><link>https://arxiv.org/abs/2206.04742</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#20250;&#24615;&#31227;&#21160;&#20013;&#32487;&#21152;&#36895;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Accelerating Asynchronous Federated Learning Convergence via Opportunistic Mobile Relaying
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.04742
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#20250;&#24615;&#31227;&#21160;&#20013;&#32487;&#65292;&#25552;&#20986;&#20102;FedMobile&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#20026;$O(\frac{1}{\sqrt{NT}})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#31227;&#21160;&#32593;&#32476;&#29615;&#22659;&#19979;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#22823;&#22810;&#25968;FL&#31639;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#22987;&#32456;&#21487;&#29992;&#65292;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#31995;&#32479;&#20013;&#24182;&#38750;&#22914;&#27492;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#31227;&#21160;&#24615;&#23545;&#24322;&#27493;FL&#25910;&#25947;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21033;&#29992;&#31227;&#21160;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#21478;&#19968;&#23458;&#25143;&#31471;&#20805;&#24403;&#20013;&#32487;&#19982;&#26381;&#21153;&#22120;&#38388;&#25509;&#36890;&#20449;&#65292;&#21019;&#36896;&#39069;&#22806;&#30340;&#36890;&#20449;&#26426;&#20250;&#12290;&#36825;&#20351;&#23458;&#25143;&#31471;&#33021;&#22815;&#26356;&#26089;&#22320;&#19978;&#20256;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#25110;&#25509;&#25910;&#26356;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FL&#31639;&#27861;&#65292;&#31216;&#20026;FedMobile&#65292;&#35813;&#31639;&#27861;&#34701;&#21512;&#20102;&#26426;&#20250;&#24615;&#20013;&#32487;&#65292;&#24182;&#35299;&#20915;&#20102;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20013;&#32487;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;FedMobile&#23454;&#29616;&#20102;&#25910;&#25947;&#36895;&#29575;&#20026;$O(\frac{1}{\sqrt{NT}})$&#65292;&#20854;&#20013;$N$&#26159;&#23458;&#25143;&#31471;&#25968;&#37327;&#65292;$T$&#26159;&#23458;&#25143;&#31471;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.04742v2 Announce Type: replace-cross  Abstract: This paper presents a study on asynchronous Federated Learning (FL) in a mobile network setting. The majority of FL algorithms assume that communication between clients and the server is always available, however, this is not the case in many real-world systems. To address this issue, the paper explores the impact of mobility on the convergence performance of asynchronous FL. By exploiting mobility, the study shows that clients can indirectly communicate with the server through another client serving as a relay, creating additional communication opportunities. This enables clients to upload local model updates sooner or receive fresher global models. We propose a new FL algorithm, called FedMobile, that incorporates opportunistic relaying and addresses key questions such as when and how to relay. We prove that FedMobile achieves a convergence rate $O(\frac{1}{\sqrt{NT}})$, where $N$ is the number of clients and $T$ is the numbe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#23616;&#37096;&#35299;&#37322;&#65292;&#21253;&#25324;&#19982;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30456;&#20851;&#30340;&#39044;&#27979;&#35299;&#37322;&#12289;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#25506;&#27979;&#27169;&#22411;&#38544;&#34255;&#29366;&#24577;&#21644;&#35789;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2103.11072</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26412;&#22320;&#35299;&#37322;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Local Interpretations for Explainable Natural Language Processing: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2103.11072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#23616;&#37096;&#35299;&#37322;&#65292;&#21253;&#25324;&#19982;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30456;&#20851;&#30340;&#39044;&#27979;&#35299;&#37322;&#12289;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#25506;&#27979;&#27169;&#22411;&#38544;&#34255;&#29366;&#24577;&#21644;&#35789;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36807;&#21435;&#21313;&#24180;&#38388;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#19981;&#36879;&#26126;&#24615;&#30340;&#25265;&#24616;&#20063;&#22312;&#22686;&#21152;&#65292;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36879;&#26126;&#24230;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#30340;&#24320;&#22987;&#38454;&#27573;&#23545;&#21487;&#35299;&#37322;&#24615;&#19968;&#35789;&#21450;&#20854;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35752;&#35770;&#12290;&#26412;&#27425;&#35843;&#26597;&#20013;&#25910;&#38598;&#21644;&#24635;&#32467;&#30340;&#26041;&#27861;&#20165;&#28041;&#21450;&#23616;&#37096;&#35299;&#37322;&#65292;&#24182;&#20855;&#20307;&#20998;&#20026;&#19977;&#31867;&#65306;1&#65289;&#36890;&#36807;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#65307;2&#65289;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#35299;&#37322;&#65307;3&#65289;&#25506;&#27979;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#21333;&#35789;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2103.11072v3 Announce Type: replace-cross  Abstract: As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for Natural Language Processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term interpretability and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are specifically divided into three categories: 1) interpreting the model's predictions through related input features; 2) interpreting through natural language explanation; 3) probing the hidden states of models and word representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#38598;&#25104;&#31574;&#30053;&#26799;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#20462;&#25913;&#21518;&#30340;Transformer&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#36817;&#30740;&#31350;&#21644;&#24191;&#27867;&#37319;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2401.16580</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Attention-based Reinforcement Learning for Combinatorial Optimization: Application to Job Shop Scheduling Problem. (arXiv:2401.16580v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#38598;&#25104;&#31574;&#30053;&#26799;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#20462;&#25913;&#21518;&#30340;Transformer&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#36817;&#30740;&#31350;&#21644;&#24191;&#27867;&#37319;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#26159;&#19968;&#31867;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20027;&#35201;&#36890;&#36807;&#31934;&#30830;&#25110;&#36817;&#20284;&#30340;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#31934;&#30830;&#35299;&#23545;&#20110;&#23454;&#38469;&#38382;&#39064;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#37319;&#29992;&#36817;&#20284;&#35299;&#20915;&#26041;&#27861;&#65292;&#20063;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#19981;&#33021;&#24212;&#29992;&#20110;&#26032;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23558;&#31574;&#30053;&#26799;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#20462;&#25913;&#21518;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#32467;&#26524;&#26159;&#65292;&#25105;&#20204;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#35757;&#32451;&#30340;&#23398;&#20064;&#32773;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#26410;&#21442;&#19982;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#36817;&#30740;&#31350;&#21644;&#24191;&#27867;&#37319;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Job shop scheduling problems are one of the most important and challenging combinatorial optimization problems that have been tackled mainly by exact or approximate solution approaches. However, finding an exact solution can be infeasible for real-world problems, and even with an approximate solution approach, it can require a prohibitive amount of time to find a near-optimal solution, and the found solutions are not applicable to new problems in general. To address these challenges, we propose an attention-based reinforcement learning method for the class of job shop scheduling problems by integrating policy gradient reinforcement learning with a modified transformer architecture. An important result is that our trained learners in the proposed method can be reused to solve large-scale problems not used in training and demonstrate that our approach outperforms the results of recent studies and widely adopted heuristic rules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#32452;&#20214;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14405</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#36335;&#24452;&#65306;&#36890;&#36807;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;Transformer
&lt;/p&gt;
&lt;p&gt;
Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#32452;&#20214;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#38899;&#39057;&#25110;&#28857;&#20113;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;ImageNet&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#30446;&#26631;&#27169;&#24577;&#30340;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#20182;&#27169;&#24577;&#26080;&#20851;&#65292;&#36825;&#19982;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#37197;&#23545;&#25968;&#25454;&#65288;&#22914;CLIP&#65289;&#25110;&#20132;&#38169;&#25968;&#25454;&#30340;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;-&#32473;&#23450;&#30446;&#26631;&#27169;&#24577;&#21644;&#35774;&#35745;&#29992;&#20110;&#35813;&#27169;&#24577;&#30340;Transformer&#65292;&#25105;&#20204;&#20351;&#29992;&#20351;&#29992;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#36741;&#21161;Transformer&#65292;&#24182;&#26500;&#24314;&#36335;&#24452;&#26469;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#30340;&#32452;&#20214;&#65292;&#20197;&#20415;&#30446;&#26631;&#27169;&#24577;&#30340;&#25968;&#25454;&#21487;&#20197;&#34987;&#20004;&#20010;&#27169;&#22411;&#22788;&#29702;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20174;&#20004;&#20010;&#27169;&#24577;&#33719;&#24471;&#30340;Transformer&#30340;&#36890;&#29992;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;&#20316;&#20026;&#20855;&#20307;&#23454;&#29616;&#65292;&#25105;&#20204;&#36890;&#24120;&#20351;&#29992;&#29305;&#23450;&#27169;&#24577;&#30340;tokenizer&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;head&#65292;&#20294;&#26159;&#21033;&#29992;&#36741;&#21161;&#27169;&#22411;&#30340;Transformer block&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13652</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#30340;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#19981;&#36830;&#32493;&#30028;&#38754;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#35299;&#20915;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#12290;&#35757;&#32451;&#36807;&#30340;GINNs&#22312;&#31232;&#30095;&#32593;&#26684;&#19978;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#28857;&#65292;&#24182;&#21033;&#29992;&#26500;&#24314;&#22312;&#32593;&#26684;&#19978;&#30340;&#22270;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#31639;&#27861;&#29992;&#20110;&#19968;&#33324;&#30340;&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#26131;&#20110;&#24212;&#29992;&#24615;&#12290;&#22312;&#32500;&#24230;n=2&#21644;n=4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;GINNs&#22312;&#26816;&#27979;&#19981;&#36830;&#32493;&#30028;&#38754;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;GINNs&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#21508;&#31181;&#31639;&#27861;&#20013;&#24182;&#20849;&#20139;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12873</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#30340;&#21453;&#39304;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;: &#23558;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#20805;&#20998;&#24314;&#27169;&#20154;&#31867;&#20559;&#22909;&#23548;&#33268;&#22870;&#21169;&#27169;&#22411;&#22312;&#21033;&#29992;&#20154;&#30340;&#21453;&#39304;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36136;&#37327;&#20272;&#35745;(QE)&#22312;&#36807;&#21435;&#20004;&#24180;&#20013;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#23601;&#33021;&#20934;&#30830;&#39044;&#27979;&#32473;&#23450;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;QE&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;(&#22522;&#20110;QE&#30340;&#22870;&#21169;&#27169;&#22411;)&#26469;&#39044;&#27979;&#20154;&#30340;&#20559;&#22909;&#20197;&#36827;&#34892;&#21453;&#39304;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#20102;&#22312;&#22522;&#20110;QE&#30340;&#21453;&#39304;&#35757;&#32451;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#34920;&#29616;&#20026;&#22870;&#21169;&#30340;&#22686;&#21152;&#32780;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35748;&#20026;QE&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#32763;&#35793;&#30340;&#39640;&#22870;&#21169;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#20248;&#21270;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#65292;&#24182;&#20026;QE&#27169;&#22411;&#28155;&#21152;&#20102;&#19968;&#20010;&#24809;&#32602;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04536</link><description>&lt;p&gt;
&#36890;&#36807;&#35848;&#21028;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#12289;&#32452;&#32455;&#21644;&#25919;&#24220;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#23637;&#31034;&#31867;&#20284;&#20195;&#29702;&#34892;&#20026;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#38543;&#30528;LM&#34987;&#37319;&#29992;&#26469;&#25191;&#34892;&#36234;&#26469;&#36234;&#20855;&#26377;&#33258;&#20027;&#24615;&#30340;&#20219;&#21153;&#65292;&#36843;&#20999;&#38656;&#35201;&#21487;&#38752;&#19988;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#24403;&#21069;&#20027;&#35201;&#26159;&#38745;&#24577;&#30340;LM&#22522;&#20934;&#26080;&#27861;&#24456;&#22909;&#22320;&#35780;&#20272;&#27492;&#31867;&#21160;&#24577;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#26469;&#20849;&#21516;&#35780;&#20272;LM&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#20849;&#21516;&#20219;&#21153;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#35848;&#21028;&#28216;&#25103;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#35843;&#25972;&#22797;&#26434;&#24615;&#65292;&#24182;&#36991;&#20813;&#35780;&#20272;&#20013;&#30340;&#24847;&#22806;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26469;&#33258;&#20960;&#20010;&#20027;&#35201;&#20379;&#24212;&#21830;&#30340;&#20845;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;LM&#22312;&#21508;&#31181;&#35848;&#21028;&#28216;&#25103;&#19978;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;i&#65289;&#24320;&#28304;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01851</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#21147;&#37327;&#65306;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#23545;&#33021;&#28304;&#38656;&#27714;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#21464;&#21270;&#23545;&#30456;&#24212;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25552;&#39640;&#21644;&#39640;&#24615;&#33021;&#30828;&#20214;&#30340;&#21019;&#26032;&#25512;&#21160;&#20102;&#22797;&#26434;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20294;&#20063;&#25903;&#25345;&#20102;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#28040;&#38544;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#21152;&#20154;&#20204;&#23545;&#19968;&#33324;&#35757;&#32451;&#21442;&#25968;&#21644;&#36807;&#31243;&#65288;&#20174;&#23398;&#20064;&#29575;&#21040;&#25209;&#37327;&#22823;&#23567;&#20877;&#21040;&#30693;&#35782;&#20256;&#36755;&#65289;&#30340;&#33021;&#28304;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21021;&#22987;&#21270;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#30828;&#20214;&#37197;&#32622;&#19978;&#35780;&#20272;&#22810;&#31181;&#35774;&#32622;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#22312;&#22522;&#20934;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.01519</link><description>&lt;p&gt;
&#25506;&#32034;LLMs&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#12290;&#24515;&#29702;&#23398;&#32463;&#21382;&#20102;&#20960;&#27425;&#29702;&#35770;&#21464;&#38761;&#65292;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;LLMs&#30340;&#20351;&#29992;&#26377;&#26395;&#24320;&#21551;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;LLMs&#22914;ChatGPT&#22312;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#36716;&#21464;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;LLMs&#22312;&#35748;&#30693;&#19982;&#34892;&#20026;&#24515;&#29702;&#23398;&#12289;&#20020;&#24202;&#19982;&#21672;&#35810;&#24515;&#29702;&#23398;&#12289;&#25945;&#32946;&#19982;&#21457;&#23637;&#24515;&#29702;&#23398;&#20197;&#21450;&#31038;&#20250;&#19982;&#25991;&#21270;&#24515;&#29702;&#23398;&#31561;&#24515;&#29702;&#23398;&#20998;&#25903;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#25991;&#26412;&#29983;&#25104;&#30340;&#33021;&#21147;&#65292;&#20026;&#24515;&#29702;&#23398;&#20013;&#30340;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#12289;&#23454;&#39564;&#23545;&#35937;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#23398;&#26415;&#20889;&#20316;&#21644;&#21516;&#34892;&#35780;&#23457;&#31561;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#12290;&#34429;&#28982;LLMs&#22312;&#25512;&#21160;&#30740;&#31350;&#26041;&#27861;&#23398;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SVGDreamer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#36807;&#31243;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#32032;&#25511;&#21046;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#32534;&#36753;&#24615;&#21644;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39068;&#33394;&#12289;&#24179;&#28369;&#24230;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.16476</link><description>&lt;p&gt;
SVGDreamer&#65306;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SVGDreamer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#36807;&#31243;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#32032;&#25511;&#21046;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#32534;&#36753;&#24615;&#21644;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39068;&#33394;&#12289;&#24179;&#28369;&#24230;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#21487;&#32553;&#25918;&#30690;&#37327;&#22270;&#24418;&#65288;SVG&#65289;&#21512;&#25104;&#22312;&#22270;&#26631;&#35774;&#35745;&#21644;&#33609;&#22270;&#31561;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;SVG&#29983;&#25104;&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#21487;&#32534;&#36753;&#24615;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#22270;&#24418;&#21512;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;SVGDreamer&#12290;SVGDreamer&#25972;&#21512;&#20102;&#19968;&#31181;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#65288;SIVE&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#23558;&#21512;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#21069;&#26223;&#23545;&#35937;&#21644;&#32972;&#26223;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#21487;&#32534;&#36753;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SIVE&#36807;&#31243;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22522;&#26412;&#20803;&#32032;&#25511;&#21046;&#21644;&#27880;&#24847;&#21147;&#25513;&#34109;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26377;&#25928;&#25511;&#21046;&#21644;&#25805;&#20316;&#21508;&#20010;&#20803;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#65288;VPSD&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25991;&#26412;&#21040;SVG&#29983;&#25104;&#26041;&#27861;&#20013;&#39068;&#33394;&#36807;&#39281;&#21644;&#12289;&#30690;&#37327;&#22522;&#20803;&#36807;&#24179;&#28369;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic-driven image vectorization (SIVE) process that enables the decomposition of synthesis into foreground objects and background, thereby enhancing editability. Specifically, the SIVE process introduce attention-based primitive control and an attention-mask loss function for effective control and manipulation of individual elements. Additionally, we propose a Vectorized Particle-based Score Distillation (VPSD) approach to tackle the challenges of color over-saturation, vector primitives over-smoothing, and limited result diversity in existing text-to-SVG generation methods. Furthermore, on the basis of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2312.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#21644;&#20808;&#39564;&#30693;&#35782;&#22312;&#38544;&#31169;&#25915;&#20987;&#20013;&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24378;&#35843;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#25968;&#25454;&#24182;&#20165;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#65292;&#24378;&#35843;&#29992;&#25143;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26377;&#20851;&#38544;&#31169;&#25915;&#20987;&#30340;&#24037;&#20316;&#36890;&#36807;&#20174;&#32852;&#37030;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#30340;&#35757;&#32451;&#25991;&#26412;&#26469;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#25216;&#26415;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#38556;&#30861;&#65306;&#19968;&#20123;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#26377;&#38480;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026;1&#65289;&#65292;&#32780;&#20854;&#20182;&#25216;&#26415;&#21017;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#38590;&#20197;&#26816;&#27979;&#30340;&#29305;&#28857;&#65292;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#35774;&#32622;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#24674;&#22797;&#29575;&#12290;&#22522;&#20110;&#22522;&#26412;&#30340;&#26799;&#24230;&#21305;&#37197;&#21644;&#39046;&#22495;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#22686;&#24378;&#25915;&#20987;&#33021;&#21147;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#32423;&#21035;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#26799;&#24230;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#20449;&#21495;&#19981;&#20250;&#22312;&#21477;&#23376;&#21644;&#26631;&#35760;&#20043;&#38388;&#36827;&#34892;&#24179;&#22343;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.17513</link><description>&lt;p&gt;
&#12298;&#20302;&#31209;&#36866;&#24212;&#30340;&#34920;&#36798;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#30697;&#38453;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;LoRA&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#29702;&#35770;&#35282;&#24230;&#20998;&#26512;LoRA&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#39318;&#27425;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#65292;&#21017;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#24403;LoRA-rank&#20302;&#20110;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CP-MAE&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#36974;&#32617;&#26469;&#23454;&#29616;MRI&#25195;&#25551;&#20013;&#30340;&#20154;&#33080;&#21435;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.15778</link><description>&lt;p&gt;
&#22686;&#24378;MRI&#25195;&#25551;&#38544;&#31169;&#30340;3D&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
3D Masked Autoencoders for Enhanced Privacy in MRI Scans. (arXiv:2310.15778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CP-MAE&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#36974;&#32617;&#26469;&#23454;&#29616;MRI&#25195;&#25551;&#20013;&#30340;&#20154;&#33080;&#21435;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MRI&#25195;&#25551;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21307;&#23398;&#20449;&#24687;&#65292;&#20294;&#20063;&#21253;&#21547;&#25935;&#24863;&#21644;&#21487;&#35782;&#21035;&#20010;&#20154;&#20449;&#24687;&#65288;PII&#65289;&#65292;&#38656;&#35201;&#20445;&#25252;&#12290;&#20256;&#32479;&#30340;MRI&#25968;&#25454;&#21435;&#35782;&#21035;&#26041;&#27861;&#36890;&#36807;&#21024;&#38500;&#38544;&#31169;&#25935;&#24863;&#37096;&#20301;&#65288;&#22914;&#30524;&#30555;&#12289;&#40763;&#23376;&#31561;&#65289;&#26469;&#23454;&#29616;&#65292;&#20294;&#20250;&#24341;&#20837;&#39046;&#22495;&#36716;&#25442;&#65292;&#24433;&#21709;&#19979;&#28216;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CP-MAE&#27169;&#22411;&#65292;&#36890;&#36807;&#38754;&#37096;&#36974;&#32617;&#26469;&#23454;&#29616;&#20154;&#33080;&#21435;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
MRI scans provide valuable medical information, however they also contain sensitive and personally identifiable information (PII) that needs to be protected. Whereas MRI metadata is easily sanitized, MRI image data is a privacy risk because it contains information to render highly-realistic 3D visualizations of a patient's head, enabling malicious actors to possibly identify the subject by cross-referencing a database. Data anonymization and de-identification is concerned with ensuring the privacy and confidentiality of individuals' personal information. Traditional MRI de-identification methods remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This comes at the expense of introducing a domain shift that can throw off downstream analyses. Recently, a GAN-based approach was proposed to de-identify a patient's scan by remodeling it (e.g. changing the face) rather than by removing parts. In this work, we propose CP-MAE, a model that de-identifies the face using mask
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#36755;&#20986;&#32467;&#26524;&#36827;&#34892;&#30828;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#38169;&#35823;&#30340;&#32047;&#31215;&#29616;&#35937;&#65292;&#20445;&#35777;&#20102;&#31574;&#30053;&#25928;&#26524;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.06171</link><description>&lt;p&gt;
&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Memory-Consistent Neural Networks for Imitation Learning. (arXiv:2310.06171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#36755;&#20986;&#32467;&#26524;&#36827;&#34892;&#30828;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#38169;&#35823;&#30340;&#32047;&#31215;&#29616;&#35937;&#65292;&#20445;&#35777;&#20102;&#31574;&#30053;&#25928;&#26524;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#19987;&#23478;&#28436;&#31034;&#22823;&#22823;&#31616;&#21270;&#20102;&#31574;&#30053;&#21512;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#27169;&#20223;&#31574;&#30053;&#26469;&#35828;&#65292;&#36828;&#31163;&#35757;&#32451;&#26679;&#26412;&#30340;&#38169;&#35823;&#23588;&#20026;&#20851;&#38190;&#12290;&#21363;&#20351;&#22312;&#31574;&#30053;&#30340;&#34892;&#21160;&#36755;&#20986;&#20013;&#20986;&#29616;&#32597;&#35265;&#30340;&#38169;&#35823;&#65292;&#30001;&#20110;&#36825;&#20123;&#38169;&#35823;&#20250;&#23548;&#33268;&#19981;&#29087;&#24713;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#31574;&#30053;&#22312;&#36825;&#20123;&#29366;&#24577;&#19979;&#20173;&#26356;&#23481;&#26131;&#20986;&#38169;&#65292;&#26368;&#32456;&#23548;&#33268;&#20219;&#21153;&#22833;&#36133;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#31616;&#21333;&#30340;&#30417;&#30563;&#24335;&#8220;&#34892;&#20026;&#20811;&#38534;&#8221;&#26041;&#27861;&#65292;&#33021;&#22815;&#26041;&#20415;&#22320;&#20165;&#36890;&#36807;&#39044;&#20808;&#35760;&#24405;&#30340;&#28436;&#31034;&#26469;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33021;&#22815;&#25269;&#28040;&#38169;&#35823;&#32047;&#31215;&#29616;&#35937;&#30340;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#30340;&#8220;&#20869;&#23384;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;&#8221;(MCNN)&#36755;&#20986;&#34987;&#24378;&#21046;&#32422;&#26463;&#22312;&#19982;&#20856;&#22411;&#30340;&#8220;&#20869;&#23384;&#8221;&#35757;&#32451;&#26679;&#26412;&#30456;&#20851;&#30340;&#26126;&#30830;&#25351;&#23450;&#30340;&#20801;&#35768;&#21306;&#22495;&#20869;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;MCNN&#31574;&#30053;&#23548;&#33268;&#30340;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#20445;&#35777;&#19978;&#30028;&#12290;&#36890;&#36807;&#22312;9&#20010;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#20351;&#29992;MCNNs&#65292;&#37319;&#29992;MLP&#12289;Transformer&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, 
&lt;/p&gt;</description></item><item><title>MiniGPT-5&#20351;&#29992;&#29983;&#25104;&#20973;&#25454;&#20316;&#20026;&#26725;&#26753;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#38169;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#21644;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#26469;&#23454;&#29616;&#26080;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.02239</link><description>&lt;p&gt;
MiniGPT-5: &#36890;&#36807;&#29983;&#25104;&#20973;&#25454;&#23454;&#29616;&#20132;&#38169;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens. (arXiv:2310.02239v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02239
&lt;/p&gt;
&lt;p&gt;
MiniGPT-5&#20351;&#29992;&#29983;&#25104;&#20973;&#25454;&#20316;&#20026;&#26725;&#26753;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#38169;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#21644;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#26469;&#23454;&#29616;&#26080;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26080;&#19982;&#20262;&#27604;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#25991;&#26412;&#21465;&#36848;&#30340;&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#21069;&#27839;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#38169;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#20197;"&#29983;&#25104;&#20973;&#25454;"&#30340;&#27010;&#24565;&#20026;&#22522;&#30784;&#65292;&#20316;&#20026;&#21327;&#35843;&#22270;&#20687;&#25991;&#26412;&#36755;&#20986;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#28857;&#26159;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#26080;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#65292;&#35757;&#32451;&#36807;&#31243;&#19981;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#20840;&#38754;&#30340;&#25551;&#36848;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#20973;&#25454;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MiniGPT-5&#22312;MMDialog&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#22522;&#32447;Divter&#27169;&#22411;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#22987;&#32456;&#25552;&#20379;&#20248;&#36234;&#25110;&#21487;&#27604;&#30340;&#22810;&#27169;&#24577;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have garnered significant attention for their advancements in natural language processing, demonstrating unparalleled prowess in text comprehension and generation. Yet, the simultaneous generation of images with coherent textual narratives remains an evolving frontier. In response, we introduce an innovative interleaved vision-and-language generation technique anchored by the concept of "generative vokens," acting as the bridge for harmonized image-text outputs. Our approach is characterized by a distinctive two-staged training strategy focusing on description-free multimodal generation, where the training requires no comprehensive descriptions of images. To bolster model integrity, classifier-free guidance is incorporated, enhancing the effectiveness of vokens on image generation. Our model, MiniGPT-5, exhibits substantial improvement over the baseline Divter model on the MMDialog dataset and consistently delivers superior or comparable multimodal outputs 
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>ETGraph&#26159;&#19968;&#20010;&#36830;&#25509;&#20197;&#22826;&#22346;&#21644;Twitter&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#35760;&#24405;&#21644;Twitter&#20851;&#27880;&#25968;&#25454;&#65292;&#36890;&#36807;&#39564;&#35777;OpenSea&#30340;Twitter&#36134;&#25143;&#19982;&#20197;&#22826;&#22346;&#22320;&#22336;&#36827;&#34892;&#32465;&#23450;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#32479;&#35745;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;Twitter&#21305;&#37197;&#21644;&#38750;Twitter&#21305;&#37197;&#30340;&#20197;&#22826;&#22346;&#22320;&#22336;&#20043;&#38388;&#30340;&#32467;&#26500;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01015</link><description>&lt;p&gt;
ETGraph&#65306;&#19968;&#20010;&#36830;&#25509;&#20197;&#22826;&#22346;&#21644;Twitter&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ETGraph: A Pioneering Dataset Bridging Ethereum and Twitter. (arXiv:2310.01015v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01015
&lt;/p&gt;
&lt;p&gt;
ETGraph&#26159;&#19968;&#20010;&#36830;&#25509;&#20197;&#22826;&#22346;&#21644;Twitter&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#35760;&#24405;&#21644;Twitter&#20851;&#27880;&#25968;&#25454;&#65292;&#36890;&#36807;&#39564;&#35777;OpenSea&#30340;Twitter&#36134;&#25143;&#19982;&#20197;&#22826;&#22346;&#22320;&#22336;&#36827;&#34892;&#32465;&#23450;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#32479;&#35745;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;Twitter&#21305;&#37197;&#21644;&#38750;Twitter&#21305;&#37197;&#30340;&#20197;&#22826;&#22346;&#22320;&#22336;&#20043;&#38388;&#30340;&#32467;&#26500;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#20844;&#20849;&#21306;&#22359;&#38142;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#23545;&#21306;&#22359;&#38142;&#25968;&#25454;&#30340;&#21333;&#19968;&#20851;&#27880;&#12290;&#36825;&#31181;&#38480;&#21046;&#38480;&#21046;&#20102;&#30456;&#20851;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#19982;&#21306;&#22359;&#38142;&#20998;&#26512;&#30340;&#32467;&#21512;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#21487;&#20197;&#24471;&#20986;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#27934;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ETGraph&#65292;&#23427;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#30495;&#23454;&#22320;&#36830;&#25509;&#20102;&#20197;&#22826;&#22346;&#21644;Twitter&#65292;&#26159;&#20854;&#31867;&#21035;&#20013;&#31532;&#19968;&#20010;&#19988;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;ETGraph&#32467;&#21512;&#20102;&#20197;&#22826;&#22346;&#30340;&#20132;&#26131;&#35760;&#24405;&#65288;200&#19975;&#20010;&#33410;&#28857;&#21644;3000&#19975;&#26465;&#36793;&#65289;&#21644;Twitter&#30340;&#20851;&#27880;&#25968;&#25454;&#65288;100&#19975;&#20010;&#33410;&#28857;&#21644;300&#19975;&#26465;&#36793;&#65289;&#65292;&#23558;30667&#20010;&#20197;&#22826;&#22346;&#22320;&#22336;&#19982;&#26469;&#33258;OpenSea&#30340;&#24050;&#39564;&#35777;Twitter&#36134;&#25143;&#36827;&#34892;&#20102;&#32465;&#23450;&#12290;&#23545;ETGraph&#30340;&#35814;&#32454;&#32479;&#35745;&#20998;&#26512;&#31361;&#20986;&#20102;&#19982;Twitter&#21305;&#37197;&#21644;&#38750;Twitter&#21305;&#37197;&#30340;&#20197;&#22826;&#22346;&#22320;&#22336;&#20043;&#38388;&#30340;&#32467;&#26500;&#24046;&#24322;&#12290;&#21253;&#25324;&#20197;&#22826;&#22346;&#38142;&#36335;&#39044;&#27979;&#12289;&#34394;&#20551;&#20132;&#26131;&#20197;&#22826;&#22346;&#22320;&#22336;&#26816;&#27979;&#21644;Twitter-&#20197;&#22826;&#22346;&#21305;&#37197;&#38142;&#36335;&#39044;&#27979;&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
While numerous public blockchain datasets are available, their utility is constrained by a singular focus on blockchain data. This constraint limits the incorporation of relevant social network data into blockchain analysis, thereby diminishing the breadth and depth of insight that can be derived. To address the above limitation, we introduce ETGraph, a novel dataset that authentically links Ethereum and Twitter, marking the first and largest dataset of its kind. ETGraph combines Ethereum transaction records (2 million nodes and 30 million edges) and Twitter following data (1 million nodes and 3 million edges), bonding 30,667 Ethereum addresses with verified Twitter accounts sourced from OpenSea. Detailed statistical analysis on ETGraph highlights the structural differences between Twitter-matched and non-Twitter-matched Ethereum addresses. Extensive experiments, including Ethereum link prediction, wash-trading Ethereum addresses detection, and Twitter-Ethereum matching link prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2310.01012</link><description>&lt;p&gt;
CCA&#23478;&#26063;&#30340;&#39640;&#25928;&#31639;&#27861;&#65306;&#26080;&#32422;&#26463;&#30446;&#26631;&#19982;&#26080;&#20559;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#23398;&#20064;&#20013;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#27491;&#21017;&#21270;&#32447;&#24615;CCA&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#30340;&#25512;&#24191;&#65292;&#24182;&#19982;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65288;GEP&#65289;&#26694;&#26550;&#32479;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#24615;&#26041;&#27861;&#30340;&#20256;&#32479;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;CCA&#30340;&#25193;&#23637;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#35757;&#32451;&#36807;&#31243;&#32531;&#24930;&#19988;&#22797;&#26434;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#25551;&#36848;GEPs&#30340;&#39030;&#32423;&#23376;&#31354;&#38388;&#30340;&#26032;&#39062;&#26080;&#32422;&#26463;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#26159;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24212;&#29992;&#20110;&#30456;&#24212;&#30340;CCA&#30446;&#26631;&#65292;&#20174;&#32780;&#33719;&#24471;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25152;&#26377;&#26631;&#20934;CCA&#21644;&#28145;&#24230;CCA&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;&#36825;&#26679;&#30340;&#36895;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39318;&#27425;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#29289;&#25968;&#25454;&#30340;PLS&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
&lt;/p&gt;</description></item><item><title>Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00229</link><description>&lt;p&gt;
&#22312;&#35268;&#21010;&#20013;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00229
&lt;/p&gt;
&lt;p&gt;
Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#26377;&#24847;&#35782;&#35268;&#21010;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Skipper&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#25512;&#24191;&#22312;&#26032;&#24773;&#22659;&#20013;&#23398;&#21040;&#30340;&#25216;&#33021;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23427;&#33258;&#21160;&#23558;&#32473;&#23450;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#21487;&#31649;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#36825;&#20381;&#36182;&#20110;&#20174;&#22238;&#28335;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#20026;&#26377;&#21521;&#22270;&#30340;&#25277;&#35937;&#20195;&#29702;&#38382;&#39064;&#30340;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21738;&#20123;&#26041;&#38754;&#26377;&#26395;&#25552;&#20379;&#24110;&#21161;&#12290;&#38024;&#23545;&#27867;&#21270;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;ACC-DA&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.00013</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39550;&#39542;&#20013;&#22522;&#20110;&#39046;&#22495;&#21305;&#37197;&#30340;&#21327;&#21516;&#24863;&#30693;&#30340;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Adaptive Communications in Collaborative Perception with Domain Alignment for Autonomous Driving. (arXiv:2310.00013v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;ACC-DA&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36890;&#20449;&#20801;&#35768;&#36710;&#36742;&#20132;&#25442;&#34917;&#20805;&#20449;&#24687;&#65292;&#22810;&#20010;&#36830;&#25509;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21327;&#21516;&#24863;&#30693;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#24863;&#30693;&#33021;&#21147;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36890;&#36947;&#21464;&#21270;&#21644;&#21327;&#21516;&#36710;&#36742;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACC-DA&#65292;&#19968;&#20010;&#36890;&#36947;&#24863;&#30693;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#24182;&#26368;&#23567;&#21270;&#24179;&#22343;&#20256;&#36755;&#24310;&#36831;&#65292;&#21516;&#26102;&#20943;&#36731;&#25968;&#25454;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#21103;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#28857;&#21253;&#25324;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#20256;&#36755;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#36890;&#36947;&#20449;&#24687;&#29366;&#24577;&#26500;&#24314;&#36890;&#20449;&#22270;&#24182;&#26368;&#23567;&#21270;&#20256;&#36755;&#24310;&#36831;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#65292;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#30721;&#29575;-&#30072;&#21464;&#25240;&#34935;&#20197;&#22686;&#24378;&#24863;&#30693;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#26368;&#23567;&#21270;&#20102;&#26102;&#22495;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative perception among multiple connected and autonomous vehicles can greatly enhance perceptive capabilities by allowing vehicles to exchange supplementary information via communications. Despite advances in previous approaches, challenges still remain due to channel variations and data heterogeneity among collaborative vehicles. To address these issues, we propose ACC-DA, a channel-aware collaborative perception framework to dynamically adjust the communication graph and minimize the average transmission delay while mitigating the side effects from the data heterogeneity. Our novelties lie in three aspects. We first design a transmission delay minimization method, which can construct the communication graph and minimize the transmission delay according to different channel information state. We then propose an adaptive data reconstruction mechanism, which can dynamically adjust the rate-distortion trade-off to enhance perception efficiency. Moreover, it minimizes the temporal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;</title><link>http://arxiv.org/abs/2309.16701</link><description>&lt;p&gt;
MVMR: &#22312;&#22810;&#20010;&#21487;&#38752;&#35270;&#39057;&#38598;&#20013;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#28608;&#22686;&#65292;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#33268;&#21147;&#20110;&#26816;&#27979;&#19982;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21305;&#37197;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#25506;&#32034;&#22312;&#23384;&#22312;&#22810;&#20010;&#27491;&#36127;&#35270;&#39057;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#20013;&#23450;&#20301;&#19968;&#20010;&#26102;&#21051;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#65288;Massive Videos Moment Retrieval&#65289;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29616;&#26377;&#35270;&#39057;&#23450;&#20301;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30446;&#26631;&#26597;&#35810;&#19982;&#35270;&#39057;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#23450;&#20041;&#27491;&#36127;&#38598;&#12290;&#38024;&#23545;&#25552;&#20986;&#30340;MVMR&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16512</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#20998;&#26512;&#34920;&#36798;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#21270;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#30001;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#38382;&#39064;&#21487;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#22312;&#20854;&#20013;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#20197;&#25968;&#25454;&#21521;&#37327;&#29983;&#25104;&#30340;&#19977;&#35282;&#24418;&#21644;&#24179;&#34892;&#20307;&#30340;&#26377;&#31526;&#21495;&#20307;&#31215;&#34920;&#31034;&#12290;&#20984;&#38382;&#39064;&#36890;&#36807;$\ell_1$&#27491;&#21017;&#21270;&#25214;&#21040;&#26679;&#26412;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#20197;&#21457;&#29616;&#20165;&#30456;&#20851;&#30340;&#26964;&#31215;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#25968;&#25454;&#21465;&#20107;&#24037;&#20855;&#65292;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;&#20851;&#20110;&#19981;&#21516;&#38454;&#27573;&#21644;&#35282;&#33394;&#30340;&#20849;&#21516;&#21327;&#20316;&#27169;&#24335;&#65292;&#20197;&#20419;&#36827;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#24182;&#20943;&#23569;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.15723</link><description>&lt;p&gt;
&#25105;&#20204;&#30446;&#21069;&#22788;&#20110;&#21738;&#20010;&#38454;&#27573;&#65311;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#29702;&#35299;&#25968;&#25454;&#21465;&#20107;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Where Are We So Far? Understanding Data Storytelling Tools from the Perspective of Human-AI Collaboration. (arXiv:2309.15723v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#25968;&#25454;&#21465;&#20107;&#24037;&#20855;&#65292;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;&#20851;&#20110;&#19981;&#21516;&#38454;&#27573;&#21644;&#35282;&#33394;&#30340;&#20849;&#21516;&#21327;&#20316;&#27169;&#24335;&#65292;&#20197;&#20419;&#36827;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#24182;&#20943;&#23569;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21465;&#20107;&#22312;&#20256;&#36798;&#25968;&#25454;&#27934;&#23519;&#21147;&#26041;&#38754;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#38656;&#35201;&#20154;&#31867;&#21019;&#20316;&#32773;&#20855;&#22791;&#22810;&#26679;&#21270;&#30340;&#25216;&#33021;&#21644;&#30456;&#24403;&#22823;&#30340;&#24037;&#20316;&#37327;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24191;&#27867;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#25968;&#25454;&#21465;&#20107;&#20013;&#30340;&#20316;&#29992;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#26469;&#29702;&#35299;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#26469;&#30475;&#25968;&#25454;&#21465;&#20107;&#24037;&#20855;&#65292;&#36825;&#38459;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;&#29616;&#26377;&#30340;&#21327;&#20316;&#24037;&#20855;&#35774;&#35745;&#30340;&#24605;&#32771;&#65292;&#20197;&#20419;&#36827;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#24182;&#20943;&#23569;&#20182;&#20204;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#29616;&#26377;&#24037;&#20855;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#26469;&#30475;&#65306;&#20854;&#26381;&#21153;&#20110;&#21465;&#20107;&#24037;&#20316;&#27969;&#31243;&#30340;&#38454;&#27573;&#65292;&#21253;&#25324;&#20998;&#26512;&#12289;&#35268;&#21010;&#12289;&#23454;&#26045;&#21644;&#27807;&#36890;&#65307;&#20197;&#21450;&#22312;&#27599;&#20010;&#38454;&#27573;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#35282;&#33394;&#65292;&#22914;&#21019;&#20316;&#32773;&#12289;&#21161;&#25163;&#12289;&#20248;&#21270;&#22120;&#21644;&#35780;&#23457;&#21592;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#29616;&#26377;&#24037;&#20855;&#20013;&#30340;&#20849;&#21516;&#21327;&#20316;&#27169;&#24335;&#65292;&#24182;&#24635;&#32467;&#20102;&#20174;&#36825;&#20123;&#27169;&#24335;&#20013;&#24471;&#21040;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data storytelling is powerful for communicating data insights, but it requires diverse skills and considerable effort from human creators. Recent research has widely explored the potential for artificial intelligence (AI) to support and augment humans in data storytelling. However, there lacks a systematic review to understand data storytelling tools from the perspective of human-AI collaboration, which hinders researchers from reflecting on the existing collaborative tool designs that promote humans' and AI's advantages and mitigate their shortcomings. This paper investigated existing tools with a framework from two perspectives: the stages in the storytelling workflow where a tool serves, including analysis, planning, implementation, and communication, and the roles of humans and AI in each stage, such as creators, assistants, optimizers, and reviewers. Through our analysis, we recognize the common collaboration patterns in existing tools, summarize lessons learned from these pattern
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#65292;&#20855;&#20307;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;GPT&#27169;&#22411;&#30340;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.13202</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts. (arXiv:2309.13202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#65292;&#20855;&#20307;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;GPT&#27169;&#22411;&#30340;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#36890;&#24120;&#20351;&#29992;&#22797;&#26434;&#30340;&#35821;&#35328;&#21644;&#38590;&#20197;&#29702;&#35299;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#22240;&#27492;&#65292;&#31616;&#21270;&#22312;&#25552;&#39640;&#20844;&#20849;&#20581;&#24247;&#32032;&#20859;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27492;&#31867;&#20219;&#21153;&#21487;&#20197;&#20351;&#38750;&#19987;&#19994;&#35835;&#32773;&#24555;&#36895;&#30452;&#25509;&#22320;&#33719;&#21462;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#31616;&#21270;&#30340;&#25968;&#25454;&#38598;&#65288;PLABA&#65289;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#31616;&#21270;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;&#30340;&#26041;&#27861;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#65288;PBL&#65289;&#22312;&#65306;1&#65289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65288;T5&#12289;SciFive&#21644;BART&#65289;&#19978;&#65292;2&#65289;&#20165;&#35299;&#30721;&#22120;&#30340;GPT&#27169;&#22411;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#26469;&#33258;OpenAI&#21644;BioGPT&#65292;&#20197;&#21450;3&#65289;&#22522;&#20110;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#30340;&#22522;&#20110;BART&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;BLEU&#12289;ROUGE&#12289;SARI&#21644;BERTscore&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature often uses complex language and inaccessible professional terminologies. That is why simplification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (\textbf{PLABA}). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) m
&lt;/p&gt;</description></item><item><title>Choice-75&#26159;&#19968;&#20010;&#20851;&#20110;&#33050;&#26412;&#23398;&#20064;&#20013;&#20915;&#31574;&#20998;&#25903;&#30340;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#25361;&#25112;&#26234;&#33021;&#31995;&#32479;&#22312;&#25551;&#36848;&#22330;&#26223;&#19979;&#39044;&#27979;&#20915;&#31574;&#12290;&#22312;&#35768;&#22810;&#38590;&#39064;&#20013;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.11737</link><description>&lt;p&gt;
Choice-75&#65306;&#19968;&#20010;&#20851;&#20110;&#33050;&#26412;&#23398;&#20064;&#20013;&#20915;&#31574;&#20998;&#25903;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Choice-75: A Dataset on Decision Branching in Script Learning. (arXiv:2309.11737v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11737
&lt;/p&gt;
&lt;p&gt;
Choice-75&#26159;&#19968;&#20010;&#20851;&#20110;&#33050;&#26412;&#23398;&#20064;&#20013;&#20915;&#31574;&#20998;&#25903;&#30340;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#25361;&#25112;&#26234;&#33021;&#31995;&#32479;&#22312;&#25551;&#36848;&#22330;&#26223;&#19979;&#39044;&#27979;&#20915;&#31574;&#12290;&#22312;&#35768;&#22810;&#38590;&#39064;&#20013;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33050;&#26412;&#23398;&#20064;&#30740;&#31350;&#26085;&#24120;&#20107;&#20214;&#30340;&#23637;&#24320;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#23558;&#33050;&#26412;&#35270;&#20026;&#32447;&#24615;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20154;&#20204;&#22312;&#29305;&#23450;&#24773;&#22659;&#20013;&#25152;&#20570;&#20986;&#30340;&#36873;&#25321;&#21487;&#33021;&#24102;&#26469;&#30340;&#20998;&#25903;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Choice-75&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25361;&#25112;&#26234;&#33021;&#31995;&#32479;&#26681;&#25454;&#25551;&#36848;&#22330;&#26223;&#39044;&#27979;&#20915;&#31574;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;75&#20010;&#33050;&#26412;&#21644;600&#22810;&#20010;&#22330;&#26223;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25972;&#20307;&#19978;&#34920;&#29616;&#19981;&#38169;&#65292;&#20294;&#22312;&#35768;&#22810;&#22256;&#38590;&#30340;&#22330;&#26223;&#20013;&#20173;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Script learning studies how daily events unfold. Previous works tend to consider a script as a linear sequence of events while ignoring the potential branches that arise due to people's circumstantial choices. We hence propose Choice-75, the first benchmark that challenges intelligent systems to predict decisions given descriptive scenarios, containing 75 scripts and more than 600 scenarios. While large language models demonstrate overall decent performances, there is still notable room for improvement in many hard scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.10370</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#19968;&#20010;&#38544;&#34255;&#23618;&#12289;&#19968;&#20010;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#12289;&#19968;&#20010;${\mathcal L}^2$&#35889;&#33539;&#31867;&#65288;&#25110;&#32773;Hilbert-Schmidt&#65289;&#30340;&#20195;&#20215;&#20989;&#25968;&#12289;&#36755;&#20837;&#31354;&#38388;${\mathbb R}^M$&#12289;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#65288;&#20854;&#20013;$Q\leq M$&#65289;&#65292;&#20197;&#21450;&#35757;&#32451;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;$N&gt;QM$&#25152;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;$O(\delta_P)$&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$\delta_P$&#34913;&#37327;&#20102;&#35757;&#32451;&#36755;&#20837;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24212;&#20110;&#23646;&#20110;&#21516;&#19968;&#36755;&#20986;&#21521;&#37327;$y_j$&#30340;&#35757;&#32451;&#36755;&#20837;&#21521;&#37327;$\overline{x_{0,j}}$&#30340;&#25237;&#24433;&#26469;&#33719;&#24471;&#36817;&#20284;&#30340;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;$j=1,\dots,Q$&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;$M=Q$&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#30830;&#23450;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#19968;&#20010;&#30830;&#20999;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#65307;&#36825;&#20010;&#23574;&#38160;&#30340;&#20540;&#19982;&#23545;&#20110;$Q\leq M$&#25152;&#33719;&#24471;&#30340;&#19978;&#30028;&#20043;&#38388;&#26377;&#19968;&#20010;&#30456;&#23545;&#35823;&#24046;$O(\delta_P^2)$&#12290;&#19978;&#30028;&#35777;&#26126;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26500;&#36896;&#24615;&#35757;&#32451;&#30340;&#32593;&#32476;&#65307;&#25105;&#20204;&#35777;&#26126;&#23427;&#27979;&#24230;&#20102;$Q$&#32500;&#31354;&#38388;&#20013;&#30340;&#32473;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N&gt;QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00464</link><description>&lt;p&gt;
&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Theoretical and Practical Framework for Evaluating Uncertainty Calibration in Object Detection. (arXiv:2309.00464v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#21450;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#22312;&#32771;&#34385;&#28145;&#24230;&#23398;&#20064;&#30340;&#26410;&#26469;&#26102;&#65292;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#38382;&#39064;&#25104;&#20026;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#32771;&#34385;&#21040;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25351;&#26631;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/pedrormconde/Uncertainty_Calibration_Object_Detection&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Deep Neural Networks has resulted in machine learning systems becoming increasingly more present in various real-world applications. Consequently, there is a growing demand for highly reliable models in these domains, making the problem of uncertainty calibration pivotal, when considering the future of deep learning. This is especially true when considering object detection systems, that are commonly present in safety-critical application such as autonomous driving and robotics. For this reason, this work presents a novel theoretical and practical framework to evaluate object detection systems in the context of uncertainty calibration. The robustness of the proposed uncertainty calibration metrics is shown through a series of representative experiments. Code for the proposed uncertainty calibration metrics at: https://github.com/pedrormconde/Uncertainty_Calibration_Object_Detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06378</link><description>&lt;p&gt;
DCNFIS&#65306;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System. (arXiv:2308.06378v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#36879;&#26126;&#24230;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#33879;&#21517;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#20294;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#24182;&#22312;&#22235;&#20010;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#19982;&#19977;&#20010;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;DCNFIS&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#36879;&#26126;&#24230;&#65292;&#20174;DCNFIS&#20013;&#32534;&#30721;&#30340;&#27169;&#31946;&#35268;&#21017;&#20013;&#25552;&#21462;&#35299;&#37322;&#65292;&#20197;&#28176;&#21464;&#26144;&#23556;&#30340;&#24418;&#24335;&#23637;&#31034;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;Fashion-MNIST&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#35299;&#37322;&#30340;&#29305;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01313</link><description>&lt;p&gt;
&#26356;&#22810;&#19978;&#19979;&#25991;&#65292;&#26356;&#23569;&#24178;&#25200;&#65306;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;CLIP&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20154;&#31867;&#33324;&#29702;&#35299;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#20013;&#24471;&#21040;&#21551;&#21457;&#65306;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#35266;&#28857;&#35748;&#20026;&#65292;&#22312;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#20154;&#31867;&#39318;&#20808;&#25512;&#26029;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#32972;&#26223;&#21644;&#26041;&#21521;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#21306;&#20998;&#24320;&#26469;&#65292;&#28982;&#21518;&#20197;&#27492;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20915;&#31574;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20026;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#21487;&#20197;&#25913;&#21892;&#38646;&#26679;&#26412;&#20998;&#31867;&#24182;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;CLIP&#26412;&#36523;&#21487;&#20197;&#21512;&#29702;&#22320;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#35757;&#32451;&#12289;&#20004;&#27493;&#39588;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
&lt;/p&gt;</description></item><item><title>RLCD&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#33976;&#39311;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#12290;&#22312;&#22810;&#20010;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;RLCD&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.12950</link><description>&lt;p&gt;
RLCD: &#22522;&#20110;&#23545;&#27604;&#33976;&#39311;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment. (arXiv:2307.12950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12950
&lt;/p&gt;
&lt;p&gt;
RLCD&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#33976;&#39311;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#12290;&#22312;&#22810;&#20010;&#23545;&#40784;&#20219;&#21153;&#21644;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;RLCD&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Reinforcement Learning from Contrast Distillation (RLCD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#38656;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21363;&#21487;&#20351;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#30340;&#23545;&#40784;&#12290;RLCD&#20351;&#29992;&#27169;&#25311;&#30340;&#20559;&#22909;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#23545;&#21253;&#21547;&#20102;&#39640;&#36136;&#37327;&#21644;&#20302;&#36136;&#37327;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#20351;&#29992;&#23545;&#27604;&#30340;&#27491;&#36127;&#25552;&#31034;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#22522;&#30784;&#30340;&#26080;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;RLCD&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#23545;&#40784;&#20219;&#21153;&#65288;&#26080;&#23475;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#25925;&#20107;&#22823;&#32434;&#29983;&#25104;&#65289;&#20197;&#21450;7B&#21644;30B&#27169;&#22411;&#35268;&#27169;&#30340;&#20559;&#22909;&#25968;&#25454;&#27169;&#25311;&#19978;&#65292;&#37117;&#20248;&#20110;RLAIF (Bai&#31561;&#20154;&#65292;2022b)&#21644;&#19978;&#19979;&#25991;&#33976;&#39311; (Huang&#31561;&#20154;&#65292;2022) &#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09985</link><description>&lt;p&gt;
&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;MovieLens&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65306;&#36825;&#24847;&#21619;&#30528;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?. (arXiv:2307.09985v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#20856;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#22312;&#26576;&#19968;&#26102;&#38388;&#27573;&#20869;&#22312;&#24179;&#21488;&#19978;&#29983;&#25104;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#12290;&#20132;&#20114;&#29983;&#25104;&#26426;&#21046;&#37096;&#20998;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29992;&#25143;&#19982;&#29289;&#21697;&#36827;&#34892;&#20132;&#20114;&#65288;&#22914;&#21916;&#27426;&#12289;&#36141;&#20080;&#12289;&#35780;&#20998;&#65289;&#20197;&#21450;&#29305;&#23450;&#20132;&#20114;&#21457;&#29983;&#30340;&#32972;&#26223;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;MovieLens&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#65292;&#24182;&#35299;&#37322;&#20102;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#26102;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#19968;&#20123;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#22312;&#29992;&#25143;&#19982;MovieLens&#24179;&#21488;&#20132;&#20114;&#30340;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26089;&#26399;&#20132;&#20114;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23450;&#20041;&#20102;&#29992;&#25143;&#30011;&#20687;&#65292;&#24433;&#21709;&#20102;&#21518;&#32493;&#30340;&#20132;&#20114;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#20869;&#37096;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24456;&#22823;&#24433;&#21709;&#12290;&#21024;&#38500;&#38752;&#36817;&#26368;&#21518;&#20960;&#27425;&#20132;&#20114;&#30340;&#20132;&#20114;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
A typical benchmark dataset for recommender system (RecSys) evaluation consists of user-item interactions generated on a platform within a time period. The interaction generation mechanism partially explains why a user interacts with (e.g.,like, purchase, rate) an item, and the context of when a particular interaction happened. In this study, we conduct a meticulous analysis on the MovieLens dataset and explain the potential impact on using the dataset for evaluating recommendation algorithms. We make a few main findings from our analysis. First, there are significant differences in user interactions at the different stages when a user interacts with the MovieLens platform. The early interactions largely define the user portrait which affect the subsequent interactions. Second, user interactions are highly affected by the candidate movies that are recommended by the platform's internal recommendation algorithm(s). Removal of interactions that happen nearer to the last few interactions 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#31070;&#32463;&#26550;&#26500;&#26816;&#32034;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#21457;&#29616;&#30456;&#20284;&#31070;&#32463;&#26550;&#26500;&#26102;&#25152;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07919</link><description>&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Retrieval. (arXiv:2307.07919v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#31070;&#32463;&#26550;&#26500;&#26816;&#32034;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#21457;&#29616;&#30456;&#20284;&#31070;&#32463;&#26550;&#26500;&#26102;&#25152;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#35774;&#35745;&#30340;&#22686;&#21152;&#21644;&#29616;&#26377;&#31070;&#32463;&#26550;&#26500;&#30340;&#22823;&#37327;&#23384;&#22312;&#65292;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#23558;&#33258;&#24049;&#30340;&#36129;&#29486;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#27604;&#36739;&#65292;&#25110;&#32773;&#24314;&#31435;&#33258;&#24049;&#30340;&#35774;&#35745;&#19982;&#20854;&#20182;&#30456;&#20851;&#35774;&#35745;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20026;&#20102;&#20197;&#39640;&#25928;&#19988;&#33258;&#21160;&#30340;&#26041;&#24335;&#21457;&#29616;&#30456;&#20284;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#31070;&#32463;&#26550;&#26500;&#26816;&#32034;&#65292;&#23427;&#26816;&#32034;&#19968;&#32452;&#19982;&#26597;&#35810;&#31070;&#32463;&#26550;&#26500;&#20855;&#26377;&#30456;&#20284;&#35774;&#35745;&#30340;&#29616;&#26377;&#31070;&#32463;&#26550;&#26500;&#12290;&#30001;&#20110;&#31070;&#32463;&#26550;&#26500;&#20013;&#30340;&#22270;&#30340;&#22823;&#23567;&#21644;&#27169;&#24335;&#65292;&#29616;&#26377;&#30340;&#22270;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#33021;&#35299;&#20915;&#35745;&#31639;&#22270;&#38382;&#39064;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22270;&#20998;&#25104;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#37325;&#24314;&#23439;&#22270;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#23545;&#20154;&#24037;&#35774;&#35745;&#21644;&#21512;&#25104;&#31070;&#32463;&#26550;&#26500;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing number of new neural architecture designs and substantial existing neural architectures, it becomes difficult for the researchers to situate their contributions compared with existing neural architectures or establish the connections between their designs and other relevant ones. To discover similar neural architectures in an efficient and automatic manner, we define a new problem Neural Architecture Retrieval which retrieves a set of existing neural architectures which have similar designs to the query neural architecture. Existing graph pre-training strategies cannot address the computational graph in neural architectures due to the graph size and motifs. To fulfill this potential, we propose to divide the graph into motifs which are used to rebuild the macro graph to tackle these issues, and introduce multi-level contrastive learning to achieve accurate graph representation learning. Extensive evaluations on both human-designed and synthesized neural architecture
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>SAMAug&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#65292;&#32467;&#21512;&#21021;&#22987;&#25552;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;Segment Anything Model&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01187</link><description>&lt;p&gt;
SAMAug: Segment Anything Model&#30340;&#28857;&#25552;&#31034;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAMAug: Point Prompt Augmentation for Segment Anything Model. (arXiv:2307.01187v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01187
&lt;/p&gt;
&lt;p&gt;
SAMAug&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#65292;&#32467;&#21512;&#21021;&#22987;&#25552;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;Segment Anything Model&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SAMAug&#65292;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#30340;&#26032;&#22411;&#35270;&#35273;&#28857;&#25552;&#31034;&#22686;&#24378;&#26041;&#27861;&#12290;SAMAug&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#65292;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#29992;&#25143;&#24847;&#22270;&#30340;&#20449;&#24687;&#32473;SAM&#12290;SAM&#20174;&#19968;&#20010;&#21021;&#22987;&#28857;&#25552;&#31034;&#24320;&#22987;&#29983;&#25104;&#19968;&#20010;&#21021;&#22987;&#25513;&#30721;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#25105;&#20204;&#25552;&#20986;&#30340;SAMAug&#26469;&#29983;&#25104;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20123;&#39069;&#22806;&#30340;&#28857;&#65292;SAM&#21487;&#20197;&#22522;&#20110;&#22686;&#24378;&#30340;&#28857;&#25552;&#31034;&#21644;&#21021;&#22987;&#25552;&#31034;&#29983;&#25104;&#22686;&#24378;&#30340;&#20998;&#21106;&#25513;&#30721;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#28857;&#22686;&#24378;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#65306;&#38543;&#26426;&#37319;&#26679;&#65292;&#22522;&#20110;&#26368;&#22823;&#24046;&#24322;&#29109;&#30340;&#37319;&#26679;&#65292;&#26368;&#22823;&#36317;&#31163;&#21644;&#26174;&#33879;&#24615;&#12290;&#22312;COCO&#12289;Fundus&#12289;COVID QUEx&#21644;ISIC2018&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;SAMAug&#21487;&#20197;&#25552;&#21319;SAM&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#26368;&#22823;&#36317;&#31163;&#21644;&#26174;&#33879;&#24615;&#12290;SAMAug&#35777;&#26126;&#20102;&#20854;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information about the user's intention to SAM. Starting with an initial point prompt, SAM produces an initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on both the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We conducted evaluations using four different point augmentation strategies: random sampling, sampling based on maximum difference entropy, maximum distance, and saliency. Experiment results on the COCO, Fundus, COVID QUEx, and ISIC2018 datasets show that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency. SAMAug demonstrates the potenti
&lt;/p&gt;</description></item><item><title>ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16922</link><description>&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#65306;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16922
&lt;/p&gt;
&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#31185;&#23398;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#31616;&#21270;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#20381;&#38752;&#38598;&#20307;&#27963;&#21160;&#21644;&#36866;&#24403;&#35843;&#25972;&#30340;&#36830;&#25509;&#26469;&#25191;&#34892;&#22797;&#26434;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#29983;&#29289;&#30382;&#23618;&#31070;&#32463;&#20803;&#26412;&#36136;&#19978;&#37117;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#35774;&#22791;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#30740;&#31350;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#30340;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#22797;&#21046;&#35814;&#32454;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#22810;&#20010;&#21442;&#25968;&#30340;&#24517;&#35201;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#24341;&#20837;&#20102;&#34920;&#36798;&#21147;&#24378;&#30340;&#27844;&#28431;&#23384;&#20648;&#22120;&#65288;ELM&#65289;&#31070;&#32463;&#20803;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#35745;&#31639;&#34920;&#36798;&#21147;&#65292;&#21516;&#26102;&#20063;&#38750;&#24120;&#39640;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ELM&#31070;&#32463;&#20803;&#20165;&#38656;&#35201;8,000&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#21305;&#37197;&#21069;&#36848;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20934;&#30830;&#30340;&#27169;&#22411;&#38656;&#35201;&#22810;&#20010;&#31867;&#20284;&#20110;&#23384;&#20648;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#31361;&#35302;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;</title><link>http://arxiv.org/abs/2306.16001</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#20197;&#25903;&#25345;&#20844;&#20849;&#21355;&#29983;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#21033;&#29992;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35777;&#23454;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#26469;&#26816;&#32034;&#30456;&#20851;&#35821;&#26009;&#24211;&#26102;&#65292;&#24120;&#24120;&#20250;&#24341;&#20837;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#26500;&#24314;&#21307;&#23398;&#20439;&#35821;&#21644;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#27010;&#24565;&#30340;&#24191;&#27867;&#23383;&#20856;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22522;&#20110;BERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#21307;&#23398;&#23454;&#20307;&#65307;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26631;&#20934;&#21270;&#27169;&#22359;&#65292;&#29992;&#20110;&#23545;&#25552;&#21462;&#20986;&#30340;&#23454;&#20307;&#36827;&#34892;&#35268;&#33539;&#21270;&#22788;&#29702;&#65307;&#21322;&#30417;&#30563;&#32858;&#31867;&#27169;&#22359;&#65292;&#23558;&#26368;&#21487;&#33021;&#30340;UMLS&#27010;&#24565;&#20998;&#37197;&#32473;&#27599;&#20010;&#35268;&#33539;&#21270;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20174;2020&#24180;2&#26376;1&#26085;&#21040;2022&#24180;4&#26376;30&#26085;&#26399;&#38388;&#19982;COVID-19&#30456;&#20851;&#30340;&#25512;&#25991;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#30151;&#29366;&#35789;&#20856;&#65288;&#21487;&#22312;https://github.com/ningkko/UMLS_colloquialism/&#19978;&#33719;&#21462;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;9,249&#20010;&#26631;&#20934;&#21270;&#23454;&#20307;&#65292;&#26144;&#23556;&#21040;876&#20010;UMLS&#27010;&#24565;&#21644;38,175&#20010;&#20442;&#35821;&#34920;&#36798;&#12290;&#35813;&#26694;&#26550;&#30340;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
&lt;/p&gt;</description></item><item><title>Jumanji&#26159;JAX&#20013;&#19968;&#22871;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;</title><link>http://arxiv.org/abs/2306.09884</link><description>&lt;p&gt;
Jumanji: JAX&#20013;&#19968;&#22871;&#22810;&#26679;&#21270;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX. (arXiv:2306.09884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09884
&lt;/p&gt;
&lt;p&gt;
Jumanji&#26159;JAX&#20013;&#19968;&#22871;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22312;&#25512;&#21160;AI&#31639;&#27861;&#30340;&#21457;&#23637;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#38656;&#35201;&#27169;&#25311;&#29615;&#22659;&#20855;&#22791;&#24615;&#33021;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jumanji&#65292;&#36825;&#26159;&#19968;&#22871;&#35774;&#35745;&#29992;&#20110;&#24555;&#36895;&#12289;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#19981;&#21516;RL&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;Jumanji&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#30340;&#29615;&#22659;&#65292;&#19987;&#27880;&#20110;&#24037;&#19994;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#20197;&#21450;&#25361;&#25112;&#24615;&#30340;&#19968;&#33324;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;JAX&#21644;GPU&#12289;TPU&#31561;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#29575;&#65292;Jumanji&#33021;&#22815;&#36805;&#36895;&#36845;&#20195;&#30740;&#31350;&#24605;&#36335;&#21644;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#26368;&#32456;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;&#19982;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22871;&#20214;&#19981;&#21516;&#65292;Jumanji&#20855;&#26377;&#39640;&#24230;&#21487;&#23450;&#21046;&#24615;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#20854;&#38656;&#27714;&#35843;&#25972;&#21021;&#22987;&#29366;&#24577;&#20998;&#24067;&#21644;&#38382;&#39064;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms. In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications. Therefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable. Jumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks. By leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents. Unlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs. Further
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05716</link><description>&lt;p&gt;
&#20026;&#25235;&#20303;&#20219;&#20309;&#29289;&#21697;&#38138;&#24179;&#36947;&#36335;&#65306;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#36890;&#29992;&#25235;&#21462;&#25918;&#32622;&#26426;&#22120;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots. (arXiv:2306.05716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#19968;&#30452;&#26159;&#30740;&#31350;&#31038;&#21306;&#38271;&#26399;&#36861;&#27714;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25910;&#38598;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25968;&#25454;&#65292;&#22914; RT-1 &#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#26032;&#23545;&#35937;&#21644;&#22810;&#26679;&#32972;&#26223;&#30340;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20197;&#35299;&#20915;&#26085;&#24120;&#22330;&#26223;&#20013;&#24191;&#27867;&#30340;&#25342;&#25918;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#25513;&#27169;&#20256;&#36798;&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#20960;&#20309;&#24418;&#29366;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#31574;&#30053;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24863;&#30693;&#20934;&#30830;&#30340;&#29289;&#20307;&#23039;&#24577;&#24182;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;&#26032;&#23545;&#35937;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#22312;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30456;&#20284;&#24418;&#29366;&#30340;&#26032;&#29289;&#20307;&#30340;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the generalization capabilities of general-purpose robotic agents has long been a significant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efficiency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efficient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#36807;&#31243;&#20013;&#20302;&#25509;&#36817;&#24230;&#25968;&#25454;&#21644;&#39640;&#25509;&#36817;&#24230;&#25968;&#25454;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04590</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#36807;&#31243;&#20013;&#20302;&#25509;&#36817;&#24230;&#25968;&#25454;&#21644;&#39640;&#25509;&#36817;&#24230;&#25968;&#25454;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#26657;&#20934;&#23545;&#20110;&#25552;&#20379;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#12290;&#24050;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#25509;&#36817;&#24230;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#22312;&#20302;&#25509;&#36817;&#24615;&#25968;&#25454;&#65288;&#21363;&#20998;&#24067;&#30340;&#31232;&#30095;&#21306;&#22495;&#65289;&#20013;&#20542;&#21521;&#20110;&#26356;&#33258;&#20449;&#65292;&#32780;&#22312;&#39640;&#25509;&#36817;&#24615;&#26679;&#26412;&#20013;&#34920;&#29616;&#20986;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30740;&#31350;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#35266;&#23519;&#21040;&#65306;1&#65289;&#25509;&#36817;&#24230;&#20559;&#24046;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#22823;&#23567;&#20043;&#38388;&#65307;2&#65289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#27604;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25509;&#36817;&#24230;&#20559;&#24046;&#30340;&#24433;&#21709;&#65307;3&#65289;&#21363;&#20351;&#37319;&#29992;&#27969;&#34892;&#30340;&#26657;&#20934;&#31639;&#27861;&#22914;&#28201;&#24230;&#32553;&#25918;&#65292;&#25509;&#36817;&#24230;&#20559;&#24046;&#20063;&#20250;&#25345;&#32493;&#23384;&#22312;&#65307;4&#65289;&#27169;&#22411;&#22312;&#20302;&#25509;&#36817;&#24615;&#26679;&#26412;&#19978;&#30340;&#36807;&#25311;&#21512;&#31243;&#24230;&#27604;&#39640;&#25509;&#36817;&#24615;&#26679;&#26412;&#26356;&#20005;&#37325;&#12290;&#22312;&#36825;&#20123;&#23454;&#35777;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProCal&#12290;
&lt;/p&gt;
&lt;p&gt;
Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.15852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#65306;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#24819;&#30340;&#25991;&#26412;&#12290;&#33258;&#30456;&#30683;&#30462;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24187;&#35273;&#24418;&#24335;&#65292;&#25351;&#30340;&#26159;&#35821;&#35328;&#27169;&#22411;&#22312;&#21516;&#19968;&#35821;&#22659;&#20013;&#29983;&#25104;&#20004;&#20010;&#30683;&#30462;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#20808;&#36827;&#12289;&#32463;&#36807;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#33258;&#30456;&#30683;&#30462;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12289;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#33879;&#21517;&#30340;&#36824;&#26159;&#19981;&#22826;&#20986;&#21517;&#30340;&#35805;&#39064;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#30456;&#30683;&#30462;&#37117;&#32463;&#24120;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#20316;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.14534</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Detecting Propaganda Techniques in Code-Switched Social Media Text. (arXiv:2305.14534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#20316;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23459;&#20256;&#26159;&#19968;&#31181;&#26088;&#22312;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#21644;&#24515;&#24577;&#20197;&#25512;&#24191;&#29305;&#23450;&#35758;&#31243;&#30340;&#27807;&#36890;&#24418;&#24335;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#23835;&#36215;&#65292;&#23459;&#20256;&#24050;&#32463;&#36805;&#36895;&#20256;&#25773;&#65292;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23459;&#20256;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#22823;&#22810;&#25968;&#23459;&#20256;&#26816;&#27979;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#19978;&#65292;&#20960;&#20046;&#27809;&#26377;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#26816;&#27979;&#23459;&#20256;&#20570;&#20986;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#20132;&#27969;&#20013;&#21457;&#29616;&#22810;&#31181;&#35821;&#35328;&#30340;&#28151;&#21512;&#29616;&#35937;&#26159;&#24456;&#24120;&#35265;&#30340;&#65292;&#36825;&#34987;&#31216;&#20026;&#30721;&#28151;&#12290;&#30721;&#28151;&#22312;&#21516;&#19968;&#25991;&#26412;&#20013;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#36825;&#23545;&#20110;&#33258;&#21160;&#31995;&#32479;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#27492;&#25552;&#20986;&#20102;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;&#23459;&#20256;&#25216;&#26415;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20123;&#25991;&#26412;&#22312;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#37117;&#36827;&#34892;&#20102;&#28151;&#21512;&#65292;&#24182;&#29992;20&#31181;&#23459;&#20256;&#25216;&#24039;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#36825;&#20010;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#23545;&#27604;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#29305;&#24449;&#38598;&#21512;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propaganda is a form of communication intended to influence the opinions and the mindset of the public to promote a particular agenda. With the rise of social media, propaganda has spread rapidly, leading to the need for automatic propaganda detection systems. Most work on propaganda detection has focused on high-resource languages, such as English, and little effort has been made to detect propaganda for low-resource languages. Yet, it is common to find a mix of multiple languages in social media communication, a phenomenon known as code-switching. Code-switching combines different languages within the same text, which poses a challenge for automatic systems. With this in mind, here we propose the novel task of detecting propaganda techniques in code-switched text. To support this task, we create a corpus of 1,030 texts code-switching between English and Roman Urdu, annotated with 20 propaganda techniques, which we make publicly available. We perform a number of experiments contrastin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11997</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#20559;&#31227;&#65292;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#21487;&#33021;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#21453;&#20107;&#23454;&#35299;&#37322;&#20248;&#21270;&#20013;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#23558;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#38752;&#36817;&#25968;&#25454;&#27969;&#24418;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#39640;&#27010;&#29575;&#40065;&#26834;&#24615;&#12290;&#26032;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{&lt;}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>Tram&#26159;&#19968;&#31181;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#65292;&#23427;&#22312;&#35299;&#30721;&#22120;&#31471;&#31934;&#32454;&#26816;&#32034;&#24110;&#21161;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;Java&#21644;Python&#28304;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.11074</link><description>&lt;p&gt;
Tram&#65306;&#19968;&#20010;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#20196;&#29260;&#32423;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization. (arXiv:2305.11074v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11074
&lt;/p&gt;
&lt;p&gt;
Tram&#26159;&#19968;&#31181;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#65292;&#23427;&#22312;&#35299;&#30721;&#22120;&#31471;&#31934;&#32454;&#26816;&#32034;&#24110;&#21161;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;Java&#21644;Python&#28304;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#25991;&#26412;&#20197;&#25551;&#36848;&#31243;&#24207;&#30340;&#21151;&#33021;&#26159;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#32467;&#21512;&#31070;&#32463;&#27169;&#22411;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26032;&#36235;&#21183;&#27491;&#22312;&#20852;&#36215;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21477;&#23376;&#32423;&#21035;&#30340;&#26816;&#32034;&#21644;&#32452;&#21512;&#33539;&#24335;&#65288;&#26816;&#32034;&#31867;&#20284;&#30340;&#20195;&#30721;&#29255;&#27573;&#24182;&#20351;&#29992;&#30456;&#24212;&#30340;&#20195;&#30721;&#21644;&#25688;&#35201;&#23545;&#26469;&#32534;&#30721;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#26159;&#31895;&#31890;&#24230;&#30340;&#65292;&#19981;&#33021;&#30452;&#25509;&#21033;&#29992;&#35299;&#30721;&#22120;&#31471;&#39640;&#36136;&#37327;&#30340;&#26816;&#32034;&#25688;&#35201;&#20196;&#29260;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#65292;&#22312;&#35299;&#30721;&#22120;&#31471;&#24110;&#21161;&#21407;&#22987;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#26356;&#22909;&#30340;&#20195;&#30721;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32531;&#35299;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20195;&#30721;&#35821;&#20041;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20195;&#30721;&#35821;&#20041;&#38598;&#25104;&#21040;&#25688;&#35201;&#20196;&#29260;&#20013;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Tram&#22312;Java&#21644;Python&#28304;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating human-readable text describing the functionality of a program is the intent of source code summarization. Although Neural Language Models achieve significant performance in this field, an emerging trend is combining neural models with external knowledge. Most previous approaches rely on the sentence-level retrieval and combination paradigm (retrieval of similar code snippets and use of the corresponding code and summary pairs) on the encoder side. However, this paradigm is coarse-grained and cannot directly take advantage of the high-quality retrieved summary tokens on the decoder side. In this paper, we explore a fine-grained token-level retrieval-augmented mechanism on the decoder side to help the vanilla neural model generate a better code summary. Furthermore, to mitigate the limitation of token-level retrieval on capturing contextual code semantics, we propose to integrate code semantics into summary tokens. Extensive experiments and human evaluation revea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;</title><link>http://arxiv.org/abs/2305.08339</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#65306;&#26412;&#22320;&#35821;&#27861;&#20998;&#26512;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;LLMs&#22312;&#21327;&#21161;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#33258;&#21160;&#26631;&#27880;&#20026;&#29305;&#23450;&#35821;&#35328;&#20449;&#24687;&#31867;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#30340;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#12289;&#22522;&#20110;GPT-4&#30340;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#32534;&#30721;&#22120;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;ChatGPT&#12290;&#19982;&#20154;&#31867;&#26631;&#27880;&#21592;&#30456;&#27604;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25972;&#20307;&#34920;&#29616;&#30053;&#20302;&#20110;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#34920;&#29616;&#65292;&#20294;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#24471;&#20998;:&#36947;&#27465;&#26631;&#35760;99.95&#65285;&#65292;&#21407;&#22240;&#26631;&#35760;91.91&#65285;&#65292;&#36947;&#27465;&#32773;&#26631;&#35760;95.35&#65285;&#65292;&#34987;&#36947;&#27465;&#32773;&#26631;&#35760;89.74&#65285;&#21644;&#21152;&#24378;&#26631;&#35760;96.47&#65285;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#31867;&#21035;&#28165;&#26224;&#19988;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06626</link><description>&lt;p&gt;
&#24403;&#22810;&#25968;&#20154;&#26159;&#38169;&#35823;&#30340;&#65306;&#21033;&#29992;&#26631;&#27880;&#32773;&#19981;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#25237;&#31080;&#26469;&#30830;&#23450;&#26631;&#31614;&#65292;&#20294;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#65292;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#21453;&#26144;&#20986;&#32676;&#20307;&#35266;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#19968;&#20010;&#35821;&#21477;&#26159;&#21542;&#20882;&#29359;&#20102;&#23427;&#25152;&#38024;&#23545;&#30340;&#20154;&#32676;&#65292;&#32780;&#36825;&#21487;&#33021;&#21482;&#21344;&#26631;&#27880;&#32773;&#27744;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#25991;&#26412;&#19978;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30340;&#39044;&#27979;&#30446;&#26631;&#32676;&#20307;&#26469;&#27169;&#25311;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#25552;&#39640;&#20102;22&#65285;&#22312;&#39044;&#27979;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;33&#65285;&#22312;&#39044;&#27979;&#26631;&#27880;&#32773;&#20043;&#38388;&#26041;&#24046;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19979;&#28216;&#29992;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20854;&#22312;&#32447;&#24847;&#35265;&#26469;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13138</link><description>&lt;p&gt;
&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#30340;&#26356;&#26032;&#31561;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26827;&#31867;&#28216;&#25103;&#31561;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#20013;&#65292;&#21363;&#26102;&#20462;&#27491;&#65288;&#25110;&#26500;&#24314;&#65289;&#31574;&#30053;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26159;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#30340;&#20851;&#38190;&#12290;&#19968;&#20123;&#30740;&#31350;&#23558;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#25193;&#23637;&#21040;&#26356;&#26222;&#36941;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25169;&#20811;&#20013;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#38543;&#30528;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#30340;&#22686;&#21152;&#32780;&#24555;&#36895;&#22686;&#38271;&#30340;&#23376;&#28216;&#25103;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#36739;&#22823;&#26102;&#19981;&#36215;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#32780;&#19981;&#26159;&#23376;&#28216;&#25103;&#27010;&#24565;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#27169;&#25311;&#21516;&#27493;&#23398;&#20064;&#31639;&#27861;&#30340;&#26356;&#26032;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#24341;&#20837;&#19968;&#31995;&#21015;&#21407;&#21017;&#19978;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#20026;&#26032;&#30340;&#19968;&#20010;&#31995;&#21015;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#36866;&#29992;&#20110;&#29616;&#20195;CPU&#20307;&#31995;&#32467;&#26500;&#30340;&#39640;&#25928;&#12289;&#21487;&#31227;&#26893;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#20869;&#26680;&#65292;&#20351;&#29992;&#39640;&#32423;&#24490;&#29615;&#21644;&#24352;&#37327;&#25277;&#35937;&#12290;</title><link>http://arxiv.org/abs/2304.12576</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#32423;&#24490;&#29615;&#21644;&#24352;&#37327;&#25277;&#35937;&#22312;CPU&#26550;&#26500;&#19978;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;HPC&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures. (arXiv:2304.12576v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12576
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#36866;&#29992;&#20110;&#29616;&#20195;CPU&#20307;&#31995;&#32467;&#26500;&#30340;&#39640;&#25928;&#12289;&#21487;&#31227;&#26893;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#20869;&#26680;&#65292;&#20351;&#29992;&#39640;&#32423;&#24490;&#29615;&#21644;&#24352;&#37327;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#12289;&#32534;&#31243;&#31995;&#32479;&#21644;&#30828;&#20214;&#24050;&#32463;&#19982;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;DL&#21644;HPC&#31995;&#32479;&#30340;&#32534;&#31243;&#26041;&#27861;&#21364;&#20572;&#28382;&#19981;&#21069;&#65292;&#20381;&#36182;&#20110;&#39640;&#24230;&#20248;&#21270;&#12289;&#29305;&#23450;&#20110;&#24179;&#21488;&#12289;&#20725;&#21270;&#30340;&#20379;&#24212;&#21830;&#20248;&#21270;&#24211;&#12290;&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#29616;&#20195;CPU&#26550;&#26500;&#30340;&#39640;&#25928;&#12289;&#21487;&#31227;&#26893;&#30340;DL&#21644;HPC&#20869;&#26680;&#12290;&#25105;&#20204;&#23558;&#20869;&#26680;&#24320;&#21457;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;1&#65289;&#20351;&#29992;&#24352;&#37327;&#22788;&#29702;&#21407;&#35821;&#65288;TPP&#65289;&#34920;&#36798;&#35745;&#31639;&#26680;&#24515;&#65306;&#19968;&#20010;&#32039;&#20945;&#12289;&#22810;&#21151;&#33021;&#30340;2D&#24352;&#37327;&#36816;&#31639;&#31526;&#65292;2&#65289;&#20197;&#39640;&#32423;&#12289;&#22768;&#26126;&#24615;&#30340;&#26041;&#24335;&#34920;&#36798;TPP&#21608;&#22260;&#30340;&#36923;&#36753;&#24490;&#29615;&#65292;&#32780;&#30830;&#20999;&#30340;&#23454;&#20363;&#21270;&#65288;&#39034;&#24207;&#65292;&#20869;&#23384;&#24067;&#23616;&#65289;&#21017;&#36890;&#36807;&#23558;TPL&#35270;&#20026;&#40657;&#30418;&#65292;&#26681;&#25454;&#20248;&#21270;&#30446;&#26631;&#21644;&#32422;&#26463;&#30001;&#33258;&#21160;&#20248;&#21270;&#22120;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the past decade, Deep Learning (DL) algorithms, programming systems and hardware have converged with the High Performance Computing (HPC) counterparts. Nevertheless, the programming methodology of DL and HPC systems is stagnant, relying on highly-optimized, yet platform-specific and inflexible vendor-optimized libraries. Such libraries provide close-to-peak performance on specific platforms, kernels and shapes thereof that vendors have dedicated optimizations efforts, while they underperform in the remaining use-cases, yielding non-portable codes with performance glass-jaws. This work introduces a framework to develop efficient, portable DL and HPC kernels for modern CPU architectures. We decompose the kernel development in two steps: 1) Expressing the computational core using Tensor Processing Primitives (TPPs): a compact, versatile set of 2D-tensor operators, 2) Expressing the logical loops around TPPs in a high-level, declarative fashion whereas the exact instantiation (order
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09444</link><description>&lt;p&gt;
&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#39640;&#32500;&#26114;&#36149;&#38382;&#39064;&#30340;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-Based Learning and Local Model Based Evolutionary Algorithm for High-Dimensional Expensive Multi-Objective Problems. (arXiv:2304.09444v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36741;&#20197;&#20195;&#29702;&#27169;&#22411;&#30340;&#36827;&#21270;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32780;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#20294;&#26159;&#22312;&#22788;&#29702;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#36741;&#20197;&#20195;&#29702;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#20250;&#24613;&#21095;&#24694;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#31867;&#22120;&#36741;&#21161;&#30340;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861; (CLMEA)&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#30001;&#19977;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#31867;&#22120;&#36741;&#21161;&#30340;&#25490;&#21517;&#23398;&#20064;&#12289;&#36229;&#20307;&#31215;&#38750;&#25903;&#37197;&#25628;&#32034;&#21644;&#30456;&#23545;&#31232;&#30095;&#30446;&#26631;&#31354;&#38388;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;&#23558;&#21518;&#20195;&#21010;&#20998;&#20026;&#20960;&#20010;&#31561;&#32423;&#12290;&#19981;&#21516;&#31561;&#32423;&#30340;&#21518;&#20195;&#20351;&#29992;&#25490;&#21517;&#23398;&#20064;&#31574;&#30053;&#29983;&#25104;&#26356;&#20855;&#26377;&#21069;&#26223;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#20505;&#36873;&#35299;&#29992;&#20110;&#23454;&#38469;&#20248;&#21270;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate-assisted evolutionary algorithms have been widely developed to solve complex and computationally expensive multi-objective optimization problems in recent years. However, when dealing with high-dimensional optimization problems, the performance of these surrogate-assisted multi-objective evolutionary algorithms deteriorate drastically. In this work, a novel Classifier-assisted rank-based learning and Local Model based multi-objective Evolutionary Algorithm (CLMEA) is proposed for high-dimensional expensive multi-objective optimization problems. The proposed algorithm consists of three parts: classifier-assisted rank-based learning, hypervolume-based non-dominated search, and local search in the relatively sparse objective space. Specifically, a probabilistic neural network is built as classifier to divide the offspring into a number of ranks. The offspring in different ranks uses rank-based learning strategy to generate more promising and informative candidates for real funct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#32423;&#21035;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#36807;&#31070;&#32463;&#20803;&#28608;&#27963;&#31354;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#24615;&#29305;&#27931;&#20234;&#65292;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00436</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#20803;&#28608;&#27963;&#31354;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#23545;&#23454;&#20363;&#36827;&#34892;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space. (arXiv:2304.00436v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#32423;&#21035;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#36807;&#31070;&#32463;&#20803;&#28608;&#27963;&#31354;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#24615;&#29305;&#27931;&#20234;&#65292;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#25200;&#21160;&#34987;&#23884;&#20837;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#31216;&#20026;&#29305;&#27931;&#20234;&#25915;&#20987;&#65292;&#21487;&#20197;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#21363;&#20174;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#65289;&#21521;&#30446;&#26631;&#27169;&#22411;&#36716;&#31227;&#30693;&#35782;&#30340;&#36807;&#31243;&#20013;&#65292;&#29305;&#27931;&#20234;&#25915;&#20987;&#30340;&#24433;&#21709;&#26377;&#25152;&#20943;&#23567;&#12290;&#20026;&#20102;&#20943;&#36731;&#29305;&#27931;&#20234;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21487;&#20197;&#26367;&#25442;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22810;&#20010;&#23618;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#26679;&#26412;&#25928;&#29575;&#12289;&#38544;&#34109;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23454;&#20363;&#32423;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#29983;&#25104;&#36328;&#36755;&#20837;&#26679;&#26412;&#21644;&#27169;&#24577;&#30340;&#22810;&#26679;&#21270;&#29305;&#27931;&#20234;&#12290;&#23545;&#25239;&#23398;&#20064;&#24314;&#31435;&#20102;&#25351;&#23450;&#25200;&#21160;&#23618;&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#24322;&#24120;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;VQA-v2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#25351;&#26631;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#36866;&#24212;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malicious perturbations embedded in input data, known as Trojan attacks, can cause neural networks to misbehave. However, the impact of a Trojan attack is reduced during fine-tuning of the model, which involves transferring knowledge from a pretrained large-scale model like visual question answering (VQA) to the target model. To mitigate the effects of a Trojan attack, replacing and fine-tuning multiple layers of the pretrained model is possible. This research focuses on sample efficiency, stealthiness and variation, and robustness to model fine-tuning. To address these challenges, we propose an instance-level Trojan attack that generates diverse Trojans across input samples and modalities. Adversarial learning establishes a correlation between a specified perturbation layer and the misbehavior of the fine-tuned model. We conducted extensive experiments on the VQA-v2 dataset using a range of metrics. The results show that our proposed method can effectively adapt to a fine-tuned model 
&lt;/p&gt;</description></item><item><title>Magnushammer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#22823;&#24133;&#24230;&#36229;&#36234;&#20256;&#32479;&#31526;&#21495;&#31995;&#32479;&#65292;&#24182;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#25552;&#39640;&#21040;71.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.04488</link><description>&lt;p&gt;
Magnushammer: &#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04488
&lt;/p&gt;
&lt;p&gt;
Magnushammer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#22823;&#24133;&#24230;&#36229;&#36234;&#20256;&#32479;&#31526;&#21495;&#31995;&#32479;&#65292;&#24182;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#25552;&#39640;&#21040;71.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#25552;&#36873;&#25321;&#26159;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#24120;&#24120;&#20351;&#29992;&#22797;&#26434;&#30340;&#31526;&#21495;&#26041;&#27861;&#65292;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#24037;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#31070;&#32463;&#36716;&#25442;&#22120;&#30340;Magnushammer&#26041;&#27861;&#21487;&#20197;&#22823;&#24133;&#24230;&#22320;&#36229;&#36234;&#20256;&#32479;&#30340;&#31526;&#21495;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#65292;Magnushammer&#30340;&#35777;&#26126;&#29575;&#36798;&#21040;&#20102;59.5&#65285;&#65292;&#32780;&#26368;&#25104;&#29087;&#21644;&#27969;&#34892;&#30340;&#22522;&#20110;&#31526;&#21495;&#30340;&#27714;&#35299;&#22120;Sledgehammer&#30340;&#35777;&#26126;&#29575;&#21482;&#26377;38.3&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;Magnushammer&#19982;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#24418;&#24335;&#35777;&#26126;&#22120;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#22823;&#24133;&#25552;&#39640;&#21040;71.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Premise selection is a fundamental problem of automated theorem proving. Previous works often use intricate symbolic methods, rely on domain knowledge, and require significant engineering effort to solve this task. In this work, we show that Magnushammer, a neural transformer-based approach, can outperform traditional symbolic systems by a large margin. Tested on the PISA benchmark, Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of Sledgehammer, the most mature and popular symbolic-based solver. Furthermore, by combining Magnushammer with a neural formal prover based on a language model, we significantly improve the previous state-of-the-art proof rate from $57.0\%$ to $71.0\%$.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2302.01622</link><description>&lt;p&gt;
&#31169;&#23494;&#12289;&#20844;&#24179;&#19988;&#31934;&#30830;&#65306;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#35757;&#32451;&#22823;&#35268;&#27169;&#38544;&#31169;&#20445;&#25252;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#39640;&#24230;&#25935;&#24863;&#24615;&#65292;&#38656;&#35201;&#37319;&#21462;&#29305;&#27530;&#25514;&#26045;&#30830;&#20445;&#20854;&#20445;&#25252;&#12290;&#20445;&#25252;&#38544;&#31169;&#30340;&#40644;&#37329;&#26631;&#20934;&#26159;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;DP&#23545;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#22312;&#21307;&#23398;&#20013;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#24182;&#19988;&#26159;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23545;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;N=193,311&#65289;&#30340;&#39640;&#36136;&#37327;&#20020;&#24202;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65292;&#21644;&#65288;2&#65289;&#19968;&#20010;&#25968;&#25454;&#38598;&#65288;N=1,625&#65289;&#30340;3D&#33145;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#65292;&#29992;&#20110;&#20998;&#31867;&#33008;&#33146;&#23548;&#31649;&#33146;&#30284;&#65288;PDAC&#65289;&#30340;&#23384;&#22312;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20026;&#22238;&#39038;&#24615;&#37319;&#38598;&#65292;&#24182;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#23398;&#24433;&#20687;&#19987;&#23478;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11476</link><description>&lt;p&gt;
&#20351;&#29992;Tsallis KL&#25955;&#24230;&#30340;&#24191;&#20041;Munchausen&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11476
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#37117;&#37319;&#29992;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#21040;&#19978;&#19968;&#20010;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#31574;&#30053;&#21464;&#21270;&#36807;&#24555;&#12290;&#36825;&#20010;&#24819;&#27861;&#26368;&#21021;&#26159;&#22312;Conservative Policy Iteration&#30340;&#19968;&#31687;&#37325;&#35201;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#65292;&#36817;&#20284;&#31639;&#27861;&#22914;TRPO&#21644;Munchausen Value Iteration&#65288;MVI&#65289;&#32473;&#20986;&#20102;&#26377;&#38480;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19968;&#31181;&#24191;&#20041;&#30340;KL&#25955;&#24230; - &#31216;&#20026;Tsallis KL&#25955;&#24230; - &#26469;&#32487;&#32493;&#36825;&#19968;&#24037;&#20316;&#65292;&#23427;&#22312;&#23450;&#20041;&#20013;&#20351;&#29992;&#20102;$q$-&#23545;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#25512;&#24191;&#65292;&#22240;&#20026;$q = 1$&#23545;&#24212;&#20110;&#26631;&#20934;&#30340;KL&#25955;&#24230;&#65307;$q &gt; 1$&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#23545;&#22312;Tsallis KL&#19979;&#23398;&#20064;&#30340;&#31574;&#30053;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#38416;&#36848;&#20102;&#20309;&#26102;$ q &gt; 1 $&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#23558;Tsallis KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;MVI&#65292;&#23427;&#26159;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#21253;&#21547;KL&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24191;&#20041;MVI&#65288;$q$&#65289;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q &gt; 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q &gt;1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#32780;&#22522;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14769</link><description>&lt;p&gt;
&#21521;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65311;&#24314;&#31435;&#25308;&#21344;&#24237;&#40065;&#26834;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#30340;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#32780;&#22522;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#21270;&#20307;&#31995;&#19979;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#36890;&#36807;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#65288;&#21363;&#19981;&#21516;&#29615;&#22659;&#65289;&#20013;&#20445;&#25345;&#25968;&#25454;&#26469;&#20445;&#25252;&#20010;&#20154;&#35270;&#35273;&#29615;&#22659;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#19979;&#65292;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#23545;&#26381;&#21153;&#22120;&#26159;&#19981;&#21487;&#35775;&#38382;&#30340;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#36731;&#26131;&#22320;&#27745;&#26579;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#22312;&#19981;&#34987;&#36890;&#30693;&#30340;&#24773;&#20917;&#19979;&#22312;&#20195;&#29702;&#20154;&#20013;&#24314;&#31435;&#21518;&#38376;&#12290;&#20351;&#29992;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#20250;&#23545;&#20154;&#31867;&#26500;&#25104;&#28508;&#22312;&#21361;&#23475;&#65292;&#22240;&#20026;&#25915;&#20987;&#32773;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#21518;&#38376;&#25805;&#32437;&#20195;&#29702;&#20154;&#36827;&#34892;&#23548;&#33322;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20219;&#21153;&#20013;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#38656;&#35201;&#36319;&#38543;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#23548;&#33322;&#23460;&#20869;&#29615;&#22659;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21363;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#65292;&#20854;&#20013;&#24694;&#24847;&#23458;&#25143;&#31471;&#36890;&#36807;&#25805;&#32437;&#26412;&#22320;&#36712;&#36857;&#25968;&#25454;&#26469;&#21521;&#20840;&#23616;&#27169;&#22411;&#26893;&#20837;&#21518;&#38376;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NAW&#21487;&#20197;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#32780;&#19988;&#24615;&#33021;&#19979;&#38477;&#24494;&#19981;&#36275;&#36947;&#12290;&#20026;&#20102;&#38450;&#27490;NAW&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#27010;&#24565;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#12290;&#25105;&#20204;&#22312;VLN&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#32852;&#37030;&#21270;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW&#65292;&#30456;&#36739;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05208</link><description>&lt;p&gt;
&#32593;&#32476;&#27969;&#30340;&#22270;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Modeling of Network Flows. (arXiv:2209.05208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW&#65292;&#30456;&#36739;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#38382;&#39064;&#28041;&#21450;&#23558;&#27969;&#37327;&#20998;&#24067;&#22312;&#32593;&#32476;&#20013;&#65292;&#20197;&#20351;&#22522;&#30784;&#35774;&#26045;&#24471;&#21040;&#26377;&#25928;&#21033;&#29992;&#65292;&#36825;&#22312;&#20132;&#36890;&#36816;&#36755;&#21644;&#29289;&#27969;&#20013;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#20854;&#20013;&#65292;&#22810;&#21830;&#21697;&#32593;&#32476;&#27969; (MCNF) &#38382;&#39064;&#26159;&#26222;&#36941;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#22312;&#22810;&#20010;&#28304;&#21644;&#27719;&#20043;&#38388;&#20998;&#37197;&#19981;&#21516;&#22823;&#23567;&#30340;&#22810;&#20010;&#27969;&#65292;&#21516;&#26102;&#23454;&#29616;&#38142;&#36335;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;&#30001;&#20110;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#30340;&#21560;&#24341;&#21147;&#65292;&#36825;&#20123;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW (Per-Edge Weights)&#12290;&#27492;&#26041;&#27861;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#24182;&#27839;&#30528;&#27599;&#20010;&#38142;&#25509;&#20351;&#29992;&#19981;&#21516;&#21442;&#25968;&#21270;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992; $17$ &#20010;&#26381;&#21153;&#25552;&#20379;&#21830;&#25299;&#25169;&#21644; $2$ &#20010;&#36335;&#30001;&#26041;&#26696;&#36827;&#34892;&#20114;&#32852;&#32593;&#27969;&#37327;&#36335;&#30001;&#26696;&#20363;&#30740;&#31350;&#65292;&#23545;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; PEW &#30456;&#23545;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network flow problems, which involve distributing traffic over a network such that the underlying infrastructure is used effectively, are ubiquitous in transportation and logistics. Among them, the Multi-Commodity Network Flow (MCNF) problem is of general interest, as it concerns the distribution of multiple flows of different sizes between several sources and sinks, while achieving effective utilization of the links. Due to the appeal of data-driven optimization, these problems have increasingly been approached using graph learning methods. In this paper, we propose a novel graph learning architecture for network flow problems called Per-Edge Weights (PEW). This method builds on a Graph Attention Network and uses distinctly parametrized message functions along each link. We extensively evaluate the proposed solution through an Internet flow routing case study using $17$ Service Provider topologies and $2$ routing schemes. We show that PEW yields substantial gains over architectures wh
&lt;/p&gt;</description></item></channel></rss>