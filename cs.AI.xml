<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.18070</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#26159;&#21542;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30456;&#21516;&#30340;&#35748;&#30693;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18070
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35748;&#30693;&#27169;&#22411;&#24863;&#20852;&#36259;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#20102;&#35299;LLMs&#33021;&#22815;&#27169;&#25311;&#21738;&#20123;&#35748;&#30693;&#29305;&#24615;&#20197;&#21450;&#21738;&#20123;&#19981;&#33021;&#27169;&#25311;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20799;&#31461;&#24050;&#30693;&#35748;&#30693;&#20559;&#35265;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#35843;&#26597;&#23398;&#20064;&#31185;&#23398;&#25991;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#26126;&#30830;&#30340;&#27493;&#39588;&#65306;&#25991;&#26412;&#29702;&#35299;&#12289;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#21644;&#35299;&#20915;&#26041;&#26696;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#27493;&#39588;&#26500;&#24314;&#20102;&#27979;&#35797;&#65292;&#20197;&#20102;&#35299;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22914;&#20309;&#24544;&#23454;&#22320;&#27169;&#25311;&#36825;&#20010;&#36807;&#31243;&#30340;&#21738;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#20026;&#27599;&#20010;&#27979;&#35797;&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#21333;&#35789;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#20004;&#20010;&#35299;&#20915;&#36807;&#31243;&#30340;&#27493;&#39588;&#20013;&#65292;&#19981;&#35770;&#26159;&#21542;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#65292;&#37117;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but 
&lt;/p&gt;</description></item><item><title>SpeechComposer&#26159;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#21512;&#25104;&#22266;&#23450;&#25552;&#31034;&#26631;&#35760;&#26469;&#32479;&#19968;&#22810;&#20010;&#35821;&#38899;&#20219;&#21153;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#39640;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;</title><link>https://arxiv.org/abs/2401.18045</link><description>&lt;p&gt;
SpeechComposer: &#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#32479;&#19968;&#22810;&#20010;&#35821;&#38899;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18045
&lt;/p&gt;
&lt;p&gt;
SpeechComposer&#26159;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#21512;&#25104;&#22266;&#23450;&#25552;&#31034;&#26631;&#35760;&#26469;&#32479;&#19968;&#22810;&#20010;&#35821;&#38899;&#20219;&#21153;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#39640;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#22823;&#22823;&#25552;&#21319;&#20102;&#22810;&#31181;&#35821;&#38899;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#30340;&#25552;&#31034;&#26631;&#35760;&#23558;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#32479;&#19968;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#35745;&#24573;&#30053;&#20102;&#19981;&#21516;&#35821;&#38899;&#20219;&#21153;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#65292;&#36825;&#21487;&#33021;&#20250;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21482;&#35299;&#30721;&#22120;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;SpeechComposer&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#32452;&#21512;&#19968;&#32452;&#22266;&#23450;&#30340;&#25552;&#31034;&#26631;&#35760;&#32479;&#19968;&#24120;&#35265;&#30340;&#35821;&#38899;&#20219;&#21153;&#12290;SpeechComposer&#24314;&#31435;&#22312;&#22235;&#20010;&#20027;&#35201;&#20219;&#21153;&#30340;&#22522;&#30784;&#19978;--&#35821;&#38899;&#21512;&#25104;&#12289;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26412;&#35821;&#35328;&#24314;&#27169;--&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#26631;&#35760;&#30340;&#32452;&#21512;&#65292;&#22914;&#22768;&#38899;&#36716;&#25442;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#35821;&#38899;&#20219;&#21153;&#12290;&#25552;&#31034;&#26631;&#35760;&#30340;&#32479;&#19968;&#20063;&#20351;&#24471;&#19981;&#21516;&#35821;&#38899;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#26356;&#21152;&#32467;&#26500;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in language models have significantly enhanced performance in multiple speech-related tasks. Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task. In this work, we propose a novel decoder-only speech language model, SpeechComposer, that can unify common speech tasks by composing a fixed set of prompt tokens. Built upon four primary tasks -- speech synthesis, speech recognition, speech language modeling, and text language modeling -- SpeechComposer can easily extend to more speech tasks via compositions of well-designed prompt tokens, like voice conversion and speech enhancement. The unification of prompt tokens also makes it possible for knowledge sharing among different speech tasks in a more structured manner. Experimental results demonstrate that our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#25945;&#25480;&#26234;&#33021;&#20307;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#65292;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20854;&#21028;&#26029;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.18040</link><description>&lt;p&gt;
&#21152;&#24378;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#65306;&#22522;&#20110;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25913;&#36827;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#25945;&#25480;&#26234;&#33021;&#20307;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#65292;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20854;&#21028;&#26029;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#36890;&#36807;&#23545;&#35805;&#27969;&#27700;&#32447;&#30340;&#29420;&#31435;&#27169;&#22359;&#36827;&#34892;&#35774;&#35745;&#12290;&#20854;&#20013;&#65292;&#31574;&#30053;&#27169;&#22359;&#26159;&#20915;&#23450;&#23545;&#29992;&#25143;&#36755;&#20837;&#22914;&#20309;&#21709;&#24212;&#30340;&#20851;&#38190;&#12290;&#36825;&#20010;&#31574;&#30053;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#21453;&#39304;&#20449;&#21495;&#24418;&#24335;&#30340;&#29615;&#22659;&#20013;&#25509;&#25910;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23545;&#35805;&#31995;&#32479;&#21482;&#25552;&#20379;&#20102;&#31232;&#32570;&#19988;&#31616;&#21333;&#30340;&#22870;&#21169;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#31639;&#27861;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#24555;&#36895;&#21152;&#36895;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#25945;&#25480;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#26469;&#25552;&#39640;&#21028;&#26029;&#20854;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#21644;&#22909;&#22855;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27979;&#37327;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#35805;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#40723;&#21169;&#25506;&#32034;&#12290;&#22312;&#19968;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;MultiWOZ&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, sho
&lt;/p&gt;</description></item><item><title>Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.18034</link><description>&lt;p&gt;
Paramanu: &#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18034
&lt;/p&gt;
&lt;p&gt;
Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Gyan AI Paramanu&#65288;&#8220;&#21407;&#23376;&#8221;&#65289;&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#23427;&#26159;&#19968;&#20010;&#22312;&#21333;&#20010;GPU&#19978;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#30340;&#21253;&#21547;&#21333;&#35821;&#12289;&#21452;&#35821;&#21644;&#22810;&#35821;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#28085;&#30422;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#65288;&#38463;&#33832;&#22982;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#24247;&#22350;&#23612;&#35821;&#12289;&#36808;&#33922;&#21033;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#26805;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#20197;&#21450;5&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;&#23383;&#27597;&#34920;&#65288;&#23391;&#21152;&#25289;&#35821;&#12289;&#22825;&#22478;&#20307;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;1024&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#22312;&#21333;&#20010;GPU&#19978;&#39044;&#35757;&#32451;&#65292;&#38750;&#24120;&#39640;&#25928;&#12289;&#23567;&#24039;&#12289;&#24555;&#36895;&#19988;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20808;&#36827;&#30340;&#21360;&#24230;&#35821;&#20998;&#35789;&#22120;&#65292;&#29978;&#33267;&#21487;&#20197;&#26631;&#35760;&#26410;&#30693;&#35821;&#35328;&#12290;&#20026;&#20102;&#36991;&#20813;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;mParamanu&#27169;&#22411;&#20013;&#30340;&#8220;&#22810;&#35821;&#35328;&#35781;&#21650;&#8221;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#23383;&#27597;&#34920;&#25353;&#35821;&#35328;&#31867;&#22411;&#36827;&#34892;&#20102;&#21487;&#27604;&#36739;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#35780;&#20272;&#25351;&#26631;&#21253;&#25324;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Gyan AI Paramanu ("atom"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the "curse of multi-linguality" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#26469;&#22686;&#24378;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;LLMs&#27169;&#22411;&#22312;&#29983;&#25104;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.18028</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#25903;&#25345;&#39044;&#26399;&#27835;&#29702;: &#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#26469;&#22686;&#24378;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;LLMs&#27169;&#22411;&#22312;&#29983;&#25104;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21457;&#23637;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#39044;&#27979;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20351;&#29992;LLMs&#22686;&#24378;&#21644;&#25351;&#23548;&#36825;&#19968;&#36807;&#31243;&#26159;&#19968;&#31181;&#19981;&#22826;&#34987;&#30740;&#31350;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#23613;&#31649;LLMs&#21644;&#35780;&#20272;&#25351;&#26631;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#32771;&#34385;&#20559;&#24046;&#26041;&#38754;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#22914;&#20309;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;LLMs&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#24341;&#21457;&#20102;&#20851;&#20110;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#31867;&#21035;&#30340;&#36136;&#37327;&#21644;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20016;&#23500;&#30340;&#21253;&#21547;&#23545;&#26032;&#20852;&#25216;&#26415;&#30340;&#35268;&#33539;&#24615;&#35780;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#8212;&#8212;&#26032;&#38395;&#23186;&#20307;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35745;&#31639;&#20998;&#26512;&#20840;&#29699;&#25968;&#30334;&#20010;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#21457;&#24067;&#30340;&#25968;&#21315;&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;LLMs&#27169;&#22411;&#65288;GPT-4&#31561;&#65289;&#21644;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#29983;&#25104;&#30340;&#24433;&#21709;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development. An understudied approach to such anticipation is the use of LLMs to enhance and guide this process. Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks. Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating. In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against. By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts. We then evaluate both instruction-based (GPT-4 and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#20010;&#21035;&#25551;&#32472;&#20301;&#20110;&#35199;&#29677;&#29273;&#20869;&#21326;&#36798;&#23665;&#33033;&#26519;&#32447;&#19978;&#30340;&#26460;&#26494;&#28748;&#26408;&#12290;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#36965;&#24863;&#24433;&#20687;&#21644;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26816;&#27979;&#21644;&#25551;&#32472;&#33258;&#28982;&#23545;&#35937;&#22797;&#26434;&#29983;&#38271;&#27169;&#24335;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.17985</link><description>&lt;p&gt;
&#21315;&#38754;&#28748;&#26408;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#20010;&#20307;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Shrub of a thousand faces: an individual segmentation from satellite images using deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#20010;&#21035;&#25551;&#32472;&#20301;&#20110;&#35199;&#29677;&#29273;&#20869;&#21326;&#36798;&#23665;&#33033;&#26519;&#32447;&#19978;&#30340;&#26460;&#26494;&#28748;&#26408;&#12290;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#36965;&#24863;&#24433;&#20687;&#21644;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26816;&#27979;&#21644;&#25551;&#32472;&#33258;&#28982;&#23545;&#35937;&#22797;&#26434;&#29983;&#38271;&#27169;&#24335;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#38271;&#23551;&#28748;&#26408;&#65288;&#22914;&#24120;&#35265;&#30340;&#26460;&#26494;&#65289;&#30340;&#20998;&#24067;&#21644;&#22823;&#23567;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#27668;&#20505;&#21464;&#21270;&#23545;&#39640;&#23665;&#21644;&#39640;&#32428;&#24230;&#29983;&#24577;&#31995;&#32479;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;&#21382;&#21490;&#33322;&#31354;&#36229;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#25552;&#20379;&#20102;&#19968;&#31181;&#22238;&#39038;&#24615;&#24037;&#20855;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#30417;&#27979;&#28748;&#26408;&#30340;&#29983;&#38271;&#21644;&#20998;&#24067;&#12290;&#30446;&#21069;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26816;&#27979;&#21644;&#25551;&#32472;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#24418;&#29366;&#30340;&#23545;&#35937;&#30340;&#36718;&#24275;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#20110;&#26816;&#27979;&#34920;&#36798;&#22797;&#26434;&#29983;&#38271;&#27169;&#24335;&#30340;&#33258;&#28982;&#23545;&#35937;&#65288;&#20363;&#22914;&#26460;&#26494;&#65289;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36965;&#24863;RGB&#24433;&#20687;&#19982;&#22522;&#20110;Mask R-CNN&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20010;&#21035;&#25551;&#32472;&#35199;&#29677;&#29273;&#20869;&#21326;&#36798;&#23665;&#33033;&#26519;&#32447;&#19978;&#30340;&#26460;&#26494;&#28748;&#26408;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#26500;&#36896;&#35774;&#35745;&#65292;&#20854;&#20013;&#20351;&#29992;&#30340;&#26159;&#20809;&#35299;&#35793;&#25968;&#25454;&#65288;PI&#65289;&#21644;&#23454;&#22320;&#35843;&#26597;&#25968;&#25454;&#65288;FW&#65289;&#20998;&#21035;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems. Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision. Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes. However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain). In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#25552;&#39640;&#22270;&#20687;&#29702;&#35299;&#21644;&#20943;&#23569;&#22238;&#31572;&#38169;&#35823;&#25554;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2401.17981</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#26816;&#27979;&#27169;&#22411;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#25552;&#39640;&#22270;&#20687;&#29702;&#35299;&#21644;&#20943;&#23569;&#22238;&#31572;&#38169;&#35823;&#25554;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38598;&#25104;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#32454;&#33410;&#35270;&#35273;&#20803;&#32032;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;MLLMs&#32467;&#21512;&#65292;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#29702;&#35299;&#65292;&#24182;&#20943;&#23569;&#22238;&#31572;&#20013;&#30340;&#38169;&#35823;&#25554;&#20837;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#26816;&#27979;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#36825;&#31181;&#34701;&#21512;&#23545;MLLMs&#30340;&#21407;&#22987;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#20114;&#25442;&#24615;&#12290;&#25105;&#20204;&#23545;LLaVA-1.5&#12289;DINO&#21644;PaddleOCRv2&#31561;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;MLLMs&#22312;&#29305;&#23450;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20445;&#25345;&#20102;&#23427;&#20204;&#30340;&#21407;&#22987;&#20248;&#21183;&#12290;&#36890;&#36807;&#22312;10&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22686;&#24378;&#30340;MLLMs&#22312;9&#20010;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#26631;&#20934;&#21270;&#24179;&#22343;&#24471;&#20998;&#25552;&#21319;&#39640;&#36798;12.99%&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#22810;&#26680;&#37327;&#23376;&#26550;&#26500;&#30340;&#30005;&#36335;&#20998;&#21306;&#26041;&#27861;&#65292;&#26088;&#22312;&#25512;&#21160;&#37327;&#23376;&#35745;&#31639;&#21644;&#22270;&#20998;&#21306;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2401.17976</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23545;&#22810;&#26680;&#37327;&#23376;&#26550;&#26500;&#36827;&#34892;&#30005;&#36335;&#20998;&#21306;
&lt;/p&gt;
&lt;p&gt;
Circuit Partitioning for Multi-Core Quantum Architectures with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#22810;&#26680;&#37327;&#23376;&#26550;&#26500;&#30340;&#30005;&#36335;&#20998;&#21306;&#26041;&#27861;&#65292;&#26088;&#22312;&#25512;&#21160;&#37327;&#23376;&#35745;&#31639;&#21644;&#22270;&#20998;&#21306;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#20855;&#26377;&#35299;&#20915;&#32463;&#20856;&#38590;&#39064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#37327;&#23376;&#26550;&#26500;&#30340;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#22810;&#26680;&#37327;&#23376;&#26550;&#26500;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#24341;&#21457;&#20102;&#30828;&#20214;&#12289;&#36890;&#20449;&#21644;&#32534;&#35793;&#31561;&#19968;&#31995;&#21015;&#26032;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#23558;&#37327;&#23376;&#31639;&#27861;&#36866;&#24212;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#19981;&#21516;&#26680;&#24515;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30005;&#36335;&#20998;&#21306;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#37327;&#23376;&#35745;&#31639;&#21644;&#22270;&#20998;&#21306;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25972;&#21512;&#21040;&#37327;&#23376;&#30005;&#36335;&#26144;&#23556;&#20013;&#30340;&#31532;&#19968;&#27493;&#65292;&#20026;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#24320;&#36767;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing holds immense potential for solving classically intractable problems by leveraging the unique properties of quantum mechanics. The scalability of quantum architectures remains a significant challenge. Multi-core quantum architectures are proposed to solve the scalability problem, arising a new set of challenges in hardware, communications and compilation, among others. One of these challenges is to adapt a quantum algorithm to fit within the different cores of the quantum computer. This paper presents a novel approach for circuit partitioning using Deep Reinforcement Learning, contributing to the advancement of both quantum computing and graph partitioning. This work is the first step in integrating Deep Reinforcement Learning techniques into Quantum Circuit Mapping, opening the door to a new paradigm of solutions to such problems.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#32534;&#30721;&#29702;&#35770;&#21644;&#31070;&#32463;&#31185;&#23398;&#24037;&#20855;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#36341;&#26041;&#27861;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#35821;&#20041;&#24615;&#65292;&#24182;&#23545;&#22810;&#35821;&#20041;&#31070;&#32463;&#20803;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#20248;&#21183;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2401.17975</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#29702;&#35770;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#35821;&#20041;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding polysemanticity in neural networks through coding theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17975
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#32534;&#30721;&#29702;&#35770;&#21644;&#31070;&#32463;&#31185;&#23398;&#24037;&#20855;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#36341;&#26041;&#27861;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#35821;&#20041;&#24615;&#65292;&#24182;&#23545;&#22810;&#35821;&#20041;&#31070;&#32463;&#20803;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#20248;&#21183;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#20197;&#25417;&#25720;&#30340;&#30446;&#26631;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#23545;&#22823;&#22810;&#25968;&#21333;&#20010;&#31070;&#32463;&#20803;&#23545;&#32593;&#32476;&#36755;&#20986;&#30340;&#24433;&#21709;&#25552;&#20379;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;&#36825;&#20010;&#38480;&#21046;&#26159;&#30001;&#20110;&#22823;&#22810;&#25968;&#31070;&#32463;&#20803;&#30340;&#22810;&#35821;&#20041;&#24615;&#65292;&#21363;&#19968;&#20010;&#32473;&#23450;&#30340;&#31070;&#32463;&#20803;&#21442;&#19982;&#22810;&#20010;&#19981;&#30456;&#20851;&#30340;&#32593;&#32476;&#29366;&#24577;&#65292;&#20351;&#35299;&#37322;&#35813;&#31070;&#32463;&#20803;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#31070;&#32463;&#31185;&#23398;&#21644;&#20449;&#24687;&#35770;&#20013;&#24320;&#21457;&#30340;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#23454;&#36341;&#26041;&#27861;&#65292;&#24182;&#23545;&#22810;&#35821;&#20041;&#24615;&#21644;&#32534;&#30721;&#23494;&#24230;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#28608;&#27963;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#35889;&#26469;&#25512;&#26029;&#32593;&#32476;&#20195;&#30721;&#30340;&#20887;&#20313;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#25237;&#24433;&#22914;&#20309;&#25581;&#31034;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#24179;&#28369;&#30340;&#25110;&#19981;&#21487;&#24494;&#30340;&#20195;&#30721;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;&#20195;&#30721;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20010;&#30456;&#21516;&#30340;&#26694;&#26550;&#35299;&#37322;&#20102;&#22810;&#35821;&#20041;&#31070;&#32463;&#20803;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#20248;&#21183;&#24182;&#35299;&#37322;&#20102;
&lt;/p&gt;
&lt;p&gt;
Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains
&lt;/p&gt;</description></item><item><title>MelNet&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23450;&#21046;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17972</link><description>&lt;p&gt;
MelNet:&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MelNet: A Real-Time Deep Learning Algorithm for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17972
&lt;/p&gt;
&lt;p&gt;
MelNet&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23450;&#21046;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MelNet&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12290;MelNet&#21033;&#29992;KITTI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#35757;&#32451;&#12290;&#32463;&#36807;300&#20010;&#35757;&#32451;&#36718;&#27425;&#65292;MelNet&#36798;&#21040;&#20102;0.732&#30340;mAP&#65288;&#24179;&#22343;&#31934;&#24230;&#65289;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#19977;&#20010;&#22791;&#36873;&#27169;&#22411;&#65288;YOLOv5&#12289;EfficientDet&#21644;Faster-RCNN-MobileNetv3&#65289;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#19982;MelNet&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#26159;&#26377;&#25928;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#39044;&#20808;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#12289;COCO&#21644;Pascal VOC&#65289;&#19978;&#35757;&#32451;&#30340;&#29616;&#26377;&#27169;&#22411;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21478;&#19968;&#20010;&#21457;&#29616;&#24378;&#35843;&#20102;&#26681;&#25454;&#29305;&#23450;&#24773;&#26223;&#21019;&#24314;&#26032;&#27169;&#22411;&#24182;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;MelNet&#22312;150&#20010;&#36718;&#27425;&#21518;&#20063;&#36229;&#36807;&#20102;EfficientDet&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#21518;&#65292;MelNet&#30340;&#24615;&#33021;&#19982;EfficientDet&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, a novel deep learning algorithm for object detection, named MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset for object detection. Following 300 training epochs, MelNet attained an mAP (mean average precision) score of 0.732. Additionally, three alternative models -YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI dataset and juxtaposed with MelNet for object detection.   The outcomes underscore the efficacy of employing transfer learning in certain instances. Notably, preexisting models trained on prominent datasets (e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding underscores the viability of creating a new model tailored to a specific scenario and training it on a specific dataset. This investigation demonstrates that training MelNet exclusively on the KITTI dataset also surpasses EfficientDet after 150 epochs. Consequently, post-training, MelNet's performance closely aligns with that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#31038;&#20132;&#23548;&#33322;&#31574;&#30053;&#12290;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#24314;&#27169;&#26426;&#22120;&#20154;&#21644;&#34892;&#20154;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#21019;&#26032;&#22320;&#32771;&#34385;&#20102;&#22810;&#26426;&#22120;&#20154;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2401.17914</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26426;&#22120;&#20154;&#31038;&#20132;&#23548;&#33322;&#20013;&#30340;&#20851;&#27880;&#22270;
&lt;/p&gt;
&lt;p&gt;
Attention Graph for Multi-Robot Social Navigation with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#31038;&#20132;&#23548;&#33322;&#31574;&#30053;&#12290;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#24314;&#27169;&#26426;&#22120;&#20154;&#21644;&#34892;&#20154;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#21019;&#26032;&#22320;&#32771;&#34385;&#20102;&#22810;&#26426;&#22120;&#20154;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26426;&#22120;&#20154;&#22312;&#34892;&#20154;&#32676;&#20307;&#20013;&#30340;&#23548;&#33322;&#31574;&#30053;&#23545;&#20110;&#22522;&#20110;&#39046;&#22495;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#21512;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#39044;&#27979;&#21487;&#20197;&#27169;&#25311;&#26426;&#22120;&#20154;&#21644;&#34892;&#20154;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#36817;&#26041;&#27861;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#22810;&#26426;&#22120;&#20154;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#31574;&#30053;&#12290;&#21463;&#21040;&#36817;&#26399;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#26469;&#25551;&#36848;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32467;&#21512;&#20102;&#23454;&#20307;&#30340;&#20301;&#32622;&#21644;&#35270;&#37326;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#20351;&#29992;&#20004;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#21512;&#27880;&#24847;&#26426;&#21046;&#12290;&#39318;&#20808;&#65292;&#36793;&#36873;&#25321;&#22120;&#29983;&#25104;&#31232;&#30095;&#22270;&#65292;&#28982;&#21518;&#20247;&#21253;&#21327;&#35843;&#22120;&#24212;&#29992;&#33410;&#28857;&#27880;&#24847;&#21147;&#26469;&#29983;&#25104;&#34920;&#31034;&#27599;&#20010;&#23454;&#20307;&#23545;&#20854;&#20182;&#23454;&#20307;&#24433;&#21709;&#30340;&#22270;&#12290;&#36825;&#34987;&#21152;&#20837;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning robot navigation strategies among pedestrian is crucial for domain based applications. Combining perception, planning and prediction allows us to model the interactions between robots and pedestrians, resulting in impressive outcomes especially with recent approaches based on deep reinforcement learning (RL). However, these works do not consider multi-robot scenarios. In this paper, we present MultiSoc, a new method for learning multi-agent socially aware navigation strategies using RL. Inspired by recent works on multi-agent deep RL, our method leverages graph-based representation of agent interactions, combining the positions and fields of view of entities (pedestrians and agents). Each agent uses a model based on two Graph Neural Network combined with attention mechanisms. First an edge-selector produces a sparse graph, then a crowd coordinator applies node attention to produce a graph representing the influence of each entity on the others. This is incorporated into a mode
&lt;/p&gt;</description></item><item><title>ReplaceAnything3D&#26159;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;3D&#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Erase-and-Replace&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;3D&#19968;&#33268;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26367;&#25442;&#22330;&#26223;&#20013;&#30340;&#29305;&#23450;&#23545;&#35937;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#36924;&#30495;&#30340;3D&#22330;&#26223;&#20013;&#20462;&#25913;&#21069;&#26223;&#23545;&#35937;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17895</link><description>&lt;p&gt;
ReplaceAnything3D&#65306;&#20855;&#26377;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#25991;&#26412;&#24341;&#23548;&#30340;3D&#22330;&#26223;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17895
&lt;/p&gt;
&lt;p&gt;
ReplaceAnything3D&#26159;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;3D&#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Erase-and-Replace&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;3D&#19968;&#33268;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26367;&#25442;&#22330;&#26223;&#20013;&#30340;&#29305;&#23450;&#23545;&#35937;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#36924;&#30495;&#30340;3D&#22330;&#26223;&#20013;&#20462;&#25913;&#21069;&#26223;&#23545;&#35937;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;ReplaceAnything3D&#27169;&#22411;&#65288;RAM3D&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#24341;&#23548;&#30340;3D&#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#25442;&#22330;&#26223;&#20013;&#30340;&#29305;&#23450;&#23545;&#35937;&#12290;&#32473;&#23450;&#19968;&#20010;&#22330;&#26223;&#30340;&#22810;&#35270;&#35282;&#22270;&#20687;&#12289;&#25551;&#36848;&#35201;&#26367;&#25442;&#23545;&#35937;&#30340;&#25991;&#26412;&#25552;&#31034;&#21644;&#25551;&#36848;&#26032;&#23545;&#35937;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;Erase-and-Replace&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#26032;&#29983;&#25104;&#30340;&#20869;&#23481;&#20132;&#25442;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#65292;&#21516;&#26102;&#20445;&#25345;&#22810;&#35270;&#35282;&#30340;3D&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ReplaceAnything3D&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#36924;&#30495;&#30340;3D&#22330;&#26223;&#65292;&#24182;&#23637;&#31034;&#20102;&#20462;&#25913;&#21518;&#30340;&#21069;&#26223;&#23545;&#35937;&#19982;&#22330;&#26223;&#30340;&#20854;&#20182;&#37096;&#20998;&#34701;&#21512;&#24471;&#24456;&#22909;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20854;&#25972;&#20307;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene editing method that enables the replacement of specific objects within a scene. Given multi-view images of a scene, a text prompt describing the object to replace, and a text prompt describing the new object, our Erase-and-Replace approach can effectively swap objects in the scene with newly generated content while maintaining 3D consistency across multiple viewpoints. We demonstrate the versatility of ReplaceAnything3D by applying it to various realistic 3D scenes, showcasing results of modified foreground objects that are well-integrated with the rest of the scene without affecting its overall integrity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38598;&#25104;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#26469;&#25913;&#36827;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17870</link><description>&lt;p&gt;
&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#36827;&#34892;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38598;&#25104;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#26469;&#25913;&#36827;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27425;&#23395;&#33410;&#39044;&#25253;&#23545;&#20892;&#19994;&#12289;&#27700;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#39044;&#35686;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#22823;&#27668;&#30340;&#28151;&#27788;&#24615;&#65292;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#36890;&#36807;&#23454;&#29616;&#19982;&#25968;&#20540;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#38761;&#26032;&#20102;&#22825;&#27668;&#39044;&#25253;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#36825;&#23548;&#33268;&#30456;&#24403;&#22810;&#30340;&#30899;&#25490;&#25918;&#65292;&#24182;&#38480;&#21046;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#36890;&#36807;&#20135;&#29983;&#24179;&#28369;&#30340;&#32467;&#26524;&#26469;&#24858;&#24324;&#20687;&#32032;&#35823;&#24046;&#35780;&#20998;&#65292;&#36825;&#20123;&#32467;&#26524;&#32570;&#20047;&#29289;&#29702;&#19968;&#33268;&#24615;&#21644;&#27668;&#35937;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Pangu&#27169;&#22411;&#26469;&#33719;&#24471;&#33391;&#22909;&#30340;&#21021;&#22987;&#26435;&#37325;&#65292;&#24182;&#38598;&#25104;&#20102;&#19968;&#20010;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#22312;&#24310;&#38271;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#35843;&#25972;Pangu&#27169;&#22411;&#30340;1.1%&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#27833;&#27668;&#24037;&#19994;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#36807;&#31243;&#20013;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#21487;&#20197;&#32452;&#21512;&#26469;&#24418;&#25104;&#21327;&#20316;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#20123;&#24819;&#27861;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17866</link><description>&lt;p&gt;
&#29702;&#35299;&#30693;&#35782;&#23494;&#38598;&#22411;&#36807;&#31243;&#65306;&#20197;&#27833;&#27668;&#24037;&#19994;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Making Sense of Knowledge Intensive Processes: an Oil &amp; Gas Industry Scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#27833;&#27668;&#24037;&#19994;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#36807;&#31243;&#20013;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#21487;&#20197;&#32452;&#21512;&#26469;&#24418;&#25104;&#21327;&#20316;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#20123;&#24819;&#27861;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26159;&#19968;&#31181;&#19981;&#26029;&#36827;&#34892;&#30340;&#36807;&#31243;&#65292;&#20154;&#20204;&#36890;&#36807;&#32463;&#39564;&#32473;&#20107;&#29289;&#36171;&#20104;&#24847;&#20041;&#12290;&#23427;&#21487;&#20197;&#26159;&#20010;&#21035;&#30340;&#36807;&#31243;&#65292;&#34987;&#31216;&#20026;&#8220;&#29468;&#27979;&#8221;&#65292;&#20063;&#21487;&#20197;&#26159;&#19968;&#31181;&#32676;&#20307;&#36807;&#31243;&#65292;&#20154;&#20204;&#36890;&#36807;&#23427;&#32473;&#38598;&#20307;&#32463;&#39564;&#36171;&#20104;&#24847;&#20041;&#12290;&#32676;&#20307;&#30340;&#29702;&#35299;&#21463;&#21040;&#27599;&#20010;&#20154;&#23545;&#32463;&#39564;&#30340;&#29468;&#27979;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#27599;&#20010;&#21327;&#20316;&#36807;&#31243;&#37117;&#38656;&#35201;&#19968;&#23450;&#31243;&#24230;&#30340;&#29702;&#35299;&#25165;&#33021;&#21462;&#24471;&#32467;&#26524;&#12290;&#23545;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#36807;&#31243;&#65292;&#29702;&#35299;&#26159;&#20013;&#24515;&#21644;&#19982;&#22823;&#22810;&#25968;&#20219;&#21153;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27833;&#27668;&#24037;&#19994;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#36807;&#31243;&#20013;&#36827;&#34892;&#30340;&#23454;&#22320;&#35843;&#26597;&#25552;&#20986;&#20102;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#26469;&#32452;&#25104;&#29702;&#35299;&#36807;&#31243;&#30340;&#32467;&#26524;&#65288;&#22914;&#20915;&#31574;&#12289;&#38656;&#35201;&#26356;&#22810;&#35752;&#35770;&#31561;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#21021;&#22987;&#30340;&#30693;&#35782;&#31867;&#22411;&#65292;&#21487;&#20197;&#32452;&#21512;&#36215;&#26469;&#24418;&#25104;&#21327;&#20316;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#20123;&#24819;&#27861;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensemaking is a constant and ongoing process by which people associate meaning to experiences. It can be an individual process, known as abduction, or a group process by which people give meaning to collective experiences. The sensemaking of a group is influenced by the abduction process of each person about the experience. Every collaborative process needs some level of sensemaking to show results. For a knowledge intensive process, sensemaking is central and related to most of its tasks. We present findings from a fieldwork executed in knowledge intensive process from the Oil and Gas industry. Our findings indicated that different types of knowledge can be combined to compose the result of a sensemaking process (e.g. decision, the need for more discussion, etc.). This paper presents an initial set of knowledge types that can be combined to compose the result of the sensemaking of a collaborative decision making process. We also discuss ideas for using systems powered by Artificial I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#25955;&#22495;&#20013;&#30340;&#26426;&#22120;&#25945;&#23398;&#65292;&#22312;&#25805;&#32437;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#20540;&#20248;&#21183;&#65292;&#21487;&#29992;&#20110;&#30699;&#27491;&#38169;&#35823;&#30340;&#39044;&#27979;&#25110;&#24694;&#24847;&#25805;&#32437;&#27169;&#22411;&#23454;&#29616;&#20010;&#20154;&#21033;&#30410;&#12290;</title><link>https://arxiv.org/abs/2401.17865</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#25945;&#23398;&#20013;&#25805;&#32437;&#31163;&#25955;&#36755;&#20837;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Manipulating Predictions over Discrete Inputs in Machine Teaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#25955;&#22495;&#20013;&#30340;&#26426;&#22120;&#25945;&#23398;&#65292;&#22312;&#25805;&#32437;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#20540;&#20248;&#21183;&#65292;&#21487;&#29992;&#20110;&#30699;&#27491;&#38169;&#35823;&#30340;&#39044;&#27979;&#25110;&#24694;&#24847;&#25805;&#32437;&#27169;&#22411;&#23454;&#29616;&#20010;&#20154;&#21033;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#25945;&#23398;&#36890;&#24120;&#28041;&#21450;&#21019;&#24314;&#19968;&#20010;&#26368;&#20248;&#65288;&#36890;&#24120;&#26159;&#26368;&#23567;&#30340;&#65289;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#65288;&#34987;&#31216;&#20026;&#8220;&#23398;&#29983;&#8221;&#65289;&#26681;&#25454;&#25945;&#24072;&#32473;&#20986;&#30340;&#29305;&#23450;&#30446;&#26631;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#12290;&#23613;&#31649;&#22312;&#36830;&#32493;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#20294;&#22312;&#31163;&#25955;&#22495;&#20013;&#23545;&#26426;&#22120;&#25945;&#23398;&#30340;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#31163;&#25955;&#22495;&#20013;&#30340;&#26426;&#22120;&#25945;&#23398;&#65292;&#20855;&#20307;&#22320;&#35828;&#65292;&#26159;&#36890;&#36807;&#26377;&#25928;&#22320;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#26469;&#25805;&#32437;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#25945;&#24072;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#36845;&#20195;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25945;&#24072;&#35797;&#22270;&#32416;&#27491;&#38169;&#35823;&#30340;&#39044;&#27979;&#20197;&#25913;&#21892;&#23398;&#29983;&#27169;&#22411;&#65292;&#25110;&#24694;&#24847;&#25805;&#32437;&#27169;&#22411;&#20197;&#38169;&#35823;&#20998;&#31867;&#26576;&#20123;&#29305;&#23450;&#26679;&#26412;&#21040;&#19982;&#20854;&#20010;&#20154;&#21033;&#30410;&#19968;&#33268;&#30340;&#30446;&#26631;&#31867;&#21035;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#20540;&#20248;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#36229;&#32423;
&lt;/p&gt;
&lt;p&gt;
Machine teaching often involves the creation of an optimal (typically minimal) dataset to help a model (referred to as the `student') achieve specific goals given by a teacher. While abundant in the continuous domain, the studies on the effectiveness of machine teaching in the discrete domain are relatively limited. This paper focuses on machine teaching in the discrete domain, specifically on manipulating student models' predictions based on the goals of teachers via changing the training data efficiently. We formulate this task as a combinatorial optimization problem and solve it by proposing an iterative searching algorithm. Our algorithm demonstrates significant numerical merit in the scenarios where a teacher attempts at correcting erroneous predictions to improve the student's models, or maliciously manipulating the model to misclassify some specific samples to the target class aligned with his personal profits. Experimental results show that our proposed algorithm can have super
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#35299;&#37322;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;IOH-Xplainer&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#29702;&#35299;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17842</link><description>&lt;p&gt;
&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#35299;&#37322;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Explainable Benchmarking for Iterative Optimization Heuristics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#35299;&#37322;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;IOH-Xplainer&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#29702;&#35299;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#23545;&#20110;&#29702;&#35299;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#20197;&#21450;&#22312;&#20309;&#31181;&#38382;&#39064;&#19978;&#26576;&#20123;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#23545;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#30340;&#30740;&#31350;&#21482;&#25506;&#32034;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#22330;&#26223;&#12289;&#31639;&#27861;&#37197;&#32622;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#23548;&#33268;&#20102;&#19981;&#23436;&#25972;&#19988;&#24120;&#24120;&#26377;&#20559;&#35265;&#30340;&#35265;&#35299;&#21644;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#21487;&#35299;&#37322;&#22522;&#20934;&#27979;&#35797;&#12290;&#20171;&#32461;&#20102;IOH-Xplainer&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#29702;&#35299;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#20197;&#21450;&#23427;&#20204;&#19981;&#21516;&#32452;&#20214;&#21644;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22359;&#21270;&#20248;&#21270;&#26694;&#26550;&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#31639;&#27861;&#32452;&#20214;&#21644;&#37197;&#32622;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#35299;&#37322;&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#34892;&#20026;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This paper presents a novel approach we call explainable benchmarking. Introducing the IOH-Xplainer software framework, for analyzing and understanding the performance of various optimization algorithms and the impact of their different components and hyper-parameters. We showcase the framework in the context of two modular optimization frameworks. Through this framework, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterativ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT&#27169;&#22411;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24179;&#34913;&#25968;&#25454;&#38598;"&#20840;&#29699;&#35828;&#35854;&#32773;"&#65292;&#32467;&#26524;&#26174;&#31034;&#36739;&#26032;&#30340;GPT&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#24847;&#21619;&#30528;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#19968;&#20010;&#20840;&#29699;&#21335;&#26041;&#38472;&#36848;&#34987;&#20559;&#34962;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17839</link><description>&lt;p&gt;
&#20840;&#29699;&#35828;&#35854;&#32773;&#65306;LLMs&#22312;&#26102;&#38388;&#21644;&#22320;&#29702;&#21306;&#22495;&#19978;&#30340;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Global-Liar: Factuality of LLMs over Time and Geographic Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT&#27169;&#22411;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24179;&#34913;&#25968;&#25454;&#38598;"&#20840;&#29699;&#35828;&#35854;&#32773;"&#65292;&#32467;&#26524;&#26174;&#31034;&#36739;&#26032;&#30340;GPT&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#24847;&#21619;&#30528;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#19968;&#20010;&#20840;&#29699;&#21335;&#26041;&#38472;&#36848;&#34987;&#20559;&#34962;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#20687;GPT&#31995;&#21015;&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20351;&#29992;&#65292;&#31361;&#26174;&#20102;&#23545;&#23427;&#20204;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#32593;&#32476;&#19978;&#34394;&#20551;&#20449;&#24687;&#21644;&#35823;&#23548;&#20449;&#24687;&#29462;&#29527;&#20256;&#25773;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;GPT&#27169;&#22411;&#65288;&#21253;&#25324;GPT-3.5&#21644;GPT-4&#65289;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#20559;&#35265;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20171;&#23548;&#20449;&#24687;&#20256;&#25773;&#30340;&#21487;&#38752;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29420;&#29305;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#8220;&#20840;&#29699;&#35828;&#35854;&#32773;&#8221;&#65292;&#20854;&#22312;&#22320;&#29702;&#21644;&#26102;&#38388;&#34920;&#24449;&#26041;&#38754;&#26377;&#21161;&#20110;&#26356;&#32454;&#33268;&#22320;&#35780;&#20272;LLM&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#36739;&#26032;&#30340;GPT&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#24847;&#21619;&#30528;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;3&#26376;&#21457;&#24067;&#30340;GPT-4&#29256;&#26412;&#26174;&#31034;&#20986;&#27604;&#20854;&#21518;&#32493;6&#26376;&#21457;&#24067;&#29256;&#26412;&#26356;&#39640;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#35266;&#23519;&#21040;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#20559;&#35265;&#65292;&#21363;&#23545;&#20840;&#29699;&#21335;&#26041;&#30340;&#38472;&#36848;&#32473;&#20104;&#20102;&#29305;&#26435;&#65292;&#21487;&#33021;&#21152;&#21095;&#20102;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing reliance on AI-driven solutions, particularly Large Language Models (LLMs) like the GPT series, for information retrieval highlights the critical need for their factuality and fairness, especially amidst the rampant spread of misinformation and disinformation online. Our study evaluates the factual accuracy, stability, and biases in widely adopted GPT models, including GPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated information dissemination.   We introduce 'Global-Liar,' a dataset uniquely balanced in terms of geographic and temporal representation, facilitating a more nuanced evaluation of LLM biases. Our analysis reveals that newer iterations of GPT models do not always equate to improved performance. Notably, the GPT-4 version from March demonstrates higher factual accuracy than its subsequent June release. Furthermore, a concerning bias is observed, privileging statements from the Global North over the Global South, thus potentially exace
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35270;&#35282;&#20998;&#23618;&#22270;&#23398;&#20064;&#36229;&#32593;&#32476;&#65288;CHGH&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#39044;&#27979;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#12290;&#26694;&#26550;&#21253;&#25324;&#36328;&#35270;&#35282;&#22270;&#32534;&#30721;&#22120;&#12289;&#23618;&#27425;&#22270;&#32534;&#30721;&#22120;&#21644;&#26465;&#20214;&#36229;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#38656;&#27714;&#20379;&#24212;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2401.17838</link><description>&lt;p&gt;
&#36328;&#35270;&#35282;&#20998;&#23618;&#22270;&#23398;&#20064;&#36229;&#32593;&#32476;&#29992;&#20110;&#25216;&#33021;&#38656;&#27714;&#20379;&#24212;&#32852;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Cross-View Hierarchical Graph Learning Hypernetwork for Skill Demand-Supply Joint Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35270;&#35282;&#20998;&#23618;&#22270;&#23398;&#20064;&#36229;&#32593;&#32476;&#65288;CHGH&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#39044;&#27979;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#12290;&#26694;&#26550;&#21253;&#25324;&#36328;&#35270;&#35282;&#22270;&#32534;&#30721;&#22120;&#12289;&#23618;&#27425;&#22270;&#32534;&#30721;&#22120;&#21644;&#26465;&#20214;&#36229;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#38656;&#27714;&#20379;&#24212;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#21644;&#20135;&#19994;&#30340;&#36805;&#36895;&#21464;&#21270;&#23548;&#33268;&#25216;&#33021;&#38656;&#27714;&#21160;&#24577;&#21464;&#21270;&#65292;&#22240;&#27492;&#21592;&#24037;&#21644;&#38599;&#20027;&#39044;&#27979;&#36825;&#31181;&#21464;&#21270;&#20197;&#22312;&#21171;&#21160;&#24066;&#22330;&#20445;&#25345;&#31454;&#20105;&#20248;&#21183;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#20381;&#36182;&#20110;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#65292;&#35201;&#20040;&#23558;&#25216;&#33021;&#28436;&#21464;&#35270;&#20026;&#31616;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#20197;&#21450;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#21464;&#21270;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#39044;&#27979;&#30340;&#36328;&#35270;&#35282;&#20998;&#23618;&#22270;&#23398;&#20064;&#36229;&#32593;&#32476;&#65288;CHGH&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CHGH&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#21253;&#25324;&#65306;i) &#19968;&#20010;&#36328;&#35270;&#35282;&#22270;&#32534;&#30721;&#22120;&#29992;&#20110;&#25429;&#25417;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#65292;ii) &#19968;&#20010;&#23618;&#27425;&#22270;&#32534;&#30721;&#22120;&#29992;&#20110;&#20174;&#38598;&#32676;&#35282;&#24230;&#24314;&#27169;&#25216;&#33021;&#30340;&#20849;&#21516;&#28436;&#21464;&#65292;iii) &#19968;&#20010;&#26465;&#20214;&#36229;&#35299;&#30721;&#22120;&#29992;&#20110;&#20849;&#21516;&#39044;&#27979;&#38656;&#27714;&#21644;&#20379;&#24212;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly changing landscape of technology and industries leads to dynamic skill requirements, making it crucial for employees and employers to anticipate such shifts to maintain a competitive edge in the labor market. Existing efforts in this area either rely on domain-expert knowledge or regarding skill evolution as a simplified time series forecasting problem. However, both approaches overlook the sophisticated relationships among different skills and the inner-connection between skill demand and supply variations. In this paper, we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH) framework for joint skill demand-supply prediction. Specifically, CHGH is an encoder-decoder network consisting of i) a cross-view graph encoder to capture the interconnection between skill demand and supply, ii) a hierarchical graph encoder to model the co-evolution of skills from a cluster-wise perspective, and iii) a conditional hyper-decoder to jointly predict demand and supply va
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Swin Transformer&#25552;&#20986;&#20102;"SWTformer"&#65292;&#36890;&#36807;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#30340;&#35270;&#35282;&#26469;&#22686;&#24378;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17828</link><description>&lt;p&gt;
&#21033;&#29992;Swin Transformer&#36827;&#34892;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Swin Transformer&#25552;&#20986;&#20102;"SWTformer"&#65292;&#36890;&#36807;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#30340;&#35270;&#35282;&#26469;&#22686;&#24378;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21033;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#20316;&#20026;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20174;&#31867;&#28608;&#27963;&#22270;(CAM)&#20013;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35299;&#20915;&#36825;&#20123;&#26631;&#31614;&#20013;&#32570;&#20047;&#31354;&#38388;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#23616;&#37096;&#27169;&#24335;&#26816;&#27979;&#65292;CAMs&#36890;&#24120;&#21482;&#24378;&#35843;&#23545;&#35937;&#30340;&#26368;&#20855;&#21306;&#20998;&#24230;&#30340;&#37096;&#20998;&#65292;&#20351;&#24471;&#20934;&#30830;&#21306;&#20998;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#20197;&#21450;&#24444;&#27492;&#20043;&#38388;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#35270;&#35282;&#65292;Vision Transformer (ViT)&#29305;&#24449;&#22312;&#25429;&#25417;&#22330;&#26223;&#24067;&#23616;&#26041;&#38754;&#27604;CNNs&#26356;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23618;&#27425;&#21270;&#30340;ViTs&#22312;&#35813;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#8220;SWTformer&#8221;&#26469;&#25506;&#32034;&#20351;&#29992;Swin Transformer&#26469;&#25552;&#39640;&#21021;&#22987;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of Convolutional Neural Networks (CNNs), CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing "SWTformer" to enhance the accuracy of the init
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#25506;&#32034;&#20102;&#22235;&#31181;&#29983;&#25104;&#25913;&#20889;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#33521;&#35821;&#25913;&#20889;&#21644;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#19981;&#23436;&#20840;&#36866;&#29992;&#20110;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#65292;&#24378;&#35843;&#20102;&#23545;&#20110;&#39640;&#24230;&#21512;&#35789;&#24615;&#35821;&#35328;&#26356;&#32454;&#33268;&#30340;&#25913;&#20889;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2401.17827</link><description>&lt;p&gt;
&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#25913;&#20889;&#29983;&#25104;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation for Malayalam Paraphrase Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#25506;&#32034;&#20102;&#22235;&#31181;&#29983;&#25104;&#25913;&#20889;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#33521;&#35821;&#25913;&#20889;&#21644;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#19981;&#23436;&#20840;&#36866;&#29992;&#20110;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#65292;&#24378;&#35843;&#20102;&#23545;&#20110;&#39640;&#24230;&#21512;&#35789;&#24615;&#35821;&#35328;&#26356;&#32454;&#33268;&#30340;&#25913;&#20889;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22235;&#31181;&#22312;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#20013;&#29983;&#25104;&#25913;&#20889;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33521;&#35821;&#25913;&#20889;&#21644;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#65288;&#22914;BLEU&#65292;METEOR&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#20197;&#21450;&#20154;&#24037;&#26631;&#27880;&#26469;&#35780;&#20272;&#25152;&#24471;&#21040;&#30340;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#23545;&#20110;&#39532;&#25289;&#20122;&#25289;&#22982;&#35821;&#21487;&#33021;&#19981;&#23436;&#20840;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#24046;&#24322;&#31361;&#26174;&#20102;&#23545;&#20110;&#39640;&#24230;&#21512;&#35789;&#24615;&#35821;&#35328;&#23588;&#20854;&#38656;&#35201;&#26356;&#32454;&#33268;&#30340;&#25913;&#20889;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores four methods of generating paraphrases in Malayalam, utilizing resources available for English paraphrasing and pre-trained Neural Machine Translation (NMT) models. We evaluate the resulting paraphrases using both automated metrics, such as BLEU, METEOR, and cosine similarity, as well as human annotation. Our findings suggest that automated evaluation measures may not be fully appropriate for Malayalam, as they do not consistently align with human judgment. This discrepancy underscores the need for more nuanced paraphrase evaluation approaches especially for highly agglutinative languages.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30830;&#23450;&#24615;&#35745;&#31639;&#33021;&#21147;&#32593;&#32476;&#65288;Det-CPN&#65289;&#30340;&#26032;&#32593;&#32476;&#33539;&#24335;&#65292;&#26088;&#22312;&#20026;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#24310;&#36831;&#25935;&#24863;&#30340;&#20219;&#21153;&#31561;&#26032;&#22411;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#31471;&#21040;&#31471;&#20256;&#36755;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#30830;&#23450;&#24615;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#30830;&#20445;&#26381;&#21153;&#30340;&#23433;&#20840;&#39640;&#25928;&#36816;&#34892;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2401.17812</link><description>&lt;p&gt;
&#30830;&#23450;&#24615;&#35745;&#31639;&#33021;&#21147;&#32593;&#32476;&#65306;&#26550;&#26500;&#12289;&#25216;&#26415;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Deterministic Computing Power Networking: Architecture, Technologies and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17812
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30830;&#23450;&#24615;&#35745;&#31639;&#33021;&#21147;&#32593;&#32476;&#65288;Det-CPN&#65289;&#30340;&#26032;&#32593;&#32476;&#33539;&#24335;&#65292;&#26088;&#22312;&#20026;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#24310;&#36831;&#25935;&#24863;&#30340;&#20219;&#21153;&#31561;&#26032;&#22411;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#31471;&#21040;&#31471;&#20256;&#36755;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#30830;&#23450;&#24615;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#30830;&#20445;&#26381;&#21153;&#30340;&#23433;&#20840;&#39640;&#25928;&#36816;&#34892;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#24310;&#36831;&#25935;&#24863;&#30340;&#20219;&#21153;&#31561;&#26032;&#22411;&#20114;&#32852;&#32593;&#26381;&#21153;&#30340;&#21457;&#23637;&#65292;&#20256;&#32479;&#30340;&#8220;&#23613;&#21147;&#32780;&#20026;&#8221;&#32593;&#32476;&#20256;&#36755;&#26041;&#24335;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#32593;&#32476;&#31995;&#32479;&#36843;&#20999;&#38656;&#35201;&#20026;&#26032;&#24212;&#29992;&#25552;&#20379;&#31471;&#21040;&#31471;&#20256;&#36755;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#30830;&#23450;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#23433;&#20840;&#39640;&#25928;&#36816;&#34892;&#12290;&#22312;&#35745;&#31639;&#21644;&#32593;&#32476;&#30340;&#34701;&#21512;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30830;&#23450;&#24615;&#35745;&#31639;&#33021;&#21147;&#32593;&#32476;&#65288;Det-CPN&#65289;&#30340;&#26032;&#32593;&#32476;&#33539; paradigm&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#35745;&#31639;&#33021;&#21147;&#32593;&#32476;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#28982;&#21518;&#20998;&#26512;&#20102;Det-CPN&#30340;&#21160;&#26426;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;Det-CPN&#30340;&#31995;&#32479;&#26550;&#26500;&#12289;&#25216;&#26415;&#33021;&#21147;&#12289;&#24037;&#20316;&#27969;&#31243;&#20197;&#21450;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;Det-CPN&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of new Internet services such as computation-intensive and delay-sensitive tasks, the traditional "Best Effort" network transmission mode has been greatly challenged. The network system is urgently required to provide end-to-end transmission determinacy and computing determinacy for new applications to ensure the safe and efficient operation of services. Based on the research of the convergence of computing and networking, a new network paradigm named deterministic computing power networking (Det-CPN) is proposed. In this article, we firstly introduce the research advance of computing power networking. And then the motivations and scenarios of Det-CPN are analyzed. Following that, we present the system architecture, technological capabilities, workflow as well as key technologies for Det-CPN. Finally, the challenges and future trends of Det-CPN are analyzed and discussed.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#22280;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20197;&#29983;&#24577;&#20013;&#24515;&#20027;&#20041;&#20026;&#22522;&#30784;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#22280;&#30340;&#22797;&#26434;&#24615;&#24182;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#19981;&#20250;&#23545;&#20854;&#36896;&#25104;&#25439;&#23475;&#12290;</title><link>https://arxiv.org/abs/2401.17805</link><description>&lt;p&gt;
&#29983;&#29289;&#22280;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Biospheric AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17805
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#22280;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20197;&#29983;&#24577;&#20013;&#24515;&#20027;&#20041;&#20026;&#22522;&#30784;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#22280;&#30340;&#22797;&#26434;&#24615;&#24182;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#19981;&#20250;&#23545;&#20854;&#36896;&#25104;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21644;&#20215;&#20540;&#35843;&#25972;&#39046;&#22495;&#20013;&#65292;&#20154;&#31867;&#20013;&#24515;&#20027;&#20041;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#36825;&#20123;&#23398;&#31185;&#30340;&#20851;&#27880;&#37325;&#28857;&#20165;&#38480;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#23548;&#33268;&#20854;&#27934;&#23519;&#21147;&#30340;&#28145;&#24230;&#21644;&#24191;&#24230;&#21463;&#38480;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#23398;&#32773;&#24050;&#24320;&#22987;&#23581;&#35797;&#25193;&#22823;&#21040;&#20197;&#24863;&#30693;&#32773;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20004;&#31181;&#35266;&#28857;&#37117;&#19981;&#36275;&#20197;&#25429;&#25417;&#29983;&#29289;&#22280;&#30340;&#23454;&#38469;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#19981;&#20250;&#23545;&#20854;&#36896;&#25104;&#25439;&#23475;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8212;&#8212;&#29983;&#29289;&#22280;&#20154;&#24037;&#26234;&#33021;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#20010;&#29983;&#24577;&#20013;&#24515;&#20027;&#20041;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#34987;&#35774;&#35745;&#30340;&#20551;&#35774;&#24615;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19982;&#29983;&#29289;&#22280;&#21033;&#30410;&#19968;&#33268;&#30340;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#26041;&#21521;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#39033;&#24037;&#20316;&#35797;&#22270;&#36808;&#20986;&#39318;&#35201;&#27493;&#39588;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#29983;&#29289;&#22280;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm in AI ethics and value alignment is highly anthropocentric. The focus of these disciplines is strictly on human values which limits the depth and breadth of their insights. Recently, attempts to expand to a sentientist perspective have been initiated. We argue that neither of these outlooks is sufficient to capture the actual complexity of the biosphere and ensure that AI does not damage it. Thus, we propose a new paradigm -- Biospheric AI that assumes an ecocentric perspective. We discuss hypothetical ways in which such an AI might be designed. Moreover, we give directions for research and application of the modern AI models that would be consistent with the biospheric interests. All in all, this work attempts to take first steps towards a comprehensive program of research that focuses on the interactions between AI and the biosphere.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33976;&#39311;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#21644;&#24102;&#26377;&#21160;&#37327;&#26356;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#33719;&#24471;&#26356;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.17802</link><description>&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#23545;&#27604;&#23398;&#20064;&#30340;&#33976;&#39311;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33976;&#39311;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#21644;&#24102;&#26377;&#21160;&#37327;&#26356;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#33719;&#24471;&#26356;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#22122;&#22768;&#12289;&#19981;&#23436;&#25972;&#24615;&#20197;&#21450;&#30417;&#30563;&#20449;&#21495;&#31232;&#30095;&#24615;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#36890;&#24120;&#32858;&#28966;&#20110;&#26102;&#38388;&#20869;&#37096;&#29305;&#24449;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DE-TSMCL&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#29992;&#20110;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33976;&#39311;&#22686;&#24378;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#26159;&#21542;&#23631;&#34109;&#26102;&#38388;&#25139;&#20197;&#33719;&#24471;&#20248;&#21270;&#30340;&#23376;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21160;&#37327;&#26356;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#25506;&#32034;&#26102;&#38388;&#24207;&#21015;&#30340;&#26679;&#26412;&#38388;&#21644;&#26102;&#38388;&#20869;&#37096;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23398;&#20064;&#26410;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#30340;&#28508;&#22312;&#32467;&#26500;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30417;&#30563;&#20219;&#21153;&#65292;&#20197;&#23398;&#20064;&#26356;&#40065;&#26834;&#30340;&#34920;&#31034;&#24182;&#20419;&#36827;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32852;&#21512;&#20248;&#21270;&#19978;&#36848;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal. However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data. To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting. Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences. Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series. Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process. Finally, we jointly optimize the above two 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17791</link><description>&lt;p&gt;
&#19981;&#24102;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Transformers without Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#21387;&#22120;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#21333;&#29420;&#20351;&#29992;&#36824;&#26159;&#19982;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MP-GNN&#65289;&#32467;&#21512;&#12290;&#23558;&#22270;&#24402;&#32435;&#20559;&#35265;&#34701;&#20837;&#22825;&#28982;&#19982;&#32467;&#26500;&#26080;&#20851;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20197;&#32467;&#26500;&#25110;&#20301;&#32622;&#32534;&#30721;&#65288;PEs&#65289;&#30340;&#24418;&#24335;&#65292;&#26159;&#23454;&#29616;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#26159;&#26840;&#25163;&#30340;&#65292;&#20154;&#20204;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#23581;&#35797;&#26469;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#65292;&#21253;&#25324;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#12289;&#30456;&#23545;&#38543;&#26426;&#34892;&#36208;&#27010;&#29575;&#65288;RRWP&#65289;&#12289;&#31354;&#38388;&#32534;&#30721;&#12289;&#20013;&#24515;&#24230;&#32534;&#30721;&#12289;&#36793;&#32536;&#32534;&#30721;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32534;&#30721;&#21487;&#33021;&#26681;&#26412;&#19981;&#38656;&#35201;&#65292;&#21482;&#35201;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Eigenformer&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#35889;&#24863;&#30693;&#27880;&#24847;&#26426;&#21046;&#65292;&#20102;&#35299;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
&lt;/p&gt;</description></item><item><title>SDRDPy&#26159;&#19968;&#27454;&#21487;&#22270;&#24418;&#21270;&#26174;&#31034;&#36890;&#36807;&#30417;&#30563;&#24615;&#25551;&#36848;&#24615;&#35268;&#21017;&#31639;&#27861;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#30452;&#35266;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#24110;&#21161;&#19987;&#23478;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#30456;&#20851;&#20449;&#24687;&#21644;&#35268;&#21017;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23548;&#20986;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2401.17783</link><description>&lt;p&gt;
SDRDPy&#65306;&#19968;&#27454;&#21487;&#22270;&#24418;&#21270;&#26174;&#31034;&#36890;&#36807;&#30417;&#30563;&#24615;&#25551;&#36848;&#24615;&#35268;&#21017;&#31639;&#27861;&#33719;&#24471;&#30693;&#35782;&#30340;&#24212;&#29992;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
SDRDPy: An application to graphically visualize the knowledge obtained with supervised descriptive rule algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17783
&lt;/p&gt;
&lt;p&gt;
SDRDPy&#26159;&#19968;&#27454;&#21487;&#22270;&#24418;&#21270;&#26174;&#31034;&#36890;&#36807;&#30417;&#30563;&#24615;&#25551;&#36848;&#24615;&#35268;&#21017;&#31639;&#27861;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#30452;&#35266;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#24110;&#21161;&#19987;&#23478;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#30456;&#20851;&#20449;&#24687;&#21644;&#35268;&#21017;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23548;&#20986;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SDRDPy&#26159;&#19968;&#27454;&#26700;&#38754;&#24212;&#29992;&#31243;&#24207;&#65292;&#20801;&#35768;&#19987;&#23478;&#30452;&#35266;&#22320;&#22270;&#24418;&#21270;&#21644;&#34920;&#26684;&#21270;&#34920;&#31034;&#36890;&#36807;&#20219;&#20309;&#30417;&#30563;&#24615;&#25551;&#36848;&#24615;&#35268;&#21017;&#21457;&#29616;&#31639;&#27861;&#25552;&#21462;&#30340;&#30693;&#35782;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#33021;&#22815;&#25552;&#20379;&#25968;&#25454;&#20998;&#26512;&#65292;&#26174;&#31034;&#25968;&#25454;&#38598;&#30340;&#30456;&#20851;&#20449;&#24687;&#20197;&#21450;&#35268;&#21017;&#12289;&#25968;&#25454;&#21644;&#27599;&#20010;&#35268;&#21017;&#30340;&#36136;&#37327;&#24230;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26080;&#35770;&#31639;&#27861;&#22312;&#20309;&#22788;&#25191;&#34892;&#12290;&#25152;&#26377;&#20449;&#24687;&#37117;&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#21576;&#29616;&#65292;&#20197;&#20415;&#20110;&#19987;&#23478;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#21487;&#23548;&#20986;&#19981;&#21516;&#26684;&#24335;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
SDRDPy is a desktop application that allows experts an intuitive graphic and tabular representation of the knowledge extracted by any supervised descriptive rule discovery algorithm. The application is able to provide an analysis of the data showing the relevant information of the data set and the relationship between the rules, data and the quality measures associated for each rule regardless of the tool where algorithm has been executed. All of the information is presented in a user-friendly application in order to facilitate expert analysis and also the exportation of reports in different formats.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;InfoGAN&#26041;&#27861;&#29992;&#20110;&#23545;&#27604;&#20998;&#26512;&#65292;&#36890;&#36807;&#32467;&#21512;GAN&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#21644;InfoGAN&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20110;VAE&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20849;&#21516;&#22240;&#32032;&#21644;&#29305;&#27530;&#22240;&#32032;&#26102;&#30340;&#19981;&#36275;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17776</link><description>&lt;p&gt;
&#21452;&#37325;InfoGAN&#29992;&#20110;&#23545;&#27604;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Double InfoGAN for Contrastive Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17776
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;InfoGAN&#26041;&#27861;&#29992;&#20110;&#23545;&#27604;&#20998;&#26512;&#65292;&#36890;&#36807;&#32467;&#21512;GAN&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#21644;InfoGAN&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20110;VAE&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20849;&#21516;&#22240;&#32032;&#21644;&#29305;&#27530;&#22240;&#32032;&#26102;&#30340;&#19981;&#36275;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#20998;&#26512;&#65288;CA&#65289;&#22788;&#29702;&#30340;&#26159;&#21457;&#29616;&#30446;&#26631;&#39046;&#22495;&#19982;&#32972;&#26223;&#39046;&#22495;&#30456;&#27604;&#30340;&#20849;&#21516;&#28857;&#21644;&#29305;&#27530;&#28857;&#12290;&#36825;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#20363;&#22914;&#21307;&#23398;&#25104;&#20687;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#26159;&#22522;&#20110;VAE&#30340;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65288;CA-VAEs&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35201;&#20040;&#24573;&#30053;&#37325;&#35201;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#35201;&#20040;&#27809;&#26377;&#24378;&#21046;&#25191;&#34892;&#22522;&#26412;&#20551;&#35774;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#65292;&#20854;&#20013;&#23558;&#29305;&#27530;&#22240;&#32032;&#35823;&#35748;&#20026;&#20849;&#21516;&#22240;&#32032;&#65288;&#25110;&#21453;&#20043;&#20134;&#28982;&#65289;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;VAE&#30340;&#36739;&#24046;&#36136;&#37327;&#65292;&#38477;&#20302;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;InfoGAN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;GAN&#30340;CA&#26041;&#27861;&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#20102;GAN&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#21644;InfoGAN&#30340;&#20998;&#31163;&#33021;&#21147;&#12290;&#22312;&#20174;&#31616;&#21333;&#30340;&#21512;&#25104;&#31034;&#20363;&#21040;&#22797;&#26434;&#30340;&#21307;&#23398;&#22270;&#20687;&#30340;&#22235;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#28508;&#22312;&#20998;&#31163;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#26368;&#26032;&#30340;CA-VAEs&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Analysis (CA) deals with the discovery of what is common and what is distinctive of a target domain compared to a background one. This is of great interest in many applications, such as medical imaging. Current state-of-the-art (SOTA) methods are latent variable models based on VAE (CA-VAEs). However, they all either ignore important constraints or they don't enforce fundamental assumptions. This may lead to sub-optimal solutions where distinctive factors are mistaken for common ones (or viceversa). Furthermore, the generated images have a rather poor quality, typical of VAEs, decreasing their interpretability and usefulness. Here, we propose Double InfoGAN, the first GAN based method for CA that leverages the high-quality synthesis of GAN and the separation power of InfoGAN. Experimental results on four visual datasets, from simple synthetic examples to complex medical images, show that the proposed method outperforms SOTA CA-VAEs in terms of latent separation and image qu
&lt;/p&gt;</description></item><item><title>PF-GNN&#26159;&#19968;&#31181;&#21487;&#24494;&#30340;&#22522;&#20110;&#31890;&#23376;&#28388;&#27874;&#30340;&#36890;&#29992;&#22270;&#34920;&#31034;&#36924;&#36817;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31934;&#30830;&#21516;&#26500;&#27714;&#35299;&#25216;&#26415;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17752</link><description>&lt;p&gt;
PF-GNN: &#21487;&#24494;&#30340;&#22522;&#20110;&#31890;&#23376;&#28388;&#27874;&#30340;&#36890;&#29992;&#22270;&#34920;&#31034;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
PF-GNN: Differentiable particle filtering based approximation of universal graph representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17752
&lt;/p&gt;
&lt;p&gt;
PF-GNN&#26159;&#19968;&#31181;&#21487;&#24494;&#30340;&#22522;&#20110;&#31890;&#23376;&#28388;&#27874;&#30340;&#36890;&#29992;&#22270;&#34920;&#31034;&#36924;&#36817;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31934;&#30830;&#21516;&#26500;&#27714;&#35299;&#25216;&#26415;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#34920;&#31034;&#22270;&#21516;&#26500;&#24615;&#30340;1-WL&#39068;&#33394;&#31934;&#28860;&#27979;&#35797;&#26041;&#38754;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20854;&#20182;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35201;&#20040;&#38656;&#35201;&#39044;&#22788;&#29702;&#26469;&#25552;&#21462;&#32467;&#26500;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#31934;&#30830;&#21516;&#26500;&#27714;&#35299;&#25216;&#26415;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#20351;GNN&#25104;&#20026;&#36890;&#29992;&#27169;&#22411;&#12290;&#35813;&#25216;&#26415;&#22312;&#20010;&#20307;&#21270;&#21644;&#31934;&#28860;&#65288;IR&#65289;&#33539;&#24335;&#19979;&#25805;&#20316;&#65292;&#36890;&#36807;&#20154;&#20026;&#24341;&#20837;&#19981;&#23545;&#31216;&#24615;&#24182;&#36827;&#19968;&#27493;&#31934;&#28860;&#30528;&#33394;&#65292;&#24403;1-WL&#20572;&#27490;&#26102;&#12290;&#21516;&#26500;&#27714;&#35299;&#22120;&#29983;&#25104;&#19968;&#20010;&#39068;&#33394;&#30528;&#33394;&#30340;&#25628;&#32034;&#26641;&#65292;&#20854;&#21494;&#23376;&#33410;&#28857;&#33021;&#21807;&#19968;&#26631;&#35782;&#22270;&#12290;&#28982;&#32780;&#65292;&#25628;&#32034;&#26641;&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#19988;&#38656;&#35201;&#25163;&#24037;&#35774;&#35745;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#36825;&#22312;&#23398;&#20064;&#35282;&#24230;&#19978;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#20174;&#26681;&#33410;&#28857;&#21040;&#21494;&#23376;&#33410;&#28857;&#30340;&#25628;&#32034;&#26641;&#20013;&#37319;&#26679;&#22810;&#26465;&#36335;&#24452;&#65292;&#26469;&#36817;&#20284;&#39068;&#33394;&#30528;&#33394;&#30340;&#25628;&#32034;&#26641;&#65288;&#21363;&#23884;&#20837;&#65289;&#12290;&#20197;&#23398;&#20064;&#26356;&#26377;&#36776;&#21035;&#24615;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing Graph Neural Networks (GNNs) are known to be limited in expressive power by the 1-WL color-refinement test for graph isomorphism. Other more expressive models either are computationally expensive or need preprocessing to extract structural features from the graph. In this work, we propose to make GNNs universal by guiding the learning process with exact isomorphism solver techniques which operate on the paradigm of Individualization and Refinement (IR), a method to artificially introduce asymmetry and further refine the coloring when 1-WL stops. Isomorphism solvers generate a search tree of colorings whose leaves uniquely identify the graph. However, the tree grows exponentially large and needs hand-crafted pruning techniques which are not desirable from a learning perspective. We take a probabilistic view and approximate the search tree of colorings (i.e. embeddings) by sampling multiple paths from root to leaves of the search tree. To learn more discriminative represe
&lt;/p&gt;</description></item><item><title>SwarmBrain&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#22312;&#26143;&#38469;&#20105;&#38712;II&#28216;&#25103;&#29615;&#22659;&#20013;&#23454;&#29616;&#23454;&#26102;&#31574;&#30053;&#30340;&#20855;&#36523;&#21270;&#26234;&#33021;&#20307;&#65292;&#30001;Overmind Intelligence Matrix&#21644;Swarm ReflexNet&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#12290;Overmind Intelligence Matrix&#36127;&#36131;&#23439;&#35266;&#31574;&#30053;&#30340;&#20915;&#31574;&#21644;&#21327;&#35843;&#65292;&#32780;Swarm ReflexNet&#22788;&#29702;&#24494;&#35266;&#32423;&#21035;&#30340;&#25112;&#26415;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2401.17749</link><description>&lt;p&gt;
SwarmBrain: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26102;&#25112;&#30053;&#28216;&#25103;&#26143;&#38469;&#20105;&#38712;II&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17749
&lt;/p&gt;
&lt;p&gt;
SwarmBrain&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#22312;&#26143;&#38469;&#20105;&#38712;II&#28216;&#25103;&#29615;&#22659;&#20013;&#23454;&#29616;&#23454;&#26102;&#31574;&#30053;&#30340;&#20855;&#36523;&#21270;&#26234;&#33021;&#20307;&#65292;&#30001;Overmind Intelligence Matrix&#21644;Swarm ReflexNet&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#12290;Overmind Intelligence Matrix&#36127;&#36131;&#23439;&#35266;&#31574;&#30053;&#30340;&#20915;&#31574;&#21644;&#21327;&#35843;&#65292;&#32780;Swarm ReflexNet&#22788;&#29702;&#24494;&#35266;&#32423;&#21035;&#30340;&#25112;&#26415;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25506;&#32034;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#21382;&#21490;&#19978;&#20027;&#23548;&#20195;&#29702;&#31995;&#32479;&#39046;&#22495;&#30340;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#26143;&#38469;&#20105;&#38712;II&#28216;&#25103;&#29615;&#22659;&#20013;&#25191;&#34892;&#23454;&#26102;&#25112;&#30053;&#25112;&#20105;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SwarmBrain&#65292;&#19968;&#20010;&#21033;&#29992;LLM&#22312;&#26143;&#38469;&#20105;&#38712;II&#28216;&#25103;&#29615;&#22659;&#20013;&#23454;&#29616;&#23454;&#26102;&#31574;&#30053;&#30340;&#20855;&#36523;&#21270;&#26234;&#33021;&#20307;&#12290;SwarmBrain&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;1&#65289;Overmind Intelligence Matrix&#65292;&#30001;&#26368;&#20808;&#36827;&#30340;LLMs&#39537;&#21160;&#65292;&#26088;&#22312;&#20174;&#39640;&#23618;&#27425;&#35270;&#35282;&#32452;&#32455;&#23439;&#35266;&#31574;&#30053;&#12290;&#35813;&#30697;&#38453;&#27169;&#25311;&#34411;&#26063;&#26234;&#33021;&#22823;&#33041;&#30340;&#24635;&#20307;&#24847;&#35782;&#65292;&#32508;&#21512;&#25112;&#30053;&#39044;&#35265;&#21147;&#65292;&#26088;&#22312;&#20998;&#37197;&#36164;&#28304;&#65292;&#25351;&#23548;&#25193;&#24352;&#21644;&#21327;&#35843;&#22810;&#32447;&#25915;&#20987;&#12290;2&#65289;Swarm ReflexNet&#65292;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#21327;&#21516;&#20307;&#65292;&#29992;&#20110;&#22788;&#29702;&#24494;&#35266;&#32423;&#21035;&#30340;&#25112;&#26415;&#20915;&#31574;&#65292;&#23427;&#36890;&#36807;&#24863;&#30693;&#29615;&#22659;&#21464;&#21270;&#21644;&#25932;&#20154;&#21160;&#24577;&#26469;&#29983;&#25104;&#21363;&#26102;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently garnered significant accomplishments in various exploratory tasks, even surpassing the performance of traditional reinforcement learning-based methods that have historically dominated the agent-based field. The purpose of this paper is to investigate the efficacy of LLMs in executing real-time strategy war tasks within the StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an embodied agent leveraging LLM for real-time strategy implementation in the StarCraft II game environment. The SwarmBrain comprises two key components: 1) a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed to orchestrate macro-level strategies from a high-level perspective. This matrix emulates the overarching consciousness of the Zerg intelligence brain, synthesizing strategic foresight with the aim of allocating resources, directing expansion, and coordinating multi-pronged assaults. 2) a Swarm ReflexNet, which is agile co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Haris&#65292;&#19968;&#20010;&#20808;&#36827;&#30340;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#36710;&#29260;&#35782;&#21035;&#36861;&#36394;&#25317;&#25380;&#20572;&#36710;&#22330;&#20013;&#36710;&#36742;&#30340;&#20301;&#32622;&#65292;&#28040;&#38500;&#20102;&#23545;GPS&#30340;&#20381;&#36182;&#65292;&#24182;&#25552;&#20379;&#20102;&#20415;&#25463;&#30340;&#29992;&#25143;&#30028;&#38754;&#26469;&#24110;&#21161;&#29992;&#25143;&#30830;&#23450;&#36710;&#36742;&#20301;&#32622;&#21644;&#32531;&#35299;&#20572;&#36710;&#22330;&#25317;&#22581;&#12290;</title><link>https://arxiv.org/abs/2401.17741</link><description>&lt;p&gt;
Haris:&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#20572;&#36710;&#36741;&#21161;&#30340;&#20808;&#36827;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Haris&#65292;&#19968;&#20010;&#20808;&#36827;&#30340;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#36710;&#29260;&#35782;&#21035;&#36861;&#36394;&#25317;&#25380;&#20572;&#36710;&#22330;&#20013;&#36710;&#36742;&#30340;&#20301;&#32622;&#65292;&#28040;&#38500;&#20102;&#23545;GPS&#30340;&#20381;&#36182;&#65292;&#24182;&#25552;&#20379;&#20102;&#20415;&#25463;&#30340;&#29992;&#25143;&#30028;&#38754;&#26469;&#24110;&#21161;&#29992;&#25143;&#30830;&#23450;&#36710;&#36742;&#20301;&#32622;&#21644;&#32531;&#35299;&#20572;&#36710;&#22330;&#25317;&#22581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Haris&#65292;&#19968;&#20010;&#20808;&#36827;&#30340;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#36710;&#29260;&#35782;&#21035;&#36861;&#36394;&#25317;&#25380;&#20572;&#36710;&#22330;&#20013;&#36710;&#36742;&#30340;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#21516;&#26102;&#23450;&#20301;&#19982;&#24314;&#22270;&#65288;SLAM&#65289;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#21644;&#31934;&#30830;&#24314;&#22270;&#65292;&#28040;&#38500;&#20102;&#23545;GPS&#30340;&#20381;&#36182;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#23454;&#29616;&#20102;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#36710;&#29260;&#35782;&#21035;&#65288;ALPR&#65289;&#65292;&#21487;&#20197;&#35835;&#21462;&#21644;&#20851;&#32852;&#36710;&#29260;&#21495;&#19982;&#20301;&#32622;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#20449;&#24687;&#19982;&#21518;&#31471;&#26381;&#21153;&#21516;&#27493;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#32473;&#29992;&#25143;&#65292;&#26041;&#20415;&#30830;&#23450;&#36710;&#36742;&#20301;&#32622;&#24182;&#32531;&#35299;&#20572;&#36710;&#22330;&#30340;&#25317;&#22581;&#12290;&#25552;&#20986;&#30340;&#31995;&#32479;&#26377;&#28508;&#21147;&#25913;&#21892;&#25317;&#25380;&#22330;&#25152;&#22914;&#20307;&#32946;&#22330;&#31561;&#22823;&#22411;&#23460;&#22806;&#20020;&#26102;&#20572;&#36710;&#21306;&#22495;&#30340;&#31649;&#29702;&#12290;Demo&#35270;&#39057;&#21487;&#20197;&#22312;https://youtu.be/ZkTCM35f&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Haris, an advanced autonomous mobile robot system for tracking the location of vehicles in crowded car parks using license plate recognition. The system employs simultaneous localization and mapping (SLAM) for autonomous navigation and precise mapping of the parking area, eliminating the need for GPS dependency. In addition, the system utilizes a sophisticated framework using computer vision techniques for object detection and automatic license plate recognition (ALPR) for reading and associating license plate numbers with location data. This information is subsequently synchronized with a back-end service and made accessible to users via a user-friendly mobile app, offering effortless vehicle location and alleviating congestion within the parking facility. The proposed system has the potential to improve the management of short-term large outdoor parking areas in crowded places such as sports stadiums. The demo of the robot can be found on https://youtu.be/ZkTCM35f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#31639;&#23376;&#23398;&#20064;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2401.17739</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Operator learning without the adjoint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#31639;&#23376;&#23398;&#20064;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#35868;&#22242;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#24773;&#20917;&#19979;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#38750;&#33258;&#20276;&#38543;&#31639;&#23376;&#65311;&#30446;&#21069;&#30340;&#23454;&#38469;&#26041;&#27861;&#34920;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;&#30001;&#31639;&#23376;&#30340;&#27491;&#21521;&#20316;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#31639;&#23376;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20276;&#38543;&#31639;&#23376;&#12290;&#28982;&#32780;&#65292;&#20197;&#30452;&#35266;&#30340;&#26041;&#24335;&#30475;&#65292;&#20284;&#20046;&#26377;&#24517;&#35201;&#37319;&#26679;&#20276;&#38543;&#31639;&#23376;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37096;&#20998;&#35299;&#37322;&#20102;&#36825;&#20010;&#35868;&#22242;&#65292;&#36890;&#36807;&#35777;&#26126;&#22312;&#19981;&#26597;&#35810;&#20276;&#38543;&#31639;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#32467;&#26524;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20294;&#25105;&#20204;&#30340;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a mystery at the heart of operator learning: how can one recover a non-self-adjoint operator from data without probing the adjoint? Current practical approaches suggest that one can accurately recover an operator while only using data generated by the forward action of the operator without access to the adjoint. However, naively, it seems essential to sample the action of the adjoint. In this paper, we partially explain this mystery by proving that without querying the adjoint, one can approximate a family of non-self-adjoint infinite-dimensional compact operators via projection onto a Fourier basis. We then apply the result to recovering Green's functions of elliptic partial differential operators and derive an adjoint-free sample complexity bound. While existing theory justifies low sample complexity in operator learning, ours is the first adjoint-free analysis that attempts to close the gap between theory and practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#31070;&#32463;&#36827;&#21270;&#31995;&#32479;&#20013;&#29289;&#29702;&#21512;&#29702;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#26368;&#22823;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#24615;&#21516;&#26102;&#26368;&#23567;&#21270;&#21151;&#32791;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31361;&#21464;&#31574;&#30053;&#21644;&#35757;&#32451;&#25216;&#26415;&#26469;&#20248;&#21270;&#27169;&#22411;&#34920;&#29616;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.17733</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#36827;&#21270;&#31995;&#32479;&#20013;&#36808;&#21521;&#29289;&#29702;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Physical Plausibility in Neuroevolution Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#31070;&#32463;&#36827;&#21270;&#31995;&#32479;&#20013;&#29289;&#29702;&#21512;&#29702;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#26368;&#22823;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#24615;&#21516;&#26102;&#26368;&#23567;&#21270;&#21151;&#32791;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31361;&#21464;&#31574;&#30053;&#21644;&#35757;&#32451;&#25216;&#26415;&#26469;&#20248;&#21270;&#27169;&#22411;&#34920;&#29616;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#21151;&#32791;&#65292;&#24341;&#21457;&#20102;&#23545;&#26356;&#21152;&#33410;&#33021;&#31639;&#27861;&#21644;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#24182;&#24341;&#36215;&#20102;&#29615;&#22659;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25512;&#29702;&#38454;&#27573;&#26085;&#30410;&#22686;&#38271;&#30340;&#33021;&#28304;&#28040;&#32791;&#38382;&#39064;&#12290;&#21363;&#20351;&#31245;&#24494;&#20943;&#23569;&#30005;&#21147;&#20351;&#29992;&#20063;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#28304;&#33410;&#32422;&#65292;&#20351;&#29992;&#25143;&#12289;&#20844;&#21496;&#21644;&#29615;&#22659;&#37117;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30528;&#37325;&#20110;&#22312;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#20013;&#26368;&#22823;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20854;&#21151;&#32791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#36866;&#24212;&#24230;&#20989;&#25968;&#20013;&#32771;&#34385;&#20102;&#21151;&#32791;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31361;&#21464;&#31574;&#30053;&#65292;&#20197;&#38543;&#26426;&#26041;&#24335;&#37325;&#26032;&#24341;&#20837;&#23618;&#27169;&#22359;&#65292;&#20855;&#26377;&#33410;&#33021;&#27169;&#22359;&#30340;&#36873;&#25321;&#26426;&#20250;&#26356;&#39640;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#27169;&#22411;&#30340;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing usage of Artificial Intelligence (AI) models, especially Deep Neural Networks (DNNs), is increasing the power consumption during training and inference, posing environmental concerns and driving the need for more energy-efficient algorithms and hardware solutions. This work addresses the growing energy consumption problem in Machine Learning (ML), particularly during the inference phase. Even a slight reduction in power usage can lead to significant energy savings, benefiting users, companies, and the environment. Our approach focuses on maximizing the accuracy of Artificial Neural Network (ANN) models using a neuroevolutionary framework whilst minimizing their power consumption. To do so, power consumption is considered in the fitness function. We introduce a new mutation strategy that stochastically reintroduces modules of layers, with power-efficient modules having a higher chance of being chosen. We introduce a novel technique that allows training two separate models
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33041;&#30005;&#21151;&#33021;&#36830;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#39044;&#27979;&#32463;&#36807;&#38271;&#26399;tDCS&#24178;&#39044;&#21518;&#30340;&#22810;&#20219;&#21153;&#35748;&#30693;&#34920;&#29616;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2401.17711</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#33041;&#30005;&#21151;&#33021;&#36830;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#32463;&#36807;&#38271;&#26399;tDCS&#24178;&#39044;&#30340;&#22810;&#20219;&#21153;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Prediction of multitasking performance post-longitudinal tDCS via EEG-based functional connectivity and machine learning methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33041;&#30005;&#21151;&#33021;&#36830;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#39044;&#27979;&#32463;&#36807;&#38271;&#26399;tDCS&#24178;&#39044;&#21518;&#30340;&#22810;&#20219;&#21153;&#35748;&#30693;&#34920;&#29616;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21644;&#29702;&#35299;&#35748;&#30693;&#34920;&#29616;&#30340;&#21464;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#24178;&#39044;&#20043;&#21518;&#65292;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#19968;&#20010;&#22522;&#26412;&#30446;&#26631;&#12290;&#32437;&#21521;&#22522;&#20110;&#33041;&#21050;&#28608;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#22914;&#32463;&#30382;&#30452;&#27969;&#30005;&#21050;&#28608;&#65288;tDCS&#65289;&#65292;&#20250;&#24341;&#36215;&#38745;&#24687;&#33180;&#30005;&#20301;&#30340;&#30701;&#26399;&#21464;&#21270;&#65292;&#24182;&#24433;&#21709;&#35748;&#30693;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#39044;&#27979;&#24178;&#39044;&#21518;&#35748;&#30693;&#34920;&#29616;&#21464;&#21270;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#19981;&#21516;&#30340;&#22522;&#20110;&#33041;&#30005;&#21151;&#33021;&#36830;&#25509;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#22797;&#26434;&#22810;&#20219;&#21153;&#20219;&#21153;&#20013;&#35748;&#30693;&#34920;&#29616;&#30340;&#21464;&#21270;&#65292;&#26469;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#12290;&#23558;40&#21517;&#21463;&#35797;&#32773;&#20998;&#20026;&#23454;&#39564;&#32452;&#21644;&#20027;&#21160;&#23545;&#29031;&#32452;&#12290;&#22312;&#31532;&#19968;&#22825;&#65292;&#25152;&#26377;&#21463;&#35797;&#32773;&#25191;&#34892;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#20219;&#21153;&#65292;&#24182;&#21516;&#26102;&#33719;&#21462;&#21040;32&#36890;&#36947;&#30340;&#33041;&#30005;&#20449;&#21495;&#12290;&#20174;&#31532;&#20108;&#22825;&#21040;&#31532;&#19971;&#22825;&#65292;&#23454;&#39564;&#32452;&#21463;&#35797;&#32773;&#36827;&#34892;&#20102;15&#20998;&#38047;&#30340;2mA&#38452;&#26497;tDCS&#21050;&#28608;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting and understanding the changes in cognitive performance, especially after a longitudinal intervention, is a fundamental goal in neuroscience. Longitudinal brain stimulation-based interventions like transcranial direct current stimulation (tDCS) induce short-term changes in the resting membrane potential and influence cognitive processes. However, very little research has been conducted on predicting these changes in cognitive performance post-intervention. In this research, we intend to address this gap in the literature by employing different EEG-based functional connectivity analyses and machine learning algorithms to predict changes in cognitive performance in a complex multitasking task. Forty subjects were divided into experimental and active-control conditions. On Day 1, all subjects executed a multitasking task with simultaneous 32-channel EEG being acquired. From Day 2 to Day 7, subjects in the experimental condition undertook 15 minutes of 2mA anodal tDCS stimulation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#21644;&#22270;&#20687;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#39044;&#27979;&#23460;&#20869;&#35774;&#35745;&#20013;&#30340;&#23457;&#32654;&#20559;&#22909;&#12290;&#30740;&#31350;&#32467;&#21512;&#20102;&#39068;&#33394;&#21644;&#35856;&#24230;&#12289;&#20142;&#24230;&#21644;&#22797;&#26434;&#24230;&#31561;&#35270;&#35273;&#23646;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#24179;&#22343;&#27861;&#35745;&#31639;&#24635;&#20307;&#23457;&#32654;&#24471;&#20998;&#65292;&#24182;&#32771;&#34385;&#20010;&#20154;&#39068;&#33394;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2401.17710</link><description>&lt;p&gt;
&#23460;&#20869;&#35774;&#35745;&#20013;&#30340;&#23457;&#32654;&#20559;&#22909;&#39044;&#27979;&#65306;&#27169;&#31946;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Aesthetic Preference Prediction in Interior Design: Fuzzy Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#21644;&#22270;&#20687;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#39044;&#27979;&#23460;&#20869;&#35774;&#35745;&#20013;&#30340;&#23457;&#32654;&#20559;&#22909;&#12290;&#30740;&#31350;&#32467;&#21512;&#20102;&#39068;&#33394;&#21644;&#35856;&#24230;&#12289;&#20142;&#24230;&#21644;&#22797;&#26434;&#24230;&#31561;&#35270;&#35273;&#23646;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#24179;&#22343;&#27861;&#35745;&#31639;&#24635;&#20307;&#23457;&#32654;&#24471;&#20998;&#65292;&#24182;&#32771;&#34385;&#20010;&#20154;&#39068;&#33394;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#35774;&#35745;&#26159;&#20851;&#20110;&#21019;&#36896;&#30475;&#36215;&#26469;&#21644;&#24863;&#35273;&#33391;&#22909;&#30340;&#31354;&#38388;&#30340;&#12290;&#28982;&#32780;&#65292;&#23457;&#32654;&#20559;&#22909;&#30340;&#20027;&#35266;&#24615;&#36136;&#22312;&#23450;&#20041;&#21644;&#37327;&#21270;&#20309;&#20026;&#23460;&#20869;&#35774;&#35745;&#35270;&#35273;&#21560;&#24341;&#21147;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26469;&#37327;&#21270;&#21644;&#39044;&#27979;&#23460;&#20869;&#35774;&#35745;&#30340;&#23457;&#32654;&#20559;&#22909;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#27169;&#31946;&#36923;&#36753;&#19982;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25910;&#38598;&#20102;&#19968;&#32452;&#23460;&#20869;&#35774;&#35745;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20851;&#27880;&#39068;&#33394;&#21644;&#35856;&#24230;&#12289;&#20142;&#24230;&#21644;&#22797;&#26434;&#24230;&#31561;&#22522;&#26412;&#35270;&#35273;&#23646;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#27861;&#25972;&#21512;&#36825;&#20123;&#29305;&#24449;&#20197;&#35745;&#31639;&#19968;&#20010;&#24635;&#20307;&#23457;&#32654;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25972;&#20307;&#23457;&#32654;&#20559;&#22909;&#26102;&#32771;&#34385;&#20010;&#20154;&#39068;&#33394;&#20559;&#22909;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#29992;&#25143;&#23545;&#32418;&#33394;&#12289;&#26837;&#33394;&#31561;&#20027;&#35201;&#39068;&#33394;&#30340;&#35780;&#20998;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#20687;&#20013;&#21069;&#20116;&#20010;&#20027;&#23548;&#39068;&#33394;&#30340;&#20687;&#32032;&#35745;&#25968;&#26469;&#24471;&#21040;
&lt;/p&gt;
&lt;p&gt;
Interior design is all about creating spaces that look and feel good. However, the subjective nature of aesthetic preferences presents a significant challenge in defining and quantifying what makes an interior design visually appealing. The current paper addresses this gap by introducing a novel methodology for quantifying and predicting aesthetic preferences in interior design. Our study combines fuzzy logic with image processing techniques. We collected a dataset of interior design images from social media platforms, focusing on essential visual attributes such as color harmony, lightness, and complexity. We integrate these features using weighted average to compute a general aesthetic score. Our approach considers individual color preferences in calculating the overall aesthetic preference. We initially gather user ratings for primary colors like red, brown, and others to understand their preferences. Then, we use the pixel count of the top five dominant colors in the image to get t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21151;&#33021;&#36830;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#38271;&#26399;&#32463;&#39045;&#30452;&#27969;&#30005;&#21050;&#28608;&#65288;tDCS&#65289;&#21518;&#30340;&#25191;&#34892;&#21151;&#33021;&#34920;&#29616;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20026;&#29702;&#35299;&#21644;&#30740;&#31350;&#35813;&#24178;&#39044;&#26041;&#24335;&#23545;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2401.17700</link><description>&lt;p&gt;
&#38271;&#26399;&#32463;&#39045;&#30452;&#27969;&#30005;&#21050;&#28608;&#21518;&#20351;&#29992;&#21151;&#33021;&#36830;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#25191;&#34892;&#21151;&#33021;&#34920;&#29616;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of executive functioning performance post-longitudinal tDCS using functional connectivity and machine learning methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21151;&#33021;&#36830;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#38271;&#26399;&#32463;&#39045;&#30452;&#27969;&#30005;&#21050;&#28608;&#65288;tDCS&#65289;&#21518;&#30340;&#25191;&#34892;&#21151;&#33021;&#34920;&#29616;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20026;&#29702;&#35299;&#21644;&#30740;&#31350;&#35813;&#24178;&#39044;&#26041;&#24335;&#23545;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25191;&#34892;&#21151;&#33021;&#26159;&#19968;&#31181;&#35748;&#30693;&#36807;&#31243;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#20197;&#30446;&#26631;&#23548;&#21521;&#30340;&#26041;&#24335;&#35745;&#21010;&#12289;&#32452;&#32455;&#21644;&#35843;&#33410;&#34892;&#20026;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;&#32463;&#36807;&#38271;&#26399;&#24178;&#39044;&#65288;&#22914;&#32463;&#39045;&#30452;&#27969;&#30005;&#21050;&#28608;&#65288;tDCS&#65289;&#65289;&#21518;&#25191;&#34892;&#21151;&#33021;&#21464;&#21270;&#36827;&#34892;&#30740;&#31350;&#21644;&#20998;&#31867;&#30340;&#25991;&#29486;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21151;&#33021;&#36830;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;tDCS&#21518;&#30340;&#25191;&#34892;&#21151;&#33021;&#34920;&#29616;&#36827;&#34892;&#20998;&#31867;&#12290;&#23558;&#20116;&#21313;&#21517;&#21463;&#35797;&#32773;&#20998;&#20026;&#23454;&#39564;&#32452;&#21644;&#23433;&#24944;&#21058;&#23545;&#29031;&#32452;&#12290;&#22312;&#31532;&#19968;&#22825;&#65292;&#21463;&#35797;&#32773;&#22312;&#25191;&#34892;&#21151;&#33021;&#20219;&#21153;&#26102;&#25910;&#38598;&#20102;&#33041;&#30005;&#25968;&#25454;&#12290;&#23454;&#39564;&#32452;&#22312;&#31532;2&#22825;&#21040;&#31532;8&#22825;&#36827;&#34892;&#20219;&#21153;&#35757;&#32451;&#26102;&#25509;&#21463;tDCS&#65292;&#32780;&#23545;&#29031;&#32452;&#25509;&#21463;&#20102;&#23433;&#24944;&#21058;tDCS&#12290;&#31532;10&#22825;&#65292;&#21463;&#35797;&#32773;&#37325;&#22797;&#20102;&#31532;&#19968;&#22825;&#25351;&#23450;&#30340;&#20219;&#21153;&#12290;&#20174;&#33041;&#30005;&#25968;&#25454;&#20013;&#25552;&#21462;&#20102;&#19981;&#21516;&#30340;&#21151;&#33021;&#36830;&#25509;&#25351;&#26631;&#65292;&#26368;&#32456;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#25191;&#34892;&#21151;&#33021;&#34920;&#29616;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Executive functioning is a cognitive process that enables humans to plan, organize, and regulate their behavior in a goal-directed manner. Understanding and classifying the changes in executive functioning after longitudinal interventions (like transcranial direct current stimulation (tDCS)) has not been explored in the literature. This study employs functional connectivity and machine learning algorithms to classify executive functioning performance post-tDCS. Fifty subjects were divided into experimental and placebo control groups. EEG data was collected while subjects performed an executive functioning task on Day 1. The experimental group received tDCS during task training from Day 2 to Day 8, while the control group received sham tDCS. On Day 10, subjects repeated the tasks specified on Day 1. Different functional connectivity metrics were extracted from EEG data and eventually used for classifying executive functioning performance using different machine learning algorithms. Resu
&lt;/p&gt;</description></item><item><title>EnCLAP&#26159;&#19968;&#31181;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#21644;&#38899;&#39057;&#25991;&#26412;&#32852;&#21512;&#23884;&#20837;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;masked codec modeling&#35757;&#32451;&#30446;&#26631;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22768;&#23398;&#24847;&#35782;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;AudioCaps&#21644;Clotho&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17690</link><description>&lt;p&gt;
EnCLAP&#65306;&#23558;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#21644;&#38899;&#39057;&#25991;&#26412;&#32852;&#21512;&#23884;&#20837;&#32467;&#21512;&#30340;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17690
&lt;/p&gt;
&lt;p&gt;
EnCLAP&#26159;&#19968;&#31181;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#21644;&#38899;&#39057;&#25991;&#26412;&#32852;&#21512;&#23884;&#20837;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;masked codec modeling&#35757;&#32451;&#30446;&#26631;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22768;&#23398;&#24847;&#35782;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;AudioCaps&#21644;Clotho&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EnCLAP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#12290;EnCLAP&#37319;&#29992;&#20102;&#20004;&#31181;&#22768;&#23398;&#34920;&#31034;&#27169;&#22411;&#65306;EnCodec&#21644;CLAP&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BART&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;masked codec modeling&#30340;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22768;&#23398;&#24847;&#35782;&#12290;&#22312;AudioCaps&#21644;Clotho&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/jaeyeonkim99/EnCLAP&#33719;&#21462;&#12290;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;https://huggingface.co/spaces/enclap-team/enclap &#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EnCLAP, a novel framework for automated audio captioning. EnCLAP employs two acoustic representation models, EnCodec and CLAP, along with a pretrained language model, BART. We also introduce a new training objective called masked codec modeling that improves acoustic awareness of the pretrained language model. Experimental results on AudioCaps and Clotho demonstrate that our model surpasses the performance of baseline models. Source code will be available at https://github.com/jaeyeonkim99/EnCLAP . An online demo is available at https://huggingface.co/spaces/enclap-team/enclap .
&lt;/p&gt;</description></item><item><title>&#22312;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#27169;&#22411;&#19981;&#20165;&#22312;&#39044;&#27979;&#31070;&#32463;&#21709;&#24212;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31867;&#33041;&#24615;&#33021;&#65292;&#32780;&#19988;&#20854;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#36335;&#24452;&#19982;&#22823;&#33041;&#30340;&#26144;&#23556;&#26356;&#25509;&#36817;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#23618;&#27425;&#26469;&#36827;&#34892;&#30456;&#21516;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2401.17671</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#33041;&#20013;&#65292;&#19978;&#19979;&#25991;&#29305;&#24449;&#25552;&#21462;&#23618;&#27425;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Contextual Feature Extraction Hierarchies Converge in Large Language Models and the Brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17671
&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#27169;&#22411;&#19981;&#20165;&#22312;&#39044;&#27979;&#31070;&#32463;&#21709;&#24212;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31867;&#33041;&#24615;&#33021;&#65292;&#32780;&#19988;&#20854;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#36335;&#24452;&#19982;&#22823;&#33041;&#30340;&#26144;&#23556;&#26356;&#25509;&#36817;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#23618;&#27425;&#26469;&#36827;&#34892;&#30456;&#21516;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#31070;&#32463;&#22788;&#29702;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;LLM&#34920;&#31034;&#21644;&#22823;&#33041;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#20294;&#22312;&#28436;&#21270;&#30340;LLM&#32972;&#26223;&#19979;&#24341;&#21457;&#36825;&#31181;&#25910;&#25947;&#30340;&#28508;&#22312;&#35745;&#31639;&#21407;&#29702;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#24615;&#33021;&#36739;&#39640;&#30340;LLM&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#30456;&#20284;&#65292;&#20197;&#25506;&#31350;&#23548;&#33268;&#20854;&#19982;&#22823;&#33041;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#19968;&#33268;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;LLM&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#36798;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#26102;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#39044;&#27979;LLM&#23884;&#20837;&#30340;&#31070;&#32463;&#21709;&#24212;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31867;&#33041;&#24615;&#33021;&#65292;&#32780;&#19988;&#23427;&#20204;&#30340;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#36335;&#24452;&#19982;&#22823;&#33041;&#30340;&#26144;&#23556;&#26356;&#25509;&#36817;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#23618;&#27425;&#26469;&#36827;&#34892;&#30456;&#21516;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in artificial intelligence have sparked interest in the parallels between large language models (LLMs) and human neural processing, particularly in language comprehension. While prior research has established similarities in the representation of LLMs and the brain, the underlying computational principles that cause this convergence, especially in the context of evolving LLMs, remain elusive. Here, we examined a diverse selection of high-performance LLMs with similar parameter sizes to investigate the factors contributing to their alignment with the brain's language processing mechanisms. We find that as LLMs achieve higher performance on benchmark tasks, they not only become more brain-like as measured by higher performance when predicting neural responses from LLM embeddings, but also their hierarchical feature extraction pathways map more closely onto the brain's while using fewer layers to do the same encoding. We also compare the feature extraction pathways of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23458;&#25143;&#29983;&#21629;&#21608;&#26399;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#35770;&#65292;&#21487;&#20197;&#24110;&#21161;&#20013;&#23567;&#22411;&#21046;&#36896;&#20225;&#19994;&#30340;&#36719;&#20214;&#24037;&#31243;&#24072;&#22312;&#23454;&#26045;&#24037;&#19994;4.0&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2401.17661</link><description>&lt;p&gt;
&#26397;&#30528;&#24037;&#19994;4.0&#30340;&#23454;&#29616;: &#20197;&#23458;&#25143;&#29983;&#21629;&#21608;&#26399;&#20026;&#23548;&#21521;&#30340;&#22522;&#20110;&#26041;&#27861;&#35770;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards the implementation of Industry 4.0: A methodology-based approach oriented to the customer life cycle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23458;&#25143;&#29983;&#21629;&#21608;&#26399;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#35770;&#65292;&#21487;&#20197;&#24110;&#21161;&#20013;&#23567;&#22411;&#21046;&#36896;&#20225;&#19994;&#30340;&#36719;&#20214;&#24037;&#31243;&#24072;&#22312;&#23454;&#26045;&#24037;&#19994;4.0&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#19981;&#21516;&#30340;&#20840;&#29699;&#20513;&#35758;&#27491;&#22312;&#25512;&#21160;&#20174;&#26426;&#22120;&#20027;&#23548;&#22411;&#21046;&#36896;&#36716;&#21521;&#25968;&#23383;&#21270;&#21046;&#36896;&#12290;&#20026;&#20102;&#23454;&#29616;&#25104;&#21151;&#36716;&#22411;&#21040;&#24037;&#19994;4.0&#26631;&#20934;&#65292;&#21046;&#36896;&#20225;&#19994;&#38656;&#35201;&#23454;&#26045;&#28165;&#26224;&#30340;&#36335;&#32447;&#22270;&#12290;&#28982;&#32780;&#65292;&#20013;&#23567;&#22411;&#21046;&#36896;&#20225;&#19994;&#22312;&#24037;&#19994;4.0&#30340;&#23454;&#26045;&#36807;&#31243;&#20013;&#36935;&#21040;&#35768;&#22810;&#38556;&#30861;&#21644;&#22256;&#38590;&#65288;&#32463;&#27982;&#12289;&#25216;&#26415;&#12289;&#25991;&#21270;&#31561;&#65289;&#12290;&#34429;&#28982;&#24050;&#26377;&#19968;&#20123;&#30740;&#31350;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#24037;&#19994;4.0&#25216;&#26415;&#24212;&#29992;&#20110;&#20135;&#21697;&#29983;&#21629;&#21608;&#26399;&#21644;&#20379;&#24212;&#38142;&#29983;&#21629;&#21608;&#26399;&#65292;&#20026;&#20013;&#23567;&#22411;&#21046;&#36896;&#20225;&#19994;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#20294;&#23545;&#20110;&#23458;&#25143;&#29983;&#21629;&#21608;&#26399;&#26469;&#35828;&#24773;&#20917;&#24182;&#38750;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#21487;&#20197;&#24110;&#21161;&#20013;&#23567;&#22411;&#21046;&#36896;&#20225;&#19994;&#30340;&#36719;&#20214;&#24037;&#31243;&#24072;&#22312;&#23458;&#25143;&#29983;&#21629;&#21608;&#26399;&#20013;&#24341;&#20837;&#24037;&#19994;4.0&#25216;&#26415;&#30340;&#36129;&#29486;&#12290;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#26041;&#27861;&#35770;&#65292;&#21487;&#20197;&#24110;&#21161;&#36825;&#20123;&#36719;&#20214;&#24037;&#31243;&#24072;&#21019;&#36896;&#19982;&#24037;&#19994;4.0&#30456;&#19968;&#33268;&#30340;&#26032;&#36719;&#20214;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many different worldwide initiatives are promoting the transformation from machine dominant manufacturing to digital manufacturing. Thus, to achieve a successful transformation to Industry 4.0 standard, manufacturing enterprises are required to implement a clear roadmap. However, Small and Medium Manufacturing Enterprises (SMEs) encounter many barriers and difficulties (economical, technical, cultural, etc.) in the implementation of Industry 4.0. Although several works deal with the incorporation of Industry 4.0 technologies in the area of the product and supply chain life cycles, which SMEs could use as reference, this is not the case for the customer life cycle. Thus, we present two contributions that can help the software engineers of those SMEs to incorporate Industry 4.0 technologies in the context of the customer life cycle. The first contribution is a methodology that can help those software engineers in the task of creating new software services, aligned with Industry 4.0, that
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#36827;&#34892;&#26725;&#26753;&#21019;&#26032;&#65292;&#36890;&#36807;&#21338;&#24328;&#35770;&#35299;&#37322;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#25216;&#26415;&#29983;&#25104;&#33021;&#37327;&#20540;&#36739;&#20302;&#30340;&#26032;&#26679;&#26412;&#65292;&#24314;&#31435;&#22522;&#20110;&#33021;&#37327;&#30340;&#26725;&#26753;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17657</link><description>&lt;p&gt;
&#20174;&#33021;&#37327;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#30340;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
An attempt to generate new bridge types from latent space of energy-based model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17657
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#36827;&#34892;&#26725;&#26753;&#21019;&#26032;&#65292;&#36890;&#36807;&#21338;&#24328;&#35770;&#35299;&#37322;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#25216;&#26415;&#29983;&#25104;&#33021;&#37327;&#20540;&#36739;&#20302;&#30340;&#26032;&#26679;&#26412;&#65292;&#24314;&#31435;&#22522;&#20110;&#33021;&#37327;&#30340;&#26725;&#26753;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#36827;&#34892;&#26725;&#26753;&#21019;&#26032;&#12290;&#36890;&#36807;&#21338;&#24328;&#35770;&#35299;&#37322;&#25439;&#22833;&#20989;&#25968;&#65292;&#36923;&#36753;&#28165;&#26224;&#65292;&#20844;&#24335;&#31616;&#21333;&#26126;&#20102;&#12290;&#22240;&#27492;&#36991;&#20813;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#35299;&#37322;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#28040;&#38500;&#20102;&#35299;&#20915;&#26631;&#20934;&#21270;&#20998;&#27597;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#20551;&#35774;&#26725;&#26753;&#31867;&#22411;&#30340;&#31181;&#32676;&#31526;&#21512;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#33021;&#37327;&#20989;&#25968;&#12290;&#21033;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#25216;&#26415;&#29983;&#25104;&#33021;&#37327;&#20540;&#36739;&#20302;&#30340;&#26032;&#26679;&#26412;&#65292;&#20174;&#32780;&#24314;&#31435;&#22522;&#20110;&#33021;&#37327;&#30340;&#26725;&#26753;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#19977;&#36328;&#26753;&#26725;&#12289;&#25329;&#26725;&#12289;&#26012;&#25289;&#26725;&#21644;&#24748;&#32034;&#26725;&#30340;&#23545;&#31216;&#32467;&#26500;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#33021;&#37327;&#20989;&#25968;&#35757;&#32451;&#65292;&#31934;&#30830;&#35745;&#31639;&#30495;&#23454;&#21644;&#20266;&#36896;&#26679;&#26412;&#30340;&#33021;&#37327;&#20540;&#12290;&#20174;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#33021;&#37327;&#20989;&#25968;&#23558;&#37319;&#26679;&#28857;&#36716;&#21270;&#20026;&#33021;&#37327;&#24471;&#20998;&#20302;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Use energy-based model for bridge-type innovation. The loss function is explained by the game theory, the logic is clear and the formula is simple and clear. Thus avoid the use of maximum likelihood estimation to explain the loss function and eliminate the need for Monte Carlo methods to solve the normalized denominator. Assuming that the bridge-type population follows a Boltzmann distribution, a neural network is constructed to represent the energy function. Use Langevin dynamics technology to generate a new sample with low energy value, thus a generative model of bridge-type based on energy is established. Train energy function on symmetric structured image dataset of three span beam bridge, arch bridge, cable-stayed bridge, and suspension bridge to accurately calculate the energy values of real and fake samples. Sampling from latent space, using gradient descent algorithm, the energy function transforms the sampling points into low energy score samples, thereby generating new bridge
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32852;&#37030;&#25628;&#32034;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36164;&#28304;&#36873;&#25321;&#33021;&#21147;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29305;&#24449;&#30340;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2401.17645</link><description>&lt;p&gt;
ReSLLM: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#32852;&#37030;&#25628;&#32034;&#24378;&#22823;&#30340;&#36164;&#28304;&#36873;&#25321;&#22120;
&lt;/p&gt;
&lt;p&gt;
ReSLLM: Large Language Models are Strong Resource Selectors for Federated Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17645
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32852;&#37030;&#25628;&#32034;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36164;&#28304;&#36873;&#25321;&#33021;&#21147;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29305;&#24449;&#30340;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25628;&#32034;&#26159;&#23558;&#22810;&#20010;&#29420;&#31435;&#25628;&#32034;&#24341;&#25806;&#30340;&#32467;&#26524;&#25972;&#21512;&#36215;&#26469;&#30340;&#36807;&#31243;&#65292;&#22312;&#22686;&#24378;&#26816;&#32034;&#29983;&#25104;&#27969;&#27700;&#32447;&#20013;&#20855;&#26377;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#25552;&#20379;&#25903;&#25345;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#26681;&#25454;&#29992;&#25143;&#30340;&#35805;&#35821;&#24615;&#36136;&#65292;&#23558;&#26597;&#35810;&#20998;&#21457;&#21040;&#21508;&#31181;&#25628;&#32034;&#24341;&#25806;&#20013;&#65292;&#20174;&#19987;&#38376;&#30340;&#65288;&#22914;PubMed&#65289;&#21040;&#36890;&#29992;&#30340;&#65288;&#22914;Google&#65289;&#12290;&#32852;&#37030;&#25628;&#32034;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#36164;&#28304;&#36873;&#25321;&#65292;&#21363;&#22312;&#21457;&#20986;&#26597;&#35810;&#20043;&#21069;&#36873;&#25321;&#36866;&#24403;&#30340;&#36164;&#28304;&#65292;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#21644;&#24555;&#36895;&#21709;&#24212;&#65292;&#24182;&#38477;&#20302;&#35843;&#29992;&#22806;&#37096;&#25628;&#32034;&#24341;&#25806;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;SOTA&#36164;&#28304;&#36873;&#25321;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#29305;&#24449;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#20154;&#21147;&#23494;&#38598;&#21644;&#26114;&#36149;&#30340;&#35757;&#32451;&#26631;&#31614;&#30340;&#21019;&#24314;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;LLM&#22312;NLP&#21644;IR&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;...
&lt;/p&gt;
&lt;p&gt;
Federated search, which involves integrating results from multiple independent search engines, will become increasingly pivotal in the context of Retrieval-Augmented Generation pipelines empowering LLM-based applications such as chatbots. These systems often distribute queries among various search engines, ranging from specialized (e.g., PubMed) to general (e.g., Google), based on the nature of user utterances. A critical aspect of federated search is resource selection - the selection of appropriate resources prior to issuing the query to ensure high-quality and rapid responses, and contain costs associated with calling the external search engines. However, current SOTA resource selection methodologies primarily rely on feature-based learning approaches. These methods often involve the labour intensive and expensive creation of training labels for each resource. In contrast, LLMs have exhibited strong effectiveness as zero-shot methods across NLP and IR tasks. We hypothesise that in t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36807;&#24230;&#26432;&#20260;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20013;&#23384;&#22312;&#30340;&#25463;&#24452;&#21644;&#23545;&#26377;&#23475;&#35789;&#35821;&#30340;&#36807;&#24230;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#23545;&#27604;&#35299;&#30721;&#65288;Self-CD&#65289;&#31574;&#30053;&#26469;&#32531;&#35299;&#36807;&#24230;&#26432;&#20260;&#29616;&#35937;&#65292;&#35813;&#31574;&#30053;&#26080;&#38656;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17633</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#36807;&#24230;&#26432;&#20260;&#38382;&#39064;&#30340;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Navigating the OverKill in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36807;&#24230;&#26432;&#20260;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20013;&#23384;&#22312;&#30340;&#25463;&#24452;&#21644;&#23545;&#26377;&#23475;&#35789;&#35821;&#30340;&#36807;&#24230;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#23545;&#27604;&#35299;&#30721;&#65288;Self-CD&#65289;&#31574;&#30053;&#26469;&#32531;&#35299;&#36807;&#24230;&#26432;&#20260;&#29616;&#35937;&#65292;&#35813;&#31574;&#30053;&#26080;&#38656;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#31934;&#24515;&#35843;&#25972;&#65292;&#20197;&#26082;&#26377;&#21161;&#30410;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#23384;&#22312;&#28508;&#22312;&#30340;&#36807;&#24230;&#26432;&#20260;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#21487;&#33021;&#20250;&#25298;&#32477;&#22238;&#31572;&#26080;&#23475;&#26597;&#35810;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#21644;&#30830;&#23450;&#26597;&#35810;&#30340;&#23433;&#20840;&#24615;&#65292;&#26469;&#30740;&#31350;&#36807;&#24230;&#26432;&#20260;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#23384;&#22312;&#25463;&#24452;&#65292;&#23548;&#33268;&#23545;&#8220;&#26432;&#20260;&#8221;&#31561;&#26377;&#23475;&#35789;&#35821;&#36807;&#24230;&#20851;&#27880;&#65292;&#32780;&#24378;&#35843;&#23433;&#20840;&#24615;&#30340;&#25552;&#31034;&#23558;&#21152;&#21095;&#36807;&#24230;&#26432;&#20260;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#21517;&#20026;&#33258;&#23545;&#27604;&#35299;&#30721;&#65288;Self-Contrastive Decoding&#65292;Self-CD&#65289;&#65292;&#26469;&#32531;&#35299;&#36825;&#19968;&#29616;&#35937;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25918;&#22823;&#27169;&#22411;&#22312;&#22238;&#24212;&#31995;&#32479;&#25552;&#31034;&#26102;&#36755;&#20986;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#25552;&#21462;&#36825;&#31181;&#36807;&#24230;&#20851;&#27880;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#26469;&#20943;&#24369;&#27169;&#22411;&#23545;&#36825;&#31181;&#36807;&#24230;&#20851;&#27880;&#30340;&#24433;&#21709;&#65292;&#20197;&#30830;&#23450;&#26368;&#32456;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to an over-attention of harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such over-attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the over-attention from the model via contrastive decoding. Empirical results indicate that our method has
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2401.17626</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative AI to Generate Test Data Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20551;&#25968;&#25454;&#26159;&#29616;&#20195;&#36719;&#20214;&#27979;&#35797;&#30340;&#37325;&#35201;&#32500;&#24230;&#20043;&#19968;&#65292;&#20247;&#22810;&#25968;&#25454;&#20266;&#36896;&#24211;&#30340;&#25968;&#37327;&#21644;&#37325;&#35201;&#24615;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#20266;&#36896;&#24211;&#30340;&#24320;&#21457;&#32773;&#26080;&#27861;&#28385;&#36275;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#21644;&#39046;&#22495;&#25152;&#38656;&#29983;&#25104;&#30340;&#24191;&#27867;&#25968;&#25454;&#33539;&#22260;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#25191;&#34892;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#65306;1&#65289;&#21407;&#22987;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#65292;2&#65289;&#21512;&#25104;&#29305;&#23450;&#35821;&#35328;&#30340;&#31243;&#24207;&#20197;&#29983;&#25104;&#26377;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;3&#65289;&#29983;&#25104;&#20351;&#29992;&#23574;&#31471;&#20266;&#36896;&#24211;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;LLMs&#20026;11&#20010;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;&#38598;&#25104;&#32423;&#21035;&#19978;&#65292;LLMs&#33021;&#22815;&#25104;&#21151;&#22320;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for Large Language Models (LLMs), which perform test data generation tasks at different levels of integrability: 1) raw test data generation, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic test data generators in a wide range of domains at all three levels of integrability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#24863;&#30693;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#21453;&#36523;&#24615;&#12289;&#23545;&#31216;&#24615;&#21644;&#20256;&#36882;&#24615;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#22810;&#20154;&#29289;&#20851;&#32852;&#19982;&#36319;&#36394;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17617</link><description>&lt;p&gt;
&#25581;&#31034;&#33258;&#30417;&#30563;&#22312;&#22810;&#35270;&#35282;&#22810;&#20154;&#29289;&#20851;&#32852;&#19982;&#36319;&#36394;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Power of Self-supervision for Multi-view Multi-human Association and Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#24863;&#30693;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#21453;&#36523;&#24615;&#12289;&#23545;&#31216;&#24615;&#21644;&#20256;&#36882;&#24615;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#22810;&#20154;&#29289;&#20851;&#32852;&#19982;&#36319;&#36394;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#22810;&#20154;&#29289;&#20851;&#32852;&#19982;&#36319;&#36394;(MvMHAT)&#26159;&#22810;&#20154;&#22330;&#26223;&#35270;&#39057;&#30417;&#25511;&#20013;&#30340;&#19968;&#20010;&#26032;&#32780;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#27599;&#20010;&#35270;&#35282;&#20013;&#36319;&#36394;&#19968;&#32452;&#20154;&#65292;&#24182;&#19988;&#21516;&#26102;&#22312;&#19981;&#21516;&#35270;&#35282;&#20013;&#35782;&#21035;&#30456;&#21516;&#30340;&#20154;&#65292;&#36825;&#19982;&#20808;&#21069;&#20165;&#32771;&#34385;&#36328;&#26102;&#38388;&#20154;&#29289;&#36319;&#36394;&#30340;MOT&#21644;&#22810;&#25668;&#20687;&#22836;MOT&#20219;&#21153;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;MvMHAT&#30340;&#35270;&#39057;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#27880;&#37322;&#65292;&#24182;&#21253;&#21547;&#26356;&#22810;&#30340;&#20449;&#24687;&#20379;&#33258;&#23398;&#20064;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#24863;&#30693;&#30340;&#31471;&#21040;&#31471;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#33258;&#19968;&#33268;&#24615;&#25512;&#29702;&#26469;&#32771;&#34385;&#21453;&#36523;&#24615;&#12289;&#23545;&#31216;&#24615;&#21644;&#20256;&#36882;&#24615;&#19977;&#20010;&#23646;&#24615;&#12290;&#38500;&#20102;&#33258;&#28982;&#28385;&#36275;&#30340;&#21453;&#36523;&#24615;&#23646;&#24615;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#22522;&#20110;&#23545;&#31216;&#24615;&#21644;&#20256;&#36882;&#24615;&#23646;&#24615;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25439;&#22833;&#65292;&#29992;&#20110;&#22806;&#35266;&#29305;&#24449;&#23398;&#20064;&#21644;&#20998;&#37197;&#30697;&#38453;&#20248;&#21270;&#65292;&#20197;&#36827;&#34892;&#20851;&#32852;&#21644;&#36319;&#36394;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view multi-human association and tracking (MvMHAT), is a new but important problem for multi-person scene video surveillance, aiming to track a group of people over time in each view, as well as to identify the same person across different views at the same time, which is different from previous MOT and multi-camera MOT tasks only considering the over-time human tracking. This way, the videos for MvMHAT require more complex annotations while containing more information for self learning. In this work, we tackle this problem with a self-supervised learning aware end-to-end network. Specifically, we propose to take advantage of the spatial-temporal self-consistency rationale by considering three properties of reflexivity, symmetry and transitivity. Besides the reflexivity property that naturally holds, we design the self-supervised learning losses based on the properties of symmetry and transitivity, for both appearance feature learning and assignment matrix optimization, to associ
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;GPT-4V&#27169;&#22411;&#22312;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;GPT-4V&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#22914;&#20301;&#32622;&#29702;&#35299;&#21644;&#22270;&#20687;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#30446;&#26631;&#23450;&#20301;&#21644;&#35745;&#25968;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17600</link><description>&lt;p&gt;
&#22312;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#19978;&#23545;GPT-4V&#36827;&#34892;&#26631;&#27880;&#20219;&#21153;&#35780;&#20272;&#65306;&#23545;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#19978;&#30340;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17600
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;GPT-4V&#27169;&#22411;&#22312;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;GPT-4V&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#22914;&#20301;&#32622;&#29702;&#35299;&#21644;&#22270;&#20687;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#30446;&#26631;&#23450;&#20301;&#21644;&#35745;&#25968;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#28041;&#21450;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#30340;&#22797;&#26434;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#20197;&#21355;&#26143;&#21644;&#33322;&#31354;&#22270;&#20687;&#20026;&#20027;&#30340;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#25968;&#25454;&#19978;&#30340;&#33021;&#21147;&#65292;&#36825;&#31867;&#25968;&#25454;&#22312;VLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#36739;&#20026;&#32597;&#35265;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#35780;&#20272;VLMs&#22312;&#22330;&#26223;&#29702;&#35299;&#12289;&#23450;&#20301;&#21644;&#35745;&#25968;&#20197;&#21450;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#20197;&#34913;&#37327;&#23427;&#20204;&#22312;EO&#25968;&#25454;&#19978;&#20316;&#20026;&#26377;&#25928;&#24037;&#20855;&#30340;&#36827;&#23637;&#12290;&#21463;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#20102;&#22478;&#24066;&#30417;&#27979;&#12289;&#28798;&#23475;&#25937;&#25588;&#12289;&#22303;&#22320;&#21033;&#29992;&#21644;&#20445;&#25252;&#31561;&#22330;&#26223;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#20687;GPT-4V&#36825;&#26679;&#30340;&#26368;&#26032;VLMs&#20855;&#26377;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20301;&#32622;&#29702;&#35299;&#21644;&#22270;&#20687;&#26631;&#27880;&#31561;&#24320;&#25918;&#24335;&#20219;&#21153;&#19978;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#23450;&#20301;&#21644;&#35745;&#25968;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30001;&#20110;&#35270;&#35282;&#21644;&#20809;&#29031;&#21464;&#21270;&#31561;&#22240;&#32032;&#65292;&#21305;&#37197;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24341;&#20837;&#20351;&#24471;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#23545;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#23545;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2401.17592</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Local Feature Matching Using Deep Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30001;&#20110;&#35270;&#35282;&#21644;&#20809;&#29031;&#21464;&#21270;&#31561;&#22240;&#32032;&#65292;&#21305;&#37197;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24341;&#20837;&#20351;&#24471;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#23545;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#23545;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22270;&#20687;&#26816;&#32034;&#12289;&#19977;&#32500;&#37325;&#24314;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#35282;&#21644;&#20809;&#29031;&#21464;&#21270;&#31561;&#22240;&#32032;&#65292;&#25552;&#39640;&#21305;&#37197;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24341;&#20837;&#24341;&#21457;&#20102;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#25216;&#26415;&#30340;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23616;&#37096;&#29305;&#24449;&#21305;&#37197;&#26041;&#27861;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#26159;&#21542;&#23384;&#22312;&#26816;&#27979;&#22120;&#34987;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#22522;&#20110;&#26816;&#27979;&#22120;&#30340;&#31867;&#21035;&#21253;&#25324;&#26816;&#27979;&#28982;&#21518;&#25551;&#36848;&#12289;&#32852;&#21512;&#26816;&#27979;&#21644;&#25551;&#36848;&#12289;&#25551;&#36848;&#28982;&#21518;&#26816;&#27979;&#20197;&#21450;&#22522;&#20110;&#22270;&#30340;&#25216;&#26415;&#12290;&#30456;&#21453;&#65292;&#19981;&#38656;&#35201;&#26816;&#27979;&#22120;&#30340;&#31867;&#21035;&#21253;&#25324;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#12289;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#22359;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36229;&#36234;&#20102;&#26041;&#27861;&#35770;&#20998;&#26512;&#65292;&#36824;&#21253;&#25324;&#23545;&#20808;&#21069;&#24037;&#20316;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local feature matching enjoys wide-ranging applications in the realm of computer vision, encompassing domains such as image retrieval, 3D reconstruction, and object recognition. However, challenges persist in improving the accuracy and robustness of matching due to factors like viewpoint and lighting variations. In recent years, the introduction of deep learning models has sparked widespread exploration into local feature matching techniques. The objective of this endeavor is to furnish a comprehensive overview of local feature matching methods. These methods are categorized into two key segments based on the presence of detectors. The Detector-based category encompasses models inclusive of Detect-then-Describe, Joint Detection and Description, Describe-then-Detect, as well as Graph Based techniques. In contrast, the Detector-free category comprises CNN Based, Transformer Based, and Patch Based methods. Our study extends beyond methodological analysis, incorporating evaluations of prev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;ReCoE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#25152;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#29305;&#23450;&#30340;&#25512;&#29702;&#26041;&#26696;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32534;&#36753;&#27169;&#22411;&#24605;&#32500;&#38142;&#29983;&#25104;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#32534;&#36753;&#12289;&#20107;&#23454;&#22238;&#24518;&#33021;&#21147;&#21644;&#36830;&#36143;&#24615;&#30340;&#32771;&#37327;&#12290;</title><link>https://arxiv.org/abs/2401.17585</link><description>&lt;p&gt;
&#20256;&#25773;&#19982;&#38519;&#38449;&#65306;&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#35780;&#20272;&#22522;&#20110;&#25512;&#29702;&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;ReCoE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#25152;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#29305;&#23450;&#30340;&#25512;&#29702;&#26041;&#26696;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32534;&#36753;&#27169;&#22411;&#24605;&#32500;&#38142;&#29983;&#25104;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#32534;&#36753;&#12289;&#20107;&#23454;&#22238;&#24518;&#33021;&#21147;&#21644;&#36830;&#36143;&#24615;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#26377;&#25928;&#20256;&#25773;&#26356;&#26032;&#30340;&#30456;&#20114;&#20851;&#32852;&#20107;&#23454;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#38459;&#30861;&#20934;&#30830;&#25512;&#29702;&#27169;&#22411;&#20013;&#26356;&#26032;&#30693;&#35782;&#36866;&#24403;&#20256;&#25773;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25512;&#29702;&#30340;&#22522;&#20934;&#8212;&#8212;ReCoE&#65288;&#22522;&#20110;&#25512;&#29702;&#30340;&#21453;&#20107;&#23454;&#32534;&#36753;&#25968;&#25454;&#38598;&#65289;&#65292;&#28085;&#30422;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20845;&#31181;&#24120;&#35265;&#25512;&#29702;&#26041;&#26696;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21253;&#25324;&#36755;&#20837;&#22686;&#24378;&#12289;&#24494;&#35843;&#21644;&#23450;&#20301;&#32534;&#36753;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#37117;&#26126;&#26174;&#36739;&#20302;&#65292;&#23588;&#20854;&#26159;&#22312;&#26576;&#20123;&#25512;&#29702;&#26041;&#26696;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#32534;&#36753;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#29983;&#25104;&#30340;&#20998;&#26512;&#65292;&#20174;&#25512;&#29702;&#30340;&#35282;&#24230;&#25581;&#31034;&#20102;&#29616;&#26377;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19981;&#36275;&#30340;&#20851;&#38190;&#21407;&#22240;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#32534;&#36753;&#12289;&#20107;&#23454;&#22238;&#24518;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#30340;&#36830;&#36143;&#24615;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches of knowledge editing struggle to effectively propagate updates to interconnected facts. In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing dataset) -- which covers six common reasoning schemes in real world. We conduct a thorough analysis of existing knowledge editing techniques, including input augmentation, finetuning, and locate-and-edit. We found that all model editing methods show notably low performance on this dataset, especially in certain reasoning schemes. Our analysis over the chain-of-thought generation of edited models further uncover key reasons behind the inadequacy of existing knowledge editing methods from a reasoning standpoint, involving aspects on fact-wise editing, fact recall ability, and coherence in generation. We will make our ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2401.17583</link><description>&lt;p&gt;
&#25935;&#25463;&#20294;&#23433;&#20840;&#65306;&#23398;&#20064;&#26080;&#30896;&#25758;&#39640;&#36895;&#22235;&#36275;&#26426;&#22120;&#20154;&#34892;&#36208;
&lt;/p&gt;
&lt;p&gt;
Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#34892;&#36208;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#24517;&#39035;&#26082;&#25935;&#25463;&#20197;&#25552;&#39640;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#65292;&#21448;&#35201;&#30830;&#20445;&#23433;&#20840;&#65292;&#36991;&#20813;&#19982;&#38556;&#30861;&#29289;&#25110;&#20154;&#30896;&#25758;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#24320;&#21457;&#20445;&#23432;&#30340;&#25511;&#21046;&#22120;&#65288;&#36895;&#24230;&#23567;&#20110;1.0 m/s&#65289;&#20197;&#30830;&#20445;&#23433;&#20840;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#25935;&#25463;&#24615;&#32780;&#26410;&#32771;&#34385;&#28508;&#22312;&#33268;&#21629;&#30340;&#30896;&#25758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23454;&#29616;&#20102;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#30340;&#34892;&#36208;&#12290;ABS&#21253;&#25324;&#19968;&#20010;&#25935;&#25463;&#31574;&#30053;&#26469;&#22312;&#38556;&#30861;&#29289;&#20013;&#25191;&#34892;&#28789;&#27963;&#30340;&#21160;&#20316;&#25216;&#33021;&#65292;&#24182;&#19988;&#26377;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#26469;&#36991;&#20813;&#22833;&#36133;&#65292;&#20849;&#21516;&#23454;&#29616;&#39640;&#36895;&#19988;&#26080;&#30896;&#25758;&#30340;&#23548;&#33322;&#12290;ABS&#20013;&#30340;&#31574;&#30053;&#20999;&#25442;&#30001;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#25511;&#21046;&#65292;&#35813;&#32593;&#32476;&#20063;&#25351;&#23548;&#24674;&#22797;&#31574;&#30053;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#38381;&#29615;&#20013;&#20445;&#25252;&#26426;&#22120;&#20154;&#12290;&#35757;&#32451;&#36807;&#31243;&#28041;&#21450;&#25935;&#25463;&#31574;&#30053;&#12289;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#12289;&#24674;&#22797;&#31574;&#30053;&#21644;&#22806;&#24863;&#30693;&#34920;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (&lt; 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17542</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#21512;&#21307;&#23398;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Effective Learning: A Comprehensive Medical Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26088;&#22312;&#20197;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#20854;&#28041;&#21450;&#20851;&#27880;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#25968;&#37327;&#30340;&#31574;&#30053;&#65292;&#30830;&#20445;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#20855;&#26377;&#39640;&#20449;&#24687;&#20215;&#20540;&#12290;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#22312;&#21152;&#24555;AI&#35757;&#32451;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#33410;&#30465;&#25968;&#25454;&#23384;&#20648;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#22312;&#36817;&#24180;&#26469;&#21307;&#23398;&#25968;&#25454;&#30340;&#25968;&#37327;&#36229;&#20986;&#20102;&#35768;&#22810;&#20154;&#30340;&#39044;&#26399;&#26102;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21644;&#32508;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;31&#20010;&#21307;&#30103;&#20013;&#24515;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;(DataDEL)&#65292;&#29992;&#20110;&#27604;&#36739;&#30340;&#22522;&#20934;&#26041;&#27861;(MedDEL)&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#23458;&#35266;&#34913;&#37327;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#24615;&#33021;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;(NormDEL)&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#22312;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive e
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20572;&#27490;&#21106;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#28151;&#21512;&#22270;&#34920;&#31034;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21160;&#24577;&#22320;&#20915;&#31574;&#20309;&#26102;&#20572;&#27490;&#21106;&#29983;&#25104;&#65292;&#24182;&#26377;&#25928;&#25429;&#25417;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2401.17527</link><description>&lt;p&gt;
&#23398;&#20064;&#20572;&#27490;&#21106;&#29983;&#25104;&#20197;&#25552;&#39640;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20572;&#27490;&#21106;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#28151;&#21512;&#22270;&#34920;&#31034;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21160;&#24577;&#22320;&#20915;&#31574;&#20309;&#26102;&#20572;&#27490;&#21106;&#29983;&#25104;&#65292;&#24182;&#26377;&#25928;&#25429;&#25417;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#21106;&#24179;&#38754;&#22312;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#26174;&#33879;&#21152;&#32039;&#20102;&#23545;&#20598;&#30028;&#38480;&#24182;&#25913;&#21892;&#20102;&#27714;&#35299;&#24615;&#33021;&#12290;&#21106;&#29983;&#25104;&#20572;&#27490;&#38382;&#39064;&#26159;&#21106;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#23545;&#20110;&#25552;&#39640;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#20195;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#20351;&#29992;&#30828;&#32534;&#30721;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#24448;&#24448;&#24573;&#35270;&#20102;&#26469;&#33258;&#26576;&#20123;&#24212;&#29992;&#20013;&#30340;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#28508;&#22312;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#21106;&#29983;&#25104;&#20572;&#27490;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#22270;&#34920;&#31034;&#27169;&#22411;&#65288;HYGRO&#65289;&#65292;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#20572;&#27490;&#31574;&#30053;&#12290;HYGRO&#30340;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#26159;&#23427;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#21040;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20572;&#27490;&#31574;&#30053;&#30340;&#21160;&#24577;&#20915;&#31574;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;HYGRO&#26159;&#35299;&#20915;&#21106;&#29983;&#25104;&#20572;&#27490;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#25972;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#22312; &#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cutting planes (cuts) play an important role in solving mixed-integer linear programs (MILPs), as they significantly tighten the dual bounds and improve the solving performance. A key problem for cuts is when to stop cuts generation, which is important for the efficiency of solving MILPs. However, many modern MILP solvers employ hard-coded heuristics to tackle this problem, which tends to neglect underlying patterns among MILPs from certain applications. To address this challenge, we formulate the cuts generation stopping problem as a reinforcement learning problem and propose a novel hybrid graph representation model (HYGRO) to learn effective stopping strategies. An appealing feature of HYGRO is that it can effectively capture both the dynamic and static features of MILPs, enabling dynamic decision-making for the stopping strategies. To the best of our knowledge, HYGRO is the first data-driven method to tackle the cuts generation stopping problem. By integrating our approach with mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#26412;&#22320;&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#30340;PNP&#31163;&#23376;&#36890;&#36947;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#12290;&#35813;&#27714;&#35299;&#22120;&#33021;&#22815;&#36739;&#24555;&#22320;&#35757;&#32451;&#24182;&#29983;&#25104;&#39640;&#20934;&#30830;&#24230;&#30340;&#25968;&#20540;&#35299;&#65292;&#22312;&#22788;&#29702;&#19981;&#21516;&#25200;&#21160;&#24773;&#20917;&#21644;&#31163;&#23376;&#36890;&#36947;&#23376;&#21306;&#22495;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2401.17513</link><description>&lt;p&gt;
&#19968;&#20010;&#20855;&#26377;&#26412;&#22320;&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#36755;&#20837;&#25968;&#25454;&#30340;PNP&#31163;&#23376;&#36890;&#36947;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A PNP ion channel deep learning solver with local neural network and finite element input data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#26412;&#22320;&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#30340;PNP&#31163;&#23376;&#36890;&#36947;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#12290;&#35813;&#27714;&#35299;&#22120;&#33021;&#22815;&#36739;&#24555;&#22320;&#35757;&#32451;&#24182;&#29983;&#25104;&#39640;&#20934;&#30830;&#24230;&#30340;&#25968;&#20540;&#35299;&#65292;&#22312;&#22788;&#29702;&#19981;&#21516;&#25200;&#21160;&#24773;&#20917;&#21644;&#31163;&#23376;&#36890;&#36947;&#23376;&#21306;&#22495;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#25913;&#36827;&#30340;&#19968;&#32500;Poisson-Nernst-Planck&#31163;&#23376;&#36890;&#36947;&#65288;PNPic&#65289;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;PNPic&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#23558;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#22320;&#31070;&#32463;&#32593;&#32476;&#26041;&#26696;&#19982;&#26377;&#25928;&#30340;PNPic&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#26041;&#26696;&#30340;&#36755;&#20837;&#25968;&#25454;&#21482;&#28041;&#21450;&#31895;&#32593;&#26684;&#35299;&#30340;&#19968;&#23567;&#22359;&#23616;&#37096;&#21306;&#22495;&#65292;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#65292;&#22240;&#27492;PNPic&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;&#20219;&#20309;&#23545;&#24212;&#30340;&#20256;&#32479;&#20840;&#23616;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#37117;&#35201;&#24555;&#12290;&#32463;&#36807;&#36866;&#24403;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#36755;&#20986;&#19968;&#20010;&#27604;&#20302;&#25104;&#26412;&#31895;&#32593;&#26684;&#35299;&#26356;&#39640;&#20934;&#30830;&#24230;&#30340;&#39044;&#27979;PNPic&#35299;&#65292;&#24182;&#33021;&#21453;&#26144;&#19981;&#21516;&#21442;&#25968;&#25200;&#21160;&#24773;&#20917;&#12289;&#31163;&#23376;&#36890;&#36947;&#23376;&#21306;&#22495;&#20197;&#21450;&#30028;&#38754;&#21644;&#36793;&#30028;&#20540;&#31561;&#12290;&#22240;&#27492;&#65292;PNPic&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#21487;&#20197;&#20026;&#19968;&#31867;PNPic&#27169;&#22411;&#29983;&#25104;&#39640;&#20934;&#30830;&#24230;&#30340;&#25968;&#20540;&#35299;&#12290;&#20316;&#20026;&#19968;&#20010;&#21021;&#27493;&#30740;&#31350;&#65292;&#20004;&#20010;......
&lt;/p&gt;
&lt;p&gt;
In this paper, a deep learning method for solving an improved one-dimensional Poisson-Nernst-Planck ion channel (PNPic) model, called the PNPic deep learning solver, is presented. In particular, it combines a novel local neural network scheme with an effective PNPic finite element solver. Since the input data of the neural network scheme only involves a small local patch of coarse grid solutions, which the finite element solver can quickly produce, the PNPic deep learning solver can be trained much faster than any corresponding conventional global neural network solvers. After properly trained, it can output a predicted PNPic solution in a much higher degree of accuracy than the low cost coarse grid solutions and can reflect different perturbation cases on the parameters, ion channel subregions, and interface and boundary values, etc. Consequently, the PNPic deep learning solver can generate a numerical solution with high accuracy for a family of PNPic models. As an initial study, two 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#38754;&#21521;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20307;&#22806;&#21463;&#31934;&#32467;&#26524;&#39044;&#27979;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2401.17511</link><description>&lt;p&gt;
&#22312;&#38754;&#21521;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#35821;&#35328;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Linguistically Communicating Uncertainty in Patient-Facing Risk Prediction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#38754;&#21521;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20307;&#22806;&#21463;&#31934;&#32467;&#26524;&#39044;&#27979;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#24212;&#29992;&#20110;&#38754;&#21521;&#24739;&#32773;&#29615;&#22659;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#19982;&#20026;&#27169;&#22411;&#24320;&#21457;&#32773;&#25110;&#39046;&#22495;&#19987;&#23478;&#37327;&#36523;&#23450;&#21046;&#30340;&#20256;&#32479;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#37324;&#38656;&#35201;&#32771;&#34385;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#12289;&#23637;&#31034;&#21644;&#35780;&#20272;&#21487;&#29702;&#35299;&#24615;&#30340;&#38468;&#21152;&#22240;&#32032;&#12290;&#25105;&#20204;&#22312;&#39118;&#38505;&#39044;&#27979;&#30340;&#35821;&#22659;&#19979;&#35782;&#21035;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#27807;&#36890;&#27169;&#22411;&#24615;&#33021;&#12289;&#32622;&#20449;&#24230;&#12289;&#25512;&#29702;&#21644;&#26410;&#30693;&#24050;&#30693;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#20307;&#22806;&#21463;&#31934;&#32467;&#26524;&#39044;&#27979;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the unique challenges associated with uncertainty quantification in AI models when applied to patient-facing contexts within healthcare. Unlike traditional eXplainable Artificial Intelligence (XAI) methods tailored for model developers or domain experts, additional considerations of communicating in natural language, its presentation and evaluating understandability are necessary. We identify the challenges in communication model performance, confidence, reasoning and unknown knowns using natural language in the context of risk prediction. We propose a design aimed at addressing these challenges, focusing on the specific application of in-vitro fertilisation outcome prediction.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>LeTO &#26159;&#19968;&#31181;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#23454;&#29616;&#21463;&#38480;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#20248;&#21270;&#23618;&#34920;&#31034;&#20026;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#20197;&#23433;&#20840;&#21487;&#25511;&#30340;&#26041;&#24335;&#31471;&#21040;&#31471;&#29983;&#25104;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#32422;&#26463;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#24179;&#34913;&#28385;&#36275;&#32422;&#26463;&#12289;&#24179;&#28369;&#36712;&#36857;&#21644;&#26368;&#23567;&#21270;&#28436;&#31034;&#35823;&#24046;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#26126;LeTO&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2401.17500</link><description>&lt;p&gt;
LeTO&#65306;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#23398;&#20064;&#21463;&#38480;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
LeTO: Learning Constrained Visuomotor Policy with Differentiable Trajectory Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17500
&lt;/p&gt;
&lt;p&gt;
LeTO &#26159;&#19968;&#31181;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#23454;&#29616;&#21463;&#38480;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#20248;&#21270;&#23618;&#34920;&#31034;&#20026;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#20197;&#23433;&#20840;&#21487;&#25511;&#30340;&#26041;&#24335;&#31471;&#21040;&#31471;&#29983;&#25104;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#32422;&#26463;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#24179;&#34913;&#28385;&#36275;&#32422;&#26463;&#12289;&#24179;&#28369;&#36712;&#36857;&#21644;&#26368;&#23567;&#21270;&#28436;&#31034;&#35823;&#24046;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#26126;LeTO&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LeTO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#23454;&#29616;&#21463;&#38480;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23558;&#19968;&#20010;&#21487;&#24494;&#20998;&#20248;&#21270;&#23618;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#23558;&#20248;&#21270;&#23618;&#34920;&#31034;&#20026;&#19968;&#20010;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#27169;&#22411;&#20197;&#23433;&#20840;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#31471;&#21040;&#31471;&#29983;&#25104;&#21160;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#32422;&#26463;&#20449;&#24687;&#65292;&#20174;&#32780;&#24179;&#34913;&#28385;&#36275;&#32422;&#26463;&#12289;&#24179;&#28369;&#36712;&#36857;&#21644;&#26368;&#23567;&#21270;&#28436;&#31034;&#35823;&#24046;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#8220;&#28784;&#30418;&#8221;&#26041;&#27861;&#23558;&#22522;&#20110;&#20248;&#21270;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#34920;&#36798;&#33021;&#21147;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#23545;LeTO&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#22312;&#20223;&#30495;&#20013;&#65292;LeTO&#30340;&#25104;&#21151;&#29575;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#65292;&#20294;&#29983;&#25104;&#30340;&#36712;&#36857;&#30340;&#19981;&#19968;&#33268;&#24615;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces LeTO, a method for learning constrained visuomotor policy via differentiable trajectory optimization. Our approach uniquely integrates a differentiable optimization layer into the neural network. By formulating the optimization layer as a trajectory optimization problem, we enable the model to end-to-end generate actions in a safe and controlled fashion without extra modules. Our method allows for the introduction of constraints information during the training process, thereby balancing the training objectives of satisfying constraints, smoothing the trajectories, and minimizing errors with demonstrations. This "gray box" method marries the optimization-based safety and interpretability with the powerful representational abilities of neural networks. We quantitatively evaluate LeTO in simulation and on the real robot. In simulation, LeTO achieves a success rate comparable to state-of-the-art imitation learning methods, but the generated trajectories are of less un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#23545;&#35805;&#20195;&#29702;&#22120;ChatGPT&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#24182;&#20511;&#21161;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#35835;&#30340;&#35780;&#35770;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2401.17477</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#24515;&#29702;&#38556;&#30861;&#65306;&#22522;&#20110;ChatGPT&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting mental disorder on social media: a ChatGPT-augmented explainable approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#23545;&#35805;&#20195;&#29702;&#22120;ChatGPT&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#24182;&#20511;&#21161;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#35835;&#30340;&#35780;&#35770;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#34920;&#36798;&#30340;&#25233;&#37057;&#30151;&#29366;&#30340;&#39057;&#29575;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#65292;&#36843;&#20999;&#38656;&#35201;&#20808;&#36827;&#30340;&#26041;&#27861;&#26469;&#21450;&#26102;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;ChatGPT&#31561;&#23545;&#35805;&#20195;&#29702;&#22120;&#26377;&#25928;&#22320;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24212;&#23545;&#21487;&#35299;&#37322;&#24615;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25513;&#30721;&#27880;&#24847;&#21147;&#25552;&#20379;&#20998;&#31867;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#21487;&#35835;&#24615;&#24378;&#30340;&#35780;&#35770;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26377;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#21487;&#35299;&#37322;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#20570;&#20986;&#36129;&#29486;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the digital era, the prevalence of depressive symptoms expressed on social media has raised serious concerns, necessitating advanced methodologies for timely detection. This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT. In our methodology, explanations are achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a novel self-explanatory model, namely BERT-XDD, capable of providing both classification and explanations via masked attention. The interpretability is further enhanced using ChatGPT to transform technical explanations into human-readable commentaries. By introducing an effective and modular approach for interpretable depression detection, our methodology can contribute to the development of socially responsible digital platforms, fostering early intervention and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;LLM&#26234;&#33021;&#20307;&#29983;&#25104;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#26234;&#33021;&#20307;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#27969;&#26469;&#33719;&#21462;&#29983;&#25104;&#32447;&#24615;&#27169;&#22411;&#25152;&#38656;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#35805;&#30340;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.17461</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#26234;&#33021;&#20307;&#29983;&#25104;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synthetic Dialogue Dataset Generation using LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;LLM&#26234;&#33021;&#20307;&#29983;&#25104;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#26234;&#33021;&#20307;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#27969;&#26469;&#33719;&#21462;&#29983;&#25104;&#32447;&#24615;&#27169;&#22411;&#25152;&#38656;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#35805;&#30340;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#34920;&#38754;&#19978;&#31616;&#21333;&#65292;&#20294;&#26410;&#32463;&#35757;&#32451;&#30340;&#29992;&#25143;&#21487;&#33021;&#38590;&#20197;&#30830;&#23450;&#20854;&#29305;&#23450;&#38382;&#39064;&#30340;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#24819;&#21019;&#24314;&#19968;&#20010;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#26234;&#33021;&#20307;&#65292;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#27969;&#65292;&#20197;&#33719;&#21462;&#29983;&#25104;&#32447;&#24615;&#27169;&#22411;&#25152;&#38656;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20174;&#32780;&#24314;&#31435;&#19968;&#20010;&#21518;&#32493;&#30340;&#26234;&#33021;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#21644;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#23545;&#35805;&#26234;&#33021;&#20307;&#30340;&#31034;&#20363;&#23545;&#35805;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21551;&#21457;&#24335;&#35774;&#35745;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#30456;&#20114;&#8220;&#23545;&#35805;&#8221;&#30340;&#26234;&#33021;&#20307;&#65292;&#19968;&#20010;&#20805;&#24403;&#23545;&#35805;&#26234;&#33021;&#20307;&#65292;&#21478;&#19968;&#20010;&#20805;&#24403;&#29992;&#25143;&#12290;&#20351;&#29992;&#20165;&#23545;&#29992;&#25143;&#21487;&#35265;&#30340;&#19968;&#32452;&#32447;&#24615;&#38382;&#39064;&#25991;&#26412;&#25551;&#36848;&#65292;&#26234;&#33021;&#20307;&#21644;&#29992;&#25143;&#36827;&#34892;&#23545;&#35805;&#65292;&#30452;&#21040;&#26234;&#33021;&#20307;&#20174;&#21407;&#22987;&#38382;&#39064;&#25551;&#36848;&#20013;&#33719;&#21462;&#21040;&#25152;&#26377;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;&#35805;&#30340;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear programming (LP) problems are pervasive in real-life applications. However, despite their apparent simplicity, an untrained user may find it difficult to determine the linear model of their specific problem. We envisage the creation of a goal-oriented conversational agent that will engage in conversation with the user to elicit all information required so that a subsequent agent can generate the linear model. In this paper, we present an approach for the generation of sample dialogues that can be used to develop and train such a conversational agent. Using prompt engineering, we develop two agents that "talk" to each other, one acting as the conversational agent, and the other acting as the user. Using a set of text descriptions of linear problems from NL4Opt available to the user only, the agent and the user engage in conversation until the agent has retrieved all key information from the original problem description. We also propose an extrinsic evaluation of the dialogues by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36719;&#20214;&#28183;&#36879;&#27979;&#35797;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#25913;&#36827;&#25552;&#31034;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17459</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#28183;&#36879;&#27979;&#35797;&#20013;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Study on Using Large Language Models in Software Pentesting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36719;&#20214;&#28183;&#36879;&#27979;&#35797;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#25913;&#36827;&#25552;&#31034;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#35748;&#20026;&#20855;&#26377;&#33258;&#21160;&#21270;&#23433;&#20840;&#20219;&#21153;&#30340;&#28508;&#22312;&#28508;&#21147;&#65292;&#20363;&#22914;&#23433;&#20840;&#25805;&#20316;&#20013;&#24515;&#65288;SOC&#65289;&#20013;&#30340;&#20219;&#21153;&#12290;&#20316;&#20026;&#35780;&#20272;&#36825;&#31181;&#28508;&#21147;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36719;&#20214;&#28183;&#36879;&#27979;&#35797;&#20013;&#20351;&#29992;LLMs&#65292;&#20854;&#20013;&#20027;&#35201;&#20219;&#21153;&#26159;&#33258;&#21160;&#35782;&#21035;&#28304;&#20195;&#30721;&#20013;&#30340;&#36719;&#20214;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#20551;&#35774;&#22522;&#20110;LLM&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21487;&#20197;&#38543;&#30528;&#20154;&#21592;&#30340;&#20114;&#21160;&#32780;&#19981;&#26029;&#25913;&#36827;&#29305;&#23450;&#23433;&#20840;&#20219;&#21153;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#36890;&#36807;&#26681;&#25454;&#29983;&#25104;&#30340;&#21709;&#24212;&#65292;&#23558;&#30456;&#20851;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#21253;&#21547;&#22312;LLM&#20013;&#30340;&#25552;&#31034;&#20043;&#20013;&#65292;&#26469;&#25913;&#36827;AI&#20195;&#29702;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#24037;&#31243;&#21162;&#21147;&#21487;&#20197;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#26410;&#26469;&#30340;&#26410;&#30693;&#20219;&#21153;&#19978;&#20063;&#33021;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#21017;&#36825;&#26679;&#30340;&#24037;&#31243;&#21162;&#21147;&#23558;&#20855;&#26377;&#21487;&#25345;&#32493;&#24615;&#12290;&#20026;&#20102;&#26816;&#39564;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#21033;&#29992;&#21253;&#21547;2,740&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;OWASP&#22522;&#20934;&#39033;&#30446;1.2&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) are perceived to offer promising potentials for automating security tasks, such as those found in security operation centers (SOCs). As a first step towards evaluating this perceived potential, we investigate the use of LLMs in software pentesting, where the main task is to automatically identify software security vulnerabilities in source code. We hypothesize that an LLM-based AI agent can be improved over time for a specific security task as human operators interact with it. Such improvement can be made, as a first step, by engineering prompts fed to the LLM based on the responses produced, to include relevant contexts and structures so that the model provides more accurate results. Such engineering efforts become sustainable if the prompts that are engineered to produce better results on current tasks, also produce better results on future unknown tasks. To examine this hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains 2,740 hand-craft
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24182;&#34892;&#35843;&#28201;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#35282;&#33394;&#20998;&#21106;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23616;&#37096;&#31227;&#21160;&#22312;&#19981;&#21516;&#23610;&#24230;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#28385;&#36275;&#21508;&#31181;&#22522;&#20110;&#25919;&#31574;&#30340;&#25351;&#26631;&#65292;&#24182;&#22312;&#24247;&#28037;&#29380;&#26684;&#24030;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2401.17455</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#24182;&#34892;&#35843;&#28201;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#35282;&#33394;&#20998;&#21106;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Multiscale Parallel Tempering for Fast Sampling on Redistricting Plans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24182;&#34892;&#35843;&#28201;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#35282;&#33394;&#20998;&#21106;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23616;&#37096;&#31227;&#21160;&#22312;&#19981;&#21516;&#23610;&#24230;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#28385;&#36275;&#21508;&#31181;&#22522;&#20110;&#25919;&#31574;&#30340;&#25351;&#26631;&#65292;&#24182;&#22312;&#24247;&#28037;&#29380;&#26684;&#24030;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23457;&#26597;&#19968;&#20010;&#35282;&#33394;&#20998;&#21106;&#26041;&#26696;&#26102;&#65292;&#19968;&#20010;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#26041;&#27861;&#26159;&#23558;&#35813;&#26041;&#26696;&#19982;&#20379;&#21442;&#32771;&#30340;&#20013;&#31435;&#35282;&#33394;&#20998;&#21106;&#26041;&#26696;&#38598;&#21512;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#20123;&#38598;&#21512;&#36890;&#36807;&#23545;&#24179;&#34913;&#22270;&#20998;&#21306;&#36827;&#34892;&#20998;&#24067;&#37319;&#26679;&#31639;&#27861;&#29983;&#25104;&#12290;&#20026;&#20102;&#23457;&#26597;&#38598;&#21512;&#19982;&#32473;&#23450;&#26041;&#26696;&#20043;&#38388;&#30340;&#20826;&#27966;&#24046;&#24322;&#65292;&#24517;&#39035;&#30830;&#20445;&#21305;&#37197;&#38750;&#20826;&#27966;&#26631;&#20934;&#65292;&#20197;&#20415;&#21487;&#20197;&#24471;&#20986;&#20826;&#27966;&#24046;&#24322;&#26469;&#33258;&#20110;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#26469;&#33258;&#20110;&#32039;&#20945;&#24615;&#27700;&#24179;&#25110;&#31038;&#21306;&#20445;&#25252;&#30340;&#24046;&#24322;&#31561;&#12290;&#26576;&#20123;&#37319;&#26679;&#31639;&#27861;&#20801;&#35768;&#26126;&#30830;&#35268;&#23450;&#22522;&#20110;&#25919;&#31574;&#30340;&#26041;&#26696;&#27010;&#29575;&#20998;&#24067;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#22823;&#22411;&#22270;&#65288;&#21363;&#35282;&#33394;&#20998;&#21106;&#31354;&#38388;&#65289;&#20013;&#30340;&#28151;&#21512;&#26102;&#38388;&#34920;&#29616;&#20986;&#36739;&#24046;&#65292;&#38500;&#20102;&#19968;&#20123;&#19987;&#38376;&#21270;&#30340;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24182;&#34892;&#35843;&#28201;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27599;&#20010;&#23610;&#24230;&#19978;&#36827;&#34892;&#23616;&#37096;&#31227;&#21160;&#12290;&#23616;&#37096;&#31227;&#21160;&#20351;&#25105;&#20204;&#33021;&#22815;&#37319;&#29992;&#22810;&#31181;&#22522;&#20110;&#25919;&#31574;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#22312;&#24247;&#28037;&#29380;&#26684;&#24030;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
When auditing a redistricting plan, a persuasive method is to compare the plan with an ensemble of neutrally drawn redistricting plans. Ensembles are generated via algorithms that sample distributions on balanced graph partitions. To audit the partisan difference between the ensemble and a given plan, one must ensure that the non-partisan criteria are matched so that we may conclude that partisan differences come from bias rather than, for example, levels of compactness or differences in community preservation. Certain sampling algorithms allow one to explicitly state the policy-based probability distribution on plans, however, these algorithms have shown poor mixing times for large graphs (i.e. redistricting spaces) for all but a few specialized measures. In this work, we generate a multiscale parallel tempering approach that makes local moves at each scale. The local moves allow us to adopt a wide variety of policy-based measures. We examine our method in the state of Connecticut and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28082;&#24577;&#27665;&#20027;&#23454;&#29616;&#20302;&#25104;&#26412;&#38598;&#21512;&#21098;&#26525;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#28082;&#24577;&#27665;&#20027;&#30340;&#22996;&#27966;&#26426;&#21046;&#35782;&#21035;&#21644;&#31227;&#38500;&#20887;&#20313;&#20998;&#31867;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#38598;&#21512;&#35757;&#32451;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#27604;&#26576;&#20123;&#22686;&#24378;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#25991;&#29486;&#26694;&#26550;&#22312;&#38750;&#20256;&#32479;&#39046;&#22495;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2401.17443</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#38598;&#21512;&#21098;&#26525;&#30340;&#28082;&#24577;&#27665;&#20027;
&lt;/p&gt;
&lt;p&gt;
Liquid Democracy for Low-Cost Ensemble Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28082;&#24577;&#27665;&#20027;&#23454;&#29616;&#20302;&#25104;&#26412;&#38598;&#21512;&#21098;&#26525;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#28082;&#24577;&#27665;&#20027;&#30340;&#22996;&#27966;&#26426;&#21046;&#35782;&#21035;&#21644;&#31227;&#38500;&#20887;&#20313;&#20998;&#31867;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#38598;&#21512;&#35757;&#32451;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#27604;&#26576;&#20123;&#22686;&#24378;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#25991;&#29486;&#26694;&#26550;&#22312;&#38750;&#20256;&#32479;&#39046;&#22495;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#38598;&#21512;&#23398;&#20064;&#21644;&#19968;&#31181;&#20195;&#29702;&#25237;&#31080;&#27169;&#24335;&#8212;&#8212;&#28082;&#24577;&#27665;&#20027;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#28872;&#30340;&#32852;&#31995;&#65292;&#21487;&#20197;&#36890;&#36807;&#28082;&#24577;&#27665;&#20027;&#26469;&#38477;&#20302;&#38598;&#21512;&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35757;&#32451;&#30340;&#36807;&#31243;&#65292;&#36890;&#36807;&#28082;&#24577;&#27665;&#20027;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#22996;&#27966;&#26426;&#21046;&#26469;&#35782;&#21035;&#21644;&#31227;&#38500;&#38598;&#21512;&#20013;&#30340;&#20887;&#20313;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#36807;&#31243;&#22823;&#22823;&#38477;&#20302;&#20102;&#35757;&#32451;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#30456;&#27604;&#20110;&#35757;&#32451;&#19968;&#20010;&#23436;&#25972;&#30340;&#38598;&#21512;&#12290;&#36890;&#36807;&#31934;&#36873;&#24213;&#23618;&#30340;&#22996;&#27966;&#26426;&#21046;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#32676;&#20307;&#30340;&#26435;&#37325;&#38598;&#20013;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#65292;&#36825;&#39033;&#24037;&#20316;&#20063;&#23637;&#31034;&#20102;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#25991;&#29486;&#20013;&#30340;&#26694;&#26550;&#22914;&#20309;&#24212;&#29992;&#20110;&#38750;&#20256;&#32479;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that there is a strong connection between ensemble learning and a delegative voting paradigm -- liquid democracy -- that can be leveraged to reduce ensemble training costs. We present an incremental training procedure that identifies and removes redundant classifiers from an ensemble via delegation mechanisms inspired by liquid democracy. Through both analysis and extensive experiments we show that this process greatly reduces the computational cost of training compared to training a full ensemble. By carefully selecting the underlying delegation mechanism, weight centralization in the classifier population is avoided, leading to higher accuracy than some boosting methods. Furthermore, this work serves as an exemplar of how frameworks from computational social choice literature can be applied to problems in nontraditional domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20027;&#35201;&#21463;&#21040;&#21333;&#20010;&#29305;&#24449;&#25110;&#29305;&#24449;&#20043;&#38388;&#20056;&#31215;&#30456;&#20114;&#20316;&#29992;&#30340;&#20108;&#38454;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36825;&#20123;&#20108;&#38454;&#24433;&#21709;&#26469;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#30340;&#21327;&#26041;&#24046;&#35745;&#31639;&#23545;&#19968;&#38454;&#35299;&#37322;&#36827;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#23558;&#24120;&#35265;&#30340;&#24402;&#22240;&#25216;&#26415;&#36716;&#21270;&#20026;&#24378;&#22823;&#30340;&#20108;&#38454;&#19981;&#30830;&#23450;&#24615;&#35299;&#37322;&#22120;&#12290;&#20316;&#32773;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25972;&#20307;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17441</link><description>&lt;p&gt;
&#25581;&#31034;&#20108;&#38454;&#24433;&#21709;&#26469;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explaining Predictive Uncertainty by Exposing Second-Order Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20027;&#35201;&#21463;&#21040;&#21333;&#20010;&#29305;&#24449;&#25110;&#29305;&#24449;&#20043;&#38388;&#20056;&#31215;&#30456;&#20114;&#20316;&#29992;&#30340;&#20108;&#38454;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36825;&#20123;&#20108;&#38454;&#24433;&#21709;&#26469;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#30340;&#21327;&#26041;&#24046;&#35745;&#31639;&#23545;&#19968;&#38454;&#35299;&#37322;&#36827;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#23558;&#24120;&#35265;&#30340;&#24402;&#22240;&#25216;&#26415;&#36716;&#21270;&#20026;&#24378;&#22823;&#30340;&#20108;&#38454;&#19981;&#30830;&#23450;&#24615;&#35299;&#37322;&#22120;&#12290;&#20316;&#32773;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25972;&#20307;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;Explainable AI&#65289;&#20351;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#40657;&#31665;&#21464;&#24471;&#36879;&#26126;&#65292;&#29305;&#21035;&#26159;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#29992;&#26469;&#36827;&#34892;&#39044;&#27979;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21363;&#20026;&#20160;&#20040;&#27169;&#22411;&#8220;&#19981;&#30830;&#23450;&#8221;&#65292;&#30446;&#21069;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20027;&#35201;&#30001;&#28041;&#21450;&#21333;&#20010;&#29305;&#24449;&#25110;&#29305;&#24449;&#20043;&#38388;&#30340;&#20056;&#31215;&#30456;&#20114;&#20316;&#29992;&#30340;&#20108;&#38454;&#24433;&#21709;&#25152;&#20027;&#23548;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36825;&#20123;&#20108;&#38454;&#24433;&#21709;&#26469;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#35745;&#31639;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#25104;&#23545;&#19968;&#32452;&#19968;&#38454;&#35299;&#37322;&#36827;&#34892;&#31616;&#21333;&#21327;&#26041;&#24046;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#21487;&#20197;&#23558;&#24120;&#35265;&#30340;&#24402;&#22240;&#25216;&#26415;&#65288;LRP&#65292;Gradient x Input&#31561;&#65289;&#36716;&#21270;&#20026;&#24378;&#22823;&#30340;&#20108;&#38454;&#19981;&#30830;&#23450;&#24615;&#35299;&#37322;&#22120;&#65292;&#31216;&#20026;CovLRP&#65292;CovGI&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#37327;&#21270;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#20135;&#29983;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25972;&#20307;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI has brought transparency into complex ML blackboxes, enabling, in particular, to identify which features these models use for their predictions. So far, the question of explaining predictive uncertainty, i.e. why a model 'doubts', has been scarcely studied. Our investigation reveals that predictive uncertainty is dominated by second-order effects, involving single features or product interactions between them. We contribute a new method for explaining predictive uncertainty based on these second-order effects. Computationally, our method reduces to a simple covariance computation over a collection of first-order explanations. Our method is generally applicable, allowing for turning common attribution techniques (LRP, Gradient x Input, etc.) into powerful second-order uncertainty explainers, which we call CovLRP, CovGI, etc. The accuracy of the explanations our method produces is demonstrated through systematic quantitative evaluations, and the overall usefulness of our m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#26041;&#27861;&#32467;&#21512;&#29609;&#23478;&#20998;&#26512;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#31227;&#21160;&#25340;&#22270;&#28216;&#25103;&#20013;&#38590;&#24230;&#30340;&#20934;&#30830;&#20272;&#31639;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#22522;&#20110;&#38431;&#21015;&#32479;&#35745;&#21644;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#33021;&#22815;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#20135;&#29983;&#26368;&#20934;&#30830;&#30340;&#20272;&#31639;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.17436</link><description>&lt;p&gt;
&#31227;&#21160;&#25340;&#22270;&#28216;&#25103;&#20013;&#30340;&#38590;&#24230;&#24314;&#27169;&#65306;&#23545;&#29609;&#23478;&#20998;&#26512;&#21644;&#27169;&#25311;&#25968;&#25454;&#32467;&#21512;&#30340;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Difficulty Modelling in Mobile Puzzle Games: An Empirical Study on Different Methods to Combine Player Analytics and Simulated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#26041;&#27861;&#32467;&#21512;&#29609;&#23478;&#20998;&#26512;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#31227;&#21160;&#25340;&#22270;&#28216;&#25103;&#20013;&#38590;&#24230;&#30340;&#20934;&#30830;&#20272;&#31639;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#22522;&#20110;&#38431;&#21015;&#32479;&#35745;&#21644;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#33021;&#22815;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#20135;&#29983;&#26368;&#20934;&#30830;&#30340;&#20272;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38590;&#24230;&#26159;&#29609;&#23478;&#21442;&#19982;&#24230;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#20043;&#19968;&#65292;&#21516;&#26102;&#20063;&#26159;&#35774;&#35745;&#24072;&#20248;&#21270;&#29609;&#23478;&#20307;&#39564;&#30340;&#37325;&#35201;&#26041;&#38754;&#20043;&#19968;&#65307;&#22240;&#27492;&#65292;&#23545;&#28216;&#25103;&#24320;&#21457;&#24037;&#20316;&#23460;&#26469;&#35828;&#65292;&#38590;&#24230;&#25805;&#20316;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#26159;&#26681;&#25454;&#29609;&#23478;&#19982;&#20869;&#23481;&#30340;&#20114;&#21160;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#21019;&#24314;&#24230;&#37327;&#26631;&#20934;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#20801;&#35768;&#22312;&#20869;&#23481;&#21457;&#24067;&#21518;&#36827;&#34892;&#20272;&#31639;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#28508;&#22312;&#26410;&#26469;&#29609;&#23478;&#30340;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#36825;&#31181;&#26465;&#20214;&#19979;&#20272;&#31639;&#38590;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#26088;&#22312;&#20102;&#35299;&#19981;&#21516;&#22330;&#26223;&#19979;&#21738;&#31181;&#26041;&#27861;&#21644;&#21738;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#34920;&#29616;&#26356;&#22909;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#22330;&#26223;&#20013;&#65292;&#22522;&#20110;&#38431;&#21015;&#32479;&#35745;&#21644;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#26368;&#20934;&#30830;&#30340;&#38590;&#24230;&#20272;&#31639;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#26368;&#20026;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Difficulty is one of the key drivers of player engagement and it is often one of the aspects that designers tweak most to optimise the player experience; operationalising it is, therefore, a crucial task for game development studios. A common practice consists of creating metrics out of data collected by player interactions with the content; however, this allows for estimation only after the content is released and does not consider the characteristics of potential future players.   In this article, we present a number of potential solutions for the estimation of difficulty under such conditions, and we showcase the results of a comparative study intended to understand which method and which types of data perform better in different scenarios.   The results reveal that models trained on a combination of cohort statistics and simulated data produce the most accurate estimations of difficulty in all scenarios. Furthermore, among these models, artificial neural networks show the most cons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#23458;&#39532;&#25289;&#26494;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#22312;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.17434</link><description>&lt;p&gt;
&#22312;&#40657;&#23458;&#39532;&#25289;&#26494;&#20013;&#38598;&#25104;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;: &#26426;&#36935;&#65292;&#25361;&#25112;&#21644;&#25945;&#32946;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17434
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#23458;&#39532;&#25289;&#26494;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#22312;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#23458;&#39532;&#25289;&#26494;&#21644;&#36719;&#20214;&#31454;&#36187;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#23427;&#20204;&#23545;&#32452;&#32455;&#21644;&#23398;&#29983;&#30340;&#21019;&#26032;&#21644;&#25216;&#33021;&#21457;&#23637;&#36215;&#21040;&#37325;&#35201;&#25512;&#21160;&#20316;&#29992;&#12290;&#36825;&#20123;&#24179;&#21488;&#20351;&#20844;&#21496;&#33021;&#22815;&#36805;&#36895;&#21407;&#22411;&#21270;&#24819;&#27861;&#65292;&#32780;&#23398;&#29983;&#21017;&#33719;&#24471;&#20016;&#23500;&#30340;&#23398;&#20064;&#32463;&#39564;&#65292;&#22686;&#24378;&#20182;&#20204;&#30340;&#23454;&#36341;&#25216;&#33021;&#12290;&#22810;&#24180;&#26469;&#65292;&#40657;&#23458;&#39532;&#25289;&#26494;&#24050;&#32463;&#20174;&#31616;&#21333;&#30340;&#31454;&#20105;&#27963;&#21160;&#36716;&#21464;&#20026;&#37325;&#35201;&#30340;&#25945;&#32946;&#24037;&#20855;&#65292;&#23558;&#29702;&#35770;&#30693;&#35782;&#19982;&#23454;&#38469;&#38382;&#39064;&#35299;&#20915;&#30456;&#32467;&#21512;&#12290;&#23558;&#40657;&#23458;&#39532;&#25289;&#26494;&#32435;&#20837;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#36719;&#20214;&#24037;&#31243;&#35838;&#31243;&#30340;&#25972;&#21512;&#26088;&#22312;&#22312;&#21512;&#20316;&#30340;&#29615;&#22659;&#20013;&#23545;&#40784;&#25945;&#32946;&#33021;&#21147;&#65292;&#36890;&#36807;&#20135;&#23398;&#21512;&#20316;&#20419;&#36827;&#21516;&#34892;&#20043;&#38388;&#30340;&#36830;&#25509;&#21644;&#20016;&#23500;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#39640;&#32423;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#34701;&#21512;&#36827;&#40657;&#23458;&#39532;&#25289;&#26494;&#27491;&#22312;&#25913;&#21464;&#23427;&#20204;&#30340;&#32467;&#26500;&#21644;&#32467;&#26524;&#12290;&#36825;&#31181;&#28436;&#21464;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#22914;&#22686;&#24378;&#30340;&#23398;&#20064;&#20307;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Hackathons and software competitions, increasingly pivotal in the software industry, serve as vital catalysts for innovation and skill development for both organizations and students. These platforms enable companies to prototype ideas swiftly, while students gain enriched learning experiences, enhancing their practical skills. Over the years, hackathons have transitioned from mere competitive events to significant educational tools, fusing theoretical knowledge with real-world problem-solving. The integration of hackathons into computer science and software engineering curricula aims to align educational proficiencies within a collaborative context, promoting peer connectivity and enriched learning via industry-academia collaborations. However, the infusion of advanced technologies, notably artificial intelligence (AI), and machine learning, into hackathons is revolutionizing their structure and outcomes. This evolution brings forth both opportunities, like enhanced learning experienc
&lt;/p&gt;</description></item><item><title>&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#21333;&#22836;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#22823;&#23884;&#20837;&#32500;&#24230;&#24773;&#20917;&#19979;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#35774;&#32622;&#19979;&#37117;&#26174;&#31034;&#20986;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2401.17426</link><description>&lt;p&gt;
&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Superiority of Multi-Head Attention in In-Context Linear Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17426
&lt;/p&gt;
&lt;p&gt;
&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#21333;&#22836;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#22823;&#23884;&#20837;&#32500;&#24230;&#24773;&#20917;&#19979;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#35774;&#32622;&#19979;&#37117;&#26174;&#31034;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;softmax&#27880;&#24847;&#21147;&#30340;transformer&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#21333;&#22836;/&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#25910;&#25947;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#37325;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#36739;&#22823;&#23884;&#20837;&#32500;&#24230;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#27604;&#21333;&#22836;&#27880;&#24847;&#21147;&#34920;&#29616;&#26356;&#22909;&#12290;&#24403;&#19978;&#19979;&#25991;&#31034;&#20363;&#25968;&#37327;D&#22686;&#21152;&#26102;&#65292;&#20351;&#29992;&#21333;&#22836;/&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#39044;&#27979;&#25439;&#22833;&#20026;O(1/D)&#65292;&#32780;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#20056;&#27861;&#24120;&#25968;&#36739;&#23567;&#12290;&#38500;&#20102;&#26368;&#31616;&#21333;&#30340;&#25968;&#25454;&#20998;&#24067;&#35774;&#32622;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26356;&#22810;&#24773;&#26223;&#65292;&#20363;&#22914;&#22122;&#22768;&#26631;&#31614;&#65292;&#23616;&#37096;&#31034;&#20363;&#65292;&#30456;&#20851;&#29305;&#24449;&#21644;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24635;&#30340;&#26469;&#35828;&#65292;&#22810;&#22836;&#27880;&#24847;&#21147;&#20248;&#20110;&#21333;&#22836;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. When the number of in-context examples D increases, the prediction loss using single-/multi-head attention is in O(1/D), and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head at
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#65292;&#22522;&#20110;&#22810;&#33021;&#37327;&#20013;&#24494;&#23376;&#27668;&#20307;&#20013;&#20013;&#24494;&#23376;&#35282;&#20998;&#24067;&#30340;&#21069;&#20004;&#20010;&#30697;&#65292;&#39044;&#27979;&#20102;&#24555;&#36895; flavor &#36716;&#25442;&#30340;&#32467;&#26524;&#65292;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2401.17424</link><description>&lt;p&gt;
&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#24555;&#36895;&#20013;&#24494;&#23376;&#25442; flavor &#21518;&#30340;&#33021;&#35889;
&lt;/p&gt;
&lt;p&gt;
Application of Neural Networks for the Reconstruction of Supernova Neutrino Energy Spectra Following Fast Neutrino Flavor Conversions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#65292;&#22522;&#20110;&#22810;&#33021;&#37327;&#20013;&#24494;&#23376;&#27668;&#20307;&#20013;&#20013;&#24494;&#23376;&#35282;&#20998;&#24067;&#30340;&#21069;&#20004;&#20010;&#30697;&#65292;&#39044;&#27979;&#20102;&#24555;&#36895; flavor &#36716;&#25442;&#30340;&#32467;&#26524;&#65292;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24494;&#23376;&#22312;&#26497;&#23494;&#24230;&#30340;&#22825;&#20307;&#29615;&#22659;&#20013;&#65292;&#22914;&#26680;&#24515;&#22349;&#32553;&#36229;&#26032;&#26143;&#21644;&#20013;&#23376;&#26143;&#21512;&#24182;&#20013;&#65292;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#30340; flavor &#36716;&#25442;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#33021;&#37327;&#20013;&#24494;&#23376;&#27668;&#20307;&#20013;&#30340;&#24555;&#36895; flavor &#36716;&#25442;&#65292;&#21457;&#29616;&#24403;&#24555;&#36895; flavor &#36716;&#25442;&#30340;&#22686;&#38271;&#29575;&#26174;&#33879;&#36229;&#36807;&#30495;&#31354;&#21704;&#23494;&#39039;&#37327;&#30340;&#22686;&#38271;&#29575;&#26102;&#65292;&#25152;&#26377;&#20013;&#24494;&#23376;&#65288;&#26080;&#35770;&#20854;&#33021;&#37327;&#22914;&#20309;&#65289;&#37117;&#20849;&#20139;&#19968;&#20010;&#30001;&#33021;&#37327;&#31215;&#20998;&#20013;&#24494;&#23376;&#35889;&#20915;&#23450;&#30340;&#23384;&#27963;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26469;&#39044;&#27979;&#22312;&#36825;&#31181;&#22810;&#33021;&#37327;&#20013;&#24494;&#23376;&#27668;&#20307;&#20013;&#24555;&#36895; flavor &#36716;&#25442;&#30340;&#28176;&#36817;&#32467;&#26524;&#12290;&#36825;&#20123;&#39044;&#27979;&#22522;&#20110;&#27599;&#20010;&#33021;&#37327; bin &#30340;&#20013;&#24494;&#23376;&#35282;&#20998;&#24067;&#30340;&#21069;&#20004;&#20010;&#30697;&#65292;&#36890;&#24120;&#21487;&#20197;&#20174;&#26368;&#20808;&#36827;&#30340;&#36229;&#26032;&#26143;&#21644;&#20013;&#23376;&#26143;&#27169;&#25311;&#20013;&#33719;&#24471;&#12290;&#25105;&#20204;&#30340; PINNs &#23545;&#20110;&#39044;&#27979;&#30005;&#23376;&#36890;&#36947;&#20013;&#30340;&#20013;&#24494;&#23376;&#25968;&#37327;&#21644;&#20013;&#24494;&#23376;&#30697;&#30340;&#30456;&#23545;&#32477;&#23545;&#35823;&#24046;&#20998;&#21035;&#36798;&#21040;&#19981;&#21040;6&#65285;&#21644;&#19981;&#21040;18&#65285;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neutrinos can undergo fast flavor conversions (FFCs) within extremely dense astrophysical environments such as core-collapse supernovae (CCSNe) and neutron star mergers (NSMs). In this study, we explore FFCs in a \emph{multi-energy} neutrino gas, revealing that when the FFC growth rate significantly exceeds that of the vacuum Hamiltonian, all neutrinos (regardless of energy) share a common survival probability dictated by the energy-integrated neutrino spectrum. We then employ physics-informed neural networks (PINNs) to predict the asymptotic outcomes of FFCs within such a multi-energy neutrino gas. These predictions are based on the first two moments of neutrino angular distributions for each energy bin, typically available in state-of-the-art CCSN and NSM simulations. Our PINNs achieve errors as low as $\lesssim6\%$ and $\lesssim 18\%$ for predicting the number of neutrinos in the electron channel and the relative absolute error in the neutrino moments, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17417</link><description>&lt;p&gt;
&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#31359;&#22681;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Through-Wall Imaging based on WiFi Channel State Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#22312;&#31359;&#22681;&#22330;&#26223;&#20013;&#21512;&#25104;&#22270;&#20687;&#12290;&#21033;&#29992;WiFi&#30340;&#20248;&#21183;&#65292;&#22914;&#25104;&#26412;&#25928;&#30410;&#65292;&#20809;&#29031;&#19981;&#21464;&#24615;&#21644;&#31359;&#22681;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#29615;&#22659;&#30340;&#21487;&#35270;&#21270;&#30417;&#27979;&#65292;&#36234;&#36807;&#25151;&#38388;&#36793;&#30028;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#23427;&#36890;&#36807;&#35299;&#38145;&#25191;&#34892;&#22522;&#20110;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;&#27963;&#21160;&#35782;&#21035;&#65289;&#30340;&#36873;&#39033;&#65292;&#25552;&#39640;&#20102;WiFi CSI&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#20174;WiFi CSI&#21040;&#22270;&#20687;&#30340;&#36328;&#27169;&#24577;&#36716;&#25442;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#36866;&#24212;&#25105;&#20204;&#38382;&#39064;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#26550;&#26500;&#37197;&#32622;&#30340;&#21076;&#38500;&#30740;&#31350;&#21644;&#37325;&#24314;&#22270;&#20687;&#30340;&#23450;&#37327;/&#23450;&#24615;&#35780;&#20272;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a seminal approach for synthesizing images from WiFi Channel State Information (CSI) in through-wall scenarios. Leveraging the strengths of WiFi, such as cost-effectiveness, illumination invariance, and wall-penetrating capabilities, our approach enables visual monitoring of indoor environments beyond room boundaries and without the need for cameras. More generally, it improves the interpretability of WiFi CSI by unlocking the option to perform image-based downstream tasks, e.g., visual activity recognition. In order to achieve this crossmodal translation from WiFi CSI to images, we rely on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics. We extensively evaluate our proposed methodology through an ablation study on architecture configuration and a quantitative/qualitative assessment of reconstructed images. Our results demonstrate the viability of our method and highlight its potential for practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#27493;&#38271;&#20248;&#21270;&#38382;&#39064;&#65292;&#25351;&#20986;&#20256;&#32479;&#31639;&#27861;&#24573;&#35270;&#20102;&#23545;&#25972;&#20307;&#30446;&#26631;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#32780;&#38543;&#26426;&#20803;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#33021;&#22815;&#26126;&#30830;&#20248;&#21270;&#27493;&#38271;&#21521;&#37327;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>https://arxiv.org/abs/2401.17401</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#27493;&#38271;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Step-size Optimization for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#27493;&#38271;&#20248;&#21270;&#38382;&#39064;&#65292;&#25351;&#20986;&#20256;&#32479;&#31639;&#27861;&#24573;&#35270;&#20102;&#23545;&#25972;&#20307;&#30446;&#26631;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#32780;&#38543;&#26426;&#20803;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#33021;&#22815;&#26126;&#30830;&#20248;&#21270;&#27493;&#38271;&#21521;&#37327;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#38656;&#35201;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#19981;&#26029;&#23398;&#20064;&#25968;&#25454;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20915;&#23450;&#35201;&#20445;&#30041;&#20160;&#20040;&#30693;&#35782;&#21644;&#25918;&#24323;&#20160;&#20040;&#30693;&#35782;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#27493;&#38271;&#21521;&#37327;&#26469;&#32553;&#25918;&#26799;&#24230;&#26679;&#26412;&#23545;&#32593;&#32476;&#26435;&#37325;&#30340;&#25913;&#21464;&#31243;&#24230;&#26469;&#23454;&#29616;&#12290;&#24120;&#35265;&#30340;&#31639;&#27861;&#65292;&#22914;RMSProp&#21644;Adam&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26631;&#20934;&#21270;&#65292;&#26469;&#36866;&#24212;&#36825;&#20010;&#27493;&#38271;&#21521;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#24573;&#35270;&#20102;&#23427;&#20204;&#23545;&#25972;&#20307;&#30446;&#26631;&#20989;&#25968;&#30340;&#36866;&#24212;&#25928;&#26524;&#65292;&#20363;&#22914;&#23558;&#27493;&#38271;&#21521;&#37327;&#36828;&#31163;&#26356;&#22909;&#30340;&#27493;&#38271;&#21521;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;IDBD&#65288;Sutton&#65292;1992&#65289;&#36825;&#26679;&#30340;&#38543;&#26426;&#20803;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#26126;&#30830;&#22320;&#38024;&#23545;&#25972;&#20307;&#30446;&#26631;&#20989;&#25968;&#20248;&#21270;&#27493;&#38271;&#21521;&#37327;&#12290;&#22312;&#31616;&#21333;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IDBD&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#27493;&#38271;&#21521;&#37327;&#65292;&#32780;RMSProp&#21644;Adam&#21017;&#19981;&#34892;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#23616;&#38480;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitat
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#22303;&#32819;&#20854;&#35821;&#22522;&#20934;&#27979;&#35797;&#65292;&#25104;&#21151;&#22320;&#23545;&#21517;&#20026;BERTurk&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2401.17396</link><description>&lt;p&gt;
&#23545;&#22303;&#32819;&#20854;&#35821;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17396
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#22303;&#32819;&#20854;&#35821;&#22522;&#20934;&#27979;&#35797;&#65292;&#25104;&#21151;&#22320;&#23545;&#21517;&#20026;BERTurk&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#29702;&#35299;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#26368;&#36817;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#30001;&#20110;&#20854;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#24494;&#35843;&#29305;&#24615;&#65292;&#23427;&#20204;&#24050;&#32463;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22914;BERT&#65288;&#21452;&#21521;Transformer&#32534;&#30721;&#22120;&#34920;&#31034;&#65289;&#65292;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#36825;&#20123;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#23558;&#39044;&#20808;&#26500;&#24314;&#22909;&#30340;&#27169;&#22411;&#36716;&#31227;&#24182;&#38024;&#23545;&#29305;&#23450;&#30340;NLU&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#22914;&#38382;&#31572;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20026;&#22303;&#32819;&#20854;&#35821;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23545;&#19968;&#31181;&#21517;&#20026;BERTurk&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#26159;&#20351;&#29992;&#22522;&#26412;&#35774;&#32622;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based and lately Transformer-based language models have been dominating the studies of natural language processing in the last years. Thanks to their accurate and fast fine-tuning characteristics, they have outperformed traditional machine learning-based approaches and achieved state-of-the-art results for many challenging natural language understanding (NLU) problems. Recent studies showed that the Transformer-based models such as BERT, which is Bidirectional Encoder Representations from Transformers, have reached impressive achievements on many tasks. Moreover, thanks to their transfer learning capacity, these architectures allow us to transfer pre-built models and fine-tune them to specific NLU tasks such as question answering. In this study, we provide a Transformer-based model and a baseline benchmark for the Turkish Language. We successfully fine-tuned a Turkish BERT model, namely BERTurk that is trained with base settings, to many downstream tasks and evaluated wit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#22238;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#23398;&#20250;&#22914;&#20309;&#22238;&#36991;&#36127;&#38754;&#29305;&#24449;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2401.17390</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Customizing Language Model Responses with Contrastive In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#22238;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#23398;&#20250;&#22914;&#20309;&#22238;&#36991;&#36127;&#38754;&#29305;&#24449;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#19982;&#25105;&#20204;&#30340;&#24847;&#22270;&#23545;&#40784;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#24076;&#26395;&#29983;&#25104;&#20248;&#20110;&#20854;&#20182;&#20869;&#23481;&#30340;&#20869;&#23481;&#65292;&#25110;&#32773;&#24403;&#25105;&#20204;&#24076;&#26395;LLMs&#20197;&#19968;&#31181;&#38590;&#20197;&#25551;&#36848;&#30340;&#39118;&#26684;&#25110;&#35821;&#27668;&#36827;&#34892;&#22238;&#24212;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#26356;&#22909;&#22320;&#25551;&#36848;&#25105;&#20204;&#30340;&#24847;&#22270;&#30340;&#26041;&#27861;&#12290;&#36825;&#28041;&#21450;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#26469;&#35828;&#26126;&#30495;&#23454;&#30340;&#24847;&#22270;&#65292;&#20197;&#21450;&#36127;&#38754;&#31034;&#20363;&#26469;&#23637;&#31034;&#25105;&#20204;&#24076;&#26395;LLMs&#36991;&#20813;&#30340;&#29305;&#24449;&#12290;&#36127;&#38754;&#31034;&#20363;&#21487;&#20197;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#26816;&#32034;&#65292;&#30001;&#20154;&#24037;&#32534;&#20889;&#65292;&#25110;&#30001;LLMs&#33258;&#21160;&#29983;&#25104;&#12290;&#22312;&#29983;&#25104;&#31572;&#26696;&#20043;&#21069;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#31034;&#20363;&#65292;&#20197;&#25945;&#20250;&#33258;&#24049;&#36991;&#20813;&#20160;&#20040;&#12290;&#36825;&#20010;&#25512;&#29702;&#27493;&#39588;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#29992;&#25143;&#38656;&#27714;&#30456;&#20851;&#30340;&#36866;&#24403;&#34920;&#36798;&#65292;&#24182;&#24341;&#23548;&#20854;&#29983;&#25104;&#26356;&#22909;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#25512;&#29305;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;&#30340;&#21152;&#26435;&#38598;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;BERT&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#38463;&#25289;&#20271;&#26041;&#35328;&#30340;&#31934;&#30830;&#20998;&#31867;&#65292;&#20026;&#29702;&#35299;&#29992;&#25143;&#35266;&#28857;&#21644;&#24577;&#24230;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2401.17373</link><description>&lt;p&gt;
&#38463;&#25289;&#20271;&#25512;&#25991;&#34892;&#20026;&#65306;&#22522;&#20110;&#21152;&#26435;&#38598;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#38463;&#25289;&#20271;&#35821;&#25512;&#29305;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for Classifying Arabic Speech Acts on Twitter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#25512;&#29305;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;&#30340;&#21152;&#26435;&#38598;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;BERT&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#38463;&#25289;&#20271;&#26041;&#35328;&#30340;&#31934;&#30830;&#20998;&#31867;&#65292;&#20026;&#29702;&#35299;&#29992;&#25143;&#35266;&#28857;&#21644;&#24577;&#24230;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#34892;&#20026;&#26159;&#35828;&#35805;&#32773;&#22312;&#23545;&#35805;&#20013;&#34920;&#36798;&#24847;&#24605;&#26102;&#30340;&#34892;&#20026;&#65292;&#20363;&#22914;&#35810;&#38382;&#12289;&#25512;&#33616;&#12289;&#38382;&#20505;&#12289;&#36947;&#35874;&#12289;&#34920;&#36798;&#24819;&#27861;&#25110;&#25552;&#20986;&#24314;&#35758;&#12290;&#29702;&#35299;&#35821;&#35328;&#34892;&#20026;&#26377;&#21161;&#20110;&#35299;&#37322;&#35828;&#35805;&#32773;&#25110;&#20316;&#32773;&#35328;&#35821;&#32972;&#21518;&#30340;&#24847;&#22270;&#21644;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#38463;&#25289;&#20271;&#35821;&#25512;&#29305;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;&#26041;&#27861;&#12290;&#25512;&#29305;&#21644;&#31038;&#20132;&#23186;&#20307;&#36234;&#26469;&#36234;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24050;&#32463;&#28436;&#21464;&#25104;&#20102;&#34920;&#36798;&#29992;&#25143;&#35266;&#28857;&#21644;&#24577;&#24230;&#30340;&#37325;&#35201;&#20449;&#24687;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#21152;&#26435;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25972;&#21512;&#26041;&#35328;&#38463;&#25289;&#20271;&#35821;&#35821;&#35328;&#34892;&#20026;&#20998;&#31867;&#20013;&#21508;&#31181;BERT&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#20960;&#20010;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26041;&#35328;&#38463;&#25289;&#20271;&#25512;&#29305;&#34892;&#20026;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech acts are a speakers actions when performing an utterance within a conversation, such as asking, recommending, greeting, or thanking someone, expressing a thought, or making a suggestion. Understanding speech acts helps interpret the intended meaning and actions behind a speakers or writers words. This paper proposes a Twitter dialectal Arabic speech act classification approach based on a transformer deep learning neural network. Twitter and social media, are becoming more and more integrated into daily life. As a result, they have evolved into a vital source of information that represents the views and attitudes of their users. We proposed a BERT based weighted ensemble learning approach to integrate the advantages of various BERT models in dialectal Arabic speech acts classification. We compared the proposed model against several variants of Arabic BERT models and sequence-based models. We developed a dialectal Arabic tweet act dataset by annotating a subset of a large existing
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#21644;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;</title><link>https://arxiv.org/abs/2401.17350</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#20248;&#21270;&#26102;&#38388;&#24207;&#21015;&#20379;&#24212;&#21830;&#20998;&#37197;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17350
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#21644;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;BL&#27169;&#22411;&#21644;Perspective&#30697;&#38453;&#65292;&#20197;&#20248;&#21270;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#37325;&#28857;&#20851;&#27880;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#20379;&#24212;&#21830;&#20851;&#31995;&#32593;&#32476;&#65292;&#22686;&#24378;&#20102;&#23545;&#22797;&#26434;&#20379;&#24212;&#21830;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;Masked Ranking&#26426;&#21046;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20379;&#24212;&#21830;&#25490;&#24207;&#25928;&#29575;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;DBLM&#22312;&#25552;&#20379;&#20934;&#30830;&#39044;&#27979;&#21644;&#31934;&#30830;&#32622;&#20449;&#21306;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#20998;&#36776;&#29575;&#24773;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the BL model and the Perspective Matrix to optimize supplier selection and order allocation, focusing on both temporal and spatial dynamics. Our development of a Supplier Relationship Network, using a Spatio-Temporal Graph Neural Network, enhances the understanding of complex supplier interdependencies. Additionally, we address credibility issues in zero-order scenarios with a Masked Ranking Mechanism, improving supplier ranking efficiency. Our model demonstrates superior results on two datasets compared to the traditional models. Our evaluations using real-world datasets highlight DBLM's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;YTCommentQA&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;YouTube&#30340;&#33258;&#28982;&#38382;&#39064;&#65292;&#20998;&#31867;&#20854;&#21487;&#22238;&#31572;&#24615;&#21644;&#25152;&#38656;&#27169;&#24577;&#65292;&#20197;&#35299;&#20915;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#38382;&#39064;&#21487;&#22238;&#31572;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17343</link><description>&lt;p&gt;
YTCommentQA: &#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#38382;&#39064;&#21487;&#22238;&#31572;&#24615;
&lt;/p&gt;
&lt;p&gt;
YTCommentQA: Video Question Answerability in Instructional Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;YTCommentQA&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;YouTube&#30340;&#33258;&#28982;&#38382;&#39064;&#65292;&#20998;&#31867;&#20854;&#21487;&#22238;&#31572;&#24615;&#21644;&#25152;&#38656;&#27169;&#24577;&#65292;&#20197;&#35299;&#20915;&#25351;&#23548;&#35270;&#39057;&#20013;&#30340;&#38382;&#39064;&#21487;&#22238;&#31572;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35270;&#39057;&#20026;&#21508;&#31181;&#20219;&#21153;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#22914;&#20309;&#25805;&#20316;&#25351;&#21335;&#65292;&#35266;&#20247;&#36890;&#24120;&#20250;&#23601;&#20869;&#23481;&#25552;&#20986;&#38382;&#39064;&#12290;&#35299;&#31572;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#29702;&#35299;&#20869;&#23481;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#24456;&#38590;&#31435;&#21363;&#33719;&#24471;&#31572;&#26696;&#12290;&#30446;&#21069;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#35270;&#39057;&#38382;&#39064;&#22238;&#31572;&#65288;Video QA&#65289;&#20219;&#21153;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#26159;&#26681;&#25454;&#35270;&#39057;&#20869;&#23481;&#29983;&#25104;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#20869;&#23481;&#20013;&#20135;&#29983;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#33021;&#25552;&#20986;&#36229;&#20986;&#35270;&#39057;&#20449;&#24687;&#36793;&#30028;&#30340;&#38382;&#39064;&#65292;&#36825;&#31361;&#26174;&#20986;&#30830;&#23450;&#35270;&#39057;&#26159;&#21542;&#33021;&#25552;&#20379;&#31572;&#26696;&#30340;&#24517;&#35201;&#24615;&#12290;&#30001;&#20110;&#35270;&#39057;&#20855;&#26377;&#22810;&#27169;&#24577;&#24615;&#65292;&#35270;&#35273;&#21644;&#21475;&#22836;&#20449;&#24687;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#22240;&#27492;&#21028;&#26029;&#19968;&#20010;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#22238;&#31572;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;YTCommentQA&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;YouTube&#29983;&#25104;&#30340;&#33258;&#28982;&#38382;&#39064;&#65292;&#26681;&#25454;&#20854;&#21487;&#22238;&#31572;&#24615;&#21644;&#25152;&#38656;&#27169;&#24577;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instructional videos provide detailed how-to guides for various tasks, with viewers often posing questions regarding the content. Addressing these questions is vital for comprehending the content, yet receiving immediate answers is difficult. While numerous computational models have been developed for Video Question Answering (Video QA) tasks, they are primarily trained on questions generated based on video content, aiming to produce answers from within the content. However, in real-world situations, users may pose questions that go beyond the video's informational boundaries, highlighting the necessity to determine if a video can provide the answer. Discerning whether a question can be answered by video content is challenging due to the multi-modal nature of videos, where visual and verbal information are intertwined. To bridge this gap, we present the YTCommentQA dataset, which contains naturally-generated questions from YouTube, categorized by their answerability and required modali
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;&#34442;&#23376;&#31181;&#32676;&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;&#22320;&#21306;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17342</link><description>&lt;p&gt;
&#25552;&#39640;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17342
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;&#34442;&#23376;&#31181;&#32676;&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;&#22320;&#21306;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#37325;&#28857;&#20851;&#27880;&#34442;&#23376;&#31181;&#32676;&#65288;MA&#65289;&#20272;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;EO&#25968;&#25454;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#24314;&#31435;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;MA&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#65288;AE&#65289;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20102;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;EO&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#22320;&#21306;&#21463;&#34442;&#23376;&#31181;&#32676;&#30340;&#24433;&#21709;&#26174;&#33879;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#21457;&#29616;&#26159;MA&#39044;&#27979;&#30340;AE&#19982;&#25152;&#25552;&#20986;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#20043;&#38388;&#23384;&#22312;0.46&#30340;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#36825;&#20010;&#30456;&#20851;&#24615;&#24847;&#21619;&#30528;&#36825;&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#12289;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;AI&#27169;&#22411;&#22312;&#35813;&#32972;&#26223;&#19979;&#30340;&#39044;&#27979;&#21487;&#38752;&#24615;&#21644;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a new approach for estimating confidence in machine learning model predictions, specifically in regression tasks utilizing Earth Observation (EO) data, with a particular focus on mosquito abundance (MA) estimation. We take advantage of a Variational AutoEncoder architecture, to derive a confidence metric by the latent space representations of EO datasets. This methodology is pivotal in establishing a correlation between the Euclidean distance in latent representations and the Absolute Error (AE) in individual MA predictions. Our research focuses on EO datasets from the Veneto region in Italy and the Upper Rhine Valley in Germany, targeting areas significantly affected by mosquito populations. A key finding is a notable correlation of 0.46 between the AE of MA predictions and the proposed confidence metric. This correlation signifies a robust, new metric for quantifying the reliability and enhancing the trustworthiness of the AI model's predictions in the context of 
&lt;/p&gt;</description></item><item><title>&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#20801;&#35768;&#20445;&#25252;&#38544;&#31169;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#65292;&#35813;&#32508;&#36848;&#23545;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23041;&#32961;&#12289;&#23545;&#25163;&#21644;&#38450;&#24481;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2401.17319</link><description>&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65306;&#23433;&#20840;&#19982;&#38544;&#31169;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: A Survey on Security and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17319
&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#20801;&#35768;&#20445;&#25252;&#38544;&#31169;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#65292;&#35813;&#32508;&#36848;&#23545;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23041;&#32961;&#12289;&#23545;&#25163;&#21644;&#38450;&#24481;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#31561;&#20248;&#21183;&#65292;&#22312;&#36817;&#24180;&#26469;&#36805;&#36895;&#21457;&#23637;&#24182;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26550;&#26500;&#20013;&#65292;&#27169;&#22411;&#26356;&#26032;&#21644;&#26799;&#24230;&#30340;&#20132;&#25442;&#20026;&#32593;&#32476;&#20013;&#30340;&#24694;&#24847;&#29992;&#25143;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#21487;&#33021;&#21361;&#21450;&#27169;&#22411;&#24615;&#33021;&#20197;&#21450;&#29992;&#25143;&#21644;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#26159;&#36890;&#36807;&#21435;&#38500;&#26381;&#21153;&#22120;&#24182;&#36890;&#36807;&#21306;&#22359;&#38142;&#31561;&#25216;&#26415;&#36827;&#34892;&#34917;&#20607;&#26469;&#28040;&#38500;&#19982;&#26381;&#21153;&#22120;&#30456;&#20851;&#30340;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20248;&#21183;&#21364;&#20197;&#25361;&#25112;&#31995;&#32479;&#38754;&#20020;&#26032;&#30340;&#38544;&#31169;&#23041;&#32961;&#20026;&#20195;&#20215;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#31181;&#26032;&#33539; paradigm&#65292;&#24182;&#36827;&#34892;&#20840;&#38754;&#30340;&#23433;&#20840;&#20998;&#26512;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#23041;&#32961;&#21644;&#23545;&#25163;&#21464;&#21270;&#65292;&#24182;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#36824;&#32771;&#34385;&#20102;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#20449;&#24230;&#21644;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has been rapidly evolving and gaining popularity in recent years due to its privacy-preserving features, among other advantages. Nevertheless, the exchange of model updates and gradients in this architecture provides new attack surfaces for malicious users of the network which may jeopardize the model performance and user and data privacy. For this reason, one of the main motivations for decentralized federated learning is to eliminate server-related threats by removing the server from the network and compensating for it through technologies such as blockchain. However, this advantage comes at the cost of challenging the system with new privacy threats. Thus, performing a thorough security analysis in this new paradigm is necessary. This survey studies possible variations of threats and adversaries in decentralized federated learning and overviews the potential defense mechanisms. Trustability and verifiability of decentralized federated learning are also considered 
&lt;/p&gt;</description></item><item><title>HEQuant&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21516;&#24577;&#21152;&#23494;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#31169;&#26377;&#25512;&#26029;&#12290;&#36890;&#36807;&#20302;&#31934;&#24230;&#37327;&#21270;&#24863;&#30693;&#20248;&#21270;&#21644;&#19968;&#31995;&#21015;&#20854;&#20182;&#20248;&#21270;&#25514;&#26045;&#65292;HEQuant&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;HE&#21327;&#35758;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#25968;&#25454;&#20256;&#36755;&#30340;&#25968;&#37327;&#21644;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.15970</link><description>&lt;p&gt;
HEQuant: &#32467;&#21512;&#21516;&#24577;&#21152;&#23494;&#21644;&#37327;&#21270;&#20197;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#31169;&#26377;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
HEQuant: Marrying Homomorphic Encryption and Quantization for Communication-Efficient Private Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15970
&lt;/p&gt;
&lt;p&gt;
HEQuant&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21516;&#24577;&#21152;&#23494;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#31169;&#26377;&#25512;&#26029;&#12290;&#36890;&#36807;&#20302;&#31934;&#24230;&#37327;&#21270;&#24863;&#30693;&#20248;&#21270;&#21644;&#19968;&#31995;&#21015;&#20854;&#20182;&#20248;&#21270;&#25514;&#26045;&#65292;HEQuant&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;HE&#21327;&#35758;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#25968;&#25454;&#20256;&#36755;&#30340;&#25968;&#37327;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#30340;&#23433;&#20840;&#21452;&#26041;&#35745;&#31639;&#20351;&#29992;&#24418;&#24335;&#21270;&#30340;&#23433;&#20840;&#20445;&#35777;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20294;&#36890;&#20449;&#24320;&#38144;&#36739;&#39640;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#22914;Cheetah&#12289;Iron&#31561;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25805;&#20316;&#30340;&#39640;&#25928;HE&#21327;&#35758;&#65292;&#20294;&#23427;&#20204;&#20173;&#20551;&#35774;NN&#25805;&#20316;&#38656;&#35201;&#39640;&#31934;&#24230;&#65292;&#20363;&#22914;&#22266;&#23450;&#28857;37&#20301;&#65292;&#24182;&#24573;&#35270;&#20102;NN&#23545;&#37327;&#21270;&#35823;&#24046;&#30340;&#22266;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HEQuant&#65292;&#23427;&#20855;&#26377;&#38024;&#23545;HE&#21327;&#35758;&#30340;&#20302;&#31934;&#24230;&#37327;&#21270;&#24863;&#30693;&#20248;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20301;&#31934;&#24230;&#38477;&#20302;&#26102;&#65292;&#37327;&#21270;&#21644;HE&#30340;&#31616;&#21333;&#32452;&#21512;&#22909;&#22788;&#36805;&#36895;&#39281;&#21644;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20248;&#21270;&#25514;&#26045;&#65292;&#21253;&#25324;&#19968;&#31181;&#20869;&#37096;&#31995;&#25968;&#25171;&#21253;&#31639;&#27861;&#21644;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#29926;&#29255;&#31639;&#27861;&#65292;&#20197;&#21516;&#26102;&#20943;&#23569;&#20256;&#36755;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#31934;&#24230;&#12290;&#19982;&#20808;&#21069;&#30340;HE&#21327;&#35758;&#65288;&#22914;CrypTFlow2&#65289;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
Secure two-party computation with homomorphic encryption (HE) protects data privacy with a formal security guarantee but suffers from high communication overhead. While previous works, e.g., Cheetah, Iron, etc, have proposed efficient HE-based protocols for different neural network (NN) operations, they still assume high precision, e.g., fixed point 37 bit, for the NN operations and ignore NNs' native robustness against quantization error. In this paper, we propose HEQuant, which features low-precision-quantization-aware optimization for the HE-based protocols. We observe the benefit of a naive combination of quantization and HE quickly saturates as bit precision goes down. Hence, to further improve communication efficiency, we propose a series of optimizations, including an intra-coefficient packing algorithm and a quantization-aware tiling algorithm, to simultaneously reduce the number and precision of the transferred data. Compared with prior-art HE-based protocols, e.g., CrypTFlow2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.15496</link><description>&lt;p&gt;
Baichuan2-Sum: &#20351;&#29992;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;Llama&#12289;Baichuan&#21644;Bloom&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#30340;&#19981;&#21516;&#35282;&#33394;&#29983;&#25104;&#25688;&#35201;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#23567;&#27169;&#22411;&#65288;&#20363;&#22914;Bart&#21644;Bert&#65289;&#36827;&#34892;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#22312;&#23567;&#27169;&#22411;&#19978;&#28155;&#21152;&#20219;&#21153;&#25351;&#23450;&#30340;&#20248;&#21270;&#65292;&#22914;&#21521;&#27169;&#22411;&#28155;&#21152;&#20840;&#23616;-&#23616;&#37096;&#20013;&#24515;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#65306;Baichuan2-Sum&#65292;&#29992;&#20110;&#38754;&#21521;&#35282;&#33394;&#30340;&#23545;&#35805;&#25688;&#35201;&#12290;&#36890;&#36807;&#20026;&#19981;&#21516;&#35282;&#33394;&#35774;&#32622;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#27169;&#22411;&#21487;&#20197;&#20174;&#23545;&#35805;&#20132;&#20114;&#20013;&#23398;&#20064;&#24182;&#36755;&#20986;&#26399;&#26395;&#30340;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;NEFTune&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#21512;&#36866;&#30340;&#22122;&#22768;&#20197;&#25552;&#39640;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#20844;&#24320;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;CSDS&#21644;SAMSUM&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36817;&#20284;CERRA&#30340;&#38477;&#23610;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ERA5&#25968;&#25454;&#38598;&#36827;&#34892;&#39118;&#36895;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2401.15469</link><description>&lt;p&gt;
&#39118;&#36895;&#36229;&#20998;&#36776;&#29575;&#19982;&#39564;&#35777;&#65306;&#20174;ERA5&#21040;CERRA&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Wind speed super-resolution and validation: from ERA5 to CERRA via diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36817;&#20284;CERRA&#30340;&#38477;&#23610;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ERA5&#25968;&#25454;&#38598;&#36827;&#34892;&#39118;&#36895;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copernicus&#21306;&#22495;&#20877;&#20998;&#26512;&#25968;&#25454;&#38598;CERRA&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#27431;&#27954;&#21306;&#22495;&#20877;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#21508;&#31181;&#19982;&#27668;&#20505;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#23427;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#22825;&#27668;&#39044;&#25253;&#12289;&#27668;&#20505;&#21464;&#21270;&#30740;&#31350;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#39044;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#12289;&#31354;&#27668;&#36136;&#37327;&#39118;&#38505;&#35780;&#20272;&#20197;&#21450;&#32597;&#35265;&#20107;&#20214;&#30340;&#39044;&#27979;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#33719;&#21462;&#25152;&#38656;&#22806;&#37096;&#25968;&#25454;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#35745;&#31639;&#37327;&#22823;&#65292;CERRA&#30340;&#21487;&#29992;&#24615;&#28382;&#21518;&#20110;&#24403;&#21069;&#26085;&#26399;&#20004;&#24180;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36817;&#20284;CERRA&#30340;&#38477;&#23610;&#24230;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#36793;&#30028;&#26465;&#20214;&#30001;&#20302;&#20998;&#36776;&#29575;ERA5&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;CERRA&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#12290;&#20197;&#24847;&#22823;&#21033;&#21608;&#22260;&#30340;&#39118;&#36895;&#20026;&#37325;&#28857;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#29616;&#26377;CERRA&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolution regional reanalysis dataset for the European domain. In recent years it has shown significant utility across various climate-related tasks, ranging from forecasting and climate change research to renewable energy prediction, resource management, air quality risk assessment, and the forecasting of rare events, among others. Unfortunately, the availability of CERRA is lagging two years behind the current date, due to constraints in acquiring the requisite external data and the intensive computational demands inherent in its generation. As a solution, this paper introduces a novel method using diffusion models to approximate CERRA downscaling in a data-driven manner, without additional informations. By leveraging the lower resolution ERA5 dataset, which provides boundary conditions for CERRA, we approach this as a super-resolution task. Focusing on wind speed around Italy, our model, trained on existing CERRA data,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.07518</link><description>&lt;p&gt;
&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35843;&#26597;&#65306;&#20998;&#31867;&#20307;&#31995;&#12289;&#31995;&#32479;&#32508;&#36848;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#25216;&#26415;&#20998;&#26512;&#25991;&#26412;&#65292;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#12289;&#21830;&#19994;&#21644;&#25945;&#32946;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;NLP&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#25945;&#23398;&#21644;&#23398;&#20064;&#26041;&#38754;&#30340;&#24110;&#21161;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35299;&#20915;&#19982;&#25945;&#32946;&#39046;&#22495;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;NLP&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#20171;&#32461;&#30456;&#20851;&#32972;&#26223;&#24320;&#22987;&#65292;&#28982;&#21518;&#25552;&#20986;&#25945;&#32946;&#39046;&#22495;NLP&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#26681;&#25454;&#19978;&#36848;&#20998;&#31867;&#31995;&#32479;&#35828;&#26126;&#20219;&#21153;&#23450;&#20041;&#12289;&#25361;&#25112;&#21644;&#30456;&#24212;&#30340;&#25216;&#26415;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#29616;&#26377;&#28436;&#31034;&#65292;&#24182;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) aims to analyze the text via techniques in the computer science field. It serves the applications in healthcare, commerce, and education domains. Particularly, NLP has been applied to the education domain to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems related to the education domain. In detail, we begin with introducing the relevant background. Then, we present the taxonomy of NLP in the education domain. Next, we illustrate the task definition, challenges, and corresponding techniques based on the above taxonomy. After that, we showcase some off-the-shelf demonstrations in this domain and conclude with future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20462;&#22797;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#29983;&#25104;&#20462;&#22797;&#20195;&#30721;&#65292;&#20294;&#23384;&#22312;&#23433;&#20840;&#25514;&#26045;&#20195;&#30721;&#21644;&#21151;&#33021;&#20195;&#30721;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.07031</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20462;&#22797;&#20195;&#30721;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Code Security Vulnerability Repair Using Reinforcement Learning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20462;&#22797;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#29983;&#25104;&#20462;&#22797;&#20195;&#30721;&#65292;&#20294;&#23384;&#22312;&#23433;&#20840;&#25514;&#26045;&#20195;&#30721;&#21644;&#21151;&#33021;&#20195;&#30721;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23545;&#20110;&#21508;&#31181;&#24320;&#21457;&#32773;&#26469;&#35828;&#65292;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#21464;&#24471;&#19981;&#20877;&#37027;&#20040;&#22797;&#26434;&#12290;&#34429;&#28982;&#20351;&#29992;LLMs&#21152;&#36895;&#20102;&#21151;&#33021;&#24320;&#21457;&#36807;&#31243;&#65292;&#20294;&#23545;&#20195;&#30721;&#23433;&#20840;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#39118;&#38505;&#12290;&#22312;&#20351;&#29992;LLMs&#36827;&#34892;&#24102;&#26377;&#23433;&#20840;&#25514;&#26045;&#30340;&#20195;&#30721;&#29983;&#25104;&#26102;&#65292;&#30456;&#36739;&#20110;&#21151;&#33021;&#20195;&#30721;&#29983;&#25104;&#65292;&#20219;&#21153;&#26356;&#20026;&#22256;&#38590;&#12290;&#23433;&#20840;&#25514;&#26045;&#21487;&#33021;&#21253;&#25324;&#22312;&#21407;&#22987;&#20195;&#30721;&#20013;&#28155;&#21152;&#19968;&#23545;&#20195;&#30721;&#34892;&#65292;&#21253;&#25324;&#31354;&#25351;&#38024;&#26816;&#26597;&#25110;&#20934;&#22791;&#22909;&#30340;&#35821;&#21477;&#20197;&#38450;&#27490;SQL&#27880;&#20837;&#12290;&#30446;&#21069;&#65292;&#21487;&#29992;&#30340;&#20195;&#30721;&#20462;&#22797;LLMs&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#29983;&#25104;&#20195;&#30721;&#20462;&#22797;&#65292;&#27169;&#22411;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#20195;&#30721;&#21644;&#20462;&#22797;&#21518;&#30340;&#20195;&#30721;&#22312;&#21151;&#33021;&#21644;&#35821;&#27861;&#19978;&#22823;&#33268;&#30456;&#20284;&#65292;&#38500;&#20102;&#23569;&#25968;&#65288;1-2&#65289;&#34892;&#20316;&#20026;&#23433;&#20840;&#25514;&#26045;&#12290;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#25152;&#38656;&#34892;&#25968;&#19982;&#21151;&#33021;&#20195;&#30721;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#23548;&#33268;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#29983;&#25104;&#21151;&#33021;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent advancement of Large Language Models (LLMs), generating functionally correct code has become less complicated for a wide array of developers. While using LLMs has sped up the functional development process, it poses a heavy risk to code security. Code generation with proper security measures using LLM is a significantly more challenging task than functional code generation. Security measures may include adding a pair of lines of code with the original code, consisting of null pointer checking or prepared statements for SQL injection prevention. Currently, available code repair LLMs generate code repair by supervised fine-tuning, where the model looks at cross-entropy loss. However, the original and repaired codes are mostly similar in functionality and syntactically, except for a few (1-2) lines, which act as security measures. This imbalance between the lines needed for security measures and the functional code enforces the supervised fine-tuned model to prioritize gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Separate-and-Enhance"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#25439;&#22833;&#21644;&#22686;&#24378;&#25439;&#22833;&#26469;&#35299;&#20915;&#29616;&#26377;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#38169;&#20301;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#23545;&#35937;&#25513;&#30721;&#37325;&#21472;&#21644;&#26368;&#22823;&#21270;&#27880;&#24847;&#21147;&#24471;&#20998;&#26469;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#19982;&#25991;&#26412;&#25552;&#31034;&#30340;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36924;&#30495;&#24230;&#12289;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.06712</link><description>&lt;p&gt;
Separate-and-Enhance: &#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#21512;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Separate-and-Enhance"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#25439;&#22833;&#21644;&#22686;&#24378;&#25439;&#22833;&#26469;&#35299;&#20915;&#29616;&#26377;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#38169;&#20301;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#23545;&#35937;&#25513;&#30721;&#37325;&#21472;&#21644;&#26368;&#22823;&#21270;&#27880;&#24847;&#21147;&#24471;&#20998;&#26469;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#19982;&#25991;&#26412;&#25552;&#31034;&#30340;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36924;&#30495;&#24230;&#12289;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#27169;&#22411;&#22312;&#36817;&#26399;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#30446;&#21069;&#30340;&#31995;&#32479;&#20173;&#28982;&#19981;&#22826;&#33021;&#22815;&#30830;&#20445;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#30340;&#33391;&#22909;&#32452;&#21512;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#23545;&#35937;&#29983;&#25104;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#31181;&#38169;&#20301;&#30340;&#22522;&#26412;&#21407;&#22240;&#65292;&#25351;&#20986;&#20102;&#19982;&#20302;&#27880;&#24847;&#21147;&#28608;&#27963;&#24471;&#20998;&#21644;&#25513;&#30721;&#37325;&#21472;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#20998;&#21035;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#25972;&#20307;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#30446;&#26631;&#65292;&#21363;&#20998;&#31163;&#25439;&#22833;&#21644;&#22686;&#24378;&#25439;&#22833;&#65292;&#20998;&#21035;&#20943;&#23569;&#23545;&#35937;&#25513;&#30721;&#37325;&#21472;&#21644;&#26368;&#22823;&#21270;&#27880;&#24847;&#21147;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#24120;&#35268;&#30340;&#27979;&#35797;&#26102;&#35843;&#36866;&#25216;&#26415;&#65292;&#30528;&#37325;&#20110;&#24494;&#35843;&#20851;&#38190;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#36924;&#30495;&#24230;&#12289;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#65292;&#23588;&#20854;&#26159;&#22312;&#23454;&#29616;&#19978;&#36848;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent significant strides achieved by diffusion-based Text-to-Image (T2I) models, current systems are still less capable of ensuring decent compositional generation aligned with text prompts, particularly for the multi-object generation. This work illuminates the fundamental reasons for such misalignment, pinpointing issues related to low attention activation scores and mask overlaps. While previous research efforts have individually tackled these issues, we assert that a holistic approach is paramount. Thus, we propose two novel objectives, the Separate loss and the Enhance loss, that reduce object mask overlaps and maximize attention scores, respectively. Our method diverges from conventional test-time-adaptation techniques, focusing on finetuning critical parameters, which enhances scalability and generalizability. Comprehensive evaluations demonstrate the superior performance of our model in terms of image realism, text-image alignment, and adaptability, notably outperform
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20174;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#30340;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#26469;&#25910;&#38598;&#21644;&#26356;&#26032;&#30456;&#20851;&#35770;&#25991;&#12290;</title><link>https://arxiv.org/abs/2312.03863</link><description>&lt;p&gt;
&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Efficient Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#23545;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20174;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#30340;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#26469;&#25910;&#38598;&#21644;&#26356;&#26032;&#30456;&#20851;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#35201;&#20219;&#21153;&#22914;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#35821;&#35328;&#29983;&#25104;&#21644;&#22797;&#26434;&#25512;&#29702;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#23545;&#25105;&#20204;&#30340;&#31038;&#20250;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20276;&#38543;&#30528;&#23427;&#20204;&#25152;&#38656;&#30340;&#30456;&#24403;&#22823;&#30340;&#36164;&#28304;&#65292;&#31361;&#26174;&#20102;&#35299;&#20915;&#25928;&#29575;&#25361;&#25112;&#30340;&#26377;&#25928;&#25216;&#26415;&#30340;&#24378;&#28872;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#39640;&#25928;LLMs&#30740;&#31350;&#30340;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#23558;&#25991;&#29486;&#25353;&#29031;&#27169;&#22411;&#20026;&#20013;&#24515;&#12289;&#25968;&#25454;&#20026;&#20013;&#24515;&#21644;&#26694;&#26550;&#20026;&#20013;&#24515;&#30340;&#19977;&#20010;&#20027;&#35201;&#20998;&#31867;&#36827;&#34892;&#32452;&#32455;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#20294;&#30456;&#20114;&#20851;&#32852;&#30340;&#39640;&#25928;LLMs&#20027;&#39064;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;GitHub&#23384;&#20648;&#24211;&#65292;&#20854;&#20013;&#25910;&#38598;&#20102;&#26412;&#35843;&#26597;&#20013;&#21015;&#20986;&#30340;&#35770;&#25991;&#65292;&#24182;&#23558;&#31215;&#26497;&#32500;&#25252;&#35813;&#23384;&#20648;&#24211;&#65292;&#24182;&#38543;&#30528;&#26032;&#30340;&#30740;&#31350;&#30340;&#20986;&#29616;&#32780;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges.In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.15623</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Injecting linguistic knowledge into BERT for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#25512;&#29702;&#36807;&#31243;&#32570;&#20047;&#36879;&#26126;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26694;&#26550;&#25552;&#21462;&#35821;&#35328;&#30693;&#35782;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#22686;&#24378;BERT&#22312;DST&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#35745;&#31639;&#32463;&#27982;&#39640;&#25928;&#65292;&#19981;&#38656;&#35201;&#27880;&#37322;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27880;&#20837;&#25552;&#21462;&#30340;&#30693;&#35782;&#21482;&#38656;&#35201;&#28155;&#21152;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#27169;&#22411;(CPM)&#20316;&#20026;DST&#20219;&#21153;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#24182;&#34920;&#26126;&#25152;&#33719;&#21462;&#30340;&#29305;&#24449;&#19982;&#23545;&#35805;&#20013;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#20840;&#38754;&#29702;&#35299;&#24433;&#21709;DST&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;DST&#20219;&#21153;&#19978;&#23545;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
&lt;/p&gt;</description></item><item><title>GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.11319</link><description>&lt;p&gt;
GeoSAM: &#20351;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#23545;SAM&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11319
&lt;/p&gt;
&lt;p&gt;
GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#26102;&#65292;Segment Anything Model (SAM)&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22320;&#29702;&#22270;&#20687;&#65288;&#22914;&#33322;&#25293;&#21644;&#21355;&#26143;&#22270;&#20687;&#65289;&#20013;&#38754;&#20020;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#21106;&#36947;&#36335;&#12289;&#20154;&#34892;&#36947;&#21644;&#20154;&#34892;&#27178;&#36947;&#31561;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#26102;&#12290;&#36825;&#31181;&#36739;&#24046;&#30340;&#24615;&#33021;&#28304;&#20110;&#36825;&#20123;&#23545;&#35937;&#30340;&#31364;&#23567;&#29305;&#24449;&#65292;&#23427;&#20204;&#30340;&#32441;&#29702;&#34701;&#20837;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#26641;&#26408;&#12289;&#24314;&#31569;&#29289;&#12289;&#36710;&#36742;&#21644;&#34892;&#20154;&#31561;&#29289;&#20307;&#30340;&#24178;&#25200;&#65292;&#36825;&#20123;&#37117;&#21487;&#33021;&#20351;&#27169;&#22411;&#22833;&#21435;&#23450;&#21521;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#20998;&#21106;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22320;&#29702;SAM&#65288;GeoSAM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#23494;&#38598;&#35270;&#35273;&#25552;&#31034;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#31232;&#30095;&#35270;&#35273;&#25552;&#31034;&#23454;&#26045;&#20102;&#32454;&#35843;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;GeoSAM&#22312;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#12289;&#34892;&#20154;&#22522;&#30784;&#35774;&#26045;&#30340;&#20998;&#21106;&#24615;&#33021;&#25552;&#21319;&#20102;26&#65285;&#12289;7&#65285;&#21644;17&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 26%, 7%, and 17% for road infrastructure, pedestrian infrastructur
&lt;/p&gt;</description></item><item><title>&#22823;&#35268;&#27169;&#36712;&#36857;&#27169;&#22411;&#65288;LTMs&#65289;&#37319;&#29992;State Transformer (STR)&#27169;&#22411;&#65292;&#23558;&#36816;&#21160;&#39044;&#27979;&#21644;&#35268;&#21010;&#38382;&#39064;&#32479;&#19968;&#24314;&#27169;&#65292;&#26377;&#25928;&#24212;&#23545;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.19620</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#36712;&#36857;&#27169;&#22411;&#26159;&#21487;&#25193;&#23637;&#30340;&#36816;&#21160;&#39044;&#27979;&#21644;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Trajectory Models are Scalable Motion Predictors and Planners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19620
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#36712;&#36857;&#27169;&#22411;&#65288;LTMs&#65289;&#37319;&#29992;State Transformer (STR)&#27169;&#22411;&#65292;&#23558;&#36816;&#21160;&#39044;&#27979;&#21644;&#35268;&#21010;&#38382;&#39064;&#32479;&#19968;&#24314;&#27169;&#65292;&#26377;&#25928;&#24212;&#23545;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#21644;&#35268;&#21010;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#38754;&#20020;&#30340;&#25361;&#25112;&#21253;&#25324;&#29702;&#35299;&#22810;&#26679;&#21270;&#30340;&#36947;&#36335;&#25299;&#25169;&#65292;&#25512;&#29702;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#20132;&#36890;&#21160;&#24577;&#65292;&#35299;&#37322;&#24322;&#36136;&#34892;&#20026;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#29983;&#25104;&#31574;&#30053;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#27169;&#22411;&#25193;&#23637;&#35299;&#20915;&#31867;&#20284;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36712;&#36857;&#27169;&#22411; State Transformer (STR)&#12290;STR&#36890;&#36807;&#23558;&#35266;&#23519;&#12289;&#29366;&#24577;&#21644;&#21160;&#20316;&#25490;&#21015;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#26469;&#37325;&#26032;&#23450;&#20041;&#36816;&#21160;&#39044;&#27979;&#21644;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36712;&#36857;&#29983;&#25104;&#38382;&#39064;&#19982;&#20854;&#20182;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#32467;&#21512;&#36215;&#26469;&#65292;&#20511;&#21161;&#39046;&#22495;&#38388;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#65288;&#22914;&#35821;&#35328;&#24314;&#27169;&#65289;&#65292;&#23454;&#29616;&#24555;&#36895;&#36845;&#20195;&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#36712;&#36857;&#27169;&#22411;&#65288;&#22914;STR&#65289;&#36981;&#24490;&#25193;&#23637;&#23450;&#24459;&#65292;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Motion prediction and planning are vital tasks in autonomous driving, and recent efforts have shifted to machine learning-based approaches. The challenges include understanding diverse road topologies, reasoning traffic dynamics over a long time horizon, interpreting heterogeneous behaviors, and generating policies in a large continuous state space. Inspired by the success of large language models in addressing similar complexities through model scaling, we introduce a scalable trajectory model called State Transformer (STR). STR reformulates the motion prediction and motion planning problems by arranging observations, states, and actions into one unified sequence modeling task. Our approach unites trajectory generation problems with other sequence modeling problems, powering rapid iterations with breakthroughs in neighbor domains such as language modeling. Remarkably, experimental results reveal that large trajectory models (LTMs), such as STR, adhere to the scaling laws by presenting
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;Wav2Vec2.0&#21644;GPT-2&#27169;&#22411;&#30340;&#22823;&#33041;&#39044;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#35821;&#38899;&#21453;&#24212;&#65292;&#20854;&#22823;&#33041;&#39044;&#27979;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#65292;&#19988;&#20849;&#20139;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#35299;&#37322;&#22823;&#33041;&#27963;&#21160;&#20013;&#21464;&#24322;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2310.04645</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#25552;&#21462;&#20102;&#19982;&#20154;&#31867;&#22823;&#33041;&#31867;&#20284;&#30340;&#34920;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do self-supervised speech and language models extract similar representations as human brain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04645
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;Wav2Vec2.0&#21644;GPT-2&#27169;&#22411;&#30340;&#22823;&#33041;&#39044;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#35821;&#38899;&#21453;&#24212;&#65292;&#20854;&#22823;&#33041;&#39044;&#27979;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#65292;&#19988;&#20849;&#20139;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#35299;&#37322;&#22823;&#33041;&#27963;&#21160;&#20013;&#21464;&#24322;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#24863;&#30693;&#26399;&#38388;&#23637;&#29616;&#20986;&#19982;&#22823;&#33041;&#27963;&#21160;&#30340;&#24378;&#22823;&#23545;&#40784;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#19981;&#21516;&#35757;&#32451;&#26041;&#24335;&#65292;&#23427;&#20204;&#26159;&#21542;&#19982;&#30456;&#21516;&#30340;&#31070;&#32463;&#26041;&#38754;&#30456;&#20851;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20004;&#31181;&#20195;&#34920;&#24615;&#30340;SSL&#27169;&#22411;&#65288;Wav2Vec2.0&#21644;GPT-2&#65289;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#26469;&#30452;&#25509;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#27169;&#22411;&#37117;&#33021;&#20934;&#30830;&#39044;&#27979;&#21548;&#35273;&#30382;&#23618;&#20013;&#30340;&#35821;&#38899;&#21709;&#24212;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#22823;&#33041;&#39044;&#27979;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Wav2Vec2.0&#21644;GPT-2&#20043;&#38388;&#30340;&#20849;&#20139;&#35821;&#38899;&#19978;&#19979;&#25991;&#20449;&#24687;&#35299;&#37322;&#20102;&#22823;&#33041;&#27963;&#21160;&#20013;&#30340;&#22823;&#37096;&#20998;&#21464;&#24322;&#65292;&#36229;&#36807;&#20102;&#38745;&#24577;&#35821;&#20041;&#21644;&#36739;&#20302;&#32423;&#30340;&#22768;&#38899;-&#38899;&#20301;&#20449;&#24687;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;SSL&#27169;&#22411;&#20013;&#35821;&#38899;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#25910;&#25947;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#35821;&#38899;&#30693;&#35273;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech and language models trained through self-supervised learning (SSL) demonstrate strong alignment with brain activity during speech and language perception. However, given their distinct training modalities, it remains unclear whether they correlate with the same neural aspects. We directly address this question by evaluating the brain prediction performance of two representative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and language tasks. Our findings reveal that both models accurately predict speech responses in the auditory cortex, with a significant correlation between their brain predictions. Notably, shared speech contextual information between Wav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain activity, surpassing static semantic and lower-level acoustic-phonetic information. These results underscore the convergence of speech contextual representations in SSL models and their alignment with the neural network underlying speech percept
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#27604;&#36739;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#30340;&#25104;&#26412;&#25928;&#30410;&#65292;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;&#65292;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#21333;&#29420;&#35843;&#25972;&#27599;&#31181;&#35821;&#35328;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#37319;&#29992;&#19979;&#37319;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35843;&#25972;&#21487;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#25928;&#26524;&#21644;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.08958</link><description>&lt;p&gt;
&#21333;&#35821;&#25110;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#65306;&#21738;&#31181;&#26041;&#24335;&#26356;&#36866;&#21512;alpaca&#65311;
&lt;/p&gt;
&lt;p&gt;
Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08958
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#27604;&#36739;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#30340;&#25104;&#26412;&#25928;&#30410;&#65292;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;&#65292;&#22810;&#35821;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#21333;&#29420;&#35843;&#25972;&#27599;&#31181;&#35821;&#35328;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#37319;&#29992;&#19979;&#37319;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35843;&#25972;&#21487;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#25928;&#26524;&#21644;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26469;&#25191;&#34892;&#24320;&#25918;&#22495;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#32842;&#22825;&#21161;&#25163;&#31561;&#24212;&#29992;&#12290;&#34429;&#28982;&#36825;&#31867;&#21162;&#21147;&#36890;&#24120;&#21482;&#22312;&#21333;&#19968;&#35821;&#35328;&#20013;&#36827;&#34892;&#65292;&#20294;&#25105;&#20204;&#23454;&#35777;&#20998;&#26512;&#20102;&#22810;&#35821;&#35328;&#22330;&#26223;&#19979;&#30340;&#25104;&#26412;&#25928;&#30410;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;Alpaca&#25968;&#25454;&#38598;&#21644;&#20854;&#20013;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#24418;&#25104;&#22810;&#35821;&#35328;&#35843;&#25972;&#30340;&#35757;&#32451;&#38598;&#65292;&#28982;&#21518;&#37319;&#29992;&#20302;&#31209;&#35843;&#25972;&#25110;&#23436;&#20840;&#21442;&#25968;&#35757;&#32451;&#30340;&#26041;&#24335;&#23545;LLM&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#21463;&#25511;&#31639;&#21147;&#39044;&#31639;&#19979;&#30340;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#35843;&#25972;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27599;&#31181;&#35821;&#35328;&#21333;&#29420;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19979;&#37319;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35328;&#35843;&#25972;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#29978;&#33267;&#26356;&#24378;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26469;&#25193;&#23637;&#35821;&#35328;&#25903;&#25345;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational large language models (LLMs) can be instruction-tuned to perform open-domain question answering, facilitating applications like chat assistants. While such efforts are often carried out in a single language, we empirically analyze cost-efficient strategies for multilingual scenarios. Our study employs the Alpaca dataset and machine translations of it to form multilingual data, which is then used to tune LLMs through either low-rank adaptation or full-parameter training. Under a controlled computation budget, comparisons show that multilingual tuning is on par or better than tuning a model for each language. Furthermore, multilingual tuning with downsampled data can be as powerful and more robust. Our findings serve as a guide for expanding language support through instruction tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CLIP&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#23398;&#20064;&#35821;&#38899;&#21644;&#22768;&#23398;&#31354;&#38388;&#30340;&#20849;&#20139;&#34920;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#23545;&#35821;&#38899;&#21464;&#21270;&#25935;&#24863;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2307.12445</link><description>&lt;p&gt;
SCRAPS: &#22768;&#38899;&#21644;&#35821;&#38899;&#31354;&#38388;&#30340;&#23545;&#27604;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.12445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CLIP&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#23398;&#20064;&#35821;&#38899;&#21644;&#22768;&#23398;&#31354;&#38388;&#30340;&#20849;&#20139;&#34920;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#23545;&#35821;&#38899;&#21464;&#21270;&#25935;&#24863;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#20247;&#22810;&#20363;&#23376;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#19978;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;&#26368;&#36817;&#65292;CLIP&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#23398;&#20064;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#20043;&#38388;&#30340;&#20849;&#20139;&#28508;&#22312;&#31354;&#38388;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#26480;&#20986;&#30340;&#38646;&#27425;&#25110;&#23569;&#27425;&#27979;&#35797;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;CLIP&#25552;&#20986;&#30340;&#30456;&#21516;&#24605;&#24819;&#65292;&#20294;&#24212;&#29992;&#20110;&#35821;&#38899;&#39046;&#22495;&#65292;&#20854;&#20013;&#35821;&#38899;&#21644;&#22768;&#23398;&#31354;&#38388;&#36890;&#24120;&#20849;&#23384;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;CLIP&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#23398;&#20064;&#35821;&#38899;&#21644;&#22768;&#23398;&#31354;&#38388;&#30340;&#20849;&#20139;&#34920;&#24449;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23545;&#35821;&#38899;&#21464;&#21270;&#25935;&#24863;&#65292;&#23558;20&#65285;&#38543;&#26426;&#26367;&#25442;&#30340;&#38899;&#32032;&#24471;&#20998;&#19979;&#38477;&#20102;91&#65285;&#65292;&#21516;&#26102;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#25552;&#20379;&#20102;&#23454;&#36136;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#23558;&#38899;&#39057;&#19982;75&#65285;&#30340;&#39640;&#26031;&#22122;&#22768;&#28151;&#21512;&#26102;&#24615;&#33021;&#19979;&#38477;&#20102;10&#65285;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#23884;&#20837;&#23545;&#20110;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#26159;&#26377;&#29992;&#30340;&#65292;&#27604;&#22914;&#21487;&#25026;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#26102;&#21435;&#22122;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21033;&#29992;&#36719;&#22686;&#24378;&#26631;&#31614;&#21644;&#33258;&#25105;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36890;&#36807;&#20174;&#26356;&#24178;&#20928;&#30340;&#21407;&#22987;&#25968;&#25454;&#23398;&#20064;&#26469;&#20445;&#35777;&#22686;&#24378;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2212.10558</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#21363;&#26102;&#21435;&#22122;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
On-the-fly Denoising for Data Augmentation in Natural Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#26102;&#21435;&#22122;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21033;&#29992;&#36719;&#22686;&#24378;&#26631;&#31614;&#21644;&#33258;&#25105;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36890;&#36807;&#20174;&#26356;&#24178;&#20928;&#30340;&#21407;&#22987;&#25968;&#25454;&#23398;&#20064;&#26469;&#20445;&#35777;&#22686;&#24378;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#32463;&#24120;&#34987;&#29992;&#26469;&#22312;&#27809;&#26377;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#24341;&#20837;&#22122;&#22768;&#25968;&#25454;&#26469;&#24178;&#25200;&#35757;&#32451;&#12290;&#20026;&#20102;&#20445;&#35777;&#22686;&#24378;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#22686;&#24378;&#25968;&#25454;&#20013;&#27809;&#26377;&#22122;&#22768;&#65292;&#24182;&#37319;&#29992;&#19968;&#33268;&#24615;&#35757;&#32451;&#65292;&#35201;&#20040;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#22914;&#35757;&#32451;&#25439;&#22833;&#21644;&#22810;&#26679;&#24615;&#32422;&#26463;&#65289;&#26469;&#36807;&#28388;&#25481;&#8220;&#22024;&#26434;&#8221;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34987;&#36807;&#28388;&#30340;&#31034;&#20363;&#21487;&#33021;&#20173;&#28982;&#21253;&#21547;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#23436;&#20840;&#20002;&#24323;&#23427;&#20204;&#20250;&#23548;&#33268;&#30417;&#30563;&#20449;&#21495;&#30340;&#20007;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#21407;&#22987;&#25968;&#25454;&#38598;&#27604;&#22686;&#24378;&#25968;&#25454;&#26356;&#24178;&#20928;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#26102;&#21435;&#22122;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#20102;&#22312;&#26356;&#24178;&#20928;&#30340;&#21407;&#22987;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26377;&#26426;&#25945;&#24072;&#27169;&#22411;&#25552;&#20379;&#30340;&#36719;&#22686;&#24378;&#26631;&#31614;&#36827;&#34892;&#23398;&#20064;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#22122;&#22768;&#26631;&#31614;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#31616;&#21333;&#30340;&#33258;&#25105;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#24378;&#21046;&#27169;&#22411;&#39044;&#27979;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation (DA) is frequently used to provide additional training data without extra human annotation automatically. However, data augmentation may introduce noisy data that impairs training. To guarantee the quality of augmented data, existing methods either assume no noise exists in the augmented data and adopt consistency training or use simple heuristics such as training loss and diversity constraints to filter out "noisy" data. However, those filtered examples may still contain useful information, and dropping them completely causes a loss of supervision signals. In this paper, based on the assumption that the original dataset is cleaner than the augmented data, we propose an on-the-fly denoising technique for data augmentation that learns from soft augmented labels provided by an organic teacher model trained on the cleaner original data. To further prevent overfitting on noisy labels, a simple self-regularization module is applied to force the model prediction to be consi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24515;&#30005;&#22270;(ECG)&#30340;&#21387;&#21147;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#24515;&#30005;&#22270;&#29305;&#24449;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#21387;&#21147;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2210.06225</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#24515;&#30005;&#22270;(ECG)&#30340;&#21387;&#21147;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalizability of ECG-based Stress Detection Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24515;&#30005;&#22270;(ECG)&#30340;&#21387;&#21147;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#24515;&#30005;&#22270;&#29305;&#24449;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#21387;&#21147;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#21147;&#22312;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#24456;&#26222;&#36941;&#65292;&#21253;&#25324;&#24037;&#20316;&#12289;&#21307;&#30103;&#21644;&#31038;&#20132;&#20114;&#21160;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#21508;&#31181;&#29983;&#29289;&#20449;&#21495;&#30340;&#25163;&#24037;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#26159;&#21387;&#21147;&#30340;&#25351;&#26631;&#12290;&#26368;&#36817;&#65292;&#20063;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#21387;&#21147;&#12290;&#36890;&#24120;&#65292;&#21387;&#21147;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#20010;&#21387;&#21147;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#22330;&#26223;&#25910;&#38598;&#21387;&#21147;&#25968;&#25454;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#22312;&#20854;&#20182;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#31243;&#24230;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#24515;&#30005;&#22270;(ECG)&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#24515;&#30005;&#22270;&#29305;&#24449;&#65288;&#21363;&#24515;&#29575;&#21464;&#24322;&#24615;(HRV)&#29305;&#24449;&#65289;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19977;&#20010;HRV&#27169;&#22411;&#21644;&#20004;&#20010;&#20351;&#29992;ECG&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#21387;&#21147;&#25968;&#25454;&#38598;&#65288;WESAD&#21644;SWELL-KW&#65289;&#30340;ECG&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stress is prevalent in many aspects of everyday life including work, healthcare, and social interactions. Many works have studied handcrafted features from various bio-signals that are indicators of stress. Recently, deep learning models have also been proposed to detect stress. Typically, stress models are trained and validated on the same dataset, often involving one stressful scenario. However, it is not practical to collect stress data for every scenario. So, it is crucial to study the generalizability of these models and determine to what extent they can be used in other scenarios. In this paper, we explore the generalization capabilities of Electrocardiogram (ECG)-based deep learning models and models based on handcrafted ECG features, i.e., Heart Rate Variability (HRV) features. To this end, we train three HRV models and two deep learning models that use ECG signals as input. We use ECG signals from two popular stress datasets - WESAD and SWELL-KW - differing in terms of stresso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#36848;&#30103;&#27861;&#30340;&#20849;&#24773;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#35268;&#21017;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#29983;&#25104;&#27969;&#30021;&#12289;&#20849;&#24773;&#30340;&#23545;&#35805;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#38750;&#20020;&#24202;&#35797;&#39564;&#39564;&#35777;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#25913;&#36827;&#35774;&#35745;&#21644;&#24615;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>https://arxiv.org/abs/2209.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#36848;&#30103;&#27861;&#30340;&#20849;&#24773;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Empathetic AI Coach for Self-Attachment Therapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#36848;&#30103;&#27861;&#30340;&#20849;&#24773;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#35268;&#21017;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#29983;&#25104;&#27969;&#30021;&#12289;&#20849;&#24773;&#30340;&#23545;&#35805;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#38750;&#20020;&#24202;&#35797;&#39564;&#39564;&#35777;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#25913;&#36827;&#35774;&#35745;&#21644;&#24615;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#33258;&#36848;&#30103;&#27861;&#30340;&#25968;&#23383;&#36741;&#23548;&#31995;&#32479;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#23545;&#35805;&#20195;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#35782;&#21035;&#29992;&#25143;&#25991;&#26412;&#22238;&#22797;&#20013;&#30340;&#28508;&#22312;&#24773;&#32490;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#26816;&#32034;&#26041;&#27861;&#29983;&#25104;&#26032;&#39062;&#12289;&#27969;&#30021;&#21644;&#20849;&#24773;&#30340;&#35805;&#35821;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#32452;&#31867;&#20284;&#20154;&#31867;&#30340;&#35282;&#33394;&#20379;&#29992;&#25143;&#36873;&#25321;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#34394;&#25311;&#30103;&#27861;&#20250;&#35805;&#20013;&#23454;&#29616;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#39033;&#38750;&#20020;&#24202;&#35797;&#39564;&#20013;&#23545;N=16&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#65292;&#36825;&#20123;&#21442;&#19982;&#32773;&#22312;&#20116;&#22825;&#20869;&#33267;&#23569;&#19982;&#20195;&#29702;&#36827;&#34892;&#20102;&#22235;&#27425;&#20114;&#21160;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#31616;&#21333;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26694;&#26550;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24179;&#21488;&#22312;&#20849;&#24773;&#24230;&#12289;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#34987;&#35780;&#20215;&#24471;&#26356;&#39640;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#35774;&#35745;&#21644;&#24615;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a new dataset and a computational strategy for a digital coach that aims to guide users in practicing the protocols of self-attachment therapy. Our framework augments a rule-based conversational agent with a deep-learning classifier for identifying the underlying emotion in a user's text response, as well as a deep-learning assisted retrieval method for producing novel, fluent and empathetic utterances. We also craft a set of human-like personas that users can choose to interact with. Our goal is to achieve a high level of engagement during virtual therapy sessions. We evaluate the effectiveness of our framework in a non-clinical trial with N=16 participants, all of whom have had at least four interactions with the agent over the course of five days. We find that our platform is consistently rated higher for empathy, user engagement and usefulness than the simple rule-based framework. Finally, we provide guidelines to further improve the design and performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#30340;&#36328;&#39046;&#22495;&#28508;&#22312;&#35843;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#20174;&#19968;&#39046;&#22495;&#33719;&#21462;&#28145;&#23618;&#34920;&#31034;&#24182;&#24433;&#21709;&#21478;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#22810;&#20010;&#36716;&#31227;&#23398;&#20064;&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2205.15523</link><description>&lt;p&gt;
&#21464;&#20998;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#36328;&#39046;&#22495;&#28508;&#22312;&#35843;&#21046;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Variational Transfer Learning using Cross-Domain Latent Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.15523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#30340;&#36328;&#39046;&#22495;&#28508;&#22312;&#35843;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#20174;&#19968;&#39046;&#22495;&#33719;&#21462;&#28145;&#23618;&#34920;&#31034;&#24182;&#24433;&#21709;&#21478;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#22810;&#20010;&#36716;&#31227;&#23398;&#20064;&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25104;&#21151;&#22320;&#23558;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24212;&#29992;&#21040;&#26032;&#39046;&#22495;&#65292;&#24378;&#22823;&#30340;&#36716;&#31227;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#28508;&#22312;&#35843;&#21046;&#26426;&#21046;&#65292;&#23558;&#20854;&#24341;&#20837;&#21040;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20174;&#19968;&#20010;&#25968;&#25454;&#39046;&#22495;&#33719;&#21462;&#28145;&#23618;&#34920;&#31034;&#65292;&#24182;&#29992;&#23427;&#26469;&#24433;&#21709;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#21464;&#37327;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#25512;&#29702;&#27169;&#22411;&#26469;&#25552;&#21462;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#28145;&#23618;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#21453;&#36716;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#21518;&#23558;&#23398;&#20064;&#21040;&#30340;&#28145;&#23618;&#34920;&#31034;&#36328;&#35843;&#21046;&#21040;&#21478;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#32534;&#30721;&#20013;&#65292;&#24182;&#24212;&#29992;&#19968;&#33268;&#24615;&#32422;&#26463;&#12290;&#22312;&#21253;&#25324;&#19968;&#20123;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#30340;&#36716;&#31227;&#23398;&#20064;&#22522;&#20934;&#20219;&#21153;&#30340;&#23454;&#35777;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
To successfully apply trained neural network models to new domains, powerful transfer learning solutions are essential. We propose to introduce a novel cross-domain latent modulation mechanism to a variational autoencoder framework so as to achieve effective transfer learning. Our key idea is to procure deep representations from one data domain and use it to influence the reparameterization of the latent variable of another domain. Specifically, deep representations of the source and target domains are first extracted by a unified inference model and aligned by employing gradient reversal. The learned deep representations are then cross-modulated to the latent encoding of the alternative domain, where consistency constraints are also applied. In the empirical validation that includes a number of transfer learning benchmark tasks for unsupervised domain adaptation and image-to-image translation, our model demonstrates competitive performance, which is also supported by evidence obtained
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChainFL&#30340;&#21019;&#26032;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20998;&#23618;&#21644;&#20998;&#29255;&#30340;&#26041;&#24335;&#32467;&#21512;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21306;&#22359;&#38142;&#31995;&#32479;&#22788;&#29702;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#23450;&#21046;&#21270;&#20102;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#19982;&#21306;&#22359;&#38142;&#30340;&#25972;&#21512;&#12290;</title><link>https://arxiv.org/abs/2104.13130</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#21644;&#20998;&#29255;&#21306;&#22359;&#38142;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Secure and Efficient Federated Learning Through Layering and Sharding Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.13130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChainFL&#30340;&#21019;&#26032;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20998;&#23618;&#21644;&#20998;&#29255;&#30340;&#26041;&#24335;&#32467;&#21512;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21306;&#22359;&#38142;&#31995;&#32479;&#22788;&#29702;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#23450;&#21046;&#21270;&#20102;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#19982;&#21306;&#22359;&#38142;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21306;&#22359;&#38142;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#26500;&#24314;&#19968;&#20010;&#21487;&#20449;&#30340;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#26469;&#36827;&#34892;&#20256;&#36755;&#21644;&#23398;&#20064;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21306;&#22359;&#38142;&#31995;&#32479;&#30340;&#20849;&#35782;&#26426;&#21046;&#21644;&#26550;&#26500;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#36164;&#28304;&#28040;&#32791;&#12289;&#20132;&#26131;&#21534;&#21520;&#37327;&#26377;&#38480;&#21644;&#22797;&#26434;&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;ChainFL&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;&#20004;&#23618;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#12290;&#23427;&#23558;&#29289;&#32852;&#32593;&#32593;&#32476;&#20998;&#21106;&#25104;&#22810;&#20010;&#20998;&#29255;&#65292;&#24182;&#22312;&#23376;&#38142;&#23618;&#20869;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20449;&#24687;&#20132;&#25442;&#35268;&#27169;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#20027;&#38142;&#20316;&#20026;&#20027;&#38142;&#23618;&#65292;&#23454;&#29616;&#20102;&#24182;&#34892;&#21644;&#24322;&#27493;&#30340;&#36328;&#20998;&#29255;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#34987;&#23450;&#21046;&#21270;&#20197;&#19982;&#21306;&#22359;&#38142;&#25216;&#26415;&#28145;&#24230;&#25972;&#21512;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;DAG...
&lt;/p&gt;
&lt;p&gt;
Introducing blockchain into Federated Learning (FL) to build a trusted edge computing environment for transmission and learning has attracted widespread attention as a new decentralized learning pattern. However, traditional consensus mechanisms and architectures of blockchain systems face significant challenges in handling large-scale FL tasks, especially on Internet of Things (IoT) devices, due to their substantial resource consumption, limited transaction throughput, and complex communication requirements. To address these challenges, this paper proposes ChainFL, a novel two-layer blockchain-driven FL system. It splits the IoT network into multiple shards within the subchain layer, effectively reducing the scale of information exchange, and employs a Direct Acyclic Graph (DAG)-based mainchain as the mainchain layer, enabling parallel and asynchronous cross-shard validation. Furthermore, the FL procedure is customized to integrate deeply with blockchain technology, and a modified DAG
&lt;/p&gt;</description></item><item><title>BlockFusion&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#21644;&#22806;&#25512;&#25216;&#26415;&#29983;&#25104;&#19977;&#32500;&#22330;&#26223;&#30340;&#27169;&#22411;&#65292;&#33021;&#26080;&#32541;&#22320;&#28155;&#21152;&#26032;&#30340;&#22359;&#20197;&#25193;&#23637;&#22330;&#26223;&#12290;&#37319;&#29992;&#28151;&#21512;&#31070;&#32463;&#22330;&#21644;&#28508;&#22312;&#19977;&#24179;&#38754;&#31354;&#38388;&#26469;&#20445;&#35777;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.17053</link><description>&lt;p&gt;
BlockFusion: &#20351;&#29992;&#28508;&#22312;&#19977;&#24179;&#38754;&#22806;&#25512;&#25193;&#23637;&#30340;&#21487;&#25193;&#23637;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation. (arXiv:2401.17053v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17053
&lt;/p&gt;
&lt;p&gt;
BlockFusion&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#21644;&#22806;&#25512;&#25216;&#26415;&#29983;&#25104;&#19977;&#32500;&#22330;&#26223;&#30340;&#27169;&#22411;&#65292;&#33021;&#26080;&#32541;&#22320;&#28155;&#21152;&#26032;&#30340;&#22359;&#20197;&#25193;&#23637;&#22330;&#26223;&#12290;&#37319;&#29992;&#28151;&#21512;&#31070;&#32463;&#22330;&#21644;&#28508;&#22312;&#19977;&#24179;&#38754;&#31354;&#38388;&#26469;&#20445;&#35777;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;BlockFusion&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#20197;&#21333;&#20301;&#22359;&#24418;&#24335;&#29983;&#25104;&#19977;&#32500;&#22330;&#26223;&#65292;&#24182;&#26080;&#32541;&#22320;&#28155;&#21152;&#26032;&#30340;&#22359;&#20197;&#25193;&#23637;&#22330;&#26223;&#12290;BlockFusion&#20351;&#29992;&#20174;&#23436;&#25972;&#30340;&#19977;&#32500;&#22330;&#26223;&#20013;&#38543;&#26426;&#35009;&#21098;&#30340;3D&#22359;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#22359;&#25311;&#21512;&#65292;&#23558;&#25152;&#26377;&#35757;&#32451;&#22359;&#36716;&#25442;&#20026;&#28151;&#21512;&#31070;&#32463;&#22330;&#65306;&#23427;&#21253;&#21547;&#20960;&#20309;&#29305;&#24449;&#30340;&#19977;&#24179;&#38754;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#30721;&#26377;&#31526;&#21495;&#36317;&#31163;&#20540;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#19977;&#24179;&#38754;&#21387;&#32553;&#21040;&#28508;&#22312;&#19977;&#24179;&#38754;&#31354;&#38388;&#65292;&#24182;&#22312;&#20854;&#19978;&#25191;&#34892;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;&#23545;&#28508;&#22312;&#34920;&#31034;&#24212;&#29992;&#25193;&#25955;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#12290;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25193;&#23637;&#22330;&#26223;&#26102;&#65292;&#21482;&#38656;&#23558;&#31354;&#22359;&#28155;&#21152;&#21040;&#19982;&#24403;&#21069;&#22330;&#26223;&#37325;&#21472;&#65292;&#24182;&#22806;&#25512;&#29616;&#26377;&#30340;&#28508;&#22312;&#19977;&#24179;&#38754;&#20197;&#22635;&#20805;&#26032;&#22359;&#12290;&#22806;&#25512;&#36807;&#31243;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#32422;&#26463;&#26469;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature 
&lt;/p&gt;</description></item><item><title>DiffuserLite&#26159;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#25552;&#39640;&#20915;&#31574;&#39057;&#29575;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26694;&#26550;&#65292;&#23427;&#21482;&#20135;&#29983;&#20102;&#24456;&#23567;&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15443</link><description>&lt;p&gt;
DiffuserLite: &#23454;&#26102;&#25193;&#25955;&#35268;&#21010;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
DiffuserLite: Towards Real-time Diffusion Planning. (arXiv:2401.15443v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15443
&lt;/p&gt;
&lt;p&gt;
DiffuserLite&#26159;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#25552;&#39640;&#20915;&#31574;&#39057;&#29575;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26694;&#26550;&#65292;&#23427;&#21482;&#20135;&#29983;&#20102;&#24456;&#23567;&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#35268;&#21010;&#34987;&#35748;&#20026;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#26377;&#25928;&#30340;&#20915;&#31574;&#33539;&#24335;&#12290;&#38271;&#26102;&#38388;&#36328;&#24230;&#36712;&#36857;&#30340;&#39640;&#36136;&#37327;&#26465;&#20214;&#29983;&#25104;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#35268;&#21010;&#26041;&#27861;&#30001;&#20110;&#36845;&#20195;&#25277;&#26679;&#25104;&#26412;&#26114;&#36149;&#32780;&#23548;&#33268;&#20915;&#31574;&#39057;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DiffuserLite&#65292;&#19968;&#20010;&#24555;&#36895;&#32780;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#12290;DiffuserLite&#20351;&#29992;&#20102;&#19968;&#20010;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#29983;&#25104;&#31895;&#21040;&#32454;&#31890;&#24230;&#30340;&#36712;&#36857;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#20887;&#20313;&#20449;&#24687;&#30340;&#24314;&#27169;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20915;&#31574;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#26694;&#26550;&#30456;&#27604;&#65292;DiffuserLite&#20165;&#20135;&#29983;&#20102;$0.88\%$&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24179;&#22343;&#20915;&#31574;&#39057;&#29575;&#36798;&#21040;&#20102;122Hz&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24178;&#20928;DiffuserLite&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The high-quality conditional generation capability of long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies because of the expensive iterative sampling cost. To address this issue, we introduce DiffuserLite, a fast and lightweight diffusion planning framework. DiffuserLite employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, which significantly reduces the modeling of redundant information and leads to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite incurs only $0.88\%$ of the runtime cost compared to previous frameworks, achieves an average decision-making frequency of $122$Hz, and reaches state-of-the-art performance on D4RL benchmarks. In addition, our clean DiffuserLite framework can serve 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#32452;&#25351;&#21335;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#36229;&#21442;&#25968;&#23545;GPU&#19978;&#25191;&#34892;&#30340;&#35745;&#31639;&#20869;&#26680;&#30340;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#26469;&#26368;&#22823;&#21270;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290;&#30456;&#27604;&#20110;&#20855;&#26377;&#30456;&#20284;&#21442;&#25968;&#25968;&#37327;&#20294;&#24418;&#29366;&#26410;&#32463;&#20248;&#21270;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#39640;&#25928;&#27169;&#22411;&#24418;&#29366;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;39%&#30340;&#21534;&#21520;&#37327;&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14489</link><description>&lt;p&gt;
&#19982;&#30828;&#20214;&#20849;&#21516;&#35774;&#35745;&#27169;&#22411;&#26550;&#26500;&#30340;&#29702;&#30001;
&lt;/p&gt;
&lt;p&gt;
The Case for Co-Designing Model Architectures with Hardware. (arXiv:2401.14489v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#32452;&#25351;&#21335;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#36229;&#21442;&#25968;&#23545;GPU&#19978;&#25191;&#34892;&#30340;&#35745;&#31639;&#20869;&#26680;&#30340;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#26469;&#26368;&#22823;&#21270;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290;&#30456;&#27604;&#20110;&#20855;&#26377;&#30456;&#20284;&#21442;&#25968;&#25968;&#37327;&#20294;&#24418;&#29366;&#26410;&#32463;&#20248;&#21270;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#39640;&#25928;&#27169;&#22411;&#24418;&#29366;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;39%&#30340;&#21534;&#21520;&#37327;&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;GPU&#36127;&#36131;&#35757;&#32451;&#22823;&#37096;&#20998;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#22312;&#35774;&#35745;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#24448;&#24448;&#24573;&#35270;&#20102;&#20854;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20462;&#25913;&#20026;&#26356;&#36866;&#21512;&#30446;&#26631;&#30828;&#20214;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#32452;&#25351;&#21335;&#65292;&#29992;&#20110;&#20351;&#29992;&#25143;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20182;&#20204;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290;&#36825;&#20123;&#25351;&#21335;&#26159;&#36890;&#36807;&#20180;&#32454;&#32771;&#34385;&#25511;&#21046;&#27169;&#22411;&#24418;&#29366;&#30340;&#21508;&#31181;&#27169;&#22411;&#36229;&#21442;&#25968;&#23545;GPU&#19978;&#25191;&#34892;&#30340;&#24213;&#23618;&#35745;&#31639;&#20869;&#26680;&#30340;&#25928;&#29575;&#30340;&#24433;&#21709;&#32780;&#21019;&#24314;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#39640;&#25928;&#27169;&#22411;&#24418;&#29366;&#30340;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#27604;&#20855;&#26377;&#30456;&#20284;&#21442;&#25968;&#25968;&#37327;&#20294;&#24418;&#29366;&#26410;&#32463;&#20248;&#21270;&#30340;&#27169;&#22411;&#39640;&#20986;39&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While GPUs are responsible for training the vast majority of state-of-the-art deep learning models, the implications of their architecture are often overlooked when designing new deep learning (DL) models. As a consequence, modifying a DL model to be more amenable to the target hardware can significantly improve the runtime performance of DL training and inference. In this paper, we provide a set of guidelines for users to maximize the runtime performance of their transformer models. These guidelines have been created by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU. We find the throughput of models with efficient model shapes is up to 39\% higher while preserving accuracy compared to models with a similar number of parameters but with unoptimized shapes.
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.14440</link><description>&lt;p&gt;
&#35821;&#20041;&#25935;&#24863;&#24615;&#21644;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#65306;&#34913;&#37327;NLI&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22522;&#20110;transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#30340;&#26032;&#33021;&#21147;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#20855;&#22791;&#23545;&#35789;&#27719;&#21644;&#32452;&#21512;&#35821;&#20041;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#36825;&#20123;&#35828;&#27861;&#24212;&#35813;&#25345;&#20445;&#30041;&#24577;&#24230;&#65306;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#23548;&#33268;&#25512;&#26029;&#36807;&#31243;&#20013;&#20986;&#29616;&#22823;&#37327;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;&#20915;&#31574;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#21644;&#28145;&#20837;&#29702;&#35299;&#19981;&#21516;&#65292;&#32780;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#24230;&#25110;&#25506;&#31350;&#21477;&#27861;&#12289;&#21333;&#35843;&#24615;&#21644;&#36923;&#36753;&#40065;&#26834;&#24615;&#25512;&#29702;&#26102;&#22343;&#19981;&#20250;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#35821;&#20041;&#25935;&#24863;&#24615;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21547;&#26377;&#24494;&#23567;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#36755;&#20837;&#22122;&#22768;&#30340;&#23545;&#25239;&#29983;&#25104;&#26679;&#20363;&#26469;&#35780;&#20272;NLI&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13192</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#34920;&#31034;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#35774;&#35745;&#20013;&#65292;&#39640;&#25928;&#22320;&#29983;&#25104;&#33021;&#37327;&#31283;&#23450;&#30340;&#26230;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26230;&#26684;&#20013;&#21407;&#23376;&#30340;&#24040;&#22823;&#25490;&#21015;&#12290;&#20026;&#20102;&#20419;&#36827;&#31283;&#23450;&#26448;&#26009;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21487;&#21512;&#25104;&#26448;&#26009;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28857;&#20113;&#34920;&#31034;&#26469;&#32534;&#30721;&#22797;&#26434;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#25903;&#26609;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#36755;&#20837;&#32467;&#26500;&#65292;&#24182;&#20005;&#26684;&#39564;&#35777;&#20854;&#39640;&#37325;&#24314;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#20840;&#26032;&#30340;&#26448;&#26009;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#26230;&#20307;&#25193;&#25955;(PCCD)&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#21512;&#25104;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#26448;&#26009;&#35774;&#35745;&#21644;&#21512;&#25104;&#30340;&#25512;&#36827;&#20013;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#29983;&#25104;&#35774;&#35745;&#26041;&#27861;&#65292;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the con
&lt;/p&gt;</description></item><item><title>GRATH&#26159;&#19968;&#31181;&#36880;&#27493;&#33258;&#25105;&#30495;&#23454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22806;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;GRATH&#22312;&#27809;&#26377;&#26631;&#27880;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#36880;&#27493;&#25552;&#21319;&#27169;&#22411;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12292</link><description>&lt;p&gt;
GRATH: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#28176;&#33258;&#25105;&#30495;&#23454;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12292
&lt;/p&gt;
&lt;p&gt;
GRATH&#26159;&#19968;&#31181;&#36880;&#27493;&#33258;&#25105;&#30495;&#23454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22806;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;GRATH&#22312;&#27809;&#26377;&#26631;&#27880;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#36880;&#27493;&#25552;&#21319;&#27169;&#22411;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#30495;&#23454;&#24615;&#23545;&#23427;&#20204;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#22312;&#29983;&#25104;&#30495;&#23454;&#31572;&#26696;&#21644;&#20869;&#23481;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#22914;&#22312;TruthfulQA&#31561;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRAdual self-truTHifying (GRATH)&#65292;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#26041;&#27861;&#25552;&#39640;LLMs&#30495;&#23454;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;GRATH&#21033;&#29992;&#39046;&#22495;&#22806;&#30340;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#30456;&#24212;&#30340;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;GRATH&#20197;&#26080;&#38656;&#26631;&#27880;&#31572;&#26696;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GRATH&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;LLM&#33258;&#36523;&#29983;&#25104;&#25104;&#23545;&#30495;&#23454;&#24615;&#35757;&#32451;&#25968;&#25454;&#65292;&#27599;&#23545;&#21253;&#21547;&#19968;&#20010;&#38382;&#39064;&#21450;&#20854;&#27491;&#30830;&#21644;&#38169;&#35823;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#31572;&#26696;&#23545;&#30340;&#24046;&#24322;&#20013;&#23398;&#20064;&#12290;&#38543;&#21518;&#65292;GRATH&#36845;&#20195;&#22320;&#20248;&#21270;&#27169;&#22411;&#20197;&#36880;&#28176;&#25552;&#39640;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11864</link><description>&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#20855;&#26377;&#23567;&#20110;&#21313;&#20159;&#21442;&#25968;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#23553;&#35013;&#20026;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;EoTD&#25968;&#25454;&#38598;&#26469;&#23545;SLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;SLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36825;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#31243;&#24207;&#21644;&#24605;&#32500;&#26041;&#31243;&#65289;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;EoTD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;ETD&#20351;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#26694;&#26550;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.08694</link><description>&lt;p&gt;
&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#29992;&#20110;&#28040;&#38500;&#35823;&#20449;&#24687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation. (arXiv:2401.08694v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#26694;&#26550;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#30340;&#20027;&#35201;&#20505;&#36873;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#30452;&#25509;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35823;&#20449;&#24687;&#28040;&#38500;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#22522;&#20110;&#26679;&#26412;&#19968;&#33268;&#24615;&#26041;&#27861;&#30340;&#26657;&#20934;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26679;&#26412;&#35268;&#27169;&#21644;&#38543;&#26426;&#27700;&#24179;&#30340;&#19968;&#33268;&#24615;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#40065;&#26834;&#30340;&#25968;&#23383;&#21270;&#21475;&#22836;&#25552;&#31034;&#22312;&#21333;&#27493;&#21644;&#20004;&#27493;&#32622;&#20449;&#24230;&#24341;&#23548;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#21644;&#20998;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#30456;&#21516;&#25552;&#31034;&#22312;&#19981;&#21516;&#29256;&#26412;&#30340;GPT&#21644;&#19981;&#21516;&#25968;&#23383;&#23610;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#22522;&#20110;&#26679;&#26412;&#19968;&#33268;&#24615;&#21644;&#25968;&#23383;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20026;GPT&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overal
&lt;/p&gt;</description></item><item><title>Ada-Retrieval&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#22810;&#36718;&#26816;&#32034;&#33539;&#20363;&#65292;&#29992;&#20110;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#29289;&#21697;&#20505;&#36873;&#32773;&#36873;&#25321;&#36807;&#31243;&#12290;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#25913;&#36827;&#29992;&#25143;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#23436;&#25972;&#30340;&#29289;&#21697;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20505;&#36873;&#32773;&#65292;&#24182;&#20855;&#26377;&#27169;&#22411;&#26080;&#20851;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.06633</link><description>&lt;p&gt;
Ada-Retrieval&#65306;&#36866;&#24212;&#24615;&#22810;&#36718;&#26816;&#32034;&#33539;&#20363;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations. (arXiv:2401.06633v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06633
&lt;/p&gt;
&lt;p&gt;
Ada-Retrieval&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#22810;&#36718;&#26816;&#32034;&#33539;&#20363;&#65292;&#29992;&#20110;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#29289;&#21697;&#20505;&#36873;&#32773;&#36873;&#25321;&#36807;&#31243;&#12290;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#25913;&#36827;&#29992;&#25143;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#23436;&#25972;&#30340;&#29289;&#21697;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20505;&#36873;&#32773;&#65292;&#24182;&#20855;&#26377;&#27169;&#22411;&#26080;&#20851;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#27169;&#22411;&#26088;&#22312;&#36873;&#25321;&#19982;&#32473;&#23450;&#29992;&#25143;&#20559;&#22909;&#21305;&#37197;&#30340;&#19968;&#23567;&#32452;&#29289;&#21697;&#20505;&#36873;&#32773;&#12290;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#21518;&#32493;&#30340;&#27169;&#22411;&#65288;&#22914;&#25490;&#21517;&#22120;&#65289;&#39640;&#24230;&#20381;&#36182;&#20110;&#29289;&#21697;&#20505;&#36873;&#32773;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#27169;&#22411;&#37319;&#29992;&#21333;&#36718;&#25512;&#29702;&#33539;&#20363;&#65292;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#30340;&#21160;&#24577;&#24615;&#24182;&#22266;&#23450;&#22312;&#29289;&#21697;&#31354;&#38388;&#30340;&#26576;&#20010;&#21306;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-Retrieval&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#22810;&#36718;&#26816;&#32034;&#33539;&#20363;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25913;&#36827;&#29992;&#25143;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#23436;&#25972;&#30340;&#29289;&#21697;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20505;&#36873;&#32773;&#12290;Ada-Retrieval&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#29289;&#21697;&#34920;&#31034;&#36866;&#37197;&#22120;&#21644;&#29992;&#25143;&#34920;&#31034;&#36866;&#37197;&#22120;&#65292;&#26088;&#22312;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#27880;&#20837;&#29289;&#21697;&#21644;&#29992;&#25143;&#30340;&#34920;&#31034;&#20013;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#27169;&#22411;&#26080;&#20851;&#30340;&#35774;&#35745;&#65292;&#21487;&#20197;&#19982;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;RNN&#25110;Transformer&#65289;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval models aim at selecting a small set of item candidates which match the preference of a given user. They play a vital role in large-scale recommender systems since subsequent models such as rankers highly depend on the quality of item candidates. However, most existing retrieval models employ a single-round inference paradigm, which may not adequately capture the dynamic nature of user preferences and stuck in one area in the item space. In this paper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for recommender systems that iteratively refines user representations to better capture potential candidates in the full item space. Ada-Retrieval comprises two key modules: the item representation adapter and the user representation adapter, designed to inject context information into items' and users' representations. The framework maintains a model-agnostic design, allowing seamless integration with various backbone models such as RNNs or Transformers. We pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#22478;&#24066;&#21151;&#33021;&#21306;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28385;&#36275;&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#32447;&#21040;&#31163;&#32447;&#19994;&#21153;&#30340;&#31934;&#30830;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.06550</link><description>&lt;p&gt;
&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#20449;&#24687;&#26816;&#27979;&#22478;&#24066;&#21151;&#33021;&#21306;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information. (arXiv:2401.06550v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#22478;&#24066;&#21151;&#33021;&#21306;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28385;&#36275;&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#32447;&#21040;&#31163;&#32447;&#19994;&#21153;&#30340;&#31934;&#30830;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20852;&#36259;&#21306;&#65288;AOI&#65289;&#26159;&#25351;&#20855;&#26377;&#23450;&#20041;&#36793;&#30028;&#30340;&#25972;&#21512;&#30340;&#22478;&#24066;&#21151;&#33021;&#21306;&#22495;&#12290;&#22478;&#24066;&#21830;&#19994;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#23545;&#23450;&#20041;AOI&#30340;&#26356;&#31934;&#30830;&#35201;&#27714;&#30340;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#22478;&#24066;&#35268;&#21010;&#25110;&#21306;&#22495;&#32463;&#27982;&#20998;&#26512;&#30340;&#24191;&#27867;AOI&#25366;&#25496;&#65292;&#26410;&#33021;&#28385;&#36275;&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#32447;&#21040;&#31163;&#32447;&#19994;&#21153;&#30340;&#31934;&#30830;&#35201;&#27714;&#12290;&#36825;&#20123;&#19994;&#21153;&#38656;&#35201;&#21040;&#20855;&#20307;&#30340;&#31038;&#21306;&#12289;&#23398;&#26657;&#25110;&#21307;&#38498;&#30340;&#20934;&#30830;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#21442;&#32771;&#20449;&#24687;&#26816;&#27979;AOI&#22260;&#26639;&#22810;&#36793;&#24418;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;&#21160;&#24577;&#20154;&#21592;&#27969;&#21160;&#21644;&#29289;&#27969;&#22320;&#22336;&#20449;&#24687;&#30340;&#32423;&#32852;&#27169;&#22359;&#26469;&#35780;&#20272;&#20854;&#26102;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#36873;&#25321;&#29305;&#23450;&#31867;&#21035;&#30340;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#24320;&#22987;&#65292;&#24182;&#29992;&#23427;&#26469;&#21484;&#22238;&#30456;&#24212;&#30340;&#36965;&#24863;&#22270;&#20687;&#12289;&#38468;&#36817;&#30340;POI&#12289;&#36947;&#36335;n
&lt;/p&gt;
&lt;p&gt;
Urban area-of-interest (AOI) refers to an integrated urban functional zone with defined boundaries. The rapid development of urban commerce has resulted in an increased demand for more precise requirements in defining AOIs. However, existing research primarily concentrates on broad AOI mining for urban planning or regional economic analysis, failing to cater to the precise requirements of mobile Internet online-to-offline businesses. These businesses necessitate accuracy down to a specific community, school, or hospital. In this paper, we propose an end-to-end multimodal deep learning algorithm for detecting AOI fence polygon using remote sensing images and multi-semantics reference information. We then evaluate its timeliness through a cascaded module that incorporates dynamic human mobility and logistics address information. Specifically, we begin by selecting a point-of-interest (POI) of specific category, and use it to recall corresponding remote sensing images, nearby POIs, road n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20703</link><description>&lt;p&gt;
&#24378;&#21270;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#19979;&#28216;&#20219;&#21153;&#23545;&#40784;&#65292;&#21363;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26368;&#22823;&#21270;&#65288;&#21487;&#33021;&#26159;&#23398;&#20064;&#24471;&#21040;&#30340;&#65289;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;RFT&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#20248;&#21270;&#38556;&#30861;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27169;&#22411;&#19979;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#21363;&#20351;&#26399;&#26395;&#22870;&#21169;&#36828;&#31163;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;RFT&#22522;&#20934;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#20110;&#23567;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#23548;&#33268;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#19988;&#26377;&#23475;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#26497;&#20854;&#32531;&#24930;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20811;&#26381;RFT&#20013;&#26799;&#24230;&#28040;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#26368;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#23427;&#22312;RFT&#27969;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#30456;&#23545;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;SFT&#38454;&#27573;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.20381</link><description>&lt;p&gt;
GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#19981;&#21516;&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#12289;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;&#35270;&#35273;&#23450;&#20301;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#39318;&#20010;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;&#22522;&#20934;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#32473;&#20986;&#32467;&#26500;&#33391;&#22909;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4V&#22312;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#25581;&#31034;&#20102;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;(&#22914;CIDEr)&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#22312;&#21307;&#23398;VQA&#39046;&#22495;&#65292;GPT-4V&#22312;&#21306;&#20998;&#38382;&#39064;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#65292;&#20294;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#19981;&#21450;&#29616;&#26377;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#24120;&#35268;&#35780;&#20272;&#25351;&#26631;&#22914;BLEU&#20998;&#25968;&#30340;&#23616;&#38480;&#24615;&#65292;&#21628;&#21505;&#24320;&#21457;&#26356;&#22909;&#30340;&#35780;&#20215;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13786</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#21487;&#20197;&#25581;&#31034;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;MIA&#30340;&#22522;&#26412;&#32479;&#35745;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#20915;&#23450;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#25104;&#21151;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#24773;&#20917;&#65292;&#24182;&#23545;&#36825;&#20010;&#24863;&#20852;&#36259;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#25351;&#20196;&#21040;&#21160;&#20316;&#24863;&#30693;&#36793;&#30028;&#26694;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#26080;&#38656;&#24494;&#35843;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21010;&#20998;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#65292;&#29983;&#25104;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#20197;&#29983;&#25104;&#21487;&#34892;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.08873</link><description>&lt;p&gt;
&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models. (arXiv:2310.08873v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#25351;&#20196;&#21040;&#21160;&#20316;&#24863;&#30693;&#36793;&#30028;&#26694;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#26080;&#38656;&#24494;&#35843;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21010;&#20998;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#65292;&#29983;&#25104;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#20197;&#29983;&#25104;&#21487;&#34892;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(GPT-3.5)&#21644;&#24320;&#25918;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(&#22522;&#20110;Grounding DINO)&#21019;&#24314;&#20102;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#65292;&#29992;&#20110;&#36827;&#34892;&#26377;&#25928;&#30340;&#36335;&#24452;&#35268;&#21010;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#36890;&#36807;&#22823;&#22411;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#25351;&#20196;&#65288;&#20363;&#22914;&#8220;&#20320;&#33021;&#36890;&#36807;&#31383;&#24088;&#32473;&#25105;&#36865;&#33647;&#21527;&#65311;&#8221;&#65289;&#21040;&#20855;&#26377;&#21160;&#20316;&#24863;&#30693;&#23646;&#24615;&#30340;&#36793;&#30028;&#26694;&#65288;&#20363;&#22914;&#31383;&#24088;&#65289;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#23558;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#20998;&#25104;&#20004;&#37096;&#20998;&#65306;&#21487;&#36890;&#34892;&#21644;&#19981;&#21487;&#36890;&#34892;&#37096;&#20998;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#36335;&#24452;&#12290;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#24555;&#36895;&#37096;&#32626;&#20110;&#20132;&#20114;&#24335;&#23548;&#33322;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;&#22810;&#20010;&#21487;&#36890;&#34892;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like "Can you pass through the curtains to deliver medicines to me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable object
&lt;/p&gt;</description></item><item><title>MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08648</link><description>&lt;p&gt;
MAPLE: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08648
&lt;/p&gt;
&lt;p&gt;
MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#24212;&#29992;&#30340;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#65292;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE)&#27169;&#22411;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#20934;&#30830;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;MAPLE&#30340;&#33021;&#21147;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;MAPLE&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#24377;&#24615;&#12290;&#23613;&#31649;&#20854;&#20027;&#35201;&#35774;&#35745;&#38754;&#21521;&#24212;&#29992;&#39044;&#27979;&#65292;&#20294;&#32467;&#26524;&#20063;&#24378;&#35843;&#20102;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;LLM&#22312;&#24212;&#29992;&#20351;&#29992;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24314;&#35758;&#22312;&#24314;&#27169;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#65292;&#23427;&#20204;&#20855;&#26377;&#21464;&#38761;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid advancement of mobile applications, predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts. To address these issues, this paper introduces the Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model. This innovative approach utilizes Large Language Models (LLMs) to predict app usage accurately. Rigorous testing on two public datasets highlights MAPLE's capability to decipher intricate patterns and comprehend user contexts. These robust results confirm MAPLE's versatility and resilience across various scenarios. While its primary design caters to app prediction, the outcomes also emphasize the broader applicability of LLMs in different domains. Through this research, we emphasize the potential of LLMs in app usage prediction and suggest their transformative capacity in modelling human behaviours across diverse fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#34903;&#26223;&#22270;&#20687;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#38754;&#21442;&#32771;&#65292;&#35299;&#20915;&#20102;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#21019;&#24314;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05930</link><description>&lt;p&gt;
&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#34903;&#26223;&#22270;&#20687;&#26469;&#32472;&#21046;&#23567;&#20892;&#25143;&#20892;&#20316;&#29289;&#31867;&#22411;&#30340;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Combining deep learning and street view imagery to map smallholder crop types. (arXiv:2309.05930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#34903;&#26223;&#22270;&#20687;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#38754;&#21442;&#32771;&#65292;&#35299;&#20915;&#20102;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#21019;&#24314;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#22270;&#23545;&#20110;&#30417;&#27979;&#35268;&#27169;&#30340;&#20135;&#37327;&#36827;&#23637;&#12289;&#39044;&#27979;&#20840;&#29699;&#20892;&#20316;&#29289;&#29983;&#20135;&#21644;&#21046;&#23450;&#26377;&#25928;&#25919;&#31574;&#26159;&#19968;&#31181;&#24517;&#35201;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#32570;&#20047;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#30340;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#22270;&#21046;&#20316;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30000;&#38388;&#35843;&#26597;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#26159;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#12289;&#37329;&#38065;&#21644;&#32479;&#35745;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#25552;&#20379;&#20102;&#34903;&#26223;&#22270;&#20687;&#65288;&#22914;Google Street View&#65292;KartaView&#21644;Mapillary&#65289;&#12290;&#36825;&#20123;&#22270;&#20687;&#21253;&#21547;&#20102;&#29305;&#23450;&#20301;&#32622;&#21644;&#26102;&#38388;&#30340;&#20892;&#20316;&#29289;&#31181;&#31867;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;Google Street View&#22270;&#20687;&#29983;&#25104;&#20892;&#20316;&#29289;&#31867;&#22411;&#22320;&#38754;&#21442;&#32771;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#31579;&#36873;&#20102;&#19968;&#32452;&#21253;&#21547;&#20892;&#30000;&#30340;&#34903;&#26223;&#22270;&#20687;&#65292;&#36890;&#36807;&#21033;&#29992;&#24369;&#26631;&#31614;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#20892;&#20316;&#29289;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate crop type maps are an essential source of information for monitoring yield progress at scale, projecting global crop production, and planning effective policies. To date, however, crop type maps remain challenging to create in low and middle-income countries due to a lack of ground truth labels for training machine learning models. Field surveys are the gold standard in terms of accuracy but require an often-prohibitively large amount of time, money, and statistical capacity. In recent years, street-level imagery, such as Google Street View, KartaView, and Mapillary, has become available around the world. Such imagery contains rich information about crop types grown at particular locations and times. In this work, we develop an automated system to generate crop type ground references using deep learning and Google Street View imagery. The method efficiently curates a set of street view images containing crop fields, trains a model to predict crop type by utilizing weakly-label
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#37325;&#20851;&#31995;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#26174;&#24615;&#21644;&#38544;&#24615;&#20851;&#31995;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#32593;&#32476;&#24182;&#25552;&#21319;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02169</link><description>&lt;p&gt;
&#21452;&#37325;&#20851;&#31995;&#23545;&#40784;&#29992;&#20110;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dual Relation Alignment for Composed Image Retrieval. (arXiv:2309.02169v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#37325;&#20851;&#31995;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#26174;&#24615;&#21644;&#38544;&#24615;&#20851;&#31995;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#32593;&#32476;&#24182;&#25552;&#21319;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#26159;&#19968;&#39033;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#22270;&#20687;&#21644;&#34917;&#20805;&#25991;&#26412;&#20316;&#20026;&#26597;&#35810;&#65292;&#26469;&#25628;&#32034;&#30446;&#26631;&#22270;&#20687;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#36328;&#27169;&#24577;&#24314;&#27169;&#30340;&#36827;&#23637;&#65292;&#36825;&#19968;&#20219;&#21153;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#19982;&#20165;&#23384;&#22312;&#19968;&#31181;&#23545;&#40784;&#20851;&#31995;&#30340;&#19968;&#33324;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#38382;&#39064;&#19981;&#21516;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20013;&#23384;&#22312;&#30528;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#12290;&#26174;&#24615;&#20851;&#31995;&#28041;&#21450;&#21442;&#32771;&#22270;&#20687; &amp; &#34917;&#20805;&#25991;&#26412;-&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#26159;&#29616;&#26377;&#26041;&#27861;&#24120;&#29992;&#30340;&#20851;&#31995;&#12290;&#38500;&#20102;&#36825;&#31181;&#30452;&#35266;&#20851;&#31995;&#20043;&#22806;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#21478;&#19968;&#31181;&#38544;&#21547;&#20294;&#20851;&#38190;&#30340;&#20851;&#31995;&#65292;&#21363;&#21442;&#32771;&#22270;&#20687; &amp; &#30446;&#26631;&#22270;&#20687;-&#34917;&#20805;&#25991;&#26412;&#12290;&#22240;&#20026;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#30740;&#31350;&#30446;&#26631;&#22270;&#20687;&#21644;&#21442;&#32771;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#34917;&#20805;&#25991;&#26412;&#12290;&#21487;&#24796;&#30340;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21033;&#29992;&#26174;&#24615;&#20851;&#31995;&#26469;&#23398;&#20064;&#32593;&#32476;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval, a task involving the search for a target image using a reference image and a complementary text as the query, has witnessed significant advancements owing to the progress made in cross-modal modeling. Unlike the general image-text retrieval problem with only one alignment relation, i.e., image-text, we argue for the existence of two types of relations in composed image retrieval. The explicit relation pertains to the reference image &amp; complementary text-target image, which is commonly exploited by existing methods. Besides this intuitive relation, the observations during our practice have uncovered another implicit yet crucial relation, i.e., reference image &amp; target image-complementary text, since we found that the complementary text can be inferred by studying the relation between the target image and the reference image. Regrettably, existing methods largely focus on leveraging the explicit relation to learn their networks, while overlooking the implicit re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.13352</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23436;&#20840;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#27745;&#26579;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#35299;&#20915;&#20102;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#20219;&#21153;&#12290;&#36825;&#20123;&#31639;&#27861;&#20013;&#22823;&#22810;&#25968;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#23545;&#19968;&#20010;&#22522;&#20110;&#27531;&#24046;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26681;&#25454;&#26410;&#35265;&#26679;&#26412;&#19982;&#23398;&#20064;&#21040;&#30340;&#27491;&#24120;&#33539;&#22260;&#30340;&#19981;&#30456;&#20284;&#24615;&#26469;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#21487;&#20197;&#29992;&#26080;&#24322;&#24120;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#20250;&#19982;&#19968;&#23450;&#27604;&#20363;&#30340;&#24322;&#24120;&#26679;&#26412;&#28151;&#21512;&#12290;&#32780;&#21033;&#29992;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24517;&#28982;&#20250;&#23548;&#33268;&#22522;&#20110;&#27531;&#24046;&#30340;&#31639;&#27861;&#30340;AD&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#29992;&#20110;AD&#20219;&#21153;&#30340;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#30340;&#25913;&#36827;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#27531;&#24046;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22810;&#20803;&#26102;&#38388;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11234</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#20248;&#21270;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding. (arXiv:2308.11234v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#35201;&#27714;&#20026;&#19968;&#20010;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#35745;&#31639;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#25152;&#26377;&#26234;&#33021;&#20307;&#37117;&#22312;&#20849;&#20139;&#22320;&#22270;&#19978;&#31227;&#21160;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#30456;&#20851;&#30740;&#31350;&#65292;&#20294;&#24403;&#21069;&#30340;&#31639;&#27861;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#37117;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35268;&#21010;&#33258;&#30001;&#27969;&#21160;&#30340;&#26368;&#20248;&#36335;&#24452;&#65292;&#36825;&#20250;&#23548;&#33268;&#25317;&#22581;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MAPF&#26041;&#27861;&#65292;&#36890;&#36807;&#36319;&#38543;&#36991;&#20813;&#25317;&#22581;&#30340;&#36335;&#24452;&#26469;&#24341;&#23548;&#26234;&#33021;&#20307;&#21040;&#36798;&#30446;&#30340;&#22320;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#36825;&#20010;&#24819;&#27861;&#65306;&#19968;&#27425;&#24615;MAPF&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#26377;&#19968;&#20010;&#30446;&#30340;&#22320;&#65292;&#20197;&#21450;&#32456;&#36523;MAPF&#65292;&#26234;&#33021;&#20307;&#19981;&#26029;&#34987;&#20998;&#37197;&#26032;&#20219;&#21153;&#12290;&#23545;&#20110;&#19968;&#27425;&#24615;MAPF&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#32456;&#36523;MAPF&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#24635;&#20307;&#21534;&#21520;&#37327;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that asks us to compute collision-free paths for a team of agents, all moving across a shared map. Although many works appear on this topic, all current algorithms struggle as the number of agents grows. The principal reason is that existing approaches typically plan free-flow optimal paths, which creates congestion. To tackle this issue we propose a new approach for MAPF where agents are guided to their destination by following congestion-avoiding paths. We evaluate the idea in two large-scale settings: one-shot MAPF, where each agent has a single destination, and lifelong MAPF, where agents are continuously assigned new tasks. For one-shot MAPF we show that our approach substantially improves solution quality. For Lifelong MAPF we report large improvements in overall throughput.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcatPlexer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#20351;&#29992;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#26469;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21463;&#21040;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.11199</link><description>&lt;p&gt;
ConcatPlexer&#65306;&#36890;&#36807;&#38468;&#21152;Dim1&#25209;&#22788;&#29702;&#20197;&#21152;&#24555;ViTs&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
ConcatPlexer: Additional Dim1 Batching for Faster ViTs. (arXiv:2308.11199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcatPlexer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#20351;&#29992;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#26469;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21463;&#21040;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36824;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;Transformer&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#24314;&#27169;&#28789;&#27963;&#24615;&#24102;&#26469;&#20102;&#35745;&#31639;&#25104;&#26412;&#30340;&#20005;&#37325;&#22686;&#21152;&#65292;&#22240;&#27492;&#26377;&#20960;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20943;&#23569;&#36825;&#31181;&#36127;&#25285;&#30340;&#26041;&#27861;&#12290;&#21463;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#20943;&#23569;&#25104;&#26412;&#30340;&#26041;&#27861;Data Multiplexing (DataMUX)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#35270;&#35273;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#65292;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#22522;&#30784;&#19978;&#22823;&#22823;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#35270;&#35273;&#27169;&#22411;&#24341;&#20837;&#20102;DataMux&#30340;&#19968;&#31181;&#22825;&#28982;&#36866;&#24212;&#26041;&#27861;&#65292;&#22270;&#20687;&#22810;&#36335;&#22797;&#29992;&#22120;&#65288;Image Multiplexer&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#32452;&#20214;&#26469;&#20811;&#26381;&#20854;&#32570;&#28857;&#65292;&#36827;&#32780;&#24418;&#25104;&#20102;&#25105;&#20204;&#26368;&#32456;&#30340;&#27169;&#22411;ConcatPlexer&#65292;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#28857;&#12290;ConcatPlexer&#22312;ImageNet1K&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24341;&#23548;&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#29305;&#24449;&#21644;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#26469;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#20195;&#29702;&#30340;&#33322;&#28857;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02126</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#24341;&#23548;&#30340;&#22522;&#20110;Transformer&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#25913;&#36827;&#33322;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semantics-guided Transformer-based Sensor Fusion for Improved Waypoint Prediction. (arXiv:2308.02126v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24341;&#23548;&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#29305;&#24449;&#21644;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#26469;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#20195;&#29702;&#30340;&#33322;&#28857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26234;&#33021;&#33258;&#21160;&#39550;&#39542;&#20195;&#29702;&#26469;&#35828;&#65292;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#20173;&#28982;&#26159;&#39550;&#39542;&#22330;&#26223;&#29702;&#35299;&#30340;&#20851;&#38190;&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#35270;&#35273;&#20840;&#23616;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#23616;&#37096;&#33322;&#28857;&#39044;&#27979;&#20219;&#21153;&#65292;&#21333;&#27169;&#24577;&#32593;&#32476;&#20173;&#28982;&#21463;&#38480;&#20110;&#23545;&#36755;&#20837;&#20256;&#24863;&#22120;&#30340;&#28789;&#25935;&#24230;&#30340;&#24378;&#20381;&#36182;&#24615;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#24037;&#20316;&#25512;&#24191;&#20102;&#22312;&#29305;&#24449;&#32423;&#21035;&#19978;&#34701;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#20351;&#29992;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#33021;&#20419;&#36827;&#30456;&#20114;&#30340;&#19978;&#19979;&#25991;&#20132;&#20114;&#65292;&#20294;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#23454;&#26102;&#36827;&#34892;&#20840;&#23616;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#24182;&#36827;&#34892;&#26368;&#23567;&#35745;&#31639;&#65292;&#22240;&#27492;&#22312;&#32473;&#23450;&#26377;&#38480;&#25968;&#37327;&#30340;&#21487;&#23454;&#38469;&#20351;&#29992;&#30340;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#35757;&#32451;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#26356;&#21152;&#31361;&#20986;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#21512;&#19982;&#30446;&#26631;&#20219;&#21153;&#65288;&#22914;&#20132;&#36890;&#28783;&#35782;&#21035;&#21644;&#35821;&#20041;&#20998;&#21106;&#65289;&#39640;&#24230;&#30456;&#20851;&#30340;&#31934;&#24515;&#36873;&#21462;&#30340;&#36741;&#21161;&#20219;&#21153;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#22836;&#20026;&#33322;&#28857;&#39044;&#27979;&#36827;&#34892;&#20102;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensor fusion approaches for intelligent self-driving agents remain key to driving scene understanding given visual global contexts acquired from input sensors. Specifically, for the local waypoint prediction task, single-modality networks are still limited by strong dependency on the sensitivity of the input sensor, and thus recent works promote the use of multiple sensors in fusion in feature level. While it is well known that multiple data modalities promote mutual contextual exchange, deployment to practical driving scenarios requires global 3D scene understanding in real-time with minimal computations, thus placing greater significance on training strategies given a limited number of practically usable sensors. In this light, we exploit carefully selected auxiliary tasks that are highly correlated with the target task of interest (e.g., traffic light recognition and semantic segmentation) by fusing auxiliary task features and also using auxiliary heads for waypoint prediction base
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.15176</link><description>&lt;p&gt;
RCT&#25298;&#32477;&#25277;&#26679;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26080;&#20559;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#39640;&#32500;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#22914;&#25991;&#26412;&#25968;&#25454;&#12289;&#22522;&#22240;&#32452;&#23398;&#25110;&#34892;&#20026;&#31038;&#20250;&#31185;&#23398;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#25972;&#26041;&#27861;&#30340;&#32463;&#39564;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32463;&#39564;&#35780;&#20272;&#31574;&#30053;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#65306;&#23545;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#36827;&#34892;&#23376;&#25277;&#26679;&#65292;&#20197;&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#22240;&#26524;&#35782;&#21035;&#25104;&#31435;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#22522;&#20934;RCT&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
&lt;/p&gt;</description></item><item><title>GEAR&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#24037;&#20855;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#24037;&#20855;&#23545;&#24212;&#21644;&#25191;&#34892;&#20998;&#21035;&#22996;&#25176;&#32473;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#19981;&#20381;&#36182;&#20219;&#21153;&#31034;&#33539;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08775</link><description>&lt;p&gt;
GEAR: &#19982;&#36890;&#29992;&#21270;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution. (arXiv:2307.08775v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08775
&lt;/p&gt;
&lt;p&gt;
GEAR&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#24037;&#20855;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#24037;&#20855;&#23545;&#24212;&#21644;&#25191;&#34892;&#20998;&#21035;&#22996;&#25176;&#32473;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#19981;&#20381;&#36182;&#20219;&#21153;&#31034;&#33539;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#22806;&#37096;&#24037;&#20855;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36807;&#20110;&#20381;&#36182;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#20351;&#29992;&#31034;&#33539;&#65292;&#38480;&#21046;&#20102;&#20854;&#36890;&#29992;&#24615;&#65292;&#24182;&#19988;&#30001;&#20110;&#23545;&#22823;&#35268;&#27169;LLM&#36827;&#34892;&#22810;&#27425;&#35843;&#29992;&#32780;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GEAR&#65292;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26597;&#35810;-&#24037;&#20855;&#23545;&#24212;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#20381;&#36182;&#29305;&#23450;&#20219;&#21153;&#31034;&#33539;&#30340;&#21508;&#31181;&#38656;&#35201;&#20351;&#29992;&#24037;&#20855;&#30340;&#20219;&#21153;&#12290;GEAR&#36890;&#36807;&#23558;&#24037;&#20855;&#23545;&#24212;&#21644;&#25191;&#34892;&#20998;&#21035;&#22996;&#25176;&#32473;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#21644;LLM&#26469;&#23454;&#29616;&#26356;&#39640;&#30340;&#25928;&#29575;&#65307;&#21516;&#26102;&#21033;&#29992;&#35821;&#20041;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#35780;&#20272;&#22312;&#38382;&#39064;&#21644;&#31572;&#26696;&#32423;&#21035;&#19978;&#36827;&#34892;&#36890;&#29992;&#21270;&#30340;&#24037;&#20855;&#23545;&#24212;&#12290;&#25105;&#20204;&#22312;6&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;14&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GEAR&#65292;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#26032;&#20219;&#21153;&#12289;&#26032;&#24037;&#20855;&#21644;&#19981;&#21516;SLM&#30340;&#24378;&#22823;&#36890;&#29992;&#24615;&#12290;&#23613;&#31649;&#25552;&#20379;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#20294;GEAR&#22312;&#24037;&#20855;&#23545;&#24212;&#20013;&#30340;&#31934;&#30830;&#24615;&#27604;&#20351;&#29992;LLM&#30340;&#20808;&#21069;&#31574;&#30053;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLM) to use external tools enhances their performance across a variety of tasks. However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs. We introduce GEAR, a computationally efficient query-tool grounding algorithm that is generalizable to various tasks that require tool use while not relying on task-specific demonstrations. GEAR achieves better efficiency by delegating tool grounding and execution to small language models (SLM) and LLM, respectively; while leveraging semantic and pattern-based evaluation at both question and answer levels for generalizable tool grounding. We evaluate GEAR on 14 datasets across 6 downstream tasks, demonstrating its strong generalizability to novel tasks, tools and different SLMs. Despite offering more efficiency, GEAR achieves higher precision in tool grounding compared to prior strategies using LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2205.09622</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#20844;&#24179;&#24615;&#65311;&#21746;&#23398;&#30340;&#24605;&#32771;&#19982;&#23545;fairML&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#24615;&#20154;&#24037;&#26234;&#33021;(fairML)&#39046;&#22495;&#65292;&#36890;&#36807;&#23450;&#20041;&#34913;&#37327;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#21644;&#25552;&#20986;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#25968;&#25454;&#20855;&#26377;&#20302;&#20844;&#24179;&#24615;&#24230;&#37327;&#20540;&#30340;&#26041;&#27861;&#65292;&#26469;&#20943;&#36731;&#20154;&#24037;&#26234;&#33021;(ML)&#20135;&#29983;&#30340;&#30456;&#20851;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21363;"&#20844;&#24179;&#26159;&#20160;&#20040;"&#65292;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#36825;&#36896;&#25104;&#20102;&#20844;&#24179;&#24615;&#30740;&#31350;&#22312;&#21746;&#23398;&#39046;&#22495;&#20960;&#20010;&#19990;&#32426;&#30340;&#35752;&#35770;&#19982;&#36817;&#26399;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#33268;&#24615;&#20844;&#24179;&#27010;&#24565;&#21644;&#23558;&#21746;&#23398;&#24605;&#32771;&#36716;&#21270;&#20026;ADM&#31995;&#32479;&#20013;ML&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#26469;&#26550;&#36215;&#36825;&#19968;&#40511;&#27807;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#21487;&#33021;&#24050;&#32463;&#23384;&#22312;&#65292;&#21363;&#20351;&#27809;&#26377;&#21463;&#20445;&#25252;&#24615;&#23646;&#24615;&#30340;&#23384;&#22312;&#65292;&#24378;&#35843;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#32780;&#26159;&#21069;&#32773;&#23454;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#24378;&#35843;&#23558;&#20262;&#29702;&#32771;&#34385;&#32435;&#20837;ML&#31649;&#36947;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#37096;&#32626;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
&lt;/p&gt;</description></item></channel></rss>