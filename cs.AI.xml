<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Moreau&#21253;&#32476;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;MEMRL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;Moreau&#21253;&#32476;&#20195;&#29702;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#20219;&#21153;&#20998;&#24067;&#30340;&#20803;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.12216</link><description>&lt;p&gt;
&#20851;&#20110;Moreau&#21253;&#32476;&#30340;&#19968;&#38454;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On First-Order Meta-Reinforcement Learning with Moreau Envelopes. (arXiv:2305.12216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Moreau&#21253;&#32476;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;MEMRL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;Moreau&#21253;&#32476;&#20195;&#29702;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#20219;&#21153;&#20998;&#24067;&#30340;&#20803;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;(MRL)&#26159;&#19968;&#31181;&#35757;&#32451;&#26234;&#33021;&#20307;&#22312;&#26032;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#24555;&#36895;&#36866;&#24212;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;MRL&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Moreau&#21253;&#32476;&#20195;&#29702;&#27491;&#21017;&#21270;&#22120;&#26469;&#20849;&#21516;&#23398;&#20064;&#21487;&#20197;&#36866;&#24212;&#27599;&#20010;&#20219;&#21153;&#29615;&#22659;&#30340;&#20803;&#31574;&#30053;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#31216;&#20026;Moreau&#21253;&#32476;&#20803;&#24378;&#21270;&#23398;&#20064;(MEMRL)&#65292;&#23427;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#21644;Moreau&#21253;&#32476;&#27491;&#21017;&#21270;&#30340;&#32452;&#21512;&#26377;&#25928;&#22320;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#65292;&#23398;&#20064;&#21487;&#20197;&#36866;&#24212;&#20219;&#21153;&#20998;&#24067;&#30340;&#20803;&#31574;&#30053;&#12290;Moreau&#21253;&#32476;&#25552;&#20379;&#20102;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#30340;&#24179;&#28369;&#36817;&#20284;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24212;&#29992;&#26631;&#20934;&#20248;&#21270;&#25216;&#26415;&#24182;&#25910;&#25947;&#21040;&#36866;&#24403;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#23545;MEMRL&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#38750;&#20984;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#20197;&#20122;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Reinforcement Learning (MRL) is a promising framework for training agents that can quickly adapt to new environments and tasks. In this work, we study the MRL problem under the policy gradient formulation, where we propose a novel algorithm that uses Moreau envelope surrogate regularizers to jointly learn a meta-policy that is adjustable to the environment of each individual task. Our algorithm, called Moreau Envelope Meta-Reinforcement Learning (MEMRL), learns a meta-policy that can adapt to a distribution of tasks by efficiently updating the policy parameters using a combination of gradient-based optimization and Moreau Envelope regularization. Moreau Envelopes provide a smooth approximation of the policy optimization problem, which enables us to apply standard optimization techniques and converge to an appropriate stationary point. We provide a detailed analysis of the MEMRL algorithm, where we show a sublinear convergence rate to a first-order stationary point for non-convex p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#20302;&#36164;&#28304;&#24773;&#22659;&#20013;&#30340;&#21333;&#21475;&#21916;&#21095;&#25991;&#26412;&#29983;&#25104;&#35821;&#38899;&#31995;&#32479;ComedicSpeech&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#35821;&#35843;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#35843;&#34920;&#31034;&#24182;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#23558;&#20854;&#23450;&#21521;&#21040;TTS&#27169;&#22411;&#65292;&#21516;&#26102;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#22120;&#26469;&#22686;&#24378;&#20010;&#20154;&#38901;&#24459;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19982;&#21916;&#21095;&#28436;&#21592;&#30456;&#20851;&#30340;&#29305;&#27530;&#26631;&#35760;&#26469;&#27169;&#25311;&#20010;&#20154;&#22635;&#20805;&#35821;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;ComedicSpeech&#22312;&#20165;&#26377;&#21313;&#20998;&#38047;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#27604;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#34920;&#29616;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12200</link><description>&lt;p&gt;
ComedicSpeech: &#29992;&#20110;&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#21333;&#21475;&#21916;&#21095;&#25991;&#26412;&#29983;&#25104;&#35821;&#38899;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios. (arXiv:2305.12200v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#20302;&#36164;&#28304;&#24773;&#22659;&#20013;&#30340;&#21333;&#21475;&#21916;&#21095;&#25991;&#26412;&#29983;&#25104;&#35821;&#38899;&#31995;&#32479;ComedicSpeech&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#35821;&#35843;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#35843;&#34920;&#31034;&#24182;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#23558;&#20854;&#23450;&#21521;&#21040;TTS&#27169;&#22411;&#65292;&#21516;&#26102;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#22120;&#26469;&#22686;&#24378;&#20010;&#20154;&#38901;&#24459;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19982;&#21916;&#21095;&#28436;&#21592;&#30456;&#20851;&#30340;&#29305;&#27530;&#26631;&#35760;&#26469;&#27169;&#25311;&#20010;&#20154;&#22635;&#20805;&#35821;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;ComedicSpeech&#22312;&#20165;&#26377;&#21313;&#20998;&#38047;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#27604;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#27169;&#22411;&#21487;&#29983;&#25104;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#65292;&#20294;&#22312;&#21512;&#25104;&#20855;&#26377;&#25103;&#21095;&#24615;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#26102;&#65288;&#22914;&#21333;&#21475;&#21916;&#21095;&#65289;&#65292;&#20854;&#34920;&#29616;&#21147;&#19981;&#22815;&#12290;&#32771;&#34385;&#21040;&#21916;&#21095;&#28436;&#21592;&#26377;&#30528;&#22810;&#26679;&#30340;&#20010;&#20154;&#28436;&#35762;&#39118;&#26684;&#65292;&#21253;&#25324;&#20010;&#20154;&#35821;&#35843;&#12289;&#38901;&#24459;&#21644;&#22635;&#20805;&#35821;&#65292;&#36825;&#38656;&#35201;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21644;&#24378;&#22823;&#30340;&#35821;&#38899;&#39118;&#26684;&#24314;&#27169;&#33021;&#21147;&#65292;&#36825;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;ComedicSpeech&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#21333;&#21475;&#21916;&#21095;&#21512;&#25104;&#30340;TTS&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#35843;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#35843;&#34920;&#31034;&#65292;&#24182;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#23558;&#20854;&#23450;&#21521;&#21040;TTS&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#22120;&#22686;&#24378;&#20102;&#20010;&#20154;&#38901;&#24459;&#24314;&#27169;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19982;&#21916;&#21095;&#28436;&#21592;&#30456;&#20851;&#30340;&#29305;&#27530;&#26631;&#35760;&#26469;&#27169;&#25311;&#20010;&#20154;&#22635;&#20805;&#35821;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;ComedicSpeech&#22312;&#27599;&#20010;&#21916;&#21095;&#28436;&#21592;&#20165;&#26377;&#21313;&#20998;&#38047;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#27604;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#34920;&#29616;&#21147;&#12290;&#38899;&#39057;&#26679;&#26412;&#21487;&#22312;https://bit.ly/3dxm5Ol&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian. The audio samples are ava
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#20351;&#29992;&#28508;&#22312;&#23884;&#20837;&#26469;&#27169;&#25311;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23884;&#20837;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#19977;&#20010;&#27491;&#30830;&#38271;&#26399;&#34892;&#20026;&#27979;&#35797;&#30340;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12185</link><description>&lt;p&gt;
&#25105;&#20204;&#38656;&#35201;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#27169;&#25311;&#32593;&#32476;&#19978;&#30340;&#21160;&#21147;&#31995;&#32479;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do We Need an Encoder-Decoder to Model Dynamical Systems on Networks?. (arXiv:2305.12185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#20351;&#29992;&#28508;&#22312;&#23884;&#20837;&#26469;&#27169;&#25311;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23884;&#20837;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#19977;&#20010;&#27491;&#30830;&#38271;&#26399;&#34892;&#20026;&#27979;&#35797;&#30340;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#27169;&#25311;&#21160;&#21147;&#31995;&#32479;&#26041;&#38754;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#19982;&#32593;&#32476;&#21160;&#21147;&#23398;&#24314;&#27169;&#30456;&#20851;&#19988;&#26410;&#34987;&#37325;&#35270;&#30340;&#35823;&#35299;&#12290;&#21463;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#28872;&#24433;&#21709;&#65292;&#28508;&#22312;&#39030;&#28857;&#23884;&#20837;&#34987;&#33258;&#28982;&#22320;&#37319;&#29992;&#22312;&#35768;&#22810;&#31070;&#32463;&#21160;&#21147;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23884;&#20837;&#20542;&#21521;&#20110;&#20135;&#29983;&#36866;&#24212;&#35266;&#27979;&#33391;&#22909;&#20294;&#21516;&#26102;&#20855;&#26377;&#38169;&#35823;&#21160;&#21147;&#34892;&#20026;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#27491;&#30830;&#38271;&#26399;&#34892;&#20026;&#30340;&#27979;&#35797;&#65292;&#24182;&#35828;&#26126;&#20102;&#23884;&#20837;&#24335;&#21160;&#21147;&#27169;&#22411;&#22914;&#20309;&#26410;&#36890;&#36807;&#36825;&#20123;&#27979;&#35797;&#65292;&#24182;&#20998;&#26512;&#20854;&#20013;&#30340;&#21407;&#22240;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#25299;&#25169;&#20849;&#36717;&#30340;&#35282;&#24230;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#34920;&#26126;&#36991;&#20813;&#20351;&#29992;&#23884;&#20837;&#21487;&#20197;&#36991;&#20813;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#21442;&#25968;&#21270;&#20004;&#20010;&#21152;&#24615;&#30690;&#37327;&#22330;&#20998;&#37327;&#30340;&#26080;&#23884;&#20837;&#26367;&#20195;&#26041;&#26696;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning gains popularity in modelling dynamical systems, we expose an underappreciated misunderstanding relevant to modelling dynamics on networks. Strongly influenced by graph neural networks, latent vertex embeddings are naturally adopted in many neural dynamical network models. However, we show that embeddings tend to induce a model that fits observations well but simultaneously has incorrect dynamical behaviours. Recognising that previous studies narrowly focus on short-term predictions during the transient phase of a flow, we propose three tests for correct long-term behaviour, and illustrate how an embedding-based dynamical model fails these tests, and analyse the causes, particularly through the lens of topological conjugacy. In doing so, we show that the difficulties can be avoided by not using embedding. We propose a simple embedding-free alternative based on parametrising two additive vector-field components. Through extensive experiments, we verify that the proposed
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35299;&#37322;&#26435;&#21033;&#38590;&#20197;&#20805;&#20998;&#35299;&#37322;&#27861;&#24459;&#30446;&#30340;&#65292;&#28041;&#21450;&#21040;&#20844;&#27491;&#21028;&#20915;&#12289;&#27491;&#24403;&#31243;&#24207;&#12289;&#20154;&#31867;&#26426;&#26500;&#35748;&#35777;&#21644;&#20915;&#31574;&#32773;&#26435;&#23041;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12167</link><description>&lt;p&gt;
&#21453;&#23545;&#35299;&#37322;&#24615;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
The Case Against Explainability. (arXiv:2305.12167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12167
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35299;&#37322;&#26435;&#21033;&#38590;&#20197;&#20805;&#20998;&#35299;&#37322;&#27861;&#24459;&#30446;&#30340;&#65292;&#28041;&#21450;&#21040;&#20844;&#27491;&#21028;&#20915;&#12289;&#27491;&#24403;&#31243;&#24207;&#12289;&#20154;&#31867;&#26426;&#26500;&#35748;&#35777;&#21644;&#20915;&#31574;&#32773;&#26435;&#23041;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#26222;&#21450;&#65292;&#30417;&#31649;&#26426;&#26500;&#23545;&#20110;&#36825;&#20123;&#31995;&#32479;&#25152;&#20570;&#20915;&#31574;&#30340;&#35299;&#37322;&#35201;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#26435;&#21033;&#30340;&#38656;&#27714;&#21644;&#33021;&#21147;&#20043;&#38388;&#20173;&#23384;&#22312;&#24040;&#22823;&#30340;&#24046;&#36317;&#12290;AI&#31995;&#32479;&#8220;&#35299;&#37322;&#26435;&#21033;&#8221;&#30340;&#30417;&#31649;&#21628;&#22768;&#21487;&#24402;&#22240;&#20110;&#27861;&#24459;&#20013;&#35299;&#37322;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27861;&#24459;&#20013;&#35299;&#37322;&#30340;&#30446;&#30340;&#65292;&#20197;&#20998;&#26512;&#32456;&#31471;&#29992;&#25143;&#21487;&#35299;&#37322;&#24615;&#25152;&#25552;&#20379;&#30340;&#21407;&#22240;&#26159;&#21542;&#36275;&#20197;&#28385;&#36275;&#36825;&#20123;&#30446;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#27861;&#24459;&#20013;&#35299;&#37322;&#30340;&#30446;&#30340;&#21253;&#25324;&#65306;(a)&#20570;&#20986;&#26356;&#22909;&#30340;&#12289;&#26356;&#20844;&#27491;&#30340;&#20915;&#23450;&#65292;(b)&#20419;&#36827;&#27491;&#24403;&#31243;&#24207;&#65292;(c)&#37492;&#23450;&#20154;&#31867;&#26426;&#26500;&#65292;(d)&#22686;&#24378;&#20915;&#31574;&#32773;&#30340;&#26435;&#23041;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32456;&#31471;&#29992;&#25143;&#21487;&#35299;&#37322;&#24615;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) becomes more prevalent there is a growing demand from regulators to accompany decisions made by such systems with explanations. However, a persistent gap exists between the need to execute a meaningful right to explanation vs. the ability of Machine Learning systems to deliver on such a legal requirement. The regulatory appeal towards "a right to explanation" of AI systems can be attributed to the significant role of explanations, part of the notion called reason-giving, in law. Therefore, in this work we examine reason-giving's purposes in law to analyze whether reasons provided by end-user Explainability can adequately fulfill them.  We find that reason-giving's legal purposes include: (a) making a better and more just decision, (b) facilitating due-process, (c) authenticating human agency, and (d) enhancing the decision makers' authority. Using this methodology, we demonstrate end-user Explainabilty's inadequacy to fulfil reason-giving's role in law, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12162</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;DSIC&#20223;&#23556;&#26497;&#22823;&#20215;&#25293;&#21334;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25293;&#21334;&#35774;&#35745;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23547;&#25214;&#32463;&#39564;&#19978;&#39640;&#25910;&#20837;&#30340;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;&#22810;&#29289;&#21697;&#25293;&#21334;&#24773;&#26223;&#30340;&#24037;&#20316;&#21487;&#20197;&#31895;&#30053;&#22320;&#20998;&#20026;RegretNet&#31867;&#21644;&#20223;&#23556;&#26497;&#22823;&#20215;&#65288;AMAs&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#19981;&#33021;&#20005;&#26684;&#20445;&#35777;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;DSIC&#65289;&#65292;&#32780;&#21518;&#32773;&#22240;&#20026;&#20998;&#37197;&#20505;&#36873;&#20154;&#25968;&#36807;&#22810;&#32780;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMenuNet&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20174;&#20986;&#20215;&#20154;&#21644;&#29289;&#21697;&#34920;&#31034;&#20013;&#26500;&#36896;AMA&#21442;&#25968;&#65288;&#29978;&#33267;&#21253;&#25324;&#20998;&#37197;&#33756;&#21333;&#65289;&#12290;&#30001;&#20110;AMA&#30340;&#23646;&#24615;&#65292;AMenuNet&#22987;&#32456;&#26159;DSIC&#21644;&#20010;&#20154;&#29702;&#24615;&#65288;IR&#65289;&#30340;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#26469;&#22686;&#24378;&#21487;&#20280;&#32553;&#24615;&#12290;&#27492;&#22806;&#65292;AMenuNet&#26159;&#32622;&#25442;&#31561;&#21464;&#30340;&#65292;&#20854;&#21442;&#25968;&#25968;&#37327;&#19981;&#21463;&#25293;&#21334;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;AMenuNet&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LogiCoT, &#19968;&#20010;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25945;&#25480;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#21644;&#24341;&#20986;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12147</link><description>&lt;p&gt;
LogiCoT&#65306;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4. (arXiv:2305.12147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LogiCoT, &#19968;&#20010;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25945;&#25480;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#21644;&#24341;&#20986;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;4&#65288;GPT-4&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#33258;&#25105;&#25351;&#23548;&#35843;&#25972;&#30740;&#31350;&#65288;&#22914;Alpaca&#65289;&#20391;&#37325;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;&#36825;&#20123;&#25351;&#20196;&#20351;&#27169;&#22411;&#22312;&#19968;&#33324;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#21644;&#37322;&#20041;&#65289;&#19978;&#33021;&#22815;&#36798;&#21040;&#19982;GPT-3.5&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LogiCoT&#65292;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#25910;&#38598;&#25351;&#20196;&#20197;&#25552;&#31034;GPT-4&#29983;&#25104;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;LogiCoT&#20316;&#20026;&#25945;&#25480;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#25351;&#20196;&#38598;&#65292;&#24182;&#24341;&#20986;&#20102;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.
&lt;/p&gt;</description></item><item><title>DiffCap&#36890;&#36807;&#24212;&#29992;&#36830;&#32493;&#25193;&#25955;&#65292;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#36716;&#25442;&#31163;&#25955;&#26631;&#35760;&#65292;&#24182;&#25104;&#21151;&#34701;&#21512;&#25552;&#21462;&#30340;&#22270;&#20687;&#29305;&#24449;&#36827;&#34892;&#25193;&#25955;&#23383;&#24149;&#29983;&#25104;&#65292;&#20854;&#20855;&#26377;&#26356;&#22810;&#30340;&#35299;&#30721;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12144</link><description>&lt;p&gt;
DiffCap&#65306;&#25506;&#32034;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#36830;&#32493;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
DiffCap: Exploring Continuous Diffusion on Image Captioning. (arXiv:2305.12144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12144
&lt;/p&gt;
&lt;p&gt;
DiffCap&#36890;&#36807;&#24212;&#29992;&#36830;&#32493;&#25193;&#25955;&#65292;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#36716;&#25442;&#31163;&#25955;&#26631;&#35760;&#65292;&#24182;&#25104;&#21151;&#34701;&#21512;&#25552;&#21462;&#30340;&#22270;&#20687;&#29305;&#24449;&#36827;&#34892;&#25193;&#25955;&#23383;&#24149;&#29983;&#25104;&#65292;&#20854;&#20855;&#26377;&#26356;&#22810;&#30340;&#35299;&#30721;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#25551;&#36848;&#19978;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#25551;&#36848;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#31181;&#26041;&#24335;&#21487;&#20197;&#24102;&#26469;&#26356;&#22810;&#30340;&#35299;&#30721;&#22810;&#26679;&#24615;&#12290;&#21463;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#33258;&#28982;&#22270;&#20687;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DiffCap&#65292;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20013;&#24212;&#29992;&#36830;&#32493;&#25193;&#25955;&#12290;&#19982;&#36755;&#20986;&#22823;&#23567;&#36830;&#32493;&#30340;&#22270;&#20687;&#29983;&#25104;&#19981;&#21516;&#65292;&#22270;&#20687;&#25551;&#36848;&#38271;&#24230;&#38543;&#30528;&#31163;&#25955;&#26631;&#35760;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#36716;&#25442;&#31163;&#25955;&#26631;&#35760;&#65292;&#24182;&#23545;&#23427;&#20204;&#24212;&#29992;&#36830;&#32493;&#25193;&#25955;&#65292;&#20197;&#25104;&#21151;&#34701;&#21512;&#25552;&#21462;&#30340;&#22270;&#20687;&#29305;&#24449;&#36827;&#34892;&#25193;&#25955;&#23383;&#24149;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#32467;&#26500;&#65292;&#21363;&#21487;&#36798;&#21040;&#19982;&#20197;&#24448;&#38750;&#33258;&#22238;&#24402;&#20316;&#21697;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#38500;&#20102;&#36136;&#37327;&#65292;DiffCap&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#24615;&#26159;&#20854;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#39640;&#22810;&#26679;&#24615;&#65292;&#36825;&#22312;&#35768;&#22810;&#33258;&#22238;&#24402;&#20316;&#21697;&#20013;&#37117;&#32570;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current image captioning works usually focus on generating descriptions in an autoregressive manner. However, there are limited works that focus on generating descriptions non-autoregressively, which brings more decoding diversity. Inspired by the success of diffusion models on generating natural-looking images, we propose a novel method DiffCap to apply continuous diffusions on image captioning. Unlike image generation where the output is fixed-size and continuous, image description length varies with discrete tokens. Our method transforms discrete tokens in a natural way and applies continuous diffusion on them to successfully fuse extracted image features for diffusion caption generation. Our experiments on COCO dataset demonstrate that our method uses a much simpler structure to achieve comparable results to the previous non-autoregressive works. Apart from quality, an intriguing property of DiffCap is its high diversity during generation, which is missing from many autoregressive 
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;ChatGPT&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#37096;&#20998;&#21151;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12138</link><description>&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#65306;&#20840;&#38754;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Scope of ChatGPT in Software Engineering: A Thorough Investigation. (arXiv:2305.12138v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12138
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;ChatGPT&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#37096;&#20998;&#21151;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#23637;&#31034;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#36716;&#21270;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#34920;&#29616;&#20986;&#22312;&#20195;&#30721;&#21644;&#25991;&#26723;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36719;&#20214;&#24037;&#31243;&#38656;&#35201;&#39640;&#21487;&#38752;&#24615;&#21644;&#39118;&#38505;&#25511;&#21046;&#65292;&#20351;ChatGPT&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;AI&#27169;&#22411;&#24212;&#23545;SE&#20219;&#21153;&#25152;&#38656;&#30340;&#33021;&#21147;&#20998;&#20026;&#19977;&#31867;&#65306;1&#65289;&#35821;&#27861;&#29702;&#35299;&#65292;2&#65289;&#38745;&#24577;&#34892;&#20026;&#29702;&#35299;&#65292;&#21644;3&#65289;&#21160;&#24577;&#34892;&#20026;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#37325;&#28857;&#26159;ChatGPT&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#12289;&#25511;&#21046;&#27969;&#31243;&#22270;&#65288;CFG&#65289;&#21644;&#35843;&#29992;&#22270;&#65288;CG&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#28041;&#21450;C&#12289;Java&#12289;Python&#21644;Solidity&#30340;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;ChatGPT&#34920;&#29616;&#20986;&#20102;&#23545;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#65288;AST&#65289;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#20294;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#21644;&#37096;&#20998;&#21151;&#33021;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT demonstrates immense potential to transform software engineering (SE) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of SE make the lack of interpretability for ChatGPT a concern. To address this issue, we carried out a study evaluating ChatGPT's capabilities and limitations in SE. We broke down the abilities needed for AI models to tackle SE tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on ChatGPT's ability to comprehend code syntax and semantic structures, including abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed ChatGPT's performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while ChatGPT excels at understanding code syntax (AST), it struggles with comprehending code semantics, parti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#31995;&#32479;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#22833;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12134</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Privacy in Multimodal Federated Human Activity Recognition. (arXiv:2305.12134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#31995;&#32479;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#22833;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#38544;&#31169;&#20449;&#24687;&#25110;&#30001;&#19981;&#21512;&#20316;&#23454;&#20307;&#25345;&#26377;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#12289;&#29615;&#22659;&#21644;&#20256;&#24863;&#22120;&#32423;&#21035;&#19978;&#38544;&#31169;&#23545;&#32852;&#37030;HAR&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;FL&#23545;HAR&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;FL&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#31243;&#24230;&#65292;&#24182;&#19988;&#20027;&#35201;&#21462;&#20915;&#20110;&#26469;&#33258;&#19981;&#21516;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#30340;&#37197;&#32622;&#12290;&#23613;&#31649;&#36991;&#20813;&#25968;&#25454;&#20849;&#20139;&#24182;&#22312;&#20154;&#31867;&#25110;&#29615;&#22659;&#32423;&#21035;&#19978;&#20551;&#35774;&#38544;&#31169;&#65292;&#22914;&#20043;&#21069;&#30340;&#24037;&#20316;&#25152;&#20570;&#30340;&#37027;&#26679;&#65292;&#31934;&#24230;&#20250;&#38477;&#20302;5-7&#65285;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#38544;&#31169;&#24310;&#20280;&#21040;&#27169;&#24577;&#32423;&#21035;&#24182;&#20005;&#26684;&#20998;&#31163;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#31934;&#24230;&#38477;&#20302;19-42&#65285;&#12290;&#30001;&#20110;&#36825;&#31181;&#24418;&#24335;&#30340;&#38544;&#31169;&#26159;HAR&#20013;&#34987;&#35201;&#27714;&#30340;&#36947;&#24503;&#21033;&#29992;&#34987;&#21160;&#20256;&#24863;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#22312;&#35813;&#31995;&#32479;&#20013;&#23458;&#25143;&#31471;&#30456;&#20114;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;FL&#27169;&#22411;&#21644;&#19968;&#20010;&#27599;&#31181;&#27169;&#24577;&#19968;&#20010;&#30340;&#32452;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;HAR&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) training data is often privacy-sensitive or held by non-cooperative entities. Federated Learning (FL) addresses such concerns by training ML models on edge clients. This work studies the impact of privacy in federated HAR at a user, environment, and sensor level. We show that the performance of FL for HAR depends on the assumed privacy level of the FL system and primarily upon the colocation of data from different sensors. By avoiding data sharing and assuming privacy at the human or environment level, as prior works have done, the accuracy decreases by 5-7%. However, extending this to the modality level and strictly separating sensor data between multiple clients may decrease the accuracy by 19-42%. As this form of privacy is necessary for the ethical utilisation of passive sensing methods in HAR, we implement a system where clients mutually train both a general FL model and a group-level one per modality. Our evaluation shows that this method leads to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#30340;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;(PBT)&#31639;&#27861;&#65292;&#20351;&#21333;&#33218;&#25110;&#21452;&#33218;&#26426;&#22120;&#20154;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21487;&#20197;&#23398;&#20064;&#28789;&#24039;&#30340;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#65292;&#21253;&#25324;&#37325;&#26032;&#25235;&#21462;&#12289;&#25235;&#21462;-&#25237;&#25527;&#21644;&#29289;&#20307;&#37325;&#26032;&#23450;&#20301;&#65292;&#30456;&#36739;&#20110;&#24120;&#35268;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#35813;&#31639;&#27861;&#21487;&#22823;&#22823;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#33021;&#21147;&#21644;&#21457;&#29616;&#31283;&#20581;&#25511;&#21046;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12127</link><description>&lt;p&gt;
DexPBT&#65306;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#20026;&#25163;&#33218;&#31995;&#32479;&#30340;&#28789;&#24039;&#25805;&#20316;&#25193;&#23637;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
DexPBT: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training. (arXiv:2305.12127v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#30340;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;(PBT)&#31639;&#27861;&#65292;&#20351;&#21333;&#33218;&#25110;&#21452;&#33218;&#26426;&#22120;&#20154;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21487;&#20197;&#23398;&#20064;&#28789;&#24039;&#30340;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#65292;&#21253;&#25324;&#37325;&#26032;&#25235;&#21462;&#12289;&#25235;&#21462;-&#25237;&#25527;&#21644;&#29289;&#20307;&#37325;&#26032;&#23450;&#20301;&#65292;&#30456;&#36739;&#20110;&#24120;&#35268;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#35813;&#31639;&#27861;&#21487;&#22823;&#22823;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#33021;&#21147;&#21644;&#21457;&#29616;&#31283;&#20581;&#25511;&#21046;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31639;&#27861;&#21644;&#26041;&#27861;&#65292;&#21033;&#29992;&#24102;&#26377;&#22810;&#25351;&#25163;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#21333;&#33218;&#25110;&#21452;&#33218;&#26426;&#22120;&#20154;&#36827;&#34892;&#27169;&#25311;&#30340;&#28789;&#24039;&#29289;&#20307;&#25805;&#20316;&#23398;&#20064;&#12290;&#20351;&#29992;&#24182;&#34892;&#30340;GPU&#21152;&#36895;&#29289;&#29702;&#27169;&#25311;&#22120;(Isaac Gym)&#65292;&#25105;&#20204;&#20026;&#36825;&#20123;&#26426;&#22120;&#20154;&#23454;&#29616;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#37325;&#26032;&#25235;&#21462;&#12289;&#25235;&#21462;-&#25237;&#25527;&#21644;&#29289;&#20307;&#37325;&#26032;&#23450;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#25955;&#30340;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;(PBT)&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22823;&#35268;&#27169;&#22320;&#25193;&#23637;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#24120;&#35268;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#21457;&#29616;&#31283;&#20581;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#21487;&#20197;&#22312; https://sites.google.com/view/dexpbt &#25214;&#21040;&#25152;&#23398;&#34892;&#20026;&#30340;&#35270;&#39057;&#28436;&#31034;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose algorithms and methods that enable learning dexterous object manipulation using simulated one- or two-armed robots equipped with multi-fingered hand end-effectors. Using a parallel GPU-accelerated physics simulator (Isaac Gym), we implement challenging tasks for these robots, including regrasping, grasp-and-throw, and object reorientation. To solve these problems we introduce a decentralized Population-Based Training (PBT) algorithm that allows us to massively amplify the exploration capabilities of deep reinforcement learning. We find that this method significantly outperforms regular end-to-end learning and is able to discover robust control policies in challenging tasks. Video demonstrations of learned behaviors and the code can be found at https://sites.google.com/view/dexpbt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#19968;&#33268;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#26680;&#24515;&#22312;&#20110;&#32467;&#21512;&#20102;&#26631;&#20934;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26799;&#24230;&#35009;&#21098;&#26041;&#27861;&#20197;&#21450;&#20351;&#29992;&#33258;&#21160;&#31283;&#23450;&#30340;&#21387;&#32553;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12125</link><description>&lt;p&gt;
&#28145;&#24230;&#21069;&#39304;&#32593;&#32476;&#30340;&#31283;&#23450;&#19968;&#33268;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Provably Stable and Consistent Training of Deep Feedforward Networks. (arXiv:2305.12125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#19968;&#33268;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#26680;&#24515;&#22312;&#20110;&#32467;&#21512;&#20102;&#26631;&#20934;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26799;&#24230;&#35009;&#21098;&#26041;&#27861;&#20197;&#21450;&#20351;&#29992;&#33258;&#21160;&#31283;&#23450;&#30340;&#21387;&#32553;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#30417;&#30563;&#65288;&#20998;&#31867;&#21644;&#22238;&#24402;&#65289;&#21644;&#38750;&#30417;&#30563;&#65288;&#24378;&#21270;&#23398;&#20064;&#65289;&#22330;&#26223;&#19979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26631;&#20934;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26799;&#24230;&#35009;&#21098;&#26041;&#27861;&#12290;&#36755;&#20986;&#23618;&#20351;&#29992;&#35009;&#21098;&#26799;&#24230;&#26356;&#26032;&#65292;&#20854;&#20313;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#26356;&#26032;&#12290;&#20351;&#29992;&#35009;&#21098;&#26799;&#24230;&#26356;&#26032;&#36755;&#20986;&#23618;&#21487;&#20197;&#20351;&#20854;&#31283;&#23450;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21482;&#35201;&#31070;&#32463;&#32593;&#32476;&#20165;&#30001;&#21387;&#32553;&#65288;&#32039;&#20945;&#33539;&#22260;&#65289;&#28608;&#27963;&#20989;&#25968;&#32452;&#25104;&#65292;&#20854;&#20313;&#23618;&#23558;&#33258;&#21160;&#34987;&#31283;&#23450;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21387;&#32553;&#28608;&#27963;&#20989;&#25968; - &#36890;&#36807;&#20462;&#25913;&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65288;GELU&#65289;&#26469;&#33719;&#24471; - &#25105;&#20204;&#31216;&#20043;&#20026;Truncated GELU&#65288;tGELU&#65289;&#12290;&#19982;&#20854;&#20182;&#21387;&#32553;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;Sigmoid&#65289;&#19981;&#21516;&#65292;tGELU&#30340;&#33539;&#22260;&#21487;&#20197;&#26126;&#30830;&#25351;&#23450;&#12290;&#22240;&#27492;&#65292;&#22312;&#19968;&#20123;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;Sigmod&#65289;&#30340;&#33539;&#22260;&#36739;&#23567;&#26102;&#20250;&#20986;&#29616;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#21487;&#20197;&#36991;&#20813;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel algorithm for training deep neural networks in supervised (classification and regression) and unsupervised (reinforcement learning) scenarios. This algorithm combines the standard stochastic gradient descent and the gradient clipping method. The output layer is updated using clipped gradients, the rest of the neural network is updated using standard gradients. Updating the output layer using clipped gradient stabilizes it. We show that the remaining layers are automatically stabilized provided the neural network is only composed of squashing (compact range) activations. We also present a novel squashing activation function - it is obtained by modifying a Gaussian Error Linear Unit (GELU) to have compact range - we call it Truncated GELU (tGELU). Unlike other squashing activations, such as sigmoid, the range of tGELU can be explicitly specified. As a consequence, the problem of vanishing gradients that arise due to a small range, e.g., in the case of a sigmoid activat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#26041;&#27861;&#65292;&#20854;&#33021;&#29983;&#25104;&#36719;&#26631;&#31614;&#29992;&#20316;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.12118</link><description>&lt;p&gt;
&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;&#25913;&#36827;&#20102;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Annealing Self-Distillation Rectification Improves Adversarial Training. (arXiv:2305.12118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#26041;&#27861;&#65292;&#20854;&#33021;&#29983;&#25104;&#36719;&#26631;&#31614;&#29992;&#20316;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#27169;&#22411;&#34987;&#20248;&#21270;&#20197;&#36866;&#24212;&#21487;&#25509;&#21463;&#30340;&#23545;&#25239;&#25200;&#21160;&#39044;&#31639;&#20869;&#30340;&#19968;&#28909;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24573;&#30053;&#30001;&#25200;&#21160;&#24102;&#26469;&#30340;&#22522;&#30784;&#20998;&#24067;&#21464;&#21270;&#65292;&#23548;&#33268;&#20102;&#24378;&#20581;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22686;&#24378;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24378;&#20581;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#24182;&#30830;&#23450;&#24378;&#20581;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#26356;&#24179;&#28369;&#21644;&#26356;&#33391;&#22909;&#26657;&#20934;&#30340;&#36755;&#20986;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#36719;&#26631;&#31614;&#20316;&#20026;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#33021;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;ADR&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20462;&#27491;&#30340;&#20998;&#24067;&#65292;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#39069;&#22806;&#30340;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26367;&#25442;&#21367;&#31215;&#23618;&#20197;&#23454;&#29616;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In standard adversarial training, models are optimized to fit one-hot labels within allowable adversarial perturbation budgets. However, the ignorance of underlying distribution shifts brought by perturbations causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that accurately reflects the distribution shift under attack during adversarial training. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by repl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#30340;&#39063;&#31890;&#32858;&#31867;&#31639;&#27861;GFDC&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20811;&#26381;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#22312;&#34913;&#37327;&#20840;&#23616;&#23494;&#24230;&#12289;&#30830;&#23450;&#21512;&#29702;&#30340;&#32858;&#31867;&#20013;&#24515;&#25110;&#32467;&#26500;&#12289;&#20934;&#30830;&#22320;&#20998;&#37197;&#26679;&#26412;&#20197;&#21450;&#22788;&#29702;&#20855;&#26377;&#22823;&#23494;&#24230;&#24046;&#24322;&#30340;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#30340;&#32570;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;GFDC &#37319;&#29992;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#39063;&#31890;&#34701;&#21512;&#31574;&#30053;&#26469;&#23558;&#39063;&#31890;&#32452;&#21512;&#25104;&#31283;&#23450;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#26816;&#27979;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.12114</link><description>&lt;p&gt;
GFDC&#65306;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#30340;&#39063;&#31890;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GFDC: A Granule Fusion Density-Based Clustering with Evidential Reasoning. (arXiv:2305.12114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12114
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#30340;&#39063;&#31890;&#32858;&#31867;&#31639;&#27861;GFDC&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20811;&#26381;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#22312;&#34913;&#37327;&#20840;&#23616;&#23494;&#24230;&#12289;&#30830;&#23450;&#21512;&#29702;&#30340;&#32858;&#31867;&#20013;&#24515;&#25110;&#32467;&#26500;&#12289;&#20934;&#30830;&#22320;&#20998;&#37197;&#26679;&#26412;&#20197;&#21450;&#22788;&#29702;&#20855;&#26377;&#22823;&#23494;&#24230;&#24046;&#24322;&#30340;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#30340;&#32570;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;GFDC &#37319;&#29992;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#39063;&#31890;&#34701;&#21512;&#31574;&#30053;&#26469;&#23558;&#39063;&#31890;&#32452;&#21512;&#25104;&#31283;&#23450;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#26816;&#27979;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#22240;&#20854;&#33021;&#22815;&#26816;&#27979;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#30340;&#31867;&#32676;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#34913;&#37327;&#20840;&#23616;&#23494;&#24230;&#12289;&#30830;&#23450;&#21512;&#29702;&#30340;&#32858;&#31867;&#20013;&#24515;&#25110;&#32467;&#26500;&#12289;&#20934;&#30830;&#22320;&#20998;&#37197;&#26679;&#26412;&#20197;&#21450;&#22788;&#29702;&#20855;&#26377;&#22823;&#23494;&#24230;&#24046;&#24322;&#30340;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#23427;&#20204;&#30340;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#30340;&#39063;&#31890;&#32858;&#31867;&#31639;&#27861;&#65288;GFDC&#65289;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#19968;&#31181;&#31232;&#30095;&#24230;&#37327;&#25351;&#26631;&#26469;&#27979;&#37327;&#26679;&#26412;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#12290;&#28982;&#21518;&#22312;&#39640;&#23494;&#24230;&#21644;&#20302;&#23494;&#24230;&#21306;&#22495;&#20013;&#29983;&#25104;&#20449;&#24687;&#39063;&#31890;&#65292;&#24110;&#21161;&#22788;&#29702;&#20855;&#26377;&#26174;&#33879;&#23494;&#24230;&#24046;&#24322;&#30340;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#39063;&#31890;&#34701;&#21512;&#31574;&#30053;&#26469;&#23558;&#39063;&#31890;&#32452;&#21512;&#25104;&#31283;&#23450;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#26816;&#27979;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#22522;&#20110;Dempster-Shafer&#29702;&#35770;&#24320;&#21457;&#30340;&#20998;&#37197;&#26041;&#27861;&#26469;&#20998;&#37197;&#19981;&#31283;&#23450;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;GFDC&#21518;&#65292;&#21487;&#20197;&#24471;&#21040;&#21512;&#29702;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, density-based clustering algorithms are widely applied because they can detect clusters with arbitrary shapes. However, they perform poorly in measuring global density, determining reasonable cluster centers or structures, assigning samples accurately and handling data with large density differences among clusters. To overcome their drawbacks, this paper proposes a granule fusion density-based clustering with evidential reasoning (GFDC). Both local and global densities of samples are measured by a sparse degree metric first. Then information granules are generated in high-density and low-density regions, assisting in processing clusters with significant density differences. Further, three novel granule fusion strategies are utilized to combine granules into stable cluster structures, helping to detect clusters with arbitrary shapes. Finally, by an assignment method developed from Dempster-Shafer theory, unstable samples are assigned. After using GFDC, a reasonable clustering
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#20013;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#35823;&#24046;&#38382;&#39064;&#65292;&#25910;&#38598;&#20102;&#30495;&#23454;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#36825;&#20123;&#35823;&#24046;&#22914;&#20309;&#24433;&#21709;&#19977;&#20010;ConvNets&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#20855;&#26377;&#26174;&#30528;&#30340;&#31867;&#21644;&#23454;&#20363;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;CNN&#32467;&#26500;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#22240;&#27492;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#36825;&#20123;&#35823;&#24046;&#24182;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;ConvNets&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.12106</link><description>&lt;p&gt;
&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#21450;&#20854;&#23545;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#30340;ConvNets&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Human labeling errors and their impact on ConvNets for satellite image scene classification. (arXiv:2305.12106v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#20013;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#35823;&#24046;&#38382;&#39064;&#65292;&#25910;&#38598;&#20102;&#30495;&#23454;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#36825;&#20123;&#35823;&#24046;&#22914;&#20309;&#24433;&#21709;&#19977;&#20010;ConvNets&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#20855;&#26377;&#26174;&#30528;&#30340;&#31867;&#21644;&#23454;&#20363;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;CNN&#32467;&#26500;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#22240;&#27492;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#36825;&#20123;&#35823;&#24046;&#24182;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;ConvNets&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNets)&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#12290;&#23545;&#20110;ConvNets&#25191;&#34892;&#20934;&#30830;&#20998;&#31867;&#65292;&#20154;&#24037;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#22797;&#26434;&#24615;&#65292;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20013;&#30340;&#35823;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#22312;&#21355;&#26143;&#22270;&#20687;&#19978;&#30340;&#20998;&#24067;&#21450;&#20854;&#23545;ConvNets&#30340;&#24433;&#21709;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#25910;&#38598;&#20102;&#26469;&#33258;32&#20301;&#21442;&#19982;&#32773;&#30340;&#30495;&#23454;&#26631;&#31614;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#35823;&#24046;&#22914;&#20309;&#24433;&#21709;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#20013;&#30340;&#19977;&#20010;ConvNets&#65288;VGG16&#65292;GoogleNet&#21644;ResNet-50&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#20855;&#26377;&#26174;&#30528;&#30340;&#31867;&#21644;&#23454;&#20363;&#20381;&#36182;&#24615;&#65292;&#36825;&#19982;&#20197;&#21069;&#30740;&#31350;&#20013;&#30340;&#27169;&#25311;&#22122;&#22768;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#65288;2&#65289;&#20851;&#20110;&#25152;&#26377;&#31867;&#30340;&#24635;&#20307;&#31934;&#24230;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#22686;&#21152;1&#20010;&#21333;&#20301;&#26102;&#65292;ConvNets&#30340;&#24635;&#20307;&#31934;&#24230;&#20250;&#38477;&#20302;0.67%~1.02%&#65307;&#65288;3&#65289;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#23545;&#19981;&#21516;CNN&#32467;&#26500;&#30340;&#24433;&#21709;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#20180;&#32454;&#26816;&#26597;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#20013;&#30340;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21628;&#21505;&#24320;&#21457;&#24378;&#22823;&#30340;ConvNets&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (ConvNets) have been successfully applied to satellite image scene classification. Human-labeled training datasets are essential for ConvNets to perform accurate classification. Errors in human-labeled training datasets are unavoidable due to the complexity of satellite images. However, the distribution of human labeling errors on satellite images and their impact on ConvNets have not been investigated. To fill this research gap, this study, for the first time, collected real-world labels from 32 participants and explored how their errors affect three ConvNets (VGG16, GoogleNet and ResNet-50) for high-resolution satellite image scene classification. We found that: (1) human labeling errors have significant class and instance dependence, which is fundamentally different from the simulation noise in previous studies; (2) regarding the overall accuracy of all classes, when human labeling errors in training data increase by one unit, the overall accuracy of Co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;UP5&#65292;&#23427;&#37319;&#29992;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#26469;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.12090</link><description>&lt;p&gt;
UP5: &#38754;&#21521;&#20844;&#24179;&#24615;&#25512;&#33616;&#30340;&#26080;&#20559;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UP5: Unbiased Foundation Model for Fairness-aware Recommendation. (arXiv:2305.12090v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;UP5&#65292;&#23427;&#37319;&#29992;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#26469;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31561;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24050;&#23558;&#23427;&#20204;&#25512;&#21040;&#20102;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#30340;&#21069;&#27839;&#12290;&#27492;&#22806;&#65292;RS&#20013;&#30340;&#20844;&#24179;&#24615;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#35768;&#22810;&#29992;&#25143;&#23558;&#20854;&#29992;&#20110;&#20915;&#31574;&#21644;&#38656;&#27714;&#23653;&#34892;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20844;&#24179;&#24615;&#27700;&#24179;&#21644;&#20844;&#24179;&#22788;&#29702;&#19981;&#21516;&#29992;&#25143;&#32676;&#32452;&#30340;&#36866;&#24403;&#26041;&#27861;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#29992;&#25143;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24443;&#24213;&#26816;&#26597;&#34920;&#26126;&#65292;LLMs&#20013;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#65292;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;&#20026;&#20102;&#28040;&#38500;LLM&#20013;&#30340;&#20559;&#24046;&#20197;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#30340;&#26032;&#22411;&#26080;&#20559;P5&#65288;UP5&#65289;&#22522;&#30784;&#27169;&#22411;&#12290;CFP&#21253;&#25324;&#20004;&#20010;&#23376;&#27169;&#22359;&#65306;&#20010;&#24615;&#21270;&#21069;&#32512;&#25552;&#31034;&#21644;Prompt&#28151;&#21512;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20010;&#20307;&#25935;&#24863;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in foundation models such as large language models (LLM) have propelled them to the forefront of recommender systems (RS). Moreover, fairness in RS is critical since many users apply it for decision-making and demand fulfillment. However, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. In this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in LLMs that lead to unfair recommendation results. To eliminate bias from LLM for fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5) foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a Prompt Mixture that int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21516;&#34892;&#35780;&#23457;&#26399;&#21002;&#20013;&#20195;&#37329;&#21048;&#22870;&#21169;&#21046;&#24230;&#21487;&#33021;&#20250;&#23548;&#33268;&#35780;&#23457;&#20154;&#21592;&#20108;&#20803;&#20915;&#31574;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22870;&#21169;&#31995;&#32479;&#65292;&#26377;&#25928;&#22320;&#20419;&#36827;&#20102;&#26356;&#20840;&#38754;&#30340;&#23457;&#26597;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#26356;&#21152;&#24179;&#34913;&#19988;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2305.12088</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21516;&#34892;&#35780;&#23457;&#26399;&#21002;&#31995;&#32479;&#20013;&#35780;&#23457;&#22870;&#21169;&#30340;&#21338;&#24328;&#35770;&#20998;&#26512;&#19982;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Game-Theoretical Analysis of Reviewer Rewards in Peer-Review Journal Systems: Analysis and Experimental Evaluation using Deep Reinforcement Learning. (arXiv:2305.12088v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21516;&#34892;&#35780;&#23457;&#26399;&#21002;&#20013;&#20195;&#37329;&#21048;&#22870;&#21169;&#21046;&#24230;&#21487;&#33021;&#20250;&#23548;&#33268;&#35780;&#23457;&#20154;&#21592;&#20108;&#20803;&#20915;&#31574;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22870;&#21169;&#31995;&#32479;&#65292;&#26377;&#25928;&#22320;&#20419;&#36827;&#20102;&#26356;&#20840;&#38754;&#30340;&#23457;&#26597;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#26356;&#21152;&#24179;&#34913;&#19988;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25968;&#23398;&#20934;&#30830;&#24615;&#21644;&#21338;&#24328;&#35770;&#31574;&#30053;&#27934;&#23519;&#21147;&#65292;&#25506;&#35752;&#20102;&#24320;&#25918;&#33719;&#21462;&#23398;&#26415;&#20986;&#29256;&#20013;&#30340;&#35780;&#23457;&#22870;&#21169;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#20195;&#37329;&#21048;&#22870;&#21169;&#31995;&#32479;&#27010;&#24565;&#21270;&#20026;&#19968;&#20010;&#20004;&#20010;&#29609;&#23478;&#30340;&#21338;&#24328;&#65292;&#24182;&#35782;&#21035;&#20102;&#21487;&#33021;&#23548;&#33268;&#35780;&#23457;&#20154;&#21592;&#20542;&#21521;&#20110;&#20108;&#20803;&#20915;&#31574;&#30340;&#28508;&#22312;&#32570;&#28857;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#25968;&#23398;&#19978;&#24418;&#24335;&#21270;&#20102;&#19968;&#31181;&#26367;&#20195;&#22870;&#21169;&#31995;&#32479;&#65292;&#26088;&#22312;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#65292;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#23457;&#26597;&#12290;&#25105;&#20204;&#36816;&#29992;&#20005;&#26684;&#30340;&#21338;&#24328;&#35770;&#20998;&#26512;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#65292;&#23545;&#20004;&#31181;&#31995;&#32479;&#30340;&#23646;&#24615;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#31181;&#31995;&#32479;&#20043;&#38388;&#30340;&#26174;&#30528;&#24046;&#24322;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#23637;&#29616;&#20986;&#20102;&#26356;&#24179;&#34913;&#30340;&#20915;&#31574;&#20998;&#37197;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#29305;&#28857;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#25193;&#20805;&#20102;&#26377;&#20851;&#35780;&#23457;&#22870;&#21169;&#31995;&#32479;&#30340;&#25968;&#23398;&#29702;&#35299;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#35299;&#20915;&#24403;&#21069;&#20195;&#37329;&#21048;&#22870;&#21169;&#31995;&#32479;&#20559;&#35265;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we navigate the intricate domain of reviewer rewards in open-access academic publishing, leveraging the precision of mathematics and the strategic acumen of game theory. We conceptualize the prevailing voucher-based reviewer reward system as a two-player game, subsequently identifying potential shortcomings that may incline reviewers towards binary decisions. To address this issue, we propose and mathematically formalize an alternative reward system with the objective of mitigating this bias and promoting more comprehensive reviews. We engage in a detailed investigation of the properties and outcomes of both systems, employing rigorous game-theoretical analysis and deep reinforcement learning simulations. Our results underscore a noteworthy divergence between the two systems, with our proposed system demonstrating a more balanced decision distribution and enhanced stability. This research not only augments the mathematical understanding of reviewer reward systems, but it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2305.12081</link><description>&lt;p&gt;
AnyPredict: &#34920;&#26684;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324; (1) &#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#24102;&#26377;&#26631;&#20934;&#26631;&#31614;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450; (2) &#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25972;&#21512;&#34920;&#26684;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#27169;&#24335;&#34920;&#26684;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#24182;&#20351;&#29992;&#8220;&#23398;&#20064;&#65292;&#27880;&#37322;&#21644;&#23457;&#35745;&#8221;&#27969;&#31243;&#23558;&#39046;&#22495;&#22806;&#25968;&#25454;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#12290;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#20351;&#39044;&#35757;&#32451;&#30340; AnyPredict &#33021;&#22815;&#25903;&#25345;&#27599;&#20010;&#34920;&#26684;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12073</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;GELU&#28608;&#27963;&#20989;&#25968;&#65306;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance. (arXiv:2305.12073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#28608;&#27963;&#20989;&#25968;&#26159;&#24433;&#21709;&#20854;&#23398;&#20064;&#33021;&#21147;&#12289;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36817;&#24180;&#26469;&#65292;&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65288;GELU&#65289;&#28608;&#27963;&#20989;&#25968;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20027;&#27969;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#12290;&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#35814;&#32454;&#25506;&#35752;&#20102;&#20854;&#21487;&#24494;&#24615;&#12289;&#26377;&#30028;&#24615;&#12289;&#24179;&#31283;&#24615;&#21644;&#20809;&#28369;&#24615;&#31561;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;GELU&#20989;&#25968;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#21033;&#29992;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;STL-10&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27531;&#24046;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#23454;&#35777;&#27979;&#35797;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;GELU&#30456;&#23545;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#30830;&#31435;&#20102;&#23427;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the most suitable activation function is a critical factor in the effectiveness of deep learning models, as it influences their learning capacity, stability, and computational efficiency. In recent years, the Gaussian Error Linear Unit (GELU) activation function has emerged as a dominant method, surpassing traditional functions such as the Rectified Linear Unit (ReLU) in various applications. This study presents a rigorous mathematical investigation of the GELU activation function, exploring its differentiability, boundedness, stationarity, and smoothness properties in detail. Additionally, we conduct an extensive experimental comparison of the GELU function against a broad range of alternative activation functions, utilizing a residual convolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets as the empirical testbed. Our results demonstrate the superior performance of GELU compared to other activation functions, establishing its suitability for a wide ra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#21367;&#31215;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#26816;&#27979;&#20083;&#33146;X&#32447;&#22270;&#20687;&#20013;&#30340;&#25216;&#26415;&#24322;&#24120;&#28857;&#65292;&#20197;&#20445;&#35777;&#19968;&#20010;&#20083;&#33146;&#30284;&#26816;&#27979;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12068</link><description>&lt;p&gt;
&#21033;&#29992;&#21367;&#31215;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#25216;&#26415;&#24322;&#24120;&#28857;&#26816;&#27979;&#65306;&#20197;ADMAMI&#20083;&#33146;X&#32447;&#25968;&#25454;&#38598;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Technical outlier detection via convolutional variational autoencoder for the ADMANI breast mammogram dataset. (arXiv:2305.12068v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12068
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#21367;&#31215;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#26816;&#27979;&#20083;&#33146;X&#32447;&#22270;&#20687;&#20013;&#30340;&#25216;&#26415;&#24322;&#24120;&#28857;&#65292;&#20197;&#20445;&#35777;&#19968;&#20010;&#20083;&#33146;&#30284;&#26816;&#27979;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28595;&#22823;&#21033;&#20122;BreastScreen Victoria&#36816;&#33829;&#30528;Transforming Breast Cancer Screening with AI&#35745;&#21010;&#65292;&#25910;&#38598;&#20102;&#27880;&#37322;&#25968;&#23383;&#20083;&#33146;X&#32447;&#22270;&#20687;&#21450;&#20854;&#30456;&#20851;&#38750;&#22270;&#20687;&#25968;&#25454;&#65292;&#32452;&#25104;&#20102;&#22823;&#35268;&#27169;&#12289;&#20020;&#24202;&#31579;&#36873;&#12289;&#30495;&#23454;&#19990;&#30028;&#30340;ADMANI&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#26395;&#20026;&#20083;&#33146;&#30284;&#26816;&#27979;&#12289;&#26089;&#26399;&#35786;&#26029;&#21644;&#20854;&#20182;&#24212;&#29992;&#24320;&#21457;&#20020;&#24202;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#22312;&#36827;&#34892;&#36817;&#19979;&#28216;&#31639;&#27861;&#24320;&#21457;&#20043;&#21069;&#65292;&#24517;&#39035;&#21024;&#38500;&#25216;&#26415;&#24322;&#24120;&#20540;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#38543;&#26426;&#36873;&#21462;3&#19975;&#20010;&#20083;&#33146;X&#32447;&#22270;&#20687;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#20013;&#19968;&#31181;&#31867;&#22411;&#8212;&#8212;&#21367;&#31215;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23613;&#31649;CVAE&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#24615;&#33021;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#23427;&#39044;&#35745;&#21487;&#26816;&#27979;&#21040;&#21508;&#31181;&#24322;&#24120;&#20540;&#31867;&#22411;&#12290;&#23545;&#20110;CVAE&#30340;&#24615;&#33021;&#19981;&#36275;&#65292;&#20256;&#32479;&#30340;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#65292;&#22914;&#33104;&#34432;&#21644;&#33016;&#22823;&#32908;&#20998;&#26512;&#65292;&#21017;&#21487;&#36827;&#34892;&#34917;&#20607;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ADMANI datasets (annotated digital mammograms and associated non-image datasets) from the Transforming Breast Cancer Screening with AI programme (BRAIx) run by BreastScreen Victoria in Australia are multi-centre, large scale, clinically curated, real-world databases. The datasets are expected to aid in the development of clinically relevant Artificial Intelligence (AI) algorithms for breast cancer detection, early diagnosis, and other applications. To ensure high data quality, technical outliers must be removed before any downstream algorithm development. As a first step, we randomly select 30,000 individual mammograms and use Convolutional Variational Autoencoder (CVAE), a deep generative neural network, to detect outliers. CVAE is expected to detect all sorts of outliers, although its detection performance differs among different types of outliers. Traditional image processing techniques such as erosion and pectoral muscle analysis can compensate for the poor performance of CVAE 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#28145;&#24230;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#39046;&#22495;&#23545;&#25239;&#28145;&#24230;&#20852;&#36259;&#32593;&#32476;&#65288;DADIN&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#19981;&#21487;&#30693;&#23618;&#21644;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#65292;&#21019;&#26032;&#22320;&#23454;&#29616;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#32852;&#21512;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#19982;&#28857;&#20987;&#29575;&#39044;&#27979;&#25439;&#22833;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#65292;&#30456;&#27604;&#31454;&#20105;&#22522;&#32447;&#31639;&#27861;&#25552;&#21319;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2305.12058</link><description>&lt;p&gt;
DADIN: &#38754;&#21521;&#36328;&#22495;&#25512;&#33616;&#31995;&#32479;&#30340;&#39046;&#22495;&#23545;&#25239;&#28145;&#24230;&#20852;&#36259;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DADIN: Domain Adversarial Deep Interest Network for Cross Domain Recommender Systems. (arXiv:2305.12058v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12058
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#28145;&#24230;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#39046;&#22495;&#23545;&#25239;&#28145;&#24230;&#20852;&#36259;&#32593;&#32476;&#65288;DADIN&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#19981;&#21487;&#30693;&#23618;&#21644;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#65292;&#21019;&#26032;&#22320;&#23454;&#29616;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#32852;&#21512;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#19982;&#28857;&#20987;&#29575;&#39044;&#27979;&#25439;&#22833;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#65292;&#30456;&#27604;&#31454;&#20105;&#22522;&#32447;&#31639;&#27861;&#25552;&#21319;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#26159;&#25512;&#33616;&#31995;&#32479;&#30340;&#20027;&#35201;&#20219;&#21153;&#20043;&#19968;&#65292;&#29992;&#25143;&#38024;&#23545;&#19981;&#21516;&#39033;&#30446;&#36827;&#34892;&#28857;&#20987;&#20197;&#33719;&#21462;&#25512;&#33616;&#32467;&#26524;&#12290;&#38024;&#23545;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30340;&#38271;&#23614;&#20998;&#24067;&#21644;&#39033;&#30446;&#25110;&#29992;&#25143;&#30340;&#20919;&#21551;&#21160;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#12290;&#20026;&#20102;&#20351;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#26356;&#21152;&#39034;&#30021;&#65292;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#28145;&#24230;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#39046;&#22495;&#23545;&#25239;&#28145;&#24230;&#20852;&#36259;&#32593;&#32476; (DADIN)&#65292;&#23558;&#36328;&#22495;&#25512;&#33616;&#20219;&#21153;&#36716;&#21270;&#20026;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#19981;&#21487;&#30693;&#23618;&#21644;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#65292;&#21019;&#26032;&#22320;&#23454;&#29616;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#32852;&#21512;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#19982;&#28857;&#20987;&#29575;&#39044;&#27979;&#25439;&#22833;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21326;&#20026;&#25968;&#25454;&#38598;&#19978;&#65292;DADIN &#30340;&#26354;&#32447;&#19979;&#38754;&#31215; (AUC) &#27604;&#26368;&#20855;&#31454;&#20105;&#21147;&#30340;&#22522;&#32447;&#39640;&#20986;0.08&#65285;&#65292;&#39640;&#20986;0.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction is one of the main tasks of the recommendation system, which is conducted by a user for different items to give the recommendation results. Cross-domain CTR prediction models have been proposed to overcome problems of data sparsity, long tail distribution of user-item interactions, and cold start of items or users. In order to make knowledge transfer from source domain to target domain more smoothly, an innovative deep learning cross-domain CTR prediction model, Domain Adversarial Deep Interest Network (DADIN) is proposed to convert the cross-domain recommendation task into a domain adaptation problem. The joint distribution alignment of two domains is innovatively realized by introducing domain agnostic layers and specially designed loss, and optimized together with CTR prediction loss in a way of adversarial training. It is found that the Area Under Curve (AUC) of DADIN is 0.08% higher than the most competitive baseline on Huawei dataset and is 0.7
&lt;/p&gt;</description></item><item><title>CodeCompose&#26159;&#19968;&#20010;&#22522;&#20110;InCoder LLM&#30340;AI&#36741;&#21161;&#30340;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#65292;&#24050;&#32463;&#22312;Meta&#20869;&#37096;&#37096;&#32626;&#65292;&#21487;&#22312;10&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#20960;&#20010;&#32534;&#30721;&#34920;&#38754;&#19978;&#20351;&#29992;&#12290; CodeCompose&#24050;&#25104;&#21151;&#24110;&#21161;&#25552;&#39640;Meta&#30340;&#24320;&#21457;&#20154;&#21592;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12050</link><description>&lt;p&gt;
CodeCompose&#65306;&#22522;&#20110;AI&#36741;&#21161;&#30340;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#30340;&#22823;&#35268;&#27169;&#24037;&#19994;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
CodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Authoring. (arXiv:2305.12050v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12050
&lt;/p&gt;
&lt;p&gt;
CodeCompose&#26159;&#19968;&#20010;&#22522;&#20110;InCoder LLM&#30340;AI&#36741;&#21161;&#30340;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#65292;&#24050;&#32463;&#22312;Meta&#20869;&#37096;&#37096;&#32626;&#65292;&#21487;&#22312;10&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#20960;&#20010;&#32534;&#30721;&#34920;&#38754;&#19978;&#20351;&#29992;&#12290; CodeCompose&#24050;&#25104;&#21151;&#24110;&#21161;&#25552;&#39640;Meta&#30340;&#24320;&#21457;&#20154;&#21592;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#35299;&#38145;&#20102;&#35813;&#25216;&#26415;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#24050;&#32463;&#35777;&#26126;&#29983;&#25104;&#24335;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#39537;&#21160;AI&#22522;&#30784;&#30340;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#65292;&#22312;&#32534;&#20889;&#20195;&#30721;&#26399;&#38388;&#21487;&#20197;&#24314;&#35758;&#25972;&#20010;&#35821;&#21477;&#25110;&#20195;&#30721;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;InCoder LLM&#30340;CodeCompose&#65292;&#36825;&#26159;&#19968;&#20010;Meta&#20869;&#37096;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;AI&#36741;&#21161;&#20195;&#30721;&#32534;&#20889;&#24037;&#20855;&#12290;CodeCompose&#22522;&#20110;&#21512;&#24182;&#29983;&#25104;&#33021;&#21147;&#21644;&#21452;&#21521;&#24615;&#30340;LLM&#12290;&#25105;&#20204;&#24050;&#32463;&#23558;CodeCompose&#25193;&#23637;&#21040;Meta&#30340;&#25968;&#20197;&#19975;&#35745;&#30340;&#24320;&#21457;&#20154;&#21592;&#65292;&#22312;10&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#20960;&#20010;&#32534;&#30721;&#34920;&#38754;&#19978;&#20351;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#29615;&#22659;&#20013;&#37096;&#32626;&#27492;&#31867;&#24037;&#20855;&#26102;&#20986;&#29616;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#25351;&#26631;&#26041;&#38754;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;CodeCompose&#30340;&#27169;&#22411;&#21644;&#31995;&#32479;&#26550;&#26500;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#23545;CodeCompose&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#23427;&#22914;&#20309;&#24110;&#21161;&#25552;&#39640;Meta&#30340;&#24320;&#21457;&#20154;&#21592;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of large language models (LLMs) has unlocked various applications of this technology in software development. In particular, generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 10+ programming languages and several coding surfaces.  We discuss unique challenges in terms of user experience and metrics that arise when deploying such tools in large-scale industrial settings. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges. Finally, we present metrics from our large-scale deployment of C
&lt;/p&gt;</description></item><item><title>&#20020;&#24202;&#39558;&#39548;&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12031</link><description>&lt;p&gt;
&#20020;&#24202;&#39558;&#39548;&#65306;&#19968;&#31181;&#20855;&#26377;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#19987;&#23478;&#32423;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding. (arXiv:2305.12031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12031
&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39558;&#39548;&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#25968;&#25454;&#38544;&#31169;&#12289;&#30417;&#31649;&#21512;&#35268;&#24615;&#21644;&#27169;&#22411;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#65288;DBKE&#65289;&#12290;DBKE&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#38544;&#24335;&#30693;&#35782;&#24211;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#24378;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20026;&#21518;&#32493;&#29992;&#20363;&#25552;&#20379;&#20102;&#36719;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Camel&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#19987;&#27880;&#20110;&#21307;&#30103;&#20445;&#20581;&#30340;&#20250;&#35805;&#27169;&#22411;&#65292;&#26469;&#23637;&#31034;DBKE&#30340;&#26377;&#25928;&#24615;&#12290;Clinical Camel&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;&#23427;&#36824;&#20026;&#21307;&#30103;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#12289;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present immense potential in the medical field, yet concerns over data privacy, regulatory compliance, and model stability restrict their widespread adoption. Although the distillation of high-performing closed-source LLMs has proven effective for general tasks, their application in healthcare is limited due to reduced domain knowledge and remnants of alignment behavior hindering clinical tasks. To address these challenges, we propose Dialogue-Based Knowledge Encoding (DBKE). DBKE enhances models' implicit knowledge base and primes them for conversational recall, augmenting their conversational capabilities and enabling a soft alignment for subsequent use cases. By transforming dense academic source text into synthetic dialogue, DBKE broadens the model's knowledge base and enables a soft alignment that guides downstream behaviours. We present Clinical Camel, an open-source, healthcare-focused conversational model, to showcase the effectiveness of DBKE. Clin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#30340;&#26159;&#22914;&#20309;&#36827;&#34892;&#22270;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#38754;&#20020;&#30528;&#30001;&#20110;&#25968;&#25454;&#22522;&#30784;&#29305;&#24615;&#25152;&#23548;&#33268;&#30340;&#29702;&#35770;&#19982;&#26041;&#27861;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.12030</link><description>&lt;p&gt;
&#23398;&#20064;&#36830;&#32493;&#30340;&#22270;&#24207;&#21015; -- &#21160;&#21147;&#31995;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Continually on a Sequence of Graphs -- The Dynamical System Way. (arXiv:2305.12030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#30340;&#26159;&#22914;&#20309;&#36827;&#34892;&#22270;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#38754;&#20020;&#30528;&#30001;&#20110;&#25968;&#25454;&#22522;&#30784;&#29305;&#24615;&#25152;&#23548;&#33268;&#30340;&#29702;&#35770;&#19982;&#26041;&#27861;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;(CL)&#26159;&#19968;&#20010;&#39046;&#22495;&#65292;&#20851;&#27880;&#20110;&#23398;&#20064;&#19968;&#31995;&#21015;&#30456;&#20114;&#20851;&#32852;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#26159;&#20197;&#22238;&#24402;&#25110;&#20998;&#31867;&#30340;&#26041;&#24335;&#23450;&#20041;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#24403;&#36825;&#20123;&#20219;&#21153;&#26159;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#23450;&#20041;&#30340;&#26102;&#65292;&#22914;&#22270;&#20687;&#65292;CL&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;CL&#20219;&#21153;&#30456;&#23545;&#24212;&#30340;&#25968;&#25454;&#26159;&#38750;&#27431;&#20960;&#37324;&#24503;&#30340;&#26102;&#65292;&#22914;&#22270;&#24418;&#12289;&#28857;&#20113;&#25110;&#27969;&#24418;&#65292;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#24847;&#20041;&#19979;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#24182;&#19981;&#36866;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#38750;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#24320;&#21457;CL&#38754;&#20020;&#30528;&#20960;&#20010;&#29702;&#35770;&#21644;&#26041;&#27861;&#19978;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#22270;&#20013;&#30340;CL&#38656;&#35201;&#26174;&#24335;&#22320;&#27169;&#25311;&#33410;&#28857;&#21644;&#36793;&#30340;&#38750;&#24179;&#31283;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning~(CL) is a field concerned with learning a series of inter-related task with the tasks typically defined in the sense of either regression or classification. In recent years, CL has been studied extensively when these tasks are defined using Euclidean data-- data, such as images, that can be described by a set of vectors in an n-dimensional real space. However, the literature is quite sparse, when the data corresponding to a CL task is nonEuclidean-- data , such as graphs, point clouds or manifold, where the notion of similarity in the sense of Euclidean metric does not hold. For instance, a graph is described by a tuple of vertices and edges and similarities between two graphs is not well defined through a Euclidean metric. Due to this fundamental nature of the data, developing CL for nonEuclidean data presents several theoretical and methodological challenges. In particular, CL for graphs requires explicit modelling of nonstationary behavior of vertices and edges an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12029</link><description>&lt;p&gt;
MultiTurnCleanup&#65306;&#29992;&#20110;&#22810;&#36718;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#28165;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35843;&#19981;&#36830;&#32493;&#26816;&#27979;&#27169;&#22411;&#20391;&#37325;&#20110;&#21333;&#20010;&#35828;&#35805;&#32773;&#30340;&#27599;&#20010;&#35805;&#35821;&#12290;&#28982;&#32780;&#65292;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#35768;&#22810;&#19981;&#36830;&#32493;&#29616;&#35937;&#37117;&#21457;&#29983;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#36825;&#24433;&#21709;&#20102;&#20154;&#31867;&#30340;&#21487;&#35835;&#24615;&#21644;&#19979;&#28216; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#8220;MultiTurnCleanup&#8221;&#20219;&#21153;&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#65292;&#24182;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#27880;&#27169;&#24335;&#20197;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#25226;&#30418;&#23884;&#20837;&#27010;&#24565;&#24341;&#20837;&#21040;&#26497;&#22352;&#26631;&#20013;&#65292;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#65292;&#23454;&#29616;&#32858;&#31867;&#12290;&#22312;&#23454;&#20307;&#38142;&#25509;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2305.12027</link><description>&lt;p&gt;
&#26497;&#22320;&#40493;&#23376;&#30340;&#21457;&#29616;&#20043;&#26053;&#65306;&#20351;&#29992;&#40493;&#24335;&#36776;&#26512;&#21644;&#26497;&#22352;&#26631;&#30418;&#23884;&#20837;&#22686;&#24378;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings. (arXiv:2305.12027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#25226;&#30418;&#23884;&#20837;&#27010;&#24565;&#24341;&#20837;&#21040;&#26497;&#22352;&#26631;&#20013;&#65292;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#65292;&#23454;&#29616;&#32858;&#31867;&#12290;&#22312;&#23454;&#20307;&#38142;&#25509;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#26816;&#32034;&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#26159;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#39640;&#25928;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#19981;&#22914;&#29983;&#25104;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#32467;&#26500;&#25935;&#24863;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#21463;&#32534;&#31243;&#35821;&#35328;&#20013;&#40493;&#24335;&#36776;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#35758;&#26681;&#25454;&#23454;&#20307;&#22312;&#30693;&#35782;&#22270;&#20013;&#30340;&#20851;&#31995;&#23450;&#20041;&#23454;&#20307;&#30340;&#31867;&#22411;&#12290;&#28982;&#21518;&#65292;&#23558;&#30418;&#23884;&#20837;&#30340;&#27010;&#24565;&#31227;&#26893;&#21040;&#29699;&#24418;&#26497;&#22352;&#26631;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#26469;&#20248;&#21270;&#27169;&#22411;&#20197;&#32858;&#31867;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#23454;&#20307;&#28040;&#27495;&#22522;&#20934;&#27979;&#35797;&#19978;&#35774;&#32622;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#23427;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29305;&#21035;&#19981;&#36866;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#19981;&#21487;&#34892;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking methods based on dense retrieval are an efficient and widely used solution in large-scale applications, but they fall short of the performance of generative models, as they are sensitive to the structure of the embedding space. In order to address this issue, this paper introduces DUCK, an approach to infusing structural information in the space of entity representations, using prior knowledge of entity types. Inspired by duck typing in programming languages, we propose to define the type of an entity based on the relations that it has with other entities in a knowledge graph. Then, porting the concept of box embeddings to spherical polar coordinates, we propose to represent relations as boxes on the hypersphere. We optimize the model to cluster entities of similar type by placing them inside the boxes corresponding to their relations. Our experiments show that our method sets new state-of-the-art results on standard entity-disambiguation benchmarks, it improves the perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#25928;&#38543;&#24577;&#30005;&#23481;&#22120;&#30340;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#21644;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#31995;&#32479;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#36739;&#23567;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.12025</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#25928;&#38543;&#24577;&#30005;&#23481;&#22120;&#30340;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#29992;&#20110;&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient memcapacitive physical reservoir computing system for temporal data processing. (arXiv:2305.12025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#25928;&#38543;&#24577;&#30005;&#23481;&#22120;&#30340;&#29289;&#29702;&#20648;&#23618;&#35745;&#31639;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#21644;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#31995;&#32479;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#36739;&#23567;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#23618;&#35745;&#31639;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#26469;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#12290;&#29289;&#29702;&#20648;&#23618;&#21487;&#20351;&#29992;&#30913;&#26059;&#30005;&#23376;&#12289;&#21407;&#23376;&#24320;&#20851;&#32593;&#32476;&#12289;&#30789;&#20809;&#23398;&#27169;&#22359;&#12289;&#38081;&#30005;&#26230;&#20307;&#31649;&#21644;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#22791;&#30001;&#20110;&#20854;&#30005;&#38459;&#24615;&#36136;&#26412;&#36136;&#19978;&#23384;&#22312;&#33021;&#37327;&#32791;&#25955;&#38382;&#39064;&#65292;&#23548;&#33268;&#21151;&#32791;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#30005;&#23481;&#23384;&#20648;&#22120;&#35774;&#22791;&#21487;&#25552;&#20379;&#26356;&#20026;&#33021;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#25311;&#21644;&#23454;&#39564;&#20013;&#36817;&#20284;&#26576;&#20123;&#30701;&#26399;&#31361;&#35302;&#21487;&#22609;&#24615;&#21151;&#33021;&#30340;&#26131;&#22833;&#29983;&#29289;&#33180;&#22522;&#36136;&#37327;&#20316;&#20026;&#20648;&#23618;&#65292;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#21644;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#21475;&#38899;&#25968;&#23383;&#20998;&#31867;&#20013;&#23454;&#29616;&#20102;98&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#20108;&#38454;&#38750;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;0.0012&#30340;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing is a highly efficient machine learning framework for processing temporal data by extracting features from the input signal and mapping them into higher dimensional spaces. Physical reservoir layers have been realized using spintronic oscillators, atomic switch networks, silicon photonic modules, ferroelectric transistors, and volatile memristors. However, these devices are intrinsically energy-dissipative due to their resistive nature, which leads to increased power consumption. Therefore, capacitive memory devices can provide a more energy-efficient approach. Here, we leverage volatile biomembrane-based memcapacitors that closely mimic certain short-term synaptic plasticity functions as reservoirs to solve classification tasks and analyze time-series data in simulation and experimentally. Our system achieves a 98% accuracy rate for spoken digit classification and a normalized mean square error of 0.0012 in a second-order non-linear regression task. Further, to demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#33258;&#28982;&#22270;&#20687;&#35757;&#32451;&#30340;&#27169;&#22411;&#21019;&#24314;&#32472;&#30011;&#39118;&#26684;&#65292;&#32780;&#19981;&#38656;&#35201;&#33402;&#26415;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#33021;&#20026;&#33402;&#26415;&#20013;&#30340;&#29983;&#25104;AI&#21512;&#27861;&#20351;&#29992;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.12015</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#28789;&#24863;&#21457;&#26126;&#32472;&#30011;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
Inventing painting styles through natural inspiration. (arXiv:2305.12015v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#33258;&#28982;&#22270;&#20687;&#35757;&#32451;&#30340;&#27169;&#22411;&#21019;&#24314;&#32472;&#30011;&#39118;&#26684;&#65292;&#32780;&#19981;&#38656;&#35201;&#33402;&#26415;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#33021;&#20026;&#33402;&#26415;&#20013;&#30340;&#29983;&#25104;AI&#21512;&#27861;&#20351;&#29992;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#20165;&#20351;&#29992;&#33258;&#28982;&#22270;&#20687;&#35757;&#32451;&#30340;&#27169;&#22411;&#21019;&#24314;&#32472;&#30011;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23458;&#35266;&#35777;&#25454;&#35777;&#26126;&#27169;&#22411;&#27809;&#26377;&#21117;&#31363;&#20154;&#31867;&#33402;&#26415;&#39118;&#26684;&#12290;&#22312;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33402;&#26415;&#23186;&#20171;&#30340;&#24402;&#32435;&#20559;&#32622;&#26469;&#23454;&#29616;&#21019;&#36896;&#24615;&#30340;&#34920;&#36798;&#12290;&#36890;&#36807;&#20351;&#29992;&#37325;&#24314;&#25439;&#22833;&#26469;&#23454;&#29616;&#25277;&#35937;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#20351;&#29992;&#39069;&#22806;&#30340;&#33258;&#28982;&#22270;&#20687;&#20316;&#20026;&#28789;&#24863;&#26469;&#21019;&#24314;&#26032;&#30340;&#39118;&#26684;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#21457;&#26126;&#26032;&#30340;&#32472;&#30011;&#39118;&#26684;&#65292;&#32780;&#26080;&#38656;&#33402;&#26415;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20026;&#33402;&#26415;&#20013;&#29983;&#25104;AI&#30340;&#21512;&#27861;&#20351;&#29992;&#38138;&#24179;&#36947;&#36335;&#65292;&#32780;&#19981;&#20405;&#29359;&#20154;&#31867;&#21019;&#36896;&#32773;&#30340;&#29420;&#21019;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two procedures to create painting styles using models trained only on natural images, providing objective proof that the model is not plagiarizing human art styles. In the first procedure we use the inductive bias from the artistic medium to achieve creative expression. Abstraction is achieved by using a reconstruction loss. The second procedure uses an additional natural image as inspiration to create a new style. These two procedures make it possible to invent new painting styles with no artistic training data. We believe that our approach can help pave the way for the ethical employment of generative AI in art, without infringing upon the originality of human creators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#8220;&#19982;AI&#19968;&#21516;&#20570;&#26790;&#8221;&#30340;&#23398;&#20064;&#36710;&#38388;&#65292;&#35753;&#39640;&#20013;&#29983;&#20102;&#35299;&#21644;&#20351;&#29992;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#24182;&#21453;&#24605;&#20102;&#28508;&#22312;&#21033;&#30410;&#21644;&#20260;&#23475;&#65292;&#25552;&#20986;&#20102;&#25919;&#31574;&#24847;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.12013</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#24335;AI&#26500;&#24314;&#26790;&#24819;
&lt;/p&gt;
&lt;p&gt;
Constructing Dreams using Generative AI. (arXiv:2305.12013v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#8220;&#19982;AI&#19968;&#21516;&#20570;&#26790;&#8221;&#30340;&#23398;&#20064;&#36710;&#38388;&#65292;&#35753;&#39640;&#20013;&#29983;&#20102;&#35299;&#21644;&#20351;&#29992;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#24182;&#21453;&#24605;&#20102;&#28508;&#22312;&#21033;&#30410;&#21644;&#20260;&#23475;&#65292;&#25552;&#20986;&#20102;&#25919;&#31574;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#24037;&#20855;&#20026;&#38738;&#23569;&#24180;&#24341;&#20837;&#20102;&#26032;&#30340;&#12289;&#26131;&#20110;&#25509;&#35302;&#30340;&#23186;&#20307;&#21019;&#20316;&#24418;&#24335;&#12290;&#23427;&#20204;&#20063;&#24341;&#21457;&#20102;&#26377;&#20851;&#34394;&#20551;&#23186;&#20307;&#29983;&#25104;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#38544;&#31169;&#21644;AI&#29983;&#25104;&#33402;&#26415;&#25152;&#26377;&#26435;&#31561;&#26041;&#38754;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;&#37492;&#20110;&#29983;&#25104;&#24335;AI&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#38738;&#23569;&#24180;&#20351;&#29992;&#30340;&#20135;&#21697;&#20013;&#65292;&#20351;&#20182;&#20204;&#29702;&#35299;&#36825;&#20123;&#24037;&#20855;&#30340;&#24037;&#20316;&#21407;&#29702;&#21450;&#20854;&#20351;&#29992;&#21644;&#35823;&#29992;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#34920;&#36798;&#20182;&#20204;&#24819;&#35937;&#20013;&#30340;&#26410;&#26469;&#36523;&#20221;&#26469;&#20419;&#36827;&#23398;&#29983;&#29983;&#25104;&#24335;AI&#23398;&#20064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#36710;&#38388;&#65292;&#8220;&#19982;AI&#19968;&#21516;&#20570;&#26790;&#8221;&#65292;&#23398;&#29983;&#20204;&#20102;&#35299;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#20351;&#29992;&#25991;&#26412;&#36716;&#22270;&#20687;&#29983;&#25104;&#31639;&#27861;&#21019;&#24314;&#20102;&#20182;&#20204;&#30340;&#24418;&#35937;&#26410;&#26469;&#26790;&#24819;&#65292;&#24182;&#21453;&#24605;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#30340;&#28508;&#22312;&#21033;&#30410;&#21644;&#20260;&#23475;&#65292;&#24182;&#23601;&#35838;&#22530;&#20013;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#30340;&#25919;&#31574;&#21457;&#34920;&#20102;&#24847;&#35265;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;34&#21517;&#39640;&#20013;&#23398;&#29983;&#21442;&#21152;&#25105;&#20204;&#24037;&#20316;&#22346;&#30340;&#23398;&#20064;&#27963;&#21160;&#21644;&#20307;&#39564;&#12290;&#23398;&#29983;&#20204;&#36798;&#21040;&#20102;&#21019;&#24847;&#24615;&#30340;&#39640;&#24230;&#65292;&#24182;&#21453;&#24605;&#20102;AI&#24037;&#20855;&#30340;&#21033;&#24330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI tools introduce new and accessible forms of media creation for youth. They also raise ethical concerns about the generation of fake media, data protection, privacy and ownership of AI-generated art. Since generative AI is already being used in products used by youth, it is critical that they understand how these tools work and how they can be used or misused. In this work, we facilitated students' generative AI learning through expression of their imagined future identities. We designed a learning workshop - Dreaming with AI - where students learned about the inner workings of generative AI tools, used text-to-image generation algorithms to create their imaged future dreams, reflected on the potential benefits and harms of generative AI tools and voiced their opinions about policies for the use of these tools in classrooms. In this paper, we present the learning activities and experiences of 34 high school students who engaged in our workshops. Students reached creative l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OMPify&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#20018;&#34892;&#20195;&#30721;&#30340;&#20998;&#26512;&#65292;&#33258;&#21160;&#26816;&#27979;&#21644;&#39044;&#27979;&#24182;&#34892;&#20195;&#30721;&#20013;&#30340;OpenMP&#32534;&#35793;&#25351;&#31034;&#31526;&#21644;&#20849;&#20139;&#20869;&#23384;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11999</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22270;&#24418;&#26041;&#27861;&#20026;OpenMP&#24182;&#34892;&#21270;&#25552;&#20379;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Advising OpenMP Parallelization via a Graph-Based Approach with Transformers. (arXiv:2305.11999v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OMPify&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#20018;&#34892;&#20195;&#30721;&#30340;&#20998;&#26512;&#65292;&#33258;&#21160;&#26816;&#27979;&#21644;&#39044;&#27979;&#24182;&#34892;&#20195;&#30721;&#20013;&#30340;OpenMP&#32534;&#35793;&#25351;&#31034;&#31526;&#21644;&#20849;&#20139;&#20869;&#23384;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22810;&#26680;&#26550;&#26500;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#38656;&#35201;&#20849;&#20139;&#20869;&#23384;&#24182;&#34892;&#21270;&#26041;&#26696;&#12290;&#24403;&#21069;&#26368;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;OpenMP&#24182;&#34892;&#32534;&#31243;&#25509;&#21475;&#12290;&#34429;&#28982;&#25163;&#21160;&#32534;&#20889;&#24182;&#34892;&#20195;&#30721;&#26159;&#22797;&#26434;&#21644;&#36153;&#21147;&#30340;&#65292;&#20294;&#26159;&#35768;&#22810;&#30830;&#23450;&#24615;&#28304;&#21040;&#28304;&#65288;S2S&#65289;&#32534;&#35793;&#22120;&#24050;&#32463;&#28044;&#29616;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#24182;&#34892;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#36825;&#20123;&#32534;&#35793;&#22120;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;AI&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#22823;&#37327;&#30340;&#24320;&#28304;&#20195;&#30721;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#24182;&#34892;&#21270;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;OMPify&#65292;&#36890;&#36807;&#20018;&#34892;&#20195;&#30721;&#26469;&#26816;&#27979;&#21644;&#39044;&#27979;&#24182;&#34892;&#20195;&#30721;&#20013;&#30340;OpenMP&#32534;&#35793;&#25351;&#31034;&#31526;&#21644;&#20849;&#20139;&#20869;&#23384;&#23646;&#24615;&#65292;OMPify&#22522;&#20110;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#28304;&#20195;&#30721;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an ever-present need for shared memory parallelization schemes to exploit the full potential of multi-core architectures. The most common parallelization API addressing this need today is OpenMP. Nevertheless, writing parallel code manually is complex and effort-intensive. Thus, many deterministic source-to-source (S2S) compilers have emerged, intending to automate the process of translating serial to parallel code. However, recent studies have shown that these compilers are impractical in many scenarios. In this work, we combine the latest advancements in the field of AI and natural language processing (NLP) with the vast amount of open-source code to address the problem of automatic parallelization. Specifically, we propose a novel approach, called OMPify, to detect and predict the OpenMP pragmas and shared-memory attributes in parallel code, given its serial version. OMPify is based on a Transformer-based model that leverages a graph-based representation of source code that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11997</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#20559;&#31227;&#65292;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#21487;&#33021;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#21453;&#20107;&#23454;&#35299;&#37322;&#20248;&#21270;&#20013;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#23558;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#38752;&#36817;&#25968;&#25454;&#27969;&#24418;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#39640;&#27010;&#29575;&#40065;&#26834;&#24615;&#12290;&#26032;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{&lt;}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;EEG&#21644;EMG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#26469;&#33258;&#34987;&#21327;&#21161;&#20351;&#29992;&#20027;&#21160;&#30699;&#24418;&#22120;&#31227;&#21160;&#21491;&#33218;&#30340;&#21463;&#35797;&#32773;&#12290;&#30740;&#31350;&#32773;&#25925;&#24847;&#24341;&#20837;&#20102;&#19968;&#20123;&#38169;&#35823;&#65292;&#20197;&#26816;&#27979;&#26032;&#30340;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11996</link><description>&lt;p&gt;
EEG&#21644;EMG&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#20027;&#21160;&#30699;&#24418;&#22120;&#24341;&#20837;&#30340;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
EEG and EMG dataset for the detection of errors introduced by an active orthosis device. (arXiv:2305.11996v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;EEG&#21644;EMG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#26469;&#33258;&#34987;&#21327;&#21161;&#20351;&#29992;&#20027;&#21160;&#30699;&#24418;&#22120;&#31227;&#21160;&#21491;&#33218;&#30340;&#21463;&#35797;&#32773;&#12290;&#30740;&#31350;&#32773;&#25925;&#24847;&#24341;&#20837;&#20102;&#19968;&#20123;&#38169;&#35823;&#65292;&#20197;&#26816;&#27979;&#26032;&#30340;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21253;&#21547;&#30005;&#33041;&#22270;&#35889;&#65288;EEG&#65289;&#21644;&#32908;&#30005;&#22270;&#65288;EMG&#65289;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#26469;&#33258;8&#21517;&#34987;&#21327;&#21161;&#20351;&#29992;&#20027;&#21160;&#30699;&#24418;&#22120;&#31227;&#21160;&#21491;&#33218;&#30340;&#21463;&#35797;&#32773;&#12290;&#35813;&#35774;&#22791;&#25903;&#25345;&#32920;&#20851;&#33410;&#36816;&#21160;&#65292;&#21363;&#21491;&#33218;&#30340;&#23624;&#26354;&#21644;&#20280;&#23637;&#12290;&#24403;&#30699;&#24418;&#22120;&#20027;&#21160;&#31227;&#21160;&#34987;&#35797;&#32773;&#30340;&#25163;&#33218;&#26102;&#65292;&#25925;&#24847;&#24341;&#20837;&#20102;&#19968;&#20123;&#38169;&#35823;&#65292;&#25345;&#32493;&#26102;&#38388;&#24456;&#30701;&#12290;&#22312;&#27492;&#26399;&#38388;&#65292;&#30699;&#24418;&#22120;&#21521;&#30456;&#21453;&#30340;&#26041;&#21521;&#31227;&#21160;&#12290;&#26412;&#25991;&#35299;&#37322;&#20102;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#23637;&#31034;&#20102;&#36328;&#25152;&#26377;&#21463;&#35797;&#32773;&#30340;&#19968;&#20123;&#34892;&#20026;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#19968;&#20010;&#21463;&#35797;&#32773;&#25552;&#20379;&#20102;&#24179;&#22343;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#20998;&#26512;&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#36136;&#37327;&#21644;&#38169;&#35823;&#24341;&#20837;&#25152;&#24341;&#36215;&#30340;EEG&#27963;&#21160;&#30340;&#35265;&#35299;&#12290;&#22312;&#27492;&#20171;&#32461;&#30340;&#25968;&#25454;&#38598;&#26159;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#20026;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#24322;&#27493;&#26816;&#27979;&#38169;&#35823;&#30340;&#26032;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a dataset containing recordings of the electroencephalogram (EEG) and the electromyogram (EMG) from eight subjects who were assisted in moving their right arm by an active orthosis device. The supported movements were elbow joint movements, i.e., flexion and extension of the right arm. While the orthosis was actively moving the subject's arm, some errors were deliberately introduced for a short duration of time. During this time, the orthosis moved in the opposite direction. In this paper, we explain the experimental setup and present some behavioral analyses across all subjects. Additionally, we present an average event-related potential analysis for one subject to offer insights into the data quality and the EEG activity caused by the error introduction. The dataset described herein is openly accessible. The aim of this study was to provide a dataset to the research community, particularly for the development of new methods in the asynchronous detection of erroneo
&lt;/p&gt;</description></item><item><title>&#38754;&#23545;&#26085;&#30410;&#22797;&#26434;&#30340;&#20379;&#24212;&#38142;&#65292;&#21387;&#21147;&#26469;&#33258;&#20110;&#28040;&#36153;&#32773;&#12289;&#25209;&#35780;&#30340;&#20844;&#20247;&#20197;&#21450;&#20379;&#24212;&#38142;&#27861;&#35268;&#31561;&#65292;&#39046;&#20808;&#30340;&#20844;&#21496;&#27491;&#22312;&#23581;&#35797;&#36890;&#36807;&#31639;&#27861;&#39044;&#27979;&#21830;&#19994;&#12289;&#29615;&#22659;&#21644;&#31038;&#20250;&#39118;&#38505;&#26469;&#25913;&#21892;&#21171;&#24037;&#26465;&#20214;&#65292;&#24182;&#20026;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#32676;&#20307;&#25552;&#20379;&#25919;&#31574;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2305.11981</link><description>&lt;p&gt;
&#8220;&#32654;&#22909;&#30340;&#26032;&#20379;&#24212;&#38142;&#19990;&#30028;&#8221;&#65306;&#31639;&#27861;&#39044;&#27979;&#26102;&#20195;&#19979;&#30340;&#24037;&#20154;&#21457;&#22768;&#21644;&#21171;&#24037;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
"Sch\"one neue Lieferkettenwelt": Workers' Voice und Arbeitsstandards in Zeiten algorithmischer Vorhersage. (arXiv:2305.11981v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11981
&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#26085;&#30410;&#22797;&#26434;&#30340;&#20379;&#24212;&#38142;&#65292;&#21387;&#21147;&#26469;&#33258;&#20110;&#28040;&#36153;&#32773;&#12289;&#25209;&#35780;&#30340;&#20844;&#20247;&#20197;&#21450;&#20379;&#24212;&#38142;&#27861;&#35268;&#31561;&#65292;&#39046;&#20808;&#30340;&#20844;&#21496;&#27491;&#22312;&#23581;&#35797;&#36890;&#36807;&#31639;&#27861;&#39044;&#27979;&#21830;&#19994;&#12289;&#29615;&#22659;&#21644;&#31038;&#20250;&#39118;&#38505;&#26469;&#25913;&#21892;&#21171;&#24037;&#26465;&#20214;&#65292;&#24182;&#20026;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#32676;&#20307;&#25552;&#20379;&#25919;&#31574;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20379;&#24212;&#38142;&#30340;&#22797;&#26434;&#24615;&#21644;&#36234;&#26469;&#36234;&#32039;&#23494;&#30340;&#32806;&#21512;&#23545;&#20110;&#39046;&#20808;&#30340;&#20844;&#21496;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#21518;&#21220;&#25361;&#25112;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#65292;&#39046;&#20808;&#30340;&#20844;&#21496;&#22312;&#28040;&#36153;&#32773;&#12289;&#25209;&#35780;&#30340;&#20844;&#20247;&#20197;&#21450;&#20379;&#24212;&#38142;&#27861;&#35268;&#31561;&#21387;&#21147;&#19979;&#38656;&#35201;&#27604;&#20197;&#21069;&#26356;&#22810;&#22320;&#23545;&#20182;&#20204;&#30340;&#20379;&#24212;&#21830;&#21171;&#24037;&#26631;&#20934;&#36127;&#36131;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#39046;&#20808;&#20225;&#19994;&#29992;&#20110;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#31639;&#27861;&#39044;&#27979;&#21830;&#19994;&#39118;&#38505;&#65292;&#20197;&#21450;&#29615;&#22659;&#21644;&#31038;&#20250;&#39118;&#38505;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#31639;&#27861;&#39044;&#27979;&#30340;&#25216;&#26415;&#21644;&#25991;&#21270;&#26465;&#20214;&#65292;&#24182;&#35299;&#37322;&#20102;&#20174;&#39046;&#20808;&#20844;&#21496;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#22330;&#26223;&#65292;&#35828;&#26126;&#21644;&#21738;&#31181;&#31038;&#20250;&#21518;&#26524;&#19979;&#65292;&#39046;&#20808;&#30340;&#20844;&#21496;&#21487;&#20197;&#20351;&#29992;&#31639;&#27861;&#39044;&#27979;&#12290;&#36890;&#36807;&#36825;&#20123;&#22330;&#26223;&#65292;&#25105;&#20204;&#20026;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#32676;&#20307;&#25552;&#20379;&#20102;&#25919;&#31574;&#36873;&#25321;&#65292;&#20197;&#24110;&#21161;&#21457;&#23637;&#31639;&#27861;&#39044;&#27979;&#24182;&#25913;&#21892;&#21171;&#24037;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complexity and increasingly tight coupling of supply chains poses a major logistical challenge for leading companies. Another challenge is that leading companies -- under pressure from consumers, a critical public and legislative measures such as supply chain laws -- have to take more responsibility than before for their suppliers' labour standards. In this paper, we discuss a new approach that leading companies are using to try to address these challenges: algorithmic prediction of business risks, but also environmental and social risks. We describe the technical and cultural conditions for algorithmic prediction and explain how -- from the perspective of leading companies -- it helps to address both challenges. We then develop scenarios on how and with what kind of social consequences algorithmic prediction can be used by leading companies. From the scenarios, we derive policy options for different stakeholder groups to help develop algorithmic prediction towards improving labour
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;Benders&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24930;&#24615;&#38376;&#35786;&#39044;&#32422;&#31561;&#22823;&#35268;&#27169;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.11969</link><description>&lt;p&gt;
&#22522;&#20110;&#36923;&#36753;&#30340;Benders&#20998;&#35299;&#22312;&#31572;&#26696;&#38598;&#35268;&#21010;&#20013;&#29992;&#20110;&#24930;&#24615;&#38376;&#35786;&#39044;&#32422;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Logic-Based Benders Decomposition in Answer Set Programming for Chronic Outpatients Scheduling. (arXiv:2305.11969v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;Benders&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24930;&#24615;&#38376;&#35786;&#39044;&#32422;&#31561;&#22823;&#35268;&#27169;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31572;&#26696;&#38598;&#35268;&#21010;&#65288;ASP&#65289;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#22768;&#26126;&#24615;&#22320;&#23450;&#20041;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#23427;; ASP&#30340;&#23454;&#38469;&#24212;&#29992;&#26080;&#25968;&#65292;&#19988;&#24050;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#20960;&#20010;&#32422;&#26463;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#35299;&#20915;&#26102;&#38388;&#36890;&#24120;&#20197;&#36229;&#32447;&#24615;&#26041;&#24335;&#22686;&#38271;&#65288;&#36890;&#24120;&#26159;&#25351;&#25968;&#32423;&#65289;&#30456;&#23545;&#20110;&#23454;&#20363;&#30340;&#22823;&#23567;&#65292;&#36825;&#23545;&#20110;&#22823;&#22411;&#23454;&#20363;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#26159;&#23558;&#20248;&#21270;&#38382;&#39064;&#20998;&#21106;&#20026;&#39034;&#24207;&#35299;&#20915;&#30340;&#23376;&#38382;&#39064;&#65292;&#26576;&#20123;&#23376;&#38382;&#39064;&#25552;&#20132;&#21040;&#30001;&#20854;&#20182;&#23376;&#38382;&#39064;&#25351;&#23450;&#30340;&#20540;&#65292;&#24182;&#36890;&#36807;&#24182;&#32622;&#21333;&#20010;&#23376;&#38382;&#39064;&#30340;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#37325;&#26500;&#25972;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#20998;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#19968;&#26041;&#38754;&#26356;&#24555;&#65292;&#30001;&#20110;&#36229;&#32447;&#24615;&#34892;&#20026;;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#19981;&#25552;&#20379;&#20219;&#20309;&#26368;&#20248;&#24615;&#20445;&#35777;&#65306;&#25215;&#35834;&#35299;&#20915;&#19968;&#20010;&#23376;&#38382;&#39064;&#30340;&#20998;&#37197;&#21487;&#33021;&#20250;&#23558;&#26368;&#20248;&#35299;&#20174;&#25628;&#32034;&#31354;&#38388;&#20013;&#25490;&#38500;&#12290;&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#22522;&#20110;&#36923;&#36753;&#30340;Benders&#20998;&#35299;&#65288;LBBD&#65289;&#35777;&#26126;&#26159;&#35299;&#20915;&#22823;&#35268;&#27169;&#20248;&#21270;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#30830;&#20445;&#26368;&#20248;&#24615;&#20445;&#35777;;&#20294;&#26159;&#65292;&#22312;ASP&#20013;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;LBBD&#26041;&#27861;&#65292;&#29992;&#20110;&#31649;&#29702;&#24322;&#26500;&#36164;&#28304;&#21644;&#30446;&#26631;&#30340;&#23454;&#38469;&#38382;&#39064;&#24930;&#24615;&#38376;&#35786;&#30340;&#39044;&#32422;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#65292;&#21487;&#25193;&#23637;&#24615;&#21644;&#26368;&#20248;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;ASP&#21644;&#22522;&#20110;CP&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Answer Set Programming (ASP), the user can define declaratively a problem and solve it with efficient solvers; practical applications of ASP are countless and several constraint problems have been successfully solved with ASP. On the other hand, solution time usually grows in a superlinear way (often, exponential) with respect to the size of the instance, which is impractical for large instances. A widely used approach is to split the optimization problem into sub-problems that are solved in sequence, some committing to the values assigned by others, and reconstructing a valid assignment for the whole problem by juxtaposing the solutions of the single sub-problems. On the one hand this approach is much faster, due to the superlinear behavior; on the other hand, it does not provide any guarantee of optimality: committing to the assignment of one sub-problem can rule out the optimal solution from the search space. In other research areas, Logic-Based Benders Decomposition (LBBD) prove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20010;&#24615;&#21270;&#28201;&#24230;&#30340;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#33258;&#21160;&#35843;&#25972;&#28201;&#24230;&#20197;&#20351;&#24471;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.11965</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#35821;&#20041;&#37117;&#26159;&#24179;&#31561;&#30340;&#65306;&#20855;&#26377;&#33258;&#23450;&#20041;&#28201;&#24230;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization. (arXiv:2305.11965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20010;&#24615;&#21270;&#28201;&#24230;&#30340;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#33258;&#21160;&#35843;&#25972;&#28201;&#24230;&#20197;&#20351;&#24471;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#24615;&#30340;&#26041;&#24335;&#65292;&#20248;&#21270;&#20855;&#26377;&#20010;&#24615;&#21270;&#28201;&#24230;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#26222;&#36941;&#20570;&#27861;&#26159;&#23558;&#20840;&#23616;&#28201;&#24230;&#21442;&#25968;&#964;&#29992;&#20110;&#25152;&#26377;&#25968;&#25454;&#65292;&#24573;&#30053;&#20102;&#8220;&#19981;&#26159;&#25152;&#26377;&#30340;&#35821;&#20041;&#37117;&#26159;&#24179;&#31561;&#30340;&#8221;&#36825;&#20010;&#20107;&#23454;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#23637;&#31034;&#38271;&#23614;&#20998;&#24067;&#26102;&#65292;&#19981;&#21516;&#30340;&#38170;&#28857;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327;&#30340;&#31867;&#20284;&#35821;&#20041;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#24615;&#20248;&#21270;&#65288;DRO&#65289;&#30340;&#26032;&#22411;&#40065;&#26834;&#23545;&#27604;&#25439;&#22833;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#964;&#30340;&#24433;&#21709;&#30340;&#30452;&#35273;&#21644;&#33258;&#21160;&#28201;&#24230;&#20010;&#24615;&#21270;&#30340;&#26426;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38543;&#26426;&#31639;&#27861;&#26469;&#20248;&#21270;&#40065;&#26834;&#24615;&#23545;&#27604;&#25439;&#22833;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#22823;&#22411;&#23567;&#25209;&#37327;&#22823;&#23567;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33258;&#21160;&#23398;&#20064;&#27599;&#20010;&#26679;&#26412;&#30340;&#21512;&#36866;&#964;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20855;&#26377;&#39057;&#32321;&#35821;&#20041;&#30340;&#26679;&#26412;&#20351;&#29992;&#36739;&#22823;&#28201;&#24230;&#20197;&#20445;&#25345;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to optimize a contrastive loss with individualized temperatures in a principled and systematic manner for self-supervised learning. The common practice of using a global temperature parameter $\tau$ ignores the fact that ``not all semantics are created equal", meaning that different anchor data may have different numbers of samples with similar semantics, especially when data exhibits long-tails. First, we propose a new robust contrastive loss inspired by distributionally robust optimization (DRO), providing us an intuition about the effect of $\tau$ and a mechanism for automatic temperature individualization. Then, we propose an efficient stochastic algorithm for optimizing the robust contrastive loss with a provable convergence guarantee without using large mini-batch sizes. Theoretical and experimental results show that our algorithm automatically learns a suitable $\tau$ for each sample. Specifically, samples with frequent semantics use large temperatures to k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11930</link><description>&lt;p&gt;
PyTorch&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#8212;&#8212;&#38754;&#21521;spotPython&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
PyTorch Hyperparameter Tuning -- A Tutorial for spotPython. (arXiv:2305.11930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#35843;&#25972;&#65288;&#25110;&#36229;&#21442;&#25968;&#20248;&#21270;&#65289;&#30340;&#30446;&#26631;&#26159;&#20248;&#21270;&#36229;&#21442;&#25968;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;spotPython&#26159;&#30693;&#21517;&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;SPOT&#30340;Python&#29256;&#26412;&#65292;SPOT&#24050;&#32463;&#22312;R&#32534;&#31243;&#29615;&#22659;&#20013;&#20026;&#32479;&#35745;&#20998;&#26512;&#24320;&#21457;&#20102;&#21313;&#24180;&#20197;&#19978;&#12290;PyTorch&#26159;&#19968;&#31181;&#22522;&#20110;GPU&#21644;CPU&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#24211;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#12290;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#65292;&#20171;&#32461;&#20102;spotPython&#20197;&#21450;&#19982;Ray Tune&#30340;&#31616;&#30701;&#27604;&#36739;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;spotPython&#30340;&#20351;&#29992;&#32463;&#39564;&#65292;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;hook&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model. spotPython (``Sequential Parameter Optimization Toolbox in Python'') is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This document shows how to integrate the spotPython hyperparameter tuner into the PyTorch training workflow. As an example, the results of the CIFAR10 image classifier are used. In addition to an introduction to spotPython, this tutorial also includes a brief comparison with Ray Tune, a Python library for running experiments and tuning hyperparameters. This comparison is based on the PyTorch hyperparameter tuning tutorial. The advantages and disadvantages of both approaches are discussed. We show that spotPytho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#23454;&#29616;&#20102;&#33410;&#33021;&#30340;AI&#30828;&#20214;&#35774;&#35745;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11928</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#30340;&#33410;&#33021;&#19988;&#21487;&#35299;&#37322;AI&#30828;&#20214;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Energy-frugal and Interpretable AI Hardware Design using Learning Automata. (arXiv:2305.11928v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#23454;&#29616;&#20102;&#33410;&#33021;&#30340;AI&#30828;&#20214;&#35774;&#35745;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#19979;&#65292;&#33021;&#25928;&#26159;&#23454;&#29616;&#24378;&#22823;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#37325;&#35201;&#38656;&#27714;&#12290;&#36890;&#36807;&#33410;&#33021;&#30340;&#35745;&#31639;&#36164;&#28304;&#37197;&#32622;&#23454;&#29616;&#30828;&#20214;&#21152;&#36895;&#26159;&#38477;&#20302;&#33021;&#32791;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26032;&#20852;&#24212;&#29992;&#36824;&#38656;&#35201;&#37319;&#29992;&#21487;&#35299;&#37322;&#20915;&#31574;&#27169;&#22411;&#65292;&#20197;&#30830;&#31435;&#36131;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#22330;&#26223;&#20013;&#25552;&#20379;&#21487;&#36798;&#29366;&#24577;&#38656;&#35201;&#39069;&#22806;&#30340;&#36164;&#28304;&#65292;&#36825;&#32473;&#33021;&#25928;&#35774;&#35745;&#24102;&#26469;&#20102;&#20914;&#31361;&#24615;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;Tsetlin&#26426;&#22120;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#21407;&#29702;&#65292;&#19982;&#31639;&#26415;&#19981;&#21516;&#65292;&#21463;&#30410;&#20110;&#33258;&#28982;&#36923;&#36753;&#25903;&#25745;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#35843;&#25972;&#36229;&#21442;&#25968;&#26469;&#23454;&#29616;&#33410;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#30828;&#20214;&#35774;&#35745;&#65292;&#24182;&#20445;&#25345;&#39640;&#25928;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#28508;&#21147;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#20248;&#21270;&#25216;&#26415;&#19979;&#22312;&#21487;&#32534;&#31243;&#36923;&#36753;&#38376;&#38453;&#21015;&#65288;FPGAs&#65289;&#19978;&#23454;&#29616;&#20102;Tsetlin&#26426;&#22120;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#33258;&#21160;&#26426;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#26174;&#33879;&#30340;&#33021;&#28304;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy efficiency is a crucial requirement for enabling powerful artificial intelligence applications at the microedge. Hardware acceleration with frugal architectural allocation is an effective method for reducing energy. Many emerging applications also require the systems design to incorporate interpretable decision models to establish responsibility and transparency. The design needs to provision for additional resources to provide reachable states in real-world data scenarios, defining conflicting design tradeoffs between energy efficiency. is challenging.  Recently a new machine learning algorithm, called the Tsetlin machine, has been proposed. The algorithm is fundamentally based on the principles of finite-state automata and benefits from natural logic underpinning rather than arithmetic. In this paper, we investigate methods of energy-frugal artificial intelligence hardware design by suitably tuning the hyperparameters, while maintaining high learning efficacy. To demonstrate i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27604;&#36739;&#32467;&#26524;&#23637;&#31034;&#26041;&#27861;&#8212;&#8212;&#22810;&#20803;&#27604;&#36739;&#30697;&#38453;&#65288;MCM&#65289;&#65292;&#20351;&#24471;&#27604;&#36739;&#38598;&#21512;&#31283;&#23450;&#26080;&#35823;&#24046;&#65292;&#21487;&#36991;&#20813;&#24120;&#29992;&#26041;&#27861;&#23384;&#22312;&#30340;&#26080;&#24847;&#21644;&#26377;&#24847;&#30340;&#25805;&#32437;&#31354;&#38388;&#65292;&#24182;&#19988;&#20854;&#37319;&#29992;Python&#23454;&#29616;&#65292;&#24050;&#22312;&#20844;&#24320;&#25552;&#20379;&#12290;</title><link>http://arxiv.org/abs/2305.11921</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#27604;&#38598;&#21512;&#31283;&#23450;&#26080;&#35823;&#24046;&#30340;&#22810;&#37325;&#27604;&#36739;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Approach to Multiple Comparison Benchmark Evaluations that is Stable Under Manipulation of the Comparate Set. (arXiv:2305.11921v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27604;&#36739;&#32467;&#26524;&#23637;&#31034;&#26041;&#27861;&#8212;&#8212;&#22810;&#20803;&#27604;&#36739;&#30697;&#38453;&#65288;MCM&#65289;&#65292;&#20351;&#24471;&#27604;&#36739;&#38598;&#21512;&#31283;&#23450;&#26080;&#35823;&#24046;&#65292;&#21487;&#36991;&#20813;&#24120;&#29992;&#26041;&#27861;&#23384;&#22312;&#30340;&#26080;&#24847;&#21644;&#26377;&#24847;&#30340;&#25805;&#32437;&#31354;&#38388;&#65292;&#24182;&#19988;&#20854;&#37319;&#29992;Python&#23454;&#29616;&#65292;&#24050;&#22312;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#35780;&#20272;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#34913;&#37327;&#36827;&#27493;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24120;&#29992;&#30340;&#26041;&#27861;&#23545;&#20110;&#22810;&#20010;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#20934;&#27604;&#36739;&#32467;&#26524;&#20998;&#26512;&#21644;&#23637;&#31034;&#65292;&#22914;Dem\v{s}ar&#65288;2006&#65289;&#24341;&#20837;&#30340;&#20851;&#38190;&#24046;&#24322;&#22270;&#23384;&#22312;&#37325;&#22823;&#32570;&#38519;&#65292;&#24182;&#19988;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26080;&#24847;&#21644;&#26377;&#24847;&#30340;&#25805;&#32437;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27604;&#36739;&#32467;&#26524;&#23637;&#31034;&#26041;&#27861;&#8212;&#8212;&#22810;&#20803;&#27604;&#36739;&#30697;&#38453;&#65288;MCM&#65289;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#25104;&#23545;&#27604;&#36739;&#65292;&#25490;&#38500;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#25805;&#32437;&#23454;&#39564;&#32467;&#26524;&#30340;&#26041;&#24335;&#12290;MCM&#21487;&#29992;&#20110;&#26174;&#31034;&#20840;&#23545;&#27604;&#32467;&#26524;&#65292;&#25110;&#26174;&#31034;&#19968;&#20010;&#25110;&#22810;&#20010;&#36873;&#25321;&#30340;&#31639;&#27861;&#19982;&#25216;&#26415;&#30340;&#23545;&#27604;&#32467;&#26524;&#12290;MCM&#37319;&#29992;Python&#23454;&#29616;&#65292;&#24182;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The measurement of progress using benchmarks evaluations is ubiquitous in computer science and machine learning. However, common approaches to analyzing and presenting the results of benchmark comparisons of multiple algorithms over multiple datasets, such as the critical difference diagram introduced by Dem\v{s}ar (2006), have important shortcomings and, we show, are open to both inadvertent and intentional manipulation. To address these issues, we propose a new approach to presenting the results of benchmark comparisons, the Multiple Comparison Matrix (MCM), that prioritizes pairwise comparisons and precludes the means of manipulating experimental results in existing approaches. MCM can be used to show the results of an all-pairs comparison, or to show the results of a comparison between one or more selected algorithms and the state of the art. MCM is implemented in Python and is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#24230;&#29616;&#23454;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#31232;&#30095;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.11913</link><description>&lt;p&gt;
&#29992;&#20110;&#20174;&#31232;&#30095;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#37325;&#24314;&#38750;&#32447;&#24615;&#28023;&#27915;&#27874;&#28010;&#34920;&#38754;&#30456;&#20301;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning for phase-resolved reconstruction of nonlinear ocean wave surface elevations from sparse remote sensing data. (arXiv:2305.11913v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#24230;&#29616;&#23454;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#31232;&#30095;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#30456;&#20301;&#30456;&#20851;&#30340;&#27700;&#27874;&#26465;&#20214;&#23545;&#20110;&#28023;&#27915;&#24037;&#31243;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36828;&#31243;&#30417;&#27979;&#27874;&#28010;&#39044;&#27979;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#39318;&#20808;&#38656;&#35201;&#20174;&#31867;&#20284;&#38647;&#36798;&#30340;&#31232;&#30095;&#27979;&#37327;&#20013;&#37325;&#24314;&#27874;&#28010;&#34920;&#38754;&#12290;&#29616;&#26377;&#30340;&#37325;&#24314;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#31616;&#21270;&#30340;&#27169;&#22411;&#20551;&#35774;&#65292;&#36825;&#20250;&#24433;&#21709;&#25972;&#20010;&#39044;&#27979;&#36807;&#31243;&#30340;&#23454;&#26102;&#24615;&#25110;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#21644;Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20855;&#26377;&#39640;&#24230;&#29616;&#23454;&#24615;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#22343;&#21248;&#30340;&#19968;&#32500;&#32593;&#26684;&#19978;&#30001;&#27874;&#28010;&#27169;&#25311;&#30340;&#39640;&#38454;&#35889;&#26041;&#27861;&#21644;&#20960;&#20309;&#38647;&#36798;&#24314;&#27169;&#26041;&#27861;&#29983;&#25104;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#31181;&#27169;&#22411;&#37117;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#27874;&#28010;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate short-term prediction of phase-resolved water wave conditions is crucial for decision-making in ocean engineering. However, the initialization of remote-sensing-based wave prediction models first requires a reconstruction of wave surfaces from sparse measurements like radar. Existing reconstruction methods either rely on computationally intensive optimization procedures or simplistic modeling assumptions that compromise real-time capability or accuracy of the entire prediction process. We therefore address these issues by proposing a novel approach for phase-resolved wave surface reconstruction using neural networks based on the U-Net and Fourier neural operator (FNO) architectures. Our approach utilizes synthetic yet highly realistic training data on uniform one-dimensional grids, that is generated by the high-order spectral method for wave simulation and a geometric radar modeling approach. The investigation reveals that both models deliver accurate wave reconstruction resul
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28023;&#27915;&#35760;&#24518;&#25928;&#24212;&#24320;&#21457;&#20102;&#19968;&#20010;LSTM&#27169;&#22411;&#65292;&#32467;&#21512;&#36807;&#21435;&#30340;ASI&#21644;&#23612;&#23068;&#25351;&#25968;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;ASI&#39044;&#27979;&#65292;&#20026;&#25552;&#21069;&#21046;&#23450;&#31354;&#27668;&#36136;&#37327;&#31649;&#29702;&#35745;&#21010;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.11901</link><description>&lt;p&gt;
&#21033;&#29992;&#28023;&#27915;&#35760;&#24518;&#25928;&#24212;&#39044;&#27979;&#20013;&#22269;&#21335;&#26041;&#20908;&#23395;&#31354;&#27668;&#31283;&#23450;&#25351;&#25968;&#30340;&#38271;&#26399;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Long-lead forecasts of wintertime air stagnation index in southern China using oceanic memory effects. (arXiv:2305.11901v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28023;&#27915;&#35760;&#24518;&#25928;&#24212;&#24320;&#21457;&#20102;&#19968;&#20010;LSTM&#27169;&#22411;&#65292;&#32467;&#21512;&#36807;&#21435;&#30340;ASI&#21644;&#23612;&#23068;&#25351;&#25968;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;ASI&#39044;&#27979;&#65292;&#20026;&#25552;&#21069;&#21046;&#23450;&#31354;&#27668;&#36136;&#37327;&#31649;&#29702;&#35745;&#21010;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21033;&#20110;&#31354;&#27668;&#27745;&#26579;&#29289;&#30340;&#31232;&#37322;&#21644;&#28165;&#38500;&#65292;&#26159;&#23548;&#33268;&#31354;&#27668;&#27745;&#26579;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#31354;&#27668;&#31283;&#23450;&#25351;&#25968;&#65288;ASI&#65289;&#26159;&#27979;&#37327;&#22823;&#27668;&#28165;&#38500;&#31354;&#27668;&#27745;&#26579;&#29289;&#33021;&#21147;&#30340;&#19968;&#39033;&#37325;&#35201;&#27668;&#35937;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#36827;&#34892;&#38271;&#26399;ASI&#39044;&#25253;&#23545;&#20110;&#25552;&#21069;&#21046;&#23450;&#31354;&#27668;&#36136;&#37327;&#31649;&#29702;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#30001;&#28023;&#34920;&#28201;&#24230;&#24322;&#24120;&#25512;&#23548;&#20986;&#30340;&#31179;&#23395;&#23612;&#23068;&#25351;&#25968;&#19982;&#20013;&#22269;&#21335;&#26041;&#20908;&#23395;ASI&#21576;&#36127;&#30456;&#20851;&#65292;&#20026;&#39044;&#27979;&#20908;&#23395;ASI&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;ASI&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#20803;&#36755;&#20837;&#65288;&#36807;&#21435;&#30340;ASI&#21644;&#23612;&#23068;&#25351;&#25968;&#65289;&#27604;&#21333;&#20803;&#36755;&#20837;&#65288;&#20165;&#36807;&#21435;&#30340;ASI&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#38469;&#21644;&#39044;&#27979;ASI&#20043;&#38388;&#23454;&#29616;&#20102;0.778&#30340;&#30456;&#20851;&#31995;&#25968;&#65292;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stagnant weather condition is one of the major contributors to air pollution as it is favorable for the formation and accumulation of pollutants. To measure the atmosphere's ability to dilute air pollutants, Air Stagnation Index (ASI) has been introduced as an important meteorological index. Therefore, making long-lead ASI forecasts is vital to make plans in advance for air quality management. In this study, we found that autumn Ni\~no indices derived from sea surface temperature (SST) anomalies show a negative correlation with wintertime ASI in southern China, offering prospects for a prewinter forecast. We developed an LSTM-based model to predict the future wintertime ASI. Results demonstrated that multivariate inputs (past ASI and Ni\~no indices) achieve better forecast performance than univariate input (only past ASI). The model achieves a correlation coefficient of 0.778 between the actual and predicted ASI, exhibiting a high degree of consistency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102; ChatGPT&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35752;&#35770;&#23545;&#32654;&#22269;&#23398;&#29983;&#39044;&#26399;&#21171;&#21160;&#24066;&#22330;&#32467;&#26524;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#23398;&#29983;&#20449;&#24515;&#20250;&#38477;&#20302;&#65292;&#23545;&#26410;&#26469;&#30340;&#25910;&#20837;&#21069;&#26223;&#20445;&#25345;&#24754;&#35266;&#24577;&#24230;&#65292;&#36825;&#31181;&#24433;&#21709;&#24191;&#27867;&#23384;&#22312;&#20110;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#20013;&#12290;&#36825;&#20010;&#30740;&#31350;&#32473;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#31649;&#29702;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#20250;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23398;&#29983;&#30340;&#25285;&#24551;&#24182;&#25913;&#36827;&#25945;&#32946;&#35838;&#31243;&#65292;&#20351;&#23398;&#29983;&#26356;&#22909;&#22320;&#20934;&#22791;&#26410;&#26469;&#65292;&#36825;&#20010;&#26410;&#26469;&#24517;&#28982;&#20250;&#34987;AI&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2305.11900</link><description>&lt;p&gt;
ChatGPT&#19982;&#21171;&#21160;&#21147;&#24066;&#22330;&#65306;&#25581;&#31034;AI&#35752;&#35770;&#23545;&#23398;&#29983;&#25910;&#20837;&#39044;&#26399;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and the Labor Market: Unraveling the Effect of AI Discussions on Students' Earnings Expectations. (arXiv:2305.11900v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102; ChatGPT&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35752;&#35770;&#23545;&#32654;&#22269;&#23398;&#29983;&#39044;&#26399;&#21171;&#21160;&#24066;&#22330;&#32467;&#26524;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#23398;&#29983;&#20449;&#24515;&#20250;&#38477;&#20302;&#65292;&#23545;&#26410;&#26469;&#30340;&#25910;&#20837;&#21069;&#26223;&#20445;&#25345;&#24754;&#35266;&#24577;&#24230;&#65292;&#36825;&#31181;&#24433;&#21709;&#24191;&#27867;&#23384;&#22312;&#20110;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#20013;&#12290;&#36825;&#20010;&#30740;&#31350;&#32473;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#31649;&#29702;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#20250;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23398;&#29983;&#30340;&#25285;&#24551;&#24182;&#25913;&#36827;&#25945;&#32946;&#35838;&#31243;&#65292;&#20351;&#23398;&#29983;&#26356;&#22909;&#22320;&#20934;&#22791;&#26410;&#26469;&#65292;&#36825;&#20010;&#26410;&#26469;&#24517;&#28982;&#20250;&#34987;AI&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36127;&#38754;&#21644;&#27491;&#38754; ChatGPT&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35752;&#35770;&#23545;&#32654;&#22269;&#23398;&#29983;&#39044;&#26399;&#21171;&#21160;&#24066;&#22330;&#32467;&#26524;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25509;&#35302;AI&#35752;&#35770;&#21518;&#65292;&#23398;&#29983;&#20449;&#24515;&#20250;&#38477;&#20302;&#65292;&#29305;&#21035;&#26159;&#22312;&#38405;&#35835;&#36127;&#38754;&#24773;&#32490;&#30340;&#35752;&#35770;&#25688;&#24405;&#26102;&#65292;&#36825;&#31181;&#24433;&#21709;&#26356;&#21152;&#26126;&#26174;&#12290;&#19982;STEM&#19987;&#19994;&#19981;&#21516;&#65292;&#38750;STEM&#39046;&#22495;&#30340;&#23398;&#29983;&#34920;&#29616;&#20986;&#19981;&#23545;&#31216;&#21644;&#24754;&#35266;&#30340;&#20449;&#20208;&#21464;&#21270;&#65292;&#34920;&#26126;&#20182;&#20204;&#21487;&#33021;&#24863;&#21463;&#21040;&#26032;&#20852;AI&#25216;&#26415;&#30340;&#24433;&#21709;&#26356;&#24378;&#12290;&#23545;&#20110;&#26410;&#26469;&#25910;&#20837;&#30340;&#24754;&#35266;&#20449;&#20208;&#26356;&#26032;&#20063;&#26222;&#36941;&#23384;&#22312;&#20110;&#21508;&#20010;&#24615;&#21035;&#21644;GPA&#27700;&#24179;&#20043;&#38388;&#65292;&#36825;&#34920;&#26126;&#25152;&#26377;&#23398;&#29983;&#32676;&#20307;&#37117;&#23384;&#22312;&#24191;&#27867;&#30340;AI&#25285;&#24551;&#12290;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#31649;&#29702;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#21487;&#20197;&#23450;&#26399;&#19982;&#23398;&#29983;&#20114;&#21160;&#20197;&#35299;&#20915;&#20182;&#20204;&#30340;&#25285;&#24551;&#65292;&#24182;&#25913;&#36827;&#25945;&#32946;&#35838;&#31243;&#65292;&#20351;&#20182;&#20204;&#26356;&#22909;&#22320;&#20934;&#22791;&#26410;&#26469;&#65292;&#36825;&#20010;&#26410;&#26469;&#24517;&#28982;&#20250;&#34987;AI&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the causal impact of negatively and positively framed ChatGPT Artificial Intelligence (AI) discussions on US students' anticipated labor market outcomes. Our findings reveal students reduce their confidence regarding their future earnings prospects after exposure to AI debates, and this effect is more pronounced after reading discussion excerpts with a negative tone. Unlike STEM majors, students in Non-STEM fields show asymmetric and pessimistic belief changes, suggesting that they might feel more vulnerable to emerging AI technologies. Pessimistic belief updates regarding future earnings are also prevalent across gender and GPA levels, indicating widespread AI concerns among all student subgroups. Educators, administrators, and policymakers may regularly engage with students to address their concerns and enhance educational curricula to better prepare them for a future that will be inevitably shaped by AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#26031;&#22270;&#20687;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Leaky Integrate and Fire (LIF)&#31070;&#32463;&#20803;&#36827;&#34892;&#20449;&#24687;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#36895;&#29575;&#32534;&#30721;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#30340;&#21516;&#26102;&#36215;&#21040;&#20102;&#38477;&#20302;&#35745;&#31639;&#36127;&#36733;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.11898</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#33033;&#20914;&#32534;&#30721;&#22270;&#20687;&#21435;&#22122;&#31070;&#32463;&#20449;&#24687;&#22788;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural information coding for efficient spike-based image denoising. (arXiv:2305.11898v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#26031;&#22270;&#20687;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Leaky Integrate and Fire (LIF)&#31070;&#32463;&#20803;&#36827;&#34892;&#20449;&#24687;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#36895;&#29575;&#32534;&#30721;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#30340;&#21516;&#26102;&#36215;&#21040;&#20102;&#38477;&#20302;&#35745;&#31639;&#36127;&#36733;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#24050;&#32463;&#36229;&#36234;&#20102;&#20256;&#32479;&#31639;&#27861;&#22312;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#25968;&#24182;&#19981;&#36866;&#21512;&#35745;&#31639;&#25928;&#29575;&#65292;&#22240;&#27492;&#25191;&#34892;&#36215;&#26469;&#36807;&#20110;&#26114;&#36149;&#65292;&#26080;&#27861;&#22312;&#23884;&#20837;&#24335;&#21644;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#39640;&#26031;&#21435;&#22122;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;DCNN&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#36127;&#36733;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Leaky Integrate and Fire (LIF)&#31070;&#32463;&#20803;&#36827;&#34892;&#20449;&#24687;&#36716;&#25442;&#22788;&#29702;&#30340;&#24418;&#24335;&#20998;&#26512;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#20256;&#32479;&#30340;&#36895;&#29575;&#32534;&#30721;&#26426;&#21046;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#31070;&#32463;&#32534;&#30721;&#26041;&#26696;&#22312;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21435;&#22122;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;LIF&#31070;&#32463;&#20803;&#30340;SNNs&#21487;&#20197;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#21435;&#22122;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Deep Convolutional Neural Networks (DCNNs) have outreached the performance of classical algorithms for image restoration tasks. However most of these methods are not suited for computational efficiency and are therefore too expensive to be executed on embedded and mobile devices. In this work we investigate Spiking Neural Networks (SNNs) for Gaussian denoising, with the goal of approaching the performance of conventional DCNN while reducing the computational load. We propose a formal analysis of the information conversion processing carried out by the Leaky Integrate and Fire (LIF) neurons and we compare its performance with the classical rate-coding mechanism. The neural coding schemes are then evaluated through experiments in terms of denoising performance and computation efficiency for a state-of-the-art deep convolutional neural network. Our results show that SNNs with LIF neurons can provide competitive denoising performance but at a reduced computational cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35821;&#35328;&#25945;&#32946;&#20013;&#20171;&#20132;&#27969;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#24378;&#35843;&#35821;&#35328;&#25945;&#24072;&#31215;&#26497;&#21442;&#19982;CALL&#25945;&#24072;&#25945;&#32946;&#21644;&#19987;&#19994;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11897</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#20171;&#20132;&#27969;&#30340;&#20851;&#38190;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Critical Appraisal of Artificial Intelligence-Mediated Communication. (arXiv:2305.11897v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35821;&#35328;&#25945;&#32946;&#20013;&#20171;&#20132;&#27969;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#24378;&#35843;&#35821;&#35328;&#25945;&#24072;&#31215;&#26497;&#21442;&#19982;CALL&#25945;&#24072;&#25945;&#32946;&#21644;&#19987;&#19994;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#30340;&#25216;&#26415;&#24212;&#29992;&#24471;&#21040;&#20102;&#26174;&#33879;&#21457;&#23637;&#65292;&#29616;&#22312;&#34987;&#31216;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#35821;&#35328;&#23398;&#20064;&#65288;CALL&#65289;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25972;&#21512;&#21040;CALL&#20013;&#65292;&#26174;&#33879;&#25913;&#21464;&#20102;&#35821;&#35328;&#25945;&#32946;&#22312;&#35838;&#22530;&#20869;&#22806;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#25506;&#35752;&#20102;AI&#20013;&#20171;&#20132;&#27969;&#22312;&#35821;&#35328;&#25945;&#32946;&#20013;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#39318;&#20808;&#31616;&#35201;&#22238;&#39038;&#20102;&#25945;&#32946;&#20013;&#30340;AI&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20171;&#32461;&#20102;ICALL&#65292;&#24182;&#23545;AI&#39537;&#21160;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#12289;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITSs&#65289;&#12289;AI&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#30340;&#28508;&#21147;&#36827;&#34892;&#20102;&#20851;&#38190;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#35748;&#20026;&#65292;&#35821;&#35328;&#25945;&#24072;&#31215;&#26497;&#21442;&#19982;CALL&#25945;&#24072;&#25945;&#32946;&#21644;&#19987;&#19994;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36319;&#19978;&#19981;&#26029;&#21457;&#23637;&#30340;&#25216;&#26415;&#26223;&#35266;&#24182;&#25552;&#39640;&#33258;&#24049;&#30340;&#25945;&#23398;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last two decades, technology use in language learning and teaching has significantly advanced and is now referred to as Computer-Assisted Language Learning (CALL). Recently, the integration of Artificial Intelligence (AI) into CALL has brought about a significant shift in the traditional approach to language education both inside and outside the classroom. In line with this book's scope, I explore the advantages and disadvantages of AI-mediated communication in language education. I begin with a brief review of AI in education. I then introduce the ICALL and give a critical appraisal of the potential of AI-powered automatic speech recognition (ASR), Machine Translation (MT), Intelligent Tutoring Systems (ITSs), AI-powered chatbots, and Extended Reality (XR). In conclusion, I argue that it is crucial for language teachers to engage in CALL teacher education and professional development to keep up with the ever-evolving technology landscape and improve their teaching effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#35201;&#20171;&#32461;&#20102;&#36229;&#32423;&#33258;&#21160;&#21270;&#22312;IT&#34892;&#19994;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23558;AI&#24037;&#20855;&#19982;RPA&#30456;&#32467;&#21512;&#65292;&#36229;&#32423;&#33258;&#21160;&#21270;&#20026;&#20960;&#20046;&#25152;&#26377;&#30001;&#19994;&#21153;&#29992;&#25143;&#25191;&#34892;&#30340;&#37325;&#22797;&#25805;&#20316;&#25552;&#20379;&#33258;&#21160;&#21270;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#33041;&#26426;&#25509;&#21475;&#21644;&#20256;&#24863;&#22120;&#26469;&#25552;&#39640;&#36229;&#32423;&#33258;&#21160;&#21270;&#30340;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#27969;&#31243;&#37096;&#32626;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11896</link><description>&lt;p&gt;
&#36229;&#32423;&#33258;&#21160;&#21270;&#8212;&#8212;IT&#34892;&#19994;&#33258;&#21160;&#21270;&#30340;&#19979;&#19968;&#20010;&#22806;&#22260;&#35774;&#22791;
&lt;/p&gt;
&lt;p&gt;
Hyper-automation-The next peripheral for automation in IT industries. (arXiv:2305.11896v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#35201;&#20171;&#32461;&#20102;&#36229;&#32423;&#33258;&#21160;&#21270;&#22312;IT&#34892;&#19994;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23558;AI&#24037;&#20855;&#19982;RPA&#30456;&#32467;&#21512;&#65292;&#36229;&#32423;&#33258;&#21160;&#21270;&#20026;&#20960;&#20046;&#25152;&#26377;&#30001;&#19994;&#21153;&#29992;&#25143;&#25191;&#34892;&#30340;&#37325;&#22797;&#25805;&#20316;&#25552;&#20379;&#33258;&#21160;&#21270;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#33041;&#26426;&#25509;&#21475;&#21644;&#20256;&#24863;&#22120;&#26469;&#25552;&#39640;&#36229;&#32423;&#33258;&#21160;&#21270;&#30340;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#27969;&#31243;&#37096;&#32626;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20256;&#32479;&#19994;&#21153;&#27969;&#31243;&#33258;&#21160;&#21270;&#25193;&#23637;&#21040;&#29305;&#23450;&#27969;&#31243;&#20043;&#22806;&#30340;&#34892;&#20026;&#34987;&#31216;&#20026;&#36229;&#32423;&#33258;&#21160;&#21270;&#12290;&#36229;&#32423;&#33258;&#21160;&#21270;&#36890;&#36807;&#23558;AI&#24037;&#20855;&#19982;RPA&#30456;&#32467;&#21512;&#65292;&#20026;&#20960;&#20046;&#25152;&#26377;&#30001;&#19994;&#21153;&#29992;&#25143;&#25191;&#34892;&#30340;&#37325;&#22797;&#25805;&#20316;&#25552;&#20379;&#33258;&#21160;&#21270;&#12290;&#23427;&#33258;&#21160;&#21270;&#20102;&#19968;&#23478;&#20844;&#21496;&#30340;&#39030;&#23574;&#20154;&#25165;&#21487;&#33021;&#26080;&#27861;&#23436;&#25104;&#30340;&#22797;&#26434;IT&#19994;&#21153;&#27969;&#31243;&#37096;&#32626;&#65292;&#23454;&#29616;&#20102;&#26631;&#20934;&#19994;&#21153;&#27969;&#31243;&#37096;&#32626;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#12290;&#23427;&#36890;&#36807;&#23558;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#19982;AI&#21644;RPA&#33258;&#21160;&#21270;&#24037;&#20855;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#20219;&#21153;&#25968;&#23383;&#21270;&#12290;BCI&#19982;&#33258;&#21160;&#21270;&#24037;&#20855;&#32467;&#21512;&#20250;&#23558;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#26816;&#27979;&#21644;&#29983;&#25104;&#25552;&#21319;&#21040;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#12290;&#23427;&#20351;&#20225;&#19994;&#33021;&#22815;&#32467;&#21512;&#19994;&#21153;&#26234;&#33021;&#31995;&#32479;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20154;&#21592;&#30340;&#19987;&#19994;&#33021;&#21147;&#21644;&#33258;&#21160;&#21270;&#20307;&#39564;&#12290;&#26412;&#25991;&#31616;&#35201;&#35752;&#35770;&#20102;&#36229;&#32423;&#33258;&#21160;&#21270;&#21450;&#20854;&#22312;&#24403;&#20170;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#35752;&#35770;&#20102;BCI&#21644;&#20256;&#24863;&#22120;&#22914;&#20309;&#24110;&#21161;&#36229;&#32423;&#33258;&#21160;&#21270;&#24182;&#22686;&#24378;IT&#34892;&#19994;&#37096;&#32626;&#30340;&#33258;&#21160;&#21270;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extension of legacy business process automation beyond the bounds of specific processes is known as hyperautomation. Hyperautomation provides automation for nearly any repetitive action performed by business users by combining AI tools with RPA. It automates complex IT business processes that a company's top brains might not be able to complete. This is an end-to-end automation of a standard business process deployment. It enables automation to perform task digitalization by combining a brain computer interface (BCI) with AI and RPA automation tools. BCI, in conjunction with automation tools, will advance the detection and generation of automation processes to the next level. It allows enterprises to combine business intelligence systems, address complex requirements, and enhance human expertise and automation experience. Hyperautomation and its importance in today's environment are briefly discussed in this paper. The article then goes on to discuss how BCI and sensors might aid H
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#20891;&#20107;&#20154;&#26426;&#22242;&#38431;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#65288;MHC&#65289;&#23545;&#20110;&#38450;&#24481;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23439;&#35266;&#35774;&#35745;&#36873;&#39033;&#22242;&#38431;&#35774;&#35745;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25628;&#32034;&#19982;&#25628;&#25937;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.11892</link><description>&lt;p&gt;
&#20891;&#20107;&#20154;&#26426;&#22242;&#38431;&#20013;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#30340;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Designing for Meaningful Human Control in Military Human-Machine Teams. (arXiv:2305.11892v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#20891;&#20107;&#20154;&#26426;&#22242;&#38431;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#65288;MHC&#65289;&#23545;&#20110;&#38450;&#24481;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23439;&#35266;&#35774;&#35745;&#36873;&#39033;&#22242;&#38431;&#35774;&#35745;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25628;&#32034;&#19982;&#25628;&#25937;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#65288;MHC&#65289;&#30340;&#26041;&#27861;&#65292;&#20174;&#20891;&#20107;&#20154;&#26426;&#22242;&#38431;&#65288;HMT&#65289;&#30340;&#35282;&#24230;&#26469;&#23457;&#35270;&#38450;&#24481;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19977;&#20010;&#21407;&#21017;&#12290;&#39318;&#20808;&#65292;MHC&#24212;&#34987;&#35270;&#20026;&#25351;&#23548;&#25152;&#26377;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#26680;&#24515;&#30446;&#26631;&#12290;&#20854;&#27425;&#65292;MHC&#24433;&#21709;&#21040;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#25152;&#26377;&#37096;&#20998;&#65292;&#21253;&#25324;&#20154;&#31867;&#12289;&#26426;&#22120;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#20114;&#21160;&#21644;&#32972;&#26223;&#12290;&#26368;&#21518;&#65292;MHC&#24212;&#34987;&#35270;&#20026;&#19968;&#20010;&#36328;&#36234;&#36739;&#38271;&#26102;&#38388;&#30340;&#23646;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#21442;&#19982;&#32773;&#30340;&#20107;&#20808;&#21644;&#23454;&#26102;&#25511;&#21046;&#12290;&#20026;&#20102;&#25551;&#36848;&#23454;&#29616;MHC&#30340;&#23439;&#35266;&#35774;&#35745;&#36873;&#39033;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#22242;&#38431;&#35774;&#35745;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#20854;&#20013;&#24212;&#29992;&#20102;&#36825;&#20123;&#26041;&#27861;&#26469;&#26500;&#24819;&#19968;&#31181;&#28041;&#21450;&#26426;&#22120;&#20154;&#21644;&#22763;&#20853;&#22312;&#20891;&#20107;&#32972;&#26223;&#19979;&#36827;&#34892;&#25628;&#25937;&#20219;&#21153;&#30340;HMT&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose methods for analysis, design, and evaluation of Meaningful Human Control (MHC) for defense technologies from the perspective of military human-machine teaming (HMT). Our approach is based on three principles. Firstly, MHC should be regarded as a core objective that guides all phases of analysis, design and evaluation. Secondly, MHC affects all parts of the socio-technical system, including humans, machines, AI, interactions, and context. Lastly, MHC should be viewed as a property that spans longer periods of time, encompassing both prior and realtime control by multiple actors. To describe macrolevel design options for achieving MHC, we propose various Team Design Patterns. Furthermore, we present a case study, where we applied some of these methods to envision HMT, involving robots and soldiers in a search and rescue task in a military context.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#20998;&#26512;&#23398;&#29983;&#22242;&#38431;&#21512;&#20316;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#23454;&#29616;90&#65285;&#20197;&#19978;&#30340;&#26631;&#31614;&#31934;&#24230;&#65292;&#20026;&#20998;&#26512;&#22242;&#38431;&#39033;&#30446;&#30340;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#33021;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.11882</link><description>&lt;p&gt;
&#25506;&#32034;&#20351;&#29992;ChatGPT&#20998;&#26512;&#29616;&#26377;&#20998;&#31867;&#27861;&#20013;&#30340;&#23398;&#29983;&#22242;&#38431;&#21512;&#20316;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Efficacy of ChatGPT in Analyzing Student Teamwork Feedback with an Existing Taxonomy. (arXiv:2305.11882v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#20998;&#26512;&#23398;&#29983;&#22242;&#38431;&#21512;&#20316;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#23454;&#29616;90&#65285;&#20197;&#19978;&#30340;&#26631;&#31614;&#31934;&#24230;&#65292;&#20026;&#20998;&#26512;&#22242;&#38431;&#39033;&#30446;&#30340;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#33021;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22242;&#38431;&#21512;&#20316;&#26159;&#35768;&#22810;&#23398;&#26415;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#20123;&#24773;&#22659;&#19979;&#65292;&#22242;&#38431;&#25104;&#21592;&#20043;&#38388;&#30340;&#21453;&#39304;&#26159;&#20419;&#36827;&#25104;&#21151;&#21644;&#21487;&#25345;&#32493;&#22242;&#38431;&#21327;&#20316;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#22312;&#35838;&#22530;&#19978;&#65292;&#38543;&#30528;&#22242;&#38431;&#25968;&#37327;&#21644;&#25104;&#21592;&#25968;&#37327;&#20197;&#21450;&#35780;&#20272;&#39057;&#29575;&#30340;&#22686;&#21152;&#65292;&#35780;&#35770;&#30340;&#25968;&#37327;&#21487;&#33021;&#20250;&#20196;&#25945;&#24072;&#26080;&#27861;&#38405;&#35835;&#21644;&#36319;&#36394;&#65292;&#20174;&#32780;&#38590;&#20197;&#35782;&#21035;&#27169;&#24335;&#21644;&#23398;&#29983;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;ChatGPT&#65292;&#20998;&#26512;&#22242;&#38431;&#23398;&#20064;&#29615;&#22659;&#19979;&#23398;&#29983;&#30340;&#35780;&#35770;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;ChatGPT&#26681;&#25454;&#30001;&#31215;&#26497;&#21644;&#28040;&#26497;&#35780;&#35770;&#32452;&#25104;&#30340;&#29616;&#26377;&#26694;&#26550;&#65292;&#20934;&#30830;&#35782;&#21035;&#23398;&#29983;&#35780;&#35770;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#21487;&#20197;&#23454;&#29616;90&#65285;&#20197;&#19978;&#30340;&#26631;&#31614;&#31934;&#24230;&#65292;&#20026;&#20998;&#26512;&#22242;&#38431;&#39033;&#30446;&#30340;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#33021;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#23545;&#23398;&#26415;&#30028;&#21644;&#23454;&#36341;&#30028;&#30340;&#22242;&#38431;&#39033;&#30446;&#21453;&#39304;&#20998;&#26512;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teamwork is a critical component of many academic and professional settings. In those contexts, feedback between team members is an important element to facilitate successful and sustainable teamwork. However, in the classroom, as the number of teams and team members and frequency of evaluation increase, the volume of comments can become overwhelming for an instructor to read and track, making it difficult to identify patterns and areas for student improvement. To address this challenge, we explored the use of generative AI models, specifically ChatGPT, to analyze student comments in team based learning contexts. Our study aimed to evaluate ChatGPT's ability to accurately identify topics in student comments based on an existing framework consisting of positive and negative comments. Our results suggest that ChatGPT can achieve over 90\% accuracy in labeling student comments, providing a potentially valuable tool for analyzing feedback in team projects. This study contributes to the gro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#22312;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#29992;&#25143;&#20449;&#20219;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#36235;&#21183;&#65292;&#21628;&#21505;&#28548;&#28165;&#27010;&#24565;&#20197;&#36991;&#20813;&#21487;&#33021;&#30340;&#20449;&#20219;&#24046;&#36317;&#21644;&#35823;&#35299;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.11876</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#29992;&#25143;&#20449;&#20219;&#35805;&#35821;&#30340;&#25361;&#25112;&#19982;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Challenges and Trends in User Trust Discourse in AI. (arXiv:2305.11876v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#22312;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#29992;&#25143;&#20449;&#20219;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#36235;&#21183;&#65292;&#21628;&#21505;&#28548;&#28165;&#27010;&#24565;&#20197;&#36991;&#20813;&#21487;&#33021;&#30340;&#20449;&#20219;&#24046;&#36317;&#21644;&#35823;&#35299;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1990&#24180;&#30340;&#20114;&#32852;&#32593;&#38761;&#21629;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20449;&#24687;&#38761;&#21629;&#25913;&#21464;&#20102;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#19990;&#30028;&#12290;&#29616;&#22312;&#65292;&#26366;&#32463;&#34987;&#35270;&#20026;&#31185;&#24187;&#24819;&#27861;&#65288;&#21363;&#26426;&#22120;&#32479;&#27835;&#19990;&#30028;&#65289;&#30340;&#20107;&#24773;&#34987;&#35748;&#20026;&#26159;&#21487;&#33021;&#30340;&#12290;&#36825;&#22330;&#38761;&#21629;&#20063;&#24341;&#21457;&#20102;&#23545;&#26032;&#30340;&#30417;&#31649;&#23454;&#36341;&#30340;&#38656;&#27714;&#65292;&#20854;&#20013;&#29992;&#25143;&#20449;&#20219;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35805;&#35821;&#21457;&#25381;&#20102;&#26680;&#24515;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#28548;&#28165;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#29992;&#25143;&#20449;&#20219;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#21453;&#23545;&#20542;&#21521;&#20110;&#35774;&#35745;&#23481;&#26131;&#24341;&#36215;&#20449;&#20219;&#30772;&#35010;&#30340;&#20132;&#20114;&#30340;&#36235;&#21183;&#65292;&#21253;&#25324;&#30495;&#23454;&#21644;&#24863;&#30693;&#19978;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#20204;&#23545;&#20110;&#29992;&#25143;&#20449;&#20219;&#29702;&#35299;&#19981;&#28165;&#26224;&#65292;&#24182;&#24050;&#32463;&#24433;&#21709;&#21040;&#35745;&#31639;&#26426;&#31185;&#23398;&#65292;&#23588;&#20854;&#26159;&#22312;&#27979;&#37327;&#29992;&#25143;&#20449;&#20219;&#29305;&#24449;&#26041;&#38754;&#12290;&#30740;&#31350;&#21628;&#21505;&#28548;&#28165;&#36825;&#20123;&#27010;&#24565;&#65292;&#20197;&#36991;&#20813;&#22312;&#20154;&#24037;&#26234;&#33021;&#37319;&#29992;&#19982;&#36866;&#29992;&#20013;&#20986;&#29616;&#28508;&#22312;&#30340;&#20449;&#20219;&#24046;&#36317;&#21644;&#35823;&#35299;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet revolution in 1990, followed by the data-driven and information revolution, has transformed the world as we know it. Nowadays, what seam to be 10 to 20 years ago, a science fiction idea (i.e., machines dominating the world) is seen as possible. This revolution also brought a need for new regulatory practices where user trust and artificial Intelligence (AI) discourse has a central role. This work aims to clarify some misconceptions about user trust in AI discourse and fight the tendency to design vulnerable interactions that lead to further breaches of trust, both real and perceived. Findings illustrate the lack of clarity in understanding user trust and its effects on computer science, especially in measuring user trust characteristics. It argues for clarifying those notions to avoid possible trust gaps and misinterpretations in AI adoption and appropriation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25226;&#30740;&#31350;&#20013;&#37096;&#20998;&#20869;&#23481;&#20132;&#30001;&#29983;&#25104;&#24335;AI&#23436;&#25104;&#65292;&#20250;&#23548;&#33268;&#20154;&#20204;&#19981;&#20449;&#20219;&#21644;&#36140;&#20302;&#30740;&#31350;&#20154;&#21592;&#21644;&#31185;&#23398;&#36755;&#20986;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#21040;&#29983;&#25104;&#24335;AI&#20351;&#29992;&#30340;&#25253;&#21578;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11873</link><description>&lt;p&gt;
&#30001;&#29983;&#25104;&#24335;AI&#21512;&#20316;&#21019;&#20316;&#30340;&#30740;&#31350;&#30340;&#35780;&#20215;&#65306;&#23454;&#39564;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Judgments of research co-created by generative AI: experimental evidence. (arXiv:2305.11873v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25226;&#30740;&#31350;&#20013;&#37096;&#20998;&#20869;&#23481;&#20132;&#30001;&#29983;&#25104;&#24335;AI&#23436;&#25104;&#65292;&#20250;&#23548;&#33268;&#20154;&#20204;&#19981;&#20449;&#20219;&#21644;&#36140;&#20302;&#30740;&#31350;&#20154;&#21592;&#21644;&#31185;&#23398;&#36755;&#20986;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#21040;&#29983;&#25104;&#24335;AI&#20351;&#29992;&#30340;&#25253;&#21578;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24341;&#20837;&#24341;&#21457;&#20102;&#20851;&#20110;&#29983;&#25104;&#24335;AI&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65307;LLMs&#65289;&#30340;&#20351;&#29992;&#30340;&#20844;&#20849;&#20105;&#35770;&#65292;&#21253;&#25324;&#30740;&#31350;&#20154;&#21592;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#23558;&#30740;&#31350;&#36807;&#31243;&#30340;&#26576;&#20123;&#37096;&#20998;&#22996;&#25176;&#32473;LLMs&#26159;&#21542;&#20250;&#23548;&#33268;&#20154;&#20204;&#19981;&#20449;&#20219;&#21644;&#36140;&#20302;&#30740;&#31350;&#20154;&#21592;&#21644;&#31185;&#23398;&#36755;&#20986;&#12290;&#21442;&#19982;&#32773;&#65288;N=402&#65289;&#32771;&#34385;&#19968;&#20010;&#23558;&#30740;&#31350;&#36807;&#31243;&#30340;&#20803;&#32032;&#22996;&#25176;&#32473;&#21338;&#22763;&#29983;&#25110;LLM&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#24182;&#23545;&#20197;&#19979;&#36827;&#34892;&#35780;&#20998;&#65306;&#65288;1&#65289;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#65292;&#65288;2&#65289;&#30456;&#20449;&#31185;&#23398;&#23478;&#30417;&#30563;&#26410;&#26469;&#39033;&#30446;&#65292;&#20197;&#21450;&#65288;3&#65289;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;&#20154;&#20204;&#35748;&#20026;&#23558;&#20219;&#21153;&#22996;&#25176;&#32473;LLMs&#27604;&#22996;&#25176;&#32473;&#20154;&#31867;&#19981;&#22826;&#21487;&#25509;&#21463;&#65288;d=-0.78&#65289;&#12290;&#22996;&#25176;&#32473;LLMs&#20063;&#20250;&#38477;&#20302;&#20154;&#20204;&#23545;&#20110;&#30417;&#30563;&#26410;&#26469;&#30740;&#31350;&#39033;&#30446;&#30340;&#20449;&#20219;&#65288;d=-0.80&#65289;&#65292;&#32780;&#19988;&#20154;&#20204;&#35748;&#20026;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#20250;&#26356;&#20302;&#65288;d=-0.85&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#36140;&#20540;&#22914;&#20309;&#36716;&#21270;&#20026;LLMs&#20351;&#29992;&#30340;&#20302;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of ChatGPT has fuelled a public debate on the use of generative AI (large language models; LLMs), including its use by researchers. In the current work, we test whether delegating parts of the research process to LLMs leads people to distrust and devalue researchers and scientific output. Participants (N=402) considered a researcher who delegates elements of the research process to a PhD student or LLM, and rated (1) moral acceptability, (2) trust in the scientist to oversee future projects, and (3) the accuracy and quality of the output. People judged delegating to an LLM as less acceptable than delegating to a human (d = -0.78). Delegation to an LLM also decreased trust to oversee future research projects (d = -0.80), and people thought the results would be less accurate and of lower quality (d = -0.85). We discuss how this devaluation might transfer into the underreporting of generative AI use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.11863</link><description>&lt;p&gt;
&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21333;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#22823;&#33041;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#22823;&#33041;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#31867;&#20284;GPT-2&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26159;&#21542;&#26356;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;OPT&#21644;LLaMA&#31995;&#21015;&#65289;&#26356;&#36866;&#29992;&#20110;&#39044;&#27979;&#20351;&#29992;fMRI&#35760;&#24405;&#30340;&#22823;&#33041;&#21453;&#24212;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20174;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#36328;3&#20010;&#21463;&#35797;&#32773;&#30340;&#20445;&#30041;&#27979;&#35797;&#38598;&#30456;&#20851;&#24615;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;&#24403;&#25193;&#23637;fMRI&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#26102;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#23545;&#25968;&#32447;&#24615;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23545;&#20351;&#29992;HuBERT&#65292;WavLM&#21644;Whisper&#30340;&#22768;&#23398;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#24102;&#26469;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22122;&#38899;&#22825;&#33457;&#26495;&#20998;&#26512;&#20102;&#36825;&#20123;&#22823;&#35268;&#27169;&#19988;&#39640;&#24615;&#33021;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales log-linearly with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.11828</link><description>&lt;p&gt;
LLM&#22312;&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#23545;&#20110;&#21046;&#23450;&#20020;&#24202;&#20915;&#31574;&#21644;&#21307;&#30103;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#21046;&#20316;&#36825;&#26679;&#30340;&#32508;&#36848;&#24456;&#36153;&#21147;&#19988;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#24456;&#22810;&#38382;&#39064;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#35777;&#25454;&#32508;&#36848;&#65292;&#21363;&#20351;&#36825;&#20123;&#32508;&#36848;&#21487;&#29992;&#65292;&#22312;&#23457;&#26597;&#36807;&#31243;&#20013;&#21487;&#33021;&#24050;&#32463;&#36807;&#26102;&#12290;&#29616;&#22312;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#33021;&#22815;&#29983;&#25104;&#38271;&#31687;&#25991;&#26412;&#65292;&#36825;&#24847;&#21619;&#30528;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#30340;&#35825;&#20154;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34394;&#26500;&#25110;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#65292;LLM&#26377;&#26102;&#20250;&#20135;&#29983;&#19981;&#20934;&#30830;&#65288;&#29978;&#33267;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65289;&#30340;&#25991;&#26412;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#20351;LLM&#22312;&#26368;&#22909;&#24773;&#20917;&#19979;&#26080;&#27861;&#20351;&#29992;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20250;&#24102;&#26469;&#21361;&#38505;&#12290;&#23545;&#20110;LLM&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;&#30340;&#22823;&#22810;&#25968;&#35752;&#35770;&#19982;&#20855;&#20307;&#24212;&#29992;&#33073;&#31163;&#20102;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23450;&#24615;&#25551;&#36848;LLM&#22312;&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#12290;&#25105;&#20204;&#23545;16&#20301;&#22269;&#38469;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11595</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;&#65306;&#36890;&#36807;&#36777;&#35770;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#37327;&#26679;&#26412;&#36890;&#35782;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#25317;&#26377;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21508;&#31181;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#25506;&#32034;&#20004;&#20010;&#25110;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#19981;&#21516;&#21644;&#31934;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#22312;7&#20010;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;LLMs&#19981;&#20165;&#36890;&#36807;&#22949;&#21327;&#21644;&#21453;&#39539;&#21464;&#24471;&#26356;&#20855;&#20869;&#37096;&#19968;&#33268;&#24615;&#65292;&#32780;&#19988;&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 commonsense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#27880;&#24847;&#21147;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11474</link><description>&lt;p&gt;
RAMiT&#65306;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration. (arXiv:2305.11474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#27880;&#24847;&#21147;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#35768;&#22810;&#24037;&#20316;&#22312;&#22270;&#20687;&#24674;&#22797;&#65288;IR&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;Transformer&#30340;IR&#26041;&#27861;&#21482;&#20381;&#38752;&#26412;&#22320;&#25110;&#20840;&#23616;&#29305;&#24449;&#65292;&#23548;&#33268;&#25509;&#21463;&#22495;&#26377;&#38480;&#25110;&#23384;&#22312;&#21442;&#25968;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;IR&#32593;&#32476;&#65306;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#23427;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#32500;&#24230;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;D-RAMiT&#65289;&#22359;&#65292;&#22312;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#22810;&#22836;&#24182;&#34892;&#35745;&#31639;&#21452;&#21521;&#65288;&#31354;&#38388;&#21644;&#36890;&#36947;&#65289;&#33258;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#21452;&#21521;&#20851;&#27880;&#24110;&#21161;&#24444;&#27492;&#24357;&#34917;&#23545;&#26041;&#30340;&#32570;&#28857;&#65292;&#28982;&#21518;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;&#65288;H-RAMi&#65289;&#23618;&#65292;&#23427;&#34917;&#20607;&#20687;&#32032;&#32423;&#20449;&#24687;&#20002;&#22833;&#24182;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#22312;IR&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#20197;&#25552;&#39640;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;IR&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#21435;&#22122;&#12289;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;JPEG&#22270;&#20687;&#21435;&#22359;&#65292;&#25105;&#20204;&#30340;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22823;&#22823;&#20943;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although many recent works have made advancements in the image restoration (IR) field, they often suffer from an excessive number of parameters. Another issue is that most Transformer-based IR methods focus only on either local or global features, leading to limited receptive fields or deficient parameter issues. To address these problems, we propose a lightweight IR network, Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which compute bi-dimensional (spatial and channel) self-attentions in parallel with different numbers of multi-heads. The bi-dimensional attentions help each other to complement their counterpart's drawbacks and are then mixed. Additionally, we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that compensates for pixel-level information losses and utilizes semantic information while maintaining an efficient hierarchical structure. Furthermore, we revisit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;SimOAP&#31574;&#30053;&#65292;&#36890;&#36807;&#36807;&#37319;&#26679;&#21644;&#21518;&#35780;&#20272;&#26469;&#25552;&#39640;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11130</link><description>&lt;p&gt;
SimOAP&#65306;&#36890;&#36807;&#36807;&#37319;&#26679;&#21644;&#21518;&#35780;&#20272;&#25552;&#39640;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation. (arXiv:2305.11130v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;SimOAP&#31574;&#30053;&#65292;&#36890;&#36807;&#36807;&#37319;&#26679;&#21644;&#21518;&#35780;&#20272;&#26469;&#25552;&#39640;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#27969;&#30021;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#35282;&#33394;&#20026;&#22522;&#30784;&#30340;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#20063;&#26159;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#23545;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#20215;&#20540;&#25968;&#25454;&#36807;&#28388;&#12289;&#27169;&#22411;&#32467;&#26500;&#20462;&#25913;&#25110;&#30446;&#26631;&#20989;&#25968;&#35774;&#35745;&#19978;&#65292;&#20294;&#23427;&#20204;&#30340;&#25913;&#36827;&#26377;&#38480;&#65292;&#24182;&#19988;&#24456;&#38590;&#25512;&#24191;&#21040;&#25152;&#26377;&#31867;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#25105;&#20204;&#32771;&#34385;&#36275;&#22815;&#22810;&#30340;&#29983;&#25104;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#38382;&#39064;&#22312;&#20110;&#22823;&#35268;&#27169;&#21709;&#24212;&#29983;&#25104;&#21644;&#30446;&#26631;&#21709;&#24212;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;SimOAP&#31574;&#30053;&#65292;&#21363;&#36807;&#37319;&#26679;&#21644;&#21518;&#35780;&#20272;&#12290;&#36807;&#37319;&#26679;&#38454;&#27573;&#36890;&#36807;&#29616;&#26377;&#35757;&#32451;&#27169;&#22411;&#30340;&#29616;&#25104;&#33976;&#39311;&#21644;&#21387;&#32553;&#26041;&#27861;&#39640;&#25928;&#22320;&#33719;&#21462;&#22823;&#35268;&#27169;&#21709;&#24212;&#65292;&#21518;&#35780;&#20272;&#38454;&#27573;&#22312;&#36807;&#37319;&#26679;&#29983;&#25104;&#30340;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#19968;&#20010;&#22909;&#30340;&#21709;&#24212;&#65292;&#24182;&#25552;&#39640;&#26368;&#32456;&#21709;&#24212;&#30340;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#20844;&#20849;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we find that language models can produce consistent and coherent responses if we consider enough generations. Thus, the problems lay in large-scale response generation and target response selection. In this work, a simple but effective two-stage SimOAP strategy is proposed, i.e., over-sampling and post-evaluation. The over-sampling stage takes large-scale responses from existing trained models efficiently via off-the-shelf distilling and compressing methods, and the post-evaluation stage selects a goo
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#20173;&#38754;&#20020;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10435</link><description>&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65306;&#21551;&#29992;&#25216;&#26415;&#12289;&#28508;&#22312;&#24212;&#29992;&#12289;&#26032;&#20852;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10435
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#20173;&#38754;&#20020;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#20195;&#34920;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#22823;&#31361;&#30772;&#65292;&#23558;&#25105;&#20204;&#25512;&#21521;&#24320;&#21457;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#21644;&#20351;&#29992;&#35821;&#35328;&#36827;&#34892;&#20132;&#27969;&#30340;&#26426;&#22120;&#12290;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#22312;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#30028;&#31038;&#21306;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#30693;&#21517;&#24230;&#65292;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21450;&#30456;&#20851;&#39046;&#22495;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#21644;&#26377;&#25928;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#36825;&#20419;&#20351;&#36827;&#34892;&#20102;&#26412;&#32508;&#36848;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65292;&#21253;&#25324;&#20854;&#26550;&#26500;&#12289;&#24037;&#20316;&#36807;&#31243;&#12289;&#35757;&#32451;&#36807;&#31243;&#12289;&#21551;&#29992;&#25216;&#26415;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#26412;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#35813;&#27169;&#22411;&#38754;&#20020;&#30340;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Generative Pre-trained Transformer models represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. Generative Pre-trained Transformer models are based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, Generative Pre-trained Transformer models have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the Generative Pre-trained Transformer, including its architecture, working process, training procedures, enabling technologies, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;Random Continuous Embedding&#65292;RCE&#65289;&#65292;&#33021;&#22815;&#25552;&#39640; Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#22823;&#24133;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10308</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#32771;&#34385;&#34920;&#26684;&#25968;&#25454;&#25968;&#25454;&#22686;&#24378;&#30340;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;
Rethinking Data Augmentation for Tabular Data in Deep Learning. (arXiv:2305.10308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;Random Continuous Embedding&#65292;RCE&#65289;&#65292;&#33021;&#22815;&#25552;&#39640; Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#22823;&#24133;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#26684;&#24335;&#12290;&#34429;&#28982;&#22312;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#26641;&#24418;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65307;&#20294;&#26368;&#36817;&#30340;&#25991;&#29486;&#25253;&#21578;&#31216;&#65292;Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;&#22312;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#20027;&#23548;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29420;&#29305;&#32467;&#26500;&#21644;&#39640;&#22797;&#26434;&#24615;&#65292;&#34920;&#26684;&#25968;&#25454;&#30340;&#25968;&#25454;&#22686;&#24378;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#27169;&#22411;&#32467;&#26500;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#19968;&#36215;&#25552;&#20986;&#12290;&#22240;&#27492;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23545;&#27604;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;RCE&#65289;&#65292;&#36890;&#36807;&#21521;&#36830;&#32493;&#21464;&#37327;&#27880;&#20837;&#22122;&#22768;&#26469;&#29983;&#25104;&#22686;&#24378;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126; RCE &#22312;&#20351;&#29992; Transformer-based &#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#26102;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#31579;&#36873;&#30740;&#31350;&#20197;&#26174;&#31034; RCE &#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126; RCE &#20351; Transformer-based &#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is the most widely used data format in machine learning (ML). While tree-based methods outperform DL-based methods in supervised learning, recent literature reports that self-supervised learning with Transformer-based models outperforms tree-based methods. In the existing literature on self-supervised learning for tabular data, contrastive learning is the predominant method. In contrastive learning, data augmentation is important to generate different views. However, data augmentation for tabular data has been difficult due to the unique structure and high complexity of tabular data. In addition, three main components are proposed together in existing methods: model structure, self-supervised learning methods, and data augmentation. Therefore, previous works have compared the performance without comprehensively considering these components, and it is not clear how each component affects the actual performance.  In this study, we focus on data augmentation to address these 
&lt;/p&gt;</description></item><item><title>UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10306</link><description>&lt;p&gt;
UniEX&#65306;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25552;&#21462;&#30340;&#32479;&#19968;&#20449;&#24687;&#25277;&#21462;&#30340;&#26377;&#25928;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10306
&lt;/p&gt;
&lt;p&gt;
UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#33539;&#24335;&#65292;&#23427;&#19982;&#20219;&#20309;&#27169;&#24335;&#26684;&#24335;&#20860;&#23481;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026; token-pair &#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#31181;&#32479;&#19968;&#30340;&#25552;&#21462;&#26694;&#26550; UniEX&#65292;&#23558;&#25152;&#26377;&#25552;&#21462;&#30446;&#26631;&#37117;&#32479;&#19968;&#20998;&#35299;&#20026;&#32852;&#21512;&#36328;&#24230;&#26816;&#27979;&#12289;&#20998;&#31867;&#21644;&#20851;&#32852;&#38382;&#39064;&#12290;UniEX &#21487;&#20197;&#21516;&#26102;&#32534;&#30721;&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#23398;&#20064;&#39044;&#23450;&#20041;&#20449;&#24687;&#30340;&#24191;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102; traffine &#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#21253;&#25324;&#20219;&#21153;&#12289;&#26631;&#31614;&#21644;&#20869;&#37096; token &#22312;&#20869;&#30340;&#24322;&#26500;&#22240;&#32032;&#38598;&#25104;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#30697;&#38453;&#33719;&#24471;&#25552;&#21462;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniEX &#22312; $14$&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#37117;&#20248;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on $14$ benchmar
&lt;/p&gt;</description></item><item><title>MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10250</link><description>&lt;p&gt;
MemoryBank: &#29992;&#38271;&#26399;&#35760;&#24518;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10250
&lt;/p&gt;
&lt;p&gt;
MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20854;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#19981;&#36275;&#20043;&#22788;&#26159;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#36825;&#22312;&#38656;&#35201;&#25345;&#32493;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#20363;&#22914;&#20010;&#20154;&#20276;&#20387;&#31995;&#32479;&#21644;&#24515;&#29702;&#21672;&#35810;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemoryBank&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;LLM&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#12290;MemoryBank&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;&#20026;&#20102;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#24182;&#26377;&#36873;&#25321;&#22320;&#20445;&#23384;&#35760;&#24518;&#65292;MemoryBank&#37319;&#29992;&#20102;&#21463;Ebbinghaus&#36951;&#24536;&#26354;&#32447;&#29702;&#35770;&#21551;&#21457;&#30340;&#35760;&#24518;&#26356;&#26032;&#26426;&#21046;&#65292;&#36825;&#26679;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26681;&#25454;&#26102;&#38388;&#21644;&#35760;&#24518;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#36951;&#24536;&#21644;&#21152;&#24378;&#35760;&#24518;&#65292;&#20174;&#32780;&#20026;LLM&#25552;&#20379;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>BIMT&#26041;&#27861;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#30452;&#25509;&#23637;&#31034;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.08746</link><description>&lt;p&gt;
&#35265;&#35777;&#23601;&#26159;&#20449;&#20208;&#65306;&#33041;&#21551;&#21457;&#27169;&#22359;&#21270;&#35757;&#32451;&#20419;&#36827;&#26426;&#29702;&#35808;&#37322;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability. (arXiv:2305.08746v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08746
&lt;/p&gt;
&lt;p&gt;
BIMT&#26041;&#27861;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#30452;&#25509;&#23637;&#31034;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33041;&#21551;&#21457;&#27169;&#22359;&#21270;&#35757;&#32451;&#65288;Brain-Inspired Modular Training, BIMT&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#21152;&#27169;&#22359;&#21270;&#21644;&#21487;&#35808;&#37322;&#12290;BIMT&#20174;&#22823;&#33041;&#21463;&#21551;&#21457;&#65292;&#23558;&#31070;&#32463;&#20803;&#23884;&#20837;&#21040;&#20960;&#20309;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25104;&#26412;&#19982;&#31070;&#32463;&#20803;&#36830;&#25509;&#38271;&#24230;&#25104;&#27491;&#27604;&#30340;&#26041;&#24335;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;BIMT&#21487;&#20197;&#20026;&#35768;&#22810;&#31616;&#21333;&#20219;&#21153;&#21457;&#29616;&#26377;&#29992;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25581;&#31034;&#20102;&#31526;&#21495;&#20844;&#24335;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#36793;&#30028;&#21644;&#20998;&#31867;&#29305;&#24449;&#65292;&#20197;&#21450;&#31639;&#27861;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#12290;&#30452;&#25509;&#30524;&#30555;&#30475;&#21040;&#27169;&#22359;&#30340;&#33021;&#21147;&#21487;&#20197;&#34917;&#20805;&#24403;&#21069;&#30340;&#26426;&#29702;&#35299;&#37322;&#31574;&#30053;&#65292;&#20363;&#22914;&#25506;&#38024;&#65292;&#24178;&#39044;&#25110;&#20957;&#35270;&#25152;&#26377;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#32456;&#27490;&#33258;&#21160;&#26426;&#65292;&#22312;LTLf&#27169;&#22411;&#26816;&#26597;&#20013;&#20351;&#29992;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#20250;&#26356;&#21152;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.08319</link><description>&lt;p&gt;
&#20851;&#20110;&#26377;&#38480;&#36712;&#36857;&#19978;&#32508;&#21512;&#31574;&#30053;&#30340;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Strategies in Synthesis Over Finite Traces. (arXiv:2305.08319v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#32456;&#27490;&#33258;&#21160;&#26426;&#65292;&#22312;LTLf&#27169;&#22411;&#26816;&#26597;&#20013;&#20351;&#29992;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#20250;&#26356;&#21152;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LTLf&#65288;&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#20013;&#30340;&#26377;&#38480;&#36712;&#36857;&#65289;&#30340;&#32508;&#21512;&#26041;&#27861;&#36816;&#29992;&#20110;&#39564;&#35777;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;LTLf&#27169;&#22411;&#26816;&#26597;&#24182;&#19981;&#31616;&#21333;&#65292;&#30001;LTLf&#32508;&#21512;&#29983;&#25104;&#30340;&#31574;&#30053;&#21487;&#20197;&#29992;&#21040;&#26377;&#38480;&#20294;&#26080;&#30028;&#38271;&#24230;&#25110;&#26080;&#38480;&#38271;&#24230;&#30340;&#32456;&#27490;&#21644;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#20013;&#12290;&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#19982;&#32456;&#27490;&#33258;&#21160;&#26426;&#22312;&#27169;&#22411;&#26816;&#26597;&#20013;&#30340;&#21306;&#21035;&#65292;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#26174;&#31034;&#20986;LTLf&#27169;&#22411;&#26816;&#26597;&#20013;&#20351;&#29992;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#27604;&#32456;&#27490;&#33258;&#21160;&#26426;&#25351;&#25968;&#32423;&#26356;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The innovations in reactive synthesis from {\em Linear Temporal Logics over finite traces} (LTLf) will be amplified by the ability to verify the correctness of the strategies generated by LTLf synthesis tools. This motivates our work on {\em LTLf model checking}. LTLf model checking, however, is not straightforward. The strategies generated by LTLf synthesis may be represented using {\em terminating} transducers or {\em non-terminating} transducers where executions are of finite-but-unbounded length or infinite length, respectively. For synthesis, there is no evidence that one type of transducer is better than the other since they both demonstrate the same complexity and similar algorithms.  In this work, we show that for model checking, the two types of transducers are fundamentally different. Our central result is that LTLf model checking of non-terminating transducers is \emph{exponentially harder} than that of terminating transducers. We show that the problems are EXPSPACE-complete
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;CPACE&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.08135</link><description>&lt;p&gt;
&#21306;&#20998;&#20808;&#20110;&#22238;&#31572;&#65306;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#20316;&#20026;&#24120;&#35782;&#38382;&#31572;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering. (arXiv:2305.08135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;CPACE&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#33719;&#21462;&#19981;&#21516;&#30340;&#30693;&#35782;&#65292;&#22312;&#26576;&#20123;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#21463;&#21040;&#26816;&#32034;&#30693;&#35782;&#30340;&#29305;&#24615;&#38480;&#21046;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#21516;&#26102;&#20174;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#21306;&#20998;&#24615;&#26041;&#38754;&#21463;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CPACE&#65292;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#32473;&#23450;&#20505;&#36873;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing knowledge-enhanced methods have achieved remarkable results in certain QA tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose CPACE, a Concept-centric Prompt-bAsed Contrastive Explanation Generation model, which aims to convert obtained symbolic knowledge into a contrastive explanation for better distinguishing the differences among given candidates. Firstly, following previous works, we retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module. After that, we generate corresponding contrastive explanations using acquired symbolic knowledge and explanation prompts as guidance for better modeling the knowledge distinguishment and interpretability. Finally, we regard the generated contrastive explanation as external knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.07845</link><description>&lt;p&gt;
&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#24179;&#22343;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20250;&#32858;&#38598;&#35757;&#32451;&#20110;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#20197;&#33719;&#24471;&#34920;&#29616;&#33391;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#29702;&#23578;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#26469;&#30740;&#31350;&#27169;&#22411;&#24179;&#22343;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21363;&#20351;&#20840;&#23616;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20063;&#21487;&#33021;&#20559;&#31163;&#30406;&#22320;&#24213;&#37096;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;C-LLM&#65289;&#65292;&#20197;&#21450;&#23427;&#22312;&#22797;&#26434;&#23398;&#29983;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#21644;&#23457;&#26597;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.07605</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#25945;&#32946;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative AI: Implications and Applications for Education. (arXiv:2305.07605v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;C-LLM&#65289;&#65292;&#20197;&#21450;&#23427;&#22312;&#22797;&#26434;&#23398;&#29983;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#21644;&#23457;&#26597;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;11&#26376;ChatGPT&#30340;&#25512;&#20986;&#24341;&#21457;&#20102;&#19968;&#20123;&#25945;&#32946;&#24037;&#20316;&#32773;&#30340;&#24656;&#24908;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#20854;&#20182;&#20154;&#30340;&#28909;&#24773;&#12290;&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#36825;&#20010;&#22823;&#20254;&#19979;&#65292;ChatGPT&#26159;&#19968;&#31995;&#21015;&#25216;&#26415;&#30340;&#20363;&#23376;&#65292;&#29992;&#20110;&#25552;&#20379;&#35745;&#31639;&#26426;&#29983;&#25104;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#20854;&#20182;&#25968;&#23383;&#21270;&#23186;&#20307;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20043;&#19968;&#8212;&#8212;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(C-LLM)&#65292;&#20197;&#21450;&#20854;&#22312;&#22797;&#26434;&#23398;&#29983;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#21644;&#23457;&#26597;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22312;&#35752;&#35770;&#20013;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20869;&#22312;&#38480;&#21046;&#65292;&#21363;&#32465;&#23450;&#20110;&#35821;&#26009;&#24211;&#21450;&#20854;&#36890;&#36807;&#20108;&#36827;&#21046;&#34920;&#31034;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#22312;&#36825;&#20123;&#38480;&#21046;&#20043;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#20013;&#26032;&#20852;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#38388;&#22270;&#21644;&#21338;&#24328;&#35770;&#32593;&#32476;&#24418;&#25104;&#65292;&#26088;&#22312;&#35299;&#20915;&#26102;&#38388;&#36335;&#24452;&#20445;&#35777;&#25152;&#26377;&#23545;&#20043;&#38388;&#30340;&#21487;&#36798;&#24615;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07494</link><description>&lt;p&gt;
&#26102;&#38388;&#32593;&#32476;&#21019;&#24314;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Temporal Network Creation Games. (arXiv:2305.07494v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#38388;&#22270;&#21644;&#21338;&#24328;&#35770;&#32593;&#32476;&#24418;&#25104;&#65292;&#26088;&#22312;&#35299;&#20915;&#26102;&#38388;&#36335;&#24452;&#20445;&#35777;&#25152;&#26377;&#23545;&#20043;&#38388;&#30340;&#21487;&#36798;&#24615;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#32593;&#32476;&#19981;&#26159;&#38745;&#24577;&#30340;&#23545;&#35937;&#65292;&#32780;&#26159;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#22312;&#26368;&#36817;&#20960;&#24180;&#20869;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#22270;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;&#22312;&#26102;&#38388;&#22270;&#20013;&#65292;&#25105;&#20204;&#26377;&#19968;&#32452;&#22266;&#23450;&#30340;&#33410;&#28857;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#36830;&#36890;&#24615;&#21482;&#22312;&#26576;&#20123;&#26102;&#38388;&#27493;&#39588;&#19978;&#21487;&#29992;&#12290;&#36825;&#24341;&#21457;&#20102;&#22823;&#37327;&#31639;&#27861;&#38382;&#39064;&#65292;&#20854;&#20013;&#26368;&#20026;&#31361;&#20986;&#30340;&#26159;&#23547;&#25214;&#26102;&#38388;&#24352;&#37327;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#26102;&#38388;&#36335;&#24452;&#20445;&#35777;&#25152;&#26377;&#23545;&#20043;&#38388;&#30340;&#21487;&#36798;&#24615;&#30340;&#35745;&#31639;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#20165;&#30693;&#36947;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#38598;&#20013;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32593;&#32476;&#24182;&#19981;&#26159;&#30001;&#20013;&#22830;&#35774;&#35745;&#24072;&#22609;&#36896;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#35768;&#22810;&#25112;&#30053;&#20195;&#29702;&#20154;&#30340;&#20114;&#21160;&#32780;&#20135;&#29983;&#21644;&#21457;&#23637;&#30340;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#26159;&#26368;&#36817;&#23545;&#21338;&#24328;&#35770;&#32593;&#32476;&#24418;&#25104;&#27169;&#22411;&#36827;&#34892;&#23494;&#38598;&#30740;&#31350;&#30340;&#25512;&#21160;&#21147;&#37327;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#36817;&#26399;&#30340;&#30740;&#31350;&#26041;&#21521;&#32467;&#21512;&#22312;&#19968;&#36215;&#65306;&#26102;&#38388;&#22270;&#21644;&#21338;&#24328;&#35770;&#32593;&#32476;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most networks are not static objects, but instead they change over time. This observation has sparked rigorous research on temporal graphs within the last years. In temporal graphs, we have a fixed set of nodes and the connections between them are only available at certain time steps. This gives rise to a plethora of algorithmic problems on such graphs, most prominently the problem of finding temporal spanners, i.e., the computation of subgraphs that guarantee all pairs reachability via temporal paths. To the best of our knowledge, only centralized approaches for the solution of this problem are known. However, many real-world networks are not shaped by a central designer but instead they emerge and evolve by the interaction of many strategic agents. This observation is the driving force of the recent intensive research on game-theoretic network formation models.  In this work we bring together these two recent research directions: temporal graphs and game-theoretic network formation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06695</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#29992;&#20110;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#65292;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#36890;&#24120;&#29992;&#20110;&#35782;&#21035;&#29289;&#31181;&#21644;&#20010;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#19978;&#22686;&#24378;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#31232;&#26377;&#31867;&#21035;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#38754;&#65292;&#35813;&#39046;&#22495;&#23578;&#26410;&#36827;&#34892;&#23581;&#35797;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#65292;&#26088;&#22312;&#38544;&#24335;&#32534;&#30721;&#36328;&#22495;&#20851;&#32852;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36825;&#31181;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#30452;&#25509;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65288;LTR&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20110;32&#20010;&#29289;&#31181;&#12289;&#36229;&#36807;30,000&#20010;&#28014;&#28216;&#26377;&#23380;&#34411;&#22771;&#30340;&#26174;&#24494;&#22270;&#20687;&#24182;&#19982;&#29420;&#31435;&#30340;&#36951;&#20256;&#25968;&#25454;&#26679;&#26412;&#19968;&#36215;&#20351;&#29992;&#26469;&#23454;&#39564;&#23460;&#23637;&#29616;&#20102;&#35813;&#27010;&#24565;&#30340;&#25928;&#21147;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;&#20174;&#19994;&#32773;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35270;&#35273;-&#36951;&#20256;&#23545;&#40784;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR) particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest speci
&lt;/p&gt;</description></item><item><title>&#33258;&#20027;GIS&#26159;&#19968;&#31181;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#20855;&#26377;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65306;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;</title><link>http://arxiv.org/abs/2305.06453</link><description>&lt;p&gt;
&#33258;&#20027;GIS&#65306;&#19979;&#19968;&#20195;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;GIS
&lt;/p&gt;
&lt;p&gt;
Autonomous GIS: the next-generation AI-powered GIS. (arXiv:2305.06453v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06453
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;GIS&#26159;&#19968;&#31181;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#20855;&#26377;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65306;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; ChatGPT &#65292;&#23637;&#31034;&#20102;&#23545;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#25512;&#29702;&#12289;&#21019;&#36896;&#24615;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#19982;&#25506;&#32034;&#12290;&#25105;&#20204;&#37319;&#29992;LLM&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;&#8220;&#33258;&#20027;GIS&#8221;&#30340;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65288;GIS&#65289;&#65292;&#20197;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#26469;&#35299;&#20915;&#31354;&#38388;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#33258;&#20027;GIS&#23558;&#38656;&#35201;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65292;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#20027;GIS&#30340;&#35774;&#35745;&#21407;&#21017;&#26469;&#23454;&#29616;&#36825;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65292;&#20174;&#20449;&#24687;&#20805;&#20998;&#24615;&#12289;LLM&#33021;&#21147;&#21644;&#20195;&#29702;&#26550;&#26500;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#31216;&#20026;LLM-Geo &#65292;&#23427;&#22312;Python&#29615;&#22659;&#20013;&#20351;&#29992;GPT-4 API&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, demonstrate a strong understanding of human natural language and have been explored and applied in various fields, including reasoning, creative writing, code generation, translation, and information retrieval. By adopting LLM as the reasoning core, we propose Autonomous GIS, an AI-powered geographic information system (GIS) that leverages the LLM's general abilities in natural language understanding, reasoning and coding for addressing spatial problems with automatic spatial data collection, analysis and visualization. We envision that autonomous GIS will need to achieve five autonomous goals including self-generating, self-organizing, self-verifying, self-executing, and self-growing. We introduce the design principles of autonomous GIS to achieve these five autonomous goals from the aspects of information sufficiency, LLM ability, and agent architecture. We developed a prototype system called LLM-Geo using GPT-4 API in a Python environme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.04160</link><description>&lt;p&gt;
X-LLM: &#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#35270;&#20026;&#22806;&#35821;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21551;&#21160;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#22522;&#20110;&#39640;&#32423;LLM&#30340;GPT-4&#34920;&#29616;&#20986;&#36229;&#24120;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#24402;&#21151;&#20110;&#19982;&#20197;&#21069;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#20102;&#26356;&#20808;&#36827;&#30340;LLM&#12290;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;GPT-4&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;X-LLM&#65292;&#36890;&#36807;&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;&#12289;&#35821;&#38899;&#12289;&#35270;&#39057;&#65289;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGLM&#65289;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;X-LLM&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#20010;&#20923;&#32467;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;LLM&#23545;&#40784;&#65292;&#20854;&#20013;&#8220;X&#8221;&#34920;&#31034;&#22810;&#27169;&#24577;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35270;&#39057;&#65292;&#8220;L&#8221;&#34920;&#31034;&#35821;&#35328;&#12290;X-LLM&#30340;&#35757;&#32451;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;1&#65289;&#36716;&#25442;&#22810;&#27169;&#24577;&#20449;&#24687;&#65306;&#31532;&#19968;&#38454;&#27573;&#20998;&#21035;&#35757;&#32451;&#27599;&#20010;X2L&#25509;&#21475;&#19982;&#20854;&#21508;&#33258;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#36755;&#20837;&#21040;ChatGLM&#20013;&#12290;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimod
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2305.01918</link><description>&lt;p&gt;
&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#30340;&#31163;&#25955;&#24615;&#20351;&#24471;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#38590;&#20197;&#20445;&#35777;&#12290;&#34429;&#28982;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26631;&#31614;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26679;&#26412;&#23545;&#65292;&#20294;&#20173;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#21453;&#39304;&#26469;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;CLAIF&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;AI&#21453;&#39304;&#26500;&#24314;&#24102;&#26377;&#32454;&#31890;&#24230;&#26679;&#26412;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#26679;&#26412;&#23545;&#65292;&#20197;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#20154;&#24037;&#21453;&#39304;&#21644;AI&#21453;&#39304;&#20026;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings. However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings. Experimental results show that our method achieves stat
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#35859;&#30340;&#26032;&#20852;&#25216;&#33021;&#26159;&#30740;&#31350;&#32773;&#20998;&#26512;&#30340;&#20135;&#29289;&#65292;&#19981;&#26159;&#27169;&#22411;&#34892;&#20026;&#30340;&#22522;&#26412;&#21464;&#21270;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#24230;&#37327;&#26631;&#20934;&#36873;&#25321;&#21644;&#21487;&#33021;&#30740;&#31350;&#20154;&#21592;&#30340;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#26032;&#20852;&#25216;&#33021;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.15004</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#34920;&#29616;&#30340;&#26032;&#20852;&#25216;&#33021;&#26159;&#21542;&#20026;&#24187;&#35273;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Emergent Abilities of Large Language Models a Mirage?. (arXiv:2304.15004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.15004
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#35859;&#30340;&#26032;&#20852;&#25216;&#33021;&#26159;&#30740;&#31350;&#32773;&#20998;&#26512;&#30340;&#20135;&#29289;&#65292;&#19981;&#26159;&#27169;&#22411;&#34892;&#20026;&#30340;&#22522;&#26412;&#21464;&#21270;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#24230;&#37327;&#26631;&#20934;&#36873;&#25321;&#21644;&#21487;&#33021;&#30740;&#31350;&#20154;&#21592;&#30340;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#26032;&#20852;&#25216;&#33021;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#26032;&#20852;&#25216;&#33021;&#65292;&#36825;&#20123;&#25216;&#33021;&#22312;&#26356;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#65292;&#20294;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#12290;&#26032;&#20852;&#25216;&#33021;&#35753;&#20154;&#24863;&#21040;&#22256;&#24785;&#30340;&#26159;&#20004;&#26041;&#38754;&#65306;&#23427;&#20204;&#30340;&#28165;&#26224;&#24230;&#65292;&#20284;&#20046;&#30636;&#38388;&#20174;&#19981;&#23384;&#22312;&#21040;&#23384;&#22312;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#20284;&#20046;&#22312;&#19981;&#21487;&#39044;&#35265;&#30340;&#27169;&#22411;&#35268;&#27169;&#19979;&#20986;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#20852;&#25216;&#33021;&#30340;&#21478;&#19968;&#31181;&#35299;&#37322;&#65292;&#21363;&#23545;&#20110;&#29305;&#23450;&#20219;&#21153;&#21644;&#27169;&#22411;&#26063;&#65292;&#24403;&#20998;&#26512;&#22266;&#23450;&#30340;&#27169;&#22411;&#36755;&#20986;&#26102;&#65292;&#21487;&#20197;&#36873;&#25321;&#23548;&#33268;&#25512;&#26029;&#20986;&#26032;&#20852;&#25216;&#33021;&#25110;&#19981;&#23548;&#33268;&#25512;&#26029;&#20986;&#26032;&#20852;&#25216;&#33021;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#37322;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26032;&#20852;&#25216;&#33021;&#22768;&#26126;&#26159;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;&#30340;&#20135;&#29289;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#20219;&#21153;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#22522;&#26412;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#23398;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#35299;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#19977;&#31181;&#20114;&#34917;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#25105;&#20204;(1)&#21046;&#20316;&#12289;&#27979;&#35797;&#24182;&#39564;&#35777;&#20102;&#20851;&#20110;&#25253;&#21578;&#30340;&#26032;&#20852;&#25216;&#33021;&#30340;&#24230;&#37327;&#36873;&#25321;&#30340;&#19977;&#20010;&#39044;&#27979;&#25928;&#24212;&#65307;(2)&#23637;&#31034;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#30340;&#31616;&#21333;&#21464;&#21270;&#20250;&#22312;&#19968;&#20010;&#24050;&#32463;&#30830;&#23450;&#30340;&#20219;&#21153;&#20013;&#20135;&#29983;&#22823;&#30340;&#26032;&#20852;&#33021;&#21147;&#24046;&#24322;&#65307;(3)&#23637;&#31034;&#25152;&#35859;&#30340;&#26032;&#20852;&#25216;&#33021;&#21487;&#20197;&#36890;&#36807;&#26377;&#24847;&#20248;&#21270;&#25152;&#36873;&#25321;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26032;&#20852;&#33021;&#21147;&#30340;&#22768;&#26126;&#24456;&#21487;&#33021;&#24182;&#19981;&#26159;&#30495;&#23454;&#23384;&#22312;&#30340;&#65292;&#32780;&#26159;&#24230;&#37327;&#26631;&#20934;&#20219;&#24847;&#36873;&#25321;&#21644;&#21487;&#33021;&#30340;&#30740;&#31350;&#20154;&#21592;&#20559;&#35265;&#30340;&#20135;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20379;&#24212;&#38142;&#21450;&#20854;&#23545;&#31639;&#27861;&#31995;&#32479;&#27835;&#29702;&#21644;&#36131;&#20219;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#24433;&#21709;&#65292;&#35748;&#20026;&#31639;&#27861;&#36131;&#20219;&#30340;&#35752;&#35770;&#24517;&#39035;&#32771;&#34385;&#21040;&#20379;&#24212;&#38142;&#12290;</title><link>http://arxiv.org/abs/2304.14749</link><description>&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#20379;&#24212;&#38142;&#20013;&#30340;&#36131;&#20219;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding accountability in algorithmic supply chains. (arXiv:2304.14749v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20379;&#24212;&#38142;&#21450;&#20854;&#23545;&#31639;&#27861;&#31995;&#32479;&#27835;&#29702;&#21644;&#36131;&#20219;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#24433;&#21709;&#65292;&#35748;&#20026;&#31639;&#27861;&#36131;&#20219;&#30340;&#35752;&#35770;&#24517;&#39035;&#32771;&#34385;&#21040;&#20379;&#24212;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#21644;&#25919;&#31574;&#19978;&#23545;&#31639;&#27861;&#36131;&#20219;&#30340;&#25552;&#35758;&#24120;&#24120;&#35797;&#22270;&#22312;&#31038;&#20250;&#25216;&#26415;&#32972;&#26223;&#19979;&#29702;&#35299;&#31639;&#27861;&#31995;&#32479;&#65292;&#35748;&#35782;&#21040;&#23427;&#20204;&#30001;&#8220;&#22810;&#26041;&#8221;&#20849;&#21516;&#21046;&#36896;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#31639;&#27861;&#31995;&#32479;&#20063;&#26159;&#30001;&#22810;&#20010;&#21442;&#19982;&#32773;&#32452;&#25104;&#30340;&#20379;&#24212;&#38142;&#29983;&#20135;&#12289;&#37096;&#32626;&#21644;&#20351;&#29992;&#30340;&#65292;&#23427;&#20204;&#20043;&#38388;&#36890;&#36807;&#25968;&#25454;&#27969;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#20379;&#24212;&#38142;&#20013;&#19981;&#21516;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#25512;&#21160;&#31995;&#32479;&#24182;&#20135;&#29983;&#29305;&#23450;&#32467;&#26524;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#31639;&#27861;&#36131;&#20219;&#30340;&#35752;&#35770;&#24517;&#39035;&#32771;&#34385;&#21040;&#20379;&#24212;&#38142;&#20197;&#21450;&#23427;&#20204;&#23545;&#31639;&#27861;&#31995;&#32479;&#27835;&#29702;&#21644;&#36131;&#20219;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#31639;&#27861;&#20379;&#24212;&#38142;&#65292;&#23558;&#20854;&#23450;&#20301;&#20110;&#24191;&#27867;&#30340;&#25216;&#26415;&#21644;&#25919;&#27835;&#32463;&#27982;&#32972;&#26223;&#20013;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#20851;&#38190;&#29305;&#24449;&#65292;&#24212;&#22312;&#26410;&#26469;&#30340;&#31639;&#27861;&#27835;&#29702;&#21644;&#36131;&#20219;&#30740;&#31350;&#20013;&#21152;&#20197;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Academic and policy proposals on algorithmic accountability often seek to understand algorithmic systems in their socio-technical context, recognising that they are produced by 'many hands'. Increasingly, however, algorithmic systems are also produced, deployed, and used within a supply chain comprising multiple actors tied together by flows of data between them. In such cases, it is the working together of an algorithmic supply chain of different actors who contribute to the production, deployment, use, and functionality that drives systems and produces particular outcomes. We argue that algorithmic accountability discussions must consider supply chains and the difficult implications they raise for the governance and accountability of algorithmic systems. In doing so, we explore algorithmic supply chains, locating them in their broader technical and political economic context and identifying some key features that should be understood in future work on algorithmic governance and accou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;DCR&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;&#65292;&#24182;&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;</title><link>http://arxiv.org/abs/2304.14068</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Neural-Symbolic Concept Reasoning. (arXiv:2304.14068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;DCR&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;&#65292;&#24182;&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#38459;&#27490;&#20102;&#23427;&#20204;&#33719;&#24471;&#23436;&#20840;&#30340;&#20154;&#31867;&#20449;&#20219;&#12290;&#27010;&#24565;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#27010;&#24565;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32500;&#27010;&#24565;&#23884;&#20837;&#34920;&#31034;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#22240;&#27492;&#36136;&#30097;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Concept Reasoner(DCR)&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#12290;&#22312;DCR&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#19981;&#30452;&#25509;&#36827;&#34892;&#20219;&#21153;&#39044;&#27979;&#65292;&#32780;&#26159;&#20351;&#29992;&#27010;&#24565;&#23884;&#20837;&#24314;&#31435;&#35821;&#27861;&#35268;&#21017;&#32467;&#26500;&#12290;&#28982;&#21518;DCR&#22312;&#26377;&#24847;&#20041;&#30340;&#27010;&#24565;&#30495;&#20540;&#24230;&#19978;&#25191;&#34892;&#36825;&#20123;&#35268;&#21017;&#65292;&#20197;&#19981;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#25552;&#20379;&#26368;&#32456;&#30340;&#21487;&#35299;&#37322;&#21644;&#35821;&#20041;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DCR&#65306;(i)&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;;(ii)&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;;(iii)&#24456;&#23481;&#26131;&#36866;&#24212;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#38544;&#24335;Q&#23398;&#20064;(IQL)&#20316;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#26435;&#37325;&#26469;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#23792;&#29305;&#24449;&#30340;Actor&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10573</link><description>&lt;p&gt;
IDQL: &#20316;&#20026;&#19968;&#31181;&#25193;&#25955;&#31574;&#30053;&#30340;Actor-Critic&#26041;&#27861;&#30340;&#38544;&#24335;Q&#23398;&#20064;&#12290; (arXiv:2304.10573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies. (arXiv:2304.10573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#38544;&#24335;Q&#23398;&#20064;(IQL)&#20316;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#26435;&#37325;&#26469;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#23792;&#29305;&#24449;&#30340;Actor&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#38656;&#35201;&#27491;&#30830;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#30340;&#34892;&#20026;&#12290;&#38544;&#24335;Q&#23398;&#20064;&#65288;IQL&#65289;&#36890;&#36807;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#34892;&#21160;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;Bellman Backup&#26469;&#35757;&#32451;Q&#20989;&#25968;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#19981;&#28165;&#26970;&#21738;&#20010;&#31574;&#30053;&#23454;&#38469;&#19978;&#23454;&#29616;&#20102;&#27492;&#38544;&#21547;&#35757;&#32451;&#30340;Q&#20989;&#25968;&#25152;&#20195;&#34920;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;IQL&#37325;&#26032;&#35299;&#37322;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#36890;&#36807;&#24191;&#20041;&#21270;&#35780;&#21028;&#30446;&#26631;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#34892;&#20026;&#35268;&#33539;&#21270;&#30340;&#38544;&#24335;Actor&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#27867;&#21270;&#26174;&#31034;&#20102;&#24341;&#20837;&#30340;Actor&#22914;&#20309;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#65292;&#20855;&#20307;&#30340;&#25439;&#22833;&#36873;&#25321;&#20915;&#23450;&#20102;&#36825;&#31181;&#26435;&#34913;&#30340;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;Actor&#21487;&#20197;&#34920;&#29616;&#20986;&#22797;&#26434;&#21644;&#22810;&#23792;&#30340;&#29305;&#24449;&#65292;&#36825;&#34920;&#26126;&#20102;&#21033;&#29992;&#20248;&#21183;&#21152;&#26435;&#22238;&#24402;&#65288;AWR&#65289;&#20013;&#20351;&#29992;&#30340;&#26465;&#20214;&#39640;&#26031;Actor&#30340;&#25311;&#21512;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#21442;&#25968;&#21270;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#30340;&#26679;&#26412;&#21644;&#30001;&#35780;&#21028;&#22120;&#35745;&#31639;&#30340;&#26435;&#37325;&#65292;&#28982;&#21518;&#23558;&#20854;&#23548;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective offline RL methods require properly handling out-of-distribution actions. Implicit Q-learning (IQL) addresses this by training a Q-function using only dataset actions through a modified Bellman backup. However, it is unclear which policy actually attains the values represented by this implicitly trained Q-function. In this paper, we reinterpret IQL as an actor-critic method by generalizing the critic objective and connecting it to a behavior-regularized implicit actor. This generalization shows how the induced actor balances reward maximization and divergence from the behavior policy, with the specific loss choice determining the nature of this tradeoff. Notably, this actor can exhibit complex and multimodal characteristics, suggesting issues with the conditional Gaussian actor fit with advantage weighted regression (AWR) used in prior methods. Instead, we propose using samples from a diffusion parameterized behavior policy and weights computed from the critic to then importa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03843</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#36880;&#27493;&#24605;&#32771;&#65311;&#25512;&#29702;&#28304;&#20110;&#32463;&#39564;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#30528;&#24378;&#22823;&#32780;&#31070;&#31192;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#32431;&#31929;&#30340;&#24605;&#32500;&#27493;&#39588;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#29702;&#20986;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#24471;&#20986;&#30340;&#25512;&#35770; - &#23613;&#31649;&#25105;&#20204;&#20174;&#19990;&#30028;&#19978;&#27809;&#26377;&#24471;&#21040;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#12290;&#21516;&#26679;&#22320;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36825;&#20123;&#35757;&#32451;&#26465;&#20214;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23450;&#20041;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#21697;&#23545;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27599;&#20010;&#26679;&#21697;&#21482;&#21253;&#25324;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21464;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#25512;&#29702;&#29983;&#25104;&#30340;&#21464;&#37327;&#23376;&#38598;&#19982;&#20351;&#29992;&#23436;&#25972;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2304.00186</link><description>&lt;p&gt;
&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Subject-driven Text-to-Image Generation via Apprenticeship Learning. (arXiv:2304.00186v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DreamBooth&#65289;&#22312;&#36890;&#36807;&#38024;&#23545;&#30446;&#26631;&#20027;&#39064;&#24494;&#35843;&#8220;&#19987;&#23478;&#27169;&#22411;&#8221;&#65292;&#29983;&#25104;&#39640;&#24230;&#33258;&#23450;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#24456;&#26114;&#36149;&#65292;&#22240;&#20026;&#27599;&#20010;&#20027;&#39064;&#37117;&#24517;&#39035;&#23398;&#20064;&#19968;&#20010;&#26032;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#20027;&#39064;&#29305;&#23450;&#24494;&#35843;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#12290;&#32473;&#23450;&#19968;&#20010;&#26032;&#20027;&#39064;&#30340;&#23569;&#37327;&#28436;&#31034;&#65292;SuTI&#21487;&#20197;&#21363;&#26102;&#29983;&#25104;&#19981;&#21516;&#22330;&#26223;&#20013;&#20027;&#39064;&#30340;&#26032;&#29256;&#26412;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#20027;&#39064;&#29305;&#23450;&#30340;&#20248;&#21270;&#12290;SuTI&#30001;&#8220;&#24466;&#24351;&#23398;&#20064;&#8221;&#39537;&#21160;&#65292;&#20854;&#20013;&#20174;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21333;&#20010;&#30340;&#24466;&#24351;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20114;&#32852;&#32593;&#25366;&#25496;&#20102;&#25968;&#30334;&#19975;&#20010;&#22270;&#20687;&#31751;&#65292;&#27599;&#20010;&#22270;&#20687;&#31751;&#37117;&#32858;&#28966;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#35270;&#35273;&#20027;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#36825;&#20123;&#31751;&#26469;&#35757;&#32451;&#22823;&#37327;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#35270;&#35273;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#24466;&#24351;&#27169;&#22411;&#36890;&#36807;&#25512;&#26029;&#22522;&#20110;&#20854;&#25991;&#26412;&#25551;&#36848;&#30340;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#24182;&#29983;&#25104;&#22270;&#20687;&#26469;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SuTI&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#19981;&#21516;&#20027;&#39064;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#27604;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with \emph{in-context} learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by {\em apprenticeship learning}, where a single apprentice model is learned from data generated by massive amount of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train massive amount of expert models specialized on diff
&lt;/p&gt;</description></item><item><title>DAMO-StreamNet&#26159;&#19968;&#20010;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#20013;&#27969;&#24335;&#24863;&#30693;&#30340;&#26694;&#26550;&#65292;&#23427;&#34701;&#21512;&#20102;YOLO&#31995;&#21015;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#36890;&#36807;&#39048;&#37096;&#32467;&#26500;&#12289;&#21452;&#20998;&#25903;&#32467;&#26500;&#12289;&#33976;&#39311;&#26426;&#21046;&#21644;&#23454;&#26102;&#39044;&#27979;&#26426;&#21046;&#31561;&#20851;&#38190;&#21019;&#26032;&#28857;&#65292;&#25552;&#20379;&#20102;&#23574;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.17144</link><description>&lt;p&gt;
DAMO-StreamNet&#65306;&#33258;&#21160;&#39550;&#39542;&#20013;&#27969;&#24335;&#24863;&#30693;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving. (arXiv:2303.17144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17144
&lt;/p&gt;
&lt;p&gt;
DAMO-StreamNet&#26159;&#19968;&#20010;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#20013;&#27969;&#24335;&#24863;&#30693;&#30340;&#26694;&#26550;&#65292;&#23427;&#34701;&#21512;&#20102;YOLO&#31995;&#21015;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#36890;&#36807;&#39048;&#37096;&#32467;&#26500;&#12289;&#21452;&#20998;&#25903;&#32467;&#26500;&#12289;&#33976;&#39311;&#26426;&#21046;&#21644;&#23454;&#26102;&#39044;&#27979;&#26426;&#21046;&#31561;&#20851;&#38190;&#21019;&#26032;&#28857;&#65292;&#25552;&#20379;&#20102;&#23574;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#24863;&#30693;&#65292;&#25110;&#32773;&#35828;&#27969;&#24335;&#24863;&#30693;&#65292;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#30340;&#26041;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAMO-StreamNet&#65292;&#23427;&#23558;YOLO&#31995;&#21015;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#31354;&#38388;&#21644;&#26102;&#38388;&#24863;&#30693;&#26426;&#21046;&#30340;&#20840;&#38754;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23574;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;DAMO-StreamNet&#30340;&#20851;&#38190;&#21019;&#26032;&#28857;&#21253;&#25324;&#65306;(1)&#19968;&#20010;&#40065;&#26834;&#30340;&#39048;&#37096;&#32467;&#26500;&#65292;&#34701;&#21512;&#20102;&#21487;&#21464;&#24418;&#21367;&#31215;&#65292;&#22686;&#24378;&#20102;&#24863;&#21463;&#37326;&#21644;&#29305;&#24449;&#23545;&#40784;&#33021;&#21147;&#12290;(2)&#19968;&#20010;&#21452;&#20998;&#25903;&#32467;&#26500;&#65292;&#25972;&#21512;&#20102;&#30701;&#36890;&#36947;&#35821;&#20041;&#29305;&#24449;&#21644;&#38271;&#36890;&#36947;&#26102;&#24207;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#29366;&#24577;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;(3)&#19968;&#20010;&#22312;logits&#32423;&#21035;&#19978;&#36827;&#34892;&#30340;&#33976;&#39311;&#26426;&#21046;&#65292;&#23545;&#40784;&#25945;&#24072;&#32593;&#32476;&#21644;&#23398;&#29983;&#32593;&#32476;&#30340;&#35821;&#20041;&#31354;&#38388;&#12290;(4)&#19968;&#20010;&#23454;&#26102;&#39044;&#27979;&#26426;&#21046;&#65292;&#26356;&#26032;&#25903;&#25345;&#24103;&#30340;&#29305;&#24449;&#19982;&#24403;&#21069;&#24103;&#65292;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#26080;&#32541;&#27969;&#24335;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time perception, or streaming perception, is a crucial aspect of autonomous driving that has yet to be thoroughly explored in existing research. To address this gap, we present DAMO-StreamNet, an optimized framework that combines recent advances from the YOLO series with a comprehensive analysis of spatial and temporal perception mechanisms, delivering a cutting-edge solution. The key innovations of DAMO-StreamNet are: (1) A robust neck structure incorporating deformable convolution, enhancing the receptive field and feature alignment capabilities. (2) A dual-branch structure that integrates short-path semantic features and long-path temporal features, improving motion state prediction accuracy. (3) Logits-level distillation for efficient optimization, aligning the logits of teacher and student networks in semantic space. (4) A real-time forecasting mechanism that updates support frame features with the current frame, ensuring seamless streaming perception during inference. Our ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#20013;&#12290;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20132;&#36890;&#35268;&#21017;&#30340;&#36981;&#23432;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11888</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36328;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving. (arXiv:2303.11888v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#20013;&#12290;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20132;&#36890;&#35268;&#21017;&#30340;&#36981;&#23432;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#24335;&#35782;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#30340;&#20934;&#30830;&#24230;&#24050;&#32463;&#36229;&#36807;&#20154;&#31867;&#12290;&#33258;&#21160;&#39550;&#39542;&#20316;&#20026;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#24443;&#24213;&#25913;&#21464;&#26410;&#26469;&#30340;&#20132;&#36890;&#21644;&#20986;&#34892;&#26041;&#24335;&#12290;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#34892;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#30001;&#20110;&#20854;&#22810;&#32500;&#24863;&#30693;&#21644;&#38598;&#25104;&#33021;&#21147;&#30340;&#28508;&#21147;&#32780;&#25104;&#20026;&#24403;&#21069;&#30740;&#31350;&#30340;&#28909;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#21644;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20027;&#35201;&#20851;&#27880;&#20110;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#30340;&#33021;&#21147;&#24182;&#32479;&#19968;&#27169;&#20223;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of Pattern Recognition and Computer Vision technologies, tasks like object detection or semantic segmentation have achieved even better accuracy than human beings. Based on these solid foundations, autonomous driving is becoming an important research direction, aiming to revolute the future of transportation and mobility. Sensors are critical to autonomous driving's security and feasibility to perceive the surrounding environment. Multi-Sensor fusion has become a current research hot spot because of its potential for multidimensional perception and integration ability. In this paper, we propose a novel feature-level multi-sensor fusion technology for end-to-end autonomous driving navigation with imitation learning. Our paper mainly focuses on fusion technologies for Lidar and RGB information. We also provide a brand-new penalty-based imitation learning method to reinforce the model's compliance with traffic rules and unify the objective of imitation learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#20154;&#31867;&#36830;&#25509;&#32452;&#35745;&#21010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20256;&#32479;&#24494;&#32467;&#26500;&#21644;&#36830;&#25509;&#29305;&#24449;&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.09124</link><description>&lt;p&gt;
&#32420;&#32500;&#26463;&#24418;&#29366;&#27979;&#37327;&#20449;&#24687;&#21487;&#29992;&#20110;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fiber Tract Shape Measures Inform Prediction of Non-Imaging Phenotypes. (arXiv:2303.09124v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#20154;&#31867;&#36830;&#25509;&#32452;&#35745;&#21010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20256;&#32479;&#24494;&#32467;&#26500;&#21644;&#36830;&#25509;&#29305;&#24449;&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30333;&#36136;&#36830;&#25509;&#30340;&#31070;&#32463;&#24433;&#20687;&#23398;&#27979;&#37327;&#21487;&#20197;&#39044;&#27979;&#35832;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#35748;&#30693;&#27979;&#37327;&#31561;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#22312;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#38271;&#24230;&#12289;&#30452;&#24452;&#21644;&#20280;&#38271;&#27604;&#31561;&#19977;&#20010;&#22522;&#26412;&#24418;&#29366;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#20256;&#32479;&#22238;&#24402;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#24494;&#32467;&#26500;&#12289;&#36830;&#25509;&#21644;&#24418;&#29366;&#27979;&#37327;&#30340;&#39640;&#25928;&#20004;&#38454;&#27573;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32420;&#32500;&#26463;&#24418;&#29366;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#20026;&#20256;&#32479;&#24494;&#32467;&#26500;&#21644;&#36830;&#25509;&#29305;&#24449;&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuroimaging measures of the brain's white matter connections can enable the prediction of non-imaging phenotypes, such as demographic and cognitive measures. Existing works have investigated traditional microstructure and connectivity measures from diffusion MRI tractography, without considering the shape of the connections reconstructed by tractography. In this paper, we investigate the potential of fiber tract shape features for predicting non-imaging phenotypes, both individually and in combination with traditional features. We focus on three basic shape features: length, diameter, and elongation. Two different prediction methods are used, including a traditional regression method and a deep-learning-based prediction method. Experiments use an efficient two-stage fusion strategy for prediction using microstructure, connectivity, and shape measures. To reduce predictive bias due to brain size, normalized shape features are also investigated. Experimental results on the Human Connect
&lt;/p&gt;</description></item><item><title>&#24037;&#20316;&#22330;&#25152;&#24863;&#30693;&#25216;&#26415;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#20135;&#21147;&#21644;&#31119;&#31049;&#65292;&#20294;&#22914;&#20309;&#33719;&#21462;&#24037;&#20154;&#30340;&#26377;&#25928;&#21516;&#24847;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#38590;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#24037;&#20154;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.07242</link><description>&lt;p&gt;
&#24037;&#20154;&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#21516;&#24847;&#24037;&#20316;&#22330;&#25152;&#20581;&#24247;&#31185;&#25216;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Workers Meaningfully Consent to Workplace Wellbeing Technologies?. (arXiv:2303.07242v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07242
&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#22330;&#25152;&#24863;&#30693;&#25216;&#26415;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#20135;&#21147;&#21644;&#31119;&#31049;&#65292;&#20294;&#22914;&#20309;&#33719;&#21462;&#24037;&#20154;&#30340;&#26377;&#25928;&#21516;&#24847;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#38590;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#24037;&#20154;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#22330;&#25152;&#37096;&#32626;&#30340;&#24863;&#30693;&#25216;&#26415;&#21487;&#20197;&#25910;&#38598;&#20010;&#20154;&#27963;&#21160;&#21644;&#32676;&#20307;&#20114;&#21160;&#30340;&#35814;&#32454;&#25968;&#25454;&#65292;&#21542;&#21017;&#24456;&#38590;&#25429;&#25417;&#12290;&#36825;&#20123;&#25216;&#26415;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#26159;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#20225;&#19994;&#21644;&#24037;&#20154;&#20248;&#21270;&#29983;&#20135;&#21147;&#21644;&#31119;&#31049;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#20869;&#22312;&#21644;&#32467;&#26500;&#24615;&#26435;&#21147;&#21160;&#24577;&#65292;&#25509;&#21463;&#21547;&#33988;&#30340;&#36981;&#20174;&#20197;&#30417;&#27979;&#24037;&#20316;&#27963;&#21160;&#32780;&#19981;&#26159;&#23547;&#27714;&#24037;&#20154;&#30340;&#26377;&#24847;&#20041;&#30340;&#21516;&#24847;&#30340;&#26222;&#36941;&#20570;&#27861;&#24341;&#21457;&#20102;&#38544;&#31169;&#21644;&#20262;&#29702;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#24037;&#20154;&#22312;&#21516;&#24847;&#24037;&#20316;&#22330;&#25152;&#20581;&#24247;&#31185;&#25216;&#26041;&#38754;&#38754;&#20020;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;&#21033;&#29992;&#19968;&#20010;&#34394;&#25311;&#26696;&#20363;&#65292;&#24341;&#23548;15&#20301;&#21442;&#19982;&#32773;&#21442;&#19982;&#20102;6&#20010;&#22810;&#21033;&#30410;&#30456;&#20851;&#26041;&#28966;&#28857;&#23567;&#32452;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21442;&#19982;&#32773;&#21516;&#24847;&#24037;&#20316;&#22330;&#25152;&#24863;&#30693;&#25216;&#26415;&#30340;&#26399;&#26395;&#21644;&#33021;&#21147;&#12290;&#25105;&#20204;&#25551;&#32472;&#20102;&#21487;&#33021;&#26356;&#22909;&#22320;&#25903;&#25345;&#26356;&#26377;&#24847;&#20041;&#30340;&#21516;&#24847;&#24037;&#20316;&#22330;&#25152;&#20581;&#24247;&#31185;&#25216;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#36825;&#20123;&#25514;&#26045; drawing on critical
&lt;/p&gt;
&lt;p&gt;
Sensing technologies deployed in the workplace can collect detailed data about individual activities and group interactions that are otherwise difficult to capture. A hopeful application of these technologies is that they can help businesses and workers optimize productivity and wellbeing. However, given the inherent and structural power dynamics in the workplace, the prevalent approach of accepting tacit compliance to monitor work activities rather than seeking workers' meaningful consent raises privacy and ethical concerns. This paper unpacks a range of challenges that workers face when consenting to workplace wellbeing technologies. Using a hypothetical case to prompt reflection among six multi-stakeholder focus groups involving 15 participants, we explored participants' expectations and capacity to consent to workplace sensing technologies. We sketched possible interventions that could better support more meaningful consent to workplace wellbeing technologies by drawing on critical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#31283;&#20581;&#20998;&#38454;&#27573;&#20215;&#20540;&#23398;&#20064;&#65288;RPVL&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#34920;&#26684;&#21095;&#24773;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02783</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning. (arXiv:2303.02783v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#31283;&#20581;&#20998;&#38454;&#27573;&#20215;&#20540;&#23398;&#20064;&#65288;RPVL&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#34920;&#26684;&#21095;&#24773;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#35757;&#32451;&#29615;&#22659;&#19982;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#21442;&#25968;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#21046;&#23450;&#20026;&#19968;&#20010;&#20998;&#24067;&#24335;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;(DR-RL)&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#38598;&#20013;&#38024;&#23545;&#29615;&#22659;&#26368;&#22351;&#30340;&#38543;&#26426;&#27169;&#22411;&#19979;&#26368;&#22823;&#21270;&#20215;&#20540;&#20989;&#25968;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#34920;&#26684;&#21095;&#24773;&#23398;&#20064;&#29615;&#22659;&#65292;&#22312;&#19981;&#30830;&#23450;&#38598;&#34987;&#23450;&#20041;&#22312;&#21517;&#20041;&#65288;&#35757;&#32451;&#65289;&#29615;&#22659;&#30340;&#29983;&#25104;&#27169;&#22411;&#21608;&#22260;&#30340;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#21487;&#20197;&#35775;&#38382;&#35813;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#20581;&#20998;&#38454;&#27573;&#20215;&#20540;&#23398;&#20064;(RPVL)&#31639;&#27861;&#26469;&#35299;&#20915;&#29992;&#22235;&#31181;&#19981;&#21516;&#21457;&#25955;&#24230;&#25351;&#23450;&#30340;&#19981;&#30830;&#23450;&#38598;&#30340;&#38382;&#39064;: &#20840;&#21464;&#20998;&#12289;&#21345;&#26041;&#12289;Kullback-Leibler&#21644;Wasserstein&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102; $\tilde{\mathcal{O}}(|\mathcal{S}||\mathcal{A}| H^{5})$ &#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#36825;&#27604;&#29616;&#26377;&#32467;&#26524;&#24179;&#22343;&#22909;&#20102;&#19968;&#20493;&#30340; $|\mathcal{S}|$
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning a control policy that is robust against the parameter mismatches between the training environment and testing environment. We formulate this as a distributionally robust reinforcement learning (DR-RL) problem where the objective is to learn the policy which maximizes the value function against the worst possible stochastic model of the environment in an uncertainty set. We focus on the tabular episodic learning setting where the algorithm has access to a generative model of the nominal (training) environment around which the uncertainty set is defined. We propose the Robust Phased Value Learning (RPVL) algorithm to solve this problem for the uncertainty sets specified by four different divergences: total variation, chi-square, Kullback-Leibler, and Wasserstein. We show that our algorithm achieves $\tilde{\mathcal{O}}(|\mathcal{S}||\mathcal{A}| H^{5})$ sample complexity, which is uniformly better than the existing results by a factor of $|\mathcal{S}|
&lt;/p&gt;</description></item><item><title>EvoTorch&#26159;&#19968;&#20010;Python&#24211;&#65292;&#20026;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#21487;&#25193;&#23637;&#12289;&#21487;&#37325;&#29992;&#21644;&#23454;&#29992;&#30340;&#36827;&#21270;&#31639;&#27861;&#23454;&#29616;&#65292;&#25903;&#25345;GPU&#21644;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12600</link><description>&lt;p&gt;
EvoTorch&#65306;Python&#20013;&#21487;&#25193;&#23637;&#30340;&#36827;&#21270;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
EvoTorch: Scalable Evolutionary Computation in Python. (arXiv:2302.12600v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12600
&lt;/p&gt;
&lt;p&gt;
EvoTorch&#26159;&#19968;&#20010;Python&#24211;&#65292;&#20026;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#21487;&#25193;&#23637;&#12289;&#21487;&#37325;&#29992;&#21644;&#23454;&#29992;&#30340;&#36827;&#21270;&#31639;&#27861;&#23454;&#29616;&#65292;&#25903;&#25345;GPU&#21644;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#24037;&#19994;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#12289;&#24037;&#31243;&#35774;&#35745;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#32771;&#34385;&#21040;&#29616;&#20195;&#20248;&#21270;&#38382;&#39064;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#23545;&#21487;&#25193;&#23637;&#12289;&#21487;&#37325;&#29992;&#21644;&#23454;&#29992;&#30340;&#36827;&#21270;&#31639;&#27861;&#23454;&#29616;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; EvoTorch&#65306;&#19968;&#20010;&#26088;&#22312;&#22788;&#29702;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#12289;&#25903;&#25345; GPU &#21644;&#39640;&#24182;&#34892;&#24615;&#33021;&#30340;&#36827;&#21270;&#35745;&#31639;&#24211;&#12290;EvoTorch &#22522;&#20110; PyTorch &#24211;&#65292;&#24182;&#33021;&#22815;&#26080;&#32541;&#22320;&#19982;&#20043;&#37197;&#21512;&#65292;&#22240;&#27492;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340; API &#23450;&#20041;&#33258;&#24049;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary computation is an important component within various fields such as artificial intelligence research, reinforcement learning, robotics, industrial automation and/or optimization, engineering design, etc. Considering the increasing computational demands and the dimensionalities of modern optimization problems, the requirement for scalable, re-usable, and practical evolutionary algorithm implementations has been growing. To address this requirement, we present EvoTorch: an evolutionary computation library designed to work with high-dimensional optimization problems, with GPU support and with high parallelization capabilities. EvoTorch is based on and seamlessly works with the PyTorch library, and therefore, allows the users to define their optimization problems using a well-known API.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#21508;&#31181;&#26426;&#21046;&#65292;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22312;12&#20010;&#39046;&#22495;&#20869;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#31639;&#27861;&#21482;&#33021;&#24212;&#23545;&#26576;&#20123;&#36716;&#21464;&#65292;&#36827;&#19968;&#27493;&#22320;&#65292;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#36873;&#25321;&#26631;&#20934;&#26469;&#25913;&#21892;&#29616;&#26377;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12254</link><description>&lt;p&gt;
&#25913;&#21464;&#24456;&#38590;&#65306;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#28145;&#20837;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Change is Hard: A Closer Look at Subpopulation Shift. (arXiv:2302.12254v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#21508;&#31181;&#26426;&#21046;&#65292;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22312;12&#20010;&#39046;&#22495;&#20869;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#31639;&#27861;&#21482;&#33021;&#24212;&#23545;&#26576;&#20123;&#36716;&#21464;&#65292;&#36827;&#19968;&#27493;&#22320;&#65292;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#36873;&#25321;&#26631;&#20934;&#26469;&#25913;&#21892;&#29616;&#26377;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#23376;&#32676;&#20307;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23548;&#33268;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#26426;&#21046;&#20197;&#21450;&#31639;&#27861;&#22312;&#22914;&#27492;&#19981;&#21516;&#30340;&#36716;&#21464;&#20013;&#22914;&#20309;&#36827;&#34892;&#26222;&#36941;&#21270;&#65292;&#25105;&#20204;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#23376;&#32676;&#20307;&#36716;&#21464;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#21078;&#26512;&#21644;&#35299;&#37322;&#23376;&#32676;&#20307;&#20013;&#30340;&#24120;&#35265;&#36716;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#21307;&#30103;&#39046;&#22495;&#30340;12&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35757;&#32451;10,000&#22810;&#20010;&#27169;&#22411;&#24471;&#21040;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#30340;&#26377;&#36259;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#31639;&#27861;&#20165;&#33021;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#36716;&#21464;&#19978;&#25552;&#39640;&#23376;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#20854;&#20182;&#31867;&#22411;&#30340;&#36716;&#21464;&#19978;&#21017;&#19981;&#33021;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#24403;&#21069;&#31639;&#27861;&#20381;&#36182;&#20110;&#32676;&#20307;&#26631;&#27880;&#30340;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#26368;&#24046;&#31867;&#21035;&#20934;&#30830;&#24230;&#30340;&#31616;&#21333;&#36873;&#25321;&#26631;&#20934;&#20854;&#23454;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning&#65288;MPT&#65289;&#22914;&#20309;&#24110;&#21161;&#25913;&#21892;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;&#20803;&#23398;&#20064;&#21487;&#20197;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.08143</link><description>&lt;p&gt;
&#23398;&#20064;&#21021;&#22987;&#21270;&#65306;&#20803;&#23398;&#20064;&#33021;&#21542;&#25552;&#39640;Prompt Tuning&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?. (arXiv:2302.08143v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning&#65288;MPT&#65289;&#22914;&#20309;&#24110;&#21161;&#25913;&#21892;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;&#20803;&#23398;&#20064;&#21487;&#20197;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning (PT)&#26159;&#19968;&#31181;&#21482;&#35843;&#25972;&#27599;&#20010;&#20219;&#21153;&#30340;&#19968;&#20010;&#39069;&#22806;&#26631;&#35760;&#24207;&#21015;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#23436;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#19981;&#21464;&#65292;&#24050;&#32463;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;PT&#24050;&#32463;&#34987;&#35777;&#26126;&#26497;&#22823;&#22320;&#20381;&#36182;&#20110;&#24456;&#22909;&#30340;Prompt&#23884;&#20837;&#30340;&#21021;&#22987;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning (MPT) &#26469;&#31995;&#32479;&#22320;&#25506;&#32034;&#20803;&#23398;&#20064;&#22914;&#20309;&#24110;&#21161;&#36890;&#36807;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#26469;&#25913;&#21892;&#65288;&#22914;&#26524;&#21487;&#20197;&#65289;PT&#20013;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#20351;&#29992;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#26469;&#32463;&#39564;&#20998;&#26512;&#20102;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#20998;&#26512;&#19981;&#21516;&#28304;/&#30446;&#26631;&#20219;&#21153;&#37197;&#32622;&#19979;&#30340;&#21508;&#31181;&#35843;&#25972;&#35774;&#32622;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#29305;&#21035;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#25552;&#21319;&#26159;&#26174;&#33879;&#30340;&#12290;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#38382;&#39064;&#22238;&#31572;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;MPT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;PT&#65292;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most case
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#20215;&#26684;&#25968;&#25454;&#24066;&#22330;&#20013;&#30340;&#20080;&#23478;&#20043;&#38388;&#30340;&#31454;&#20105;&#31574;&#30053;&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25991;&#31456;&#25581;&#31034;&#20102;&#20080;&#23478;&#20043;&#38388;&#30340;&#36127;&#22806;&#37096;&#24615;&#21450;&#20854;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#24066;&#22330;&#24178;&#39044;&#25514;&#26045;&#65292;&#23454;&#29616;&#20102;&#32431;&#31574;&#30053;&#22343;&#34913;&#65292;&#24182;&#20445;&#35777;&#20102;&#24378;&#31119;&#21033;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2302.08012</link><description>&lt;p&gt;
&#24102;&#26377;&#22806;&#37096;&#24615;&#30340;&#22266;&#23450;&#20215;&#26684;&#25968;&#25454;&#24066;&#22330;&#30340;&#22343;&#34913;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equilibrium and Learning in Fixed-Price Data Markets with Externality. (arXiv:2302.08012v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#20215;&#26684;&#25968;&#25454;&#24066;&#22330;&#20013;&#30340;&#20080;&#23478;&#20043;&#38388;&#30340;&#31454;&#20105;&#31574;&#30053;&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25991;&#31456;&#25581;&#31034;&#20102;&#20080;&#23478;&#20043;&#38388;&#30340;&#36127;&#22806;&#37096;&#24615;&#21450;&#20854;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#24066;&#22330;&#24178;&#39044;&#25514;&#26045;&#65292;&#23454;&#29616;&#20102;&#32431;&#31574;&#30053;&#22343;&#34913;&#65292;&#24182;&#20445;&#35777;&#20102;&#24378;&#31119;&#21033;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#23558;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#24066;&#22330;&#24314;&#27169;&#20026;&#19968;&#20010;&#20080;&#23478;&#20043;&#38388;&#30340;&#21516;&#26102;&#31227;&#21160;&#21338;&#24328;&#65292;&#20854;&#20013;&#21334;&#23478;&#21457;&#24067;&#22266;&#23450;&#20215;&#26684;&#65292;&#32780;&#20080;&#23478;&#21487;&#20197;&#33258;&#30001;&#22320;&#20174;&#20219;&#20309;&#19968;&#32452;&#21334;&#23478;&#36141;&#20080;&#12290;&#35813;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20080;&#23478;&#36890;&#36807;&#36141;&#20080;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#30340;&#25968;&#25454;&#23545;&#24444;&#27492;&#20135;&#29983;&#30340;&#36127;&#22806;&#37096;&#24615;&#65292;&#36825;&#31181;&#29616;&#35937;&#22240;&#25968;&#25454;&#26131;&#20110;&#22797;&#21046;&#32780;&#21152;&#21095;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#20917;&#12290;&#22312;&#26356;&#31616;&#21333;&#30340;&#23436;&#20840;&#20449;&#24687;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23384;&#22312;&#20110;&#23384;&#22312;&#20080;&#23478;&#22806;&#37096;&#24615;&#30340;&#32431;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#31119;&#21033;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#31867;&#26631;&#20934;&#30340;&#22806;&#37096;&#24615;&#20989;&#25968;&#65292;&#20197;&#20132;&#26131;&#25104;&#26412;&#30340;&#24418;&#24335;&#30340;&#24066;&#22330;&#24178;&#39044;&#21487;&#20197;&#23548;&#33268;&#24378;&#31119;&#21033;&#20445;&#35777;&#30340;&#32431;&#31574;&#30053;&#22343;&#34913;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#32771;&#34385;&#20080;&#23478;&#36215;&#22987;&#20272;&#20215;&#26410;&#30693;&#30340;&#26356;&#19968;&#33324;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose modeling real-world data markets, where sellers post fixed prices and buyers are free to purchase from any set of sellers, as a simultaneous-move game between the buyers. A key component of this model is the negative externality buyers induce on one another due to purchasing data with a competitive advantage, a phenomenon exacerbated by data's easy replicability. We consider two settings. In the simpler complete-information setting, where all buyers know their valuations, we characterize both the existence and welfare properties of the pure-strategy Nash equilibrium in the presence of buyer externality. While this picture is bleak without any market intervention, reinforcing the limitations of current data markets, we prove that for a standard class of externality functions, market intervention in the form of a transaction cost can lead to a pure-strategy equilibrium with strong welfare guarantees. We next consider a more general setting where buyers start with unknown valua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#20351;&#29992;6DoF IMU&#27979;&#37327;&#20540;&#30340;&#23454;&#26102;&#24815;&#24615;&#23039;&#24577;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#23545;&#20110;&#19981;&#21516;&#30340;&#36816;&#21160;&#27169;&#24335;&#12289;&#37319;&#26679;&#29575;&#21644;&#29615;&#22659;&#24178;&#25200;&#20855;&#26377;&#24456;&#22909;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19971;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06037</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;6DoF&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#30340;&#23454;&#26102;&#23039;&#24577;&#20272;&#35745;&#30340;&#36890;&#29992;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Generalizable End-to-End Deep Learning Frameworks for Real-Time Attitude Estimation Using 6DoF Inertial Measurement Units. (arXiv:2302.06037v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#20351;&#29992;6DoF IMU&#27979;&#37327;&#20540;&#30340;&#23454;&#26102;&#24815;&#24615;&#23039;&#24577;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#23545;&#20110;&#19981;&#21516;&#30340;&#36816;&#21160;&#27169;&#24335;&#12289;&#37319;&#26679;&#29575;&#21644;&#29615;&#22659;&#24178;&#25200;&#20855;&#26377;&#24456;&#22909;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19971;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20351;&#29992;6DoF IMU&#27979;&#37327;&#20540;&#30340;&#23454;&#26102;&#24815;&#24615;&#23039;&#24577;&#20272;&#35745;&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#35835;&#25968;&#20316;&#20026;&#36755;&#20837;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#38024;&#23545;&#19981;&#21516;&#30340;&#36816;&#21160;&#27169;&#24335;&#12289;&#37319;&#26679;&#29575;&#21644;&#29615;&#22659;&#24178;&#25200;&#36827;&#34892;&#36890;&#29992;&#21270;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23618;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32452;&#25104;&#65292;&#25509;&#30528;&#20351;&#29992;&#20840;&#21521;&#21069;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#22235;&#20803;&#25968;&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24635;&#35745;&#36229;&#36807;120&#23567;&#26102;&#21644;200&#20844;&#37324;&#30340;IMU&#27979;&#37327;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#28388;&#27874;&#22120;&#65292;&#24182;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel end-to-end deep learning framework for real-time inertial attitude estimation using 6DoF IMU measurements. Inertial Measurement Units are widely used in various applications, including engineering and medical sciences. However, traditional filters used for attitude estimation suffer from poor generalization over different motion patterns and environmental disturbances. To address this problem, we propose two deep learning models that incorporate accelerometer and gyroscope readings as inputs. These models are designed to be generalized to different motion patterns, sampling rates, and environmental disturbances. Our models consist of convolutional neural network layers combined with Bi-Directional Long-Short Term Memory followed by a Fully Forward Neural Network to estimate the quaternion. We evaluate the proposed method on seven publicly available datasets, totaling more than 120 hours and 200 kilometers of IMU measurements. Our results show that the propos
&lt;/p&gt;</description></item><item><title>HDFormer&#26159;&#19968;&#31181;&#38754;&#21521;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#39640;&#38454;&#27880;&#24847;&#21147;&#24418;&#25104;&#20102;&#19968;&#20010;&#22810;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#21644;&#36974;&#25377;&#36739;&#37325;&#24773;&#20917;&#19979;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01825</link><description>&lt;p&gt;
HDFormer: &#38754;&#21521;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#39640;&#38454;&#26377;&#21521;Transformer
&lt;/p&gt;
&lt;p&gt;
HDFormer: High-order Directed Transformer for 3D Human Pose Estimation. (arXiv:2302.01825v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01825
&lt;/p&gt;
&lt;p&gt;
HDFormer&#26159;&#19968;&#31181;&#38754;&#21521;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#39640;&#38454;&#27880;&#24847;&#21147;&#24418;&#25104;&#20102;&#19968;&#20010;&#22810;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#21644;&#36974;&#25377;&#36739;&#37325;&#24773;&#20917;&#19979;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#20307;&#23039;&#24577;&#25968;&#25454;&#24207;&#21015;&#30340;&#32467;&#26500;&#24615;&#36136;&#65292;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#36523;&#20307;&#20851;&#33410;&#20043;&#38388;&#30340;&#25104;&#23545;&#20132;&#20114;&#65292;&#36825;&#23545;&#20110;&#28041;&#21450;&#37325;&#21472;&#20851;&#33410;&#21644;&#23039;&#21183;&#24555;&#36895;&#21464;&#21270;&#30340;&#22330;&#26223;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#39640;&#38454;&#26377;&#21521;Transformer (HDFormer)&#65292;&#23427;&#21033;&#29992;&#39640;&#38454;&#39592;&#39612;&#21644;&#20851;&#33410;&#20851;&#31995;&#26469;&#25913;&#36827;&#23039;&#24577;&#20272;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HDFormer&#32467;&#21512;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#39640;&#38454;&#27880;&#24847;&#21147;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#22810;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#12290;&#27492;&#27169;&#22359;&#20419;&#36827;&#20102;&#31532;&#19968;&#38454;&#8220;&#20851;&#33410;$\leftrightarrow$&#20851;&#33410;&#8221;&#65292;&#31532;&#20108;&#38454;&#8220;&#39592;$\leftrightarrow$&#20851;&#33410;&#8221;&#21644;&#39640;&#38454;&#8220;&#36229;&#39592;$\leftrightarrow$&#20851;&#33410;&#8221;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#21644;&#36974;&#25377;&#36739;&#37325;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#29616;&#20195;CNN&#25216;&#26415;&#34987;&#38598;&#25104;&#21040;&#22522;&#20110;Transformer&#30340;&#32467;&#26500;&#20013;&#65292;&#24179;&#34913;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;HDFormer&#22312;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#30446;&#21069;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human pose estimation is a challenging task due to its structured data sequence nature. Existing methods primarily focus on pair-wise interaction of body joints, which is insufficient for scenarios involving overlapping joints and rapidly changing poses. To overcome these issues, we introduce a novel approach, the High-order Directed Transformer (HDFormer), which leverages high-order bone and joint relationships for improved pose estimation. Specifically, HDFormer incorporates both self-attention and high-order attention to formulate a multi-order attention module. This module facilitates first-order "joint$\leftrightarrow$joint", second-order "bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint" interactions, effectively addressing issues in complex and occlusion-heavy situations. In addition, modern CNN techniques are integrated into the transformer-based architecture, balancing the trade-off between performance and efficiency. HDFormer significantly outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#20013;&#24515;&#24322;&#26500;&#34892;&#21160;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;UCHA-DRL&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#22312;&#26080;&#32447;&#32593;&#32476;&#30340;&#20803;&#23431;&#23449;&#34394;&#25311;&#29616;&#23454;&#20013;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#22120;&#21521;&#29992;&#25143;&#19979;&#34892;&#36890;&#20449;&#30340;&#20449;&#36947;&#35775;&#38382;&#23433;&#25490;&#21644;&#20256;&#36755;&#21151;&#29575;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.01471</link><description>&lt;p&gt;
&#29992;&#25143;&#20013;&#24515;&#24322;&#26500;&#34892;&#21160;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#30340;&#20803;&#23431;&#23449;&#34394;&#25311;&#29616;&#23454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
User-centric Heterogeneous-action Deep Reinforcement Learning for Virtual Reality in the Metaverse over Wireless Networks. (arXiv:2302.01471v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#20013;&#24515;&#24322;&#26500;&#34892;&#21160;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;UCHA-DRL&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#22312;&#26080;&#32447;&#32593;&#32476;&#30340;&#20803;&#23431;&#23449;&#34394;&#25311;&#29616;&#23454;&#20013;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#22120;&#21521;&#29992;&#25143;&#19979;&#34892;&#36890;&#20449;&#30340;&#20449;&#36947;&#35775;&#38382;&#23433;&#25490;&#21644;&#20256;&#36755;&#21151;&#29575;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20803;&#23431;&#23449;&#27491;&#22312;&#23835;&#36215;&#12290;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#26159;&#20803;&#23431;&#23449;&#20013;&#34394;&#25311;&#23431;&#23449;&#30340;&#25903;&#25745;&#65292;&#33021;&#22815;&#20026;&#29992;&#25143;&#24102;&#26469;&#39640;&#24230;&#27785;&#28024;&#24335;&#30340;&#20307;&#39564;&#12290;&#22312;&#20803;&#23431;&#23449;&#20013;&#65292;&#31227;&#21160;&#24615;&#22791;&#21463;&#24378;&#35843;&#65292;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#36890;&#36807;&#20943;&#23569;&#26412;&#22320;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#20943;&#36731;&#37325;&#37327;&#12290;&#38024;&#23545;&#20803;&#23431;&#23449;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#34394;&#25311;&#29616;&#23454;&#29992;&#25143;&#30340;&#31995;&#32479;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#20917;&#65306;&#65288;i&#65289;&#26381;&#21153;&#22120;&#29983;&#25104;&#24103;&#24182;&#23558;&#23427;&#20204;&#20256;&#36755;&#32473;&#29992;&#25143;&#65307;&#65288;ii&#65289;&#29992;&#25143;&#22312;&#26412;&#22320;&#29983;&#25104;&#24103;&#65292;&#22240;&#27492;&#32791;&#36153;&#35774;&#22791;&#33021;&#37327;&#12290;&#27492;&#22806;&#65292;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#23545;&#20110;&#24103;&#29575;&#26377;&#19981;&#21516;&#30340;&#29305;&#28857;&#21644;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#22120;&#21521;&#29992;&#25143;&#19979;&#34892;&#36890;&#20449;&#30340;&#20449;&#36947;&#35775;&#38382;&#23433;&#25490;&#65288;&#21253;&#25324;&#24103;&#29983;&#25104;&#20301;&#32622;&#30340;&#20915;&#31574;&#65289;&#21644;&#20256;&#36755;&#21151;&#29575;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#25928;&#29992;&#12290;&#36825;&#20010;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#29992;&#25143;&#20013;&#24515;&#24322;&#26500;&#34892;&#21160;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;UCHA-DRL&#65289;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;Q&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#31216;&#20026;&#24322;&#26500;&#34892;&#21160;Q&#32593;&#32476;&#65288;HAQN&#65289;&#26469;&#27714;&#35299;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;UCHA-DRL&#31639;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#12289;&#29992;&#25143;&#20999;&#25442;&#27425;&#25968;&#21644;&#29992;&#25143;&#24179;&#22343;&#24103;&#29575;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Metaverse is emerging as maturing technologies are empowering the different facets. Virtual Reality (VR) technologies serve as the backbone of the virtual universe within the Metaverse to offer a highly immersive user experience. As mobility is emphasized in the Metaverse context, VR devices reduce their weights at the sacrifice of local computation abilities. In this paper, for a system consisting of a Metaverse server and multiple VR users, we consider two cases of (i) the server generating frames and transmitting them to users, and (ii) users generating frames locally and thus consuming device energy. Moreover, in our multi-user VR scenario for the Metaverse, users have different characteristics and demands for Frames Per Second (FPS). Then the channel access arrangement (including the decisions on frame generation location), and transmission powers for the downlink communications from the server to the users are jointly optimized to improve the utilities of users. This joint op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PI+ToD&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#28120;&#27760;&#27491;&#21017;&#21270;&#26469;&#39640;&#25928;&#20272;&#31639;&#32463;&#39564;&#23545;RL&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.11168</link><description>&lt;p&gt;
&#21738;&#20123;&#32463;&#39564;&#21487;&#20197;&#24433;&#21709;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#65311;&#20855;&#26377;&#28120;&#27760;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Which Experiences Are Influential for Your Agent? Policy Iteration with Turn-over Dropout. (arXiv:2301.11168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PI+ToD&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#28120;&#27760;&#27491;&#21017;&#21270;&#26469;&#39640;&#25928;&#20272;&#31639;&#32463;&#39564;&#23545;RL&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#39564;&#22238;&#25918;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#23384;&#20648;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#32463;&#39564;&#20250;&#24433;&#21709;RL&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#26377;&#20851;&#32463;&#39564;&#24433;&#21709;&#30340;&#20449;&#24687;&#23545;&#20110;&#32463;&#39564;&#28165;&#29702;&#21644;&#20998;&#26512;&#31561;&#21508;&#31181;&#30446;&#30340;&#37117;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#19968;&#20010;&#20272;&#35745;&#21333;&#20010;&#32463;&#39564;&#24433;&#21709;&#30340;&#26041;&#27861;&#26159;&#20195;&#29702;&#27604;&#36739;&#65292;&#20294;&#24403;&#32463;&#39564;&#25968;&#37327;&#24456;&#22810;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#25104;&#26412;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI+ToD&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#20272;&#31639;&#32463;&#39564;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290; PI+ToD&#26159;&#19968;&#31181;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28120;&#27760;&#27491;&#21017;&#21270;&#26469;&#39640;&#25928;&#20272;&#31639;&#32463;&#39564;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;MuJoCo&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;PI + ToD&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL) with experience replay, experiences stored in a replay buffer influence the RL agent's performance. Information about the influence is valuable for various purposes, including experience cleansing and analysis. One method for estimating the influence of individual experiences is agent comparison, but it is prohibitively expensive when there is a large number of experiences. In this paper, we present PI+ToD as a method for efficiently estimating the influence of experiences. PI+ToD is a policy iteration that efficiently estimates the influence of experiences by utilizing turn-over dropout. We demonstrate the efficiency of PI+ToD with experiments in MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2301.07068</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#23433;&#20840;&#36755;&#20837;&#35745;&#25968;&#30340;#DNN-Verification&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#38656;&#35201;&#39640;&#24230;&#23433;&#20840;&#24615;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#20013;&#36234;&#26469;&#36234;&#34987;&#37319;&#29992;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21487;&#20197;&#29992;&#26469;&#26816;&#26597;DNN&#26159;&#21542;&#19981;&#23433;&#20840;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#19981;&#23433;&#20840;&#30340;&#36755;&#20837;&#37197;&#32622;&#65292;&#20294;&#23427;&#20204;&#30340;&#26159;/&#21542;&#36755;&#20986;&#23545;&#20110;&#20854;&#20182;&#30446;&#30340;&#65288;&#22914;&#23631;&#34109;&#12289;&#27169;&#22411;&#36873;&#25321;&#25110;&#22521;&#35757;&#25913;&#36827;&#65289;&#30340;&#20449;&#24687;&#19981;&#36275;&#22815;&#35814;&#32454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#35745;&#31639;&#23548;&#33268;DNN&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#36820;&#22238;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#12290;&#30001;&#20110;&#35813;&#38382;&#39064;&#30340;#P&#23436;&#22791;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#27491;&#30830;&#35745;&#25968;&#30340;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#21576;&#29616;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21644;&#22522;&#20110;&#35745;&#25968;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21512;&#22863;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#20249;&#20276;&#30340;&#22810;&#26679;&#24615;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26041;&#27861;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#20998;&#26512;&#24182;&#35782;&#21035;&#20249;&#20276;&#30340;&#28508;&#22312;&#31574;&#30053;&#22522;&#20803;&#65292;&#20174;&#32780;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21327;&#20316;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2301.06387</link><description>&lt;p&gt;
PECAN&#65306;&#21033;&#29992;&#31574;&#30053;&#21512;&#22863;&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#38646;&#26679;&#26412;&#20154;&#26426;&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
PECAN: Leveraging Policy Ensemble for Context-Aware Zero-Shot Human-AI Coordination. (arXiv:2301.06387v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21512;&#22863;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#20249;&#20276;&#30340;&#22810;&#26679;&#24615;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26041;&#27861;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#20998;&#26512;&#24182;&#35782;&#21035;&#20249;&#20276;&#30340;&#28508;&#22312;&#31574;&#30053;&#22522;&#20803;&#65292;&#20174;&#32780;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21327;&#20316;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#20154;&#31867;&#25968;&#25454;&#21363;&#21487;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#38646;&#26679;&#26412;&#20154;&#26426;&#21327;&#21516;&#22823;&#26377;&#21069;&#36884;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#33258;&#25105;&#23545;&#24328;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#20197;&#35757;&#32451;&#20986;&#33021;&#22815;&#19982;&#22810;&#31181;&#20154;&#31867;&#20249;&#20276;&#21327;&#21516;&#30340;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#36825;&#26679;&#20570;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#26377;&#38480;&#30340;&#20249;&#20276;&#25968;&#37327;&#20250;&#38480;&#21046;&#35757;&#32451;&#20986;&#30340;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#33021;&#21147;&#65307;2&#65289;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#33021;&#38024;&#23545;&#20247;&#22810;&#20249;&#20276;&#25552;&#20379;&#19968;&#20010;&#20844;&#20849;&#30340;&#26368;&#20339;&#31572;&#26696;&#65292;&#23548;&#33268;&#22312;&#38754;&#23545;&#26032;&#20249;&#20276;&#25110;&#20154;&#31867;&#26102;&#38646;&#26679;&#26412;&#21327;&#21516;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#31574;&#30053;&#21512;&#22863;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#20249;&#20276;&#30340;&#22810;&#26679;&#24615;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#20998;&#26512;&#24182;&#35782;&#21035;&#20249;&#20276;&#30340;&#28508;&#22312;&#31574;&#30053;&#22522;&#20803;&#65292;&#20174;&#32780;&#37319;&#21462;&#19981;&#21516;&#30340;&#21160;&#20316;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#26234;&#33021;&#20307;&#23558;&#33021;&#22815;&#23398;&#20064;&#26356;&#21152;&#26222;&#36941;&#30340;&#21512;&#20316;&#34892;&#20026;&#65292;&#19982;&#19981;&#21516;&#30340;&#20249;&#20276;&#21327;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot human-AI coordination holds the promise of collaborating with humans without human data. Prevailing methods try to train the ego agent with a population of partners via self-play. However, these methods suffer from two problems: 1) The diversity of a population with finite partners is limited, thereby limiting the capacity of the trained ego agent to collaborate with a novel human; 2) Current methods only provide a common best response for every partner in the population, which may result in poor zero-shot coordination performance with a novel partner or humans. To address these issues, we first propose the policy ensemble method to increase the diversity of partners in the population, and then develop a context-aware method enabling the ego agent to analyze and identify the partner's potential policy primitives so that it can take different actions accordingly. In this way, the ego agent is able to learn more universal cooperative behaviors for collaborating with diverse par
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;KNIFE&#65292;&#21487;&#20197;&#20174;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#20013;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;&#65292;&#36827;&#32780;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09721</link><description>&lt;p&gt;
KNIFE: &#20174;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#20013;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales. (arXiv:2212.09721v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09721
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;KNIFE&#65292;&#21487;&#20197;&#20174;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#20013;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;&#65292;&#36827;&#32780;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#24847;&#22806;&#38169;&#35823;&#24341;&#36215;&#20102;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#24576;&#30097;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24494;&#35843;/&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#24863;&#20852;&#36259;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#20219;&#21153;&#23454;&#20363;&#21644;&#20854;&#20851;&#32852;&#30340;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#65288;FTR&#65289;&#65292;&#36825;&#20123;&#29702;&#30001;&#35299;&#37322;&#20102;&#39044;&#27979;&#27491;&#30830;&#20219;&#21153;&#36755;&#20986;&#30340;&#27491;&#30830;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#26080;&#27861;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#25552;&#31034;&#38656;&#35201;&#36807;&#22823;&#65288;&#21363;&gt;50B&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#25165;&#33021;&#33391;&#22909;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KNIFE&#65292;&#35777;&#26126;&#20174;FTR&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;&#65292;&#23558;&#20854;&#28748;&#36755;&#21040;&#23567;&#22411;&#65288;&#21363;&lt;1B&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;KNIFE&#23545;&#19968;&#20010;&#24072;&#29983;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;&#32473;&#23450;&#20219;&#21153;&#36755;&#20837;&#21644;FTR&#65289;&#65292;&#20197;&#39044;&#27979;&#20219;&#21153;&#36755;&#20986;&#65292;&#23558;&#25512;&#29702;&#30693;&#35782;&#20174;FTR&#36716;&#31227;&#33267;&#24072;&#29983;&#38544;&#34255;&#29366;&#24577;&#12290;&#20854;&#27425;&#65292;KNIFE&#23545;&#19968;&#20010;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;&#20165;&#32473;&#23450;&#20219;&#21153;&#36755;&#20837;&#65289;&#65292;&#20197;&#20351;&#20854;&#38544;&#34255;&#29366;&#24577;&#31867;&#20284;&#20110;&#24072;&#29983;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have yielded impressive results on many language reasoning tasks, but their unexpected errors raise doubts about their reasoning abilities. In light of this, there is growing interest in finetuning/prompting LMs with both task instances and their associated free-text rationales (FTRs), which explain the correct reasoning process for predicting the correct task output (i.e., how to be "right for the right reasons"). However, existing finetuning methods fail to improve LM performance, while prompting needs prohibitively large (i.e., &gt;50B) LMs to work well. We propose KNIFE, which shows that reasoning knowledge can be effectively distilled from FTRs into a small (i.e., &lt;1B) LM and improve the LM's performance. First, KNIFE finetunes a teacher LM (given task input and FTR) to predict the task output, transferring reasoning knowledge from the FTRs to the teacher's hidden states. Second, KNIFE finetunes a student LM (given task input only) such that its hidden states ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#19977;&#20154; Kuhn poker &#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26126;&#26174;&#36229;&#36807;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.06027</link><description>&lt;p&gt;
&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games. (arXiv:2212.06027v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#19977;&#20154; Kuhn poker &#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26126;&#26174;&#36229;&#36807;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#20195;&#29702;&#21830;&#19982;&#22810;&#20010;&#23545;&#31435;&#20195;&#29702;&#21830;&#36827;&#34892;&#25112;&#30053;&#20114;&#21160;&#65292;&#23545;&#25163;&#21487;&#33021;&#37319;&#29992;&#21508;&#31181;&#31574;&#30053;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#24773;&#22659;&#65292;&#35774;&#35745;&#20195;&#29702;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#35745;&#31639;&#25110;&#36924;&#36817;&#30456;&#20851;&#30340;&#21338;&#24328;&#29702;&#35770;&#35299;&#65292;&#22914;&#32435;&#20160;&#22343;&#34913;&#65292;&#28982;&#21518;&#36981;&#24490;&#35268;&#23450;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31574;&#30053;&#24573;&#30053;&#20102;&#23545;&#25163;&#29609;&#28216;&#25103;&#30340;&#20219;&#20309;&#35266;&#23519;&#65292;&#36825;&#20123;&#35266;&#23519;&#21487;&#33021;&#34920;&#26126;&#21487;&#20197;&#21033;&#29992;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20154;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#23545;&#25163;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20132;&#20114;&#25910;&#38598;&#23545;&#25163;&#29609;&#28216;&#25103;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#23545;&#19977;&#20154; Kuhn &#25169;&#20811;&#23637;&#24320;&#20102;&#23545;&#35768;&#22810;&#30495;&#23454;&#23545;&#25163;&#21644;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#30340;&#20195;&#29702;&#21830;&#65292;&#21253;&#25324;&#20934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#29992;&#20110;&#27721;&#35821;&#28431;&#23383;&#26816;&#26597;&#30340;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#30340;&#25928;&#26524;&#21644;&#24212;&#29992;&#26041;&#21521;&#65292;&#25552;&#20986;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#27979;&#35797;&#35774;&#32622;&#65292;&#24182;&#20844;&#24320;&#20102;&#25152;&#26377;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2212.04068</link><description>&lt;p&gt;
&#25506;&#31350;&#29992;&#20110;&#27721;&#35821;&#28431;&#23383;&#26816;&#26597;&#30340;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#65306;&#26377;&#25928;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Investigating Glyph Phonetic Information for Chinese Spell Checking: What Works and What's Next. (arXiv:2212.04068v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#29992;&#20110;&#27721;&#35821;&#28431;&#23383;&#26816;&#26597;&#30340;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#30340;&#25928;&#26524;&#21644;&#24212;&#29992;&#26041;&#21521;&#65292;&#25552;&#20986;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#27979;&#35797;&#35774;&#32622;&#65292;&#24182;&#20844;&#24320;&#20102;&#25152;&#26377;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39044;&#35757;&#32451;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27721;&#35821;&#28431;&#23383;&#26816;&#26597;&#65288;CSC&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23383;&#24418;&#21644;&#38899;&#26631;&#31561;&#20449;&#24687;&#26469;&#25913;&#21892;&#21306;&#20998;&#38169;&#25340;&#23383;&#31526;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#19981;&#22909;&#29702;&#35299;&#65306;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#21253;&#21547;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#65292;&#20197;&#21450;&#22914;&#26524;&#21253;&#21547;&#36825;&#20123;&#20449;&#24687;&#65292;&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#22312; CSC &#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#30340; CSC &#27169;&#22411;&#27867;&#21270;&#24615;&#27979;&#35797;&#35774;&#32622;&#12290;&#25152;&#26377;&#20195;&#30721;&#37117;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge. Previous research has explored using information such as glyphs and phonetics to improve the ability to distinguish misspelled characters, with good results. However, the generalization ability of these models is not well understood: it is unclear whether they incorporate glyph-phonetic information and, if so, whether this information is fully utilized. In this paper, we aim to better understand the role of glyph-phonetic information in the CSC task and suggest directions for improvement. Additionally, we propose a new, more challenging, and practical setting for testing the generalizability of CSC models. All code is made publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#32771;&#34385;&#25104;&#26412;&#30340;&#20844;&#24179;&#35268;&#21010;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29983;&#25104;&#20844;&#24179;&#35745;&#21010;&#19981;&#38656;&#35201;&#29306;&#29298;&#22826;&#22810;&#30340;&#35745;&#21010;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.00506</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Multi-Agent Planning. (arXiv:2212.00506v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#32771;&#34385;&#25104;&#26412;&#30340;&#20844;&#24179;&#35268;&#21010;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29983;&#25104;&#20844;&#24179;&#35745;&#21010;&#19981;&#38656;&#35201;&#29306;&#29298;&#22826;&#22810;&#30340;&#35745;&#21010;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20013;&#65292;&#19968;&#32452;&#26234;&#33021;&#20307;&#38656;&#35201;&#23436;&#25104;&#19968;&#32452;&#30446;&#26631;&#12290;&#26080;&#35770;&#26159;&#22312;&#30446;&#26631;&#20998;&#37197;&#20043;&#21069;&#36824;&#26159;&#30452;&#25509;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#20316;&#21697;&#37117;&#27809;&#26377;&#20851;&#27880;&#26234;&#33021;&#20307;&#20043;&#38388;&#30446;&#26631;&#30340;&#20844;&#24179;&#20998;&#37197;/&#23436;&#25104;&#12290;&#26412;&#25991;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;&#20844;&#24179;&#24615;&#26041;&#26696;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#29983;&#25104;&#32771;&#34385;&#25104;&#26412;&#30340;&#20844;&#24179;&#35745;&#21010;&#30340;&#26032;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#26469;&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#30446;&#26631;&#39044;&#20998;&#37197;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#20998;&#37197;&#26469;&#35299;&#20915;&#38598;&#20013;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20219;&#21153;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#20197;&#35745;&#21010;&#20026;&#22522;&#30784;&#30340;&#32534;&#35793;&#26041;&#27861;&#65292;&#20801;&#35768;&#21516;&#26102;&#35299;&#20915;&#30446;&#26631;&#20998;&#37197;&#21644;&#35268;&#21010;&#30340;&#32852;&#21512;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#32473;&#23450;&#30340;&#20844;&#24179;&#24615;&#26041;&#26696;&#12290;&#22312;&#22810;&#20010;&#26631;&#20934;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#20248;&#20110;&#19981;&#21516;&#30340;&#22522;&#32447;&#12290;&#20182;&#20204;&#36824;&#34920;&#26126;&#65292;&#29983;&#25104;&#20844;&#24179;&#35745;&#21010;&#19981;&#38656;&#35201;&#29306;&#29298;&#22826;&#22810;&#30340;&#35745;&#21010;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cooperative Multi-Agent Planning (MAP), a set of goals has to be achieved by a set of agents. Independently of whether they perform a pre-assignment of goals to agents or they directly search for a solution without any goal assignment, most previous works did not focus on a fair distribution/achievement of goals by agents. This paper adapts well-known fairness schemes to MAP, and introduces two novel approaches to generate cost-aware fair plans. The first one solves an optimization problem to pre-assign goals to agents, and then solves a centralized MAP task using that assignment. The second one consists of a planning-based compilation that allows solving the joint problem of goal assignment and planning while taking into account the given fairness scheme. Empirical results in several standard MAP benchmarks show that these approaches outperform different baselines. They also show that there is no need to sacrifice much plan cost to generate fair plans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;MetaC&#24694;&#24847;&#25915;&#20987;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27450;&#35784;&#32773;&#26816;&#27979;&#27169;&#22359;PDR&#65292;&#26126;&#30830;&#32771;&#34385;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11534</link><description>&lt;p&gt;
&#20174;&#33258;&#36866;&#24212;&#27450;&#35784;&#32773;&#26816;&#27979;&#25506;&#31350;&#23545;&#25239;&#40065;&#26834;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Adversarially Robust Recommendation from Adaptive Fraudster Detection. (arXiv:2211.11534v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;MetaC&#24694;&#24847;&#25915;&#20987;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27450;&#35784;&#32773;&#26816;&#27979;&#27169;&#22359;PDR&#65292;&#26126;&#30830;&#32771;&#34385;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#22791;&#21463;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;GraphRfi&#65292;&#23427;&#26377;&#25928;&#20943;&#36731;&#20102;&#27880;&#20837;&#30340;&#34394;&#20551;&#29992;&#25143;&#30340;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GraphRfi&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22240;&#20026;&#20854;&#27450;&#35784;&#32773;&#26816;&#27979;&#32452;&#20214;&#30340;&#30417;&#30563;&#24615;&#36136;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#33719;&#24471;&#24178;&#20928;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;MetaC&#24694;&#24847;&#25915;&#20987;&#65292;&#38024;&#23545;GNN-based&#21644;MF-based&#25512;&#33616;&#31995;&#32479;&#12290;&#26681;&#25454;&#25105;&#20204;&#20174;&#26131;&#21463;&#25915;&#20987;&#24615;&#20998;&#26512;&#20013;&#24471;&#21040;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27450;&#35784;&#32773;&#26816;&#27979;&#27169;&#22359;&#65292;&#26126;&#30830;&#32771;&#34385;&#20102;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22359;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#25512;&#33616;&#31995;&#32479;&#30340;&#25554;&#20214;&#65292;&#24418;&#25104;&#19968;&#20010;&#31283;&#20581;&#30340;&#26694;&#26550;&#65288;PDR&#65289;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;&#22312;&#25915;&#20987;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22312;&#26500;&#24314;&#27450;&#35784;&#32773;&#26816;&#27979;&#27169;&#22359;&#26102;&#32771;&#34385;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#23545;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of recommender systems under node injection attacks has garnered significant attention. Recently, GraphRfi, a GNN-based recommender system, was proposed and shown to effectively mitigate the impact of injected fake users. However, we demonstrate that GraphRfi remains vulnerable to attacks due to the supervised nature of its fraudster detection component, where obtaining clean labels is challenging in practice. In particular, we propose a powerful poisoning attack, MetaC, against both GNN-based and MF-based recommender systems. Furthermore, we analyze why GraphRfi fails under such an attack. Then, based on our insights obtained from vulnerability analysis, we design an adaptive fraudster detection module that explicitly considers label uncertainty. This module can serve as a plug-in for different recommender systems, resulting in a robust framework named PDR. Comprehensive experiments show that our defense approach outperforms other benchmark methods under attacks. Overal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.11093</link><description>&lt;p&gt;
[RE]VER&#65306;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#20197;&#38416;&#36848;&#23454;&#20307;&#21644;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
[RE]VER: Learning Natural Language Representations for Verbalizing Entities and Relations. (arXiv:2211.11093v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#36890;&#36807;&#29702;&#35299;&#23454;&#20307;&#21644;&#20851;&#31995;&#26469;&#20102;&#35299;&#19990;&#30028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entities and relationships between entities are vital in the real world. Essentially, we understand the world by understanding entities and relations. For instance, to understand a field, e.g., computer science, we need to understand the relevant concepts, e.g., machine learning, and the relationships between concepts, e.g., machine learning and artificial intelligence. To understand a person, we should first know who he/she is and how he/she is related to others. To understand entities and relations, humans may refer to natural language descriptions. For instance, when learning a new scientific term, people usually start by reading its definition in dictionaries or encyclopedias. To know the relationship between two entities, humans tend to create a sentence to connect them. In this paper, we propose [RE]VER: A Unified Model for Verbalizing Entities and Relations. Specifically, we attempt to build a system that takes any entity or entity set as input and generates a sentence to repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.08073</link><description>&lt;p&gt;
GLUE-X: &#20174;ODD&#26222;&#36866;&#24615;&#35282;&#24230;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24050;&#30693;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#30340;ODD&#26222;&#36866;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#24378;&#35843;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#22914;&#20309;&#34913;&#37327;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#65288;&#21253;&#25324;GPT-3&#21644;GPT-3.5&#65289;&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#30830;&#35748;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#19979;&#65292;&#19982;ID&#20934;&#30830;&#24230;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#38656;&#35201;&#25913;&#21892;NLP&#20219;&#21153;&#20013;&#30340;OOD&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26234;&#33021;&#32593;&#26684;&#29983;&#25104;&#65288;IMG&#65289;&#29616;&#29366;&#65292;&#27010;&#25324;&#20102;113&#31181;IMG&#26041;&#27861;&#30340;&#26680;&#24515;&#25216;&#26415;&#12289;&#24212;&#29992;&#33539;&#22260;&#12289;&#20195;&#29702;&#23398;&#20064;&#30446;&#26631;&#12289;&#25968;&#25454;&#31867;&#22411;&#12289;&#30446;&#26631;&#25361;&#25112;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.06009</link><description>&lt;p&gt;
&#26234;&#33021;&#32593;&#26684;&#29983;&#25104;&#30340;&#29616;&#29366;&#65306;&#32508;&#36848;&#19982;&#21069;&#26223;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
What's the Situation with Intelligent Mesh Generation: A Survey and Perspectives. (arXiv:2211.06009v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26234;&#33021;&#32593;&#26684;&#29983;&#25104;&#65288;IMG&#65289;&#29616;&#29366;&#65292;&#27010;&#25324;&#20102;113&#31181;IMG&#26041;&#27861;&#30340;&#26680;&#24515;&#25216;&#26415;&#12289;&#24212;&#29992;&#33539;&#22260;&#12289;&#20195;&#29702;&#23398;&#20064;&#30446;&#26631;&#12289;&#25968;&#25454;&#31867;&#22411;&#12289;&#30446;&#26631;&#25361;&#25112;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#32593;&#26684;&#29983;&#25104;&#65288;IMG&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#32593;&#26684;&#30340;&#25216;&#26415;&#65292;&#26159;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#30701;&#26242;&#30340;&#21382;&#21490;&#20013;&#65292;IMG&#26497;&#22823;&#22320;&#25299;&#23637;&#20102;&#32593;&#26684;&#29983;&#25104;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#21462;&#24471;&#20102;&#35768;&#22810;&#31361;&#30772;&#65292;&#24182;&#20026;&#32593;&#26684;&#29983;&#25104;&#21019;&#36896;&#20102;&#28508;&#22312;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;IMG&#30340;&#32508;&#36848;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#31995;&#32479;&#32508;&#36848;&#21644;&#25551;&#36848;&#24403;&#19979;IMG&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#25628;&#38598;&#20102;113&#31181;IMG&#26041;&#27861;&#65292;&#20174;&#22810;&#20010;&#35282;&#24230;&#65288;&#21253;&#25324;&#31639;&#27861;&#30340;&#26680;&#24515;&#25216;&#26415;&#21644;&#24212;&#29992;&#33539;&#22260;&#12289;&#20195;&#29702;&#23398;&#20064;&#30446;&#26631;&#12289;&#25968;&#25454;&#31867;&#22411;&#12289;&#30446;&#26631;&#25361;&#25112;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65289;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#22522;&#20110;&#20869;&#23481;&#25552;&#21462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#27861;&#65306;&#20851;&#38190;&#25216;&#26415;&#12289;&#36755;&#20986;&#32593;&#26684;&#21333;&#20803;&#20803;&#32032;&#12289;&#25152;&#38024;&#23545;&#30340;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#25991;&#29486;&#25910;&#38598;&#21644;&#20998;&#31867;&#25972;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent mesh generation (IMG) refers to a technique for generating mesh by machine learning, which is a relatively new and promising research field. Within its short lifespan, IMG has greatly expanded the generalizability and practicality of mesh generation techniques, achieved many breakthroughs and created potential possibilities for mesh generation. However, there is a lack of surveys that focus on IMG methods in recent works. In this paper, we are committed to a systematic and comprehensive survey that describes the contemporary IMG landscape. Focusing on 113 preliminary IMG methods, we conducted an in-depth analysis from multiple perspectives, including the core technique and application scope of the algorithm, agent learning goals, data types, targeting challenges, advantages, and limitations. With the aim of literature collection and classification based on content extraction, we propose three different taxonomies from three views: key techniques, output mesh unit elements, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#12290;</title><link>http://arxiv.org/abs/2211.05732</link><description>&lt;p&gt;
&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Online Contract Design. (arXiv:2211.05732v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#32447;&#24773;&#22659;&#19979;&#30340;&#38544;&#34255;-&#34892;&#21160;&#22996;&#25176;&#38382;&#39064;&#12290;&#22312;&#27599;&#36718;&#20013;&#65292;&#22996;&#25176;&#20154;&#21457;&#24067;&#19968;&#20221;&#21512;&#21516;&#65292;&#26681;&#25454;&#27599;&#20010;&#32467;&#26524;&#35268;&#23450;&#20195;&#29702;&#20154;&#30340;&#25903;&#20184;&#12290;&#20195;&#29702;&#20154;&#28982;&#21518;&#20570;&#20986;&#19968;&#20010;&#26368;&#22823;&#21270;&#22905;&#33258;&#24049;&#25928;&#29992;&#30340;&#25112;&#30053;&#34892;&#21160;&#36873;&#25321;&#65292;&#20294;&#30452;&#25509;&#35266;&#23519;&#19981;&#21040;&#34892;&#21160;&#12290;&#22996;&#25176;&#20154;&#35266;&#23519;&#32467;&#26524;&#24182;&#20174;&#20195;&#29702;&#20154;&#30340;&#34892;&#21160;&#36873;&#25321;&#20013;&#33719;&#24471;&#25928;&#29992;&#12290;&#26681;&#25454;&#36807;&#21435;&#30340;&#35266;&#23519;&#65292;&#22996;&#25176;&#20154;&#21160;&#24577;&#22320;&#35843;&#25972;&#21512;&#21516;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20854;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20854;Stackelberg&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21512;&#21516;&#31354;&#38388;&#20026;$[0,1]^m$&#26102;&#65292;Stackelberg&#36951;&#25022;&#30340;&#19978;&#30028;&#20026;$\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$&#65292;&#19979;&#30028;&#20026;$\Omega(T^{1-1/(m+2)})$&#65292;&#20854;&#20013;$\widetilde O$&#25490;&#38500;&#23545;&#25968;&#22240;&#23376;&#12290; &#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the hidden-action principal-agent problem in an online setting. In each round, the principal posts a contract that specifies the payment to the agent based on each outcome. The agent then makes a strategic choice of action that maximizes her own utility, but the action is not directly observable by the principal. The principal observes the outcome and receives utility from the agent's choice of action. Based on past observations, the principal dynamically adjusts the contracts with the goal of maximizing her utility.  We introduce an online learning algorithm and provide an upper bound on its Stackelberg regret. We show that when the contract space is $[0,1]^m$, the Stackelberg regret is upper bounded by $\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$, and lower bounded by $\Omega(T^{1-1/(m+2)})$, where $\widetilde O$ omits logarithmic factors. This result shows that exponential-in-$m$ samples are sufficient and necessary to learn a near-optimal contract, resolving an open probl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#22797;&#26434;&#31995;&#32479;&#26694;&#26550;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#39118;&#38505;&#21644;&#25552;&#20379;&#26410;&#26469;&#20998;&#26512;&#27169;&#26495;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;AI&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#38598;&#25104;&#24230;&#30340;&#25285;&#24551;&#65292;&#24182;&#24378;&#35843;&#20102;&#22686;&#24378;&#31995;&#32479;&#33021;&#21147;&#21644;&#33258;&#27835;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#26435;&#21147;&#21160;&#24577;&#30340;&#24847;&#22806;&#25913;&#21464;&#25110;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;</title><link>http://arxiv.org/abs/2211.03157</link><description>&lt;p&gt;
&#26816;&#39564;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#24046;&#24322;&#39118;&#38505;&#21644;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Examining the Differential Risk from High-level Artificial Intelligence and the Question of Control. (arXiv:2211.03157v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#22797;&#26434;&#31995;&#32479;&#26694;&#26550;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#39118;&#38505;&#21644;&#25552;&#20379;&#26410;&#26469;&#20998;&#26512;&#27169;&#26495;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;AI&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#38598;&#25104;&#24230;&#30340;&#25285;&#24551;&#65292;&#24182;&#24378;&#35843;&#20102;&#22686;&#24378;&#31995;&#32479;&#33021;&#21147;&#21644;&#33258;&#27835;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#26435;&#21147;&#21160;&#24577;&#30340;&#24847;&#22806;&#25913;&#21464;&#25110;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;21&#19990;&#32426;&#26368;&#20855;&#21464;&#38761;&#24615;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#26410;&#26469;AI&#33021;&#21147;&#30340;&#31243;&#24230;&#21644;&#33539;&#22260;&#20173;&#23384;&#22312;&#20851;&#38190;&#19981;&#30830;&#23450;&#24615;&#65292;&#21508;&#26041;&#23545;&#26102;&#38388;&#34920;&#21644;&#28508;&#22312;&#24433;&#21709;&#26377;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#38543;&#30528;&#22269;&#23478;&#21644;&#25216;&#26415;&#20844;&#21496;&#31454;&#30456;&#36861;&#27714;&#26356;&#22797;&#26434;&#21644;&#33258;&#20027;&#30340;AI&#31995;&#32479;&#65292;&#20154;&#20204;&#25285;&#24515;&#21322;&#36879;&#26126;AI&#20915;&#31574;&#36807;&#31243;&#30340;&#38598;&#25104;&#21644;&#30417;&#30563;&#31243;&#24230;&#12290;&#36825;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#20854;&#20013;&#31995;&#32479;&#23398;&#20064;&#20248;&#21270;&#30446;&#26631;&#32780;&#26080;&#38656;&#20154;&#31867;&#24110;&#21161;&#12290;&#30446;&#26631;&#21487;&#33021;&#26080;&#27861;&#23436;&#32654;&#22320;&#21046;&#23450;&#25110;&#20197;&#24847;&#22806;&#25110;&#28508;&#22312;&#26377;&#23475;&#30340;&#26041;&#24335;&#25191;&#34892;&#12290;&#38543;&#30528;&#31995;&#32479;&#22686;&#21152;&#21151;&#29575;&#21644;&#33258;&#20027;&#24615;&#65292;&#36825;&#21464;&#24471;&#26356;&#21152;&#20196;&#20154;&#25285;&#24551;&#65292;&#19968;&#20010;&#31361;&#28982;&#30340;&#33021;&#21147;&#36291;&#21319;&#21487;&#33021;&#23548;&#33268;&#26435;&#21147;&#21160;&#24577;&#24847;&#22806;&#36716;&#21464;&#29978;&#33267;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#22797;&#26434;&#31995;&#32479;&#26694;&#26550;&#26469;&#27169;&#25311;AI&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#26367;&#20195;&#26410;&#26469;&#20998;&#26512;&#30340;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is one of the most transformative technologies of the 21st century. The extent and scope of future AI capabilities remain a key uncertainty, with widespread disagreement on timelines and potential impacts. As nations and technology companies race toward greater complexity and autonomy in AI systems, there are concerns over the extent of integration and oversight of opaque AI decision processes. This is especially true in the subfield of machine learning (ML), where systems learn to optimize objectives without human assistance. Objectives can be imperfectly specified or executed in an unexpected or potentially harmful way. This becomes more concerning as systems increase in power and autonomy, where an abrupt capability jump could result in unexpected shifts in power dynamics or even catastrophic failures. This study presents a hierarchical complex systems framework to model AI risk and provide a template for alternative futures analysis. Survey data were co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#39044;&#27979;&#36741;&#21161;&#25628;&#32034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35268;&#21010;&#25928;&#29575;&#65292;&#36816;&#34892;&#26102;&#38388;&#32553;&#30701;&#20102;80%&#12290;</title><link>http://arxiv.org/abs/2211.01576</link><description>&lt;p&gt;
&#22522;&#20110;&#24207;&#21015;&#30340;&#35745;&#21010;&#21487;&#34892;&#24615;&#39044;&#27979;&#29992;&#20110;&#39640;&#25928;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning. (arXiv:2211.01576v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#39044;&#27979;&#36741;&#21161;&#25628;&#32034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35268;&#21010;&#25928;&#29575;&#65292;&#36816;&#34892;&#26102;&#38388;&#32553;&#30701;&#20102;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21487;&#29992;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#22810;&#20010;&#20851;&#33410;&#21644;&#21487;&#31227;&#21160;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#35299;&#20915;&#31227;&#21160;&#25805;&#20316;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#36890;&#36807;&#23398;&#20064;&#30340;&#35745;&#21010;&#21487;&#34892;&#24615;&#39044;&#27979;&#22120;&#23545;&#20256;&#32479;TAMP&#35268;&#21010;&#22120;&#30340;&#25628;&#32034;&#36807;&#31243;&#36827;&#34892;&#20559;&#32622;&#12290;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;PIGINet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#25509;&#25910;&#20219;&#21153;&#35745;&#21010;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#29366;&#24577;&#65292;&#24182;&#39044;&#27979;&#19982;&#20219;&#21153;&#35745;&#21010;&#30456;&#20851;&#32852;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#23558;PIGINet&#38598;&#25104;&#21040;&#19968;&#20010;TAMP&#35268;&#21010;&#22120;&#20013;&#65292;&#35813;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#39640;&#23618;&#20219;&#21153;&#35745;&#21010;&#65292;&#25353;&#29031;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#25490;&#24207;&#65292;&#24182;&#20381;&#27425;&#36827;&#34892;&#32454;&#21270;&#12290;&#25105;&#20204;&#23545;&#19971;&#31181;&#21416;&#25151;&#37325;&#25490;&#38382;&#39064;&#30340;TAMP&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#36827;&#34892;&#35780;&#20272;&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#38750;&#23398;&#20064;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PIGINet&#26174;&#30528;&#25552;&#39640;&#20102;&#35268;&#21010;&#25928;&#29575;&#65292;&#22312;&#29366;&#24577;&#36739;&#23567;&#30340;&#38382;&#39064;&#19978;&#32553;&#30701;&#20102;&#36816;&#34892;&#26102;&#38388;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a learning-enabled Task and Motion Planning (TAMP) algorithm for solving mobile manipulation problems in environments with many articulated and movable obstacles. Our idea is to bias the search procedure of a traditional TAMP planner with a learned plan feasibility predictor. The core of our algorithm is PIGINet, a novel Transformer-based learning method that takes in a task plan, the goal, and the initial state, and predicts the probability of finding motion trajectories associated with the task plan. We integrate PIGINet within a TAMP planner that generates a diverse set of high-level task plans, sorts them by their predicted likelihood of feasibility, and refines them in that order. We evaluate the runtime of our TAMP algorithm on seven families of kitchen rearrangement problems, comparing its performance to that of non-learning baselines. Our experiments show that PIGINet substantially improves planning efficiency, cutting down runtime by 80% on problems with small state
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#20013;&#8220;&#26080;&#28165;&#26224;&#35821;&#38899;&#8221;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#22122;&#22768;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#26377;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.15368</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#22122;&#22768;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#26080;&#28165;&#26224;&#35821;&#38899;&#30340;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech. (arXiv:2210.15368v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15368
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#20013;&#8220;&#26080;&#28165;&#26224;&#35821;&#38899;&#8221;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#22122;&#22768;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#26377;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28165;&#26224;&#35821;&#38899;&#26159;&#21457;&#23637;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#23454;&#38469;&#25361;&#25112;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#35757;&#32451;&#20934;&#21017;&#21644;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#23384;&#22312;&#19981;&#21487;&#36991;&#20813;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#19981;&#21033;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#26469;&#25913;&#36827;&#20808;&#21069;&#25552;&#20986;&#30340;&#22122;&#22768;&#30446;&#26631;&#35757;&#32451;&#65288;NyTT&#65289;&#12290;&#30001;&#20110;&#22495;&#20869;&#22122;&#22768;&#19982;&#22806;&#37096;&#22122;&#22768;&#30340;&#21516;&#36136;&#24615;&#26159;NyTT&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#65292;&#25105;&#20204;&#36890;&#36807;&#28151;&#38899;&#35757;&#32451;&#22810;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20351;&#29992;&#25945;&#24072;&#27169;&#22411;&#20272;&#35745;&#30340;&#35821;&#38899;&#21644;&#22122;&#22768;&#36827;&#34892;&#22686;&#24378;&#30446;&#26631;&#35757;&#32451;&#65292;&#25110;&#32773;2&#65289;&#20351;&#29992;&#21407;&#22987;&#30340;&#22122;&#22768;&#35821;&#38899;&#21644;&#25945;&#24072;&#27169;&#22411;&#20272;&#35745;&#30340;&#22122;&#22768;&#36827;&#34892;&#22122;&#22768;&#30446;&#26631;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#24072;/&#23398;&#29983;&#25512;&#29702;&#26041;&#38754;&#65292;&#20854;&#20013;&#39044;&#27979;&#30340;&#28165;&#26224;&#35821;&#38899;&#26159;&#36890;&#36807;&#25945;&#24072;&#21644;&#26368;&#32456;&#23398;&#29983;&#27169;&#22411;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of clean speech is a practical challenge to the development of speech enhancement systems, which means that there is an inevitable mismatch between their training criterion and evaluation metric. In response to this unfavorable situation, we propose a training and inference strategy that additionally uses enhanced speech as a target by improving the previously proposed noisy-target training (NyTT). Because homogeneity between in-domain noise and extraneous noise is the key to the effectiveness of NyTT, we train various student models by remixing 1) the teacher model's estimated speech and noise for enhanced-target training or 2) raw noisy speech and the teacher model's estimated noise for noisy-target training. Experimental results show that our proposed method outperforms several baselines, especially with the teacher/student inference, where predicted clean speech is derived successively through the teacher and final student models.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DiffusionDB&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#21253;&#21547;1400&#19975;&#24352;&#22270;&#20687;&#21644;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26102;&#25152;&#38656;&#30340;&#36866;&#24403;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#25552;&#31034;&#26679;&#24335;&#21644;&#36229;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#65292;&#29978;&#33267;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2210.14896</link><description>&lt;p&gt;
DiffusionDB: &#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#30011;&#24266;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models. (arXiv:2210.14896v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14896
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DiffusionDB&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#21253;&#21547;1400&#19975;&#24352;&#22270;&#20687;&#21644;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26102;&#25152;&#38656;&#30340;&#36866;&#24403;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#25552;&#31034;&#26679;&#24335;&#21644;&#36229;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#65292;&#29978;&#33267;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#32454;&#33410;&#30340;&#22270;&#20687;&#38656;&#35201;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#32780;&#19988;&#24448;&#24448;&#19981;&#28165;&#26970;&#27169;&#22411;&#23545;&#19981;&#21516;&#25552;&#31034;&#30340;&#21453;&#24212;&#25110;&#26368;&#20339;&#25552;&#31034;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffusionDB&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;6.5TB&#65292;&#21253;&#21547;&#20351;&#29992;Stable Diffusion&#29983;&#25104;&#30340;1400&#19975;&#24352;&#22270;&#20687;&#65292;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#21644;&#30001;&#30495;&#23454;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#31034;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#25351;&#20986;&#20102;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#30340;&#29305;&#23450;&#36229;&#21442;&#25968;&#20540;&#21644;&#25552;&#31034;&#26679;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#28508;&#22312;&#26377;&#23475;&#27169;&#22411;&#20351;&#29992;&#30340;&#35777;&#25454;&#65292;&#22914;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;&#36825;&#20010;&#20154;&#20026;&#39537;&#21160;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#21069;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20026;&#20102;&#35299;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#28608;&#21160;&#20154;&#24515;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#36890;&#29992;&#30446;&#26631;&#26465;&#20214;&#27169;&#22411;GNM&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#23454;&#29616;&#26356;&#24378;&#22823;&#12289;&#26356;&#20581;&#22766;&#30340;&#23548;&#33322;&#24615;&#33021;&#65292;&#24182;&#33021;&#39537;&#21160;&#20219;&#20309;&#20855;&#26377;&#36866;&#24403;&#35270;&#35273;&#24863;&#30693;&#36755;&#20837;&#30340;&#26426;&#22120;&#20154;&#12290;</title><link>http://arxiv.org/abs/2210.03370</link><description>&lt;p&gt;
GNM: &#36890;&#29992;&#23548;&#33322;&#27169;&#22411;&#39537;&#21160;&#20219;&#20309;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
GNM: A General Navigation Model to Drive Any Robot. (arXiv:2210.03370v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03370
&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30446;&#26631;&#26465;&#20214;&#27169;&#22411;GNM&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#23454;&#29616;&#26356;&#24378;&#22823;&#12289;&#26356;&#20581;&#22766;&#30340;&#23548;&#33322;&#24615;&#33021;&#65292;&#24182;&#33021;&#39537;&#21160;&#20219;&#20309;&#20855;&#26377;&#36866;&#24403;&#35270;&#35273;&#24863;&#30693;&#36755;&#20837;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26159;&#35270;&#35273;&#23548;&#33322;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#22522;&#20110;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#33021;&#21147;&#21463;&#21040;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#26469;&#33258;&#22810;&#20010;&#19981;&#21516;&#20294;&#32467;&#26500;&#30456;&#20284;&#30340;&#26426;&#22120;&#20154;&#30340;&#25968;&#25454;&#22522;&#30784;&#19978;&#35757;&#32451;&#38754;&#21521;&#35270;&#35273;&#23548;&#33322;&#30340;&#36890;&#29992;&#30446;&#26631;&#26465;&#20214;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#22312;&#21508;&#31181;&#29615;&#22659;&#21644;&#26426;&#36523;&#19978;&#30340;&#24191;&#27867;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#38388;&#25968;&#25454;&#20849;&#20139;&#30340;&#24517;&#35201;&#35774;&#35745;&#20915;&#31574;&#65292;&#21253;&#25324;&#20351;&#29992;&#26102;&#38388;&#19978;&#19979;&#25991;&#21644;&#26631;&#20934;&#21270;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#20174;&#24322;&#26500;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#20840;&#23616;&#31574;&#30053;&#20248;&#20110;&#20219;&#20309;&#21333;&#19968;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;6&#20010;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;60&#23567;&#26102;&#23548;&#33322;&#36712;&#36857;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#26032;&#26426;&#22120;&#20154;&#19978;&#37096;&#32626;&#35757;&#32451;&#21518;&#30340;GNM&#65292;&#21253;&#25324;&#19968;&#20010;&#27424;&#39537;&#21160;&#30340;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#21644;&#26356;&#21152;&#31283;&#20581;&#30340;&#23548;&#33322;&#24615;&#33021;&#65292;&#32780;GNM&#21487;&#20197;&#39537;&#21160;&#20219;&#20309;&#20855;&#26377;&#36866;&#24403;&#35270;&#35273;&#24863;&#30693;&#36755;&#20837;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.01969</link><description>&lt;p&gt;
&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#19968;&#33324;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#24674;&#22797;&#19987;&#23478;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#24230;&#22797;&#26434;&#30340;&#12289;&#38271;&#26102;&#31243;&#20219;&#21153;&#65292;&#24674;&#22797;&#21333;&#19968;&#25972;&#20307;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#65292;&#32780;&#19987;&#23478;&#31574;&#30053;&#36890;&#24120;&#21253;&#21547;&#23376;&#20219;&#21153;&#23618;&#27425;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#65288;HIL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36873;&#39033;&#26694;&#26550;&#20013;&#26174;&#24335;&#22320;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#27963;&#21160;&#32467;&#26500;&#26469;&#23398;&#20064;&#20998;&#23618;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;HIL&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#23376;&#20219;&#21153;&#32467;&#26500;&#19982;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35201;&#20040;&#26080;&#27861;&#21516;&#26102;&#22312;&#20998;&#23618;&#26694;&#26550;&#20013;&#23398;&#20064;&#39640;&#32423;&#21035;&#21644;&#20302;&#32423;&#21035;&#31574;&#30053;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HIL&#31639;&#27861;&#8212;&#8212;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;H-AIRL&#65289;&#65292;&#23427;&#22312;&#26368;&#26032;&#30340;IL&#31639;&#27861;AIRL&#19978;&#25193;&#23637;&#20102;&#19968;&#27493;&#36873;&#39033;&#26694;&#26550;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;AIRL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) has been proposed to recover the expert policy from demonstrations. However, it would be difficult to learn a single monolithic policy for highly-complex long-horizon tasks of which the expert policy usually contains subtask hierarchies. Therefore, Hierarchical Imitation Learning (HIL) has been developed to learn a hierarchical policy from expert demonstrations through explicitly modelling the activity structure in a task with the option framework. Existing HIL methods either overlook the causal relationship between the subtask structure and the learned policy, or fail to learn the high-level and low-level policy in the hierarchical framework in conjuncture, which leads to suboptimality. In this work, we propose a novel HIL algorithm -Hierarchical Adversarial Inverse Reinforcement Learning (H-AIRL), which extends a state-of-the-art (SOTA) IL algorithm -- AIRL, with the one-step option framework. Specifically, we redefine the AIRL objectives on the extended sta
&lt;/p&gt;</description></item><item><title>L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.14402</link><description>&lt;p&gt;
L2XGNN&#65306;&#23398;&#20064;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
L2XGNN: Learning to Explain Graph Neural Networks. (arXiv:2209.14402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14402
&lt;/p&gt;
&lt;p&gt;
L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#23398;&#20064;&#35299;&#37322;&#65288;L2X&#65289;&#33539;&#24335;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2XGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#20379;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;L2XGNN&#23398;&#20064;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#65292;&#36825;&#20123;&#23376;&#22270;&#20165;&#29992;&#20110;GNN&#30340;&#20449;&#24687;&#20256;&#36882;&#25805;&#20316;&#20013;&#12290;&#23545;&#27169;&#20307;&#26045;&#21152;&#36825;&#26679;&#30340;&#38480;&#21046;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#26131;&#35299;&#37322;&#21644;&#26356;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#22312;&#20960;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;L2XGNN&#23454;&#29616;&#20102;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#65292;&#21516;&#26102;&#30830;&#20445;&#20165;&#20351;&#29992;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;L2XGNN&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#24515;&#29702;&#23398;&#24341;&#23548;&#24605;&#32500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#38544;&#21947;&#29702;&#35299;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#38544;&#21547;&#21464;&#37327;&#21644;&#20851;&#31995;&#26469;&#36873;&#25321;&#27491;&#30830;&#30340;&#37322;&#20041;&#12290;</title><link>http://arxiv.org/abs/2209.08141</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#24605;&#32500;&#38142;&#35302;&#21457;&#35782;&#21035;&#38544;&#21547;&#21464;&#37327;&#21644;&#25512;&#29702;&#20851;&#31995;&#36827;&#34892;&#38544;&#21947;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models. (arXiv:2209.08141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#24515;&#29702;&#23398;&#24341;&#23548;&#24605;&#32500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#38544;&#21947;&#29702;&#35299;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#38544;&#21547;&#21464;&#37327;&#21644;&#20851;&#31995;&#26469;&#36873;&#25321;&#27491;&#30830;&#30340;&#37322;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29702;&#35299;&#30340;&#27010;&#29575;&#27169;&#22411;&#26159;&#30740;&#31350;&#20154;&#20204;&#35821;&#35328;&#20351;&#29992;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#29992;&#36328;&#39046;&#22495;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#27010;&#29575;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#37319;&#29992;&#24605;&#32500;&#38142;&#35302;&#21457;&#26041;&#24335;&#26469;&#23558;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24341;&#20837;LLMs&#20013;&#65292;&#20197;&#38544;&#21947;&#29702;&#35299;&#20026;&#20363;&#26469;&#25506;&#31350;&#36825;&#19968;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24605;&#32500;&#38142;&#35302;&#21457;&#26041;&#24335;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38544;&#21547;&#21464;&#37327;&#65292;&#24182;&#24605;&#32771;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#36873;&#25321;&#36866;&#24403;&#30340;&#38544;&#21947;&#37322;&#20041;&#12290;&#25152;&#36873;&#25321;&#30340;&#38544;&#21547;&#21464;&#37327;&#21644;&#20851;&#31995;&#37117;&#22522;&#20110;&#35748;&#30693;&#24515;&#29702;&#23398;&#20013;&#30340;&#38544;&#21947;&#29702;&#35299;&#29702;&#35770;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#31034;&#24212;&#29992;&#20110;GPT-3&#30340;&#20004;&#20010;&#26368;&#22823;&#29256;&#26412;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#37322;&#20041;&#36873;&#25321;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic models of language understanding are valuable tools for investigating human language use. However, they need to be hand-designed for a particular domain. In contrast, large language models (LLMs) are trained on text that spans a wide array of domains, but they lack the structure and interpretability of probabilistic models. In this paper, we use chain-of-thought prompts to introduce structures from probabilistic models into LLMs. We explore this approach in the case of metaphor understanding. Our chain-of-thought prompts lead language models to infer latent variables and reason about their relationships in order to choose appropriate paraphrases for metaphors. The latent variables and relationships chosen are informed by theories of metaphor understanding from cognitive psychology. We apply these prompts to the two largest versions of GPT-3 and show that they can improve performance in a paraphrase selection task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27969;&#34892;&#30149;&#27169;&#25311;&#35774;&#35745;&#8212;&#8212;GradABM&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#26102;&#38388;&#20869;&#24555;&#36895;&#27169;&#25311;&#30334;&#19975;&#32423;&#21035;&#30340;&#20154;&#21475;&#65292;&#24182;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21644;&#25509;&#21463;&#24322;&#26500;&#25968;&#25454;&#28304;&#65292;&#20026;&#26657;&#20934;&#12289;&#39044;&#27979;&#21644;&#35780;&#20272;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;</title><link>http://arxiv.org/abs/2207.09714</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27969;&#34892;&#30149;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Differentiable Agent-based Epidemiology. (arXiv:2207.09714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27969;&#34892;&#30149;&#27169;&#25311;&#35774;&#35745;&#8212;&#8212;GradABM&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#26102;&#38388;&#20869;&#24555;&#36895;&#27169;&#25311;&#30334;&#19975;&#32423;&#21035;&#30340;&#20154;&#21475;&#65292;&#24182;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21644;&#25509;&#21463;&#24322;&#26500;&#25968;&#25454;&#28304;&#65292;&#20026;&#26657;&#20934;&#12289;&#39044;&#27979;&#21644;&#35780;&#20272;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#27169;&#25311;&#22120;&#26159;&#27969;&#34892;&#30149;&#23398;&#25506;&#32034;&#22797;&#26434;&#65292;&#21160;&#24577;&#24863;&#26579;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#24182;&#22312;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#19981;&#21487;&#25110;&#32570;&#24037;&#20855;&#12290; &#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#20223;&#30495;&#33539;&#20363;&#65292;&#21487;&#20197;&#29992;&#32454;&#33410;&#34920;&#31034;&#25509;&#35302;&#20132;&#20114;&#30340;&#24322;&#36136;&#24615;&#21644;&#20010;&#20307;&#34892;&#20026;&#30340;&#20195;&#29702;&#12290; &#28982;&#32780;&#65292;&#20256;&#32479;ABM&#26694;&#26550;&#19981;&#21487;&#24494;&#20998;&#24182;&#19988;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#23384;&#22312;&#25361;&#25112;&#65307;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#36830;&#25509;&#21040;&#36741;&#21161;&#25968;&#25454;&#28304;&#26159;&#19981;&#24179;&#20961;&#30340;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GradABM&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#65292;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#36866;&#21512;&#20110;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#12290; GradABM&#21487;&#20197;&#22312;&#21830;&#21697;&#30828;&#20214;&#19978;&#24555;&#36895;&#27169;&#25311;&#25968;&#30334;&#19975;&#35268;&#27169;&#30340;&#20154;&#21475;&#65292;&#24182;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21644;&#25509;&#21463;&#24322;&#26500;&#25968;&#25454;&#28304;&#12290; &#36825;&#20026;&#26657;&#20934;&#65292;&#39044;&#27979;&#21644;&#35780;&#20272;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#23454;&#38469;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic simulators are an indispensable tool for epidemiology to explore the behavior of complex, dynamic infections under varying conditions and navigate uncertain environments. Agent-based models (ABMs) are an increasingly popular simulation paradigm that can represent the heterogeneity of contact interactions with granular detail and agency of individual behavior. However, conventional ABM frameworks are not differentiable and present challenges in scalability; due to which it is non-trivial to connect them to auxiliary data sources. In this paper, we introduce GradABM: a scalable, differentiable design for agent-based modeling that is amenable to gradient-based learning with automatic differentiation. GradABM can quickly simulate million-size populations in few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous data sources. This provides an array of practical benefits for calibration, forecasting, and evaluating policy interventions. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeepTime&#26694;&#26550;&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#28145;&#24230;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#30340;&#20803;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#19982;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.06046</link><description>&lt;p&gt;
&#23398;&#20064;&#28145;&#24230;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Deep Time-index Models for Time Series Forecasting. (arXiv:2207.06046v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepTime&#26694;&#26550;&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#28145;&#24230;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#30340;&#20803;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#19982;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#23548;&#33268;&#21382;&#21490;&#20215;&#20540;&#27169;&#22411;&#31867;&#21035;&#20013;&#28044;&#29616;&#20986;&#22823;&#37327;&#26032;&#26041;&#27861;&#12290;&#34429;&#28982;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#65292;&#27604;&#22914;&#33021;&#22815;&#23545;&#24213;&#23618;&#26102;&#38388;&#24207;&#21015;&#21160;&#24577;&#24615;&#24314;&#27169;&#65292;&#20294;&#20173;&#26410;&#21463;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepTime&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#28145;&#24230;&#26102;&#38388;&#32034;&#24341;&#27169;&#22411;&#65292;&#22312;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35774;&#32622;&#19979;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#24320;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been actively applied to time series forecasting, leading to a deluge of new methods, belonging to the class of historical-value models. Yet, despite the attractive properties of time-index models, such as being able to model the continuous nature of underlying time series dynamics, little attention has been given to them. Indeed, while naive deep time-index models are far more expressive than the manually predefined function representations of classical time-index models, they are inadequate for forecasting, being unable to generalize to unseen time steps due to the lack of inductive bias. In this paper, we propose DeepTime, a meta-optimization framework to learn deep time-index models which overcome these limitations, yielding an efficient and accurate forecasting model. Extensive experiments on real world datasets in the long sequence time-series forecasting setting demonstrate that our approach achieves competitive results with state-of-the-art methods, and is hig
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#20027;&#21160;&#23547;&#27714;&#24110;&#21161;&#20197;&#35299;&#20915;&#34920;&#36798;&#38656;&#27714;&#24335;&#35270;&#35273;&#23548;&#33322;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#38590;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#32570;&#20047;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#31283;&#20581;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.10606</link><description>&lt;p&gt;
&#22909;&#26102;&#26426;: &#34920;&#36798;&#38656;&#27714;&#24335;&#35270;&#35273;&#23548;&#33322;&#20013;&#21551;&#31034;&#27714;&#21161;&#30340;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Good Time to Ask: A Learning Framework for Asking for Help in Embodied Visual Navigation. (arXiv:2206.10606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#20027;&#21160;&#23547;&#27714;&#24110;&#21161;&#20197;&#35299;&#20915;&#34920;&#36798;&#38656;&#27714;&#24335;&#35270;&#35273;&#23548;&#33322;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#38590;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#32570;&#20047;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#31283;&#20581;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19978;&#65292;&#35810;&#38382;&#27714;&#21161;&#27604;&#22312;&#26410;&#30693;&#20301;&#32622;&#30340;&#31354;&#38388;&#20869;&#25628;&#32034;&#35201;&#26356;&#21152;&#39640;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#22312;&#36825;&#31181;&#24863;&#30693;&#35270;&#35273;&#20219;&#21153;&#20013;&#20027;&#21160;&#22320;&#23547;&#27714;&#24110;&#21161;&#65292;&#20854;&#20013;&#21453;&#39304;&#20449;&#24687;&#21578;&#30693;&#20195;&#29702;&#30446;&#26631;&#22312;&#20854;&#35270;&#37326;&#20013;&#30340;&#20301;&#32622;&#12290;&#20026;&#20102;&#27169;&#25311;&#29616;&#23454;&#24773;&#20917;&#19979;&#32769;&#24072;&#19981;&#24635;&#26159;&#22312;&#22330;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35757;&#32451;&#35838;&#31243;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#21453;&#39304;&#20449;&#24687;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30446;&#26631;&#20301;&#32622;&#65292;&#22312;&#32463;&#39564;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20195;&#29702;&#20381;&#28982;&#20445;&#25345;&#20102;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reality, it is often more efficient to ask for help than to search the entire space to find an object with an unknown location. We present a learning framework that enables an agent to actively ask for help in such embodied visual navigation tasks, where the feedback informs the agent of where the goal is in its view. To emulate the real-world scenario that a teacher may not always be present, we propose a training curriculum where feedback is not always available. We formulate an uncertainty measure of where the goal is and use empirical results to show that through this approach, the agent learns to ask for help effectively while remaining robust when feedback is not available.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#25668;&#20687;&#22836;&#38236;&#22836;&#65292;&#23545;&#35270;&#39057;&#36827;&#34892;&#22788;&#29702;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#24182;&#22312;&#32500;&#25252;&#27963;&#21160;&#35782;&#21035;&#29305;&#24449;&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.03891</link><description>&lt;p&gt;
PrivHAR&#65306;&#20174;&#38544;&#31169;&#20445;&#25252;&#35282;&#24230;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
PrivHAR: Recognizing Human Actions From Privacy-preserving Lens. (arXiv:2206.03891v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#25668;&#20687;&#22836;&#38236;&#22836;&#65292;&#23545;&#35270;&#39057;&#36827;&#34892;&#22788;&#29702;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#24182;&#22312;&#32500;&#25252;&#27963;&#21160;&#35782;&#21035;&#29305;&#24449;&#30340;&#21516;&#26102;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#25668;&#20687;&#26426;&#30340;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#25668;&#20687;&#22836;&#38236;&#22836;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#35270;&#39057;&#36136;&#37327;&#65292;&#20197;&#25233;&#21046;&#38544;&#31169;&#23646;&#24615;&#24182;&#38450;&#27490;&#25932;&#23545;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#27963;&#21160;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#35270;&#35273;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerated use of digital cameras prompts an increasing concern about privacy and security, particularly in applications such as action recognition. In this paper, we propose an optimizing framework to provide robust visual privacy protection along the human action recognition pipeline. Our framework parameterizes the camera lens to successfully degrade the quality of the videos to inhibit privacy attributes and protect against adversarial attacks while maintaining relevant features for activity recognition. We validate our approach with extensive simulations and hardware experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#21270;SVM&#20998;&#31867;&#22120;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32422;&#26463;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#32908;&#30005;&#20449;&#21495;&#30340;&#39640;&#25928;&#25511;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21151;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2204.02179</link><description>&lt;p&gt;
&#22522;&#20110;&#36827;&#21270;&#35745;&#31639;&#30340;&#32908;&#30005;&#25511;&#21046;&#22120;&#30340;&#21151;&#32791;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Towards Power-Efficient Design of Myoelectric Controller based on Evolutionary Computation. (arXiv:2204.02179v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#21270;SVM&#20998;&#31867;&#22120;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32422;&#26463;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#32908;&#30005;&#20449;&#21495;&#30340;&#39640;&#25928;&#25511;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21151;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32908;&#30005;&#27169;&#24335;&#35782;&#21035;&#26159;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;&#19978;&#32930;&#20551;&#32930;&#21644;&#29983;&#29289;&#26426;&#22120;&#20154;&#25163;&#37096;&#36816;&#21160;&#31995;&#32479;&#65289;&#30340;&#25511;&#21046;&#31574;&#30053;&#35774;&#35745;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#20351;&#29992;&#26680;&#21270;SVM&#20998;&#31867;&#22120;&#23545;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;sEMG&#65289;&#20449;&#21495;&#30340;&#20449;&#24687;&#36827;&#34892;&#35299;&#30721;&#26469;&#25512;&#26029;&#28508;&#22312;&#30340;&#32908;&#32905;&#36816;&#21160;&#65292;&#35774;&#35745;&#19968;&#31181;&#33021;&#37327;&#39640;&#25928;&#30340;&#22522;&#20110;EMG&#30340;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#23454;&#29616;EMG&#25511;&#21046;&#22120;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#31867;&#22120;&#35774;&#35745;&#31574;&#30053;&#26159;&#20943;&#23569;&#25972;&#20010;&#31995;&#32479;&#35823;&#21160;&#20316;&#65288;&#24403;EMG&#25511;&#21046;&#22120;&#22788;&#20110;&#8220;&#20241;&#24687;&#8221;&#20301;&#32622;&#26102;&#65289;&#12290;&#20026;&#27492;&#65292;&#19982;&#20256;&#32479;&#30340;&#36719;&#38388;&#38548;&#26680;&#21270;SVM&#21333;&#20010;&#35757;&#32451;&#30446;&#26631;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#30417;&#30563;&#23398;&#20064;&#31995;&#32479;&#30340;&#35757;&#32451;&#31639;&#27861;&#21046;&#23450;&#20026;&#19968;&#33324;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#37319;&#29992;&#31934;&#33521;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;NSGA-II&#26469;&#25214;&#21040;&#21463;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#65292;&#24182;&#26681;&#25454;&#25903;&#25345;&#21521;&#37327;&#30340;&#25968;&#37327;&#21644;&#20998;&#31867;&#20934;&#30830;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#36873;&#25321;&#26368;&#32456;&#20998;&#31867;&#22120;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;sEMG&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21151;&#29575;&#25928;&#29575;&#21644;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Myoelectric pattern recognition is one of the important aspects in the design of the control strategy for various applications including upper-limb prostheses and bio-robotic hand movement systems. The current work has proposed an approach to design an energy-efficient EMG-based controller by considering a supervised learning framework using a kernelized SVM classifier for decoding the information of surface electromyography (sEMG) signals to infer the underlying muscle movements. In order to achieve the optimized performance of the EMG-based controller, our main strategy of classifier design is to reduce the false movements of the overall system (when the EMG-based controller is at the `Rest' position). To this end, unlike the traditional single training objective of soft margin kernelized SVM, we have formulated the training algorithm of the proposed supervised learning system as a general constrained multi-objective optimization problem. An elitist multi-objective evolutionary algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#26354;&#32447;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#19978;&#26356;&#21152;&#31616;&#21333;&#19988;&#23481;&#26131;&#22788;&#29702;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#36710;&#36947;&#26816;&#27979;&#39046;&#22495;&#30340;&#26032;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2203.02431</link><description>&lt;p&gt;
&#26354;&#32447;&#24314;&#27169;&#19979;&#30340;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#37325;&#26032;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Rethinking Efficient Lane Detection via Curve Modeling. (arXiv:2203.02431v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#26354;&#32447;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#19978;&#26356;&#21152;&#31616;&#21333;&#19988;&#23481;&#26131;&#22788;&#29702;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#36710;&#36947;&#26816;&#27979;&#39046;&#22495;&#30340;&#26032;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21442;&#25968;&#26354;&#32447;&#30340;RGB&#22270;&#20687;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#21106;&#21644;&#22522;&#20110;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26354;&#32447;&#26041;&#27861;&#21487;&#20197;&#33258;&#28982;&#22320;&#23398;&#20064;&#25972;&#20307;&#36710;&#36947;&#34920;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#35299;&#30721;&#39044;&#27979;&#25110;&#21046;&#23450;&#22823;&#37327;&#38170;&#28857;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#20026;&#20102;&#22788;&#29702;&#29616;&#26377;&#22810;&#39033;&#24335;&#26354;&#32447;&#26041;&#27861;&#30340;&#20248;&#21270;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21442;&#25968;B&#233;zier&#26354;&#32447;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#35745;&#31639;&#12289;&#31283;&#23450;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#33258;&#30001;&#21464;&#25442;&#24230;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21464;&#24418;&#21367;&#31215;&#30340;&#29305;&#24449;&#32763;&#36716;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#39550;&#39542;&#22330;&#26223;&#20013;&#36710;&#36947;&#30340;&#23545;&#31216;&#24615;&#36136;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;LLAMAS&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;&#23427;&#20063;&#22312;TuSimple&#21644;CULane&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#20302;&#30340;&#24310;&#36831;&#65288;&gt; 150 FPS&#65289;&#21644;&#23567;&#27169;&#22411;&#22823;&#23567;&#65288;&lt;10M&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#26032;&#30340;&#22522;&#20934;&#65292;&#20197;&#22312;&#36335;&#26631;&#26816;&#27979;&#39046;&#22495;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel parametric curve-based method for lane detection in RGB images. Unlike state-of-the-art segmentation-based and point detection-based methods that typically require heuristics to either decode predictions or formulate a large sum of anchors, the curve-based methods can learn holistic lane representations naturally. To handle the optimization difficulties of existing polynomial curve methods, we propose to exploit the parametric B\'ezier curve due to its ease of computation, stability, and high freedom degrees of transformations. In addition, we propose the deformable convolution-based feature flip fusion, for exploiting the symmetry properties of lanes in driving scenes. The proposed method achieves a new state-of-the-art performance on the popular LLAMAS benchmark. It also achieves favorable accuracy on the TuSimple and CULane datasets, while retaining both low latency (&gt; 150 FPS) and small model size (&lt; 10M). Our method can serve as a new baseline, to shed 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.13001</link><description>&lt;p&gt;
&#28145;&#24230;&#21028;&#21035;&#21040;&#26680;&#29983;&#25104;&#32593;&#32476;&#30340;&#23450;&#26631;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Discriminative to Kernel Generative Networks for Calibrated Inference. (arXiv:2201.13001v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#19982;&#29983;&#25104;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#26234;&#33021;&#30340;&#30740;&#31350;&#20013;&#37117;&#26377;&#20854;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20108;&#32773;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#26680;&#29983;&#25104;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#27169;&#22411;&#35270;&#20026;&#24191;&#20041;&#30340;&#21010;&#20998;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#30001;&#35757;&#32451;&#25968;&#25454;&#26500;&#25104;&#30340;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#65292;&#26469;&#33719;&#24471;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fight between discriminative versus generative goes deep, in both the study of artificial and natural intelligence. In our view, both camps have complementary values. So, we sought to synergistically combine them. Here, we propose a methodology to convert deep discriminative networks to kernel generative networks. We leveraged the fact that deep models, including both random forests and deep networks, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as generalized partitioning rules. We replace the affine function in each polytope populated by the training data with Gaussian kernel that results in a generative model. Theoretically, we derive the conditions under which our generative models are a consistent estimator of the corresponding class conditional density. Moreover, our proposed models obtain well calibrated posteriors for in-distribution, and extrapolate beyond the training data to handle out-of-distrib
&lt;/p&gt;</description></item><item><title>PoNet&#26159;&#19968;&#31181;&#27744;&#21270;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#38271;&#24207;&#21015;&#20013;&#30340;token&#28151;&#21512;&#65292;&#20854;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#27604;Transformer&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#27880;&#24847;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2110.02442</link><description>&lt;p&gt;
PoNet&#65306;&#38271;&#24207;&#21015;&#20013;&#39640;&#25928;Token&#28151;&#21512;&#30340;&#27744;&#21270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02442
&lt;/p&gt;
&lt;p&gt;
PoNet&#26159;&#19968;&#31181;&#27744;&#21270;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#38271;&#24207;&#21015;&#20013;&#30340;token&#28151;&#21512;&#65292;&#20854;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#27604;Transformer&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#27880;&#24847;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;NLP&#12289;&#35270;&#35273;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;Transformer&#30340;&#26680;&#24515;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#24179;&#26041;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#38459;&#30861;&#20102;Transformer-based&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#20363;&#22914;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#12289;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#21644;&#21487;&#25193;&#23637;&#30340;&#26680;&#20989;&#25968;&#65292;&#20197;&#21450;&#26367;&#20195;&#33258;&#27880;&#24847;&#30340;token&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#24207;&#21015;&#20013;token&#28151;&#21512;&#30340;&#26032;&#22411;&#27744;&#21270;&#32593;&#32476;(PoNet)&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;&#32447;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31890;&#24230;&#27744;&#21270;&#21644;&#27744;&#21270;&#34701;&#21512;&#26469;&#25429;&#25417;&#19981;&#21516;&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#19982;token&#30340;&#20132;&#20114;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#38271;&#24207;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;PoNet&#26174;&#33879;&#20248;&#20110;Transformer&#65292;&#24182;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#22312;&#25152;&#26377;&#22312;GPU&#19978;&#27979;&#37327;&#30340;&#24207;&#21015;&#38271;&#24230;&#19978;&#65292;&#23427;&#20165;&#27604;&#26368;&#24555;&#30340;&#27169;&#22411;FNet&#31245;&#24930;&#12290;&#25105;&#20204;&#36824;&#33021;&#21487;&#35270;&#21270;PoNet&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#20197;&#23637;&#31034;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#28151;&#21512;&#20855;&#26377;&#19981;&#21516;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;token&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#22788;&#29702;&#38271;&#24207;&#21015;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#27880;&#24847;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#32508;&#36848;&#20102;NeSy&#21644;StarAI&#20004;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20849;&#26377;&#19971;&#20010;&#32500;&#24230;&#29992;&#20110;&#23545;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2108.11451</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#20851;&#31995;&#21040;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Statistical Relational to Neural Symbolic Artificial Intelligence: a Survey. (arXiv:2108.11451v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#32508;&#36848;&#20102;NeSy&#21644;StarAI&#20004;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20849;&#26377;&#19971;&#20010;&#32500;&#24230;&#29992;&#20110;&#23545;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#20004;&#20010;&#19981;&#21516;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#38598;&#25104;&#26041;&#27861;&#65306;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#65288;NeSy&#65289;&#21644;&#32479;&#35745;&#20851;&#31995;&#20154;&#24037;&#26234;&#33021;&#65288;StarAI&#65289;&#12290;NeSy&#26088;&#22312;&#23558;&#31526;&#21495;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#32780;StarAI&#21017;&#19987;&#27880;&#20110;&#23558;&#36923;&#36753;&#19982;&#27010;&#29575;&#22270;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#35813;&#35843;&#26597;&#20851;&#27880;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#19971;&#20010;&#20849;&#21516;&#32500;&#24230;&#12290;&#36825;&#20123;&#32500;&#24230;&#29992;&#20110;&#23545;&#20004;&#31181;&#39046;&#22495;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;&#27169;&#22411;&#36824;&#26159;&#22522;&#20110;&#35777;&#26126;&#65307;&#65288;2&#65289;&#36923;&#36753;&#29702;&#35770;&#30340;&#35821;&#27861;&#65307;&#65288;3&#65289;&#31995;&#32479;&#30340;&#36923;&#36753;&#35821;&#20041;&#21450;&#20854;&#25193;&#23637;&#20197;&#20419;&#36827;&#23398;&#20064;&#65307;&#65288;4&#65289;&#23398;&#20064;&#30340;&#33539;&#22260;&#65292;&#21253;&#25324;&#20165;&#28041;&#21450;&#21442;&#25968;&#36824;&#26159;&#28041;&#21450;&#25972;&#20010;&#36923;&#36753;&#29702;&#35770;&#65307;&#65288;5&#65289;&#34920;&#31034;&#27861;&#20013;&#31526;&#21495;&#21644;&#23376;&#31526;&#21495;&#25104;&#20998;&#30340;&#23384;&#22312;&#65307;&#65288;6&#65289;&#31995;&#32479;&#25429;&#25417;&#21407;&#22987;&#36923;&#36753;&#12289;&#27010;&#29575;&#21644;&#31070;&#32463;&#33539;&#20363;&#30340;&#31243;&#24230;&#65307;&#21644;&#65288;7&#65289;&#20219;&#21153;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neural-symbolic computation (NeSy) and statistical relational artificial intelligence (StarAI). NeSy aims to integrate symbolic reasoning and neural networks while StarAI focuses on integrating logic with probabilistic graphical models. The survey brings attention to seven shared dimensions between the two approaches. These dimensions are employed to categorize both fields and include: (1) the approach to logic inference, whether model or proof-based; (2) the syntax of logical theories; (3) the logic semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either the parameters alone or the entire logic theory; (5) the presence of symbolic and subsymbolic components in representations; (6) the degree to which the systems can capture the original logic, probabilistic, and neural paradigms; and (7) the classes of tasks the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2106.16046</link><description>&lt;p&gt;
&#25506;&#32034;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65306;&#22522;&#20934;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Exploring the Context Generalizability in Spatiotemporal Crowd Flow Prediction: Benchmark and Guideline. (arXiv:2106.16046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.16046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#29305;&#24449;&#26159;&#26500;&#24314;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#65288;STCFP&#65289;&#27169;&#22411;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#22312;&#20110;&#19981;&#21516;&#22330;&#26223;&#20013;&#19978;&#19979;&#25991;&#29305;&#24449;&#65288;&#20363;&#22914;&#22825;&#27668;&#12289;&#20551;&#26085;&#21644;&#20852;&#36259;&#28857;&#65289;&#21644;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#26410;&#30693;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#30001;&#22823;&#35268;&#27169;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#25968;&#25454;&#12289;&#19978;&#19979;&#25991;&#25968;&#25454;&#21644;&#26368;&#20808;&#36827;&#30340;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#32452;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22478;&#24066;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#23450;&#37327;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#29305;&#24449;&#21644;&#24314;&#27169;&#25216;&#26415;&#30340;&#27867;&#21270;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#27969;&#34892;&#30740;&#31350;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#24320;&#21457;&#20102;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25968;&#30334;&#19975;&#26465;&#35760;&#24405;&#21644;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#25968;&#30334;&#31181;&#27169;&#22411;&#20197;&#25429;&#25417;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;STCFP&#20013;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual features are important data sources for building spatiotemporal crowd flow prediction (STCFP) models. However, the difficulty of applying context lies in the unknown generalizability of both contextual features (e.g., weather, holiday, and points of interests) and context modeling techniques across different scenarios. In this paper, we build a benchmark composed of large-scale spatiotemporal crowd flow data, contextual data, and state-of-the-art spatiotemporal prediction models. We conduct a comprehensive experimental study to quantitatively investigate the generalizability of different contextual features and modeling techniques in several urban crowd flow prediction scenarios (including bike flow, metro passenger flow, electric vehicle charging demand and so on). In particular, we develop a general taxonomy of context modeling techniques based on extensive investigations in prevailing research. With millions of records and rich context data, we have trained and tested hun
&lt;/p&gt;</description></item><item><title>LARC&#26159;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20219;&#21153;&#35299;&#20915;&#35828;&#26126;&#65292;&#21487;&#29992;&#20110;&#27979;&#35797;&#20195;&#29702;&#31243;&#24207;&#35299;&#20915;&#26032;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#20998;&#26512;&#33258;&#28982;&#31243;&#24207;&#30340;&#29420;&#29305;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2106.07824</link><description>&lt;p&gt;
&#21521;&#20154;&#31867;&#21644;&#26426;&#22120;&#20256;&#36798;&#33258;&#28982;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Communicating Natural Programs to Humans and Machines. (arXiv:2106.07824v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07824
&lt;/p&gt;
&lt;p&gt;
LARC&#26159;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20219;&#21153;&#35299;&#20915;&#35828;&#26126;&#65292;&#21487;&#29992;&#20110;&#27979;&#35797;&#20195;&#29702;&#31243;&#24207;&#35299;&#20915;&#26032;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#20998;&#26512;&#33258;&#28982;&#31243;&#24207;&#30340;&#29420;&#29305;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#26159;&#19968;&#32452;&#31243;&#24207;&#20219;&#21153;&#65292;&#29992;&#20110;&#27979;&#35797;&#20195;&#29702;&#31243;&#24207;&#35299;&#20915;&#26032;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;ARC&#20219;&#21153;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#65292;&#20294;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#21364;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24314;&#31435;&#33021;&#22815;&#25512;&#24191;&#21040;&#35832;&#22914;ARC&#31561;&#26032;&#24773;&#20917;&#30340;&#26234;&#33021;&#31995;&#32479;&#30340;&#38590;&#28857;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#35748;&#20026;&#65292;&#31572;&#26696;&#21487;&#33021;&#22312;&#20110;&#30740;&#31350;\emph{&#35821;&#35328;}&#30340;&#24046;&#24322;&#65306;&#34429;&#28982;&#20154;&#31867;&#23481;&#26131;&#29983;&#25104;&#21644;&#35299;&#37322;&#36890;&#29992;&#35821;&#35328;&#30340;&#25351;&#20196;&#65292;&#20294;&#35745;&#31639;&#26426;&#31995;&#32479;&#21364;&#34987;&#26463;&#32538;&#22312;&#29421;&#31364;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#20013;&#65292;&#21482;&#33021;&#31934;&#30830;&#25191;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LARC&#65288;Language-complete ARC&#65289;&#65306;&#19968;&#32452;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20219;&#21153;&#35299;&#20915;&#35828;&#26126;&#65292;&#30001;&#19968;&#32452;&#21442;&#19982;&#32773;&#20351;&#29992;&#35821;&#35328;&#29420;&#31435;&#25351;&#23548;&#24444;&#27492;&#22914;&#20309;&#35299;&#20915;ARC&#20219;&#21153;&#65292;&#20854;&#20013;&#25104;&#21151;&#30340;&#35828;&#26126;&#21344;88&#65285;&#30340;ARC&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#25910;&#38598;&#21040;&#30340;&#35828;&#26126;&#20998;&#26512;&#20026;&#8220;&#33258;&#28982;&#31243;&#24207;&#8221;&#65292;&#21457;&#29616;&#23427;&#20204;&#34429;&#28982;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#31243;&#24207;&#65292;&#20294;&#22312;&#20004;&#20010;&#26041;&#38754;&#26159;&#29420;&#29305;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of \emph{language}: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the \textit{Language-complete ARC}: a collection of natural language descriptions by a group of human participants who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in tw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#31070;&#32463;&#20803;&#20013;&#19982;/&#25110;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#23545;&#20998;&#31867;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22686;&#21152;&#31867;AND&#31070;&#32463;&#20803;&#22312;&#32593;&#32476;&#20013;&#27604;&#20363;&#30340;&#25514;&#26045;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2102.07389</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#20803;&#20013;&#30340;&#19982;/&#25110;&#26435;&#34913;&#65306;&#23545;&#25239;&#40065;&#26834;&#24615;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
And/or trade-off in artificial neurons: impact on adversarial robustness. (arXiv:2102.07389v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#31070;&#32463;&#20803;&#20013;&#19982;/&#25110;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#23545;&#20998;&#31867;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22686;&#21152;&#31867;AND&#31070;&#32463;&#20803;&#22312;&#32593;&#32476;&#20013;&#27604;&#20363;&#30340;&#25514;&#26045;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20998;&#31867;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;&#20154;&#24037;&#31070;&#32463;&#20803;&#20013;&#23454;&#29616;&#30340;&#20989;&#25968;&#36830;&#32493;&#24615;&#65292;&#20174;&#32431;AND&#38376;&#21040;&#32431;OR&#38376;&#30340;&#33539;&#22260;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#32593;&#32476;&#20013;&#23384;&#22312;&#36275;&#22815;&#25968;&#37327;&#30340;&#31867;OR&#30340;&#31070;&#32463;&#20803;&#20250;&#23548;&#33268;&#20998;&#31867;&#30340;&#33030;&#24369;&#24615;&#21644;&#22686;&#21152;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#31867;AND&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#21152;&#23427;&#20204;&#22312;&#32593;&#32476;&#20013;&#27604;&#20363;&#30340;&#25514;&#26045;&#12290;&#36825;&#20123;&#25514;&#26045;&#28041;&#21450;&#23558;&#36755;&#20837;&#25918;&#32553;&#21040;[-1,1]&#21306;&#38388;&#24182;&#20943;&#23569;S&#22411;&#28608;&#27963;&#20989;&#25968;&#38497;&#23789;&#37096;&#20998;&#30340;&#28857;&#25968;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#27604;&#36739;&#24403;&#31070;&#32463;&#20803;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#21644;&#31216;&#20026;&#8220;&#20081;&#24207;&#25968;&#25454;&#38598;&#8221;&#30340;&#38543;&#26426;&#29256;&#26412;&#36755;&#20837;&#26102;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#26377;&#21069;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of neural networks, the issue of classification robustness remains, particularly highlighted by adversarial examples. In this paper, we address this challenge by focusing on the continuum of functions implemented in artificial neurons, ranging from pure AND gates to pure OR gates. Our hypothesis is that the presence of a sufficient number of OR-like neurons in a network can lead to classification brittleness and increased vulnerability to adversarial attacks. We define AND-like neurons and propose measures to increase their proportion in the network. These measures involve rescaling inputs to the [-1,1] interval and reducing the number of points in the steepest section of the sigmoidal activation function. A crucial component of our method is the comparison between a neuron's output distribution when fed with the actual dataset and a randomised version called the "scrambled dataset." Experimental results on the MNIST dataset suggest that our approach holds promise a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26694;&#26550;&#65292;&#20801;&#35768;&#20154;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#21021;&#22987;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#35789;&#27719;&#20915;&#31574;&#26641;&#65292;&#20026;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2101.07140</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;RL&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Natural Language Specification of Reinforcement Learning Policies through Differentiable Decision Trees. (arXiv:2101.07140v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.07140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26694;&#26550;&#65292;&#20801;&#35768;&#20154;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#21021;&#22987;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#35789;&#27719;&#20915;&#31574;&#26641;&#65292;&#20026;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#35268;&#33539;&#36807;&#31243;&#65292;&#21487;&#20197;&#20351;&#20154;&#31867;&#19982;&#26426;&#22120;&#20154;&#20849;&#21516;&#21551;&#21160;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#36807;&#31243;&#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#25919;&#31574;&#35268;&#33539;&#19982;&#25919;&#31574;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#20801;&#35768;&#20154;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#21021;&#22987;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#35789;&#27719;&#20915;&#31574;&#26641;&#26469;&#21551;&#21160;&#21644;&#35299;&#37322;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-AI policy specification is a novel procedure we define in which humans can collaboratively warm-start a robot's reinforcement learning policy. This procedure is comprised of two steps; (1) Policy Specification, i.e. humans specifying the behavior they would like their companion robot to accomplish, and (2) Policy Optimization, i.e. the robot applying reinforcement learning to improve the initial policy. Existing approaches to enabling collaborative policy specification are often unintelligible black-box methods, and are not catered towards making the autonomous system accessible to a novice end-user. In this paper, we develop a novel collaborative framework to allow humans to initialize and interpret an autonomous agent's behavior. Through our framework, we enable humans to specify an initial behavior model via unstructured, natural language (NL), which we convert to lexical decision trees. Next, we leverage these translated specifications, to warm-start reinforcement learning an
&lt;/p&gt;</description></item></channel></rss>