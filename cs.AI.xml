<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20840;&#29699;&#20116;&#20010;&#25968;&#25454;&#38598;&#20013;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#33016;&#29255;&#35786;&#26029;&#20013;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#19982;&#33891;&#20107;&#20250;&#35748;&#35777;&#30340;&#25918;&#23556;&#31185;&#21307;&#24072;&#30456;&#27604;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#35786;&#26029;&#36793;&#32536;&#21270;&#32676;&#20307;&#26102;&#19968;&#36143;&#23384;&#22312;&#20302;&#35786;&#26029;&#29575;&#65292;&#29978;&#33267;&#22312;&#35832;&#22914;&#40657;&#20154;&#22899;&#24615;&#20043;&#31867;&#30340;&#20132;&#21449;&#20122;&#32452;&#20013;&#30475;&#21040;&#26356;&#39640;&#30340;&#27604;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.14815</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#19987;&#23478;&#32423;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20840;&#29699;&#20116;&#20010;&#25968;&#25454;&#38598;&#20013;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#33016;&#29255;&#35786;&#26029;&#20013;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#19982;&#33891;&#20107;&#20250;&#35748;&#35777;&#30340;&#25918;&#23556;&#31185;&#21307;&#24072;&#30456;&#27604;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#35786;&#26029;&#36793;&#32536;&#21270;&#32676;&#20307;&#26102;&#19968;&#36143;&#23384;&#22312;&#20302;&#35786;&#26029;&#29575;&#65292;&#29978;&#33267;&#22312;&#35832;&#22914;&#40657;&#20154;&#22899;&#24615;&#20043;&#31867;&#30340;&#20132;&#21449;&#20122;&#32452;&#20013;&#30475;&#21040;&#26356;&#39640;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24050;&#32463;&#22312;&#21307;&#23398;&#24433;&#20687;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#33258;&#30417;&#30563;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#26126;&#30830;&#22521;&#35757;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24191;&#27867;&#30340;&#30149;&#29702;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19981;&#21453;&#26144;&#25110;&#25918;&#22823;&#20154;&#31867;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#20351;&#22899;&#24615;&#25110;&#40657;&#20154;&#24739;&#32773;&#31561;&#21382;&#21490;&#19978;&#34987;&#36793;&#32536;&#21270;&#30340;&#32676;&#20307;&#22788;&#20110;&#19981;&#21033;&#22320;&#20301;&#12290;&#36825;&#31181;&#20559;&#35265;&#30340;&#20307;&#29616;&#21487;&#33021;&#20250;&#31995;&#32479;&#24615;&#22320;&#24310;&#36831;&#29305;&#23450;&#24739;&#32773;&#20122;&#32452;&#30340;&#37325;&#35201;&#21307;&#30103;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14815v1 Announce Type: cross  Abstract: Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black fem
&lt;/p&gt;</description></item><item><title>WeakSAM&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#23398;&#20064;&#30340;&#20840;&#29699;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;PGT&#29983;&#25104;&#21644;RoI&#20002;&#24323;&#27491;&#21017;&#21270;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14812</link><description>&lt;p&gt;
WeakSAM: &#20219;&#24847;&#20998;&#21106;&#36935;&#19978;&#24369;&#30417;&#30563;&#23454;&#20363;&#32423;&#21035;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14812
&lt;/p&gt;
&lt;p&gt;
WeakSAM&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#23398;&#20064;&#30340;&#20840;&#29699;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;PGT&#29983;&#25104;&#21644;RoI&#20002;&#24323;&#27491;&#21017;&#21270;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#35270;&#35273;&#35782;&#21035;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#30417;&#30563;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#23427;&#26174;&#33879;&#38477;&#20302;&#20102;&#20154;&#24037;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#19988;&#20256;&#32479;&#19978;&#20381;&#36182;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;WeakSAM&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;&#22312;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#39044;&#20808;&#23398;&#20064;&#30340;&#20840;&#29699;&#30693;&#35782;&#65292;&#21363;Segment Anything Model (SAM)&#65292;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#29289;&#20307;&#26816;&#27979;&#65288;WSOD&#65289;&#21644;&#20998;&#21106;&#12290;WeakSAM&#36890;&#36807;&#33258;&#36866;&#24212;PGT&#29983;&#25104;&#21644;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;RoI&#65289;&#20002;&#24323;&#27491;&#21017;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;WSOD&#37325;&#26032;&#35757;&#32451;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#21363;&#20266;&#26631;&#20934;&#22320;&#38754;&#30495;&#30456;&#65288;PGT&#65289;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#20855;&#26377;&#22024;&#26434;PGT&#23454;&#20363;&#12290;&#23427;&#36824;&#35299;&#20915;&#20102;SAM&#22312;&#33258;&#21160;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#26102;&#38656;&#35201;&#25552;&#31034;&#21644;&#31867;&#21035;&#26080;&#24863;&#30693;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;WeakSAM&#22312;WSOD&#21644;WSIS&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14812v1 Announce Type: cross  Abstract: Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with larg
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;GeneOH&#25193;&#25955;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#21435;&#22122;&#65292;&#20854;&#20013;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#25509;&#35302;&#30340;HOI&#34920;&#31034;&#21644;&#39046;&#22495;&#36890;&#29992;&#30340;&#21435;&#22122;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.14810</link><description>&lt;p&gt;
GeneOH&#25193;&#25955;: &#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14810
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;GeneOH&#25193;&#25955;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#21435;&#22122;&#65292;&#20854;&#20013;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#25509;&#35302;&#30340;HOI&#34920;&#31034;&#21644;&#39046;&#22495;&#36890;&#29992;&#30340;&#21435;&#22122;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21435;&#22122;&#25163;-&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;&#19968;&#20010;&#38169;&#35823;&#30340;&#20132;&#20114;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#23545;&#19981;&#27491;&#30830;&#30340;&#25163;&#30340;&#36712;&#36857;&#36827;&#34892;&#32454;&#21270;&#65292;&#20197;&#28040;&#38500;&#20132;&#20114;&#20266;&#24433;&#65292;&#33719;&#24471;&#19968;&#20010;&#24863;&#30693;&#19978;&#30495;&#23454;&#30340;&#24207;&#21015;&#12290;&#36825;&#19968;&#25361;&#25112;&#28041;&#21450;&#22797;&#26434;&#30340;&#20132;&#20114;&#22122;&#22768;&#65292;&#21253;&#25324;&#19981;&#33258;&#28982;&#30340;&#25163;&#37096;&#23039;&#21183;&#21644;&#19981;&#27491;&#30830;&#30340;&#25163;-&#29289;&#20307;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#26032;&#20132;&#20114;&#21644;&#19981;&#21516;&#22122;&#22768;&#27169;&#24335;&#30340;&#31283;&#20581;&#27867;&#21270;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;GeneOH Diffusion&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;:&#19968;&#31181;&#21517;&#20026;GeneOH&#30340;&#21019;&#26032;&#30340;&#22522;&#20110;&#25509;&#35302;&#30340;HOI&#34920;&#31034;&#21644;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#36890;&#29992;&#30340;&#21435;&#22122;&#26041;&#26696;&#12290;&#22522;&#20110;&#25509;&#35302;&#30340;&#34920;&#31034;GeneOH&#23545;HOI&#36807;&#31243;&#36827;&#34892;&#20449;&#24687;&#21270;&#21442;&#25968;&#21270;&#65292;&#20419;&#36827;&#22312;&#21508;&#31181;HOI&#22330;&#26223;&#20013;&#23454;&#29616;&#22686;&#24378;&#30340;&#27867;&#21270;&#12290;&#26032;&#30340;&#21435;&#22122;&#26041;&#26696;&#21253;&#25324;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#29992;&#20110;&#25237;&#24433;&#22024;&#26434;&#25968;&#25454;&#26679;&#26412;&#30340;&#32463;&#20856;&#21435;&#22122;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14810v1 Announce Type: cross  Abstract: In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data sample
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.14807</link><description>&lt;p&gt;
&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#20013;&#21160;&#24577;&#19981;&#23433;&#38745;&#22810;&#33218;&#32769;&#34382;&#26426;&#20219;&#21153;&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65288;DLM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#38477;&#20302;&#23381;&#20135;&#22919;&#27515;&#20129;&#29575;&#30340;&#21162;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39044;&#38450;&#20445;&#20581;&#35745;&#21010;&#65292;&#21521;&#39640;&#39118;&#38505;&#20154;&#32676;&#20256;&#25773;&#37325;&#35201;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DLM&#65306;&#19968;&#31181;&#29992;&#20110;RMAB&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14807v1 Announce Type: cross  Abstract: Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret hu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#29305;&#24449;</title><link>https://arxiv.org/abs/2402.14805</link><description>&lt;p&gt;
&#36890;&#36807;&#22806;&#37096;&#35780;&#20272;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#22810;&#37325;&#20154;&#26684;
&lt;/p&gt;
&lt;p&gt;
Identifying Multiple Personalities in Large Language Models with External Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#19982;&#20154;&#31867;&#26085;&#24120;&#24212;&#29992;&#25972;&#21512;&#65292;&#20851;&#20110;LLMs&#34892;&#20026;&#30340;&#35768;&#22810;&#31038;&#20250;&#21644;&#20262;&#29702;&#20851;&#20999;&#34987;&#25552;&#20986;&#12290;&#20102;&#35299;LLMs&#34892;&#20026;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#20998;&#26512;&#23427;&#20204;&#30340;&#20154;&#26684;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20026;&#20154;&#31867;&#21019;&#24314;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#37327;&#21270;LLMs&#30340;&#20154;&#26684;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25209;&#35780;&#36136;&#30097;&#23558;&#36825;&#20123;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#24212;&#29992;&#20110;LLMs&#26102;&#30340;&#36866;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26367;&#20195;&#30340;&#20154;&#26684;&#27979;&#37327;&#26041;&#27861;&#26469;&#30740;&#31350;LLMs&#30340;&#20154;&#26684;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#65292;&#36825;&#37324;&#25105;&#20204;&#19981;&#26159;&#36890;&#36807;&#22312;&#26446;&#20811;&#29305;&#37327;&#34920;&#19978;&#25552;&#31034;LLMs&#22238;&#31572;&#22810;&#36873;&#39064;&#65292;&#32780;&#26159;&#36890;&#36807;&#20998;&#26512;LLMs&#23545;&#22806;&#37096;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#30340;&#24320;&#25918;&#24335;&#24773;&#22659;&#38382;&#39064;&#30340;&#22238;&#31572;&#26469;&#35780;&#20272;LLMs&#30340;&#20154;&#26684;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;Llama2-7B&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20316;&#20026;MBTI&#20154;&#26684;&#39044;&#27979;&#22120;&#65292;&#35813;&#39044;&#27979;&#22120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14805v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs. One of the ways to comprehend LLMs' behavior is to analyze their personalities. Many recent studies quantify LLMs' personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs. In this paper, we investigate LLM personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting LLMs with multiple-choice questions in the Likert scale, we evaluate LLMs' personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor that outperforms the state-of-the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATH-Vision&#65288;MATH-V&#65289;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#24403;&#21069;LMMs&#21644;&#20154;&#31867;&#22312;MATH-V&#19978;&#30340;&#34920;&#29616;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.14804</link><description>&lt;p&gt;
&#20351;&#29992;MATH-Vision&#25968;&#25454;&#38598;&#27979;&#37327;&#22810;&#27169;&#24577;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATH-Vision&#65288;MATH-V&#65289;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#24403;&#21069;LMMs&#21644;&#20154;&#31867;&#22312;MATH-V&#19978;&#30340;&#34920;&#29616;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#35270;&#35273;&#32972;&#26223;&#19979;&#30340;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;MathVista&#65289;&#19978;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#22312;&#38382;&#39064;&#22810;&#26679;&#24615;&#21644;&#28085;&#30422;&#23398;&#31185;&#33539;&#22260;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MATH-Vision&#65288;MATH-V&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25910;&#38598;&#20102;&#26469;&#33258;&#30495;&#23454;&#25968;&#23398;&#31454;&#36187;&#30340;3,040&#20010;&#39640;&#36136;&#37327;&#25968;&#23398;&#38382;&#39064;&#21644;&#35270;&#35273;&#32972;&#26223;&#30340;&#25968;&#25454;&#38598;&#12290;&#36328;&#36234;16&#20010;&#19981;&#21516;&#30340;&#25968;&#23398;&#23398;&#31185;&#65292;&#20998;&#20026;5&#20010;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20998;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20026;&#35780;&#20272;LMMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#19988;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24403;&#21069;LMMs&#19982;MATH-V&#19978;&#20154;&#31867;&#34920;&#29616;&#20043;&#38388;&#30340;&#26174;&#33879;&#34920;&#29616;&#24046;&#36317;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#25512;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14804v1 Announce Type: cross  Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements i
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14800</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#19987;&#23478;&#37117;&#30456;&#31561;: &#28151;&#21512;&#19987;&#23478;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;
&lt;/p&gt;
&lt;p&gt;
Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14800
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#23637;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#26159;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;LLMs&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;LLMs&#30456;&#27604;&#65292;MoE LLMs&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#65292;&#20173;&#28982;&#24456;&#38590;&#37096;&#32626;&#23427;&#20204;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#19987;&#38376;&#35774;&#35745;&#30340;&#30828;&#20214;&#30340;&#26435;&#37325;&#21098;&#26525;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#20027;&#35201;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#21363;&#25554;&#21363;&#29992;&#30340;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#26469;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;MoE LLMs&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#22686;&#21152;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#39281;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.14798</link><description>&lt;p&gt;
&#21033;&#29992;&#38750;&#27491;&#24335;&#36923;&#36753;&#22686;&#24378;&#31995;&#32479;&#21270;&#20998;&#35299;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20026;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20363;&#22914;&#22312;&#19981;&#20381;&#36182;&#33030;&#24369;&#30340;&#24418;&#24335;&#36923;&#36753;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21644;&#35780;&#20272;&#30452;&#35266;&#30340;&#12289;&#31867;&#20284;&#35777;&#26126;&#30340;&#25991;&#26412;&#34164;&#28085;&#26641;&#12290;&#28982;&#32780;&#65292;&#27839;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#36827;&#23637;&#21463;&#21040;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#32570;&#20047;&#26126;&#30830;&#30340;&#30830;&#23450;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#28165;&#26224;&#21327;&#35758;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25968;&#25454;&#38598;RDTE (Recognizing Decompositional Textual Entailment) &#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#27604;&#20808;&#21069;&#30340;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#39640;&#24471;&#22810;&#65288;+9%&#65289;&#65292;&#34920;&#26126;RDTE&#22312;&#38271;&#26399;&#23384;&#22312;&#30340;&#20851;&#20110;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14798v1 Announce Type: cross  Abstract: Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of for
&lt;/p&gt;</description></item><item><title>Snap Video&#26159;&#19968;&#20010;&#35270;&#39057;&#20248;&#20808;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#23637;EDM&#26694;&#26550;&#24182;&#25552;&#20986;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;&#20013;&#30340;&#21160;&#24577;&#20445;&#30495;&#24230;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14797</link><description>&lt;p&gt;
Snap Video: &#35268;&#27169;&#21270;&#26102;&#31354;Transformer&#29992;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14797
&lt;/p&gt;
&lt;p&gt;
Snap Video&#26159;&#19968;&#20010;&#35270;&#39057;&#20248;&#20808;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#23637;EDM&#26694;&#26550;&#24182;&#25552;&#20986;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;&#20013;&#30340;&#21160;&#24577;&#20445;&#30495;&#24230;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#29983;&#25104;&#22270;&#20687;&#30340;&#27169;&#22411;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#36136;&#37327;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#21463;&#21040;&#36825;&#20123;&#20248;&#21183;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#22242;&#20307;&#37325;&#26032;&#36816;&#29992;&#23427;&#20204;&#26469;&#29983;&#25104;&#35270;&#39057;&#12290;&#30001;&#20110;&#35270;&#39057;&#20869;&#23481;&#39640;&#24230;&#20887;&#20313;&#65292;&#25105;&#20204;&#35748;&#20026;&#21333;&#32431;&#22320;&#23558;&#22270;&#20687;&#27169;&#22411;&#30340;&#36827;&#23637;&#24102;&#21040;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#20250;&#38477;&#20302;&#21160;&#24577;&#20445;&#30495;&#24230;&#12289;&#35270;&#35273;&#36136;&#37327;&#24182;&#24433;&#21709;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Snap Video&#65292;&#19968;&#20010;&#20197;&#35270;&#39057;&#20026;&#20808;&#30340;&#27169;&#22411;&#65292;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25193;&#23637;EDM&#26694;&#26550;&#20197;&#32771;&#34385;&#26102;&#31354;&#20887;&#20313;&#20687;&#32032;&#24182;&#33258;&#28982;&#22320;&#25903;&#25345;&#35270;&#39057;&#29983;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;U-Net - &#22270;&#20687;&#29983;&#25104;&#32972;&#21518;&#30340;&#24471;&#21147;&#24037;&#20855; - &#22312;&#29983;&#25104;&#35270;&#39057;&#26102;&#25193;&#23637;&#24615;&#36739;&#24046;&#65292;&#38656;&#35201;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;U-Nets&#24555;3.31&#20493;&#65288;&#25512;&#29702;&#36895;&#24230;&#32422;&#24555;4.5&#20493;&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14797v1 Announce Type: cross  Abstract: Contemporary models for generating images show remarkable quality and versatility. Swayed by these advantages, the research community repurposes them to generate videos. Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability. In this work, we build Snap Video, a video-first model that systematically addresses these challenges. To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation. Second, we show that a U-Net - a workhorse behind image generation - scales poorly when generating videos, requiring significant computational overhead. Hence, we propose a new transformer-based architecture that trains 3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us to efficiently train a text-to-vid
&lt;/p&gt;</description></item><item><title>&#33258;&#23548;&#33945;&#38754;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SMA&#65289;&#26159;&#19968;&#31181;&#23436;&#20840;&#39046;&#22495;&#26080;&#20851;&#30340;&#33945;&#38754;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33945;&#38754;&#37319;&#26679;&#32780;&#19981;&#20570;&#20219;&#20309;&#39046;&#22495;&#29305;&#23450;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14789</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#23548;&#33945;&#38754;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14789
&lt;/p&gt;
&lt;p&gt;
&#33258;&#23548;&#33945;&#38754;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SMA&#65289;&#26159;&#19968;&#31181;&#23436;&#20840;&#39046;&#22495;&#26080;&#20851;&#30340;&#33945;&#38754;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33945;&#38754;&#37319;&#26679;&#32780;&#19981;&#20570;&#20219;&#20309;&#39046;&#22495;&#29305;&#23450;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#30340;&#20855;&#20307;&#32454;&#33410;&#26159;&#38024;&#23545;&#27599;&#20010;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#27604;&#22914;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#24378;&#21453;&#26144;&#20102;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#19981;&#21464;&#24615;&#12290; &#32780;&#33945;&#38754;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#24456;&#26377;&#21069;&#36884;&#65292;&#22240;&#20026;&#23427;&#19981;&#20381;&#36182;&#20110;&#36755;&#20837;&#22686;&#24378;&#65292;&#20294;&#20854;&#33945;&#38754;&#37319;&#26679;&#36807;&#31243;&#20173;&#28982;&#39046;&#22495;&#29305;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#23548;&#33945;&#38754;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SMA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23436;&#20840;&#39046;&#22495;&#26080;&#20851;&#30340;&#33945;&#38754;&#24314;&#27169;&#26041;&#27861;&#12290;SMA&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#33945;&#38754;&#24314;&#27169;&#30446;&#26631;&#23398;&#20064;&#33945;&#38754;&#37319;&#26679;&#65292;&#32780;&#19981;&#20570;&#20219;&#20309;&#39046;&#22495;&#29305;&#23450;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#34507;&#30333;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#21644;&#31890;&#23376;&#29289;&#29702;&#23398;&#19977;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;SMA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14789v1 Announce Type: cross  Abstract: Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#39034;&#24207;&#21270;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#21644;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#25512;&#26029;&#38382;&#39064;&#20998;&#35299;&#20026;&#21464;&#37327;&#25299;&#25169;&#39034;&#24207;&#25512;&#26029;&#21644;&#21464;&#37327;&#29238;&#33410;&#28857;&#38598;&#21512;&#25512;&#26029;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22240;&#26524;&#26426;&#21046;&#24314;&#27169;&#23454;&#29616;&#31934;&#30830;&#36793;&#32536;&#21270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.14781</link><description>&lt;p&gt;
Rao-Blackwellising Bayesian Causal Inference
&lt;/p&gt;
&lt;p&gt;
Rao-Blackwellising Bayesian Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#39034;&#24207;&#21270;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#21644;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#25512;&#26029;&#38382;&#39064;&#20998;&#35299;&#20026;&#21464;&#37327;&#25299;&#25169;&#39034;&#24207;&#25512;&#26029;&#21644;&#21464;&#37327;&#29238;&#33410;&#28857;&#38598;&#21512;&#25512;&#26029;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22240;&#26524;&#26426;&#21046;&#24314;&#27169;&#23454;&#29616;&#31934;&#30830;&#36793;&#32536;&#21270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#65292;&#21363;&#25512;&#26029;&#29992;&#20110;&#19979;&#28216;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#22240;&#26524;&#27169;&#22411;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#22312;&#25991;&#29486;&#20013;&#40092;&#26377;&#25506;&#35752;&#30340;&#38590;&#35299;&#30340;&#35745;&#31639;&#25512;&#26029;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#19982;&#26368;&#36817;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#30340;&#38382;&#39064;&#20998;&#35299;&#20026;(i)&#25512;&#26029;&#21464;&#37327;&#20043;&#38388;&#30340;&#25299;&#25169;&#39034;&#24207;&#20197;&#21450;(ii)&#25512;&#26029;&#27599;&#20010;&#21464;&#37327;&#30340;&#29238;&#33410;&#28857;&#38598;&#21512;&#12290;&#24403;&#38480;&#21046;&#27599;&#20010;&#21464;&#37327;&#30340;&#29238;&#33410;&#28857;&#25968;&#37327;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23436;&#20840;&#36793;&#32536;&#21270;&#29238;&#33410;&#28857;&#38598;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#24314;&#27169;&#26410;&#30693;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20174;&#32780;&#20801;&#35768;&#20854;&#31934;&#30830;&#36793;&#32536;&#21270;&#12290;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#65292;&#20854;&#20013;&#38500;&#20102;&#22240;&#26524;&#39034;&#24207;&#20043;&#22806;&#65292;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#37117;&#34987;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14781v1 Announce Type: cross  Abstract: Bayesian causal inference, i.e., inferring a posterior over causal models for the use in downstream causal reasoning tasks, poses a hard computational inference problem that is little explored in literature. In this work, we combine techniques from order-based MCMC structure learning with recent advances in gradient-based graph learning into an effective Bayesian causal inference framework. Specifically, we decompose the problem of inferring the causal structure into (i) inferring a topological order over variables and (ii) inferring the parent sets for each variable. When limiting the number of parents per variable, we can exactly marginalise over the parent sets in polynomial time. We further use Gaussian processes to model the unknown causal mechanisms, which also allows their exact marginalisation. This introduces a Rao-Blackwellization scheme, where all components are eliminated from the model, except for the causal order, for whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#25903;&#25345;&#19979;&#65292;&#33521;&#35821;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20934;&#30830;&#12289;&#26377;&#29992;&#22238;&#24212;&#65292;&#20294;&#23384;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.14778</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer in instruction tuning of large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#25903;&#25345;&#19979;&#65292;&#33521;&#35821;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20934;&#30830;&#12289;&#26377;&#29992;&#22238;&#24212;&#65292;&#20294;&#23384;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#65288;IT&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25945;&#23548;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#20219;&#24847;&#25351;&#20196;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;IT&#20013;&#30340;&#38646;&#27425;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#24403;LLM&#22312;&#20165;&#33521;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#28982;&#21518;&#22312;&#20854;&#20182;&#35821;&#35328;&#29992;&#25143;&#25552;&#31034;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27169;&#22411;&#37197;&#32622;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#35780;&#20272;&#31574;&#30053;&#29992;&#20110;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#27169;&#22411;&#35757;&#32451;&#30340;&#25152;&#26377;&#38454;&#27573;&#37117;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#22312;IT&#20013;&#20063;&#20250;&#25104;&#21151;&#21457;&#29983;&#65292;&#20294;&#21482;&#26377;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#32771;&#34385;&#21040;&#22810;&#35821;&#35328;&#24615;&#20197;&#21450;&#26377;&#36275;&#22815;&#22823;&#30340;IT&#25968;&#25454;&#26102;&#25165;&#20250;&#21457;&#29983;&#12290;&#32463;&#36807;&#33521;&#35821;&#35757;&#32451;&#30340;LLMs&#33021;&#22815;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#29983;&#25104;&#20934;&#30830;&#12289;&#20840;&#38754;&#19988;&#26377;&#24110;&#21161;&#30340;&#22238;&#24212;&#65292;&#20294;&#32570;&#20047;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20598;&#23572;&#21487;&#33021;&#23384;&#22312;&#27969;&#30021;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14778v1 Announce Type: cross  Abstract: Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MT-Bench-101&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;4208&#36718;&#23545;&#35805;&#25968;&#25454;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20102;21&#31181;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19981;&#21516;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14762</link><description>&lt;p&gt;
MT-Bench-101: &#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MT-Bench-101&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#21253;&#21547;4208&#36718;&#23545;&#35805;&#25968;&#25454;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#20102;21&#31181;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19981;&#21516;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22823;&#22823;&#22686;&#24378;&#20102;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#36718;&#23545;&#35805;&#25110;&#32773;&#25552;&#20379;&#31895;&#31890;&#24230;&#21644;&#19981;&#23436;&#25972;&#30340;&#22810;&#36718;&#23545;&#35805;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#30495;&#23454;&#23545;&#35805;&#30340;&#22797;&#26434;&#24615;&#21644;&#32454;&#24494;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MT-Bench-101&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#32454;&#31890;&#24230;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#22810;&#36718;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;13&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;1388&#20010;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;4208&#36718;&#30340;&#19977;&#32423;&#20998;&#23618;&#33021;&#21147;&#20998;&#31867;&#12290;&#28982;&#21518;&#25105;&#20204;&#22522;&#20110;MT-Bench-101&#35780;&#20272;&#20102;21&#20010;&#27969;&#34892;&#30340;LLMs&#65292;&#20174;&#33021;&#21147;&#21644;&#20219;&#21153;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#35266;&#23519;&#21040;LLMs&#22312;&#23545;&#35805;&#36718;&#27425;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14762v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns with
&lt;/p&gt;</description></item><item><title>&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20013;&#24515;&#27010;&#24565;&#22312;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#28304;&#33258;&#30456;&#21516;&#32622;&#20449;&#38598;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25512;&#24191;&#65292;&#26159;&#23545;&#32479;&#35745;&#23398;&#20064;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#26356;&#19968;&#33324;&#22788;&#29702;&#30340;&#39318;&#35201;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2402.14759</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#25512;&#24191;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#21487;&#23454;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalising realisability in statistical learning theory under epistemic uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14759
&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20013;&#24515;&#27010;&#24565;&#22312;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#28304;&#33258;&#30456;&#21516;&#32622;&#20449;&#38598;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25512;&#24191;&#65292;&#26159;&#23545;&#32479;&#35745;&#23398;&#20064;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#26356;&#19968;&#33324;&#22788;&#29702;&#30340;&#39318;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20013;&#24515;&#27010;&#24565;&#65292;&#22914;&#21487;&#23454;&#29616;&#24615;&#65292;&#22312;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#28304;&#33258;&#30456;&#21516;&#32622;&#20449;&#38598;&#65292;&#21363;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#20984;&#38598;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25512;&#24191;&#12290;&#36825;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#23545;&#32479;&#35745;&#23398;&#20064;&#36827;&#34892;&#26356;&#19968;&#33324;&#22788;&#29702;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14759v1 Announce Type: cross  Abstract: The purpose of this paper is to look into how central notions in statistical learning theory, such as realisability, generalise under the assumption that train and test distribution are issued from the same credal set, i.e., a convex set of probability distributions. This can be considered as a first step towards a more general treatment of statistical learning under epistemic uncertainty.
&lt;/p&gt;</description></item><item><title>BaM&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#65292;&#38024;&#23545;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#24930;&#25910;&#25947;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#21464;&#20998;&#26063;&#20013;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#26102;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#23558;&#25351;&#25968;&#24555;&#36895;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;BaM&#22312;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14758</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;&#21644;&#21305;&#37197;&#65306;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Batch and match: black-box variational inference with a score-based divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14758
&lt;/p&gt;
&lt;p&gt;
BaM&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#65292;&#38024;&#23545;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#24930;&#25910;&#25947;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#21464;&#20998;&#26063;&#20013;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#26102;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#23558;&#25351;&#25968;&#24555;&#36895;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;BaM&#22312;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20027;&#35201;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#23454;&#29616;&#37117;&#26159;&#22522;&#20110;&#20248;&#21270;&#38543;&#26426;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;BBVI&#26041;&#27861;&#36890;&#24120;&#30001;&#20110;&#20854;&#26799;&#24230;&#20272;&#35745;&#30340;&#39640;&#26041;&#24046;&#32780;&#25910;&#25947;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;&#21644;&#21305;&#37197;&#65288;BaM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#21487;&#20197;&#36890;&#36807;&#23545;&#20855;&#26377;&#20840;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#21464;&#20998;&#26063;&#20351;&#29992;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#26102;BaM&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#22312;&#25209;&#37327;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#20250;&#25351;&#25968;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;BaM&#22312;&#28304;&#33258;&#23618;&#27425;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21518;&#39564;&#25512;&#26029;&#30340;&#39640;&#26031;&#21644;&#38750;&#39640;&#26031;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;BaM&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14758v1 Announce Type: cross  Abstract: Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM ty
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DRL&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#24102;&#20132;&#36890;&#30340;&#26725;&#26753;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#20132;&#36890;&#25345;&#32493;&#36827;&#34892;&#26102;&#23545;&#28151;&#20957;&#22303;&#26725;&#38754;&#36827;&#34892;&#35010;&#32541;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#20855;&#20307;&#26725;&#38754;&#26816;&#27979;&#25216;&#26415;&#24212;&#29992;</title><link>https://arxiv.org/abs/2402.14757</link><description>&lt;p&gt;
SHM-Traffic: &#22522;&#20110;DRL&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#25511;&#21046;&#29992;&#20110;&#24102;&#20132;&#36890;&#30340;&#26725;&#26753;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
SHM-Traffic: DRL and Transfer learning based UAV Control for Structural Health Monitoring of Bridges with Traffic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DRL&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#24102;&#20132;&#36890;&#30340;&#26725;&#26753;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#20132;&#36890;&#25345;&#32493;&#36827;&#34892;&#26102;&#23545;&#28151;&#20957;&#22303;&#26725;&#38754;&#36827;&#34892;&#35010;&#32541;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#20855;&#20307;&#26725;&#38754;&#26816;&#27979;&#25216;&#26415;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20391;&#37325;&#20110;&#21033;&#29992;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;&#24102;&#26377;&#20132;&#36890;&#30340;&#26725;&#26753;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#25511;&#21046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20132;&#36890;&#25345;&#32493;&#36827;&#34892;&#26102;&#36827;&#34892;&#28151;&#20957;&#22303;&#26725;&#38754;&#35843;&#26597;&#24182;&#26816;&#27979;&#35010;&#32541;&#12290;&#26080;&#20154;&#26426;&#25191;&#34892;&#35010;&#32541;&#26816;&#27979;&#65292;&#35010;&#32541;&#30340;&#20301;&#32622;&#26368;&#21021;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#31181;&#36793;&#32536;&#26816;&#27979;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;Canny&#36793;&#32536;&#26816;&#27979;&#36827;&#34892;&#35010;&#32541;&#26816;&#27979;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#35010;&#32541;&#26816;&#27979;&#65292;&#24182;&#23558;&#20854;&#19982;Canny&#36793;&#32536;&#26816;&#27979;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20351;&#29992;&#20855;&#26377;&#26469;&#33258;&#35010;&#32541;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;CNN&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#36825;&#20351;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#24182;&#25552;&#39640;&#20854;&#22312;&#35782;&#21035;&#21644;&#23450;&#20301;&#35010;&#32541;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#24212;&#29992;Proximal Policy Optimization&#65288;PPO&#65289;&#36827;&#34892;UAV&#25511;&#21046;&#21644;&#26725;&#26753;&#35843;&#26597;&#12290;&#36827;&#34892;&#20102;&#36328;&#19981;&#21516;&#22330;&#26223;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14757v1 Announce Type: new  Abstract: This work focuses on using advanced techniques for structural health monitoring (SHM) for bridges with Traffic. We propose an approach using deep reinforcement learning (DRL)-based control for Unmanned Aerial Vehicle (UAV). Our approach conducts a concrete bridge deck survey while traffic is ongoing and detects cracks. The UAV performs the crack detection, and the location of cracks is initially unknown. We use two edge detection techniques. First, we use canny edge detection for crack detection. We also use a Convolutional Neural Network (CNN) for crack detection and compare it with canny edge detection. Transfer learning is applied using CNN with pre-trained weights obtained from a crack image dataset. This enables the model to adapt and improve its performance in identifying and localizing cracks. Proximal Policy Optimization (PPO) is applied for UAV control and bridge surveys. The experimentation across various scenarios is performed
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;Pretrained Transformer&#21487;&#20197;&#25104;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#29978;&#33267;&#27604;&#20043;&#21069;&#35748;&#20026;&#30340;&#26356;&#23567;&#30340;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14753</link><description>&lt;p&gt;
Pretrained Transformer&#30340;&#24341;&#23548;&#21487;&#20197;&#25104;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;
&lt;/p&gt;
&lt;p&gt;
Prompting a Pretrained Transformer Can Be a Universal Approximator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14753
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;Pretrained Transformer&#21487;&#20197;&#25104;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#29978;&#33267;&#27604;&#20043;&#21069;&#35748;&#20026;&#30340;&#26356;&#23567;&#30340;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Prompting&#12289;Prompt&#35843;&#25972;&#21644;&#21069;&#32512;&#35843;&#25972;transformer&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#25105;&#20204;&#23545;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#24418;&#24335;&#19978;&#65292;&#25552;&#31034;&#21644;&#21069;&#32512;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#21542;&#26222;&#36941;&#36924;&#36817;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20989;&#25968;&#12290;&#26412;&#25991;&#32943;&#23450;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#27604;&#20808;&#21069;&#35748;&#20026;&#30340;&#35201;&#23567;&#24471;&#22810;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#28155;&#21152;&#21069;&#32512;&#21518;&#21487;&#20197;&#25104;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#20107;&#23454;&#19978;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#38750;&#24120;&#36866;&#21512;&#20110;&#21069;&#32512;&#35843;&#25972;&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#27880;&#24847;&#21147;&#22836;&#23601;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;transformer&#30340;&#28145;&#24230;&#20013;&#28155;&#21152;&#21069;&#32512;&#65292;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#37117;&#21487;&#20197;&#34987;&#36924;&#36817;&#65292;&#20854;&#28145;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#38500;&#20102;&#36825;&#20123;&#23494;&#24230;&#31867;&#22411;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;Jack...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14753v1 Announce Type: cross  Abstract: Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited. A key question is whether one can arbitrarily modify the behavior of pretrained model by prompting or prefix-tuning it. Formally, whether prompting and prefix-tuning a pretrained model can universally approximate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when prefixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length. Beyond these density-type results, we also offer Jack
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.14744</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22478;&#24066;&#23621;&#27665;&#65306;&#29992;&#20110;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#30340;LLM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#28789;&#27963;&#39640;&#25928;&#30340;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#12290;LLMs&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#24182;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#22810;&#21151;&#33021;&#24615;, &#20811;&#26381;&#20102;&#20197;&#24448;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#36843;&#20999;&#38656;&#27714;, &#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;: &#23558;LLMs&#19982;&#20016;&#23500;&#30340;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;, &#24320;&#21457;&#21487;&#38752;&#30340;&#27963;&#21160;&#29983;&#25104;&#31574;&#30053;, &#20197;&#21450;&#25506;&#32034;LLMs&#22312;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20195;&#29702;&#26694;&#26550;, &#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#20010;&#20307;&#27963;&#21160;&#27169;&#24335;&#21644;&#21160;&#26426;, &#21253;&#25324;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#33258;&#27965;&#26041;&#27861;&#21644;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;, &#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14730</link><description>&lt;p&gt;
Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford-Steerable Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14730
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;$\mathrm{E}(p, q)$&#31561;&#21464;CNN&#31867;&#12290; CS-CNNs&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;$\mathbb{R}^{p,q}$&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#12290; &#23427;&#20204;&#28085;&#30422;&#20102;&#20363;&#22914;$\mathrm{E}(3)$&#22312;$\mathbb{R}^3$&#19978;&#21644;Poincar\'e&#22312;&#38389;&#21487;&#22827;&#26031;&#22522;&#26102;&#31354;$\mathbb{R}^{1,3}$&#19978;&#30340;&#31561;&#21464;&#24615;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#12290; &#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#26041;&#27861;&#19978;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14730v1 Announce Type: cross  Abstract: We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance, $\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.
&lt;/p&gt;</description></item><item><title>&#27431;&#30431;AI&#27861;&#26696;&#24378;&#35843;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20154;&#26412;AI&#31995;&#32479;&#30340;&#27665;&#20027;&#21628;&#21505;&#65292;&#21516;&#26102;&#21046;&#23450;&#20102;&#20154;&#26412;&#21019;&#26032;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35758;&#31243;&#65292;&#20197;&#36991;&#20813;&#37325;&#22797;GDPR&#30340;&#38169;&#35823;&#24182;&#36991;&#20813;&#23454;&#26045;&#28151;&#20081;&#12290;</title><link>https://arxiv.org/abs/2402.14728</link><description>&lt;p&gt;
&#27431;&#27954;&#23545;&#20154;&#26412;&#31185;&#25216;&#30340;&#25215;&#35834;&#65306;HCI&#22312;&#27431;&#30431;AI&#27861;&#26696;&#25104;&#21151;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The European Commitment to Human-Centered Technology: The Integral Role of HCI in the EU AI Act's Success
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14728
&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;AI&#27861;&#26696;&#24378;&#35843;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20154;&#26412;AI&#31995;&#32479;&#30340;&#27665;&#20027;&#21628;&#21505;&#65292;&#21516;&#26102;&#21046;&#23450;&#20102;&#20154;&#26412;&#21019;&#26032;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35758;&#31243;&#65292;&#20197;&#36991;&#20813;&#37325;&#22797;GDPR&#30340;&#38169;&#35823;&#24182;&#36991;&#20813;&#23454;&#26045;&#28151;&#20081;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30340;&#21457;&#23637;&#23558;&#28145;&#21051;&#37325;&#22609;&#26410;&#26469;&#12290;&#27431;&#30431;&#35748;&#35782;&#21040;&#36825;&#19968;&#21363;&#23558;&#21040;&#26469;&#30340;&#37325;&#35201;&#24615;&#65292;&#24050;&#32463;&#36890;&#36807;&#20102;AI&#27861;&#26696;&#65292;&#23545;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#30340;&#24066;&#22330;&#20934;&#20837;&#36827;&#34892;&#30417;&#31649;&#12290;&#35813;&#27861;&#26696;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24449;&#26159;&#36890;&#36807;&#19987;&#27880;&#20110;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#20154;&#31867;&#29702;&#35299;&#21644;&#25511;&#21046;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#32500;&#25252;&#27665;&#20027;&#21644;&#20154;&#36947;&#20027;&#20041;&#20215;&#20540;&#35266;&#12290;&#22240;&#27492;&#65292;&#27431;&#30431;AI&#27861;&#26696;&#19981;&#20165;&#20165;&#35268;&#23450;&#20102;AI&#31995;&#32479;&#30340;&#25216;&#26415;&#35201;&#27714;&#12290;&#27431;&#30431;&#21457;&#20986;&#20102;&#19968;&#20010;&#27665;&#20027;&#21495;&#21484;&#65292;&#35201;&#27714;&#20154;&#26412;AI&#31995;&#32479;&#65292;&#36827;&#32780;&#21046;&#23450;&#20102;&#20154;&#26412;&#21019;&#26032;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35758;&#31243;&#65292;&#20419;&#36827;AI&#21457;&#23637;&#20013;&#30340;&#20154;&#26412;&#21019;&#26032;&#12290;&#22914;&#26524;&#27809;&#26377;&#24378;&#22823;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#31995;&#32479;&#21450;&#20854;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#24433;&#21709;&#65292;&#27431;&#30431;AI&#27861;&#26696;&#21487;&#33021;&#20250;&#23548;&#33268;&#37325;&#22797;&#27431;&#30431;&#12298;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#30340;&#38169;&#35823;&#65292;&#23548;&#33268;&#20179;&#20419;&#12289;&#28151;&#20081;&#12289;&#20020;&#26102;&#21644;&#27169;&#31946;&#30340;&#23454;&#26045;&#65292;&#24102;&#26469;&#26356;&#22810;&#30340;&#22256;&#24785;&#32780;&#19981;&#26159;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14728v1 Announce Type: cross  Abstract: The evolution of AI is set to profoundly reshape the future. The European Union, recognizing this impending prominence, has enacted the AI Act, regulating market access for AI-based systems. A salient feature of the Act is to guard democratic and humanistic values by focusing regulation on transparency, explainability, and the human ability to understand and control AI systems. Hereby, the EU AI Act does not merely specify technological requirements for AI systems. The EU issues a democratic call for human-centered AI systems and, in turn, an interdisciplinary research agenda for human-centered innovation in AI development. Without robust methods to assess AI systems and their effect on individuals and society, the EU AI Act may lead to repeating the mistakes of the General Data Protection Regulation of the EU and to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more confusion than lending guidance. Moreover, determine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19987;&#23478;&#35268;&#21017;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#32422;&#26463;&#21644;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#26469;&#20445;&#35777;&#36755;&#20986;&#27010;&#29575;&#19981;&#36829;&#21453;&#19987;&#23478;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#24402;&#32435;&#19982;&#28436;&#32462;&#23398;&#20064;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14726</link><description>&lt;p&gt;
&#22312;&#27010;&#24565;&#23398;&#20064;&#26694;&#26550;&#20013;&#23558;&#19987;&#23478;&#35268;&#21017;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Incorporating Expert Rules into Neural Networks in the Framework of Concept-Based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19987;&#23478;&#35268;&#21017;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#32422;&#26463;&#21644;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#26469;&#20445;&#35777;&#36755;&#20986;&#27010;&#29575;&#19981;&#36829;&#21453;&#19987;&#23478;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#24402;&#32435;&#19982;&#28436;&#32462;&#23398;&#20064;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#23558;&#19987;&#23478;&#35268;&#21017;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20197;&#25193;&#23637;&#22522;&#20110;&#27010;&#24565;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#22914;&#20309;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#39044;&#27979;&#27010;&#24565;&#27010;&#29575;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#35813;&#32452;&#21512;&#32972;&#21518;&#30340;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#24418;&#25104;&#32422;&#26463;&#65292;&#20197;&#28385;&#36275;&#19987;&#23478;&#35268;&#21017;&#30340;&#25152;&#26377;&#27010;&#24565;&#20540;&#32452;&#21512;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#31532;&#20108;&#20010;&#24819;&#27861;&#26159;&#20197;&#20984;&#22810;&#38754;&#20307;&#30340;&#24418;&#24335;&#34920;&#31034;&#27010;&#29575;&#20998;&#24067;&#30340;&#21487;&#34892;&#38598;&#65292;&#24182;&#20351;&#29992;&#20854;&#39030;&#28857;&#25110;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14726v1 Announce Type: cross  Abstract: A problem of incorporating the expert rules into machine learning models for extending the concept-based learning is formulated in the paper. It is proposed how to combine logical rules and neural networks predicting the concept probabilities. The first idea behind the combination is to form constraints for a joint probability distribution over all combinations of concept values to satisfy the expert rules. The second idea is to represent a feasible set of probability distributions in the form of a convex polytope and to use its vertices or faces. We provide several approaches for solving the stated problem and for training neural networks which guarantee that the output probabilities of concepts would not violate the expert rules. The solution of the problem can be viewed as a way for combining the inductive and deductive learning. Expert rules are used in a broader sense when any logical function that connects concepts and class labe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35789;&#27719;&#25193;&#23637;&#26041;&#27861;&#65288;EEVE&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#20854;&#22312;&#38889;&#25991;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14714</link><description>&lt;p&gt;
&#39640;&#25928;&#26377;&#25928;&#30340;&#35789;&#27719;&#25193;&#23637;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35789;&#27719;&#25193;&#23637;&#26041;&#27861;&#65288;EEVE&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#20854;&#22312;&#38889;&#25991;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25253;&#21578;&#20171;&#32461;&#20102;\texttt{EEVE-Korean-v1.0}&#65292;&#36825;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38889;&#25991;&#36866;&#37197;&#29256;&#26412;&#65292;&#23637;&#29616;&#20986;&#22312;&#33521;&#25991;&#21644;&#38889;&#25991;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35789;&#27719;&#25193;&#23637;&#26041;&#27861;&#65288;EEVE&#65289;&#65292;&#21253;&#25324;&#21442;&#25968;&#20923;&#32467;&#21644;&#23376;&#35789;&#21021;&#22987;&#21270;&#12290;&#19982;&#20808;&#21069;&#35748;&#20026;&#26032;&#23884;&#20837;&#38656;&#35201;&#19978;&#19975;&#20159;&#35757;&#32451;&#26631;&#35760;&#30340;&#21162;&#21147;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;20&#20159;&#26631;&#35760;&#20869;&#26174;&#30528;&#25552;&#21319;&#38750;&#33521;&#35821;&#29087;&#32451;&#24230;&#12290;&#25130;&#33267;2024&#24180;1&#26376;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\texttt{EEVE-Korean-10.8B-v1.0}&#22312;Open Ko-LLM&#27036;&#21333;&#19978;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#65292;&#25104;&#20026;&#20102;&#24320;&#28304;&#31038;&#21306;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#38889;&#25991;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26681;&#25454;Hugging Face&#30340;&#25490;&#34892;&#27036;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14714v1 Announce Type: cross  Abstract: This report introduces \texttt{EEVE-Korean-v1.0}, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard. We ope
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT-GNN&#30340;&#26032;&#22411;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14708</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT-GNN&#30340;&#26032;&#22411;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#29992;&#21345;&#27450;&#35784;&#23545;&#32463;&#27982;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#33410;&#28857;&#30340;&#26412;&#22320;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#8212;&#8212;CaT-GNN&#65288;Causal Temporal Graph Neural Networks&#65289;&#65292;&#21033;&#29992;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#26469;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#21457;&#29616;&#21644;&#24178;&#39044;&#38454;&#27573;&#65292;CaT-GNN&#30830;&#23450;&#20132;&#26131;&#22270;&#20013;&#30340;&#22240;&#26524;&#33410;&#28857;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;CaT-GNN&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;Causal-Inspector&#21644;Causal-Intervener&#12290;Causal-Inspector&#21033;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#35782;&#21035;&#22240;&#26524;&#21644;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14708v1 Announce Type: cross  Abstract: Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal \textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#65292;&#20197;&#35299;&#20915;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#26041;&#27861;&#20013;&#30340;&#38271;&#24230;&#25351;&#25968;&#22686;&#38271;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14703</link><description>&lt;p&gt;
&#22312;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#20013;&#25506;&#35752;&#26410;&#26469;&#21644;&#21382;&#21490;&#30340;&#35781;&#21650;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#65292;&#20197;&#35299;&#20915;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#26041;&#27861;&#20013;&#30340;&#38271;&#24230;&#25351;&#25968;&#22686;&#38271;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#22797;&#26434;&#35266;&#27979;&#30340;&#31163;&#32447;&#35780;&#20272;(OPE)&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#36991;&#20813;&#23545;&#26102;&#38388;&#36328;&#24230;&#25351;&#25968;&#20381;&#36182;&#30340;&#20272;&#35745;&#22120;&#12290;&#26368;&#36817;&#65292;Uehara&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#20102;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#20316;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#35813;&#26694;&#26550;&#20063;&#21462;&#20915;&#20110;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#30340;&#26377;&#30028;&#24615;&#20197;&#21450;&#20854;&#20182;&#30456;&#20851;&#25968;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25968;&#37327;&#21487;&#33021;&#20250;&#38543;&#30528;&#38271;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#20174;&#32780;&#25273;&#21435;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14703v1 Announce Type: cross  Abstract: We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14701</link><description>&lt;p&gt;
COMPASS&#65306;&#21033;&#29992;&#35821;&#35328;&#24314;&#27169;&#23545;&#24739;&#32773;-&#27835;&#30103;&#24072;&#32852;&#30431;&#31574;&#30053;&#36827;&#34892;&#35745;&#31639;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#26159;&#39044;&#27979;&#24515;&#29702;&#27835;&#30103;&#27835;&#30103;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20256;&#32479;&#19978;&#65292;&#24037;&#20316;&#32852;&#30431;&#35780;&#20272;&#20381;&#36182;&#20110;&#27835;&#30103;&#24072;&#21644;&#24739;&#32773;&#22635;&#20889;&#30340;&#38382;&#21367;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;COMPASS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#30452;&#25509;&#20174;&#24515;&#29702;&#27835;&#30103;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#30340;&#36716;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;&#24037;&#20316;&#32852;&#30431;&#28165;&#21333;&#20013;&#38472;&#36848;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#28085;&#30422;&#22810;&#31181;&#31934;&#31070;&#30142;&#30149;&#30340;&#36229;&#36807;950&#20010;&#20250;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#24494;&#22320;&#26144;&#23556;&#24739;&#32773;-&#27835;&#30103;&#24072;&#23545;&#40784;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#31070;&#32463;&#20027;&#39064;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.14698</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#20998;&#26512;&#29992;&#20110;&#20998;&#31867;&#19982;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65306;&#25104;&#37117;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Big data analytics to classify earthwork-related locations: A Chengdu study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14698
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26174;&#33879;&#21152;&#21095;&#65292;&#23548;&#33268;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20005;&#37325;&#20581;&#24247;&#21518;&#26524;&#12290;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65288;ERLs&#65289;&#26159;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;ERLs&#30340;&#26377;&#25928;&#31649;&#29702;&#19968;&#30452;&#26159;&#25919;&#24220;&#21644;&#29615;&#22659;&#26426;&#26500;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#20027;&#35201;&#21407;&#22240;&#21253;&#25324;&#20854;&#20998;&#31867;&#20998;&#23646;&#19981;&#21516;&#30340;&#30417;&#31649;&#37096;&#38376;&#12289;&#20449;&#24687;&#38556;&#30861;&#12289;&#25968;&#25454;&#26356;&#26032;&#24310;&#36831;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#28304;&#22836;&#28784;&#23576;&#27745;&#26579;&#30340;&#25233;&#21046;&#25514;&#26045;&#30340;&#32570;&#20047;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23454;&#38469;&#25968;&#25454;&#30740;&#31350;&#20102;&#29305;&#24449;&#19982;&#28784;&#23576;&#27745;&#26579;&#28304;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#25104;&#21151;&#23454;&#26045;&#22312;&#19968;&#20010;&#21517;&#20026;&#30340;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14698v1 Announce Type: cross  Abstract: Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#30340;&#35270;&#35273;&#24187;&#35273;&#23454;&#20363;&#26469;&#26816;&#39564;&#20854;&#24615;&#33021;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#32447;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.14683</link><description>&lt;p&gt;
&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Visual Hallucinations of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14683
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#30340;&#35270;&#35273;&#24187;&#35273;&#23454;&#20363;&#26469;&#26816;&#39564;&#20854;&#24615;&#33021;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24187;&#35273;&#65288;VH&#65289;&#24847;&#21619;&#30528;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#23545;&#22270;&#20687;&#24819;&#35937;&#20986;&#38169;&#35823;&#30340;&#32454;&#33410;&#12290;&#29616;&#26377;&#30740;&#31350;&#21457;&#29616;VH&#23454;&#20363;&#20165;&#23384;&#22312;&#20110;&#29616;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;MLLM&#22312;VH&#19979;&#30340;&#24615;&#33021;&#29702;&#35299;&#23384;&#22312;&#20559;&#24046;&#65292;&#21407;&#22240;&#22312;&#20110;&#36825;&#31867;VH&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VHTest&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;VH&#23454;&#20363;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;VHTest&#22312;&#29616;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;COCO&#65289;&#20013;&#25214;&#21040;&#19968;&#20123;&#21021;&#22987;&#30340;VH&#23454;&#20363;&#65292;&#20026;&#27599;&#20010;VH&#27169;&#24335;&#29983;&#25104;&#19968;&#20010;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;DALL-E-3&#65289;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;VH&#22270;&#20687;&#12290;&#25105;&#20204;&#21033;&#29992;VHTest&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;8&#20010;VH&#27169;&#24335;&#20013;1,200&#20010;VH&#23454;&#20363;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#65288;&#20363;&#22914;GPT-4V&#12289;LLaVA-1.5&#21644;MiniGPT-v2&#65289;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#22823;&#37096;&#20998;&#23454;&#20363;&#20135;&#29983;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14683v1 Announce Type: cross  Abstract: Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark data
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#24037;&#20855;&#26469;&#36741;&#21161;&#35821;&#35328;&#20195;&#29702;&#22312;&#24222;&#22823;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#30693;&#35782;&#24211;&#21644;&#25968;&#25454;&#24211;&#31561;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#20511;&#21161;&#24037;&#20855;&#22686;&#24378;&#35821;&#35328;&#20195;&#29702;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14672</link><description>&lt;p&gt;
&#35821;&#35328;&#20013;&#38388;&#20214;&#65306;&#24037;&#20855;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23545;&#35821;&#35328;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14672
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#24037;&#20855;&#26469;&#36741;&#21161;&#35821;&#35328;&#20195;&#29702;&#22312;&#24222;&#22823;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#30693;&#35782;&#24211;&#21644;&#25968;&#25454;&#24211;&#31561;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#20511;&#21161;&#24037;&#20855;&#22686;&#24378;&#35821;&#35328;&#20195;&#29702;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#24050;&#32463;&#36828;&#36828;&#36229;&#20986;&#20102;&#25991;&#26412;&#22788;&#29702;&#30340;&#33539;&#22260;&#65292;&#39044;&#31034;&#30528;&#19968;&#20010;&#26032;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#22312;&#36825;&#20010;&#26102;&#20195;&#65292;LLMs&#34987;&#35774;&#24819;&#20026;&#33021;&#22815;&#22312;&#22797;&#26434;&#29616;&#23454;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#36890;&#29992;&#35821;&#35328;&#20195;&#29702;&#12290;&#36825;&#20123;&#29615;&#22659;&#36890;&#24120;&#38750;&#24120;&#24191;&#38420;&#65292;&#20351;&#24471;LLM&#19981;&#21487;&#33021;&#22312;&#20854;&#30701;&#26399;&#35760;&#24518;&#20013;&#22788;&#29702;&#23427;&#20204;&#12290;&#21463;&#26368;&#36817;&#20851;&#20110;&#36890;&#36807;&#24037;&#20855;&#25193;&#23637;LLMs&#33021;&#21147;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#24037;&#20855;&#22312;&#22686;&#24378;LLMs&#22788;&#29702;&#36825;&#31181;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23450;&#21046;&#24037;&#20855;&#65292;&#20197;&#21327;&#21161;&#22312;&#36825;&#20123;&#24222;&#22823;&#29615;&#22659;&#20013;&#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#20214;&#23618;&#65292;&#20351;LLM&#20813;&#21463;&#29615;&#22659;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#22797;&#26434;&#29615;&#22659;--&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#21644;&#25968;&#25454;&#24211;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20351;&#29992;&#24037;&#20855;&#22686;&#24378;&#35821;&#35328;&#20195;&#29702;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14672v1 Announce Type: cross  Abstract: The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist language agents capable of operating within complex real-world environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, this paper investigates the intriguing potential of tools to augment LLMs in handling such complexity. To this end, we design customized tools to aid in the proactive exploration within these massive environments. Such tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments -- knowledge bases (KBs) and databases -- we demonstrate the significant potential of augmenting language agents with tools in complex environments. N
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14664</link><description>&lt;p&gt;
&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#36125;&#21494;&#26031;&#31163;&#31574;&#30053;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Off-Policy Evaluation and Learning for Large Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#31995;&#32479;&#20013;&#65292;&#21160;&#20316;&#32463;&#24120;&#26159;&#30456;&#20851;&#30340;&#65292;&#36825;&#20026;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#26356;&#26377;&#25928;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#21644;&#23398;&#20064;&#65288;OPL&#65289;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#26469;&#25429;&#25417;&#36825;&#20123;&#30456;&#20851;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;sDM&#65292;&#19968;&#20010;&#20026;OPE&#21644;OPL&#35774;&#35745;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#26082;&#26377;&#31639;&#27861;&#22522;&#30784;&#21448;&#26377;&#29702;&#35770;&#22522;&#30784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;sDM&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#32780;&#19981;&#20250;&#24433;&#21709;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#21463;&#22312;&#32447;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#24615;&#33021;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20559;&#31163;&#20256;&#32479;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#30340;&#34920;&#29616;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#22909;&#22788;&#12290;&#23454;&#35777;&#35777;&#25454;&#23637;&#31034;&#20102;sDM&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14664v1 Announce Type: cross  Abstract: In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach designed for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;ConceptMath&#65292;&#19968;&#31181;&#21452;&#35821;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2402.14660</link><description>&lt;p&gt;
ConceptMath&#65306;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#21452;&#35821;&#27010;&#24565;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14660
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;ConceptMath&#65292;&#19968;&#31181;&#21452;&#35821;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ConceptMath&#65292;&#36825;&#26159;&#19968;&#20010;&#21452;&#35821;&#65288;&#33521;&#35821;&#21644;&#20013;&#25991;&#65289;&#65292;&#32454;&#31890;&#24230;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#35780;&#20272;&#19968;&#33324;&#25968;&#23398;&#25512;&#29702;&#30340;&#20256;&#32479;&#22522;&#20934;&#19981;&#21516;&#65292;ConceptMath&#23558;&#25968;&#23398;&#38382;&#39064;&#31995;&#32479;&#22320;&#32452;&#32455;&#22312;&#25968;&#23398;&#27010;&#24565;&#30340;&#23618;&#27425;&#32467;&#26500;&#19979;&#65292;&#20174;&#32780;&#21487;&#20197;&#20197;&#27010;&#24565;&#20026;&#21333;&#20301;&#20934;&#30830;&#24615;&#35780;&#20272;&#25968;&#23398;&#25512;&#29702;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;ConceptMath&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24191;&#27867;&#33539;&#22260;&#30340;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;LLMs&#23613;&#31649;&#22312;&#20256;&#32479;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#39640;&#24179;&#22343;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#20005;&#37325;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24494;&#35843;&#31574;&#30053;&#26469;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#24369;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24076;&#26395;ConceptMath&#33021;&#22815;&#25351;&#23548;&#24320;&#21457;&#32773;&#29702;&#35299;&#32454;&#33268;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14660v1 Announce Type: cross  Abstract: This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grai
&lt;/p&gt;</description></item><item><title>OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.14658</link><description>&lt;p&gt;
OpenCodeInterpreter&#65306;&#38598;&#25104;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14658
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24341;&#20837;&#26174;&#33879;&#25512;&#21160;&#20102;&#20195;&#30721;&#29983;&#25104;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#31867;&#20284;GPT-4 Code Interpreter&#36825;&#26679;&#30340;&#39640;&#32423;&#31995;&#32479;&#30340;&#25191;&#34892;&#33021;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenCodeInterpreter&#65292;&#36825;&#26159;&#19968;&#26063;&#26088;&#22312;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#36845;&#20195;&#32454;&#21270;&#20195;&#30721;&#30340;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#12290;&#36890;&#36807;Code-Feedback&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#25191;&#34892;&#21644;&#20154;&#31867;&#21453;&#39304;&#65292;&#29992;&#20110;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#12290;&#25105;&#20204;&#23545;OpenCodeInterpreter&#22312;&#35832;&#22914;HumanEval&#12289;MBPP&#20197;&#21450;&#23427;&#20204;&#26469;&#33258;EvalPlus&#30340;&#22686;&#24378;&#29256;&#26412;&#31561;&#20851;&#38190;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#23454;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenCodeInterpreter-33B&#22312;HumanEval&#21644;MBPP&#30340;&#24179;&#22343;&#20540;&#65288;&#20197;&#21450;&#20854;&#22686;&#24378;&#29256;&#26412;&#65289;&#19978;&#21462;&#24471;&#20102;83.2&#65288;76.4&#65289;&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;GPT-4&#30340;84.2&#65288;76.2&#65289;&#32039;&#23494;&#21305;&#25932;&#65292;&#24182;&#19988;&#36890;&#36807;&#21512;&#25104;hum
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14658v1 Announce Type: cross  Abstract: The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized hum
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.14648</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#37325;&#26032;&#24605;&#32771;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#20197;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14648
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#19968;&#30452;&#26159;&#25269;&#25239;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AEs&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#65292;&#23398;&#20064;&#20855;&#26377;&#36776;&#21035;&#24615;&#21364;&#23545;&#25239;&#24615;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#30830;&#23450;&#20102;&#22952;&#30861;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19981;&#21464;&#24615;&#25439;&#22833;&#21644;&#20998;&#31867;&#30446;&#26631;&#20043;&#38388;&#30340;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#65292;&#34920;&#26126;&#23384;&#22312;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30001;&#20110;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#20998;&#24067;&#21457;&#25955;&#32780;&#20986;&#29616;&#30340;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AR-AT&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#20572;&#27490;&#26799;&#24230;&#25805;&#20316;&#21644;&#19968;&#20010;&#39044;&#27979;&#22120;&#26469;&#36991;&#20813;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#28789;&#24863;&#26469;&#33258;&#26368;&#36817;&#30340;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14648v1 Announce Type: cross  Abstract: Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a "gradient conflict" between invariance loss and classification objectives, indicating the existence of "collapsing solutions," and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid "collapsing solutions," inspired by a recent non-contrastive self-supervised learning a
&lt;/p&gt;</description></item><item><title>RoboScript&#26159;&#19968;&#20010;&#26088;&#22312;&#22635;&#34917;&#8220;&#29702;&#24819;&#21040;&#23454;&#38469;&#8221;&#24046;&#36317;&#30340;&#24179;&#21488;&#65292;&#25552;&#20379;&#21487;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27969;&#27700;&#32447;&#65292;&#24182;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.14623</link><description>&lt;p&gt;
RoboScript: &#36328;&#36234;&#30495;&#23454;&#21644;&#20223;&#30495;&#30340;&#33258;&#30001;&#24418;&#24335;&#25805;&#20316;&#20219;&#21153;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14623
&lt;/p&gt;
&lt;p&gt;
RoboScript&#26159;&#19968;&#20010;&#26088;&#22312;&#22635;&#34917;&#8220;&#29702;&#24819;&#21040;&#23454;&#38469;&#8221;&#24046;&#36317;&#30340;&#24179;&#21488;&#65292;&#25552;&#20379;&#21487;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27969;&#27700;&#32447;&#65292;&#24182;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#22312;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#25110;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24120;&#35782;&#25512;&#29702;&#21644;&#20219;&#21153;&#35268;&#21010;&#33021;&#21147;&#19978;&#65292;&#23545;&#20110;&#29983;&#25104;&#20195;&#30721;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#33258;&#20027;&#26426;&#22120;&#20154;&#31995;&#32479;&#22522;&#26412;&#32452;&#20214;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#65289;&#19978;&#30340;&#37096;&#32626;&#24615;&#20184;&#20986;&#30340;&#21162;&#21147;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#8220;&#29702;&#24819;&#21040;&#23454;&#38469;&#8221;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;RoboScript&#65292;&#19968;&#20010;&#24179;&#21488;&#65292;&#29992;&#20110;1&#65289;&#30001;&#20195;&#30721;&#29983;&#25104;&#39537;&#21160;&#30340;&#21487;&#37096;&#32626;&#26426;&#22120;&#20154;&#25805;&#20316;&#27969;&#27700;&#32447;&#65307;&#21644;2&#65289;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#20013;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#12290;RoboScript&#24179;&#21488;&#36890;&#36807;&#24378;&#35843;&#19982;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#32479;&#19968;&#25509;&#21475;&#35299;&#20915;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#22522;&#20110;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#65288;ROS&#65289;&#30340;&#25277;&#35937;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14623v1 Announce Type: cross  Abstract: Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI. However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control. To bridge this ``ideal-to-real'' gap, this paper presents \textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language. The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14622</link><description>&lt;p&gt;
&#20174;&#20851;&#38190;&#35789;&#21040;&#32467;&#26500;&#21270;&#25688;&#35201;: &#31934;&#31616;&#23398;&#26415;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30701;&#25991;&#24378;&#35843;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#26085;&#30410;&#37325;&#35201;&#65292;&#25351;&#20986;&#20256;&#32479;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#24341;&#25806;&#30001;&#20110;&#20986;&#29256;&#29289;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#32467;&#26500;&#21270;&#35760;&#24405;&#65292;&#25903;&#25345;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#20202;&#34920;&#26495;&#65292;&#20197;&#24443;&#24213;&#25913;&#21464;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#65292;&#21462;&#20195;&#20256;&#32479;&#30340;&#25991;&#26412;&#23494;&#38598;&#22411;&#26041;&#27861;&#12290;&#36825;&#19968;&#24895;&#26223;&#36890;&#36807;&#19968;&#20010;&#20197;&#8220;&#20256;&#26579;&#30149;&#30340;&#32321;&#27542;&#25968;&#20272;&#35745;&#8221;&#30740;&#31350;&#20027;&#39064;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#39564;&#35777;&#24471;&#20197;&#20307;&#29616;&#65292;&#20351;&#29992;&#32463;&#36807;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33258;&#21160;&#21019;&#24314;&#32467;&#26500;&#21270;&#35760;&#24405;&#20197;&#22635;&#20805;&#19968;&#20010;&#36229;&#36234;&#20851;&#38190;&#35789;&#30340;&#21518;&#31471;&#25968;&#25454;&#24211;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#22312;https://orkg.org/usecases/r0-estimates &#19978;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14622v1 Announce Type: cross  Abstract: This short paper highlights the growing importance of information retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications. The proposed solution involves structured records, underpinning advanced information technology (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach. This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases'' research theme, using a fine-tuned large language model (LLM) to automate the creation of structured records to populate a backend database that now goes beyond keywords. The result is a next-generation IR method accessible at https://orkg.org/usecases/r0-estimates.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.14609</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Complex Qeury Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#31572;&#26696;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#25191;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;&#30340;&#33021;&#21147;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#22270;&#25512;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#36923;&#36753;&#26597;&#35810;&#34920;&#31034;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#36923;&#36753;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26597;&#35810;&#21333;&#20010;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#24182;&#19981;&#33021;&#24212;&#29992;&#20110;&#22810;&#20010;&#22270;&#24418;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#20849;&#20139;&#24102;&#26377;&#25935;&#24863;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#33021;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#20351;&#24471;&#20849;&#20139;&#21644;&#26500;&#24314;&#19968;&#20010;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#25512;&#29702;&#20197;&#26816;&#32034;&#26597;&#35810;&#31572;&#26696;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#19981;&#28165;&#26970;&#22914;&#20309;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#26597;&#35810;&#12290;&#19968;&#20010;&#23454;&#20307;&#21487;&#33021;&#28041;&#21450;&#21040;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#23545;&#20110;&#21457;&#29616;&#30693;&#35782;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.14601</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#25945;&#32946;&#20013;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bringing Generative AI to Adaptive Learning in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14601
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#28608;&#22686;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#21457;&#23637;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#36825;&#19968;&#27010;&#24565;&#22312;&#25945;&#32946;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#25552;&#39640;&#23398;&#29983;&#23398;&#20064;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#36890;&#36807;&#35752;&#35770;&#36825;&#19968;&#39046;&#22495;&#30340;&#22909;&#22788;&#12289;&#25361;&#25112;&#21644;&#28508;&#21147;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#32467;&#21512;&#23558;&#20026;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#24418;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14601v1 Announce Type: cross  Abstract: The recent surge in generative AI technologies, such as large language models and diffusion models, have boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next stage learning format in education.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27773;&#27833;&#35843;&#37197;&#35843;&#24230;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#25972;&#25968;&#32422;&#26463;&#21644;&#29983;&#25104;&#21487;&#34892;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36798;&#21040;&#22810;&#20010;&#20248;&#21270;&#30446;&#26631;&#24182;&#36981;&#23432;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14600</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27773;&#27833;&#35843;&#37197;&#35843;&#24230;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model-Based Multiobjective Optimization for Gasoline Blending Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27773;&#27833;&#35843;&#37197;&#35843;&#24230;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#25972;&#25968;&#32422;&#26463;&#21644;&#29983;&#25104;&#21487;&#34892;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36798;&#21040;&#22810;&#20010;&#20248;&#21270;&#30446;&#26631;&#24182;&#36981;&#23432;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27773;&#27833;&#28151;&#21512;&#35843;&#24230;&#21033;&#29992;&#36164;&#28304;&#20998;&#37197;&#21644;&#25805;&#20316;&#39034;&#24207;&#28385;&#36275;&#28860;&#27833;&#21378;&#30340;&#29983;&#20135;&#35201;&#27714;&#12290;&#38750;&#32447;&#24615;&#12289;&#25972;&#25968;&#32422;&#26463;&#21644;&#22823;&#37327;&#20915;&#31574;&#21464;&#37327;&#30340;&#23384;&#22312;&#22686;&#21152;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#20256;&#32479;&#21644;&#36827;&#21270;&#31639;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;&#31216;&#20026;DMO&#65289;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27773;&#27833;&#28151;&#21512;&#35843;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#25972;&#25968;&#32422;&#26463;&#24182;&#29983;&#25104;&#21487;&#34892;&#35843;&#24230;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#26031;&#22122;&#22768;&#21644;&#21487;&#34892;&#22495;&#20043;&#38388;&#21019;&#24314;&#22810;&#20010;&#20013;&#38388;&#20998;&#24067;&#12290;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#65292;&#35299;&#20915;&#26041;&#26696;&#20174;&#39640;&#26031;&#22122;&#22768;&#36807;&#28193;&#21040;&#21487;&#34892;&#35843;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20248;&#21270;&#30446;&#26631;&#12290;DMO&#23454;&#29616;&#20102;&#21516;&#26102;&#30446;&#26631;&#20248;&#21270;&#21644;&#32422;&#26463;&#36981;&#23432;&#12290;&#36827;&#34892;&#20102;&#27604;&#36739;&#27979;&#35797;&#20197;&#35780;&#20272;DMO&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14600v1 Announce Type: new  Abstract: Gasoline blending scheduling uses resource allocation and operation sequencing to meet a refinery's production requirements. The presence of nonlinearity, integer constraints, and a large number of decision variables adds complexity to this problem, posing challenges for traditional and evolutionary algorithms. This paper introduces a novel multiobjective optimization approach driven by a diffusion model (named DMO), which is designed specifically for gasoline blending scheduling. To address integer constraints and generate feasible schedules, the diffusion model creates multiple intermediate distributions between Gaussian noise and the feasible domain. Through iterative processes, the solutions transition from Gaussian noise to feasible schedules while optimizing the objectives using the gradient descent method. DMO achieves simultaneous objective optimization and constraint adherence. Comparative tests are conducted to evaluate DMO's p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#20248;&#21270;&#26234;&#24935;&#22478;&#24066;&#20013;&#30340;ICT&#27969;&#31243;&#26041;&#38754;&#30340;&#37325;&#35201;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14596</link><description>&lt;p&gt;
LLM&#22312;&#21487;&#25345;&#32493;&#26234;&#24935;&#22478;&#24066;&#20013;&#30340;&#20316;&#29992;&#65306;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
The Role of LLMs in Sustainable Smart Cities: Applications, Challenges, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#20248;&#21270;&#26234;&#24935;&#22478;&#24066;&#20013;&#30340;ICT&#27969;&#31243;&#26041;&#38754;&#30340;&#37325;&#35201;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#24935;&#22478;&#24066;&#22312;&#25552;&#21319;&#22478;&#24066;&#29983;&#27963;&#27700;&#24179;&#30340;&#25345;&#32493;&#36861;&#27714;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#21487;&#25345;&#32493;&#19988;&#21487;&#25193;&#23637;&#30340;&#21019;&#26032;&#39640;&#25928;&#31649;&#29702;&#36164;&#28304;&#65292;&#20419;&#36827;&#22478;&#24066;&#21306;&#22495;&#30340;&#24555;&#36895;&#25193;&#23637;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#12289;&#22823;&#25968;&#25454;&#20998;&#26512;&#20197;&#21450;&#38654;&#35745;&#31639;&#21644;&#36793;&#32536;&#35745;&#31639;&#31561;&#26032;&#20852;&#25216;&#26415;&#26085;&#30410;&#26222;&#21450;&#65292;&#26234;&#24935;&#22478;&#24066;&#30340;&#24212;&#29992;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#26426;&#23494;&#21644;&#25935;&#24863;&#25968;&#25454;&#21487;&#33021;&#34987;&#26410;&#32463;&#25480;&#26435;&#25259;&#38706;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#26032;&#20852;&#25216;&#26415;&#30340;&#26080;&#32541;&#25972;&#21512;&#23545;&#20110;&#32500;&#25345;&#20854;&#21457;&#23637;&#30340;&#21160;&#24577;&#27493;&#20240;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#12289;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12289;&#29289;&#32852;&#32593;&#12289;&#21306;&#22359;&#38142;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20248;&#21270;&#26234;&#24935;&#22478;&#24066;&#20013;&#30340;ICT&#27969;&#31243;&#26041;&#38754;&#30340;&#37325;&#35201;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14596v1 Announce Type: new  Abstract: Smart cities stand as pivotal components in the ongoing pursuit of elevating urban living standards, facilitating the rapid expansion of urban areas while efficiently managing resources through sustainable and scalable innovations. In this regard, as emerging technologies like Artificial Intelligence (AI), the Internet of Things (IoT), big data analytics, and fog and edge computing have become increasingly prevalent, smart city applications grapple with various challenges, including the potential for unauthorized disclosure of confidential and sensitive data. The seamless integration of emerging technologies has played a vital role in sustaining the dynamic pace of their development. This paper explores the substantial potential and applications of Deep Learning (DL), Federated Learning (FL), IoT, Blockchain, Natural Language Processing (NLP), and large language models (LLMs) in optimizing ICT processes within smart cities. We aim to spo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#26469;&#33258;&#21160;&#35780;&#20272;&#36741;&#23548;&#21592;&#20351;&#29992;&#31038;&#20132;&#24773;&#24863;&#36741;&#23548;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#36741;&#23548;&#23454;&#36341;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.14594</link><description>&lt;p&gt;
&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25913;&#36827;&#36741;&#23548;&#23454;&#36341;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14594
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#26469;&#33258;&#21160;&#35780;&#20272;&#36741;&#23548;&#21592;&#20351;&#29992;&#31038;&#20132;&#24773;&#24863;&#36741;&#23548;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#36741;&#23548;&#23454;&#36341;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#23545;&#19968;&#36741;&#23548;&#26159;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#30340;&#26377;&#25928;&#25945;&#23398;&#26041;&#27861;&#65292;&#28982;&#32780;&#20854;&#25928;&#21147;&#21462;&#20915;&#20110;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#12290;&#26032;&#25163;&#25968;&#23398;&#36741;&#23548;&#21592;&#36890;&#24120;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#20869;&#23481;&#30340;&#25351;&#23548;&#65292;&#24573;&#35270;&#31038;&#20132;&#24773;&#24863;&#23398;&#20064;&#31561;&#26041;&#38754;&#12290;&#31038;&#20132;&#24773;&#24863;&#23398;&#20064;&#20419;&#36827;&#20102;&#20844;&#24179;&#21644;&#21253;&#23481;&#24615;&#65292;&#24182;&#22521;&#20859;&#19982;&#23398;&#29983;&#30340;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#23398;&#29983;&#25972;&#20307;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#26377;&#25928;&#22320;&#35780;&#20272;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#21487;&#20197;&#25512;&#21160;&#23450;&#21046;&#30340;&#36741;&#23548;&#21592;&#22521;&#35757;&#35745;&#21010;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#36741;&#23548;&#20013;&#35780;&#20272;&#26032;&#25163;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#21442;&#19982;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#21021;&#27493;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#33258;&#21160;&#35780;&#20272;&#36741;&#23548;&#21592;&#20351;&#29992;&#31038;&#20132;&#24773;&#24863;&#36741;&#23548;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25253;&#21578;&#20102;&#36130;&#21153;&#32500;&#24230;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14594v1 Announce Type: cross  Abstract: One-on-one tutoring is an effective instructional method for enhancing learning, yet its efficacy hinges on tutor competencies. Novice math tutors often prioritize content-specific guidance, neglecting aspects such as social-emotional learning. Social-emotional learning promotes equity and inclusion and nurturing relationships with students, which is crucial for holistic student development. Assessing the competencies of tutors accurately and efficiently can drive the development of tailored tutor training programs. However, evaluating novice tutor ability during real-time tutoring remains challenging as it typically requires experts-in-the-loop. To address this challenge, this preliminary study aims to harness Generative Pre-trained Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess tutors' ability of using social-emotional tutoring strategies. Moreover, this study also reports on the financial dimensions an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Q&#23398;&#20064;&#35206;&#30422;&#26102;&#38388;&#24863;&#30693;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#36710;&#36733;&#32593;&#32476;&#21644;HD&#22320;&#22270;&#26356;&#26032;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#20197;&#20811;&#26381;&#32593;&#32476;&#25317;&#22622;&#12290;</title><link>https://arxiv.org/abs/2402.14582</link><description>&lt;p&gt;
&#36890;&#36807;&#35206;&#30422;&#24863;&#30693;&#21644;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#39640;&#28165;&#22320;&#22270;&#26356;&#26032;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enhancement of High-definition Map Update Service Through Coverage-aware and Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Q&#23398;&#20064;&#35206;&#30422;&#26102;&#38388;&#24863;&#30693;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#36710;&#36733;&#32593;&#32476;&#21644;HD&#22320;&#22270;&#26356;&#26032;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#20197;&#20811;&#26381;&#32593;&#32476;&#25317;&#22622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#28165;&#65288;HD&#65289;&#22320;&#22270;&#31995;&#32479;&#23558;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#21040;&#26356;&#39640;&#27700;&#24179;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#24471;&#30410;&#20110;&#30456;&#27604;&#20256;&#32479;&#20108;&#32500;&#22320;&#22270;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#21019;&#24314;HD&#22320;&#22270;&#38656;&#35201;&#22823;&#37327;&#30340;&#36335;&#38754;&#21644;&#38750;&#36335;&#38754;&#25968;&#25454;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#21407;&#22987;&#25968;&#25454;&#38598;&#36890;&#36807;&#36710;&#36733;&#32593;&#32476;&#25910;&#38598;&#24182;&#19978;&#20256;&#21040;&#22522;&#20110;&#20113;&#30340;HD&#22320;&#22270;&#26381;&#21153;&#25552;&#20379;&#21830;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21160;&#24577;&#25299;&#25169;&#65292;&#36890;&#36807;&#36710;&#36733;&#26080;&#32447;&#36890;&#36947;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;&#38543;&#30528;&#36710;&#36742;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23545;&#26381;&#21153;&#36136;&#37327;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#25104;&#20026;&#21327;&#21516;&#39550;&#39542;&#20013;&#23454;&#26102;HD&#22320;&#22270;&#31995;&#32479;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Q&#23398;&#20064;&#35206;&#30422;&#26102;&#38388;&#24863;&#30693;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#36710;&#36733;&#32593;&#32476;&#21644;HD&#22320;&#22270;&#26356;&#26032;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#20197;&#20811;&#26381;&#32593;&#32476;&#25317;&#22622;&#12290;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#27169;&#25311;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14582v1 Announce Type: cross  Abstract: High-definition (HD) Map systems will play a pivotal role in advancing autonomous driving to a higher level, thanks to the significant improvement over traditional two-dimensional (2D) maps. Creating an HD Map requires a huge amount of on-road and off-road data. Typically, these raw datasets are collected and uploaded to cloud-based HD map service providers through vehicular networks. Nevertheless, there are challenges in transmitting the raw data over vehicular wireless channels due to the dynamic topology. As the number of vehicles increases, there is a detrimental impact on service quality, which acts as a barrier to a real-time HD Map system for collaborative driving in Autonomous Vehicles (AV). In this paper, to overcome network congestion, a Q-learning coverage-time-awareness algorithm is presented to optimize the quality of service for vehicular networks and HD map updates. The algorithm is evaluated in an environment that imita
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20449;&#26234;&#33021;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26550;&#26500;Savvy&#65292;&#36890;&#36807;&#28165;&#26224;&#20998;&#31163;&#25511;&#21046;&#24179;&#38754;&#21644;&#25968;&#25454;&#24179;&#38754;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#20248;&#20808;&#21407;&#21017;&#65292;&#20351;&#24471;&#22312;&#23433;&#20840;&#26102;&#38388;&#33539;&#22260;&#20869;&#23613;&#21487;&#33021;&#20248;&#21270;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.14580</link><description>&lt;p&gt;
&#31934;&#26126;&#65306;&#21487;&#20449;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Savvy: Trustworthy Autonomous Vehicles Architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14580
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20449;&#26234;&#33021;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26550;&#26500;Savvy&#65292;&#36890;&#36807;&#28165;&#26224;&#20998;&#31163;&#25511;&#21046;&#24179;&#38754;&#21644;&#25968;&#25454;&#24179;&#38754;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#20248;&#20808;&#21407;&#21017;&#65292;&#20351;&#24471;&#22312;&#23433;&#20840;&#26102;&#38388;&#33539;&#22260;&#20869;&#23613;&#21487;&#33021;&#20248;&#21270;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#21407;&#22240;&#22312;&#20110;&#21830;&#19994;&#12289;&#23433;&#20840;&#21644;&#24615;&#33021;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;AV&#26550;&#26500;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20381;&#36182;&#20110;AI&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#20294;&#30001;&#20110;&#33268;&#21629;&#20107;&#25925;&#30340;&#22686;&#22810;&#65292;&#38459;&#30861;&#20102;&#20840;&#38754;&#25512;&#24191;AV&#12290;&#36825;&#38656;&#35201;&#37325;&#26032;&#23457;&#35270;&#26500;&#24314;&#23433;&#20840;&#20851;&#38190;AV&#26550;&#26500;&#30340;&#22522;&#26412;&#21407;&#21017;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#21521;&#19981;&#24212;&#38459;&#27490;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#21147;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Savvy&#65292;&#19968;&#20010;&#26032;&#30340;&#21487;&#20449;&#26234;&#33021;AV&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#26368;&#20339;&#32467;&#21512;&#12290;Savvy&#22312;&#25511;&#21046;&#24179;&#38754;&#21644;&#25968;&#25454;&#24179;&#38754;&#20043;&#38388;&#36827;&#34892;&#28165;&#26224;&#20998;&#31163;&#65292;&#20197;&#20445;&#35777;&#20248;&#20808;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#21069;&#32773;&#36890;&#36807;&#35774;&#35745;&#26102;&#23450;&#20041;&#30340;&#35268;&#21017;&#26469;&#25215;&#25285;&#25511;&#21046;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#65292;&#21516;&#26102;&#21551;&#21160;&#21518;&#32773;&#20197;&#22312;&#23433;&#20840;&#26102;&#38388;&#30028;&#38480;&#20869;&#23613;&#21487;&#33021;&#20248;&#21270;&#20915;&#31574;&#12290;&#36890;&#36807;&#24341;&#23548;&#30340;&#26102;&#38388;&#24863;&#30693;&#39044;&#27979;&#36136;&#37327;&#38477;&#32423;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14580v1 Announce Type: new  Abstract: The increasing interest in Autonomous Vehicles (AV) is notable due to business, safety, and performance reasons. While there is salient success in recent AV architectures, hinging on the advancements in AI models, there is a growing number of fatal incidents that impedes full AVs from going mainstream. This calls for the need to revisit the fundamentals of building safety-critical AV architectures. However, this direction should not deter leveraging the power of AI. To this end, we propose Savvy, a new trustworthy intelligent AV architecture that achieves the best of both worlds. Savvy makes a clear separation between the control plane and the data plane to guarantee the safety-first principles. The former assume control to ensure safety using design-time defined rules, while launching the latter for optimizing decisions as much as possible within safety time-bounds. This is achieved through guided Time-aware predictive quality degradati
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21487;&#21464;&#25442;&#39640;&#26031;&#22870;&#21169;&#20989;&#25968;(TGRF)&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#22870;&#21169;&#35774;&#35745;&#22797;&#26434;&#12289;&#36229;&#21442;&#25968;&#20887;&#20313;&#21644;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.14569</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#30340;&#21487;&#21464;&#25442;&#39640;&#26031;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14569
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21487;&#21464;&#25442;&#39640;&#26031;&#22870;&#21169;&#20989;&#25968;(TGRF)&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#22870;&#21169;&#35774;&#35745;&#22797;&#26434;&#12289;&#36229;&#21442;&#25968;&#20887;&#20313;&#21644;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#24050;&#32463;&#20174;&#20248;&#20808;&#32771;&#34385;&#36991;&#38556;&#36716;&#21464;&#20026;&#37319;&#29992;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#31574;&#30053;&#26469;&#36866;&#24212;&#20154;&#31867;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#22312;&#21160;&#24577;&#20154;&#31867;&#20013;&#24515;&#29615;&#22659;&#20013;&#30340;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#30340;&#35748;&#30693;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#21464;&#24471;&#37325;&#35201;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25512;&#21160;&#20102;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#30340;&#36827;&#27493;&#65292;&#20294;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#23450;&#20041;&#36866;&#24403;&#30340;&#22870;&#21169;&#20989;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#22870;&#21169;&#22312;&#24341;&#23548;&#26426;&#22120;&#20154;&#34892;&#20026;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#26080;&#27861;&#33258;&#21160;&#35774;&#32622;&#30340;&#29305;&#24615;&#65292;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#12290;&#22823;&#37327;&#25163;&#24037;&#35774;&#35745;&#30340;&#22870;&#21169;&#24102;&#26469;&#20102;&#36229;&#21442;&#25968;&#20887;&#20313;&#12289;&#19981;&#24179;&#34913;&#20197;&#21450;&#26080;&#27861;&#20805;&#20998;&#34920;&#31034;&#29420;&#29305;&#23545;&#35937;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#21464;&#25442;&#39640;&#26031;&#22870;&#21169;&#20989;&#25968;&#65288;TGRF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14569v1 Announce Type: cross  Abstract: Robot navigation has transitioned from prioritizing obstacle avoidance to adopting socially aware navigation strategies that accommodate human presence. As a result, the recognition of socially aware navigation within dynamic human-centric environments has gained prominence in the field of robotics. Although reinforcement learning technique has fostered the advancement of socially aware navigation, defining appropriate reward functions, especially in congested environments, has posed a significant challenge. These rewards, crucial in guiding robot actions, demand intricate human-crafted design due to their complex nature and inability to be automatically set. The multitude of manually designed rewards poses issues with hyperparameter redundancy, imbalance, and inadequate representation of unique object characteristics. To address these challenges, we introduce a transformable gaussian reward function (TGRF). The TGRF significantly redu
&lt;/p&gt;</description></item><item><title>CLCE&#26041;&#27861;&#32467;&#21512;&#20102;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#21327;&#21516;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.14551</link><description>&lt;p&gt;
CLCE&#65306;&#19968;&#31181;&#20248;&#21270;&#23398;&#20064;&#34701;&#21512;&#30340;&#25913;&#36827;&#20132;&#21449;&#29109;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14551
&lt;/p&gt;
&lt;p&gt;
CLCE&#26041;&#27861;&#32467;&#21512;&#20102;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#21327;&#21516;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#27169;&#22411;&#20027;&#35201;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21021;&#22987;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#65288;CE&#65289;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;CE&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CLCE&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;CE&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#25345;&#20102;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;&#20197;&#21327;&#21516;&#26041;&#24335;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14551v1 Announce Type: cross  Abstract: State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14528</link><description>&lt;p&gt;
ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24573;&#35270;&#20102;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#21407;&#22987;&#34892;&#20026;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#21160;&#20316;&#32500;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#21407;&#22987;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#29109;&#39033;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#20248;&#20808;&#22788;&#29702;&#20855;&#26377;&#39640;&#28508;&#22312;&#24433;&#21709;&#30340;&#34892;&#21160;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38450;&#27490;&#23545;&#29305;&#23450;&#21407;&#22987;&#34892;&#20026;&#36807;&#24230;&#20851;&#27880;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65292;&#22312;&#36328;7&#20010;&#39046;&#22495;&#30340;29&#20010;&#19981;&#21516;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#30456;&#36739;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#65292;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClusterClip Sampling&#30340;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20026;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.14526</link><description>&lt;p&gt;
&#24102;&#32858;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24179;&#34913;&#25968;&#25454;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Balanced Data Sampling for Language Model Training with Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClusterClip Sampling&#30340;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20026;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#23613;&#31649;&#20154;&#20204;&#24050;&#32463;&#20851;&#27880;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#21644;&#32452;&#25104;&#65292;&#20294;&#30830;&#23450;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#25277;&#26679;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;LLM&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426;&#25277;&#26679;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25277;&#26679;&#31574;&#30053;&#24573;&#35270;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#24615;&#65292;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ClusterClip Sampling&#65292;&#20197;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ClusterClip Sampling&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#26469;&#21453;&#26144;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#26681;&#25454;&#32858;&#31867;&#32467;&#26524;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#24120;&#35265;&#26679;&#26412;&#21644;&#31232;&#26377;&#26679;&#26412;&#12290;&#24341;&#20837;&#20102;&#37325;&#22797;&#35009;&#21098;&#25805;&#20316;&#26469;&#20943;&#36731;&#30001;&#20110;&#26469;&#33258;&#26576;&#20123;&#32858;&#31867;&#30340;&#26679;&#26412;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;ClusterClip Sampling&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14526v1 Announce Type: cross  Abstract: Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperfo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#26080;&#32541;&#36866;&#24212;</title><link>https://arxiv.org/abs/2402.14505</link><description>&lt;p&gt;
&#20026;&#23454;&#29616;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26080;&#32541;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#30340;&#26080;&#32541;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#29992;&#36890;&#29992;&#30340;&#35270;&#35273;&#23398;&#20064;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#21487;&#20197;&#20026;&#21508;&#31181;&#35270;&#35273;&#24863;&#30693;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#23581;&#35797;&#21033;&#29992;&#22312;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#65288;VPR&#65289;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#30001;&#20110;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;VPR&#20219;&#21153;&#20043;&#38388;&#22312;&#35757;&#32451;&#30446;&#26631;&#21644;&#25968;&#25454;&#26041;&#38754;&#30340;&#22266;&#26377;&#24046;&#24322;&#65292;&#22914;&#20309;&#24357;&#21512;&#24046;&#36317;&#24182;&#20805;&#20998;&#21457;&#25381;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;VPR&#20013;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;VPR&#30340;&#26080;&#32541;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28151;&#21512;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#23616;&#21644;&#23616;&#37096;&#36866;&#24212;&#65292;&#20174;&#32780;&#33719;&#24471;&#26082;&#20851;&#27880;&#26174;&#33879;&#22320;&#26631;&#29992;&#20110;&#21306;&#20998;&#22320;&#28857;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14505v1 Announce Type: cross  Abstract: Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to gu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30896;&#25758;&#24863;&#30693;&#30340;&#30005;&#32518;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;CG-CNN&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#31283;&#20581;&#30005;&#32518;&#25235;&#21462;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14498</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#30896;&#25758;&#24863;&#30693;&#30005;&#32518;&#25235;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Collision-Aware Cable Grasping Method in Cluttered Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30896;&#25758;&#24863;&#30693;&#30340;&#30005;&#32518;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;CG-CNN&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#31283;&#20581;&#30005;&#32518;&#25235;&#21462;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#31283;&#20581;&#30005;&#32518;&#25235;&#21462;&#32780;&#35774;&#35745;&#30340;&#30896;&#25758;&#24863;&#30693;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#21033;&#29992;&#29289;&#29702;&#20223;&#30495;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20010;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#30005;&#32518;&#25235;&#21462;&#30340;&#22797;&#26434;&#24615;&#65292;&#32771;&#34385;&#21040;&#30005;&#32518;&#19982;&#26426;&#22120;&#20154;&#22841;&#29226;&#20043;&#38388;&#30340;&#28508;&#22312;&#30896;&#25758;&#12290;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#20984;&#20998;&#35299;&#25216;&#26415;&#26469;&#20998;&#26512;&#38750;&#20984;&#30005;&#32518;&#27169;&#22411;&#65292;&#26681;&#25454;&#27169;&#25311;&#25235;&#21462;&#23581;&#35797;&#33258;&#21160;&#26631;&#35760;&#25235;&#21462;&#36136;&#37327;&#12290;&#21033;&#29992;&#36825;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#23545;CG-CNN&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24182;&#36890;&#36807;&#22495;&#38543;&#26426;&#21270;&#25216;&#26415;&#22686;&#24378;&#12290;&#38543;&#21518;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#39044;&#27979;&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#23558;&#26368;&#20339;&#25235;&#21462;&#23039;&#21183;&#25351;&#23548;&#32473;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#25191;&#34892;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25235;&#21462;&#25928;&#26524;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;&#30001;&#20110;&#25105;&#20204;&#27169;&#22411;&#38544;&#24335;&#30340;&#30896;&#25758;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#25104;&#21151;&#29575;&#65292;&#23545;&#20110;&#24050;&#30693;&#30005;&#32518;&#20026;92.3%&#65292;&#23545;&#20110;&#26410;&#30693;&#30005;&#32518;&#20026;88.4%&#65292;&#36229;&#36234;&#20102;c&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14498v1 Announce Type: cross  Abstract: We introduce a Cable Grasping-Convolutional Neural Network designed to facilitate robust cable grasping in cluttered environments. Utilizing physics simulations, we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers. We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts. The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques. Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot controller for execution. Grasping efficacy is assessed across both synthetic and real-world settings. Given our model implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing c
&lt;/p&gt;</description></item><item><title>INSTRAUG&#26159;&#19968;&#31181;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#27169;&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.14492</link><description>&lt;p&gt;
INSTRAUG&#65306;&#29992;&#20110;&#22810;&#27169;&#25351;&#20196;&#24494;&#35843;&#30340;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14492
&lt;/p&gt;
&lt;p&gt;
INSTRAUG&#26159;&#19968;&#31181;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#27169;&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20219;&#21153;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#23545;&#26032;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#26368;&#36817;&#20851;&#20110;&#39640;&#36136;&#37327;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#29983;&#25104;&#21644;&#36873;&#25321;&#30340;&#24037;&#20316;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#20197;&#20026;&#32473;&#23450;&#20219;&#21153;&#26500;&#24605;&#27169;&#22411;&#21487;&#29702;&#35299;&#30340;&#25351;&#20196;&#65292;&#24182;&#35880;&#24910;&#36807;&#28388;LLM&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;INSTRAUG&#30340;&#22810;&#27169;&#20219;&#21153;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#12290;&#23427;&#20174;&#19968;&#20123;&#22522;&#26412;&#21644;&#31616;&#21333;&#30340;&#20803;&#25351;&#20196;&#24320;&#22987;&#65292;&#20294;&#33021;&#23558;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#25193;&#22823;30&#20493;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22810;&#27169;&#25351;&#20196;&#36319;&#38543;&#22522;&#20934;&#27979;&#35797;&#38598;MULTIINSTRUCT&#21644;InstructBLIP&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;INSTRAUG&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#36328;12&#20010;&#22810;&#27169;&#20219;&#21153;&#30340;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#23545;&#40784;&#65292;&#29978;&#33267;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14492v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks. Recent works about high-quality instruction-following data generation and selection require amounts of human labor to conceive model-understandable instructions for the given tasks and carefully filter the LLM-generated data. In this work, we introduce an automatic instruction augmentation method named INSTRAUG in multimodal tasks. It starts from a handful of basic and straightforward meta instructions but can expand an instruction-following dataset by 30 times. Results on two popular multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show that INSTRAUG can significantly improve the alignment of multimodal large language models (MLLMs) across 12 multimodal tasks, which is even equivalent to the benefits of scaling up training 
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22312;&#38544;&#34255;&#21160;&#20316;&#27169;&#22411;&#19979;&#30340;&#21512;&#21516;&#19982;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#20960;&#20046;&#26368;&#20248;&#30340;&#26377;&#30028;&#21512;&#21516;&#65292;&#23545;&#20110;&#19968;&#33324;&#24773;&#20917;&#30340;&#26597;&#35810;&#27425;&#25968;&#20855;&#26377;&#22810;&#39033;&#24335;&#19978;&#30028;&#65292;&#24182;&#19988;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.14486</link><description>&lt;p&gt;
&#36793;&#30028;&#21512;&#21516;&#26159;&#21542;&#21487;&#23398;&#20064;&#24182;&#36817;&#20284;&#26368;&#20248;?
&lt;/p&gt;
&lt;p&gt;
Are Bounded Contracts Learnable and Approximately Optimal?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14486
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22312;&#38544;&#34255;&#21160;&#20316;&#27169;&#22411;&#19979;&#30340;&#21512;&#21516;&#19982;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#20960;&#20046;&#26368;&#20248;&#30340;&#26377;&#30028;&#21512;&#21516;&#65292;&#23545;&#20110;&#19968;&#33324;&#24773;&#20917;&#30340;&#26597;&#35810;&#27425;&#25968;&#20855;&#26377;&#22810;&#39033;&#24335;&#19978;&#30028;&#65292;&#24182;&#19988;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#30340;&#38544;&#34255;&#21160;&#20316;&#27169;&#22411;&#65292;&#20854;&#20013;&#22996;&#25176;&#26041;&#36890;&#36807;&#21512;&#21516;&#28608;&#21169;&#20195;&#29702;&#20154;&#25353;&#21512;&#21516;&#24320;&#23637;&#39033;&#30446;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#26377;&#30028;&#25903;&#20184;&#30340;&#21512;&#21516;&#26159;&#21542;&#21487;&#23398;&#20064;&#24182;&#36817;&#20284;&#26368;&#20248;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#20004;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#30028;&#30340;&#26597;&#35810;&#27425;&#25968;&#20869;&#25214;&#21040;&#20960;&#20046;&#26368;&#20248;&#30340;&#26377;&#30028;&#21512;&#21516;&#65292;&#22522;&#20110;&#25991;&#29486;&#20013;&#30340;&#20004;&#20010;&#26631;&#20934;&#20551;&#35774;&#65306;&#20195;&#29702;&#20154;&#30340;&#26356;&#26114;&#36149;&#30340;&#34892;&#21160;&#23548;&#33268;&#22996;&#25176;&#26041;&#30340;&#26356;&#22909;&#30340;&#32467;&#26524;&#20998;&#24067;&#65292;&#24182;&#19988;&#20195;&#29702;&#20154;&#30340;&#25104;&#26412;/&#21162;&#21147;&#20855;&#26377;&#36882;&#20943;&#22238;&#25253;&#12290;&#25105;&#20204;&#30340;&#22810;&#39033;&#24335;&#26597;&#35810;&#22797;&#26434;&#24230;&#19978;&#30028;&#34920;&#26126;&#65292;&#26631;&#20934;&#20551;&#35774;&#36275;&#20197;&#23454;&#29616;&#23545;&#19968;&#33324;&#24773;&#20917;&#24050;&#30693;&#19979;&#30028;&#30340;&#25351;&#25968;&#25913;&#36827;&#12290;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#20381;&#36182;&#20110;&#23545;&#21512;&#21516;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14486v1 Announce Type: cross  Abstract: This paper considers the hidden-action model of the principal-agent problem, in which a principal incentivizes an agent to work on a project using a contract. We investigate whether contracts with bounded payments are learnable and approximately optimal. Our main results are two learning algorithms that can find a nearly optimal bounded contract using a polynomial number of queries, under two standard assumptions in the literature: a costlier action for the agent leads to a better outcome distribution for the principal, and the agent's cost/effort has diminishing returns. Our polynomial query complexity upper bound shows that standard assumptions are sufficient for achieving an exponential improvement upon the known lower bound for general instances. Unlike the existing algorithms, which relied on discretizing the contract space, our algorithms directly learn the underlying outcome distributions. As for the approximate optimality of bo
&lt;/p&gt;</description></item><item><title>&#20010;&#24615;&#21270;&#34892;&#20026;&#24863;&#30693;Transformer&#26694;&#26550;&#29992;&#20110;&#22810;&#34892;&#20026;&#39034;&#24207;&#25512;&#33616;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#35299;&#20915;&#30701;&#24207;&#21015;&#19979;&#25512;&#33616;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14473</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#34892;&#20026;&#24863;&#30693;Transformer&#29992;&#20110;&#22810;&#34892;&#20026;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14473
&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#34892;&#20026;&#24863;&#30693;Transformer&#26694;&#26550;&#29992;&#20110;&#22810;&#34892;&#20026;&#39034;&#24207;&#25512;&#33616;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#35299;&#20915;&#30701;&#24207;&#21015;&#19979;&#25512;&#33616;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation (SR)&#36890;&#36807;&#24314;&#27169;&#29992;&#25143;&#22312;&#29289;&#21697;&#20043;&#38388;&#36716;&#25442;&#30340;&#26041;&#24335;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#21160;&#24577;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#20165;&#21033;&#29992;&#21333;&#19968;&#31867;&#22411;&#30340;&#34892;&#20026;&#20132;&#20114;&#25968;&#25454;&#30340;SR&#27169;&#22411;&#22312;&#24207;&#21015;&#36739;&#30701;&#26102;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20851;&#27880;&#22810;&#34892;&#20026;&#39034;&#24207;&#25512;&#33616;(MBSR)&#65292;&#26088;&#22312;&#21033;&#29992;&#26102;&#21464;&#24322;&#26500;&#34892;&#20026;&#20381;&#36182;&#20851;&#31995;&#26356;&#22909;&#22320;&#25506;&#32034;&#29992;&#25143;&#22312;&#30446;&#26631;&#34892;&#20026;&#19978;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#35299;&#20915;MBSR&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20010;&#20154;&#29305;&#24449;&#65292;&#29992;&#25143;&#23637;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#22810;&#34892;&#20026;&#27169;&#24335;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#34892;&#20026;&#30456;&#20851;&#24615;&#21644;&#29289;&#21697;&#21327;&#20316;&#20043;&#38388;&#23384;&#22312;&#20840;&#38754;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#20854;&#24378;&#24230;&#28145;&#21463;&#26102;&#38388;&#22240;&#32032;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;MBSR&#38382;&#39064;&#30340;Personalized Behavior-Aware Transformer&#26694;&#26550;(PBAT)&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#24314;&#27169;&#20010;&#24615;&#21270;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14473v1 Announce Type: cross  Abstract: Sequential Recommendation (SR) captures users' dynamic preferences by modeling how users transit among items. However, SR models that utilize only single type of behavior interaction data encounter performance degradation when the sequences are short. To tackle this problem, we focus on Multi-Behavior Sequential Recommendation (MBSR) in this paper, which aims to leverage time-evolving heterogeneous behavioral dependencies for better exploring users' potential intents on the target behavior. Solving MBSR is challenging. On the one hand, users exhibit diverse multi-behavior patterns due to personal characteristics. On the other hand, there exists comprehensive co-influence between behavior correlations and item collaborations, the intensity of which is deeply affected by temporal factors. To tackle these challenges, we propose a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem, which models personalized patterns 
&lt;/p&gt;</description></item><item><title>&#20027;&#21160;&#25512;&#26029;&#29702;&#35770;&#22522;&#20110;&#39044;&#26399;&#33258;&#30001;&#33021;&#65292;&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#32479;&#19968;&#26681;&#39044;&#26399;&#33258;&#30001;&#33021;&#30340;&#23450;&#20041;&#26469;&#25512;&#23548;&#22235;&#31181;&#20844;&#24335;&#65292;&#30740;&#31350;&#20102;&#20004;&#31181;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#38480;&#21046;&#20195;&#29702;&#23545;&#35266;&#23519;&#20855;&#26377;&#20219;&#24847;&#20808;&#39564;&#20559;&#22909;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.14460</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#39044;&#26399;&#33258;&#30001;&#33021;&#65306;&#22235;&#31181;&#20844;&#24335;&#20197;&#21450;&#19968;&#31181;&#32479;&#19968;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Reframing the Expected Free Energy: Four Formulations and a Unification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14460
&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#26029;&#29702;&#35770;&#22522;&#20110;&#39044;&#26399;&#33258;&#30001;&#33021;&#65292;&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#32479;&#19968;&#26681;&#39044;&#26399;&#33258;&#30001;&#33021;&#30340;&#23450;&#20041;&#26469;&#25512;&#23548;&#22235;&#31181;&#20844;&#24335;&#65292;&#30740;&#31350;&#20102;&#20004;&#31181;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#38480;&#21046;&#20195;&#29702;&#23545;&#35266;&#23519;&#20855;&#26377;&#20219;&#24847;&#20808;&#39564;&#20559;&#22909;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14460v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20027;&#21160;&#25512;&#26029;&#26159;&#24863;&#30693;&#12289;&#23398;&#20064;&#21644;&#20915;&#31574;&#30340;&#19968;&#31181;&#39046;&#20808;&#29702;&#35770;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#24515;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#12290;&#20027;&#21160;&#25512;&#26029;&#22522;&#20110;&#39044;&#26399;&#33258;&#30001;&#33021;&#65292;&#20854;&#20027;&#35201;&#30001;&#30452;&#35273;&#21487;&#20449;&#24230;&#26469;&#35777;&#26126;&#65292;&#20363;&#22914;&#39118;&#38505;&#21152;&#19981;&#30830;&#23450;&#24615;&#12289;&#20449;&#24687;&#22686;&#30410;/&#23454;&#29992;&#20215;&#20540;&#30340;&#20844;&#24335;&#12290;&#26412;&#25991;&#26088;&#22312;&#24418;&#24335;&#21270;&#20174;&#21333;&#19968;&#26681;&#39044;&#26399;&#33258;&#30001;&#33021;&#23450;&#20041;&#20013;&#25512;&#23548;&#36825;&#20123;&#20844;&#24335;&#30340;&#38382;&#39064;&#65292;&#21363;&#32479;&#19968;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#35774;&#32622;&#65292;&#27599;&#31181;&#35774;&#32622;&#37117;&#26377;&#33258;&#24049;&#30340;&#26681;&#39044;&#26399;&#33258;&#30001;&#33021;&#23450;&#20041;&#12290;&#22312;&#31532;&#19968;&#20010;&#35774;&#32622;&#20013;&#65292;&#36804;&#20170;&#20026;&#27490;&#24182;&#26410;&#25552;&#20986;&#39044;&#26399;&#33258;&#30001;&#33021;&#30340;&#29702;&#30001;&#65292;&#20294;&#21487;&#20197;&#20174;&#20013;&#24674;&#22797;&#25152;&#26377;&#20844;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#20195;&#29702;&#26080;&#27861;&#23545;&#35266;&#23519;&#20855;&#26377;&#20219;&#24847;&#30340;&#20808;&#39564;&#20559;&#22909;&#12290;&#23454;&#38469;&#19978;&#65292;&#21482;&#26377;&#19968;&#31867;&#26377;&#38480;&#30340;&#20808;&#39564;&#20559;&#22909;&#19982;&#35266;&#23519;&#26159;&#20860;&#23481;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14460v1 Announce Type: new  Abstract: Active inference is a leading theory of perception, learning and decision making, which can be applied to neuroscience, robotics, psychology, and machine learning. Active inference is based on the expected free energy, which is mostly justified by the intuitive plausibility of its formulations, e.g., the risk plus ambiguity and information gain / pragmatic value formulations. This paper seek to formalize the problem of deriving these formulations from a single root expected free energy definition, i.e., the unification problem. Then, we study two settings, each one having its own root expected free energy definition. In the first setting, no justification for the expected free energy has been proposed to date, but all the formulations can be recovered from it. However, in this setting, the agent cannot have arbitrary prior preferences over observations. Indeed, only a limited class of prior preferences over observations is compatible wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26550;&#26500;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#30340;&#26377;&#25928;&#26041;&#27861;&#12289;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#35821;&#26009;&#24211;&#20197;&#21450;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#35770;&#35777;&#26550;&#26500;&#30340;&#22522;&#32447;&#21644;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14458</link><description>&lt;p&gt;
NLAS-multi&#65306;&#19968;&#20010;&#22810;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26550;&#26500;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NLAS-multi: A Multilingual Corpus of Automatically Generated Natural Language Argumentation Schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26550;&#26500;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#30340;&#26377;&#25928;&#26041;&#27861;&#12289;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#35821;&#26009;&#24211;&#20197;&#21450;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#35770;&#35777;&#26550;&#26500;&#30340;&#22522;&#32447;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35770;&#35777;&#25366;&#25496;&#12289;&#35770;&#35777;&#29983;&#25104;&#21644;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20998;&#26512;&#39046;&#22495;&#65292;&#19968;&#20123;&#20027;&#35201;&#38480;&#21046;&#28041;&#21450;&#27880;&#37322;&#23500;&#26377;&#35770;&#35777;&#24615;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12289;&#36825;&#20123;&#35821;&#26009;&#24211;&#30340;&#26377;&#38480;&#35268;&#27169;&#65292;&#20197;&#21450;&#20195;&#34920;&#36827;&#34892;&#27880;&#37322;&#30340;&#19981;&#21516;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20197;&#19979;&#36129;&#29486;&#65306;(i) &#22312;&#19981;&#21516;&#20027;&#39064;&#21644;&#35821;&#35328;&#20013;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#30340;&#26377;&#25928;&#26041;&#27861;&#35770;&#65292;(ii) &#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#26550;&#26500;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;(iii) &#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#35770;&#35777;&#26550;&#26500;&#30340;&#19968;&#32452;&#21487;&#38752;&#22522;&#32447;&#21644;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14458v1 Announce Type: cross  Abstract: Some of the major limitations identified in the areas of argument mining, argument generation, and natural language argument analysis are related to the complexity of annotating argumentatively rich data, the limited size of these corpora, and the constraints that represent the different languages and domains in which these data is annotated. To address these limitations, in this paper we present the following contributions: (i) an effective methodology for the automatic generation of natural language arguments in different topics and languages, (ii) the largest publicly available corpus of natural language argumentation schemes, and (iii) a set of solid baselines and fine-tuned models for the automatic identification of argumentation schemes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#35821;&#35328;&#27169;&#22411;&#27010;&#24565;&#24341;&#23548;&#26694;&#26550;&#25193;&#23637;&#21040;&#26356;&#20016;&#23500;&#30340;&#27010;&#24565;&#38598;&#65292;&#25506;&#32034;&#24403;&#21069;&#26816;&#27979;&#21644;&#24341;&#23548;&#31574;&#30053;&#22312;&#36866;&#24403;&#24615;&#12289;&#24189;&#40664;&#12289;&#21019;&#36896;&#21147;&#21644;&#36136;&#37327;&#31561;&#25361;&#25112;&#24615;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14433</link><description>&lt;p&gt;
&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#28508;&#22312;&#31354;&#38388;&#30340;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Language Model's Guide Through Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#35821;&#35328;&#27169;&#22411;&#27010;&#24565;&#24341;&#23548;&#26694;&#26550;&#25193;&#23637;&#21040;&#26356;&#20016;&#23500;&#30340;&#27010;&#24565;&#38598;&#65292;&#25506;&#32034;&#24403;&#21069;&#26816;&#27979;&#21644;&#24341;&#23548;&#31574;&#30053;&#22312;&#36866;&#24403;&#24615;&#12289;&#24189;&#40664;&#12289;&#21019;&#36896;&#21147;&#21644;&#36136;&#37327;&#31561;&#25361;&#25112;&#24615;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#24341;&#23548;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24265;&#20215;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#30340;&#27010;&#24565;&#21521;&#37327;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#23427;&#20204;&#26469;&#25200;&#21160;&#28608;&#27963;&#65292;&#20174;&#32780;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#23558;&#21069;&#20154;&#24037;&#20316;&#30340;&#37325;&#28857;&#20174;&#30495;&#23454;&#24615;&#25193;&#23637;&#21040;&#20102;&#26356;&#20016;&#23500;&#30340;&#27010;&#24565;&#38598;&#65292;&#22914;&#24688;&#24403;&#24615;&#12289;&#24189;&#40664;&#12289;&#21019;&#36896;&#21147;&#21644;&#36136;&#37327;&#65292;&#25506;&#32034;&#24403;&#21069;&#26816;&#27979;&#21644;&#24341;&#23548;&#31574;&#30053;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#30340;&#24037;&#20316;&#31243;&#24230;&#12290;&#20026;&#20102;&#26041;&#20415;&#35780;&#20272;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32771;&#34385;&#27010;&#24565;&#24341;&#23548;&#25104;&#21151;&#31243;&#24230;&#20197;&#21450;&#24341;&#23548;&#27169;&#22411;&#27969;&#30021;&#24615;&#28508;&#22312;&#36864;&#21270;&#30340;&#26032;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#19968;&#20123;&#27010;&#24565;&#22914;&#30495;&#23454;&#24615;&#26356;&#23481;&#26131;&#36890;&#36807;&#24403;&#21069;&#25216;&#26415;&#36827;&#34892;&#24341;&#23548;&#65292;&#20294;&#20687;&#24688;&#24403;&#24615;&#25110;&#24189;&#40664;&#36825;&#26679;&#30340;&#26032;&#27010;&#24565;&#20173;&#28982;&#38590;&#20197;&#24341;&#20986;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14433v1 Announce Type: cross  Abstract: Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time. While the focus of previous work has largely been on truthfulness, in this paper we extend this framework to a richer set of concepts such as appropriateness, humor, creativity and quality, and explore to what degree current detection and guidance strategies work in these challenging settings. To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model. Our extensive experiments reveal that while some concepts such as truthfulness more easily allow for guidance with current techniques, novel concepts such as appropriateness or humor either remain difficult to elicit, need extensive 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;KoCoSa&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14428</link><description>&lt;p&gt;
KoCoSa: &#38889;&#25991;&#19978;&#19979;&#25991;&#24863;&#30693;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KoCoSa: Korean Context-aware Sarcasm Detection Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;KoCoSa&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#26159;&#19968;&#31181;&#35328;&#35821;&#35773;&#21050;&#30340;&#26041;&#24335;&#65292;&#25351;&#30340;&#26159;&#26377;&#20154;&#35828;&#20102;&#21644;&#20182;&#20204;&#30340;&#26412;&#24847;&#30456;&#21453;&#30340;&#35805;&#65292;&#36890;&#24120;&#26159;&#20026;&#20102;&#22066;&#31505;&#19968;&#20010;&#20154;&#12289;&#24773;&#20917;&#25110;&#24819;&#27861;&#12290;&#26816;&#27979;&#23545;&#35805;&#20013;&#30340;&#35773;&#21050;&#36890;&#24120;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#26816;&#27979;&#35773;&#21050;&#24212;&#35813;&#21453;&#26144;&#19978;&#19979;&#25991;&#65288;&#21363;&#23545;&#35805;&#21382;&#21490;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#26032;&#25968;&#25454;&#38598;KoCoSa&#65288;&#38889;&#25991;&#19978;&#19979;&#25991;&#24863;&#30693;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#65289;&#65292;&#21253;&#25324;12.8K&#20010;&#26085;&#24120;&#38889;&#25991;&#23545;&#35805;&#20197;&#21450;&#35813;&#20219;&#21153;&#22312;&#26368;&#21518;&#19968;&#27425;&#22238;&#22797;&#19978;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26500;&#24314;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65306;1&#65289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#28304;&#23545;&#35805;&#20013;&#29983;&#25104;&#26032;&#30340;&#35773;&#21050;&#23545;&#35805;&#65292;2&#65289;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#28388;&#24322;&#24120;&#21644;&#26377;&#27602;&#23545;&#35805;&#65292;3&#65289;&#20026;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#38024;&#23545;&#38889;&#25991;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#22522;&#32447;&#65292;&#35813;&#22522;&#32447;&#26159;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14428v1 Announce Type: cross  Abstract: Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on t
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#22270;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#24515;&#29702;&#23398;&#20551;&#35774;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#32852;&#21512;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.14424</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#24515;&#29702;&#23398;&#20551;&#35774;&#29983;&#25104;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14424
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#22270;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#24515;&#29702;&#23398;&#20551;&#35774;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#32852;&#21512;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#29983;&#25104;&#24515;&#29702;&#23398;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;LLM&#20998;&#26512;&#20102;43,312&#31687;&#24515;&#29702;&#23398;&#25991;&#31456;&#65292;&#25552;&#21462;&#20102;&#22240;&#26524;&#20851;&#31995;&#23545;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#24515;&#29702;&#23398;&#30340;&#22240;&#26524;&#22270;&#12290;&#24212;&#29992;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;130&#20010;&#20851;&#27880;&#8220;&#24184;&#31119;&#8221;&#30340;&#28508;&#22312;&#24515;&#29702;&#23398;&#20551;&#35774;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21338;&#22763;&#23398;&#32773;&#26500;&#24605;&#30340;&#30740;&#31350;&#24819;&#27861;&#21644;&#20165;&#30001;LLM&#20135;&#29983;&#30340;&#24819;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;LLM&#21644;&#22240;&#26524;&#22270;&#30340;&#32852;&#21512;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#26041;&#38754;&#19982;&#19987;&#23478;&#27700;&#24179;&#30340;&#27934;&#23519;&#21147;&#20445;&#25345;&#19968;&#33268;&#65292;&#26126;&#26174;&#20248;&#20110;&#20165;LLM&#30340;&#20551;&#35774;&#65288;&#20998;&#21035;&#20026;t(59)=3.34&#65292;p=0.007&#21644;t(59)=4.32&#65292;p&lt;0.001&#65289;&#12290;&#36825;&#31181;&#19968;&#33268;&#24615;&#36827;&#19968;&#27493;&#36890;&#36807;&#28145;&#24230;&#35821;&#20041;&#20998;&#26512;&#24471;&#21040;&#35777;&#23454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;LLM&#19982;&#22240;&#26524;&#22270;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#24515;&#29702;&#23398;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14424v1 Announce Type: new  Abstract: Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs. This analysis produced a specialized causal graph for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on `well-being', then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM. Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) = 4.32, p&lt;0.001, respectively). This alignment was further corroborated using deep semantic analysis. Our results show that combining LLM with machine learning techniques such as causal k
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#22522;&#20934;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#34701;&#20837;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25581;&#31034;&#20102;&#20934;&#30830;&#24615;&#26368;&#39640;&#30340;&#27169;&#22411;&#21487;&#33021;&#20063;&#20855;&#26377;&#26368;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14418</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Evaluation for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14418
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#22522;&#20934;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#34701;&#20837;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25581;&#31034;&#20102;&#20934;&#30830;&#24615;&#26368;&#39640;&#30340;&#27169;&#22411;&#21487;&#33021;&#20063;&#20855;&#26377;&#26368;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;GPT-4&#12289;LLaVA&#21644;CogVLM&#36825;&#26679;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22240;&#22312;&#20960;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#23545;&#20110;&#20840;&#38754;&#35780;&#20272;VLMs&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30095;&#24573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#34701;&#20837;&#21040;&#35780;&#20272;VLMs&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;20&#22810;&#20010;VLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#35780;&#20272;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#30340;5&#20010;&#25968;&#25454;&#38598;&#19978;&#26816;&#39564;&#20102;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#20854;&#20934;&#30830;&#24615;&#19981;&#19968;&#33268;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#20934;&#30830;&#24615;&#26368;&#39640;&#30340;&#27169;&#22411;&#21487;&#33021;&#20063;&#20855;&#26377;&#26368;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#35777;&#23454;&#20102;&#20026;VLMs&#27979;&#37327;&#20854;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#36824;&#25581;&#31034;&#20102;&#19968;&#31181;&#30456;&#20851;&#24615;&#65292;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14418v1 Announce Type: cross  Abstract: Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs.   Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities.   Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;RALMs&#23545;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#26469;&#28304;&#38388;&#30340;&#20914;&#31361;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#20250;&#20559;&#21521;&#38169;&#35823;&#30340;&#20869;&#37096;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2402.14409</link><description>&lt;p&gt;
&#30693;&#35782;&#20043;&#38388;&#30340;&#25289;&#38191;&#25112;: &#25506;&#32034;&#21644;&#35299;&#20915;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;RALMs&#23545;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#26469;&#28304;&#38388;&#30340;&#20914;&#31361;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#20250;&#20559;&#21521;&#38169;&#35823;&#30340;&#20869;&#37096;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#24050;&#32463;&#22312;&#36890;&#36807;&#20174;&#22806;&#37096;&#26469;&#28304;&#26816;&#32034;&#35777;&#25454;&#26469;&#20248;&#21270;&#21644;&#25193;&#23637;&#20854;&#20869;&#37096;&#35760;&#24518;&#26041;&#38754;&#34920;&#29616;&#20986;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;RALMs&#22312;&#23558;&#20869;&#37096;&#35760;&#24518;&#19982;&#22806;&#37096;&#26469;&#28304;&#25972;&#21512;&#26102;&#24517;&#28982;&#20250;&#36935;&#21040;&#30693;&#35782;&#20914;&#31361;&#12290;&#30693;&#35782;&#20914;&#31361;&#20250;&#20351;RALMs&#38519;&#20837;&#30693;&#35782;&#20043;&#38388;&#30340;&#25289;&#38191;&#25112;&#65292;&#38480;&#21046;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#25506;&#32034;&#21644;&#35299;&#20915;RALMs&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#32500;&#24230;&#19978;&#30340;&#30693;&#35782;&#20914;&#31361;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20197;&#19979;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;RALMs&#30340;&#34892;&#20026;&#21644;&#20559;&#22909;&#65306;&#65288;1&#65289;&#20869;&#37096;&#35760;&#24518;&#19982;&#22806;&#37096;&#26469;&#28304;&#20043;&#38388;&#30340;&#20914;&#31361;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#37011;&#23425;-&#20811;&#40065;&#26684;&#25928;&#24212;&#30340;&#22686;&#24378;&#65292;&#26356;&#24378;&#22823;&#30340;RALMs&#20250;&#25345;&#32493;&#20559;&#29233;&#20854;&#38169;&#35823;&#30340;&#20869;&#37096;&#35760;&#24518;&#65292;&#21363;&#20351;&#25552;&#20379;&#20102;&#27491;&#30830;&#30340;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;RALMs&#36824;&#34920;&#29616;&#20986;&#19968;&#31181;&#21487;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14409v1 Announce Type: cross  Abstract: Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#26377;&#20851;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#35813;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14404</link><description>&lt;p&gt;
&#22312;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#26512;&#27010;&#24565;&#34920;&#36798;&#65306;&#20511;&#21161;&#21453;&#21521;&#35789;&#20856;&#25506;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14404
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#26377;&#20851;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#35813;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#26597;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#26410;&#35299;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25506;&#26597;LLMs&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#35821;&#35328;&#25551;&#36848;&#20013;&#26263;&#31034;&#30340;&#23545;&#35937;&#27010;&#24565;&#30340;&#26415;&#35821;&#12290;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#31283;&#20581;&#22320;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#20851;&#20110;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#22312;&#21477;&#27861;&#27867;&#21270;&#34892;&#20026;&#19978;&#34920;&#29616;&#30456;&#20284;&#12290;&#25506;&#32034;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#20351;&#29992;&#25551;&#36848;$\Rightarrow$&#21333;&#35789;&#31034;&#20363;&#21487;&#33021;&#20250;&#35825;&#23548;&#20986;&#36229;&#36234;&#20219;&#21153;&#26500;&#22411;&#34920;&#38754;&#24046;&#24322;&#30340;&#27867;&#21270;&#65292;&#24182;&#20419;&#36827;&#27169;&#22411;&#23545;&#26356;&#24191;&#27867;&#30340;&#20849;&#21516;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14404v1 Announce Type: cross  Abstract: Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sliver&#30340;&#28369;&#21160;&#31383;&#21475;&#25968;&#25454;&#27969;&#35774;&#35745;&#33539;&#24335;&#65292;&#36890;&#36807;&#20943;&#23567;&#31383;&#21475;&#22823;&#23567;&#21644;&#23454;&#29616;&#28369;&#21160;&#31383;&#21475;&#26469;&#35299;&#20915;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26631;&#31614;&#30340;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.14399</link><description>&lt;p&gt;
&#30830;&#20445;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#65306;&#19968;&#31181;&#26032;&#30340;&#28369;&#21160;&#31383;&#21475;&#25968;&#25454;&#27969;&#33539;&#24335;&#29992;&#20110;&#23454;&#26102;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream Paradigm for Live Streaming Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sliver&#30340;&#28369;&#21160;&#31383;&#21475;&#25968;&#25454;&#27969;&#35774;&#35745;&#33539;&#24335;&#65292;&#36890;&#36807;&#20943;&#23567;&#31383;&#21475;&#22823;&#23567;&#21644;&#23454;&#29616;&#28369;&#21160;&#31383;&#21475;&#26469;&#35299;&#20915;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26631;&#31614;&#30340;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Live streaming recommender system&#19987;&#20026;&#21521;&#29992;&#25143;&#25512;&#33616;&#23454;&#26102;&#24863;&#20852;&#36259;&#30340;&#30452;&#25773;&#27969;&#32780;&#35774;&#35745;&#12290;&#30001;&#20110;&#30452;&#25773;&#20869;&#23481;&#21160;&#24577;&#21464;&#21270;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#21450;&#26102;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#27969;&#35774;&#35745;&#33539;&#24335;&#65292;&#21517;&#20026;Sliver&#65292;&#36890;&#36807;&#20943;&#23567;&#31383;&#21475;&#22823;&#23567;&#21644;&#30456;&#24212;&#23454;&#29616;&#28369;&#21160;&#31383;&#21475;&#26469;&#35299;&#20915;&#26631;&#31614;&#30340;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14399v1 Announce Type: cross  Abstract: Live streaming recommender system is specifically designed to recommend real-time live streaming of interest to users. Due to the dynamic changes of live content, improving the timeliness of the live streaming recommender system is a critical problem. Intuitively, the timeliness of the data determines the upper bound of the timeliness that models can learn. However, none of the previous works addresses the timeliness problem of the live streaming recommender system from the perspective of data stream design. Employing the conventional fixed window data stream paradigm introduces a trade-off dilemma between labeling accuracy and timeliness. In this paper, we propose a new data stream design paradigm, dubbed Sliver, that addresses the timeliness and accuracy problem of labels by reducing the window size and implementing a sliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco strategy reducing the latency between 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36880;&#27493;&#27531;&#20313;&#23545;&#40784;&#30340;&#21452;&#27969;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#36880;&#28176;&#23558;&#32454;&#33410;&#27880;&#20837;&#21040;&#37325;&#26500;&#21644;&#32534;&#36753;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#32454;&#33410;&#20445;&#30041;&#21644;&#32534;&#36753;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14398</link><description>&lt;p&gt;
&#36880;&#28176;&#27531;&#20313;&#23545;&#40784;&#65306;GAN&#21453;&#28436;&#21644;&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#30340;&#21452;&#27969;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36880;&#27493;&#27531;&#20313;&#23545;&#40784;&#30340;&#21452;&#27969;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#36880;&#28176;&#23558;&#32454;&#33410;&#27880;&#20837;&#21040;&#37325;&#26500;&#21644;&#32534;&#36753;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#32454;&#33410;&#20445;&#30041;&#21644;&#32534;&#36753;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GAN&#22522;&#30784;&#30340;&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#39318;&#20808;&#21033;&#29992;GAN&#21453;&#28436;&#23558;&#30495;&#23454;&#22270;&#20687;&#25237;&#24433;&#21040;GAN&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#25805;&#20316;&#30456;&#24212;&#30340;&#28508;&#22312;&#20195;&#30721;&#12290;&#26368;&#36817;&#30340;&#21453;&#28436;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#39069;&#22806;&#30340;&#39640;&#27604;&#29305;&#29305;&#24449;&#26469;&#25552;&#39640;&#22270;&#20687;&#32454;&#33410;&#30340;&#20445;&#30041;&#65292;&#22240;&#20026;&#20302;&#27604;&#29305;&#20195;&#30721;&#26080;&#27861;&#24544;&#23454;&#37325;&#26500;&#28304;&#22270;&#20687;&#65292;&#23548;&#33268;&#32454;&#33410;&#20002;&#22833;&#12290;&#28982;&#32780;&#65292;&#22312;&#32534;&#36753;&#36807;&#31243;&#20013;&#65292;&#29616;&#26377;&#24037;&#20316;&#26410;&#33021;&#20934;&#30830;&#34917;&#20805;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#24182;&#19988;&#22312;&#32534;&#36753;&#24615;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#23427;&#20204;&#19968;&#27425;&#24615;&#27880;&#20837;&#25152;&#26377;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36825;&#22312;&#26412;&#36136;&#19978;&#23548;&#33268;&#32454;&#33410;&#30340;&#20301;&#32622;&#21644;&#25968;&#37327;&#36807;&#24230;&#25311;&#21512;&#28304;&#22270;&#20687;&#65292;&#23548;&#33268;&#32534;&#36753;&#21518;&#30340;&#22270;&#20687;&#20013;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#20869;&#23481;&#21644;&#20266;&#24433;&#12290;&#35813;&#24037;&#20316;&#35748;&#20026;&#24212;&#35813;&#20197;&#22810;&#38454;&#27573;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#36880;&#28176;&#23558;&#32454;&#33410;&#27880;&#20837;&#21040;&#37325;&#26500;&#21644;&#32534;&#36753;&#36807;&#31243;&#20013;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32454;&#33410;&#20445;&#30041;&#21644;&#39640;&#32534;&#36753;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14398v1 Announce Type: cross  Abstract: GAN-based image attribute editing firstly leverages GAN Inversion to project real images into the latent space of GAN and then manipulates corresponding latent codes. Recent inversion methods mainly utilize additional high-bit features to improve image details preservation, as low-bit codes cannot faithfully reconstruct source images, leading to the loss of details. However, during editing, existing works fail to accurately complement the lost details and suffer from poor editability. The main reason is they inject all the lost details indiscriminately at one time, which inherently induces the position and quantity of details to overfit source images, resulting in inconsistent content and artifacts in edited images. This work argues that details should be gradually injected into both the reconstruction and editing process in a multi-stage coarse-to-fine manner for better detail preservation and high editability. Therefore, a novel dual
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DepL&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#23398;&#20064;&#32534;&#25490;&#65292;&#33021;&#22815;&#30830;&#20445;&#20197;&#26368;&#20302;&#35757;&#32451;&#25104;&#26412;&#36798;&#21040;&#30446;&#26631;&#23398;&#20064;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14346</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#20998;&#24067;&#24335;&#21387;&#32553;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Dependable Distributed Training of Compressed Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DepL&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#23398;&#20064;&#32534;&#25490;&#65292;&#33021;&#22815;&#30830;&#20445;&#20197;&#26368;&#20302;&#35757;&#32451;&#25104;&#26412;&#36798;&#21040;&#30446;&#26631;&#23398;&#20064;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#29616;&#26377;&#24037;&#20316;&#19968;&#30452;&#24573;&#35270;&#20102;&#23454;&#29616;&#23398;&#20064;&#36136;&#37327;&#30340;&#20998;&#24067;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#20854;&#24179;&#22343;&#20540;&#12290; &#36825;&#23548;&#33268;&#20102;&#25152;&#24471;ML&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24046;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#27604;&#39044;&#26399;&#30340;&#35201;&#24046;&#24471;&#22810;&#12290; &#25105;&#20204;&#36890;&#36807;&#25552;&#20986;DepL&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#23398;&#20064;&#32534;&#25490;&#26694;&#26550;&#65292;&#33021;&#22815;&#23601;&#65288;i&#65289;&#29992;&#20110;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#65288;ii&#65289;&#35201;&#20351;&#29992;&#30340;&#27169;&#22411;&#21450;&#20309;&#26102;&#22312;&#23427;&#20204;&#20043;&#38388;&#20999;&#25442;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#35201;&#21033;&#29992;&#30340;&#33410;&#28857;&#38598;&#32676;&#21450;&#20854;&#36164;&#28304;&#20570;&#20986;&#39640;&#36136;&#37327;&#39640;&#25928;&#30340;&#20915;&#31574;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#21487;&#33021;&#30340;&#21487;&#29992;&#27169;&#22411;&#20026;&#23436;&#25972;&#30340;DNN&#21450;&#20854;&#21387;&#32553;&#29256;&#26412;&#12290; &#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;DepL&#20445;&#35777;&#20197;&#30446;&#26631;&#27010;&#29575;&#23454;&#29616;&#30446;&#26631;&#23398;&#20064;&#36136;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25104;&#26412;&#26368;&#20302;&#12290; &#25105;&#20204;&#35777;&#26126;DepL&#20855;&#26377;&#24120;&#25968;&#31454;&#20105;&#27604;&#29575;&#21644;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14346v1 Announce Type: cross  Abstract: The existing work on the distributed training of machine learning (ML) models has consistently overlooked the distribution of the achieved learning quality, focusing instead on its average value. This leads to a poor dependability}of the resulting ML models, whose performance may be much worse than expected. We fill this gap by proposing DepL, a framework for dependable learning orchestration, able to make high-quality, efficient decisions on (i) the data to leverage for learning, (ii) the models to use and when to switch among them, and (iii) the clusters of nodes, and the resources thereof, to exploit. For concreteness, we consider as possible available models a full DNN and its compressed versions. Unlike previous studies, DepL guarantees that a target learning quality is reached with a target probability, while keeping the training cost at a minimum. We prove that DepL has constant competitive ratio and polynomial complexity, and s
&lt;/p&gt;</description></item><item><title>HyperFast&#26159;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#21363;&#26102;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#38656;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#39640;&#24230;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14335</link><description>&lt;p&gt;
&#36229;&#24555;&#36895;&#65306;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#21363;&#26102;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperFast: Instant Classification for Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14335
&lt;/p&gt;
&lt;p&gt;
HyperFast&#26159;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#21363;&#26102;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#38656;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#39640;&#24230;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#31561;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#26159;&#22823;&#22810;&#25968;&#34920;&#26684;&#25968;&#25454;&#24212;&#29992;&#30340;&#39318;&#36873;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#35201;&#20040;&#20165;&#36866;&#29992;&#20110;&#22312;&#26377;&#38480;&#35774;&#32622;&#19979;&#30340;&#29609;&#20855;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HyperFast&#65292;&#19968;&#20010;&#20026;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#31435;&#21363;&#20998;&#31867;&#34920;&#26684;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#20803;&#35757;&#32451;&#30340;&#36229;&#32593;&#32476;&#12290;HyperFast&#29983;&#25104;&#19968;&#20010;&#38024;&#23545;&#26410;&#35265;&#25968;&#25454;&#38598;&#23450;&#21046;&#30340;&#29305;&#23450;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#30452;&#25509;&#29992;&#20110;&#20998;&#31867;&#25512;&#26029;&#65292;&#26080;&#38656;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;OpenML&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23558;HyperFast&#19982;&#31454;&#20105;&#24615;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#12289;&#20256;&#32479;ML&#26041;&#27861;&#12289;AutoML&#31995;&#32479;&#21644;&#25552;&#21319;&#26426;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;HyperFast&#23637;&#29616;&#20986;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14335v1 Announce Type: cross  Abstract: Training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming. Meanwhile, traditional machine learning methods like gradient-boosting algorithms remain the preferred choice for most tabular data applications, while neural network alternatives require extensive hyperparameter tuning or work only in toy datasets under limited settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork designed for instant classification of tabular data in a single forward pass. HyperFast generates a task-specific neural network tailored to an unseen dataset that can be directly used for classification inference, removing the need for training a model. We report extensive experiments with OpenML and genomic data, comparing HyperFast to competing tabular data neural networks, traditional ML methods, AutoML systems, and boosting machines. HyperFast shows highly competitive results, wh
&lt;/p&gt;</description></item><item><title>RepoGenix&#29420;&#29305;&#34701;&#21512;&#31867;&#27604;&#19978;&#19979;&#25991;&#21644;&#29702;&#24615;&#19978;&#19979;&#25991;&#65292;&#24182;&#25552;&#20986;&#20102;&#25130;&#26029;&#25490;&#21517;&#29983;&#25104;&#65288;RTG&#65289;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#30340;&#20934;&#30830;&#24615;&#32780;&#19981;&#29306;&#29298;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14323</link><description>&lt;p&gt;
REPOFUSE&#65306;&#20855;&#26377;&#34701;&#21512;&#21452;&#37325;&#19978;&#19979;&#25991;&#30340;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
REPOFUSE: Repository-Level Code Completion with Fused Dual Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14323
&lt;/p&gt;
&lt;p&gt;
RepoGenix&#29420;&#29305;&#34701;&#21512;&#31867;&#27604;&#19978;&#19979;&#25991;&#21644;&#29702;&#24615;&#19978;&#19979;&#25991;&#65292;&#24182;&#25552;&#20986;&#20102;&#25130;&#26029;&#25490;&#21517;&#29983;&#25104;&#65288;RTG&#65289;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#30340;&#20934;&#30830;&#24615;&#32780;&#19981;&#29306;&#29298;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#36741;&#21161;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#25512;&#21160;&#20102;&#25552;&#20986;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#20316;&#20026;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25163;&#27573;&#65292;&#21033;&#29992;&#25972;&#20010;&#20195;&#30721;&#24211;&#30340;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#22686;&#21152;&#25512;&#29702;&#24310;&#36831;&#65292;&#28508;&#22312;&#22320;&#25439;&#23475;&#24320;&#21457;&#32773;&#20307;&#39564;&#24182;&#22952;&#30861;&#24037;&#20855;&#30340;&#37319;&#29992;-&#36825;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;&#19978;&#19979;&#25991;-&#24310;&#36831;&#22256;&#22659;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; RepoGenix&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#20179;&#24211;&#32423;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#32780;&#26080;&#38656;&#24310;&#36831;&#25240;&#34935;&#30340;&#24320;&#21019;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;RepoGenix &#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#19978;&#19979;&#25991;&#65306;&#26681;&#26893;&#20110;&#20195;&#30721;&#31867;&#27604;&#30340;&#31867;&#27604;&#19978;&#19979;&#25991;&#21644;&#21253;&#21547;&#28145;&#24230;&#35821;&#20041;&#20851;&#31995;&#30340;&#29702;&#24615;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25130;&#26029;&#25490;&#21517;&#29983;&#25104;&#65288;RTG&#65289;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#38480;&#21046;&#22823;&#23567;&#30340;&#25552;&#31034;&#12290;&#36825;&#20351;&#24471; RepoGenix &#33021;&#22815;&#22312;&#20445;&#25345;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#20379;&#31934;&#30830;&#30340;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14323v1 Announce Type: cross  Abstract: The success of language models in code assistance has spurred the proposal of repository-level code completion as a means to enhance prediction accuracy, utilizing the context from the entire codebase. However, this amplified context can inadvertently increase inference latency, potentially undermining the developer experience and deterring tool adoption-a challenge we termed the Context-Latency Conundrum. This paper introduces RepoGenix, a pioneering solution designed to enhance repository-level code completion without the latency trade-off. RepoGenix uniquely fuses two types of contexts: the analogy context, rooted in code analogies, and the rationale context, which encompasses in-depth semantic relationships. We propose a novel rank truncated generation (RTG) technique that efficiently condenses these contexts into prompts with restricted size. This enables RepoGenix to deliver precise code completions while maintaining inference ef
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>VLN&#20316;&#20026;&#23454;&#29616;&#23454;&#20307;&#26234;&#33021;&#30340;&#20851;&#38190;&#30740;&#31350;&#36335;&#24452;&#65292;&#33268;&#21147;&#20110;&#25506;&#32034;&#22914;&#20309;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#26377;&#25928;&#20132;&#27969;&#65292;&#23454;&#29616;&#20934;&#30830;&#23548;&#33322;&#65292;&#24182;&#34701;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.14304</link><description>&lt;p&gt;
&#20855;&#26377;&#23454;&#20307;&#26234;&#33021;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Navigation with Embodied Intelligence: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14304
&lt;/p&gt;
&lt;p&gt;
VLN&#20316;&#20026;&#23454;&#29616;&#23454;&#20307;&#26234;&#33021;&#30340;&#20851;&#38190;&#30740;&#31350;&#36335;&#24452;&#65292;&#33268;&#21147;&#20110;&#25506;&#32034;&#22914;&#20309;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#26377;&#25928;&#20132;&#27969;&#65292;&#23454;&#29616;&#20934;&#30830;&#23548;&#33322;&#65292;&#24182;&#34701;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#38271;&#26399;&#24895;&#26223;&#65292;&#23454;&#20307;&#26234;&#33021;&#30340;&#26680;&#24515;&#30446;&#26631;&#26159;&#25552;&#39640;&#20195;&#29702;&#20154;&#21644;&#29615;&#22659;&#30340;&#24863;&#30693;&#12289;&#29702;&#35299;&#21644;&#20132;&#20114;&#33021;&#21147;&#12290;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20316;&#20026;&#23454;&#29616;&#23454;&#20307;&#26234;&#33021;&#30340;&#20851;&#38190;&#30740;&#31350;&#36335;&#24452;&#20043;&#19968;&#65292;&#33268;&#21147;&#20110;&#25506;&#32034;&#20195;&#29702;&#20154;&#22914;&#20309;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26377;&#25928;&#22320;&#19982;&#20154;&#31867;&#20132;&#27969;&#65292;&#25509;&#25910;&#24182;&#29702;&#35299;&#25351;&#20196;&#65292;&#24182;&#26368;&#32456;&#20381;&#36182;&#35270;&#35273;&#20449;&#24687;&#23454;&#29616;&#20934;&#30830;&#23548;&#33322;&#12290;VLN&#34701;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#12290;&#23613;&#31649;&#38754;&#20020;&#25216;&#26415;&#25361;&#25112;&#65292;&#20294;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#65292;&#20363;&#22914;&#20154;&#26426;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#35821;&#35328;&#29702;&#35299;&#21040;&#34892;&#21160;&#25191;&#34892;&#30340;&#22797;&#26434;&#36807;&#31243;&#65292;VLN&#38754;&#20020;&#23558;&#35270;&#35273;&#20449;&#24687;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#12289;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14304v1 Announce Type: cross  Abstract: As a long-term vision in the field of artificial intelligence, the core goal of embodied intelligence is to improve the perception, understanding, and interaction capabilities of agents and the environment. Vision-language navigation (VLN), as a critical research path to achieve embodied intelligence, focuses on exploring how agents use natural language to communicate effectively with humans, receive and understand instructions, and ultimately rely on visual information to achieve accurate navigation. VLN integrates artificial intelligence, natural language processing, computer vision, and robotics. This field faces technical challenges but shows potential for application such as human-computer interaction. However, due to the complex process involved from language understanding to action execution, VLN faces the problem of aligning visual information and language instructions, improving generalization ability, and many other challenge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SpaceAgents-1&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#20998;&#23618;&#24322;&#26500;&#22810;&#20195;&#29702;&#21327;&#20316;&#26550;&#26500;&#65292;&#22312;&#24494;&#37325;&#21147;&#29615;&#22659;&#19979;&#23398;&#20064;&#20154;&#31867;&#19982;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.14299</link><description>&lt;p&gt;
&#25105;&#20204;&#36873;&#25321;&#21435;&#22826;&#31354;&#65306;&#24494;&#37325;&#21147;&#19979;&#30340;&#20154;&#31867;&#19982;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#30340;&#39537;&#21160;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
We Choose to Go to Space: Agent-driven Human and Multi-Robot Collaboration in Microgravity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14299
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SpaceAgents-1&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#20998;&#23618;&#24322;&#26500;&#22810;&#20195;&#29702;&#21327;&#20316;&#26550;&#26500;&#65292;&#22312;&#24494;&#37325;&#21147;&#29615;&#22659;&#19979;&#23398;&#20064;&#20154;&#31867;&#19982;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SpaceAgents-1&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#24494;&#37325;&#21147;&#26465;&#20214;&#19979;&#23398;&#20064;&#20154;&#31867;&#19982;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#65288;HMRC&#65289;&#31574;&#30053;&#30340;&#31995;&#32479;&#12290;&#26410;&#26469;&#30340;&#22826;&#31354;&#25506;&#32034;&#38656;&#35201;&#20154;&#31867;&#19982;&#26426;&#22120;&#20154;&#20849;&#21516;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#22320;&#38754;&#23454;&#39564;&#23460;&#20013;&#33719;&#24471;&#29087;&#32451;&#30340;&#26426;&#22120;&#20154;&#25216;&#33021;&#21644;&#22312;&#24494;&#37325;&#21147;&#26465;&#20214;&#19979;&#30340;&#29087;&#32451;&#21327;&#20316;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24494;&#37325;&#21147;&#27169;&#25311;&#29615;&#22659;&#65292;&#24182;&#23637;&#31034;&#20102;&#19977;&#31181;&#20856;&#22411;&#30340;&#33329;&#20869;&#26426;&#22120;&#20154;&#37197;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#24322;&#26500;&#22810;&#20195;&#29702;&#21327;&#20316;&#26550;&#26500;&#65306;&#22312;&#22522;&#30784;&#27169;&#22411;&#30340;&#25351;&#23548;&#19979;&#65292;&#19968;&#20010;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#20316;&#20026;&#20154;&#26426;&#21327;&#20316;&#30340;&#20219;&#21153;&#35268;&#21010;&#32773;&#65292;&#32780;&#21508;&#20010;&#25216;&#33021;&#19987;&#23478;&#20195;&#29702;&#31649;&#29702;&#26426;&#22120;&#20154;&#30340;&#23454;&#20307;&#25511;&#21046;&#12290;&#36825;&#31181;&#26426;&#21046;&#20351;SpaceAgents-1&#31995;&#32479;&#33021;&#22815;&#25191;&#34892;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#38271;&#26399;&#35270;&#37326;HMRC&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14299v1 Announce Type: cross  Abstract: We present SpaceAgents-1, a system for learning human and multi-robot collaboration (HMRC) strategies under microgravity conditions. Future space exploration requires humans to work together with robots. However, acquiring proficient robot skills and adept collaboration under microgravity conditions poses significant challenges within ground laboratories. To address this issue, we develop a microgravity simulation environment and present three typical configurations of intra-cabin robots. We propose a hierarchical heterogeneous multi-agent collaboration architecture: guided by foundation models, a Decision-Making Agent serves as a task planner for human-robot collaboration, while individual Skill-Expert Agents manage the embodied control of robots. This mechanism empowers the SpaceAgents-1 system to execute a range of intricate long-horizon HMRC tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#32531;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14279</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#20943;&#32531;&#35821;&#35328;&#24046;&#24322;&#65292;&#23454;&#29616;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14279
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#32531;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#21892;&#22810;&#35821;&#35328;&#29702;&#35299;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#65292;&#20381;&#36182;&#22797;&#26434;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#20551;&#35774;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21463;&#21040;&#36825;&#20123;&#35821;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65288;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;&#38899;&#32032;&#20316;&#20026;&#36755;&#20837;&#26631;&#35760;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#26159;&#23376;&#35789;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;&#23450;&#37327;&#35777;&#25454;&#23637;&#31034;&#20102;&#38899;&#32032;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#23545;&#36328;&#35821;&#35328;&#24615;&#33021;&#24046;&#36317;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14279v1 Announce Type: cross  Abstract: Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and -- importantly -- struggle with significant performance gaps between high-resource and low-resource languages. We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords). We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;GATE X-E&#25361;&#25112;&#38598;&#65292;&#21253;&#21547;&#20102;&#20174;&#22303;&#32819;&#20854;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#33452;&#20848;&#35821;&#21644;&#27874;&#26031;&#35821;&#32763;&#35793;&#25104;&#33521;&#35821;&#30340;&#20154;&#31867;&#32763;&#35793;&#65292;&#26088;&#22312;&#35780;&#20272;&#24369;&#24615;&#21035;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#25552;&#20986;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.14277</link><description>&lt;p&gt;
GATE X-E&#65306;&#24369;&#24615;&#21035;&#35821;&#35328;&#30340;&#24615;&#21035;&#20844;&#24179;&#32763;&#35793;&#25361;&#25112;&#38598;
&lt;/p&gt;
&lt;p&gt;
GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14277
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;GATE X-E&#25361;&#25112;&#38598;&#65292;&#21253;&#21547;&#20102;&#20174;&#22303;&#32819;&#20854;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#33452;&#20848;&#35821;&#21644;&#27874;&#26031;&#35821;&#32763;&#35793;&#25104;&#33521;&#35821;&#30340;&#20154;&#31867;&#32763;&#35793;&#65292;&#26088;&#22312;&#35780;&#20272;&#24369;&#24615;&#21035;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#25552;&#20986;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14277v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#36136;&#37327;&#21644;&#37319;&#29992;&#19978;&#25345;&#32493;&#25913;&#21892;&#65292;&#20294;&#24615;&#21035;&#20559;&#35265;&#30340;&#26080;&#24847;&#24310;&#32493;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#20174;&#24369;&#24615;&#21035;&#35821;&#35328;&#32763;&#35793;&#25104;&#33521;&#35821;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#29992;&#20110;&#35780;&#20272;&#36825;&#19968;&#29616;&#35937;&#25110;&#35780;&#20272;&#32531;&#35299;&#31574;&#30053;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GATE X-E&#65292;&#36825;&#26159;GATE&#65288;Rarrick&#31561;&#20154;&#65292;2023&#65289;&#35821;&#26009;&#24211;&#30340;&#25193;&#23637;&#65292;&#30001;&#20154;&#31867;&#32763;&#35793;&#32452;&#25104;&#65292;&#20174;&#22303;&#32819;&#20854;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#33452;&#20848;&#35821;&#21644;&#27874;&#26031;&#35821;&#32763;&#35793;&#25104;&#33521;&#35821;&#12290;&#27599;&#31181;&#32763;&#35793;&#37117;&#38468;&#26377;&#22899;&#24615;&#12289;&#30007;&#24615;&#21644;&#20013;&#24615;&#21464;&#20307;&#12290;&#35813;&#25968;&#25454;&#38598;&#27599;&#31181;&#35821;&#35328;&#23545;&#20043;&#38388;&#21253;&#21547;1250&#21040;1850&#20010;&#23454;&#20363;&#65292;&#21253;&#21547;&#20855;&#26377;&#21508;&#31181;&#21477;&#23376;&#38271;&#24230;&#21644;&#39046;&#22495;&#30340;&#33258;&#28982;&#21477;&#23376;&#65292;&#25361;&#25112;&#30528;&#32763;&#35793;&#37325;&#20889;&#32773;&#22312;&#21508;&#31181;&#35821;&#35328;&#29616;&#35937;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;GPT&#26500;&#24314;&#30340;&#32763;&#35793;&#24615;&#21035;&#37325;&#20889;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14277v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies on gender bias in translations into English from weakly gendered-languages, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present a translation gender rewriting solution built with GPT
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#31185;&#23398;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#32469;&#36807;&#29983;&#25104;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#27493;&#39588;&#65292;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14268</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Detect Misinformation in Scientific News Reporting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14268
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#31185;&#23398;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#32469;&#36807;&#29983;&#25104;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#27493;&#39588;&#65292;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20107;&#23454;&#32463;&#24120;&#34987;&#22312;&#27969;&#34892;&#23186;&#20307;&#20013;&#25805;&#32437;&#65292;&#24847;&#22270;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#21644;&#34892;&#21160;&#65292;&#27491;&#22914;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#25152;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20004;&#31181;&#23186;&#20307;&#31867;&#22411;&#30340;&#20889;&#20316;&#39118;&#26684;&#26377;&#30528;&#26126;&#26174;&#19981;&#21516;&#65292;&#24182;&#19988;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#30740;&#31350;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#26816;&#27979;&#31185;&#23398;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14268v1 Announce Type: cross  Abstract: Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Copilot&#35780;&#20272;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#24341;&#23548;&#30340;IDE&#20132;&#20114;&#65292;&#22312;&#21508;&#31181;&#32534;&#31243;&#22330;&#26223;&#21644;&#35821;&#35328;&#20013;&#25552;&#20379;&#26356;&#20026;&#31283;&#20581;&#21644;&#20449;&#24687;&#23494;&#38598;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.14261</link><description>&lt;p&gt;
Copilot&#35780;&#20272;&#24037;&#20855;&#65306;&#35780;&#20272;LLM&#24341;&#23548;&#30340;&#36719;&#20214;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Copilot&#35780;&#20272;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#24341;&#23548;&#30340;IDE&#20132;&#20114;&#65292;&#22312;&#21508;&#31181;&#32534;&#31243;&#22330;&#26223;&#21644;&#35821;&#35328;&#20013;&#25552;&#20379;&#26356;&#20026;&#31283;&#20581;&#21644;&#20449;&#24687;&#23494;&#38598;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#24320;&#21457;&#29615;&#22659;&#65288;IDEs&#65289;&#24050;&#25104;&#20026;&#29616;&#20195;&#36719;&#20214;&#24320;&#21457;&#30340;&#28966;&#28857;&#12290;LLMs&#65292;&#22914;OpenAI GPT-3.5/4&#21644;Code Llama&#65292;&#33021;&#22815;&#20316;&#20026;&#26234;&#33021;&#30340;&#12289;&#22522;&#20110;&#32842;&#22825;&#30340;&#32534;&#31243;&#21161;&#25163;&#65292;&#26174;&#33879;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;LLMs&#21487;&#33021;&#24182;&#19981;&#36866;&#29992;&#20110;&#20219;&#20309;&#22330;&#26223;&#12290;&#30456;&#21453;&#65292;&#27599;&#20010;&#31995;&#32479;&#37117;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#20197;&#36866;&#24212;&#20854;&#21551;&#21457;&#24335;&#38598;&#65292;&#20197;&#30830;&#20445;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;Copilot&#35780;&#20272;&#24037;&#20855;&#65306;&#19968;&#22871;&#29992;&#20110;&#35780;&#20272;LLM&#24341;&#23548;&#30340;IDE&#20132;&#20114;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#65292;&#28085;&#30422;&#21508;&#31181;&#32534;&#31243;&#22330;&#26223;&#21644;&#35821;&#35328;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#35780;&#20272;&#31995;&#32479;&#26356;&#20026;&#31283;&#20581;&#21644;&#20449;&#24687;&#23494;&#38598;&#12290;&#25105;&#20204;&#20026;&#28085;&#30422;&#24191;&#27867;&#30340;&#24320;&#21457;&#20154;&#21592;&#20219;&#21153;&#33539;&#22260;&#30340;&#24773;&#26223;&#35774;&#35745;&#24182;&#35745;&#31639;&#20102;&#38745;&#24577;&#21644;&#22522;&#20110;&#25191;&#34892;&#30340;&#25104;&#21151;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14261v1 Announce Type: cross  Abstract: The integration of Large Language Models (LLMs) into Development Environments (IDEs) has become a focal point in modern software development. LLMs such as OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants. However, utilizing LLMs out of the box is unlikely to be optimal for any given scenario. Rather, each system requires the LLM to be honed to its set of heuristics to ensure the best performance. In this paper, we introduce the Copilot evaluation harness: a set of data and tools for evaluating LLM-guided IDE interactions, covering various programming scenarios and languages. We propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems. We design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.14259</link><description>&lt;p&gt;
&#21333;&#35789;&#24207;&#21015;&#29109;&#65306;&#36208;&#21521;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#24212;&#29992;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22312;&#30830;&#20445;&#23433;&#20840;&#20851;&#38190;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#21487;&#38752;&#24615;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#23578;&#26410;&#24314;&#31435;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#26080;&#20851;&#30340;&#35789;&#27719;&#21644;&#35821;&#24207;&#21547;&#26377;&#26377;&#38480;&#30340;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#36825;&#26159;&#30001;&#20110;&#29983;&#25104;&#19981;&#24179;&#31561;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#35821;&#20041;&#30456;&#20851;&#24615;&#22312;&#21333;&#35789;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#27604;&#20363;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26102;&#26356;&#21152;&#24378;&#35843;&#20851;&#38190;&#35789;&#21644;&#26356;&#30456;&#20851;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;5&#20010;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;&#21033;&#29992;7&#31181;&#8220;&#29616;&#25104;&#30340;&#8221;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;WSE&#19982;6&#31181;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;WSE&#22312;&#24615;&#33021;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14259v1 Announce Type: cross  Abstract: Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE exhibits superior performance on ac
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#25552;&#20379;&#33258;&#21160;&#20559;&#22909;&#21453;&#39304;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#26524;</title><link>https://arxiv.org/abs/2402.14245</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#22686;&#24378;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14245
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#25552;&#20379;&#33258;&#21160;&#20559;&#22909;&#21453;&#39304;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23558;&#30001;LLMs&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#25351;&#20196;&#19982;&#25191;&#34892;&#25152;&#38656;&#30340;&#21521;&#37327;&#21270;&#25805;&#20316;&#23545;&#40784;&#65292;&#24120;&#24120;&#38656;&#35201;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#32454;&#33410;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#36825;&#31181;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#32454;&#24494;&#20043;&#22788;&#30340;&#38656;&#27714;&#65292;&#21463;&#21040;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;LLMs&#25552;&#20379;&#33258;&#21160;&#20559;&#22909;&#21453;&#39304;&#65292;&#20165;&#20174;&#22270;&#20687;&#36755;&#20837;&#20013;&#24341;&#23548;&#20915;&#31574;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#21517;&#20026;CriticGPT&#30340;&#22810;&#27169;&#24577;LLM&#65292;&#33021;&#22815;&#29702;&#35299;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#36712;&#36857;&#35270;&#39057;&#65292;&#20316;&#20026;&#19968;&#20010;&#35780;&#35770;&#21592;&#25552;&#20379;&#20998;&#26512;&#21644;&#20559;&#22909;&#21453;&#39304;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#22870;&#21169;&#24314;&#27169;&#30340;&#35282;&#24230;&#39564;&#35777;&#20102;CriticGPT&#29983;&#25104;&#30340;&#20559;&#22909;&#26631;&#31614;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14245v1 Announce Type: cross  Abstract: Recently, there has been considerable attention towards leveraging large language models (LLMs) to enhance decision-making processes. However, aligning the natural language text instructions generated by LLMs with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details. To circumvent the need for such task-specific granularity, inspired by preference-based policy learning approaches, we investigate the utilization of multimodal LLMs to provide automated preference feedback solely from image inputs to guide decision-making. In this study, we train a multimodal LLM, termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback. Subsequently, we validate the effectiveness of preference labels generated by CriticGPT from a reward modeling perspective. Experimental evaluation of the a
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.14244</link><description>&lt;p&gt;
MENTOR&#65306;&#22312;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14244
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#20026;&#26234;&#33021;&#20307;&#30340;&#22797;&#26434;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#24182;&#20381;&#27425;&#23436;&#25104;&#30340;&#23618;&#27425;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#38590;&#20197;&#25214;&#21040;&#36866;&#24403;&#30340;&#23376;&#30446;&#26631;&#26469;&#30830;&#20445;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#25972;&#21512;&#21040;&#20854;&#20013;&#65288;MENTOR&#65289;&#12290;MENTOR&#20805;&#24403;&#8220;&#23548;&#24072;&#8221;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#39640;&#23618;&#31574;&#30053;&#23398;&#20064;&#20013;&#65292;&#20197;&#25214;&#21040;&#26356;&#22909;&#30340;&#23376;&#30446;&#26631;&#12290;&#33267;&#20110;&#20302;&#23618;&#31574;&#30053;&#65292;MENTOR&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#31574;&#30053;&#20197;&#20998;&#21035;&#36827;&#34892;&#25506;&#32034;-&#24320;&#21457;&#35299;&#32806;&#65292;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20154;&#31867;&#21487;&#20197;&#31616;&#21333;&#22320;&#23558;&#20219;&#21153;&#25286;&#20998;&#25104;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14244v1 Announce Type: new  Abstract: Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21387;&#21147;&#22270;&#20851;&#38190;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;Encoder-Fuser-Decoder&#65288;EFD&#65289;&#27169;&#22411;&#21644;&#20998;&#31867;&#21040;&#22238;&#24402;&#26435;&#37325;&#36716;&#31227;&#65288;CRWT&#65289;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20154;&#20307;&#20851;&#38190;&#28857;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14241</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#30417;&#30563;&#21387;&#21147;&#22270;&#20154;&#20307;&#20851;&#38190;&#28857;&#26816;&#27979;&#26041;&#27861;&#65306;&#36328;&#25968;&#25454;&#38598;&#20248;&#21270;&#27867;&#21270;&#21644;&#35745;&#31639;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
A Self-supervised Pressure Map human keypoint Detection Approch: Optimizing Generalization and Computational Efficiency Across Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14241
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21387;&#21147;&#22270;&#20851;&#38190;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;Encoder-Fuser-Decoder&#65288;EFD&#65289;&#27169;&#22411;&#21644;&#20998;&#31867;&#21040;&#22238;&#24402;&#26435;&#37325;&#36716;&#31227;&#65288;CRWT&#65289;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20154;&#20307;&#20851;&#38190;&#28857;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;RGB&#22270;&#20687;&#26080;&#27861;&#28385;&#36275;&#38656;&#27714;&#30340;&#29615;&#22659;&#20013;&#65292;&#21387;&#21147;&#22270;&#25104;&#20026;&#19968;&#31181;&#22791;&#36873;&#26041;&#26696;&#65292;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#30340;&#27880;&#24847;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#21387;&#21147;&#22270;&#20851;&#38190;&#28857;&#26816;&#27979;&#65288;SPMKD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#38024;&#23545;&#21387;&#21147;&#22270;&#20013;&#20154;&#20307;&#20851;&#38190;&#28857;&#25552;&#21462;&#30340;&#19987;&#38376;&#35774;&#35745;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26680;&#24515;&#26159;&#32534;&#30721;&#22120;-&#34701;&#21512;&#22120;-&#35299;&#30721;&#22120;&#65288;EFD&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#31934;&#30830;&#20154;&#20307;&#20851;&#38190;&#28857;&#26816;&#27979;&#30340;&#36731;&#37327;&#32423;&#32534;&#30721;&#22120;&#12289;&#29992;&#20110;&#39640;&#25928;&#26799;&#24230;&#20256;&#25773;&#30340;&#34701;&#21512;&#22120;&#65292;&#20197;&#21450;&#23558;&#20154;&#20307;&#20851;&#38190;&#28857;&#36716;&#25442;&#20026;&#37325;&#26500;&#21387;&#21147;&#22270;&#30340;&#35299;&#30721;&#22120;&#12290;&#35813;&#32467;&#26500;&#36890;&#36807;&#20998;&#31867;&#21040;&#22238;&#24402;&#26435;&#37325;&#36716;&#31227;&#65288;CRWT&#65289;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#36890;&#36807;&#21021;&#22987;&#20998;&#31867;&#20219;&#21153;&#35757;&#32451;&#23545;&#31934;&#30830;&#24230;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#19968;&#21019;&#26032;&#19981;&#20165;&#25552;&#39640;&#20102;&#20154;&#20307;&#20851;&#38190;&#28857;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19988;&#23637;&#31034;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14241v1 Announce Type: cross  Abstract: In environments where RGB images are inadequate, pressure maps is a viable alternative, garnering scholarly attention. This study introduces a novel self-supervised pressure map keypoint detection (SPMKD) method, addressing the current gap in specialized designs for human keypoint extraction from pressure maps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model, which is a robust framework that integrates a lightweight encoder for precise human keypoint detection, a fuser for efficient gradient propagation, and a decoder that transforms human keypoints into reconstructed pressure maps. This structure is further enhanced by the Classification-to-Regression Weight Transfer (CRWT) method, which fine-tunes accuracy through initial classification task training. This innovation not only enhances human keypoint generalization without manual annotations but also showcases remarkable efficiency and generalization, evidenced by
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14236</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#30340;&#33258;&#21160;&#35774;&#35745;&#19982;&#20248;&#21270;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14236
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;(DFC)&#22797;&#26434;&#19988;&#32791;&#26102;&#65292;&#30005;&#36335;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#30005;&#23376;&#24037;&#31243;&#24072;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#36827;DFC&#30340;&#35774;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#24037;&#31243;&#24072;&#35774;&#35745;&#32463;&#39564;&#30340;&#20381;&#36182;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;&#30005;&#36335;&#35774;&#35745;&#30456;&#20851;&#30340;&#20027;&#35266;&#24615;&#21644;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19982;&#20256;&#32479;&#30340;&#24037;&#31243;&#24072;&#39537;&#21160;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35774;&#35745;&#25928;&#29575;&#21644;&#36136;&#37327;&#19978;&#37117;&#26377;&#26126;&#26174;&#25913;&#21892;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35774;&#35745;&#22797;&#26434;&#25110;&#24555;&#36895;&#21457;&#23637;&#30340;DFC&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14236v1 Announce Type: cross  Abstract: Designing distributed filtering circuits (DFCs) is complex and time-consuming, with the circuit performance relying heavily on the expertise and experience of electronics engineers. However, manual design methods tend to have exceedingly low-efficiency. This study proposes a novel end-to-end automated method for fabricating circuits to improve the design of DFCs. The proposed method harnesses reinforcement learning (RL) algorithms, eliminating the dependence on the design experience of engineers. Thus, it significantly reduces the subjectivity and constraints associated with circuit design. The experimental findings demonstrate clear improvements in both design efficiency and quality when comparing the proposed method with traditional engineer-driven methods. In particular, the proposed method achieves superior performance when designing complex or rapidly evolving DFCs. Furthermore, compared to existing circuit automation design techn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MerRec&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;C2C&#25512;&#33616;&#32780;&#25552;&#20986;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;C2C&#25512;&#33616;&#25968;&#25454;&#38598;&#20013;&#29289;&#21697;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#31561;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>https://arxiv.org/abs/2402.14230</link><description>&lt;p&gt;
MerRec&#65306;&#29992;&#20110;&#28040;&#36153;&#32773;&#23545;&#28040;&#36153;&#32773;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#21151;&#33021;Mercari&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MerRec&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;C2C&#25512;&#33616;&#32780;&#25552;&#20986;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;C2C&#25512;&#33616;&#25968;&#25454;&#38598;&#20013;&#29289;&#21697;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#31561;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#22320;&#22609;&#36896;&#20102;&#29992;&#25143;&#20307;&#39564;&#21644;&#21442;&#19982;&#24230;&#12290;&#28040;&#36153;&#32773;&#23545;&#28040;&#36153;&#32773;&#65288;C2C&#65289;&#25512;&#33616;&#31995;&#32479;&#30340;&#23835;&#36215;&#65292;&#20197;&#20854;&#28789;&#27963;&#24615;&#21644;&#20026;&#23458;&#25143;&#20379;&#24212;&#21830;&#25552;&#20379;&#26131;&#20110;&#35775;&#38382;&#30340;&#29305;&#28857;&#65292;&#26631;&#24535;&#30528;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#20851;&#27880;&#20027;&#35201;&#38598;&#20013;&#22312;&#21830;&#23478;&#23545;&#28040;&#36153;&#32773;&#65288;B2C&#65289;&#27169;&#22411;&#19978;&#65292;&#30041;&#19979;&#20102;&#19968;&#20010;&#31354;&#30333;&#65292;&#21363;&#32570;&#20047;&#29289;&#21697;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#30340;C2C&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;C2C&#25512;&#33616;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#36827;&#19968;&#27493;&#31361;&#20986;&#20102;&#29992;&#25143;&#25198;&#28436;&#21334;&#23478;&#21644;&#20080;&#23478;&#20004;&#31181;&#35282;&#33394;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#19981;&#37027;&#20040;&#32479;&#19968;&#21644;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MerRec&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;C2C&#25512;&#33616;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28304;&#33258;Mercari&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#65292;&#35206;&#30422;&#20102;2023&#24180;6&#20010;&#26376;&#20869;&#25968;&#30334;&#19975;&#29992;&#25143;&#21644;&#20135;&#21697;&#12290;MerRec&#19981;&#20165;&#21253;&#25324;&#26631;&#20934;&#29305;&#24449;&#65292;&#22914;user_id&#12289;item_id&#21644;session_id
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14230v1 Announce Type: cross  Abstract: In the evolving e-commerce field, recommendation systems crucially shape user experience and engagement. The rise of Consumer-to-Consumer (C2C) recommendation systems, noted for their flexibility and ease of access for customer vendors, marks a significant trend. However, the academic focus remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by the limited C2C recommendation datasets that lack in item attributes, user diversity, and scale. The intricacy of C2C recommendation systems is further accentuated by the dual roles users assume as both sellers and buyers, introducing a spectrum of less uniform and varied inputs. Addressing this, we introduce MerRec, the first large-scale dataset specifically for C2C recommendations, sourced from the Mercari e-commerce platform, covering millions of users and products over 6 months in 2023. MerRec not only includes standard features such as user_id, item_id, and session_id
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;</title><link>https://arxiv.org/abs/2402.14228</link><description>&lt;p&gt;
COPR:&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#27491;&#21017;&#21270;&#23454;&#29616;&#25345;&#32493;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COPR: Continual Human Preference Learning via Optimal Policy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14228
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#30028; &#25688;&#35201;: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#36890;&#24120;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#12290;&#37492;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#25345;&#32493;&#23545;&#40784;&#30456;&#23545;&#20110;&#20256;&#32479;&#38745;&#24577;&#23545;&#40784;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#21644;&#23454;&#38469;&#12290;&#28982;&#32780;&#65292;&#20351;RLHF&#19982;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20860;&#23481;&#30001;&#20110;&#20854;&#22797;&#26434;&#36807;&#31243;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30452;&#25509;&#23398;&#20064;&#26032;&#30340;&#20154;&#31867;&#20559;&#22909;&#21487;&#33021;&#23548;&#33268;&#21382;&#21490;&#20559;&#22909;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#23548;&#33268;&#26080;&#21161;&#25110;&#26377;&#23475;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#12290;COPR&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;&#23427;&#37319;&#29992;Lagrange&#23545;&#20598;&#65288;LD&#65289;&#26041;&#27861;&#26681;&#25454;&#21382;&#21490;&#19978;&#30340;&#26368;&#20248;&#31574;&#30053;&#21160;&#24577;&#22320;&#27491;&#21017;&#21270;&#24403;&#21069;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal p
&lt;/p&gt;</description></item><item><title>Moonwalk&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;-&#36870;-Jacobian&#20056;&#31215;&#30340;&#26032;&#25216;&#26415;&#65292;&#21152;&#36895;&#21069;&#21521;&#26799;&#24230;&#35745;&#31639;&#65292;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#20445;&#25345;&#30495;&#23454;&#26799;&#24230;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23558;&#35745;&#31639;&#26102;&#38388;&#38477;&#20302;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2402.14212</link><description>&lt;p&gt;
Moonwalk&#65306;&#36870;&#21521;-&#21069;&#21521;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
Moonwalk: Inverse-Forward Differentiation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14212
&lt;/p&gt;
&lt;p&gt;
Moonwalk&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;-&#36870;-Jacobian&#20056;&#31215;&#30340;&#26032;&#25216;&#26415;&#65292;&#21152;&#36895;&#21069;&#21521;&#26799;&#24230;&#35745;&#31639;&#65292;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#20445;&#25345;&#30495;&#23454;&#26799;&#24230;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23558;&#35745;&#31639;&#26102;&#38388;&#38477;&#20302;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#34429;&#28982;&#22312;&#26799;&#24230;&#35745;&#31639;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#22312;&#35299;&#20915;&#20869;&#23384;&#28040;&#32791;&#21644;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#21069;&#21521;&#26799;&#24230;&#35745;&#31639;&#20316;&#20026;&#21487;&#36870;&#32593;&#32476;&#20013;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#30340;&#28508;&#21147;&#65292;&#24182;&#19981;&#24102;&#26469;&#37325;&#22823;&#32570;&#28857;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;-&#36870;-Jacobian&#20056;&#31215;&#30340;&#26032;&#25216;&#26415;&#65292;&#21152;&#36895;&#20102;&#21069;&#21521;&#26799;&#24230;&#30340;&#35745;&#31639;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20943;&#23569;&#20869;&#23384;&#21644;&#20445;&#25345;&#30495;&#23454;&#26799;&#24230;&#20934;&#30830;&#24615;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Moonwalk&#22312;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#19982;&#26420;&#32032;&#21069;&#21521;&#30340;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#30456;&#27604;&#65292;&#22312;&#27809;&#26377;&#20998;&#37197;&#26356;&#22810;&#20869;&#23384;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#23454;&#35777;&#30340;&#35282;&#24230;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23558;Moonwalk&#19982;&#21453;&#21521;&#27169;&#24335;&#24494;&#20998;&#30456;&#32467;&#21512;&#26469;&#21152;&#36895;&#65292;&#20197;&#23454;&#29616;&#19982;&#21453;&#21521;&#20256;&#25773;&#30456;&#24403;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#23567;&#30340;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14212v1 Announce Type: cross  Abstract: Backpropagation, while effective for gradient computation, falls short in addressing memory consumption, limiting scalability. This work explores forward-mode gradient computation as an alternative in invertible networks, showing its potential to reduce the memory footprint without substantial drawbacks. We introduce a novel technique based on a vector-inverse-Jacobian product that accelerates the computation of forward gradients while retaining the advantages of memory reduction and preserving the fidelity of true gradients. Our method, Moonwalk, has a time complexity linear in the depth of the network, unlike the quadratic time complexity of na\"ive forward, and empirically reduces computation time by several orders of magnitude without allocating more memory. We further accelerate Moonwalk by combining it with reverse-mode differentiation to achieve time complexity comparable with backpropagation while maintaining a much smaller mem
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14208</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#20869;&#23481;&#26465;&#20214;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Conditional Debiasing for Fair Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14208
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20844;&#24179;&#30340;&#25991;&#26412;&#23884;&#20837;&#19978;&#65292;&#36825;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25928;&#29992;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#20294;&#30456;&#21516;&#20869;&#23481;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#19982;&#20854;&#23545;&#24212;&#20013;&#31435;&#25991;&#26412;&#30340;&#23884;&#20837;&#20445;&#25345;&#30456;&#21516;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25991;&#26412;&#22686;&#24378;&#20026;&#19981;&#21516;&#30340;&#25935;&#24863;&#32452;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#23884;&#20837;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STORM&#30340;&#20889;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#21512;&#25104;&#20027;&#39064;&#27010;&#35201;&#65292;&#20197;&#36741;&#21161;&#20174;&#22836;&#24320;&#22987;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#31456;&#12290;</title><link>https://arxiv.org/abs/2402.14207</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#36741;&#21161;&#25776;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;
&lt;/p&gt;
&lt;p&gt;
Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STORM&#30340;&#20889;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#21512;&#25104;&#20027;&#39064;&#27010;&#35201;&#65292;&#20197;&#36741;&#21161;&#20174;&#22836;&#24320;&#22987;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#25776;&#20889;&#22522;&#20110;&#20107;&#23454;&#21644;&#26377;&#26465;&#29702;&#30340;&#38271;&#31687;&#25991;&#31456;&#65292;&#20351;&#20854;&#22312;&#24191;&#24230;&#21644;&#28145;&#24230;&#19978;&#19982;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#21487;&#23218;&#32654;&#12290;&#36825;&#19968;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#22312;&#25776;&#20889;&#21069;&#38454;&#27573;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22914;&#20309;&#30740;&#31350;&#20027;&#39064;&#24182;&#20934;&#22791;&#22823;&#32434;&#20197;&#20415;&#25776;&#20889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STORM&#65292;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#36827;&#34892;&#20027;&#39064;&#27010;&#35201;&#21512;&#25104;&#30340;&#20889;&#20316;&#31995;&#32479;&#12290;STORM&#27169;&#25311;&#20102;&#25776;&#20889;&#21069;&#38454;&#27573;&#65292;&#20854;&#20013;&#65288;1&#65289;&#21457;&#29616;&#30740;&#31350;&#32473;&#23450;&#20027;&#39064;&#30340;&#22810;&#26679;&#21270;&#35266;&#28857;&#65292;&#65288;2&#65289;&#27169;&#25311;&#20250;&#35805;&#65292;&#25776;&#20889;&#25345;&#26377;&#19981;&#21516;&#35266;&#28857;&#30340;&#20316;&#32773;&#21521;&#22522;&#20110;&#21487;&#20449;&#20114;&#32852;&#32593;&#26469;&#28304;&#30340;&#20027;&#39064;&#19987;&#23478;&#25552;&#38382;&#65292;&#65288;3&#65289;&#25972;&#29702;&#25910;&#38598;&#21040;&#30340;&#20449;&#24687;&#20197;&#21019;&#24314;&#22823;&#32434;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;FreshWiki&#65292;&#19968;&#20010;&#21253;&#21547;&#26368;&#26032;&#39640;&#36136;&#37327;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#22823;&#32434;&#35780;&#20272;&#25351;&#26631;&#20197;&#35780;&#20272;&#25776;&#20889;&#21069;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14207v1 Announce Type: cross  Abstract: We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.   For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#33521;&#25991;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#26032;&#34920;&#24773;&#31526;&#21495;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#24773;&#20917;&#65292;&#21457;&#29616;&#26089;&#26399;&#37319;&#32435;&#32773;&#35268;&#27169;&#21644;&#34920;&#24773;&#31526;&#21495;&#35821;&#20041;&#23545;&#20854;&#27969;&#34892;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#35299;&#37322;&#26032;&#34920;&#24773;&#31526;&#21495;&#65292;&#20174;&#32780;&#25913;&#21892;&#24773;&#24863;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14187</link><description>&lt;p&gt;
&#20174;&#37319;&#32435;&#21040;&#36866;&#24212;&#65306;&#36861;&#36394;&#26032;&#34920;&#24773;&#31526;&#21495;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
From Adoption to Adaption: Tracing the Diffusion of New Emojis on Twitter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#33521;&#25991;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#26032;&#34920;&#24773;&#31526;&#21495;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#24773;&#20917;&#65292;&#21457;&#29616;&#26089;&#26399;&#37319;&#32435;&#32773;&#35268;&#27169;&#21644;&#34920;&#24773;&#31526;&#21495;&#35821;&#20041;&#23545;&#20854;&#27969;&#34892;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#35299;&#37322;&#26032;&#34920;&#24773;&#31526;&#21495;&#65292;&#20174;&#32780;&#25913;&#21892;&#24773;&#24863;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;Unicode&#21457;&#24067;&#26032;&#34920;&#24773;&#31526;&#21495;&#29256;&#26412;&#25552;&#20379;&#20102;&#19968;&#20010;&#25506;&#32034;&#25968;&#23383;&#35821;&#35328;&#28436;&#21464;&#30340;&#32467;&#26500;&#21270;&#26426;&#20250;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25277;&#26679;&#30340;&#33521;&#25991;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26032;&#21457;&#24067;&#30340;&#34920;&#24773;&#31526;&#21495;&#22914;&#20309;&#33719;&#24471;&#20851;&#27880;&#24182;&#22914;&#20309;&#22312;&#21547;&#20041;&#19978;&#28436;&#21464;&#12290;&#25105;&#20204;&#21457;&#29616;&#26089;&#26399;&#37319;&#32435;&#32773;&#30340;&#31038;&#21306;&#35268;&#27169;&#21644;&#34920;&#24773;&#31526;&#21495;&#35821;&#20041;&#23545;&#20110;&#30830;&#23450;&#23427;&#20204;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#26576;&#20123;&#34920;&#24773;&#31526;&#21495;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#21547;&#20041;&#21464;&#21270;&#21644;&#24773;&#24863;&#20851;&#32852;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20855;&#26377;&#35821;&#20041;&#19978;&#30456;&#20284;&#19978;&#19979;&#25991;&#30340;&#21333;&#35789;&#21644;&#26082;&#26377;&#34920;&#24773;&#31526;&#21495;&#30340;&#26032;&#26694;&#26550;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#37322;&#26032;&#34920;&#24773;&#31526;&#21495;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29992;&#29087;&#24713;&#30340;&#34920;&#24773;&#31526;&#21495;&#26367;&#20195;&#26410;&#30693;&#30340;&#26032;&#34920;&#24773;&#31526;&#21495;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#20998;&#31867;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25105;&#20204;&#29702;&#35299;&#26032;&#35821;&#35328;&#30340;&#37319;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14187v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of social media, the introduction of new emojis in Unicode release versions presents a structured opportunity to explore digital language evolution. Analyzing a large dataset of sampled English tweets, we examine how newly released emojis gain traction and evolve in meaning. We find that community size of early adopters and emoji semantics are crucial in determining their popularity. Certain emojis experienced notable shifts in the meanings and sentiment associations during the diffusion process. Additionally, we propose a novel framework utilizing language models to extract words and pre-existing emojis with semantically similar contexts, which enhances interpretation of new emojis. The framework demonstrates its effectiveness in improving sentiment classification performance by substituting unknown new emojis with familiar ones. This study offers a new perspective in understanding how new language un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30524;&#21160;&#36861;&#36394;&#25351;&#26631;&#21644; SHAP &#26041;&#27861;&#23545;&#27604;&#20102;&#20154;&#31867;&#19982;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#24635;&#32467;&#20013;&#30340;&#20851;&#27880;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#36127;&#38754;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14182</link><description>&lt;p&gt;
&#26426;&#22120;&#21644;&#20154;&#31867;&#26159;&#21542;&#20851;&#27880;&#30456;&#20284;&#30340;&#20195;&#30721;&#65311;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#24635;&#32467;&#20013;&#30340;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30524;&#21160;&#36861;&#36394;&#25351;&#26631;&#21644; SHAP &#26041;&#27861;&#23545;&#27604;&#20102;&#20154;&#31867;&#19982;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#24635;&#32467;&#20013;&#30340;&#20851;&#27880;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#36127;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#24635;&#32467;&#28304;&#20195;&#30721;&#26041;&#38754;&#30340;&#29087;&#32451;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#35768;&#22810;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#19968;&#26679;&#65292;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#36275;&#22815;&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#38750;&#27491;&#24335;&#22320;&#32570;&#20047;&#23545;&#27169;&#22411;&#22914;&#20309;&#20174;&#20195;&#30721;&#20013;&#23398;&#20064;&#20197;&#21450;&#23398;&#20064;&#20102;&#20160;&#20040;&#30340;&#20844;&#24335;&#21270;&#25110;&#30452;&#35273;&#24615;&#29702;&#35299;&#12290;&#22914;&#26524;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#24635;&#32467;&#65292;&#37027;&#20040;&#23427;&#20204;&#22312;&#35748;&#20026;&#30456;&#21516;&#30340;&#20195;&#30721;&#37096;&#20998;&#37325;&#35201;&#26102;&#20063;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#25152;&#35782;&#21035;&#30340;&#37096;&#20998;&#30456;&#19968;&#33268;&#65292;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#20379;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20154;&#31867;&#29702;&#35299;&#30340;&#35270;&#35282;&#65292;&#25253;&#21578;&#20102;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#24635;&#32467;&#20013;&#35299;&#37322;&#24615;&#30340;&#35843;&#26597;&#30340;&#36127;&#38754;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#30524;&#21160;&#36861;&#36394;&#25351;&#26631;&#65288;&#22914;&#27880;&#35270;&#27425;&#25968;&#21644;&#20195;&#30721;&#24635;&#32467;&#20219;&#21153;&#20013;&#30340;&#20572;&#30041;&#26102;&#38388;&#65289;&#26469;&#34913;&#37327;&#20154;&#31867;&#23545;&#20195;&#30721;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#36817;&#20284;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#27880;&#37325;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26080;&#20851;&#12289;&#40657;&#30418;&#12289;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#8212;&#8212;SHAP&#65288;SHapley Additive&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14182v1 Announce Type: cross  Abstract: Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability. Informally, we lack a formulaic or intuitive understanding of what and how models learn from code. Explainability of language models can be partially provided if, as the models learn to produce higher-quality code summaries, they also align in deeming the same code parts important as those identified by human programmers. In this paper, we report negative results from our investigation of explainability of language models in code summarization through the lens of human comprehension. We measure human focus on code using eye-tracking metrics such as fixation counts and duration in code summarization tasks. To approximate language model focus, we employ a state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP (SHapley Additive
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#27665;&#26063;&#23186;&#20307;&#39046;&#22495;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65292;&#20197;&#25552;&#21319;&#26032;&#38395;&#32763;&#35793;&#12289;&#25628;&#32034;&#21644;&#20998;&#31867;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14179</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;AI&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26063;&#35028;&#23186;&#20307;&#26426;&#22120;&#32763;&#35793;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#27665;&#26063;&#23186;&#20307;&#39046;&#22495;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65292;&#20197;&#25552;&#21319;&#26032;&#38395;&#32763;&#35793;&#12289;&#25628;&#32034;&#21644;&#20998;&#31867;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27665;&#26063;&#23186;&#20307;&#26159;&#20026;&#39547;&#30041;&#22312;&#19996;&#36947;&#22269;&#30340;&#20392;&#27665;&#31038;&#21306;&#25552;&#20379;&#26381;&#21153;&#30340;&#37325;&#35201;&#24179;&#21488;&#65292;&#26082;&#26381;&#21153;&#20110;&#36825;&#20123;&#31038;&#21306;&#21046;&#20316;&#20869;&#23481;&#65292;&#21448;&#35753;&#20182;&#20204;&#33719;&#21462;&#20449;&#24687;&#12290;&#19982;&#20351;&#29992;&#19996;&#36947;&#22269;&#35821;&#35328;&#19981;&#21516;&#65292;&#27665;&#26063;&#23186;&#20307;&#20197;&#31227;&#27665;&#31038;&#21306;&#30340;&#35821;&#35328;&#21457;&#24067;&#26032;&#38395;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#22312;&#32654;&#22269;&#65292;&#23391;&#21152;&#25289;&#26063;&#35028;&#23186;&#20307;&#20351;&#29992;&#23391;&#21152;&#25289;&#35821;&#32780;&#19981;&#26159;&#33521;&#35821;&#21457;&#24067;&#26032;&#38395;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27665;&#26063;&#23186;&#20307;&#39046;&#22495;&#28508;&#22312;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#30340;&#21487;&#33021;&#24615;&#12290;&#23427;&#30528;&#37325;&#25506;&#35752;&#20102;&#22312;&#26032;&#38395;&#32763;&#35793;&#12289;&#25628;&#32034;&#21644;&#20998;&#31867;&#30340;&#21508;&#20010;&#26041;&#38754;&#20013;&#20351;&#29992;LLM&#36827;&#34892;MMT&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;&#35770;&#25991;&#27010;&#36848;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;LLM&#21644;MMT&#25972;&#21512;&#21040;&#27665;&#26063;&#23186;&#20307;&#30340;&#26032;&#38395;&#25628;&#32034;&#21644;&#32763;&#35793;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#31616;&#35201;&#35752;&#35770;&#20102;&#19982;LLM&#21644;MMT&#25972;&#21512;&#30456;&#20851;&#30340;&#28508;&#22312;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14179v1 Announce Type: cross  Abstract: Ethnic media, which caters to diaspora communities in host nations, serves as a vital platform for these communities to both produce content and access information. Rather than utilizing the language of the host nation, ethnic media delivers news in the language of the immigrant community. For instance, in the USA, Bangla ethnic media presents news in Bangla rather than English. This research delves into the prospective integration of large language models (LLM) and multi-lingual machine translations (MMT) within the ethnic media industry. It centers on the transformative potential of using LLM in MMT in various facets of news translation, searching, and categorization. The paper outlines a theoretical framework elucidating the integration of LLM and MMT into the news searching and translation processes for ethnic media. Additionally, it briefly addresses the potential ethical challenges associated with the incorporation of LLM and MMT
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#19968;&#31181;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#23558;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#19982;&#22522;&#20110;&#20248;&#21270;&#21338;&#24328;&#25919;&#31574;&#30456;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;KLGame&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38024;&#23545;&#27599;&#20010;&#20915;&#31574;&#32773;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14174</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Blending Data-Driven Priors in Dynamic Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14174
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#19968;&#31181;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#23558;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#19982;&#22522;&#20110;&#20248;&#21270;&#21338;&#24328;&#25919;&#31574;&#30456;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;KLGame&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38024;&#23545;&#27599;&#20010;&#20915;&#31574;&#32773;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#26426;&#22120;&#20154;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20154;&#32676;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#36825;&#20123;&#31995;&#32479;&#24212;&#35813;&#22312;&#23433;&#20840;&#30340;&#12289;&#19982;&#20154;&#20114;&#21160;&#24847;&#35782;&#30456;&#20851;&#30340;&#36816;&#21160;&#35268;&#21010;&#20013;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#21338;&#24328;&#35770;&#35268;&#21010;&#22120;&#19982;&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#30340;&#31243;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#21338;&#24328;&#35770;&#25919;&#31574;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;KLGame&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;Kullback-Leibler&#65288;KL&#65289;&#27491;&#21017;&#21270;&#30340;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;&#65292;&#38024;&#23545;&#19968;&#20010;&#19968;&#33324;&#30340;&#12289;&#38543;&#26426;&#30340;&#65292;&#21487;&#33021;&#26159;&#22810;&#27169;&#24335;&#30340;&#21442;&#32771;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14174v1 Announce Type: cross  Abstract: As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question. Existing dynamic game formulations assume all agents are task-driven and behave optimally. However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm. In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy. We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly multi-modal reference policy. Our method incorporates, for each decision maker, a tunable parameter that pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#40065;&#26834;&#24615;&#65292;&#35777;&#23454;&#20102;&#23427;&#20204;&#22312;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14162</link><description>&lt;p&gt;
&#20851;&#20110;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#40065;&#26834;&#24615;&#65292;&#35777;&#23454;&#20102;&#23427;&#20204;&#22312;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#23558;LLMs&#19982;&#35270;&#35273;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25506;&#32034;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#22914;LLaVA&#12289;Flamingo&#25110;CLIP&#65292;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#27839;&#30528;&#36825;&#20010;&#26041;&#21521;&#65292;&#30446;&#21069;&#32570;&#20047;&#30456;&#20851;&#24037;&#20316;&#26469;&#23637;&#31034;&#22823;&#22411;&#27169;&#22411;&#35786;&#26029;&#30142;&#30149;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;VLMs&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#20102;VLMs&#22312;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#65288;&#22914;&#33041;MRI&#12289;&#34880;&#32454;&#32990;&#26174;&#24494;&#22270;&#20687;&#21644;&#33016;&#37096;X&#20809;&#29255;&#65289;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14162v1 Announce Type: cross  Abstract: Recently, large language models (LLMs) have taken the spotlight in natural language processing. Further, integrating LLMs with vision enables the users to explore emergent abilities with multimodal data. Visual language models (VLMs), such as LLaVA, Flamingo, or CLIP, have demonstrated impressive performance on various visio-linguistic tasks. Consequently, there are enormous applications of large models that could be potentially used in the biomedical imaging field. Along that direction, there is a lack of related work to show the ability of large models to diagnose the diseases. In this work, we study the zero-shot and few-shot robustness of VLMs on the medical imaging analysis tasks. Our comprehensive experiments demonstrate the effectiveness of VLMs in analyzing biomedical images such as brain MRIs, microscopic images of blood cells, and chest X-rays.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#26368;&#22823;&#21270;&#26641;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21152;&#36895;LLM&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.14160</link><description>&lt;p&gt;
&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;&#65306;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#21152;&#36895;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14160
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#26368;&#22823;&#21270;&#26641;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21152;&#36895;LLM&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#21152;&#36895;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#33609;&#31295;&#20196;&#29260;&#24207;&#21015;&#65292;&#35813;&#24207;&#21015;&#36827;&#19968;&#27493;&#30001;&#30446;&#26631;LLM&#24182;&#34892;&#39564;&#35777;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#33609;&#31295;&#20196;&#29260;&#26641;&#25512;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#21333;&#24207;&#21015;&#25512;&#27979;&#35299;&#30721;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#26641;&#30340;&#27599;&#20010;&#32423;&#21035;&#29420;&#31435;&#29983;&#25104;&#20196;&#29260;&#65292;&#27809;&#26377;&#21033;&#29992;&#25972;&#20010;&#26641;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22266;&#23450;&#24207;&#21015;&#38271;&#24230;&#24050;&#32463;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#20316;&#21697;&#22312;&#22266;&#23450;&#30446;&#26631;&#35745;&#31639;&#36164;&#28304;&#19978;&#24182;&#27809;&#26377;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#36825;&#26159;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65292;&#23427;&#23545;&#19981;&#37325;&#22797;&#25277;&#26679;&#30340;&#33609;&#31295;&#20196;&#29260;&#36827;&#34892;&#26368;&#22823;&#21270;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#23454;&#29616;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14160v1 Announce Type: cross  Abstract: Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the dive
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#19977;&#31181;&#39046;&#22495;&#25490;&#24207;&#31574;&#30053;&#23545;&#29983;&#25104;&#24335;&#24847;&#22270;&#35782;&#21035;&#27169;&#22411;&#32487;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#27492;&#26041;&#38754;&#26410;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.14155</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#39046;&#22495;&#25490;&#24207;&#33021;&#22815;&#20943;&#23569;&#24847;&#22270;&#35782;&#21035;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14155
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#19977;&#31181;&#39046;&#22495;&#25490;&#24207;&#31574;&#30053;&#23545;&#29983;&#25104;&#24335;&#24847;&#22270;&#35782;&#21035;&#27169;&#22411;&#32487;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#27492;&#26041;&#38754;&#26410;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#34987;&#26399;&#26395;&#22312;&#37096;&#32626;&#21518;&#33021;&#22815;&#22788;&#29702;&#19981;&#26029;&#22686;&#38271;&#30340;&#24847;&#22270;&#21644;&#39046;&#22495;&#65292;&#29978;&#33267;&#22312;&#25903;&#25345;&#36234;&#26469;&#36234;&#22810;&#21151;&#33021;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#26399;&#26395;&#65292;&#23601;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#21435;&#20943;&#36731;&#22312;&#35832;&#22914;&#24847;&#22270;&#35782;&#21035;&#31561;&#20219;&#21153;&#30340;&#32487;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#35774;&#32622;&#20013;&#21457;&#29983;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65288;CF&#65289;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22522;&#20110;&#37325;&#25918;&#21644;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#20197;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#20294;&#39046;&#22495;&#25490;&#24207;&#23545;&#24847;&#22270;&#35782;&#21035;&#27169;&#22411;&#30340;&#32487;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22914;&#26524;&#29702;&#35299;&#24471;&#24403;&#65292;&#39046;&#22495;&#25490;&#24207;&#26377;&#28508;&#21147;&#25104;&#20026;&#19968;&#20010;&#33021;&#22815;&#19982;&#29616;&#26377;&#25216;&#26415;&#22914;&#32463;&#39564;&#37325;&#25918;&#24182;&#34892;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#39046;&#22495;&#25490;&#24207;&#31574;&#30053;&#65288;&#26368;&#23567;&#21644;&#36335;&#24452;&#12289;&#26368;&#22823;&#21644;&#36335;&#24452;&#12289;&#38543;&#26426;&#65289;&#23545;&#29983;&#25104;&#24335;&#24847;&#22270;&#35782;&#21035;&#27169;&#22411;&#30340;&#32487;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14155v1 Announce Type: cross  Abstract: Task-oriented dialogue systems are expected to handle a constantly expanding set of intents and domains even after they have been deployed to support more and more functionalities. To live up to this expectation, it becomes critical to mitigate the catastrophic forgetting problem (CF) that occurs in continual learning (CL) settings for a task such as intent recognition. While existing dialogue systems research has explored replay-based and regularization-based methods to this end, the effect of domain ordering on the CL performance of intent recognition models remains unexplored. If understood well, domain ordering has the potential to be an orthogonal technique that can be leveraged alongside existing techniques such as experience replay. Our work fills this gap by comparing the impact of three domain-ordering strategies (min-sum path, max-sum path, random) on the CL performance of a generative intent recognition model. Our findings r
&lt;/p&gt;</description></item><item><title>BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14151</link><description>&lt;p&gt;
BIRCO&#65306;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14151
&lt;/p&gt;
&lt;p&gt;
BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;(IR)&#20219;&#21153;&#22522;&#20934;(BIRCO)&#12290; BIRCO&#35780;&#20272;IR&#31995;&#32479;&#26681;&#25454;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#26816;&#32034;&#25991;&#26723;&#30340;&#33021;&#21147;&#12290; &#35813;&#22522;&#20934;&#30340;&#22797;&#26434;&#24615;&#21644;&#32039;&#20945;&#22823;&#23567;&#20351;&#20854;&#36866;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#21487;&#33021;&#24433;&#21709;LLM&#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#26356;&#22797;&#26434;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290; &#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#20219;&#21153;&#19978;&#22343;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#21644;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
&lt;/p&gt;</description></item><item><title>Wikibench&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#20351;&#31038;&#21306;&#33021;&#22815;&#21327;&#20316;&#25972;&#29702;AI&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#25429;&#25417;&#31038;&#21306;&#20849;&#35782;&#12289;&#20998;&#27495;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14147</link><description>&lt;p&gt;
Wikibench&#65306;&#38754;&#21521;&#32500;&#22522;&#30334;&#31185;&#30340;&#22522;&#20110;&#31038;&#21306;&#39537;&#21160;&#30340;AI&#35780;&#20272;&#25968;&#25454;&#25972;&#29702;
&lt;/p&gt;
&lt;p&gt;
Wikibench: Community-Driven Data Curation for AI Evaluation on Wikipedia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14147
&lt;/p&gt;
&lt;p&gt;
Wikibench&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#20351;&#31038;&#21306;&#33021;&#22815;&#21327;&#20316;&#25972;&#29702;AI&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#25429;&#25417;&#31038;&#21306;&#20849;&#35782;&#12289;&#20998;&#27495;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#24037;&#20855;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#31038;&#21306;&#29615;&#22659;&#20013;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#24320;&#21457;&#20154;&#21592;&#21644;&#26631;&#27880;&#32773;&#22312;&#32473;&#23450;&#31038;&#21306;&#20043;&#22806;&#21019;&#24314;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20851;&#20110;AI&#24615;&#33021;&#30340;&#35823;&#23548;&#24615;&#32467;&#35770;&#12290;&#25105;&#20204;&#22914;&#20309;&#36171;&#20104;&#31038;&#21306;&#25512;&#21160;&#26377;&#24847;&#30340;&#35774;&#35745;&#21644;&#25972;&#29702;&#23545;&#20854;&#20135;&#29983;&#24433;&#21709;&#30340;AI&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#65311;&#25105;&#20204;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#32500;&#22522;&#30334;&#31185;&#26159;&#19968;&#20010;&#37096;&#32626;&#20102;&#22810;&#20010;&#22522;&#20110;AI&#30340;&#20869;&#23481;&#31649;&#29702;&#24037;&#20855;&#30340;&#22312;&#32447;&#31038;&#21306;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Wikibench&#65292;&#19968;&#20010;&#31995;&#32479;&#65292;&#20351;&#31038;&#21306;&#33021;&#22815;&#21327;&#20316;&#25972;&#29702;AI&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#36890;&#36807;&#35752;&#35770;&#26469;&#35299;&#20915;&#27495;&#20041;&#21644;&#35266;&#28857;&#24046;&#24322;&#12290;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#36827;&#34892;&#30340;&#19968;&#39033;&#29616;&#22330;&#30740;&#31350;&#26174;&#31034;&#65292;&#20351;&#29992;Wikibench&#25972;&#29702;&#30340;&#25968;&#25454;&#38598;&#33021;&#26377;&#25928;&#25429;&#25417;&#31038;&#21306;&#20849;&#35782;&#12289;&#20998;&#27495;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21442;&#19982;&#32773;&#20351;&#29992;Wikibench&#26469;&#22609;&#36896;&#25972;&#20307;&#30340;&#25968;&#25454;&#25972;&#29702;&#27969;&#31243;&#65292;&#21253;&#25324;&#23436;&#21892;&#26631;&#31614;&#23450;&#20041;&#12289;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14147v1 Announce Type: cross  Abstract: AI tools are increasingly deployed in community contexts. However, datasets used to evaluate AI are typically created by developers and annotators outside a given community, which can yield misleading conclusions about AI performance. How might we empower communities to drive the intentional design and curation of evaluation datasets for AI that impacts them? We investigate this question on Wikipedia, an online community with multiple AI-based content moderation tools deployed. We introduce Wikibench, a system that enables communities to collaboratively curate AI evaluation datasets, while navigating ambiguities and differences in perspective through discussion. A field study on Wikipedia shows that datasets curated using Wikibench can effectively capture community consensus, disagreement, and uncertainty. Furthermore, study participants used Wikibench to shape the overall data curation process, including refining label definitions, de
&lt;/p&gt;</description></item><item><title>SecurePose&#26159;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#23454;&#29616;&#20020;&#24202;&#24405;&#21046;&#30340;&#24739;&#32773;&#35270;&#39057;&#20013;&#30340;&#20154;&#33080;&#27169;&#31946;&#21644;&#21160;&#21147;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#39640;&#20102;&#35270;&#39057;&#35780;&#20272;&#21644;&#24739;&#32773;&#38544;&#31169;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14143</link><description>&lt;p&gt;
SecurePose&#65306;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#24405;&#21046;&#30340;&#35270;&#39057;&#20013;&#23454;&#29616;&#33258;&#21160;&#20154;&#33080;&#27169;&#31946;&#21644;&#20154;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
SecurePose: Automated Face Blurring and Human Movement Kinematics Extraction from Videos Recorded in Clinical Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14143
&lt;/p&gt;
&lt;p&gt;
SecurePose&#26159;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#23454;&#29616;&#20020;&#24202;&#24405;&#21046;&#30340;&#24739;&#32773;&#35270;&#39057;&#20013;&#30340;&#20154;&#33080;&#27169;&#31946;&#21644;&#21160;&#21147;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#39640;&#20102;&#35270;&#39057;&#35780;&#20272;&#21644;&#24739;&#32773;&#38544;&#31169;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#38556;&#30861;&#36890;&#24120;&#36890;&#36807;&#19987;&#23478;&#23545;&#20020;&#24202;&#33719;&#21462;&#30340;&#24739;&#32773;&#35270;&#39057;&#36827;&#34892;&#20849;&#35782;&#35780;&#20272;&#26469;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24191;&#27867;&#20998;&#20139;&#24739;&#32773;&#35270;&#39057;&#20250;&#23545;&#24739;&#32773;&#38544;&#31169;&#26500;&#25104;&#39118;&#38505;&#12290;&#20154;&#33080;&#27169;&#31946;&#21487;&#20197;&#29992;&#26469;&#21435;&#26631;&#35782;&#21270;&#35270;&#39057;&#65292;&#20294;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#26159;&#25163;&#21160;&#19988;&#32791;&#26102;&#30340;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#20154;&#33080;&#27169;&#31946;&#25216;&#26415;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#12289;&#19981;&#19968;&#33268;&#25110;&#19981;&#36275;&#30340;&#20154;&#33080;&#27169;&#31946; - &#36825;&#20123;&#37117;&#21487;&#33021;&#23545;&#35270;&#39057;&#35780;&#20272;&#21644;&#24739;&#32773;&#38544;&#31169;&#36896;&#25104;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#35270;&#39057;&#20013;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#24448;&#24448;&#26159;&#20027;&#35266;&#30340;&#12290;&#25552;&#21462;&#21487;&#37327;&#21270;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#21487;&#20197;&#24110;&#21161;&#20102;&#35299;&#36825;&#20123;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#38556;&#30861;&#35780;&#20272;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#39044;&#27169;&#31946;&#35270;&#39057;&#26102;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SecurePose&#30340;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#22312;&#20020;&#24202;&#24405;&#21046;&#30340;&#24739;&#32773;&#35270;&#39057;&#20013;&#23454;&#29616;&#21487;&#38752;&#30340;&#20154;&#33080;&#27169;&#31946;&#21644;&#33258;&#21160;&#21160;&#21147;&#23398;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14143v1 Announce Type: cross  Abstract: Movement disorders are typically diagnosed by consensus-based expert evaluation of clinically acquired patient videos. However, such broad sharing of patient videos poses risks to patient privacy. Face blurring can be used to de-identify videos, but this process is often manual and time-consuming. Available automated face blurring techniques are subject to either excessive, inconsistent, or insufficient facial blurring - all of which can be disastrous for video assessment and patient privacy. Furthermore, assessing movement disorders in these videos is often subjective. The extraction of quantifiable kinematic features can help inform movement disorder assessment in these videos, but existing methods to do this are prone to errors if using pre-blurred videos. We have developed an open-source software called SecurePose that can both achieve reliable face blurring and automated kinematic extraction in patient videos recorded in a clinic 
&lt;/p&gt;</description></item><item><title>DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;</title><link>https://arxiv.org/abs/2402.14123</link><description>&lt;p&gt;
DeiSAM&#65306;&#36890;&#36807;&#25351;&#31034;&#25552;&#31034;&#20998;&#21106;&#20219;&#20309;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
DeiSAM: Segment Anything with Deictic Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14123
&lt;/p&gt;
&lt;p&gt;
DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#38646;-shot&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#35782;&#21035;&#20855;&#20307;&#23545;&#35937;&#65292;&#20154;&#31867;&#26412;&#33021;&#22320;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#31034;&#24615;&#25551;&#36848;&#65292;&#21363;&#26681;&#25454;&#19978;&#19979;&#25991;&#25351;&#31216;&#26576;&#29289;&#65292;&#27604;&#22914;&#8220;&#22312;&#26700;&#23376;&#19978;&#24182;&#22312;&#26479;&#23376;&#21518;&#38754;&#30340;&#29289;&#20307;&#8221;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#32570;&#20047;&#25512;&#29702;&#33021;&#21147;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35299;&#37322;&#36825;&#31181;&#25351;&#31034;&#24615;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeiSAM&#8212;&#8212;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#12290;&#32473;&#23450;&#22797;&#26434;&#30340;&#25991;&#26412;&#20998;&#21106;&#25551;&#36848;&#65292;DeiSAM&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#22330;&#26223;&#22270;&#36827;&#34892;&#21487;&#21306;&#20998;&#30340;&#21069;&#21521;&#25512;&#29702;&#12290;&#38543;&#21518;&#65292;DeiSAM&#36890;&#36807;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14123v1 Announce Type: cross  Abstract: Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as "The object that is on the desk and behind the cup.". However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM -- a combination of large pre-trained neural networks with differentiable logic reasoners -- for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by match
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32039;&#24613;&#31232;&#30095;&#24615;&#30340;&#25513;&#30721;&#30697;&#38453;&#20056;&#27861;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#36816;&#34892;&#26102;&#35780;&#20272;&#31232;&#30095;&#24230;&#26469;&#28040;&#38500;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#21644;&#36991;&#20813;&#20998;&#25903;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#25351;&#20196;&#25968;&#21644;&#26356;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14118</link><description>&lt;p&gt;
&#29992;&#20110;&#32039;&#24613;&#31232;&#30095;&#24615;&#30340;&#25513;&#30721;&#30697;&#38453;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
Masked Matrix Multiplication for Emergent Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32039;&#24613;&#31232;&#30095;&#24615;&#30340;&#25513;&#30721;&#30697;&#38453;&#20056;&#27861;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#36816;&#34892;&#26102;&#35780;&#20272;&#31232;&#30095;&#24230;&#26469;&#28040;&#38500;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#21644;&#36991;&#20813;&#20998;&#25903;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#25351;&#20196;&#25968;&#21644;&#26356;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#34920;&#29616;&#20986;&#32039;&#24613;&#31232;&#30095;&#24615;&#65292;&#22312;&#20854;&#20013;&#35745;&#31639;&#23545;&#31264;&#23494;&#25968;&#25454;&#36827;&#34892;&#36873;&#25321;&#24615;&#31232;&#30095;&#35775;&#38382;&#12290;&#36825;&#20123;&#24037;&#20316;&#36127;&#36733;&#22312;&#20026;&#31264;&#23494;&#35745;&#31639;&#35774;&#35745;&#30340;&#30828;&#20214;&#19978;&#25928;&#29575;&#20302;&#19979;&#65292;&#24182;&#19988;&#26080;&#27861;&#24456;&#22909;&#22320;&#26144;&#23556;&#21040;&#31232;&#30095;&#25968;&#25454;&#34920;&#31034;&#19978;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#31995;&#32479; A X B = C&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#23545;&#31232;&#30095;&#24230;&#30340;&#36816;&#34892;&#26102;&#35780;&#20272;&#28040;&#38500;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24182;&#36991;&#20813;&#20102;&#20998;&#25903;&#12290;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;&#20195;&#30721;&#26597;&#25214;&#21644;&#39044;&#22788;&#29702; A &#21644; B &#30697;&#38453;&#30340;&#31232;&#30095;&#22270;&#65292;&#26469;&#36866;&#24212; B &#30697;&#38453;&#20013;&#32534;&#30721;&#30340;&#20855;&#20307;&#31232;&#30095;&#24615;&#65292;&#24182;&#20026;&#25972;&#20010;&#35745;&#31639;&#20165;&#35745;&#31639;&#19968;&#27425;&#26465;&#20214;&#20998;&#25903;&#12290;&#22312;&#20174;60%&#21040;95%&#30340;&#24191;&#27867;&#31232;&#30095;&#24230;&#33539;&#22260;&#20869;&#65292;&#19982;&#33521;&#29305;&#23572;MKL&#30340;&#31264;&#23494;&#25110;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#20363;&#31243;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#29616;&#25191;&#34892;&#30340;&#25351;&#20196;&#26356;&#23569;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25928;&#26524;&#21487;&#20197;&#36798;&#21040;2&#20493;&#21152;&#36895;&#21644;4&#20493;&#36153;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14118v1 Announce Type: cross  Abstract: Artificial intelligence workloads, especially transformer models, exhibit emergent sparsity in which computations perform selective sparse access to dense data. The workloads are inefficient on hardware designed for dense computations and do not map well onto sparse data representations. We build a vectorized and parallel matrix-multiplication system A X B = C that eliminates unnecessary computations and avoids branches based on a runtime evaluation of sparsity. We use a combination of dynamic code lookup to adapt to the specific sparsity encoded in the B matrix and preprocessing of sparsity maps of the A and B matrices to compute conditional branches once for the whole computation. For a wide range of sparsity, from 60% to 95% zeros, our implementation performs fewer instructions and increases performance when compared with Intel MKL's dense or sparse matrix multiply routines. Benefits can be as large as 2 times speedup and 4 times fe
&lt;/p&gt;</description></item><item><title>FanOutQA &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#20132;&#21449;&#25991;&#26723;&#20381;&#36182;&#25512;&#29702;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.14116</link><description>&lt;p&gt;
FanOutQA&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14116
&lt;/p&gt;
&lt;p&gt;
FanOutQA &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#20132;&#21449;&#25991;&#26723;&#20381;&#36182;&#25512;&#29702;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#24120;&#35265;&#20110;&#26085;&#24120;&#22330;&#26223;&#20013;&#30340;&#38382;&#39064;&#31867;&#22411;&#26159;&#8220;fan-out&#8221;&#38382;&#39064;&#65292;&#21363;&#22797;&#26434;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#25512;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#25214;&#21040;&#22823;&#37327;&#23454;&#20307;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#36164;&#28304;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;LLMs&#20013;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FanOutQA&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;fan-out&#38382;&#39064;-&#31572;&#26696;&#23545;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33521;&#25991;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#20154;&#24037;&#27880;&#37322;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#19978;&#21046;&#23450;&#20102;&#19977;&#31181;&#22522;&#20934;&#35774;&#32622;&#65292;&#24182;&#23545;7&#20010;LLMs&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;GPT-4&#12289;LLaMA 2&#12289;Claude-2.1&#21644;Mixtral-8x7B&#65292;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#25512;&#29702;&#36328;&#25991;&#26723;&#20381;&#36182;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#24320;&#28304;&#24037;&#20855;&#26469;&#36816;&#34892;&#27169;&#22411;&#65292;&#20197;&#40723;&#21169;&#22312;https://fanoutqa.com&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14116v1 Announce Type: cross  Abstract: One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;EyeTrans&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#34701;&#20837;&#26426;&#22120;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14096</link><description>&lt;p&gt;
EyeTrans: &#21512;&#24182;&#20154;&#31867;&#21644;&#26426;&#22120;&#27880;&#24847;&#21147;&#20197;&#23454;&#29616;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
EyeTrans: Merging Human and Machine Attention for Neural Code Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14096
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;EyeTrans&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#34701;&#20837;&#26426;&#22120;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural code summarization &#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#30340;&#31616;&#35201;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#12290;Transformer&#27169;&#22411;&#30340;&#21457;&#23637;&#23548;&#33268;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24191;&#27867;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#34701;&#20837;&#26426;&#22120;&#27880;&#24847;&#21147;&#20197;&#22686;&#24378;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#34701;&#21512;&#24182;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#24341;&#20837;&#20102;EyeTrans&#65292;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;(1) &#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30524;&#21160;&#20154;&#31867;&#30740;&#31350;&#65292;&#25910;&#38598;&#21644;&#39044;&#20998;&#26512;&#25968;&#25454;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#65292;(2) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#20154;&#31867;&#27880;&#24847;&#21147;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14096v1 Announce Type: cross  Abstract: Neural code summarization leverages deep learning models to automatically generate brief natural language summaries of code snippets. The development of Transformer models has led to extensive use of attention during model design. While existing work has primarily and almost exclusively focused on static properties of source code and related structural representations like the Abstract Syntax Tree (AST), few studies have considered human attention, that is, where programmers focus while examining and comprehending code. In this paper, we develop a method for incorporating human attention into machine attention to enhance neural code summarization. To facilitate this incorporation and vindicate this hypothesis, we introduce EyeTrans, which consists of three steps: (1) we conduct an extensive eye-tracking human study to collect and pre-analyze data for model training, (2) we devise a data-centric approach to integrate human attention wit
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14095</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-shot generalization across architectures for visual classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14095
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26497;&#31616;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#27867;&#21270;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#20174;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;CNNs&#65289;&#21040;transformers&#30340;&#27969;&#34892;&#32593;&#32476;&#22312;&#36890;&#36807;&#23618;&#21644;&#26550;&#26500;&#27867;&#21270;&#21040;&#26410;&#35265;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#24182;&#19988;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/dyballa/zero-shot-generalization &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20171;&#32461;&#20102;&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#25919;&#31574;&#21046;&#23450;&#30340;AI&#36890;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#19968;&#33324;&#32463;&#27982;&#29615;&#22659;&#65292;&#36890;&#36807;AI&#27169;&#25311;&#31995;&#32479;&#20998;&#26512;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#24378;&#35843;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#25919;&#31574;&#21046;&#23450;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14090</link><description>&lt;p&gt;
&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Social Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14090
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20171;&#32461;&#20102;&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#25919;&#31574;&#21046;&#23450;&#30340;AI&#36890;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#19968;&#33324;&#32463;&#27982;&#29615;&#22659;&#65292;&#36890;&#36807;AI&#27169;&#25311;&#31995;&#32479;&#20998;&#26512;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#24378;&#35843;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#25919;&#31574;&#21046;&#23450;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#21046;&#23450;&#30340;&#25216;&#26415;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20171;&#32461;&#20102;&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#25919;&#31574;&#21046;&#23450;&#30340;AI&#36890;&#29992;&#26694;&#26550;&#65292;&#19982;&#24378;&#21270;&#23398;&#20064;&#12289;&#32463;&#27982;&#19982;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#31038;&#21306;&#30456;&#36830;&#25509;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25429;&#25417;&#19968;&#33324;&#32463;&#27982;&#29615;&#22659;&#65292;&#21253;&#25324;&#23545;&#25919;&#31574;&#30446;&#26631;&#30340;&#25237;&#31080;&#65292;&#24182;&#20026;&#36890;&#36807;AI&#27169;&#25311;&#23545;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#25919;&#31574;&#21046;&#23450;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#24320;&#25918;&#38382;&#39064;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24076;&#26395;&#23454;&#29616;&#21508;&#31181;&#31038;&#20250;&#31119;&#21033;&#30446;&#26631;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#36947;&#24503;&#21644;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14090v1 Announce Type: new  Abstract: Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policy-making. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.
&lt;/p&gt;</description></item><item><title>LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14086</link><description>&lt;p&gt;
LexC-Gen: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21452;&#35821;&#35789;&#27719;&#34920;&#20026;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14086
&lt;/p&gt;
&lt;p&gt;
LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#21294;&#20047;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#35760;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#36880;&#23383;&#32763;&#35793;&#26469;&#35299;&#20915;&#65292;&#28982;&#32780;&#65292;&#21452;&#35821;&#35789;&#20856;&#36890;&#24120;&#19982;&#20219;&#21153;&#25968;&#25454;&#26377;&#38480;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#23548;&#33268;&#32763;&#35793;&#35206;&#30422;&#21644;&#35789;&#20856;&#21033;&#29992;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LexC-Gen&#30340;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LexC-Gen&#39318;&#20808;&#20351;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#21333;&#35789;&#29983;&#25104;&#19982;&#35789;&#20856;&#20860;&#23481;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21333;&#35789;&#32763;&#35793;&#23558;&#20854;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;17&#31181;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;LexC-Gen&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#19982;&#19987;&#23478;&#32763;&#35793;&#30340;&#40644;&#37329;&#25968;&#25454;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#19978;&#24179;&#22343;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#21333;&#35789;&#32763;&#35793;&#26041;&#27861;&#25552;&#39640;&#20102;5.6&#21644;8.9&#20010;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19987;&#23478;&#36845;&#20195;&#35757;&#32451;&#30340;Searchformer&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#23569;&#30340;&#25628;&#32034;&#27493;&#39588;&#26469;&#35299;&#20915;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#65292;&#21516;&#26102;&#29983;&#25104;&#26368;&#20339;&#35745;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.14083</link><description>&lt;p&gt;
&#36229;&#36234;A*&#65306;&#36890;&#36807;&#25628;&#32034;&#21160;&#21147;&#23398;&#24341;&#23548;&#20197;&#25913;&#36827;&#21464;&#21387;&#22120;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14083
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#36845;&#20195;&#35757;&#32451;&#30340;Searchformer&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#23569;&#30340;&#25628;&#32034;&#27493;&#39588;&#26469;&#35299;&#20915;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#65292;&#21516;&#26102;&#29983;&#25104;&#26368;&#20339;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21464;&#21387;&#22120;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#36825;&#31181;&#26550;&#26500;&#22312;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#20173;&#33853;&#21518;&#20110;&#20256;&#32479;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35757;&#32451;&#21464;&#21387;&#22120;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35268;&#21010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;Searchformer&#65292;&#36825;&#26159;&#19968;&#20010;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;93.7%&#30340;&#26102;&#38388;&#20869;&#26368;&#20248;&#22320;&#35299;&#20915;&#20197;&#21069;&#26410;&#35265;&#30340;Sokoban&#35868;&#39064;&#65292;&#21516;&#26102;&#27604;&#26631;&#20934;&#30340;$A^*$&#25628;&#32034;&#20351;&#29992;&#23569;&#36798;26.8%&#30340;&#25628;&#32034;&#27493;&#39588;&#12290;Searchformer&#26159;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;$A^*$&#30340;&#25628;&#32034;&#21160;&#21147;&#23398;&#12290;&#28982;&#21518;&#36890;&#36807;&#19987;&#23478;&#36845;&#20195;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25191;&#34892;&#27604;$A^*$&#25628;&#32034;&#26356;&#23569;&#30340;&#25628;&#32034;&#27493;&#39588;&#65292;&#21516;&#26102;&#29983;&#25104;&#19968;&#20010;&#26368;&#20339;&#35745;&#21010;&#12290;&#22312;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#20013;&#65292;$A^*$&#30340;&#25628;&#32034;&#21160;&#21147;&#23398;&#34987;&#34920;&#36798;&#20026;&#19968;&#20010;&#26631;&#35760;&#24207;&#21015;&#65292;&#25551;&#36848;&#20102;&#31526;&#21495;&#35268;&#21010;&#26399;&#38388;&#20219;&#21153;&#29366;&#24577;&#20309;&#26102;&#34987;&#21152;&#20837;&#21644;&#31227;&#38500;&#21040;&#25628;&#32034;&#26641;&#20013;&#12290;&#22312;&#25105;&#20204;&#20851;&#20110;&#36855;&#23467;&#23548;&#33322;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14083v1 Announce Type: new  Abstract: While Transformers have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks and present Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than standard $A^*$ search. Searchformer is an encoder-decoder Transformer model trained to predict the search dynamics of $A^*$. This model is then fine-tuned via expert iterations to perform fewer search steps than $A^*$ search while still generating an optimal plan. In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. In our ablation studies on maze navigation, we find that S
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14081</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#36816;&#21160;&#20195;&#30721;&#30340;&#38543;&#26426;&#36807;&#31243;&#27169;&#22411;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning of Noisy Time Series Collections Using Stochastic Process Models with Motion Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14081
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20855;&#26377;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#38271;&#24230;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24773;&#20917;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#23454;&#20363;&#21487;&#20197;&#30475;&#20316;&#26159;&#22024;&#26434;&#21160;&#24577;&#27169;&#22411;&#30340;&#19968;&#20010;&#26679;&#26412;&#23454;&#29616;&#65292;&#20854;&#29305;&#28857;&#26159;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#25968;&#25454;&#26159;&#28151;&#21512;&#30340;&#65292;&#30001;&#22810;&#20010;&#38543;&#26426;&#36807;&#31243;&#24314;&#27169;&#30340;&#20960;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#24207;&#21015;&#32452;&#25104;&#65292;&#20351;&#24471;&#39044;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#19981;&#26159;&#31616;&#21333;&#22320;&#23558;&#25968;&#25454;&#22238;&#24402;&#21040;&#27599;&#31181;&#26102;&#38388;&#24207;&#21015;&#31867;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#20026;&#27599;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33258;&#21160;&#20998;&#37197;&#19968;&#20010;&#31216;&#20026;&#20854;&#36816;&#21160;&#20195;&#30721;&#30340;&#31614;&#21517;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#20998;&#37197;&#30340;&#36816;&#21160;&#20195;&#30721;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#30456;&#20851;&#24615;&#30340;&#31232;&#30095;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14081v1 Announce Type: cross  Abstract: While time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging. Each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process. For many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging. Instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured Gaussian processes with learned spectral kernels. More specifically, we auto-assign each type of noisy time series data a signature vector called its motion code. Then, conditioned on each assigned motion code, we infer a sparse approximation of the corr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#35745;&#31639;&#26679;&#26412;&#26041;&#24046;&#65292;&#25552;&#39640;&#20102;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20013;&#30340;&#35268;&#33539;&#21270;&#32622;&#20449;&#39044;&#27979;&#25928;&#29575;&#21644;&#35206;&#30422;&#29575;</title><link>https://arxiv.org/abs/2402.14080</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#35268;&#33539;&#21270;&#32622;&#20449;&#39044;&#27979;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65306;&#22522;&#20110;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#30340;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Normalized Conformal Prediction and Uncertainty Quantification for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14080
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#35745;&#31639;&#26679;&#26412;&#26041;&#24046;&#65292;&#25552;&#39640;&#20102;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20013;&#30340;&#35268;&#33539;&#21270;&#32622;&#20449;&#39044;&#27979;&#25928;&#29575;&#21644;&#35206;&#30422;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27491;&#22312;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#20851;&#38190;&#20915;&#31574;&#20219;&#21153;&#65292;&#28982;&#32780;&#23427;&#20204;&#34987;&#35757;&#32451;&#20026;&#25552;&#20379;&#28857;&#39044;&#27979;&#32780;&#27809;&#26377;&#25552;&#20379;&#20449;&#24515;&#24230;&#12290;&#22914;&#26524;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#32467;&#21512;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#32622;&#20449;&#39044;&#27979;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#39044;&#27979;&#21306;&#38388;&#37197;&#23545;&#65292;&#20174;&#32780;&#21487;&#20197;&#30475;&#21040;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#29992;&#20110;&#32622;&#20449;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#26410;&#33021;&#25552;&#20379;&#23545;&#25152;&#26377;&#26679;&#26412;&#21516;&#26679;&#20934;&#30830;&#30340;&#24322;&#26041;&#24046;&#38388;&#38548;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#33719;&#24471;&#30340;&#26041;&#24046;&#26469;&#20272;&#35745;&#27599;&#20010;&#26679;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#30340;&#26041;&#24046;&#22914;&#20309;&#25552;&#39640;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#20219;&#21153;&#19978;&#35268;&#33539;&#21270;&#35825;&#23548;&#32622;&#20449;&#39044;&#27979;&#30340;&#25928;&#29575;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14080v1 Announce Type: cross  Abstract: Deep learning models are being adopted and applied on various critical decision-making tasks, yet they are trained to provide point predictions without providing degrees of confidence. The trustworthiness of deep learning models can be increased if paired with uncertainty estimations. Conformal Prediction has emerged as a promising method to pair machine learning models with prediction intervals, allowing for a view of the model's uncertainty. However, popular uncertainty estimation methods for conformal prediction fail to provide heteroskedastic intervals that are equally accurate for all samples. In this paper, we propose a method to estimate the uncertainty of each sample by calculating the variance obtained from a Deep Regression Forest. We show that the deep regression forest variance improves the efficiency and coverage of normalized inductive conformal prediction on a drug response prediction task.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14049</link><description>&lt;p&gt;
&#29992;&#20110;&#26497;&#31471;&#25968;&#25454;&#32553;&#25918;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Models for Extreme Downscaling of Climate Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#25361;&#25112;&#38656;&#35201;&#20934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#22320;&#26144;&#23556;&#27668;&#20505;&#21644;&#22825;&#27668;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#21482;&#33021;&#20197;&#38750;&#24120;&#31895;&#31961;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25552;&#20379;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#26497;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#25152;&#33268;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21450;&#20854;&#21464;&#20307;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#21319;&#33258;&#28982;&#22270;&#20687;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#22312;&#25913;&#36827;&#31185;&#23398;&#25968;&#25454;&#38598;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#29992;&#20110;&#26497;&#31471;&#32553;&#25918;&#32593;&#26684;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14049v1 Announce Type: cross  Abstract: Addressing the challenges of climate change requires accurate and high-resolution mapping of climate and weather variables. However, many existing climate datasets, such as the gridded outputs of the state-of-the-art numerical climate models (e.g., general circulation models), are only available at very coarse spatial resolutions due to the model complexity and extremely high computational demand. Deep-learning-based methods, particularly generative adversarial networks (GANs) and their variants, have proved effective for refining natural images, and have shown great promise in improving scientific datasets. In this paper, we describe a conditional GAN-based geospatial downscaling method for extreme downscaling of gridded climate datasets. Compared to most existing methods, the method can generate high-resolution accurate climate datasets from very low-resolution inputs. More importantly, the method explicitly considers the uncertainty
&lt;/p&gt;</description></item><item><title>PolyNet&#36890;&#36807;&#23398;&#20064;&#20114;&#34917;&#35299;&#20915;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#31354;&#38388;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#20154;&#20026;&#35268;&#21017;&#23548;&#33268;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14048</link><description>&lt;p&gt;
PolyNet&#65306;&#23398;&#20064;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14048
&lt;/p&gt;
&lt;p&gt;
PolyNet&#36890;&#36807;&#23398;&#20064;&#20114;&#34917;&#35299;&#20915;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#31354;&#38388;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#20154;&#20026;&#35268;&#21017;&#23548;&#33268;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#36805;&#36895;&#25509;&#36817;&#20154;&#31867;&#35774;&#35745;&#30340;&#31639;&#27861;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32553;&#23567;&#24046;&#36317;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#24517;&#39035;&#39640;&#25928;&#22320;&#25506;&#32034;&#35299;&#31354;&#38388;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#22810;&#26679;&#21270;&#35299;&#29983;&#25104;&#26469;&#20154;&#20026;&#22686;&#21152;&#25506;&#32034;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#25439;&#23475;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#24182;&#19988;&#38590;&#20197;&#20026;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#35774;&#35745;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PolyNet&#65292;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20114;&#34917;&#35299;&#20915;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#31354;&#38388;&#25506;&#32034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#20316;&#21697;&#19981;&#21516;&#65292;PolyNet&#20165;&#20351;&#29992;&#21333;&#20010;&#35299;&#30721;&#22120;&#65292;&#24182;&#19988;&#35757;&#32451;&#22270;&#24335;&#19981;&#36890;&#36807;&#20154;&#20026;&#35268;&#21017;&#24378;&#21046;&#23454;&#26045;&#22810;&#26679;&#21270;&#35299;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#35780;&#20272;PolyNet&#65292;&#24182;&#35266;&#23519;&#21040;&#38544;&#24335;&#22810;&#26679;&#24615;&#26426;&#21046;&#20801;&#35768;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14048v1 Announce Type: cross  Abstract: Reinforcement learning-based methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms. To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process. Recent approaches artificially increase exploration by enforcing diverse solution generation through handcrafted rules, however, these rules can impair solution quality and are difficult to design for more complex problems. In this paper, we introduce PolyNet, an approach for improving exploration of the solution space by learning complementary solution strategies. In contrast to other works, PolyNet uses only a single-decoder and a training schema that does not enforce diverse solution generation through handcrafted rules. We evaluate PolyNet on four combinatorial optimization problems and observe that the implicit diversity mechanism allows P
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14047</link><description>&lt;p&gt;
&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple and Effective Transfer Learning for Neuro-Symbolic Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27867;&#21270;&#21644;&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#24863;&#30693;&#26144;&#23556;&#21040;&#31526;&#21495;&#65292;&#24182;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#32773;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14047v1 Announce Type: cross  Abstract: Deep Learning (DL) techniques have achieved remarkable successes in recent years. However, their ability to generalize and execute reasoning tasks remains a challenge. A potential solution to this issue is Neuro-Symbolic Integration (NeSy), where neural approaches are combined with symbolic reasoning. Most of these methods exploit a neural network to map perceptions to symbols and a logical reasoner to predict the output of the downstream task. These methods exhibit superior generalization capacity compared to fully neural architectures. However, they suffer from several issues, including slow convergence, learning difficulties with complex perception tasks, and convergence to local minima. This paper proposes a simple yet effective method to ameliorate these problems. The key idea involves pretraining a neural model on the downstream task. Then, a NeSy model is trained on the same task via transfer learning, where the weights of the p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;mSHO&#30340;&#20840;&#26032;SHO&#31639;&#27861;&#21464;&#20307;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#26412;&#22320;&#25628;&#32034;&#31574;&#30053;&#65292;&#20998;&#20026;&#37051;&#22495;&#23616;&#37096;&#25628;&#32034;&#12289;&#20840;&#23616;&#38750;&#37051;&#22495;&#25628;&#32034;&#21644;&#32469;&#34892;&#26041;&#27861;&#65292;&#20027;&#35201;&#22686;&#24378;&#20102;SHO&#31639;&#27861;&#30340;&#24320;&#21457;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14044</link><description>&lt;p&gt;
&#22522;&#20110;&#25913;&#36827;&#30340;&#28023;&#39532;&#20248;&#21270;&#22120;&#30340;&#20840;&#23616;&#20248;&#21270;&#21644;&#24037;&#31243;&#38382;&#39064;&#27714;&#35299;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new approach for solving global optimization and engineering problems based on modified Sea Horse Optimizer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;mSHO&#30340;&#20840;&#26032;SHO&#31639;&#27861;&#21464;&#20307;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#26412;&#22320;&#25628;&#32034;&#31574;&#30053;&#65292;&#20998;&#20026;&#37051;&#22495;&#23616;&#37096;&#25628;&#32034;&#12289;&#20840;&#23616;&#38750;&#37051;&#22495;&#25628;&#32034;&#21644;&#32469;&#34892;&#26041;&#27861;&#65292;&#20027;&#35201;&#22686;&#24378;&#20102;SHO&#31639;&#27861;&#30340;&#24320;&#21457;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#39532;&#20248;&#21270;&#22120;&#65288;SHO&#65289;&#26159;&#19968;&#31181;&#20540;&#24471;&#27880;&#24847;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#27169;&#25311;&#20102;&#28023;&#39532;&#23637;&#31034;&#30340;&#21508;&#31181;&#26234;&#33021;&#34892;&#20026;&#65292;&#21253;&#25324;&#36827;&#39135;&#27169;&#24335;&#12289;&#38596;&#24615;&#32321;&#27542;&#31574;&#30053;&#21644;&#22797;&#26434;&#30340;&#36816;&#21160;&#27169;&#24335;&#12290;&#20026;&#20102;&#27169;&#20223;&#28023;&#39532;&#30340;&#24494;&#22937;&#36816;&#21160;&#65292;SHO&#38598;&#25104;&#20102;&#23545;&#25968;&#34746;&#26059;&#26041;&#31243;&#21644;Levy&#39134;&#34892;&#65292;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#23454;&#36136;&#27493;&#38271;&#30340;&#38543;&#26426;&#31227;&#21160;&#19982;&#31934;&#32454;&#30340;&#23616;&#37096;&#24320;&#21457;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#24067;&#26391;&#36816;&#21160;&#30340;&#21033;&#29992;&#20419;&#36827;&#20102;&#23545;&#25628;&#32034;&#31354;&#38388;&#30340;&#26356;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;mSHO&#30340;&#24378;&#22823;&#39640;&#24615;&#33021;&#30340;SHO&#31639;&#27861;&#21464;&#20307;&#12290;&#22686;&#24378;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#29992;&#21019;&#26032;&#30340;&#26412;&#22320;&#25628;&#32034;&#31574;&#30053;&#26367;&#25442;&#20854;&#21407;&#22987;&#26041;&#27861;&#26469;&#21152;&#24378;SHO&#30340;&#24320;&#21457;&#33021;&#21147;&#65292;&#35813;&#31574;&#30053;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#27493;&#39588;&#65306;&#22522;&#20110;&#37051;&#22495;&#30340;&#23616;&#37096;&#25628;&#32034;&#65292;&#20840;&#23616;&#38750;&#37051;&#22495;&#25628;&#32034;&#20197;&#21450;&#28041;&#21450;&#32469;&#34892;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14044v1 Announce Type: cross  Abstract: Sea Horse Optimizer (SHO) is a noteworthy metaheuristic algorithm that emulates various intelligent behaviors exhibited by sea horses, encompassing feeding patterns, male reproductive strategies, and intricate movement patterns. To mimic the nuanced locomotion of sea horses, SHO integrates the logarithmic helical equation and Levy flight, effectively incorporating both random movements with substantial step sizes and refined local exploitation. Additionally, the utilization of Brownian motion facilitates a more comprehensive exploration of the search space. This study introduces a robust and high-performance variant of the SHO algorithm named mSHO. The enhancement primarily focuses on bolstering SHO's exploitation capabilities by replacing its original method with an innovative local search strategy encompassing three distinct steps: a neighborhood-based local search, a global non-neighbor-based search, and a method involving circumnav
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GANs&#29983;&#25104;&#20102;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;GAN&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#24182;&#24310;&#20280;&#25968;&#25454;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14042</link><description>&lt;p&gt;
&#20351;&#29992;GANs&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24310;&#20280;&#19982;&#20445;&#25252;&#8212;&#8212;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Protect and Extend -- Using GANs for Synthetic Data Generation of Time-Series Medical Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GANs&#29983;&#25104;&#20102;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;GAN&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#24182;&#24310;&#20280;&#25968;&#25454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14042v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;: &#20445;&#25252;&#31169;&#20154;&#29992;&#25143;&#25968;&#25454;&#23545;&#20110;&#39640;&#36136;&#37327;&#20307;&#39564;(QoE)&#21644;&#21487;&#25509;&#21463;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22788;&#29702;&#25935;&#24863;&#25968;&#25454;&#30340;&#26381;&#21153;&#65292;&#22914;&#22522;&#20110;IT&#30340;&#20581;&#24247;&#26381;&#21153;&#12290;&#23613;&#31649;&#24050;&#32463;&#26174;&#31034;&#21311;&#21517;&#21270;&#25216;&#26415;&#23481;&#26131;&#34987;&#25968;&#25454;&#37325;&#26032;&#35782;&#21035;&#65292;&#20294;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36880;&#28176;&#21462;&#20195;&#20102;&#21311;&#21517;&#21270;&#65292;&#22240;&#20026;&#23427;&#30456;&#23545;&#32791;&#26102;&#21644;&#36164;&#28304;&#32791;&#36153;&#36739;&#23569;&#65292;&#24182;&#19988;&#26356;&#33021;&#25269;&#25239;&#25968;&#25454;&#27844;&#28431;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#24050;&#34987;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#36981;&#24490;&#24046;&#20998;&#38544;&#31169;&#29616;&#35937;&#30340;GAN&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#26368;&#26032;GAN&#22522;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20998;&#21457;&#12290; &#39044;&#27979;&#24314;&#27169;&#12289;&#33258;&#30456;&#20851;&#24615;&#21644;&#20998;&#24067;&#20998;&#26512;&#34987;&#29992;&#26469;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#36136;&#37327;(QoG)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14042v1 Announce Type: cross  Abstract: Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. Generative Adversarial Networks (GANs) have been used for generating synthetic datasets, especially GAN frameworks adhering to the differential privacy phenomena. This research compares state-of-the-art GAN-based models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. T
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19987;&#19994;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26816;&#27979;&#27599;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#19987;&#19994;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#27491;&#30830;&#30340;&#21307;&#29983;&#65292;&#37325;&#28857;&#26159;&#22788;&#29702;&#38463;&#25289;&#20271;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#31867;&#21035;&#21644;&#39640;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.14039</link><description>&lt;p&gt;
&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#22810;&#31867;&#20998;&#24067;&#24773;&#22659;&#19979;&#30340;&#36828;&#31243;&#21307;&#30103;&#19987;&#19994;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Specialty detection in the context of telemedicine in a highly imbalanced multi-class distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19987;&#19994;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26816;&#27979;&#27599;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#19987;&#19994;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#27491;&#30830;&#30340;&#21307;&#29983;&#65292;&#37325;&#28857;&#26159;&#22788;&#29702;&#38463;&#25289;&#20271;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#31867;&#21035;&#21644;&#39640;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Covid-19&#22823;&#27969;&#34892;&#23548;&#33268;&#20102;&#23545;&#36828;&#31243;&#21307;&#30103;&#26381;&#21153;&#30340;&#35748;&#35782;&#21644;&#38656;&#27714;&#22686;&#21152;&#65292;&#36827;&#32780;&#38656;&#35201;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#24182;&#20381;&#36182;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#20943;&#23569;&#36816;&#33829;&#36127;&#25285;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19987;&#19994;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26816;&#27979;&#27599;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#19987;&#19994;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#27491;&#30830;&#30340;&#21307;&#29983;&#12290;&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#22788;&#29702;&#38463;&#25289;&#20271;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#31867;&#21035;&#21644;&#39640;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#19968;&#20123;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#19987;&#19994;&#26816;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#20381;&#36182;&#20110;&#19987;&#19994;&#26816;&#27979;&#30340;&#38544;&#34255;&#19994;&#21153;&#39046;&#22495;&#65292;&#20363;&#22914;&#20026;&#19981;&#21516;&#19987;&#19994;&#23450;&#21046;&#21644;&#20010;&#24615;&#21270;&#21672;&#35810;&#27969;&#31243;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14039v1 Announce Type: cross  Abstract: The Covid-19 pandemic has led to an increase in the awareness of and demand for telemedicine services, resulting in a need for automating the process and relying on machine learning (ML) to reduce the operational load. This research proposes a specialty detection classifier based on a machine learning model to automate the process of detecting the correct specialty for each question and routing it to the correct doctor. The study focuses on handling multiclass and highly imbalanced datasets for Arabic medical questions, comparing some oversampling techniques, developing a Deep Neural Network (DNN) model for specialty detection, and exploring the hidden business areas that rely on specialty detection such as customizing and personalizing the consultation flow for different specialties. The proposed module is deployed in both synchronous and asynchronous medical consultations to provide more real-time classification, minimize the doctor 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21704;&#37324;&#26031;&#40560;&#20248;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#22312;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#20837;&#20405;&#26816;&#27979;&#38169;&#35823;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#35782;&#21035;&#24694;&#24847;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14037</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#21704;&#37324;&#26031;&#40560;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#26377;&#25928;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Effective Networks Intrusion Detection Approach Based on Hybrid Harris Hawks and Multi-Layer Perceptron
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21704;&#37324;&#26031;&#40560;&#20248;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#22312;&#32593;&#32476;&#20013;&#26368;&#23567;&#21270;&#20837;&#20405;&#26816;&#27979;&#38169;&#35823;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#35782;&#21035;&#24694;&#24847;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21704;&#37324;&#26031;&#40560;&#20248;&#21270;&#31639;&#27861;&#65288;HHO&#65289;&#26469;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#20559;&#32622;&#21644;&#26435;&#37325;&#21442;&#25968;&#12290;HHO-MLP&#26088;&#22312;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36873;&#25321;&#26368;&#20339;&#21442;&#25968;&#65292;&#20197;&#26368;&#23567;&#21270;&#32593;&#32476;&#20013;&#30340;&#20837;&#20405;&#26816;&#27979;&#38169;&#35823;&#12290; HHO-MLP&#20351;&#29992;EvoloPy NN&#26694;&#26550;&#36827;&#34892;&#23454;&#29616;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#35757;&#32451;MLPs&#30340;&#24320;&#28304;Python&#24037;&#20855;&#12290;&#20026;&#20102;&#23558;HHO&#27169;&#22411;&#19982;&#24403;&#21069;&#21487;&#29992;&#30340;&#20854;&#20182;&#36827;&#21270;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20351;&#29992;KDD&#25968;&#25454;&#38598;&#35745;&#31639;&#20102;&#29305;&#24322;&#24615;&#21644;&#25935;&#24863;&#24615;&#25351;&#26631;&#12289;&#20934;&#30830;&#24615;&#25351;&#26631;&#20197;&#21450;mse&#21644;rmse&#25351;&#26631;&#12290;&#23454;&#39564;&#34920;&#26126;HHO MLP&#26041;&#27861;&#22312;&#35782;&#21035;&#24694;&#24847;&#27169;&#24335;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14037v1 Announce Type: cross  Abstract: This paper proposes an Intrusion Detection System (IDS) employing the Harris Hawks Optimization algorithm (HHO) to optimize Multilayer Perceptron learning by optimizing bias and weight parameters. HHO-MLP aims to select optimal parameters in its learning process to minimize intrusion detection errors in networks. HHO-MLP has been implemented using EvoloPy NN framework, an open-source Python tool specialized for training MLPs using evolutionary algorithms. For purposes of comparing the HHO model against other evolutionary methodologies currently available, specificity and sensitivity measures, accuracy measures, and mse and rmse measures have been calculated using KDD datasets. Experiments have demonstrated the HHO MLP method is effective at identifying malicious patterns. HHO-MLP has been tested against evolutionary algorithms like Butterfly Optimization Algorithm (BOA), Grasshopper Optimization Algorithms (GOA), and Black Widow Optimi
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14035</link><description>&lt;p&gt;
&#22996;&#21592;&#20250;&#30340;&#26234;&#24935;&#65306;&#20174;&#22522;&#30784;&#27169;&#22411;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#30340;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14035
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#30784;&#27169;&#22411;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#29305;&#23450;&#24212;&#29992;&#65292;&#20174;&#19994;&#32773;&#20204;&#19968;&#30452;&#22312;&#24320;&#21457;&#19987;&#38376;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;&#20026;&#20102;&#20139;&#21463;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#22909;&#22788;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#36335;&#24452;&#26159;&#23558;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#65292;&#21518;&#32773;&#36890;&#24120;&#26356;&#26377;&#25928;&#22320;&#25552;&#20379;&#26381;&#21153;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#25216;&#26415;&#21487;&#20197;&#22312;&#36825;&#37324;&#24212;&#29992;&#65292;&#20854;&#20013;&#24212;&#29992;&#27169;&#22411;&#23398;&#20250;&#27169;&#20223;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#23481;&#37327;&#19978;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#20248;&#21270;&#12290;&#27169;&#22411;&#29305;&#24449;&#19978;&#30340;&#36825;&#20123;&#24046;&#24322;&#23548;&#33268;&#20102;&#33976;&#39311;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21019;&#24314;&#19968;&#20010;&#25945;&#23398;&#22996;&#21592;&#20250;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;&#21644;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14035v1 Announce Type: cross  Abstract: Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foun
&lt;/p&gt;</description></item><item><title>AgentScope&#26159;&#19968;&#20010;&#24320;&#21457;&#32773;&#20013;&#24515;&#30340;&#22810;&#20195;&#29702;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#20197;&#28040;&#24687;&#20132;&#25442;&#20026;&#26680;&#24515;&#36890;&#20449;&#26426;&#21046;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#24320;&#21457;&#21644;&#29702;&#35299;&#30340;&#38556;&#30861;&#65292;&#21516;&#26102;&#20855;&#22791;&#28789;&#27963;&#30340;&#23481;&#38169;&#26426;&#21046;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#30340;&#31995;&#32479;&#32423;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.14034</link><description>&lt;p&gt;
AgentScope: &#19968;&#20010;&#28789;&#27963;&#32780;&#21448;&#24378;&#22823;&#30340;&#22810;&#20195;&#29702;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
AgentScope: A Flexible yet Robust Multi-Agent Platform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14034
&lt;/p&gt;
&lt;p&gt;
AgentScope&#26159;&#19968;&#20010;&#24320;&#21457;&#32773;&#20013;&#24515;&#30340;&#22810;&#20195;&#29702;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#20197;&#28040;&#24687;&#20132;&#25442;&#20026;&#26680;&#24515;&#36890;&#20449;&#26426;&#21046;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#24320;&#21457;&#21644;&#29702;&#35299;&#30340;&#38556;&#30861;&#65292;&#21516;&#26102;&#20855;&#22791;&#28789;&#27963;&#30340;&#23481;&#38169;&#26426;&#21046;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#30340;&#31995;&#32479;&#32423;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22810;&#20195;&#29702;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#21327;&#35843;&#20195;&#29702;&#21512;&#20316;&#21644;LLMs&#30340;&#19981;&#31283;&#23450;&#24615;&#34920;&#29616;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#65292;&#32473;&#24320;&#21457;&#20581;&#22766;&#39640;&#25928;&#30340;&#22810;&#20195;&#29702;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentScope&#65292;&#19968;&#20010;&#20197;&#28040;&#24687;&#20132;&#25442;&#20026;&#26680;&#24515;&#36890;&#20449;&#26426;&#21046;&#30340;&#38754;&#21521;&#24320;&#21457;&#32773;&#30340;&#22810;&#20195;&#29702;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#36890;&#20449;&#26426;&#21046;&#36830;&#21516;&#20016;&#23500;&#30340;&#21477;&#27861;&#24037;&#20855;&#12289;&#20869;&#32622;&#36164;&#28304;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24320;&#21457;&#21644;&#29702;&#35299;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#23454;&#29616;&#20581;&#22766;&#21644;&#28789;&#27963;&#30340;&#22810;&#20195;&#29702;&#24212;&#29992;&#65292;AgentScope&#25552;&#20379;&#20102;&#20869;&#32622;&#21644;&#21487;&#23450;&#21046;&#30340;&#23481;&#38169;&#26426;&#21046;&#65292;&#21516;&#26102;&#36824;&#37197;&#22791;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#29983;&#25104;&#12289;&#23384;&#20648;&#21644;&#20256;&#36755;&#30340;&#31995;&#32479;&#32423;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;actor&#30340;&#20998;&#21457;&#26694;&#26550;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14034v1 Announce Type: cross  Abstract: With the rapid advancement of Large Language Models (LLMs), significant progress has been made in multi-agent applications. However, the complexities in coordinating agents' cooperation and LLMs' erratic performance pose notable challenges in developing robust and efficient multi-agent applications. To tackle these challenges, we propose AgentScope, a developer-centric multi-agent platform with message exchange as its core communication mechanism. Together with abundant syntactic tools, built-in resources, and user-friendly interactions, our communication mechanism significantly reduces the barriers to both development and understanding. Towards robust and flexible multi-agent application, AgentScope provides both built-in and customizable fault tolerance mechanisms while it is also armed with system-level supports for multi-modal data generation, storage and transmission. Additionally, we design an actor-based distribution framework, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#34394;&#25311;&#37051;&#23621;&#65288;VN&#65289;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23454;&#20307;&#23884;&#20837;&#20013;&#30340;&#37051;&#23621;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#25972;&#21512;&#36828;&#36317;&#31163;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.14033</link><description>&lt;p&gt;
VN&#32593;&#32476;&#65306;&#21033;&#29992;&#34394;&#25311;&#37051;&#23621;&#23884;&#20837;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
VN Network: Embedding Newly Emerging Entities with Virtual Neighbors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#34394;&#25311;&#37051;&#23621;&#65288;VN&#65289;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23454;&#20307;&#23884;&#20837;&#20013;&#30340;&#37051;&#23621;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#25972;&#21512;&#36828;&#36317;&#31163;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#21040;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#24341;&#36215;&#20102;&#36817;&#24180;&#26469;&#30340;&#22823;&#37327;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#25152;&#26377;&#27979;&#35797;&#23454;&#20307;&#22312;&#35757;&#32451;&#26399;&#38388;&#22343;&#21487;&#33719;&#24471;&#65292;&#36825;&#20351;&#24471;&#20026;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#37325;&#26032;&#35757;&#32451;&#23884;&#20837;&#21464;&#24471;&#32791;&#26102;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#26410;&#30693;&#23454;&#20307;&#30340;&#29616;&#26377;&#37051;&#23621;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#34394;&#25311;&#37051;&#23621;&#65288;VN&#65289;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#20943;&#23569;&#37051;&#23621;&#31232;&#30095;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;&#35268;&#21017;&#25512;&#26029;&#24471;&#20986;&#30340;&#34394;&#25311;&#37051;&#23621;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#21463;&#35268;&#21017;&#38480;&#21046;&#30340;&#38382;&#39064;&#20026;&#36825;&#20123;&#37051;&#23621;&#20998;&#37197;&#36719;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#23558;&#23427;&#20204;&#35270;&#20026;&#27627;&#19981;&#21547;&#31946;&#30340;&#30495;&#23454;&#12290;&#20854;&#27425;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#36339;&#25110;&#20004;&#36339;&#37051;&#23621;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#24573;&#30053;&#21487;&#33021;&#26377;&#29992;&#30340;&#36828;&#36317;&#31163;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#36923;&#36753;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14033v1 Announce Type: cross  Abstract: Embedding entities and relations into continuous vector spaces has attracted a surge of interest in recent years. Most embedding methods assume that all test entities are available during training, which makes it time-consuming to retrain embeddings for newly emerging entities. To address this issue, recent works apply the graph neural network on the existing neighbors of the unseen entities. In this paper, we propose a novel framework, namely Virtual Neighbor (VN) network, to address three key challenges. Firstly, to reduce the neighbor sparsity problem, we introduce the concept of the virtual neighbors inferred by rules. And we assign soft labels to these neighbors by solving a rule-constrained problem, rather than simply regarding them as unquestionably true. Secondly, many existing methods only use one-hop or two-hop neighbors for aggregation and ignore the distant information that may be helpful. Instead, we identify both logic an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.14029</link><description>&lt;p&gt;
&#20923;&#32467;&#32593;&#32476;&#20013;&#30340;&#37096;&#20998;&#25628;&#32034;&#36275;&#20197;&#25214;&#21040;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;
&lt;/p&gt;
&lt;p&gt;
Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31264;&#23494;&#32593;&#32476;&#21253;&#21547;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23376;&#32593;&#32476;--&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLTs&#65289;&#12290;&#26368;&#36817;&#65292;Gadhikar&#31561;&#20154;&#65288;2023&#24180;&#65289;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;SLTs&#20063;&#21487;&#20197;&#22312;&#38543;&#26426;&#20462;&#21098;&#30340;&#28304;&#32593;&#32476;&#20013;&#25214;&#21040;&#65292;&#20174;&#32780;&#20943;&#23569;SLT&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29978;&#33267;&#27604;&#28304;&#32593;&#32476;&#26356;&#31232;&#30095;&#30340;SLTs&#30340;&#25628;&#32034;&#65292;&#23548;&#33268;&#30001;&#20110;&#24847;&#22806;&#30340;&#39640;&#31232;&#30095;&#24615;&#32780;&#20934;&#30830;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#30340;&#20219;&#24847;&#27604;&#29575;&#20943;&#23569;SLT&#25628;&#32034;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20923;&#32467;&#19968;&#37096;&#20998;&#21021;&#22987;&#26435;&#37325;&#30340;&#38543;&#26426;&#23376;&#38598;&#65292;&#23558;&#20854;&#25490;&#38500;&#22312;&#25628;&#32034;&#31354;&#38388;&#20043;&#22806;--&#21363;&#65292;&#36890;&#36807;&#27704;&#20037;&#20462;&#21098;&#23427;&#20204;&#25110;&#23558;&#23427;&#20204;&#38145;&#23450;&#20026;SLT&#30340;&#22266;&#23450;&#37096;&#20998;&#12290;&#20107;&#23454;&#19978;&#65292;&#36890;&#36807;&#25105;&#20204;&#19982;&#38543;&#26426;&#20923;&#32467;&#21464;&#37327;&#30340;&#23376;&#38598;&#21644;&#36924;&#36817;&#65292;&#22312;&#36825;&#31181;&#20943;&#23569;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#65292;SLT&#30340;&#23384;&#22312;&#22312;&#29702;&#35770;&#19978;&#26159;&#24471;&#21040;&#20445;&#35777;&#30340;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26399;&#26435;&#25110;&#31561;&#21516;&#20110;&#23545;&#8220;&#39564;&#35777;-&#35777;&#20266;&#28216;&#25103;&#8221;&#30340;&#32467;&#26524;&#25276;&#27880;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#22266;&#23450;&#35299;&#20915;&#26631;&#20934;&#38382;&#39064;&#30340;&#39044;&#27979;&#24066;&#22330;&#12290;</title><link>https://arxiv.org/abs/2402.14021</link><description>&lt;p&gt;
&#23545;&#20110;&#26082;&#38750;&#21487;&#39564;&#35777;&#21448;&#38750;&#21487;&#35777;&#20266;&#30340;&#20107;&#29289;&#30340;&#25276;&#27880;
&lt;/p&gt;
&lt;p&gt;
Betting on what is neither verifiable nor falsifiable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26399;&#26435;&#25110;&#31561;&#21516;&#20110;&#23545;&#8220;&#39564;&#35777;-&#35777;&#20266;&#28216;&#25103;&#8221;&#30340;&#32467;&#26524;&#25276;&#27880;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#22266;&#23450;&#35299;&#20915;&#26631;&#20934;&#38382;&#39064;&#30340;&#39044;&#27979;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24066;&#22330;&#23545;&#20272;&#35745;&#22312;&#26576;&#19968;&#22266;&#23450;&#26102;&#38388;&#28857;&#19978;&#23558;&#25581;&#26195;&#30495;&#23454;&#24615;&#30340;&#22768;&#26126;&#30340;&#27010;&#29575;&#24456;&#26377;&#29992; - &#36825;&#21253;&#25324;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#20107;&#20214;&#20215;&#20540;&#30340;&#38382;&#39064;&#65288;&#21363;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#65289;&#65292;&#20197;&#21450;&#20851;&#20110;&#21407;&#22987;&#36882;&#24402;&#20989;&#25968;&#20215;&#20540;&#30340;&#38382;&#39064;&#65288;&#21363;&#36923;&#36753;&#25110;&#31639;&#27861;&#19981;&#30830;&#23450;&#24615;&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#27809;&#26377;&#22266;&#23450;&#35299;&#20915;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#23558;&#39044;&#27979;&#24066;&#22330;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20110;&#36825;&#31867;&#38382;&#39064;&#24448;&#24448;&#20165;&#28041;&#21450;&#39044;&#27979;&#19968;&#20010;&#21477;&#23376;&#26159;&#21542;&#20026;&#30495;&#65292;&#32780;&#19981;&#26159;&#23427;&#26159;&#21542;&#23558;&#34987;&#35777;&#26126;&#12290;&#36825;&#31867;&#38382;&#39064;&#21487;&#20197;&#34987;&#26356;&#22522;&#26412;&#20107;&#20214;&#30340;&#21487;&#25968;&#24182;&#25110;&#20132;&#38598;&#30340;&#26041;&#24335;&#34920;&#31034;&#65292;&#25110;&#32773;&#34987;&#34920;&#31034;&#20026;&#31639;&#26415;&#23618;&#27425;&#19978;&#30340;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65288;&#29978;&#33267;&#36229;&#31639;&#26415;&#21477;&#23376;&#30340; FOL &#20043;&#22806;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26399;&#26435;&#23545;&#36825;&#31181;&#20107;&#20214;&#36827;&#34892;&#25276;&#27880;&#30340;&#26041;&#27861;&#65292;&#25110;&#32773;&#31561;&#25928;&#22320;&#20316;&#20026;&#23545;&#8220;&#39564;&#35777;-&#35777;&#20266;&#28216;&#25103;&#8221;&#30340;&#32467;&#26524;&#25276;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14021v1 Announce Type: cross  Abstract: Prediction markets are useful for estimating probabilities of claims whose truth will be revealed at some fixed time -- this includes questions about the values of real-world events (i.e. statistical uncertainty), and questions about the values of primitive recursive functions (i.e. logical or algorithmic uncertainty). However, they cannot be directly applied to questions without a fixed resolution criterion, and real-world applications of prediction markets to such questions often amount to predicting not whether a sentence is true, but whether it will be proven. Such questions could be represented by countable unions or intersections of more basic events, or as First-Order-Logic sentences on the Arithmetical Hierarchy (or even beyond FOL, as hyperarithmetical sentences). In this paper, we propose an approach to betting on such events via options, or equivalently as bets on the outcome of a "verification-falsification game". Our work 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13764</link><description>&lt;p&gt;
CriticBench: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#35770;&#23478;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Evaluating Large Language Models as Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13764
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102; CriticBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22235;&#20010;&#20851;&#38190;&#35780;&#35770;&#33021;&#21147;&#32500;&#24230;&#65288;&#21453;&#39304;&#12289;&#27604;&#36739;&#12289;&#25913;&#36827;&#21644;&#20803;&#21453;&#39304;&#65289;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;CriticBench&#21253;&#21547;&#20061;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#36136;&#37327;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#35770;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#26377;&#36259;&#30340;&#20851;&#31995;&#12290;CriticBench&#30340;&#25968;&#25454;&#38598;&#12289;&#36164;&#28304;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#23558;&#22312;https://github.com/gmftbyGMFTBY/Cri&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
&lt;/p&gt;</description></item><item><title>DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13711</link><description>&lt;p&gt;
DSLR&#65306;&#22810;&#26679;&#24615;&#22686;&#24378;&#21644;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13711
&lt;/p&gt;
&lt;p&gt;
DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20013;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#22270;&#25345;&#32493;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25773;&#30340;GCL&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#33410;&#28857;&#24182;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#37325;&#25773;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#20379;&#22312;&#35757;&#32451;&#21518;&#32493;&#20219;&#21153;&#26102;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#32771;&#34385;&#27599;&#20010;&#22238;&#25918;&#33410;&#28857;&#30340;&#31867;&#21035;&#20195;&#34920;&#24615;&#20250;&#20351;&#22238;&#25918;&#33410;&#28857;&#38598;&#20013;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#20013;&#24515;&#21608;&#22260;&#65292;&#21487;&#33021;&#23384;&#22312;&#36807;&#25311;&#21512;&#20110;&#20301;&#20110;&#37027;&#20123;&#21306;&#22495;&#30340;&#33410;&#28857;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#21152;&#21095;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#23569;&#25968;&#22238;&#25918;&#33410;&#28857;&#26469;&#20445;&#30041;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#19981;&#30456;&#20851;&#37051;&#23621;&#30340;&#22238;&#25918;&#33410;&#28857;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#26174;&#30528;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLR&#30340;GCL&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#65288;CD&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2402.13463</link><description>&lt;p&gt;
RefuteBench&#65306;&#35780;&#20272;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39539;&#25351;&#20196;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#26085;&#30410;&#25193;&#22823;&#12290;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#26681;&#25454;&#27169;&#22411;&#30340;&#36755;&#20986;&#25552;&#20379;&#21453;&#39304;&#65292;&#24076;&#26395;&#24471;&#21040;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#39304;&#23436;&#25104;&#21709;&#24212;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#21542;&#24688;&#24403;&#22320;&#21709;&#24212;&#29992;&#25143;&#30340;&#21453;&#39539;&#21453;&#39304;&#24182;&#22987;&#32456;&#25191;&#34892;&#19979;&#21435;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;RefuteBench&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#20219;&#21153;&#12290;&#35780;&#20272;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#31215;&#26497;&#25509;&#21463;&#21453;&#39539;&#25351;&#20196;&#24418;&#24335;&#30340;&#21453;&#39304;&#65292;&#24182;&#26159;&#21542;&#33021;&#22815;&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#36981;&#24490;&#29992;&#25143;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#20247;&#22810;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;LLMs&#20542;&#21521;&#22266;&#25191;&#65292;&#21363;&#20542;&#21521;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#65292;&#32463;&#24120;&#26410;&#33021;&#36981;&#23432;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13463v1 Announce Type: cross  Abstract: The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the leng
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36125;&#21494;&#26031;&#35268;&#21017;&#24402;&#32435;&#65292;&#26032;&#24341;&#20837;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#25512;&#26029;&#29616;&#26377;&#20154;&#32676;&#30340;&#35268;&#33539;&#65292;&#20351;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#20849;&#20139;&#30340;&#35268;&#33539;&#65292;&#20174;&#32780;&#23454;&#29616;&#35268;&#33539;&#20307;&#31995;&#30340;&#31283;&#23450;&#24615;</title><link>https://arxiv.org/abs/2402.13399</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#35268;&#21017;&#24402;&#32435;&#22312;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#23398;&#20064;&#21644;&#32500;&#25345;&#20849;&#20139;&#30340;&#35268;&#33539;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13399
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#35268;&#21017;&#24402;&#32435;&#65292;&#26032;&#24341;&#20837;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#25512;&#26029;&#29616;&#26377;&#20154;&#32676;&#30340;&#35268;&#33539;&#65292;&#20351;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#20849;&#20139;&#30340;&#35268;&#33539;&#65292;&#20174;&#32780;&#23454;&#29616;&#35268;&#33539;&#20307;&#31995;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31038;&#20250;&#30340;&#19968;&#20010;&#26222;&#36941;&#29305;&#24449;&#26159;&#37319;&#29992;&#35268;&#21017;&#21644;&#35268;&#33539;&#20307;&#31995;&#26469;&#26381;&#21153;&#20110;&#21512;&#20316;&#30446;&#30340;&#12290;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#21487;&#20197;&#23398;&#20064;&#24182;&#36981;&#23432;&#36825;&#19968;&#20307;&#31995;&#30340;&#26234;&#33021;&#20307;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#28789;&#27963;&#22320;&#19982;&#20154;&#31867;&#26426;&#26500;&#21512;&#20316;&#65311;&#25105;&#20204;&#20551;&#35774;&#65292;&#36890;&#36807;&#20551;&#23450;&#23384;&#22312;&#19968;&#20010;&#20849;&#20139;&#30340;&#35268;&#33539;&#38598;&#65292;&#22823;&#22810;&#25968;&#20854;&#20182;&#20154;&#20250;&#36981;&#23432;&#36825;&#20123;&#35268;&#33539;&#65292;&#21516;&#26102;&#36861;&#27714;&#20182;&#20204;&#20010;&#20154;&#30340;&#24895;&#26395;&#65292;&#21363;&#20351;&#20182;&#20204;&#19981;&#30693;&#36947;&#36825;&#20123;&#35268;&#33539;&#30340;&#30830;&#20999;&#20869;&#23481;&#12290;&#36890;&#36807;&#20551;&#35774;&#20849;&#20139;&#35268;&#33539;&#65292;&#26032;&#24341;&#20837;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#20174;&#36981;&#23432;&#21644;&#36829;&#21453;&#30340;&#35266;&#23519;&#20013;&#25512;&#26029;&#29616;&#26377;&#20154;&#32676;&#30340;&#35268;&#33539;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26368;&#21021;&#22312;&#23545;&#35268;&#33539;&#30340;&#20449;&#24565;&#19978;&#23384;&#22312;&#20998;&#27495;&#65292;&#19968;&#32452;&#26234;&#33021;&#20307;&#20063;&#21487;&#20197;&#25910;&#25947;&#21040;&#20849;&#20139;&#30340;&#35268;&#33539;&#65292;&#20174;&#32780;&#23454;&#29616;&#35268;&#33539;&#20307;&#31995;&#30340;&#31283;&#23450;&#24615;&#65306;&#30001;&#20110;&#26234;&#33021;&#20307;&#21487;&#20197;&#20351;&#35268;&#33539;&#21464;&#20026;&#20849;&#35782;&#30693;&#35782;&#65292;&#36825;&#23548;&#33268;&#35268;&#33539;&#24471;&#21040;&#24191;&#27867;&#36981;&#23432;&#65292;&#20174;&#32780;&#20351;&#26032;&#30340;&#21442;&#19982;&#32773;&#24471;&#20197;&#21152;&#20837;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13399v1 Announce Type: new  Abstract: A universal feature of human societies is the adoption of systems of rules and norms in the service of cooperative ends. How can we build learning agents that do the same, so that they may flexibly cooperate with the human institutions they are embedded in? We hypothesize that agents can achieve this by assuming there exists a shared set of norms that most others comply with while pursuing their individual desires, even if they do not know the exact content of those norms. By assuming shared norms, a newly introduced agent can infer the norms of an existing population from observations of compliance and violation. Furthermore, groups of agents can converge to a shared set of norms, even if they initially diverge in their beliefs about what the norms are. This in turn enables the stability of the normative system: since agents can bootstrap common knowledge of the norms, this leads the norms to be widely adhered to, enabling new entrants 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.13352</link><description>&lt;p&gt;
KetGPT -- &#20351;&#29992;Transformer&#23545;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#65292;&#34920;&#31034;&#20026;&#37327;&#23376;&#30005;&#36335;&#65292;&#21487;&#29992;&#20316;&#35780;&#20272;&#37327;&#23376;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22312;&#35813;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#65292;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#30005;&#36335;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#30005;&#36335;&#24182;&#19981;&#26159;&#20195;&#34920;&#24615;&#22522;&#20934;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#37327;&#23376;&#31995;&#32479;&#21046;&#36896;&#30340;&#30495;&#23454;&#37327;&#23376;&#31639;&#27861;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#8220;&#26377;&#29992;&#8221;&#30340;&#37327;&#23376;&#22522;&#20934;&#26500;&#25104;&#20102;&#25512;&#21160;&#37327;&#23376;&#32534;&#35793;&#22120;&#21644;&#30828;&#20214;&#24320;&#21457;&#19982;&#27604;&#36739;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KetGPT&#65292;&#19968;&#31181;&#20197;OpenQASM&#35821;&#35328;&#29983;&#25104;&#21512;&#25104;&#30005;&#36335;&#30340;&#24037;&#20855;&#65292;&#20854;&#32467;&#26500;&#26159;&#22522;&#20110;&#25512;&#23548;&#33258;&#37327;&#23376;&#30005;&#36335;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13352v1 Announce Type: cross  Abstract: Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived f
&lt;/p&gt;</description></item><item><title>AEA&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;Project Aria&#30524;&#38236;&#35760;&#24405;&#30340;&#31532;&#19968;&#20154;&#31216;&#22810;&#27169;&#24577;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#20010;&#20329;&#25140;&#32773;&#22312;&#23460;&#20869;&#19981;&#21516;&#20301;&#32622;&#35760;&#24405;&#30340;&#26085;&#24120;&#27963;&#21160;&#24207;&#21015;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;3D&#36712;&#36857;&#12289;&#22330;&#26223;&#28857;&#20113;&#12289;&#30524;&#29699;&#27880;&#35270;&#21521;&#37327;&#21644;&#35821;&#38899;&#36716;&#24405;&#31561;&#26426;&#22120;&#24863;&#30693;&#25968;&#25454;&#65292;&#25903;&#25345;&#31070;&#32463;&#22330;&#26223;&#37325;&#24314;&#21644;&#25552;&#31034;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2402.13349</link><description>&lt;p&gt;
Aria Everyday Activities &#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Aria Everyday Activities Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13349
&lt;/p&gt;
&lt;p&gt;
AEA&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;Project Aria&#30524;&#38236;&#35760;&#24405;&#30340;&#31532;&#19968;&#20154;&#31216;&#22810;&#27169;&#24577;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#20010;&#20329;&#25140;&#32773;&#22312;&#23460;&#20869;&#19981;&#21516;&#20301;&#32622;&#35760;&#24405;&#30340;&#26085;&#24120;&#27963;&#21160;&#24207;&#21015;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;3D&#36712;&#36857;&#12289;&#22330;&#26223;&#28857;&#20113;&#12289;&#30524;&#29699;&#27880;&#35270;&#21521;&#37327;&#21644;&#35821;&#38899;&#36716;&#24405;&#31561;&#26426;&#22120;&#24863;&#30693;&#25968;&#25454;&#65292;&#25903;&#25345;&#31070;&#32463;&#22330;&#26223;&#37325;&#24314;&#21644;&#25552;&#31034;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Aria Everyday Activities (AEA)&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;Project Aria&#30524;&#38236;&#35760;&#24405;&#30340;&#31532;&#19968;&#20154;&#31216;&#22810;&#27169;&#24577;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;AEA&#21253;&#21547;&#20102;&#30001;&#22810;&#21517;&#20329;&#25140;&#32773;&#22312;&#20116;&#20010;&#22320;&#29702;&#19978;&#22810;&#26679;&#30340;&#23460;&#20869;&#20301;&#32622;&#35760;&#24405;&#30340;143&#20010;&#26085;&#24120;&#27963;&#21160;&#24207;&#21015;&#12290;&#27599;&#20010;&#35760;&#24405;&#37117;&#21253;&#21547;&#36890;&#36807;Project Aria&#30524;&#38236;&#35760;&#24405;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;AEA&#36824;&#25552;&#20379;&#20102;&#26426;&#22120;&#24863;&#30693;&#25968;&#25454;&#65292;&#21253;&#25324;&#39640;&#39057;&#20840;&#23616;&#23545;&#40784;&#30340;3D&#36712;&#36857;&#65292;&#22330;&#26223;&#28857;&#20113;&#65292;&#36880;&#24103;3D&#30524;&#29699;&#27880;&#35270;&#21521;&#37327;&#21644;&#26102;&#38388;&#23545;&#40784;&#30340;&#35821;&#38899;&#36716;&#24405;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#19968;&#25968;&#25454;&#38598;&#23454;&#29616;&#30340;&#19968;&#20123;&#31034;&#20363;&#30740;&#31350;&#24212;&#29992;&#65292;&#21253;&#25324;&#31070;&#32463;&#22330;&#26223;&#37325;&#24314;&#21644;&#25552;&#31034;&#20998;&#21106;&#12290;AEA&#26159;&#19968;&#20010;&#21487;&#20197;&#20174;projectaria.com&#19979;&#36733;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22914;&#20309;&#22312;Project Aria Tools&#20013;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#24320;&#28304;&#23454;&#29616;&#21644;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13349v1 Announce Type: cross  Abstract: We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal open dataset recorded using Project Aria glasses. AEA contains 143 daily activity sequences recorded by multiple wearers in five geographically diverse indoor locations. Each of the recording contains multimodal sensor data recorded through the Project Aria glasses. In addition, AEA provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned speech transcription. In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and prompted segmentation. AEA is an open source dataset that can be downloaded from projectaria.com. We are also providing open-source implementations and examples of how to use the dataset in Project Aria Tools.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Dynamic Multi-network Mining (DMM)&#65292;&#33021;&#22815;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#19981;&#21516;&#38271;&#24230;&#30340;&#27573;&#32452;&#65292;&#36890;&#36807;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#25552;&#20379;&#32858;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11773</link><description>&lt;p&gt;
&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#22810;&#32593;&#32476;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Dynamic Multi-Network Mining of Tensor Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11773
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Dynamic Multi-network Mining (DMM)&#65292;&#33021;&#22815;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#19981;&#21516;&#38271;&#24230;&#30340;&#27573;&#32452;&#65292;&#36890;&#36807;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#25552;&#20379;&#32858;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#23376;&#24207;&#21015;&#32858;&#31867;&#26159;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#35299;&#37322;&#32467;&#26524;&#32858;&#31867;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36890;&#24120;&#25105;&#20204;&#27809;&#26377;&#20851;&#20110;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#38754;&#23545;&#30001;&#21253;&#21547;&#26102;&#38388;&#25139;&#22312;&#20869;&#30340;&#22810;&#31181;&#27169;&#24335;&#32452;&#25104;&#30340;&#22823;&#37327;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#22914;&#20309;&#20026;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#23454;&#29616;&#23376;&#24207;&#21015;&#32858;&#31867;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35265;&#35299;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#22810;&#32593;&#32476;&#25366;&#25496;&#65288;DMM&#65289;&#65292;&#23427;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#30001;l1&#33539;&#25968;&#32422;&#26463;&#30340;&#19968;&#32452;&#21508;&#31181;&#38271;&#24230;&#30340;&#27573;&#32452;&#65288;&#21363;&#32858;&#31867;&#65289;&#29305;&#24449;&#21270;&#30340;&#20381;&#36182;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#12290;(a) &#21487;&#35299;&#37322;&#24615;&#65306;&#23427;&#20351;&#29992;&#22810;&#20010;&#32593;&#32476;&#23545;&#32858;&#31867;&#36827;&#34892;&#29305;&#24449;&#25551;&#36848;&#65292;&#27599;&#20010;&#32593;&#32476;&#26159;&#30456;&#24212;&#38750;&#26102;&#38388;&#27169;&#24335;&#30340;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#20379;&#21487;&#35265;&#19988;&#21487;&#35299;&#37322;&#30340;&#20851;&#38190;&#20851;&#31995;&#35265;&#35299;&#12290; (b) &#31934;&#30830;&#24615;&#65306;&#23427;&#21457;&#29616;&#20102;&#32858;&#31867;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11773v1 Announce Type: cross  Abstract: Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clus
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.11753</link><description>&lt;p&gt;
ArtPrompt: &#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#23545;&#40784;LLMs&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11753
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#25968;&#25454;&#36807;&#28388;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21152;&#24378;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24050;&#30693;&#30340;&#25216;&#26415;&#20551;&#35774;&#29992;&#20110;&#23545;&#40784;LLMs&#23433;&#20840;&#24615;&#30340;&#35821;&#26009;&#24211;&#20165;&#30001;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#19981;&#25104;&#31435;&#65292;&#23548;&#33268;LLMs&#23384;&#22312;&#20005;&#37325;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;Vision-in-Text Challenge&#65288;ViTC&#65289;&#26469;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#19981;&#33021;&#20165;&#36890;&#36807;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#30340;&#25552;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20116;&#20010;SOTA LLMs&#65288;GPT-3.5&#12289;GPT-4&#12289;Gemini&#12289;Claude&#21644;Llama2&#65289;&#22312;&#35782;&#21035;&#20197;ASCII&#33402;&#26415;&#24418;&#24335;&#25552;&#20379;&#30340;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11140</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#25552;&#21319;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35797;&#38169;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#24615;&#33021;&#20851;&#38190;&#21462;&#20915;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#24605;&#32500;&#38142;&#31034;&#33539;&#20316;&#20026;&#31034;&#20363;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;&#20363;&#22914;Thought Tree&#65289;&#25351;&#20986;&#20102;&#22312;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#25512;&#29702;&#27493;&#39588;&#36873;&#25321;&#20013;&#65292;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#35768;&#22810;&#24605;&#32500;&#26641;&#26469;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#36825;&#23558;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;BoT&#20174;&#19968;&#20010;&#31616;&#21333;&#25552;&#31034;&#24320;&#22987;&#65292;&#26080;&#38656;&#31034;&#20363;&#65292;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#35780;&#20272;&#22823;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#21033;&#29992;LLM&#33719;&#24471;&#30340;&#38169;&#35823;&#20998;&#26512;&#26469;&#26126;&#30830;&#20462;&#25913;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11140v1 Announce Type: new  Abstract: The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompt
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11005</link><description>&lt;p&gt;
&#25506;&#31350;&#20215;&#20540;&#20559;&#22909;&#65306;LLMs&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Exploring Value Biases: How LLMs Deviate Towards the Ideal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11005
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#37096;&#32626;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#21709;&#24212;&#23545;&#31038;&#20250;&#20135;&#29983;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#24433;&#21709;&#12290;&#29702;&#35299;LLMs&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#30340;&#38750;&#25925;&#24847;&#26426;&#21046;&#23545;&#20110;&#35299;&#37322;&#23427;&#20204;&#30340;&#24615;&#33021;&#24182;&#36776;&#21035;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20559;&#24046;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31867;&#20284;&#20110;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#36825;&#31181;&#26080;&#24847;&#35782;&#30340;&#21709;&#24212;&#34987;&#31216;&#20026;&#25277;&#26679;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#36825;&#31181;&#25277;&#26679;&#29616;&#35937;&#65292;&#21457;&#29616;LLMs&#30340;&#25277;&#26679;&#20542;&#21521;&#20110;&#20559;&#29233;&#39640;&#20215;&#20540;&#36873;&#39033;&#12290;&#20215;&#20540;&#20559;&#22909;&#23545;&#24212;&#20110;&#20174;&#26368;&#21487;&#33021;&#30340;&#21709;&#24212;&#21521;LLM&#20013;&#20195;&#34920;&#30340;&#29702;&#24819;&#20215;&#20540;&#30340;&#36716;&#21464;&#12290;&#23454;&#38469;&#19978;&#65292;&#21363;&#20415;&#26159;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#23398;&#20064;&#21040;&#30340;&#26032;&#23454;&#20307;&#65292;&#36825;&#31181;&#25928;&#26524;&#20063;&#33021;&#22815;&#20877;&#29616;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#20559;&#24046;&#34920;&#29616;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22320;&#26041;&#65292;&#24182;&#23545;&#36873;&#25321;&#20856;&#22411;&#23454;&#20363;&#31561;&#30456;&#20851;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20215;&#20540;&#20559;&#22909;&#22312;&#19981;&#21516;&#20998;&#31867;&#30340;LLMs&#20013;&#37117;&#24456;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11005v1 Announce Type: cross  Abstract: Large-Language-Models (LLMs) are deployed in a wide range of applications, and their response has an increasing social impact. Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications. This is analogous to human studies, where such inadvertent responses are referred to as sampling. We study this sampling of LLMs in light of value bias and show that the sampling of LLMs tends to favour high-value options. Value bias corresponds to this shift of response from the most likely towards an ideal value represented in the LLM. In fact, this effect can be reproduced even with new entities learnt via in-context prompting. We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars. The results show that value bias is strong in LLMs across different categor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10991</link><description>&lt;p&gt;
&#21152;&#36895;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Semi-Asynchronous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#22312;&#20854;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#65292;&#22914;Federated Averaging&#65288;FedAvg&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24050;&#32463;&#34987;&#35777;&#26126;&#25910;&#25947;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23458;&#25143;&#31471;&#20197;&#21516;&#27493;&#26041;&#24335;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#19978;&#20256;&#33267;&#26381;&#21153;&#22120;&#65292;&#36825;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#21464;&#24471;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#32487;&#32493;&#20351;&#29992;&#38472;&#26087;&#30340;&#20840;&#23616;&#27169;&#22411;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20165;&#32858;&#21512;&#20102;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#20854;&#30456;&#23545;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#38472;&#26087;&#31243;&#24230;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
&lt;/p&gt;</description></item><item><title>Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10251</link><description>&lt;p&gt;
Brant-2&#65306;&#33041;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brant-2: Foundation Model for Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10251
&lt;/p&gt;
&lt;p&gt;
Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#20998;&#26512;&#33041;&#20449;&#21495;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#19968;&#39046;&#22495;&#28085;&#30422;&#20102;&#20247;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#26159;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;Brant-2&#12290;&#19982;&#29992;&#20110;&#39045;&#20869;&#31070;&#32463;&#20449;&#21495;&#30340;&#22522;&#30784;&#27169;&#22411;Brant&#30456;&#27604;&#65292;Brant-2&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Brant-2&#23545;&#33041;&#20449;&#21495;&#20013;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;Brant-2&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#39564;&#35777;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20445;&#25345;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MM-Point&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#20132;&#20114;&#21644;&#20256;&#36755;&#23454;&#29616;&#20102;3D&#29289;&#20307;&#21644;&#22810;&#20010;2D&#35270;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#22686;&#24378;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MM-Point&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10002</link><description>&lt;p&gt;
MM-Point: &#22810;&#35270;&#35282;&#20449;&#24687;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#19977;&#32500;&#28857;&#20113;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MM-Point&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#20132;&#20114;&#21644;&#20256;&#36755;&#23454;&#29616;&#20102;3D&#29289;&#20307;&#21644;&#22810;&#20010;2D&#35270;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#22686;&#24378;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MM-Point&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24863;&#30693;&#39046;&#22495;&#20013;&#65292;&#23558;&#22810;&#31181;&#20256;&#24863;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#23558;2D&#35270;&#22270;&#19978;&#30340;&#35270;&#35273;&#20449;&#24687;&#26144;&#23556;&#21040;3D&#29289;&#20307;&#19978;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#36827;&#34892;&#29702;&#35299;&#12290;&#20294;&#26159;&#22312;&#20174;&#19981;&#21516;&#35282;&#24230;&#28210;&#26579;&#30340;&#21333;&#20010;2D&#35270;&#22270;&#20013;&#65292;&#21482;&#33021;&#25552;&#20379;&#26377;&#38480;&#30340;&#37096;&#20998;&#20449;&#24687;&#12290;&#22810;&#35270;&#35282;2D&#20449;&#24687;&#30340;&#20016;&#23500;&#24615;&#21644;&#20215;&#20540;&#21487;&#20197;&#20026;3D&#29289;&#20307;&#25552;&#20379;&#20248;&#31168;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MM-Point&#65292;&#23427;&#21463;&#21040;&#20869;&#27169;&#24577;&#21644;&#22806;&#27169;&#24577;&#30456;&#20284;&#24230;&#30446;&#26631;&#30340;&#39537;&#21160;&#12290;MM-Point&#30340;&#26680;&#24515;&#22312;&#20110;3D&#29289;&#20307;&#21644;&#22810;&#20010;2D&#35270;&#22270;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#21644;&#20256;&#36755;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#21516;&#26102;&#25191;&#34892;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;2D&#22810;&#35270;&#22270;&#20449;&#24687;&#19968;&#33268;&#24615;&#20132;&#21449;&#27169;&#24577;&#30446;&#26631;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;(Multi-MLP)&#21644;&#22810;&#23618;&#32423;&#22686;&#24378;&#31574;&#30053;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MM-Point&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10002v1 Announce Type: cross  Abstract: In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully desig
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09721</link><description>&lt;p&gt;
&#35828;&#26381;&#19968;&#20301;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Persuading a Learning Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09721
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20219;&#20309;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#20195;&#29702;&#20154;&#20351;&#29992;&#31639;&#27861;&#26469;&#23398;&#20064;&#22914;&#20309;&#23545;&#22996;&#25176;&#20154;&#30340;&#20449;&#21495;&#20570;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#36817;&#20284;&#22320;&#26368;&#20339;&#21709;&#24212;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#65306;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#21487;&#20197;&#20445;&#35777;&#20854;&#25928;&#29992;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#20043;&#38388;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#65307;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#22996;&#25176;&#20154;&#22312;&#23398;&#20064;&#27169;&#22411;&#19982;&#38750;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25928;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
&lt;/p&gt;</description></item><item><title>CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09664</link><description>&lt;p&gt;
CodeMind:&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeMind: A Framework to Challenge Large Language Models for Code Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09664
&lt;/p&gt;
&lt;p&gt;
CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#30721;&#21512;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#35780;&#20272;&#25110;&#20419;&#36827;&#20855;&#26377;&#25968;&#25454;&#27844;&#28431;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeMind&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;CodeMind&#30446;&#21069;&#25903;&#25345;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#65306;&#29420;&#31435;&#25191;&#34892;&#25512;&#29702;&#65288;IER&#65289;&#12289;&#20381;&#36182;&#25191;&#34892;&#25512;&#29702;&#65288;DER&#65289;&#21644;&#35268;&#33539;&#25512;&#29702;&#65288;SR&#65289;&#12290;&#21069;&#20004;&#32773;&#35780;&#20272;&#27169;&#22411;&#20197;&#39044;&#27979;&#20219;&#24847;&#20195;&#30721;&#30340;&#25191;&#34892;&#36755;&#20986;&#65292;&#25110;&#32773;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21512;&#25104;&#30340;&#20195;&#30721;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#23454;&#29616;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;CodeMind&#23545;&#20004;&#31181;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20116;&#20010;&#22522;&#20934;&#19979;&#30340;&#20061;&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07610</link><description>&lt;p&gt;
&#36393;&#33050;&#35843;&#26657;&#65306;&#36890;&#36807;&#33258;&#21161;&#24341;&#23548;&#25193;&#23637;LLM&#30340;&#33258;&#23545;&#40784;&#33021;&#21147;&#30340;&#35268;&#27169;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23545;&#40784;&#26159;&#19968;&#31181;&#38477;&#20302;&#20154;&#24037;&#27880;&#37322;&#25104;&#26412;&#24182;&#30830;&#20445;&#27169;&#22411;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#21333;&#27425;&#24490;&#29615;&#20013;&#23436;&#25104;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#27493;&#39588;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#33258;&#23545;&#40784;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#36827;&#34892;&#22810;&#27425;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#65292;&#20250;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#36824;&#26159;&#23548;&#33268;&#24555;&#36895;&#36864;&#21270;&#65311;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20445;&#35777;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#25381;&#33258;&#21161;&#24341;&#23548;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#24182;&#35843;&#25972;&#20102;&#25968;&#25454;&#30340;&#35757;&#32451;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36393;&#33050;&#35843;&#26657;&#65288;SOFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27169;&#22411;&#30340;&#25345;&#32493;&#22686;&#24378;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;255&#31687;&#35770;&#25991;&#30340;&#20998;&#26512;&#21644;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#32771;&#34385;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31532;&#19968;&#24180;&#21457;&#24067;&#21518;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03927</link><description>&lt;p&gt;
&#27844;&#28431;&#12289;&#27450;&#39575;&#12289;&#37325;&#22797;&#65306;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;255&#31687;&#35770;&#25991;&#30340;&#20998;&#26512;&#21644;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#32771;&#34385;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31532;&#19968;&#24180;&#21457;&#24067;&#21518;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20854;&#20013;&#19968;&#20123;&#26368;&#21463;&#27426;&#36814;&#30340;&#27169;&#22411;&#26159;&#23436;&#20840;&#25110;&#37096;&#20998;&#23553;&#38381;&#28304;&#30340;&#12290;&#23545;&#20110;&#27169;&#22411;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#35775;&#38382;&#26435;&#38480;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21453;&#22797;&#23545;&#25968;&#25454;&#27745;&#26579;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#23581;&#35797;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20165;&#38480;&#20110;&#20010;&#21035;&#26696;&#20363;&#21644;&#35797;&#38169;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#8220;&#38388;&#25509;&#8221;&#25968;&#25454;&#27844;&#28431;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#20351;&#29992;&#19978;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#20998;&#26512;&#65292;&#36825;&#20123;&#26159;&#24403;&#20170;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#65292;&#24182;&#32771;&#34385;&#20102;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#65292;&#35814;&#32454;&#35760;&#24405;&#20102;&#27169;&#22411;&#21457;&#24067;&#21518;&#19968;&#24180;&#20869;&#27844;&#38706;&#32473;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20027;&#35201;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been g
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;MOOCs&#35780;&#20998;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As MOOCs Graders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20026;&#25317;&#26377;&#30005;&#33041;&#21644;&#20114;&#32852;&#32593;&#35775;&#38382;&#26435;&#38480;&#30340;&#20840;&#29699;&#20219;&#20309;&#20154;&#25552;&#20379;&#20813;&#36153;&#25945;&#32946;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#35838;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#20876;&#24847;&#21619;&#30528;&#19968;&#20301;&#25945;&#24072;&#20960;&#20046;&#19981;&#21487;&#33021;&#35780;&#20272;&#27599;&#20010;&#23398;&#29983;&#30340;&#20889;&#20316;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21516;&#20276;&#35780;&#20998;&#36890;&#24120;&#26159;&#39318;&#36873;&#26041;&#27861;&#65292;&#36890;&#24120;&#30001;&#31616;&#21333;&#26126;&#20102;&#30340;&#35780;&#20998;&#26631;&#20934;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#21516;&#20276;&#35780;&#20998;&#22312;&#21487;&#38752;&#24230;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;18&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26367;&#20195;MOOCs&#20013;&#30340;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65306;GPT-4&#21644;GPT-3.5&#65292;&#24182;&#28085;&#30422;&#19977;&#38376;&#19981;&#21516;&#30340;&#35838;&#31243;&#65306;&#20837;&#38376;&#22825;&#25991;&#23398;&#65292;&#22825;&#20307;&#29983;&#29289;&#23398;&#20197;&#21450;&#22825;&#25991;&#23398;&#30340;&#21382;&#21490;&#19982;&#21746;&#23398;&#12290;&#20026;&#20102;&#35757;&#32451;LLMs&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#38646;-shot&#36830;&#32493;&#24605;&#32771;&#65288;Zero-shot-CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#21464;&#31181;&#30340;&#19977;&#20010;&#19981;&#21516;&#25552;&#31034;&#65306;&#32467;&#21512;Zero-shot-CoT&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.03686</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#65306;&#37325;&#26032;&#24605;&#32771;&#35821;&#35328;&#27169;&#22411;&#22312;&#34164;&#21547;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minds versus Machines: Rethinking Entailment Verification with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#25991;&#26412;&#29702;&#35299;&#20013;&#36827;&#34892;&#22823;&#37327;&#30340;&#25512;&#29702;&#20197;&#29702;&#35299;&#35770;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#20154;&#31867;&#21644;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#36890;&#36807;&#32508;&#21512;&#31574;&#21010;&#30340;&#34164;&#21547;&#39564;&#35777;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20154;&#31867;&#21644;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#31867;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#19977;&#20010;&#31867;&#21035;&#65288;NLI&#12289;&#19978;&#19979;&#25991;QA&#21644;&#35299;&#37322;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22810;&#21477;&#21069;&#25552;&#21644;&#19981;&#21516;&#30340;&#30693;&#35782;&#31867;&#22411;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;&#22797;&#26434;&#25512;&#29702;&#24773;&#20917;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;LLM&#22312;&#36328;&#25193;&#23637;&#19978;&#19979;&#25991;&#30340;&#22810;&#36339;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#38656;&#35201;&#31616;&#21333;&#28436;&#32462;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Flan-T5&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#24182;&#19982;GPT-4&#23218;&#32654;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#20379;&#34164;&#21547;&#39564;&#35777;&#20351;&#29992;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02611</link><description>&lt;p&gt;
PuzzleBench&#65306;LLMs&#33021;&#21542;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#65292;&#22914;&#36923;&#36753;&#38382;&#31572;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65292;&#19968;&#20010;&#20363;&#23376;&#26159;&#27969;&#34892;&#30340;&#25968;&#29420;&#35868;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26377;&#19968;&#20010;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22522;&#30784;&#19968;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#20363;&#21270;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PuzzleBench&#65292;&#19968;&#20010;&#21253;&#21547;31&#20010;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#30456;&#24403;&#31967;&#31957;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Puzzle-LM&#65292;&#23427;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
&lt;/p&gt;</description></item><item><title>DiffStitch&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02439</link><description>&lt;p&gt;
DiffStitch: &#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02439
&lt;/p&gt;
&lt;p&gt;
DiffStitch&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#26368;&#20339;&#36712;&#36857;&#65292;&#36825;&#32473;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#24517;&#39035;&#33719;&#24471;&#21040;&#36798;&#39640;&#22870;&#21169;&#21306;&#22495;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#65288;DiffStitch&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#65292;&#23427;&#21487;&#20197;&#31995;&#32479;&#22320;&#29983;&#25104;&#36712;&#36857;&#20043;&#38388;&#30340;&#25340;&#25509;&#36716;&#25442;&#12290;DiffStitch&#21487;&#20197;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;DiffStitch&#22312;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DiffStitch&#22312;&#19968;&#27493;&#26041;&#27861;&#65288;IQL&#65289;&#12289;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;TD3+BC&#65289;&#21644;&#36712;&#36857;&#26041;&#27861;&#65288;PPO&#65289;&#30340;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and traje
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Self-Imagine&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;Vision-Language&#27169;&#22411;&#29983;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#24182;&#23558;&#20854;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#20877;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25968;&#23398;&#20219;&#21153;&#21644;&#36890;&#29992;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.08025</link><description>&lt;p&gt;
&#33258;&#25105;&#24819;&#35937;&#65306;&#21033;&#29992;&#33258;&#25105;&#24819;&#35937;&#36827;&#34892;&#22810;&#27169;&#22411;&#33258;&#28982;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Self-Imagine&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;Vision-Language&#27169;&#22411;&#29983;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#24182;&#23558;&#20854;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#20877;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25968;&#23398;&#20219;&#21153;&#21644;&#36890;&#29992;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision-Language&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#28508;&#21147;&#22312;&#22788;&#29702;&#22797;&#26434;&#22522;&#20110;&#25991;&#26412;&#38382;&#39064;&#26102;&#24448;&#24448;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#23588;&#20854;&#26159;&#24403;&#36825;&#20123;&#38382;&#39064;&#33021;&#22815;&#20174;&#35270;&#35273;&#34920;&#36798;&#20013;&#33719;&#30410;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Self-Imagine&#65292;&#19982;&#20154;&#31867;&#36890;&#36807;&#21019;&#24314;&#38382;&#39064;&#30340;&#35270;&#35273;&#22270;&#24182;&#25512;&#26029;&#35299;&#20915;&#27493;&#39588;&#30340;&#33021;&#21147;&#30456; resonating&#12290;&#25105;&#20204;&#21033;&#29992;&#21333;&#19968;&#30340;Vision-Language&#27169;&#22411;&#65288;VLM&#65289;&#20351;&#29992;HTML&#29983;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;HTML&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#30456;&#21516;&#30340;VLM&#26681;&#25454;&#38382;&#39064;&#21644;&#22270;&#20687;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#65288;LLAVA-1.5&#21644;GEMINI PRO&#65289;VLMs&#22312;&#19977;&#20010;&#25968;&#23398;&#20219;&#21153;&#21644;&#20061;&#20010;&#36890;&#29992;&#25512;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;LLAVA-1.5&#21644;GEMINI PRO&#22312;&#25152;&#26377;&#25968;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08025v2 Announce Type: replace  Abstract: The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on aver
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#21151;&#33021;&#20195;&#30721;&#28431;&#27934;&#20998;&#26512;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#20041;&#22870;&#21169;&#20462;&#22797;&#22240;AI&#39537;&#21160;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#32780;&#20135;&#29983;&#30340;&#19981;&#23433;&#20840;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2401.03374</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#20041;&#22870;&#21169;&#30340;&#20195;&#30721;&#28431;&#27934;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
LLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03374
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#21151;&#33021;&#20195;&#30721;&#28431;&#27934;&#20998;&#26512;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#20041;&#22870;&#21169;&#20462;&#22797;&#22240;AI&#39537;&#21160;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#32780;&#20135;&#29983;&#30340;&#19981;&#23433;&#20840;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#23545;&#21151;&#33021;&#30340;&#20027;&#35201;&#24378;&#35843;&#24448;&#24448;&#36229;&#36234;&#20102;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36825;&#19968;&#36235;&#21183;&#38543;&#30528;GitHub Copilot&#31561;AI&#39537;&#21160;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#30340;&#20852;&#36215;&#32780;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#36825;&#20123;&#24037;&#20855;&#26174;&#30528;&#25552;&#39640;&#20102;&#24320;&#21457;&#20154;&#21592;&#22312;&#21151;&#33021;&#20195;&#30721;&#24320;&#21457;&#20013;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#30340;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#24037;&#20855;&#20063;&#36127;&#36131;&#21019;&#24314;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#20855;&#26377;&#28431;&#27934;&#20195;&#30721;&#30340;&#23384;&#20648;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#24320;&#21457;&#20154;&#21592;&#34987;&#31216;&#20026;&#8220;&#38142;&#26465;&#20013;&#26368;&#34180;&#24369;&#30340;&#19968;&#29615;&#8221;&#65292;&#22240;&#20026;&#20182;&#20204;&#23545;&#20195;&#30721;&#23433;&#20840;&#24615;&#20960;&#20046;&#27809;&#26377;&#20160;&#20040;&#20102;&#35299;&#12290;&#23613;&#31649;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20026;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#24517;&#39035;&#20805;&#20998;&#25551;&#36848;&#21644;&#25945;&#32946;&#24320;&#21457;&#20154;&#21592;&#26377;&#20851;&#20195;&#30721;&#23433;&#20840;&#24615;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#38382;&#39064;&#19981;&#20877;&#37325;&#28436;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20195;&#30721;&#28431;&#27934;&#20998;&#26512;&#31995;&#32479;\texttt{SecRepair}&#65292;&#23427;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;CodeGen2&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03374v2 Announce Type: replace-cross  Abstract: In software development, the predominant emphasis on functionality often supersedes security concerns, a trend gaining momentum with AI-driven automation tools like GitHub Copilot. These tools significantly improve developers' efficiency in functional code development. Nevertheless, it remains a notable concern that such tools are also responsible for creating insecure code, predominantly because of pre-training on publicly available repositories with vulnerable code. Moreover, developers are called the "weakest link in the chain" since they have very minimal knowledge of code security. Although existing solutions provide a reasonable solution to vulnerable code, they must adequately describe and educate the developers on code security to ensure that the security issues are not repeated. Therefore we introduce a multipurpose code vulnerability analysis system \texttt{SecRepair}, powered by a large language model, CodeGen2 assis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#24037;&#19994;&#29289;&#32852;&#32593;&#26234;&#33021;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#35843;&#26597;&#20559;&#35265;&#65292;&#20026;&#21046;&#36896;&#19994;&#30340;&#36716;&#22411;&#25552;&#20379;&#20102;&#25351;&#24341;&#12290;</title><link>https://arxiv.org/abs/2312.16174</link><description>&lt;p&gt;
&#24037;&#19994;&#29289;&#32852;&#32593;&#26234;&#33021;&#36171;&#33021;&#26234;&#33021;&#21046;&#36896;&#65306;&#19968;&#31687;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Industrial Internet of Things Intelligence Empowering Smart Manufacturing: A Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#24037;&#19994;&#29289;&#32852;&#32593;&#26234;&#33021;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#35843;&#26597;&#20559;&#35265;&#65292;&#20026;&#21046;&#36896;&#19994;&#30340;&#36716;&#22411;&#25552;&#20379;&#20102;&#25351;&#24341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31454;&#20105;&#28608;&#28872;&#30340;&#21830;&#19994;&#29615;&#22659;&#21644;&#26085;&#30410;&#20010;&#24615;&#21270;&#30340;&#23450;&#21046;&#38656;&#27714;&#25512;&#21160;&#30528;&#21046;&#36896;&#19994;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#21644;&#21319;&#32423;&#12290;&#24037;&#19994;&#29289;&#32852;&#32593;&#26234;&#33021;&#33021;&#22815;&#20026;&#21046;&#36896;&#20215;&#20540;&#38142;&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#21019;&#26032;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#21046;&#36896;&#19994;&#30340;&#36716;&#22411;&#25552;&#20379;&#25351;&#24341;&#12290;&#29616;&#22312;&#26159;&#25552;&#20379;&#24037;&#19994;&#29289;&#32852;&#32593;&#26234;&#33021;&#31995;&#32479;&#24615;&#35270;&#37326;&#30340;&#26102;&#20505;&#20102;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35843;&#26597;&#24448;&#24448;&#38598;&#20013;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#26234;&#33021;&#30340;&#29305;&#23450;&#39046;&#22495;&#65292;&#23548;&#33268;&#30740;&#31350;&#32773;&#21644;&#35835;&#32773;&#22312;&#29702;&#35299;&#24037;&#19994;&#29289;&#32852;&#32593;&#26234;&#33021;&#26102;&#23384;&#22312;&#20559;&#35265;&#65292;&#21363;&#35748;&#20026;&#22312;&#19968;&#20010;&#26041;&#21521;&#19978;&#30340;&#30740;&#31350;&#23545;&#24037;&#19994;&#29289;&#32852;&#32593;&#26234;&#33021;&#30340;&#21457;&#23637;&#26368;&#37325;&#35201;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#26041;&#21521;&#30340;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#24037;&#19994;&#29289;&#32852;&#32593;&#26234;&#33021;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#21046;&#36896;&#19994;&#36716;&#22411;&#30340;&#24517;&#28982;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16174v2 Announce Type: replace  Abstract: The fiercely competitive business environment and increasingly personalized customization needs are driving the digital transformation and upgrading of the manufacturing industry. IIoT intelligence, which can provide innovative and efficient solutions for various aspects of the manufacturing value chain, illuminates the path of transformation for the manufacturing industry. It's time to provide a systematic vision of IIoT intelligence. However, existing surveys often focus on specific areas of IIoT intelligence, leading researchers and readers to have biases in their understanding of IIoT intelligence, that is, believing that research in one direction is the most important for the development of IIoT intelligence, while ignoring contributions from other directions. Therefore, this paper provides a comprehensive overview of IIoT intelligence. We first conduct an in-depth analysis of the inevitability of manufacturing transformation an
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#20195;&#29702;&#22312;&#19982;&#20154;&#31867;&#21644;&#20854;&#20182;&#20195;&#29702;&#20114;&#21160;&#26102;&#23637;&#31034;&#30340;&#31038;&#20250;&#34892;&#20026;&#65292;&#21253;&#25324;&#31038;&#20250;&#23398;&#20064;&#12289;&#31038;&#20250;&#20559;&#22909;&#21644;&#21512;&#20316;&#34892;&#20026;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#30340;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2312.15198</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#34920;&#29616;&#20986;&#31038;&#20250;&#34892;&#20026;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do LLM Agents Exhibit Social Behavior?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15198
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#20195;&#29702;&#22312;&#19982;&#20154;&#31867;&#21644;&#20854;&#20182;&#20195;&#29702;&#20114;&#21160;&#26102;&#23637;&#31034;&#30340;&#31038;&#20250;&#34892;&#20026;&#65292;&#21253;&#25324;&#31038;&#20250;&#23398;&#20064;&#12289;&#31038;&#20250;&#20559;&#22909;&#21644;&#21512;&#20316;&#34892;&#20026;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#27491;&#22312;&#25193;&#22823;&#23427;&#20204;&#22312;&#23398;&#26415;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#29992;&#12290;&#26368;&#36817;&#30340;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#8220;&#40657;&#21283;&#23376;&#8221;LLM&#20195;&#29702;&#26469;&#27169;&#25311;&#22797;&#26434;&#31038;&#20250;&#31995;&#32479;&#24182;&#28508;&#22312;&#22320;&#26367;&#20195;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;LLMs&#22312;&#19982;&#20154;&#31867;&#21644;&#20854;&#20182;&#20195;&#29702;&#36827;&#34892;&#20114;&#21160;&#26102;&#23637;&#31034;&#31038;&#20250;&#23398;&#20064;&#12289;&#31038;&#20250;&#20559;&#22909;&#21644;&#21512;&#20316;&#34892;&#20026;&#65288;&#38388;&#25509;&#20114;&#24800;&#65289;&#31561;&#20851;&#38190;&#31038;&#20250;&#20132;&#20114;&#21407;&#21017;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#30740;&#31350;&#21046;&#23450;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#28041;&#21450;&#23558;&#28041;&#21450;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#30340;&#32463;&#20856;&#23454;&#39564;&#35843;&#25972;&#20026;&#20351;&#29992;LLM&#20195;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#19968;&#27493;&#19968;&#27493;&#30340;&#25512;&#29702;&#65292;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20197;&#35780;&#20272;LLMs&#30340;&#22825;&#29983;&#20559;&#22909;&#12290;&#25105;&#20204;&#23545;LLM&#20195;&#29702;&#34892;&#20026;&#30340;&#20998;&#26512;&#21253;&#25324;&#20027;&#35201;&#25928;&#24212;&#21644;&#27425;&#35201;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15198v2 Announce Type: replace  Abstract: The advances of Large Language Models (LLMs) are expanding their utility in both academic research and practical applications. Recent social science research has explored the use of these ``black-box'' LLM agents for simulating complex social systems and potentially substituting human subjects in experiments. Our study delves into this emerging domain, investigating the extent to which LLMs exhibit key social interaction principles, such as social learning, social preference, and cooperative behavior (indirect reciprocity), in their interactions with humans and other agents. We develop a framework for our study, wherein classical laboratory experiments involving human subjects are adapted to use LLM agents. This approach involves step-by-step reasoning that mirrors human cognitive processes and zero-shot learning to assess the innate preferences of LLMs. Our analysis of LLM agents' behavior includes both the primary effects and an in
&lt;/p&gt;</description></item><item><title>MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.08656</link><description>&lt;p&gt;
MaxK-GNN: &#25506;&#32034;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#29702;&#35770;&#36895;&#24230;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08656
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21152;&#36895;&#26041;&#38754;&#65292;GPU&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#24179;&#21488;&#12290; GPU&#22312;GNN&#19978;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#24037;&#20316;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20869;&#23384;&#35775;&#38382;&#19981;&#35268;&#21017;&#65292;&#23548;&#33268;&#30828;&#20214;&#21033;&#29992;&#19981;&#20805;&#20998;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20363;&#22914;PyG&#12289;DGL&#19982;cuSPARSE&#65292;&#20197;&#21450;GNNAdvisor&#26694;&#26550;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#20869;&#23384;&#27969;&#37327;&#20173;&#28982;&#24456;&#26174;&#33879;&#12290; &#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#36890;&#36807;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#22402;&#30452;&#20248;&#21270;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#26159;&#23558;&#21152;&#36895;&#20248;&#21270;&#35270;&#20026;&#8220;&#20107;&#21518;&#24605;&#32771;&#8221;&#65288;&#21363;&#65288;i&#65289;&#32473;&#23450;GNN&#31639;&#27861;&#65292;&#35774;&#35745;&#21152;&#36895;&#22120;&#65292;&#25110;&#65288;ii&#65289;&#32473;&#23450;&#30828;&#20214;&#65292;&#20027;&#35201;&#20248;&#21270;GNN&#31639;&#27861;&#65289;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;MaxK-GNN&#65292;&#19968;&#31181;&#38598;&#25104;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#20808;&#36827;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#12290; &#65288;i&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;MaxK&#38750;&#32447;&#24615;&#24182;&#25552;&#20379;&#20102;MaxK&#38750;&#32447;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an "after-thought" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#35299;&#20915;&#22270;&#20687;&#23383;&#24149;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MOCHa&#26469;&#32531;&#35299;&#24187;&#35273;</title><link>https://arxiv.org/abs/2312.03631</link><description>&lt;p&gt;
&#32531;&#35299;&#24320;&#25918;&#35789;&#27719;&#25551;&#36848;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Open-Vocabulary Caption Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#35299;&#20915;&#22270;&#20687;&#23383;&#24149;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MOCHa&#26469;&#32531;&#35299;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#26465;&#20214;&#30340;&#25991;&#26412;&#29983;&#25104;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#22270;&#20687;&#23383;&#24149;&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#19982;&#32473;&#23450;&#22270;&#20687;&#26080;&#27861;&#25512;&#26029;&#30340;&#34394;&#20551;&#32454;&#33410;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#22270;&#20687;&#23383;&#24149;&#20013;&#22823;&#22810;&#20351;&#29992;&#23553;&#38381;&#35789;&#27719;&#23545;&#35937;&#21015;&#34920;&#26469;&#32531;&#35299;&#25110;&#35780;&#20272;&#24187;&#35273;&#65292;&#24573;&#30053;&#20102;&#23454;&#36341;&#20013;&#21457;&#29983;&#30340;&#22823;&#22810;&#25968;&#24187;&#35273;&#31867;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#22270;&#20687;&#23383;&#24149;&#20013;&#30340;&#24187;&#35273;&#65292;&#21253;&#25324;&#37327;&#21270;&#23427;&#20204;&#30340;&#23384;&#22312;&#24182;&#20248;&#21270;&#20197;&#20943;&#36731;&#36825;&#31181;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;OpenCHAIR&#22522;&#20934;&#21033;&#29992;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#26469;&#35780;&#20272;&#24320;&#25918;&#35789;&#27719;&#25551;&#36848;&#24187;&#35273;&#65292;&#22312;&#22810;&#26679;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#27969;&#34892;&#30340;CHAIR&#22522;&#20934;&#12290;&#20026;&#20102;&#22312;&#24207;&#21015;&#32423;&#21035;&#19978;&#32531;&#35299;&#24320;&#25918;&#35789;&#27719;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MOCHa&#65292;&#19968;&#31181;&#21033;&#29992;&#36827;&#23637;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03631v2 Announce Type: replace-cross  Abstract: While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, namely, the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning, ignoring most types of hallucinations that occur in practice. To this end, we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting, including quantifying their presence and optimizing to mitigate such hallucinations. Our OpenCHAIR benchmark leverages generative foundation models to evaluate open-vocabulary caption hallucinations, surpassing the popular CHAIR benchmark in both diversity and accuracy. To mitigate open-vocabulary hallucinations at the sequence level, we propose MOCHa, an approach harnessing advancements in
&lt;/p&gt;</description></item><item><title>EduGym&#26159;&#19968;&#22871;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25945;&#32946;&#30340;&#29615;&#22659;&#21644;&#31508;&#35760;&#26412;&#22871;&#20214;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#29983;&#22312;&#36716;&#25442;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2311.10590</link><description>&lt;p&gt;
EduGym: &#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25945;&#32946;&#30340;&#29615;&#22659;&#21644;&#31508;&#35760;&#26412;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
EduGym: An Environment and Notebook Suite for Reinforcement Learning Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10590
&lt;/p&gt;
&lt;p&gt;
EduGym&#26159;&#19968;&#22871;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25945;&#32946;&#30340;&#29615;&#22659;&#21644;&#31508;&#35760;&#26412;&#22871;&#20214;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#29983;&#22312;&#36716;&#25442;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#23398;&#29983;&#22312;&#23398;&#20064;&#36825;&#20010;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#38469;&#25945;&#23398;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#23398;&#29983;&#22312;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#65288;&#26412;&#31185;&#29983;&#12289;&#30805;&#22763;&#29983;&#21644;&#26089;&#26399;&#21338;&#22763;&#29983;&#65289;&#26102;&#24120;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#19968;&#26041;&#38754;&#65292;&#25945;&#31185;&#20070;&#21644;&#65288;&#22312;&#32447;&#65289;&#35762;&#24231;&#25552;&#20379;&#20102;&#22522;&#30784;&#30693;&#35782;&#65292;&#20294;&#23398;&#29983;&#21457;&#29616;&#24456;&#38590;&#22312;&#26041;&#31243;&#24335;&#21644;&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20844;&#20849;&#20195;&#30721;&#24211;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#20363;&#23376;&#65292;&#20294;&#23454;&#29616;&#30340;&#31639;&#27861;&#24448;&#24448;&#22797;&#26434;&#65292;&#24182;&#19988;&#22522;&#30784;&#27979;&#35797;&#29615;&#22659;&#21516;&#26102;&#21253;&#21547;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#25361;&#25112;&#12290;&#23613;&#31649;&#36825;&#22312;&#30740;&#31350;&#35282;&#24230;&#19978;&#26159;&#29616;&#23454;&#30340;&#65292;&#20294;&#23427;&#32463;&#24120;&#38459;&#30861;&#20102;&#25945;&#32946;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;EduGym&#65292;&#36825;&#26159;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#25945;&#32946;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#30456;&#20851;&#20132;&#20114;&#24335;&#31508;&#35760;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10590v2 Announce Type: replace-cross  Abstract: Due to the empirical success of reinforcement learning, an increasing number of students study the subject. However, from our practical teaching experience, we see students entering the field (bachelor, master and early PhD) often struggle. On the one hand, textbooks and (online) lectures provide the fundamentals, but students find it hard to translate between equations and code. On the other hand, public codebases do provide practical examples, but the implemented algorithms tend to be complex, and the underlying test environments contain multiple reinforcement learning challenges at once. Although this is realistic from a research perspective, it often hinders educational conceptual understanding. To solve this issue we introduce EduGym, a set of educational reinforcement learning environments and associated interactive notebooks tailored for education. Each EduGym environment is specifically designed to illustrate a certain 
&lt;/p&gt;</description></item><item><title>Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.06607</link><description>&lt;p&gt;
Monkey: &#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#25991;&#26412;&#26631;&#31614;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06607
&lt;/p&gt;
&lt;p&gt;
Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#21644;&#35814;&#32454;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Monkey&#26469;&#22686;&#24378;LMM&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;Monkey&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#21010;&#20998;&#20026;&#32479;&#19968;&#30340;&#34917;&#19969;&#26469;&#22788;&#29702;&#22270;&#20687;&#65292;&#27599;&#20010;&#34917;&#19969;&#30340;&#22823;&#23567;&#19982;&#21407;&#26469;&#35757;&#32451;&#33391;&#22909;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20351;&#29992;&#30340;&#22823;&#23567;(&#20363;&#22914;448x448)&#30456;&#21305;&#37197;&#12290;&#37197;&#22791;&#20102;&#27599;&#20010;&#34917;&#19969;&#30340;&#36866;&#37197;&#22120;&#65292;Monkey&#21487;&#20197;&#22788;&#29702;&#39640;&#36798;1344x896&#20687;&#32032;&#30340;&#26356;&#39640;&#20998;&#36776;&#29575;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#35270;&#35273;&#20449;&#24687;&#30340;&#35814;&#32454;&#25429;&#25417;&#12290;&#20854;&#27425;&#65292;&#23427;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#65292;&#20016;&#23500;&#20102;&#22330;&#26223;-&#23545;&#35937;&#20851;&#32852;&#30340;&#19978;&#19979;&#25991;&#12290;&#36825;&#31181;&#20004;&#37096;&#20998;&#31574;&#30053;&#30830;&#20445;&#20102;&#20174;&#29983;&#25104;&#25968;&#25454;&#20013;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#65306;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#20801;&#35768;&#23545;&#35270;&#35273;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#25429;&#25417;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20840;&#38754;&#25551;&#36848;&#30340;&#25928;&#26524;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06607v3 Announce Type: replace-cross  Abstract: Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative result
&lt;/p&gt;</description></item><item><title>BrainPy&#26159;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#22823;&#33041;&#27169;&#25311;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;JAX&#21644;XLA&#30340;&#21151;&#33021;&#26469;&#26550;&#35774;&#22823;&#33041;&#27169;&#25311;&#19982;&#33041;&#21551;&#21457;&#35745;&#31639;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;</title><link>https://arxiv.org/abs/2311.05106</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#22823;&#33041;&#27169;&#25311;&#22120;&#65306;&#26550;&#36215;&#22823;&#33041;&#27169;&#25311;&#19982;&#33041;&#21551;&#21457;&#35745;&#31639;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
A differentiable brain simulator bridging brain simulation and brain-inspired computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05106
&lt;/p&gt;
&lt;p&gt;
BrainPy&#26159;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#22823;&#33041;&#27169;&#25311;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;JAX&#21644;XLA&#30340;&#21151;&#33021;&#26469;&#26550;&#35774;&#22823;&#33041;&#27169;&#25311;&#19982;&#33041;&#21551;&#21457;&#35745;&#31639;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#27169;&#25311;&#24314;&#31435;&#21160;&#21147;&#23398;&#27169;&#22411;&#20197;&#27169;&#20223;&#22823;&#33041;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;&#32780;&#22823;&#33041;&#21551;&#21457;&#35745;&#31639;&#65288;BIC&#65289;&#36890;&#36807;&#20174;&#22823;&#33041;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#20013;&#23398;&#20064;&#26469;&#21457;&#23637;&#26234;&#33021;&#31995;&#32479;&#12290;&#36825;&#20004;&#20010;&#39046;&#22495;&#30456;&#20114;&#20132;&#32455;&#65292;&#24212;&#20849;&#20139;&#19968;&#20010;&#36890;&#29992;&#30340;&#32534;&#31243;&#26694;&#26550;&#20197;&#20419;&#36827;&#24444;&#27492;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#32479;&#30340;&#22823;&#33041;&#27169;&#25311;&#22120;&#22312;&#35757;&#32451;&#26041;&#38754;&#32570;&#20047;&#21487;&#24494;&#20998;&#24615;&#65292;&#32780;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26694;&#26550;&#26080;&#27861;&#25429;&#25417;&#22823;&#33041;&#21160;&#21147;&#23398;&#30340;&#29983;&#29289;&#29289;&#29702;&#29616;&#23454;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#36825;&#20004;&#32773;&#20043;&#38388;&#26080;&#27861;&#23454;&#29616;&#30446;&#26631;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BrainPy&#65292;&#19968;&#20010;&#20351;&#29992;JAX&#21644;XLA&#24320;&#21457;&#30340;&#21487;&#24494;&#20998;&#22823;&#33041;&#27169;&#25311;&#22120;&#65292;&#26088;&#22312;&#26550;&#36215;&#22823;&#33041;&#27169;&#25311;&#19982;BIC&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;BrainPy&#22312;JAX&#24378;&#22823;&#30340;AI&#26694;&#26550;&#21151;&#33021;&#22522;&#30784;&#19978;&#25193;&#23637;&#65292;&#24341;&#20837;&#20102;&#23436;&#25972;&#30340;&#21151;&#33021;&#65292;&#29992;&#20110;&#28789;&#27963;&#12289;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22823;&#33041;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05106v2 Announce Type: replace-cross  Abstract: Brain simulation builds dynamical models to mimic the structure and functions of the brain, while brain-inspired computing (BIC) develops intelligent systems by learning from the structure and functions of the brain. The two fields are intertwined and should share a common programming framework to facilitate each other's development. However, none of the existing software in the fields can achieve this goal, because traditional brain simulators lack differentiability for training, while existing deep learning (DL) frameworks fail to capture the biophysical realism and complexity of brain dynamics. In this paper, we introduce BrainPy, a differentiable brain simulator developed using JAX and XLA, with the aim of bridging the gap between brain simulation and BIC. BrainPy expands upon the functionalities of JAX, a powerful AI framework, by introducing complete capabilities for flexible, efficient, and scalable brain simulation. It 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Controllable Machine Unlearning (ConMU)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#24179;&#34913;&#38544;&#31169;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#36816;&#34892;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2310.18574</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25511;&#26426;&#22120;&#36951;&#24536;&#25171;&#30772;&#38544;&#31169;&#12289;&#25928;&#29992;&#12289;&#25928;&#29575;&#19977;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18574
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Controllable Machine Unlearning (ConMU)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#24179;&#34913;&#38544;&#31169;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#36816;&#34892;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#31639;&#27861;&#30001;&#20110;&#23545;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#30340;&#24517;&#35201;&#36981;&#20174;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;MU&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#19981;&#38656;&#35201;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#23545;&#32473;&#23450;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20110;&#26368;&#22823;&#21270;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#29616;&#23454;&#19990;&#30028;&#22522;&#20110;&#32593;&#32476;&#30340;&#24212;&#29992;&#31243;&#24207;&#37117;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#38544;&#31169;&#27861;&#35268;&#12290;&#25506;&#32034;&#38544;&#31169;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#36816;&#34892;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#20840;&#35889;&#23545;&#20110;&#23454;&#38469;&#30340;&#36951;&#24536;&#22330;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#32780;&#19988;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#65292;&#35774;&#35745;&#20855;&#26377;&#23545;&#19978;&#36848;&#26435;&#34913;&#30340;&#31616;&#21333;&#25511;&#21046;&#30340;MU&#31639;&#27861;&#26159;&#21487;&#21462;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Controllable Machine Unlearning (ConMU)&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;MU&#26657;&#20934;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;ConMU&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18574v2 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) algorithms have become increasingly critical due to the imperative adherence to data privacy regulations. The primary objective of MU is to erase the influence of specific data samples on a given model without the need to retrain it from scratch. Accordingly, existing methods focus on maximizing user privacy protection. However, there are different degrees of privacy regulations for each real-world web-based application. Exploring the full spectrum of trade-offs between privacy, model utility, and runtime efficiency is critical for practical unlearning scenarios. Furthermore, designing the MU algorithm with simple control of the aforementioned trade-off is desirable but challenging due to the inherent complex interaction. To address the challenges, we present Controllable Machine Unlearning (ConMU), a novel framework designed to facilitate the calibration of MU. The ConMU framework contains three integra
&lt;/p&gt;</description></item><item><title>MindfulDiary&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#31934;&#31070;&#30149;&#24739;&#32773;&#36890;&#36807;&#23545;&#35805;&#35760;&#24405;&#26085;&#24120;&#20307;&#39564;&#65292;&#24182;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#21462;&#24471;&#31215;&#26497;&#25928;&#26524;</title><link>https://arxiv.org/abs/2310.05231</link><description>&lt;p&gt;
MindfulDiary&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#31934;&#31070;&#30149;&#24739;&#32773;&#30340;&#26085;&#35760;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05231
&lt;/p&gt;
&lt;p&gt;
MindfulDiary&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#31934;&#31070;&#30149;&#24739;&#32773;&#36890;&#36807;&#23545;&#35805;&#35760;&#24405;&#26085;&#24120;&#20307;&#39564;&#65292;&#24182;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#21462;&#24471;&#31215;&#26497;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#28982;&#32780;&#20854;&#22797;&#26434;&#24615;&#21644;&#20302;&#21487;&#25511;&#24615;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MindfulDiary&#65292;&#19968;&#20010;&#31227;&#21160;&#26085;&#35760;&#24212;&#29992;&#65292;&#32467;&#21512;LLM&#24110;&#21161;&#31934;&#31070;&#30149;&#24739;&#32773;&#36890;&#36807;&#23545;&#35805;&#35760;&#24405;&#26085;&#24120;&#20307;&#39564;&#12290;&#19982;&#24515;&#29702;&#20581;&#24247;&#19987;&#19994;&#20154;&#22763;&#65288;MHPs&#65289;&#21512;&#20316;&#35774;&#35745;&#65292;MindfulDiary&#37319;&#21462;&#22522;&#20110;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#23433;&#20840;&#22320;&#36981;&#23432;&#19987;&#23478;&#25351;&#21335;&#65292;&#21516;&#26102;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#28041;&#21450;28&#21517;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;&#24739;&#32773;&#21644;5&#21517;&#31934;&#31070;&#31185;&#21307;&#29983;&#30340;&#20026;&#26399;&#22235;&#21608;&#30340;&#23454;&#22320;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;MindfulDiary&#25903;&#25345;&#24739;&#32773;&#25345;&#32493;&#20016;&#23500;&#20854;&#26085;&#24120;&#35760;&#24405;&#65292;&#24182;&#24110;&#21161;&#31934;&#31070;&#31185;&#21307;&#29983;&#36890;&#36807;&#29702;&#35299;&#20182;&#20204;&#30340;&#24819;&#27861;&#21644;&#26085;&#24120;&#32972;&#26223;&#26356;&#22909;&#22320;&#21516;&#24773;&#20182;&#20204;&#30340;&#24739;&#32773;&#12290;&#26681;&#25454;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05231v2 Announce Type: replace-cross  Abstract: In the mental health domain, Large Language Models (LLMs) offer promising new opportunities, though their inherent complexity and low controllability have raised questions about their suitability in clinical settings. We present MindfulDiary, a mobile journaling app incorporating an LLM to help psychiatric patients document daily experiences through conversation. Designed in collaboration with mental health professionals (MHPs), MindfulDiary takes a state-based approach to safely comply with the experts' guidelines while carrying on free-form conversations. Through a four-week field study involving 28 patients with major depressive disorder and five psychiatrists, we found that MindfulDiary supported patients in consistently enriching their daily records and helped psychiatrists better empathize with their patients through an understanding of their thoughts and daily contexts. Drawing on these findings, we discuss the implicati
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.02233</link><description>&lt;p&gt;
&#29992;&#21307;&#23398;&#25945;&#31185;&#20070;&#22686;&#24378;&#40657;&#30418;LLMs&#36827;&#34892;&#20020;&#24202;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.02233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#31034;&#20986;&#26681;&#25454;&#20154;&#31867;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#32570;&#20047;&#29305;&#23450;&#12289;&#28145;&#20837;&#30340;&#30693;&#35782;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;LLM-AMT&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;&#19968;&#20010;&#26597;&#35810;&#22686;&#24378;&#22120;&#12289;&#19968;&#20010;&#28151;&#21512;&#25945;&#31185;&#20070;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#30693;&#35782;&#33258;&#25105;&#23436;&#21892;&#12290;&#23427;&#20204;&#20849;&#21516;&#25972;&#21512;&#26435;&#23041;&#21307;&#23398;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;LLMs&#38405;&#35835;&#22120;&#26377;&#21161;&#20110;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMAMT&#26174;&#33879;&#25552;&#39640;&#20102;&#21709;&#24212;&#36136;&#37327;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.6%&#21040;16.6%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;GPT-4-Turbo&#20026;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.02233v2 Announce Type: replace-cross  Abstract: Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an LLM Reader aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLMAMT significantly improves response quality, with accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the base mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#32472;&#30011;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#37096;&#20998;&#23457;&#32654;&#29305;&#24449;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#36739;&#23569;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2304.07999</link><description>&lt;p&gt;
&#27599;&#20010;&#20154;&#37117;&#21487;&#20197;&#25104;&#20026;&#27605;&#21152;&#32034;&#65311;&#25506;&#35752;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#32472;&#30011;&#30340;&#31070;&#35805;&#30340;&#35745;&#31639;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Everyone Can Be Picasso? A Computational Framework into the Myth of Human versus AI Painting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.07999
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#32472;&#30011;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#37096;&#20998;&#23457;&#32654;&#29305;&#24449;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#36739;&#23569;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#65292;&#20351;&#27599;&#20010;&#20154;&#37117;&#33021;&#22815;&#36890;&#36807;&#31616;&#21333;&#30340;&#25991;&#26412;&#25551;&#36848;&#36731;&#26494;&#29983;&#25104;&#32654;&#20029;&#30340;&#32472;&#30011;&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#32472;&#30011;&#30340;&#24778;&#20154;&#36136;&#37327;&#65292;&#20154;&#20204;&#26222;&#36941;&#36136;&#30097;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#32472;&#30011;&#20043;&#38388;&#26159;&#21542;&#20173;&#28982;&#23384;&#22312;&#24046;&#24322;&#65292;&#20197;&#21450;&#20154;&#31867;&#33402;&#26415;&#23478;&#26159;&#21542;&#20250;&#34987;&#20154;&#24037;&#26234;&#33021;&#20195;&#26367;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#23558;&#31070;&#32463;&#28508;&#22312;&#31354;&#38388;&#21644;&#23457;&#32654;&#29305;&#24449;&#19982;&#35270;&#35273;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#20197;&#25506;&#35752;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#32472;&#30011;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#32472;&#30011;&#25910;&#34255;&#36827;&#34892;&#20998;&#31867;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#19968;&#20123;&#23457;&#32654;&#29305;&#24449;&#65288;&#22914;&#31508;&#35302;&#21644;&#28165;&#26224;&#24230;&#65289;&#26041;&#38754;&#65292;&#20154;&#24037;&#26234;&#33021;&#20316;&#21697;&#19982;&#20154;&#31867;&#20316;&#21697;&#26174;&#31034;&#20986;&#20998;&#24067;&#24046;&#24322;&#65292;&#32780;&#22312;&#39068;&#33394;&#21644;&#26500;&#22270;&#31561;&#20854;&#20182;&#23457;&#32654;&#29305;&#24449;&#26041;&#38754;&#24046;&#24322;&#36739;&#23567;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#23545;&#27605;&#21152;&#32034;&#30340;&#20010;&#20307;&#33402;&#26415;&#23478;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.07999v2 Announce Type: replace-cross  Abstract: The recent advances of AI technology, particularly in AI-Generated Content (AIGC), have enabled everyone to easily generate beautiful paintings with simple text description. With the stunning quality of AI paintings, it is widely questioned whether there still exists difference between human and AI paintings and whether human artists will be replaced by AI. To answer these questions, we develop a computational framework combining neural latent space and aesthetics features with visual analytics to investigate the difference between human and AI paintings. First, with categorical comparison of human and AI painting collections, we find that AI artworks show distributional difference from human artworks in both latent space and some aesthetic features like strokes and sharpness, while in other aesthetic features like color and composition there is less difference. Second, with individual artist analysis of Picasso, we show human 
&lt;/p&gt;</description></item><item><title>TBAL&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#33258;&#21160;&#26631;&#27880;&#26410;&#26631;&#27880;&#25968;&#25454;&#65292;&#20943;&#23569;&#25163;&#21160;&#26631;&#27880;&#30340;&#20381;&#36182;&#65307;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#21363;&#20351;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#20063;&#21487;&#20197;&#20934;&#30830;&#33258;&#21160;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#25581;&#31034;&#20102;TBAL&#31995;&#32479;&#30340;&#28508;&#22312;&#32570;&#38519;</title><link>https://arxiv.org/abs/2211.12620</link><description>&lt;p&gt;
&#22522;&#20110;&#38408;&#20540;&#30340;&#33258;&#21160;&#26631;&#27880;&#30340;&#20248;&#21183;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Promises and Pitfalls of Threshold-based Auto-labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.12620
&lt;/p&gt;
&lt;p&gt;
TBAL&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#33258;&#21160;&#26631;&#27880;&#26410;&#26631;&#27880;&#25968;&#25454;&#65292;&#20943;&#23569;&#25163;&#21160;&#26631;&#27880;&#30340;&#20381;&#36182;&#65307;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#21363;&#20351;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#20063;&#21487;&#20197;&#20934;&#30830;&#33258;&#21160;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#25581;&#31034;&#20102;TBAL&#31995;&#32479;&#30340;&#28508;&#22312;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#26159;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#12290;&#38408;&#20540;&#33258;&#21160;&#26631;&#27880;&#65288;TBAL&#65289;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#33719;&#21462;&#30340;&#39564;&#35777;&#25968;&#25454;&#26469;&#23547;&#25214;&#19968;&#20010;&#32622;&#20449;&#38408;&#20540;&#65292;&#39640;&#20110;&#35813;&#38408;&#20540;&#30340;&#25968;&#25454;&#23558;&#30001;&#26426;&#22120;&#26631;&#35760;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;TBAL&#27491;&#36880;&#28176;&#25104;&#20026;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#37492;&#20110;&#25152;&#24471;&#25968;&#25454;&#30340;&#38271;&#26399;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#21270;&#20351;&#29992;&#65292;&#29702;&#35299;&#36825;&#31181;&#33258;&#21160;&#26631;&#27880;&#31995;&#32479;&#33719;&#21462;&#30340;&#25968;&#25454;&#20309;&#26102;&#21487;&#20197;&#34987;&#20381;&#36182;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#20998;&#26512;TBAL&#31995;&#32479;&#24182;&#25512;&#23548;&#38656;&#35201;&#20445;&#35777;&#26426;&#22120;&#26631;&#35760;&#25968;&#25454;&#36136;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#39564;&#35777;&#25968;&#25454;&#37327;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#34920;&#38754;&#19978;&#31967;&#31957;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#12289;&#20934;&#30830;&#22320;&#26631;&#35760;&#21512;&#29702;&#25968;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;TBAL&#31995;&#32479;&#30340;&#19968;&#20010;&#38544;&#34255;&#30340;&#32570;&#28857;&#26159;&#28508;&#22312;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.12620v2 Announce Type: replace-cross  Abstract: Creating large-scale high-quality labeled datasets is a major bottleneck in supervised machine learning workflows. Threshold-based auto-labeling (TBAL), where validation data obtained from humans is used to find a confidence threshold above which the data is machine-labeled, reduces reliance on manual annotation. TBAL is emerging as a widely-used solution in practice. Given the long shelf-life and diverse usage of the resulting datasets, understanding when the data obtained by such auto-labeling systems can be relied on is crucial. This is the first work to analyze TBAL systems and derive sample complexity bounds on the amount of human-labeled validation data required for guaranteeing the quality of machine-labeled data. Our results provide two crucial insights. First, reasonable chunks of unlabeled data can be automatically and accurately labeled by seemingly bad models. Second, a hidden downside of TBAL systems is potentially
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;BFAR&#21644;BFRR&#65292;&#24182;&#36890;&#36807;&#21518;&#22788;&#29702;&#26041;&#27861;&#20943;&#36731;&#38754;&#37096;&#35782;&#21035;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.13664</link><description>&lt;p&gt;
&#20351;&#29992;von Mises-Fisher&#28151;&#21512;&#27169;&#22411;&#20943;&#36731;&#38754;&#37096;&#35782;&#21035;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher Mixture Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.13664
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;BFAR&#21644;BFRR&#65292;&#24182;&#36890;&#36807;&#21518;&#22788;&#29702;&#26041;&#27861;&#20943;&#36731;&#38754;&#37096;&#35782;&#21035;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#26085;&#24120;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#65292;&#20294;&#35768;&#22810;&#35843;&#26597;&#26174;&#31034;&#35768;&#22810;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#65292;&#27495;&#35270;&#29305;&#23450;&#20154;&#32676;&#23376;&#32452;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#65289;&#65292;&#36825;&#20419;&#20351;&#20174;&#19994;&#32773;&#24320;&#21457;&#20855;&#26377;&#19968;&#33268;/&#21487;&#27604;&#24615;&#33021;&#30340;&#20844;&#24179;&#31995;&#32479;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#38754;&#37096;&#35782;&#21035;&#32593;&#32476;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;BFAR&#21644;BFRR&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#22266;&#26377;&#30340;&#37096;&#32626;&#38656;&#27714;&#12290;&#21463;&#20960;&#20309;&#32771;&#34385;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#65292;&#20854;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28145;&#24230;&#23884;&#20837;&#36716;&#25442;&#20026;&#23545;&#21463;&#27495;&#35270;&#20122;&#32452;&#26377;&#26356;&#22810;&#34920;&#31034;&#33021;&#21147;&#12290;&#23427;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;Fa
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.13664v2 Announce Type: replace-cross  Abstract: In spite of the high performance and reliability of deep learning algorithms in a wide range of everyday applications, many investigations tend to show that a lot of models exhibit biases, discriminating against specific subgroups of the population (e.g. gender, ethnicity). This urges the practitioner to develop fair systems with a uniform/comparable performance across sensitive groups. In this work, we investigate the gender bias of deep Face Recognition networks. In order to measure this bias, we introduce two new metrics, $\mathrm{BFAR}$ and $\mathrm{BFRR}$, that better reflect the inherent deployment needs of Face Recognition systems. Motivated by geometric considerations, we mitigate gender bias through a new post-processing methodology which transforms the deep embeddings of a pre-trained model to give more representation power to discriminated subgroups. It consists in training a shallow neural network by minimizing a Fa
&lt;/p&gt;</description></item><item><title>&#19968;&#20010;&#38543;&#26426;&#23431;&#23449;&#19979;&#30340;&#29983;&#21629;&#27010;&#29575;&#38382;&#39064;&#34987;&#37325;&#26032;&#24605;&#32771;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22522;&#26412;&#24120;&#25968;&#30475;&#20284;&#34987;&#31934;&#30830;&#35843;&#25972;&#20197;&#23454;&#29616;&#29983;&#21629;&#21457;&#29983;&#30340;&#39640;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2109.10241</link><description>&lt;p&gt;
&#19968;&#20010;&#38543;&#26426;&#23431;&#23449;&#20013;&#30340;&#29983;&#21629;&#65306;&#37325;&#23457;Sciama&#30340;&#35770;&#35777;
&lt;/p&gt;
&lt;p&gt;
Life in a random universe: Sciama's argument reconsidered
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.10241
&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#38543;&#26426;&#23431;&#23449;&#19979;&#30340;&#29983;&#21629;&#27010;&#29575;&#38382;&#39064;&#34987;&#37325;&#26032;&#24605;&#32771;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22522;&#26412;&#24120;&#25968;&#30475;&#20284;&#34987;&#31934;&#30830;&#35843;&#25972;&#20197;&#23454;&#29616;&#29983;&#21629;&#21457;&#29983;&#30340;&#39640;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#25277;&#26679;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35832;&#22914;&#26680;&#20849;&#25391;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#40657;&#27934;&#33976;&#21457;&#31561;&#21508;&#31181;&#29616;&#35937;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#33521;&#22269;&#29289;&#29702;&#23398;&#23478;&#20025;&#23612;&#26031;&#183;Sciama&#30340;&#19968;&#20010;&#20248;&#38597;&#35770;&#35777;&#65292;&#35813;&#35770;&#35777;&#34920;&#26126;&#65292;&#22914;&#26524;&#25105;&#20204;&#30340;&#23431;&#23449;&#26159;&#38543;&#26426;&#30340;&#65292;&#37027;&#20040;&#29983;&#21629;&#20960;&#20046;&#19981;&#21487;&#33021;&#23384;&#22312;&#12290;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#38543;&#26426;&#23431;&#23449;&#21487;&#20197;&#20266;&#35013;&#25104;&#8220;&#26234;&#33021;&#35774;&#35745;&#8221;&#65292;&#22522;&#26412;&#24120;&#25968;&#20284;&#20046;&#34987;&#31934;&#30830;&#35843;&#25972;&#20197;&#33719;&#24471;&#29983;&#21629;&#21457;&#29983;&#30340;&#26368;&#39640;&#27010;&#29575;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23431;&#23449;&#65292;&#36825;&#31181;&#26426;&#21046;&#21487;&#33021;&#21482;&#38656;&#35201;&#22823;&#32422;&#19968;&#25171;&#30446;&#21069;&#26410;&#30693;&#30340;&#22522;&#26412;&#24120;&#25968;&#12290;&#25105;&#20204;&#25512;&#27979;&#25105;&#20204;&#25152;&#21457;&#29616;&#30340;&#26426;&#21046;&#21487;&#33021;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.10241v4 Announce Type: replace-cross  Abstract: Random sampling in high dimensions has successfully been applied to phenomena as diverse as nuclear resonances, neural networks and black hole evaporation. Here we revisit an elegant argument by the British physicist Dennis Sciama, which demonstrated that were our universe random, it would almost certainly have a negligible chance for life. Under plausible assumptions, we show that a random universe can masquerade as `intelligently designed,' with the fundamental constants instead appearing to be fined tuned to be achieve the highest probability for life to occur. For our universe, this mechanism may only require there to be around a dozen currently unknown fundamental constants. We speculate on broader applications for the mechanism we uncover.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#23398;&#20064;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22870;&#21169;&#26426;&#22120;&#23454;&#29616;&#39640;&#23618;&#27493;&#24577;&#35268;&#33539;&#65292;&#25903;&#25345;&#22312;&#25191;&#34892;&#26102;&#35843;&#25972;&#27493;&#24577;&#39057;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#36991;&#20813;&#32321;&#29712;&#30340;&#36816;&#21160;&#20808;&#39564;&#12290;</title><link>https://arxiv.org/abs/2107.10969</link><description>&lt;p&gt;
&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Quadruped Locomotion Policies using Logical Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.10969
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#23398;&#20064;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22870;&#21169;&#26426;&#22120;&#23454;&#29616;&#39640;&#23618;&#27493;&#24577;&#35268;&#33539;&#65292;&#25903;&#25345;&#22312;&#25191;&#34892;&#26102;&#35843;&#25972;&#27493;&#24577;&#39057;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#36991;&#20813;&#32321;&#29712;&#30340;&#36816;&#21160;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#21160;&#29289;&#33021;&#23637;&#31034;&#22810;&#26679;&#21270;&#30340;&#36816;&#21160;&#27493;&#24577;&#12290;&#23613;&#31649;&#22312;&#26426;&#22120;&#20154;&#19978;&#23637;&#31034;&#20102;&#36825;&#20123;&#27493;&#24577;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#36816;&#21160;&#20808;&#39564;&#12289;&#21160;&#21147;&#23398;&#27169;&#22411;&#25110;&#20854;&#20182;&#24418;&#24335;&#30340;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#22120;&#23454;&#29616;&#39640;&#23618;&#27493;&#24577;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;RM-based Locomotion Learning&#65288;RMLL&#65289;&#65292;&#25903;&#25345;&#22312;&#25191;&#34892;&#26102;&#35843;&#25972;&#27493;&#24577;&#39057;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;&#27599;&#20010;&#27493;&#24577;&#30340;&#23569;&#37327;&#36923;&#36753;&#35268;&#21017;&#65288;&#20363;&#22914;&#65292;&#21069;&#33050;&#21644;&#21518;&#33050;&#20132;&#26367;&#31227;&#21160;&#65289;&#65292;&#23454;&#29616;&#20102;&#27493;&#24577;&#35268;&#33539;&#65292;&#26080;&#38656;&#36153;&#21147;&#30340;&#36816;&#21160;&#20808;&#39564;&#12290;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#23398;&#21040;&#30340;&#27493;&#24577;&#30340;&#22810;&#26679;&#24615;&#65288;&#21253;&#25324;&#20004;&#31181;&#26032;&#39062;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2107.10969v3 Announce Type: replace-cross  Abstract: Quadruped animals are capable of exhibiting a diverse range of locomotion gaits. While progress has been made in demonstrating such gaits on robots, current methods rely on motion priors, dynamics models, or other forms of extensive manual efforts. People can use natural language to describe dance moves. Could one use a formal language to specify quadruped gaits? To this end, we aim to enable easy gait specification and efficient policy learning. Leveraging Reward Machines~(RMs) for high-level gait specification over foot contacts, our approach is called RM-based Locomotion Learning~(RMLL), and supports adjusting gait frequency at execution time. Gait specification is enabled through the use of a few logical rules per gait (e.g., alternate between moving front feet and back feet) and does not require labor-intensive motion priors. Experimental results in simulation highlight the diversity of learned gaits (including two novel g
&lt;/p&gt;</description></item><item><title>AML&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20813;&#21463;&#23433;&#20840;&#23041;&#32961;&#65292;&#36125;&#21494;&#26031;&#35270;&#35282;&#20026;&#38450;&#24481;&#25552;&#20379;&#20102;&#26032;&#30340;&#22909;&#22788;</title><link>https://arxiv.org/abs/2003.03546</link><description>&lt;p&gt;
&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65306;&#36125;&#21494;&#26031;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Adversarial Machine Learning: Bayesian Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2003.03546
&lt;/p&gt;
&lt;p&gt;
AML&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20813;&#21463;&#23433;&#20840;&#23041;&#32961;&#65292;&#36125;&#21494;&#26031;&#35270;&#35282;&#20026;&#38450;&#24481;&#25552;&#20379;&#20102;&#26032;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;(AML)&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;(ML)&#31995;&#32479;&#20813;&#21463;&#23433;&#20840;&#23041;&#32961;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#23384;&#22312;&#25932;&#23545;&#26041;&#31215;&#26497;&#25805;&#32437;&#36755;&#20837;&#25968;&#25454;&#20197;&#27450;&#39575;&#23398;&#20064;&#31995;&#32479;&#12290; &#36825;&#21019;&#36896;&#20102;&#19968;&#31867;&#26032;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;ML&#31995;&#32479;&#21487;&#33021;&#20250;&#38754;&#20020;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#34987;&#31216;&#20026;&#25932;&#23545;&#31283;&#20581;&#24615;&#30340;&#21487;&#20449;&#25805;&#20316;&#25152;&#24517;&#38656;&#30340;&#24615;&#36136;&#12290; &#22823;&#37096;&#20998;AML&#24037;&#20316;&#37117;&#24314;&#31435;&#22312;&#23545;&#25239;&#23398;&#20064;&#31995;&#32479;&#21644;&#20934;&#22791;&#25805;&#32437;&#36755;&#20837;&#25968;&#25454;&#30340;&#23545;&#25163;&#20043;&#38388;&#20914;&#31361;&#30340;&#21338;&#24328;&#35770;&#24314;&#27169;&#20043;&#19978;&#12290; &#36825;&#20551;&#35774;&#27599;&#20010;&#20195;&#29702;&#37117;&#20102;&#35299;&#23545;&#25163;&#30340;&#20852;&#36259;&#21644;&#19981;&#30830;&#23450;&#24615;&#21028;&#26029;&#65292;&#20174;&#32780;&#20419;&#36827;&#22522;&#20110;Nash&#22343;&#34913;&#30340;&#25512;&#29702;&#12290; &#28982;&#32780;&#65292;&#22312;AML&#20856;&#22411;&#30340;&#23433;&#20840;&#26041;&#26696;&#20013;&#65292;&#36825;&#31181;&#20849;&#21516;&#30693;&#35782;&#20551;&#35774;&#24182;&#19981;&#29616;&#23454;&#12290; &#22312;&#22238;&#39038;&#20102;&#36825;&#31181;&#21338;&#24328;&#35770;&#26041;&#27861;&#20043;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36125;&#21494;&#26031;&#35270;&#35282;&#22312;&#38450;&#24481;&#20013;&#25552;&#20379;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
arXiv:2003.03546v2 Announce Type: replace  Abstract: Adversarial Machine Learning (AML) is emerging as a major field aimed at protecting machine learning (ML) systems against security threats: in certain scenarios there may be adversaries that actively manipulate input data to fool learning systems. This creates a new class of security vulnerabilities that ML systems may face, and a new desirable property called adversarial robustness essential to trust operations based on ML outputs. Most work in AML is built upon a game-theoretic modelling of the conflict between a learning system and an adversary, ready to manipulate input data. This assumes that each agent knows their opponent's interests and uncertainty judgments, facilitating inferences based on Nash equilibria. However, such common knowledge assumption is not realistic in the security scenarios typical of AML. After reviewing such game-theoretic approaches, we discuss the benefits that Bayesian perspectives provide when defendin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;</title><link>http://arxiv.org/abs/2401.13223</link><description>&lt;p&gt;
TAT-LLM: &#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13223
&lt;/p&gt;
&lt;p&gt;
TAT-LLM&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31163;&#25955;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#32780;&#20026;&#20102;&#24212;&#23545;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TAT-LLM&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#36739;&#23567;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#28151;&#21512;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#38382;&#31572;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;Web&#19978;&#38750;&#24120;&#24120;&#35265;&#65288;&#22914;SEC&#25991;&#20214;&#65289;&#65292;&#36890;&#24120;&#38656;&#35201;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#22810;&#27493;&#39588;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#35299;&#20915;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38754;&#21521;&#34920;&#26684;&#21644;&#25991;&#26412;&#38382;&#31572;&#30340;&#20998;&#27493;&#27969;&#27700;&#32447;&#30340;&#25277;&#35937;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#25512;&#29702;&#22120;&#21644;&#25191;&#34892;&#22120;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#24182;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20221;&#25351;&#20196;&#26469;&#23454;&#20363;&#21270;&#35813;&#27969;&#27700;&#32447;&#24182;&#39564;&#35777;GPT-4&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#20687;GPT-4&#36825;&#26679;&#30340;&#22312;&#32447;LLM&#23384;&#22312;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#25968;&#25454;&#23433;&#20840;&#39118;&#38505;&#31561;&#21508;&#31181;&#25361;&#25112;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#24320;&#21457;&#36739;&#23567;&#30340;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29616;&#26377;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#33258;&#21160;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;LLaMA 2&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;TAT-LLM&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-w
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#23545;&#25509;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#29305;&#24615;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#30450;&#30446;&#23545;&#25509;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12999</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
Quantum-Inspired Machine Learning for Molecular Docking. (arXiv:2401.12999v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12999
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#23545;&#25509;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#29305;&#24615;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#30450;&#30446;&#23545;&#25509;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#26500;&#24314;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#21487;&#20197;&#21152;&#24555;&#33647;&#29289;&#24320;&#21457;&#30340;&#25928;&#29575;&#12290;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#32467;&#21512;&#36807;&#31243;&#38656;&#35201;&#22312;&#24191;&#27867;&#30340;&#31354;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#25628;&#32034;&#21644;&#37319;&#26679;&#12290;&#20256;&#32479;&#30340;&#23545;&#25509;&#26041;&#27861;&#36890;&#36807;&#25628;&#32034;&#21487;&#33021;&#30340;&#32467;&#21512;&#20301;&#28857;&#21644;&#26500;&#35937;&#26469;&#23454;&#29616;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#22312;&#30450;&#30446;&#23545;&#25509;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#21463;&#21040;&#36825;&#19968;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#37327;&#23376;&#21551;&#21457;&#31639;&#27861;&#19982;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#30450;&#30446;&#23545;&#25509;&#20013;&#30340;&#25913;&#36827;&#12290;&#25968;&#20540;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;&#23545;&#25509;&#31639;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#19978;&#34920;&#29616;&#20986;&#20102;&#36229;&#36807;10%&#30340;&#25552;&#21319;&#12290;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23545;&#25509;&#31639;&#27861;DiffDock&#30456;&#27604;&#65292;Top-1&#65288;RMSD&lt;2&#65289;&#30340;&#25104;&#21151;&#29575;&#20174;33%&#25552;&#39640;&#21040;35%&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular docking is an important tool for structure-based drug design, accelerating the efficiency of drug development. Complex and dynamic binding processes between proteins and small molecules require searching and sampling over a wide spatial range. Traditional docking by searching for possible binding sites and conformations is computationally complex and results poorly under blind docking. Quantum-inspired algorithms combining quantum properties and annealing show great advantages in solving combinatorial optimization problems. Inspired by this, we achieve an improved in blind docking by using quantum-inspired combined with gradients learned by deep learning in the encoded molecular space. Numerical simulation shows that our method outperforms traditional docking algorithms and deep learning-based algorithms over 10\%. Compared to the current state-of-the-art deep learning-based docking algorithm DiffDock, the success rate of Top-1 (RMSD&lt;2) achieves an improvement from 33\% to 35
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#35813;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.12874</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#21040;&#24212;&#29992;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Understanding to Utilization: A Survey on Explainability for Large Language Models. (arXiv:2401.12874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#35813;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;LLMs&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20854;&#8220;&#40657;&#30418;&#8221;&#24615;&#36136;&#24341;&#21457;&#20102;&#23545;&#36879;&#26126;&#24615;&#21644;&#20262;&#29702;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;LLMs&#65292;&#22914;LLaMA&#65292;&#20854;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#20351;&#20854;&#38754;&#20020;&#29420;&#29305;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20195;&#34920;&#24615;&#30340;&#35780;&#20215;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#32508;&#36848;&#30340;&#30446;&#26631;&#26159;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20379;&#20174;&#25216;&#26415;&#35282;&#24230;&#24635;&#32467;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing. With LLMs playing a pivotal role in various applications, their "black-box" nature raises concerns about transparency and ethical use. This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models. We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity. Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability. We also discuss representative evaluation methods, highlighting their strengths and limitations. The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12873</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#30340;&#21453;&#39304;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;: &#23558;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#20805;&#20998;&#24314;&#27169;&#20154;&#31867;&#20559;&#22909;&#23548;&#33268;&#22870;&#21169;&#27169;&#22411;&#22312;&#21033;&#29992;&#20154;&#30340;&#21453;&#39304;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36136;&#37327;&#20272;&#35745;(QE)&#22312;&#36807;&#21435;&#20004;&#24180;&#20013;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#23601;&#33021;&#20934;&#30830;&#39044;&#27979;&#32473;&#23450;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;QE&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;(&#22522;&#20110;QE&#30340;&#22870;&#21169;&#27169;&#22411;)&#26469;&#39044;&#27979;&#20154;&#30340;&#20559;&#22909;&#20197;&#36827;&#34892;&#21453;&#39304;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#20102;&#22312;&#22522;&#20110;QE&#30340;&#21453;&#39304;&#35757;&#32451;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#34920;&#29616;&#20026;&#22870;&#21169;&#30340;&#22686;&#21152;&#32780;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35748;&#20026;QE&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#32763;&#35793;&#30340;&#39640;&#22870;&#21169;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#20248;&#21270;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#65292;&#24182;&#20026;QE&#27169;&#22411;&#28155;&#21152;&#20102;&#19968;&#20010;&#24809;&#32602;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
&lt;/p&gt;</description></item><item><title>E^2-LLM&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#21644;&#19981;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#20943;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#65292;E^2-LLM&#21482;&#38656;&#35201;&#36739;&#30701;&#30340;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;</title><link>http://arxiv.org/abs/2401.06951</link><description>&lt;p&gt;
E^2-LLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
E^2-LLM: Efficient and Extreme Length Extension of Large Language Models. (arXiv:2401.06951v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06951
&lt;/p&gt;
&lt;p&gt;
E^2-LLM&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#21644;&#19981;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#20943;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#65292;E^2-LLM&#21482;&#38656;&#35201;&#36739;&#30701;&#30340;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;GPU&#36164;&#28304;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#25193;&#23637;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#36807;&#31243;&#26469;&#25903;&#25345;&#30456;&#24212;&#30340;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#35757;&#32451;&#25968;&#25454;&#65288;&#20363;&#22914;32k&#65289;&#65292;&#24182;&#19988;&#20551;&#23450;&#26377;&#39640;&#26114;&#30340;GPU&#35757;&#32451;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E^2-LLM&#30340;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20063;&#19981;&#38656;&#35201;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;E^2-LLM&#30340;&#35757;&#32451;&#25968;&#25454;&#21482;&#38656;&#35201;&#24456;&#30701;&#30340;&#38271;&#24230;&#65288;&#20363;&#22914;4k&#65289;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35843;&#25972;&#25104;&#26412;&#12290;&#20854;&#27425;&#65292;&#22312;&#30701;&#35757;&#32451;&#19978;&#19979;&#25991;&#31383;&#21475;&#19978;&#30340;&#35757;&#32451;&#36807;&#31243;&#21482;&#25191;&#34892;&#19968;&#27425;&#65292;&#25105;&#20204;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#31532;&#19977;&#65292;&#22312;E^2-LLM&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly. Second, the training procedure on the short training context window is performed only once time, and we can support different evaluation context windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings, we 
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.05930</link><description>&lt;p&gt;
SH2: &#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#24110;&#21161;&#24744;&#26356;&#20934;&#30830;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05930
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#26041;&#27861;&#65292;&#21363;&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;(SH2)&#65292;&#20197;&#24110;&#21161;LLMs&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;SH2&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#20013;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#23454;&#65292;&#21363;&#23545;&#20110;LLMs&#32780;&#35328;&#65292;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#24448;&#24448;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLMs&#32473;&#20104;&#36739;&#20302;&#27010;&#29575;&#30340;&#26631;&#35760;&#26356;&#26377;&#21487;&#33021;&#19982;&#20107;&#23454;&#20449;&#24687;&#65288;&#22914;&#21517;&#35789;&#12289;&#19987;&#26377;&#21517;&#35789;&#21644;&#24418;&#23481;&#35789;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36873;&#25321;&#27010;&#29575;&#26368;&#20302;&#30340;&#26631;&#35760;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#21407;&#22987;&#19978;&#19979;&#25991;&#20013;&#26469;&#8220;&#31361;&#20986;&#8221;&#20107;&#23454;&#20449;&#24687;&#65292;&#20174;&#32780;&#36843;&#20351;&#27169;&#22411;&#22312;&#29983;&#25104;&#20043;&#21069;&#22810;&#27425;&#38405;&#35835;&#21644;&#29369;&#35947;&#36825;&#20123;&#26631;&#35760;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#23545;&#27604;&#35299;&#30721;&#30340;&#26041;&#24335;&#26469;&#24378;&#35843;&#30001;&#29369;&#35947;&#24102;&#26469;&#30340;&#36755;&#20986;&#27010;&#29575;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
&lt;/p&gt;</description></item><item><title>TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.01330</link><description>&lt;p&gt;
TREC iKAT 2023: &#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview. (arXiv:2401.01330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01330
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20449;&#24687;&#26597;&#35810;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#26377;&#24456;&#22823;&#30340;&#36129;&#29486;&#12290;TREC&#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65288;iKAT&#65289;&#24314;&#31435;&#22312;TREC&#20250;&#35805;&#36741;&#21161;&#20219;&#21153;&#65288;CAsT&#65289;&#30340;&#22522;&#30784;&#19978;&#12290;&#28982;&#32780;&#65292;iKAT&#30528;&#37325;&#20110;&#21019;&#24314;&#21644;&#30740;&#31350;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#20043;&#21069;&#30340;&#20132;&#20114;&#21644;&#24403;&#21069;&#24773;&#22659;&#33258;&#36866;&#24212;&#21709;&#24212;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#25361;&#25112;&#22312;&#20110;&#20351;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#33021;&#22815;&#23558;&#20010;&#24615;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#20837;&#21040;&#30456;&#24212;&#20013;&#65292;&#20197;&#39640;&#25928;&#22320;&#24341;&#23548;&#29992;&#25143;&#33719;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;iKAT&#36824;&#30528;&#37325;&#20110;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#21363;&#29992;&#25143;&#36890;&#36807;&#25968;&#25454;&#21644;&#20449;&#24687;&#31579;&#36873;&#26469;&#34913;&#37327;&#21508;&#31181;&#36873;&#25321;&#65292;&#20197;&#36798;&#21040;&#32467;&#35770;&#25110;&#25191;&#34892;&#21160;&#20316;&#12290;&#36825;&#20123;&#20219;&#21153;&#22312;&#26085;&#24120;&#20449;&#24687;&#25628;&#32034;&#20915;&#31574;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#26080;&#35770;&#26159;&#26053;&#28216;&#12289;&#20581;&#24247;&#36824;&#26159;&#36141;&#29289;&#31561;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#32452;&#39640;&#32423;&#20449;&#24687;&#25805;&#20316;&#31526;&#65292;&#20854;&#20013;&#26597;&#35810;&#25110;&#38382;&#39064;&#21487;&#33021;&#20250;
&lt;/p&gt;
&lt;p&gt;
Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works. The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context. The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them. iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action. These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions a
&lt;/p&gt;</description></item><item><title>AesFA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#39057;&#29575;&#20998;&#35299;&#22270;&#20687;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#24320;&#32654;&#23398;&#39118;&#26684;&#65292;&#25490;&#38500;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#24341;&#20837;&#20102;&#23545;&#27604;&#25439;&#22833;&#20197;&#25552;&#39640;&#39118;&#26684;&#21270;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AesFA&#22312;stylization quality&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#24555;&#36895;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2312.05928</link><description>&lt;p&gt;
AesFA:&#19968;&#31181;&#32654;&#23398;&#29305;&#24449;&#24863;&#30693;&#30340;&#20219;&#24847;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer. (arXiv:2312.05928v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05928
&lt;/p&gt;
&lt;p&gt;
AesFA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#39057;&#29575;&#20998;&#35299;&#22270;&#20687;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#24320;&#32654;&#23398;&#39118;&#26684;&#65292;&#25490;&#38500;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#24341;&#20837;&#20102;&#23545;&#27604;&#25439;&#22833;&#20197;&#25552;&#39640;&#39118;&#26684;&#21270;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AesFA&#22312;stylization quality&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#24555;&#36895;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#24555;&#36895;&#36827;&#23637;&#21644;&#25913;&#36827;&#65292;&#29616;&#26377;&#30340;NST&#26041;&#27861;&#35201;&#20040;&#22312;&#26377;&#25928;&#36716;&#31227;&#39118;&#26684;&#26102;&#24456;&#38590;&#20445;&#30041;&#32654;&#23398;&#20449;&#24687;&#65292;&#35201;&#20040;&#22312;&#29305;&#24449;&#35299;&#32544;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30001;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#32780;&#23384;&#22312;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20302;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;AesFA -- &#32654;&#23398;&#29305;&#24449;&#24863;&#30693;&#30340;NST&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#39057;&#29575;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#35299;&#65292;&#20197;&#26356;&#22909;&#22320;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#35299;&#24320;&#32654;&#23398;&#39118;&#26684;&#65292;&#21516;&#26102;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#23436;&#20840;&#21462;&#28040;&#20102;&#25512;&#26029;&#26102;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#32593;&#32476;&#25552;&#21462;&#26356;&#21152;&#29420;&#29305;&#30340;&#34920;&#31034;&#21644;&#36827;&#19968;&#27493;&#22686;&#24378;&#39118;&#26684;&#21270;&#36136;&#37327;&#30340;&#33021;&#21147;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32654;&#23398;&#29305;&#24449;&#65306;&#23545;&#27604;&#25439;&#22833;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#39118;&#26684;&#21270;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#26368;&#26032;&#30340;NST&#26041;&#27861;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural style transfer (NST) has evolved significantly in recent years. Yet, despite its rapid progress and advancement, existing NST methods either struggle to transfer aesthetic information from a style effectively or suffer from high computational costs and inefficiencies in feature disentanglement due to using pre-trained models. This work proposes a lightweight but effective model, AesFA -- Aesthetic Feature-Aware NST. The primary idea is to decompose the image via its frequencies to better disentangle aesthetic styles from the reference image while training the entire model in an end-to-end manner to exclude pre-trained models at inference completely. To improve the network's ability to extract more distinct representations and further enhance the stylization quality, this work introduces a new aesthetic feature: contrastive loss. Extensive experiments and ablations show the approach not only outperforms recent NST methods in terms of stylization quality, but it also achieves fast
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#30340;&#25991;&#26412;&#26631;&#20934;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#30452;&#25509;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#65292;&#24182;&#33021;&#22312;&#21508;&#31181;&#26631;&#20934;&#19979;&#26377;&#25928;&#22320;&#32858;&#31867;&#22270;&#20687;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18297</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering Conditioned on Text Criteria. (arXiv:2310.18297v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#30340;&#25991;&#26412;&#26631;&#20934;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#30452;&#25509;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#65292;&#24182;&#33021;&#22312;&#21508;&#31181;&#26631;&#20934;&#19979;&#26377;&#25928;&#22320;&#32858;&#31867;&#22270;&#20687;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32858;&#31867;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#28385;&#36275;&#29992;&#25143;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#25511;&#21046;&#38656;&#27714;&#65292;&#32780;&#19988;&#32858;&#31867;&#32467;&#26524;&#21487;&#33021;&#19982;&#29992;&#25143;&#24515;&#20013;&#30456;&#20851;&#30340;&#26631;&#20934;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#30340;&#25991;&#26412;&#26631;&#20934;&#65292;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#22522;&#20110;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#32858;&#31867;&#65288;IC|TC&#65289;&#65292;&#23427;&#20195;&#34920;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#22270;&#20687;&#32858;&#31867;&#33539;&#24335;&#12290;IC|TC&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#65292;&#24182;&#20351;&#29992;&#25143;&#33021;&#22815;&#26377;&#36739;&#22823;&#30340;&#25511;&#21046;&#26435;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IC|TC&#33021;&#22815;&#26377;&#25928;&#22320;&#25353;&#29031;&#21508;&#31181;&#26631;&#20934;&#65288;&#22914;&#20154;&#31867;&#34892;&#20026;&#12289;&#29289;&#29702;&#20301;&#32622;&#25110;&#20154;&#30340;&#24515;&#24773;&#65289;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#32780;&#19988;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.10690</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#29983;&#24314;&#27169;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20174;&#19968;&#27425;&#24615;&#35266;&#23519;&#20013;&#21512;&#25104;&#35270;&#35273;&#32534;&#31243;&#20013;&#23398;&#29983;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#24314;&#27169;&#23545;&#20110;&#35768;&#22810;&#25945;&#32946;&#25216;&#26415;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#23398;&#20064;&#32467;&#26524;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#23398;&#29983;&#34920;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#19988;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#23398;&#20064;&#25216;&#33021;&#38598;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LLM-SS&#65292;&#21033;&#29992;LLMs&#21512;&#25104;&#23398;&#29983;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#29305;&#23450;&#23398;&#29983;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#30340;&#35299;&#20915;&#23581;&#35797;&#20316;&#20026;&#35266;&#23519;&#65292;&#30446;&#26631;&#26159;&#21512;&#25104;&#35813;&#23398;&#29983;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;LLMs&#32467;&#21512;&#20351;&#29992;&#65307;&#32780;&#19988;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#23427;&#20204;&#23545;&#39046;&#22495;&#32972;&#26223;&#21644;&#23398;&#29983;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#20855;&#20307;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods bas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#22312;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#20381;&#36182;&#20110;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2310.09358</link><description>&lt;p&gt;
&#20309;&#26102;&#25165;&#33021;&#20351;&#21095;&#26412;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#20445;&#25345;&#31283;&#23450;? (arXiv:2310.09358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
When are Bandits Robust to Misspecification?. (arXiv:2310.09358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09358
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#22312;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#20381;&#36182;&#20110;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#22870;&#21169;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#12290;&#36890;&#24120;&#30340;&#20551;&#35774;&#26159;&#21487;&#34892;&#24615;&#65292;&#21363;&#34892;&#20026;&#30340;&#30495;&#23454;&#22870;&#21169;&#23436;&#20840;&#30001;&#26576;&#20010;&#21442;&#25968;&#21270;&#27169;&#22411;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#31867;&#20043;&#38388;&#23384;&#22312;&#65288;&#21487;&#33021;&#26174;&#33879;&#65289;&#30340;&#35823;&#24046;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20381;&#36182;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#22312;&#21363;&#20351;&#22870;&#21169;&#23384;&#22312;&#20005;&#37325;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#35777;&#27425;&#32447;&#24615;&#65288;&#27425;&#20110;&#26102;&#38388;&#33539;&#22260;&#65289;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;&#36825;&#19982;&#29616;&#26377;&#30340;&#38024;&#23545;&#38169;&#35823;&#35268;&#33539;&#30340;&#26368;&#22351;&#24773;&#20917;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#26174;&#31034;&#36951;&#25022;&#36793;&#30028;&#38543;&#26102;&#38388;&#25104;&#32447;&#24615;&#27604;&#20363;&#22686;&#38271;&#65292;&#24182;&#19988;&#35828;&#26126;&#23384;&#22312;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#24378;&#30423;&#38382;&#39064;&#23454;&#20363;&#38598;&#21512;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parametric feature-based reward models are widely employed by algorithms for decision making settings such as bandits and contextual bandits. The typical assumption under which they are analysed is realizability, i.e., that the true rewards of actions are perfectly explained by some parametric model in the class. We are, however, interested in the situation where the true rewards are (potentially significantly) misspecified with respect to the model class. For parameterized bandits and contextual bandits, we identify sufficient conditions, depending on the problem instance and model class, under which classic algorithms such as $\epsilon$-greedy and LinUCB enjoy sublinear (in the time horizon) regret guarantees under even grossly misspecified rewards. This is in contrast to existing worst-case results for misspecified bandits which show regret bounds that scale linearly with time, and shows that there can be a nontrivially large set of bandit instances that are robust to misspecificati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21453;&#21521;&#38142;&#30340;&#36890;&#29992;&#35268;&#21017;&#65292;&#36890;&#36807;&#21453;&#21521;&#38142;&#24605;&#36335;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#22806;&#37096;API&#23436;&#25104;&#22797;&#26434;&#30340;&#20989;&#25968;&#35843;&#29992;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22635;&#20805;&#21442;&#25968;&#30340;&#26041;&#24335;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04474</link><description>&lt;p&gt;
&#21453;&#21521;&#38142;&#65306;&#19968;&#31181;&#36890;&#29992;&#35268;&#21017;&#65292;&#29992;&#20110;&#20351;LLMs&#25484;&#25569;&#22810;API&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning. (arXiv:2310.04474v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21453;&#21521;&#38142;&#30340;&#36890;&#29992;&#35268;&#21017;&#65292;&#36890;&#36807;&#21453;&#21521;&#38142;&#24605;&#36335;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#22806;&#37096;API&#23436;&#25104;&#22797;&#26434;&#30340;&#20989;&#25968;&#35843;&#29992;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22635;&#20805;&#21442;&#25968;&#30340;&#26041;&#24335;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#20989;&#25968;&#35843;&#29992;&#65288;&#21363;API&#65289;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;API&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#20989;&#25968;&#35843;&#29992;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#21487;&#25511;&#30340;&#30446;&#26631;&#39537;&#21160;&#26041;&#27861;&#65292;&#31216;&#20026;&#21453;&#21521;&#38142;&#65292;&#20197;&#20351;LLMs&#33021;&#22815;&#20165;&#36890;&#36807;&#25552;&#31034;&#20351;&#29992;&#22806;&#37096;API&#12290;&#22312;&#21453;&#21521;&#38142;&#20013;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;LLMs&#20165;&#29992;&#20110;&#23454;&#29616;&#31616;&#21333;&#20219;&#21153;&#65292;&#20363;&#22914;API&#36873;&#25321;&#21644;&#21442;&#25968;&#34917;&#20840;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#35268;&#21017;&#23454;&#29616;&#21487;&#25511;&#30340;&#22810;&#20989;&#25968;&#35843;&#29992;&#12290;&#22312;&#36825;&#20010;&#36890;&#29992;&#35268;&#21017;&#20013;&#65292;&#36873;&#25321;&#19968;&#20010;&#26368;&#32456;API&#26469;&#22788;&#29702;&#32473;&#23450;&#20219;&#21153;&#20043;&#21518;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#27714;LLMs&#20174;&#29992;&#25143;&#26597;&#35810;&#21644;&#19978;&#19979;&#25991;&#20013;&#22635;&#20889;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;&#19968;&#20123;&#32570;&#22833;&#30340;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#35753;LLMs&#22522;&#20110;API&#25551;&#36848;&#36873;&#25321;&#21478;&#19968;&#20010;API&#26469;&#36827;&#19968;&#27493;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
While enabling large language models to implement function calling (known as APIs) can greatly enhance the performance of LLMs, function calling is still a challenging task due to the complicated relations between different APIs, especially in a context-learning setting without fine-tuning. This paper proposes a simple yet controllable target-driven approach called Reverse Chain to empower LLMs with capabilities to use external APIs with only prompts. Given that most open-source LLMs have limited tool-use or tool-plan capabilities, LLMs in Reverse Chain are only employed to implement simple tasks, e.g., API selection and argument completion, and a generic rule is employed to implement a controllable multiple functions calling. In this generic rule, after selecting a final API to handle a given task via LLMs, we first ask LLMs to fill the required arguments from user query and context. Some missing arguments could be further completed by letting LLMs select another API based on API desc
&lt;/p&gt;</description></item><item><title>ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.13007</link><description>&lt;p&gt;
ReConcile&#65306;&#22278;&#26700;&#20250;&#35758;&#36890;&#36807;&#22810;&#20803;LLM&#30340;&#20849;&#35782;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13007
&lt;/p&gt;
&lt;p&gt;
ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#21463;&#21040;&#24515;&#26234;&#31038;&#20250;&#29702;&#35770;&#65288;Minsky, 1988&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReConcile&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22810;&#26679;&#30340;LLM&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#22278;&#26700;&#20250;&#35758;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#24605;&#24819;&#21644;&#35752;&#35770;&#65292;&#20174;&#32780;&#25913;&#36827;&#19968;&#33268;&#24615;&#12290;ReConcile&#36890;&#36807;&#36827;&#34892;&#22810;&#36718;&#35752;&#35770;&#12289;&#23398;&#20064;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#25913;&#36827;&#31572;&#26696;&#20197;&#21450;&#37319;&#29992;&#32622;&#20449;&#24230;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;ReConcile&#36890;&#36807;&#8220;&#35752;&#35770;&#25552;&#31034;&#8221;&#26469;&#21551;&#21160;&#20195;&#29702;&#20154;&#38388;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#19978;&#19968;&#36718;&#27599;&#20010;&#20195;&#29702;&#20154;&#29983;&#25104;&#30340;&#31572;&#26696;&#21644;&#35299;&#37322;&#30340;&#20998;&#32452;&#12289;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#29992;&#20110;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#31572;&#26696;&#20462;&#27491;&#20154;&#31867;&#35299;&#37322;&#30340;&#28436;&#31034;&#12290;&#36825;&#20010;&#35752;&#35770;&#25552;&#31034;&#20351;&#27599;&#20010;&#20195;&#29702;&#20154;&#33021;&#22815;&#26681;&#25454;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#35265;&#35299;&#20462;&#35746;&#33258;&#24049;&#30340;&#22238;&#31572;&#12290;&#19968;&#26086;&#36798;&#25104;&#19968;&#33268;&#24182;&#32467;&#26463;&#35752;&#35770;&#65292;ReConcile&#25191;&#34892;&#19968;&#27425;&#20840;&#20307;&#25237;&#31080;&#20197;&#30830;&#23450;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.16898</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;"Attention Is All You Need"&#20013;&#24341;&#20837;&#36716;&#25442;&#22120;&#26550;&#26500;&#20197;&#26469;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#25509;&#21463;&#36755;&#20837;&#20196;&#29260;&#24207;&#21015;$X$&#24182;&#36890;&#36807;&#35745;&#31639;softmax$(XQK^\top X^\top)$&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;$(K,Q)$&#26159;&#21487;&#35757;&#32451;&#30340;&#38190;-&#26597;&#35810;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#20248;&#21270;&#20960;&#20309;&#21644;&#19968;&#20010;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#20196;&#29260;&#23545;&#30340;&#22806;&#31215;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#23558;&#26368;&#20339;&#36755;&#20837;&#20196;&#29260;&#19982;&#38750;&#26368;&#20339;&#20196;&#29260;&#20998;&#31163;&#12290;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21333;&#23618;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;(1)&#20248;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#29992;&#21487;&#21464;&#27491;&#21017;&#21270;&#21442;&#25968;$(K,Q)$&#65292;&#25910;&#25947;&#30340;&#26041;&#21521;&#26159;&#19968;&#20010;&#26368;&#23567;&#21270;&#32508;&#21512;&#21442;&#25968;$W=KQ^\top$&#30340;&#26680;&#33539;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;&#20915;&#26041;&#26696;&#12290;&#32780;&#30452;&#25509;&#20351;&#29992;$W$&#36827;&#34892;&#21442;&#25968;&#21270;&#21017;&#26368;&#23567;&#21270;&#19968;&#20010;Frobenius&#33539;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#26041;&#27861;&#26469;&#28608;&#27963;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08241</link><description>&lt;p&gt;
TEST: &#25991;&#26412;&#21407;&#22411;&#23545;&#40784;&#23884;&#20837;&#20197;&#28608;&#27963;LLM&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. (arXiv:2308.08241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#26041;&#27861;&#26469;&#28608;&#27963;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#20219;&#21153;&#30340;&#31574;&#30053;&#65306;LLM-for-TS&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#38024;&#23545;TS&#25968;&#25454;&#30340;&#22522;&#30784;&#22823;&#27169;&#22411;&#65307;TS-for-LLM&#65292;&#20351;&#39044;&#35757;&#32451;&#30340;LLM&#33021;&#22815;&#22788;&#29702;TS&#25968;&#25454;&#12290;&#37492;&#20110;&#25968;&#25454;&#31215;&#32047;&#19981;&#36275;&#12289;&#36164;&#28304;&#26377;&#38480;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;TS-for-LLM&#26041;&#27861;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;LLM&#30340;TS&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#28608;&#27963;LLM&#23545;TS&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;TEST&#12290;&#23427;&#39318;&#20808;&#23545;TS&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#65292;&#24314;&#31435;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23454;&#20363;&#12289;&#29305;&#24449;&#21644;&#25991;&#26412;&#21407;&#22411;&#23545;&#40784;&#23545;&#23427;&#20204;&#36827;&#34892;&#23884;&#20837;&#65292;&#28982;&#21518;&#21019;&#24314;&#25552;&#31034;&#20197;&#20351;LLM&#26356;&#23481;&#26131;&#25509;&#21463;&#23884;&#20837;&#65292;&#24182;&#26368;&#32456;&#23454;&#26045;TS&#20219;&#21153;&#12290;&#20351;&#29992;8&#20010;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#22823;&#23567;&#30340;LLM&#23545;TS&#20998;&#31867;&#21644;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23613;&#31649;&#20854;&#32467;&#26524;&#19981;&#33021;&#26174;&#33879;&#36229;&#36234;&#24403;&#21069;&#20026;TS&#20219;&#21153;&#23450;&#21046;&#30340;SOTA&#27169;&#22411;&#65292;&#20294;&#36890;&#36807;&#23558;LLM&#35270;&#20026;&#27169;&#24335;&#26426;&#22120;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;TS&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work summarizes two strategies for completing time-series (TS) tasks using today's language model (LLM): LLM-for-TS, design and train a fundamental large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS data. Considering the insufficient data accumulation, limited resources, and semantic context requirements, this work focuses on TS-for-LLM methods, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make LLM more open to embeddings, and finally implements TS tasks. Experiments are carried out on TS classification and forecasting tasks using 8 LLMs with different structures and sizes. Although its results cannot significantly outperform the current SOTA models customized for TS tasks, by treating LLM as the pattern machine, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06032</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#33021;&#21542;&#21462;&#20195;&#24459;&#24072;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#23545;&#27861;&#24459;&#31995;&#32479;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#22312;&#36827;&#34892;&#27861;&#24459;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30740;&#31350;&#28041;&#21450;&#21152;&#23494;&#36135;&#24065;&#30340;&#35777;&#21048;&#26696;&#20214;&#65292;&#20316;&#20026;AI&#21487;&#20197;&#25903;&#25345;&#27861;&#24459;&#36807;&#31243;&#30340;&#20247;&#22810;&#24773;&#22659;&#20043;&#19968;&#65292;&#30740;&#31350;LLMs&#30340;&#27861;&#24459;&#25512;&#29702;&#21644;&#36215;&#33609;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;a&#65289;LLM&#33021;&#21542;&#20934;&#30830;&#30830;&#23450;&#20107;&#23454;&#27169;&#24335;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36829;&#27861;&#34892;&#20026;&#65292;b&#65289;&#22522;&#20110;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#65292;&#38506;&#23457;&#22242;&#30340;&#20915;&#31574;&#26159;&#21542;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#30495;&#23454;&#26696;&#20363;&#20013;&#30340;&#20107;&#23454;&#27169;&#24335;&#36755;&#20837;GPT-3.5&#65292;&#24182;&#35780;&#20272;&#20854;&#30830;&#23450;&#27491;&#30830;&#28508;&#22312;&#36829;&#27861;&#34892;&#20026;&#24182;&#25490;&#38500;&#34394;&#20551;&#36829;&#27861;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35831;&#27169;&#25311;&#38506;&#23457;&#21592;&#35780;&#20272;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#12290;GPT-3.5&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#36739;&#24369;&#65292;&#20294;&#25105;&#20204;&#39044;&#26399;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#24314;&#35758;&#30340;&#36829;&#27861;&#34892;&#20026;&#24448;&#24448;&#26159;&#27491;&#30830;&#30340;&#65288;&#23427;&#20165;&#20165;&#36807;&#20110;&#20445;&#23432;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#38754;&#20020;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27835;&#29702;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.05962</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#22522;&#37329;&#20250;&#27169;&#22411;&#31995;&#32479;&#30340;&#21435;&#20013;&#24515;&#21270;&#27835;&#29702;&#65306;&#25506;&#35752;&#21306;&#22359;&#38142;&#22312;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralised Governance for Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI. (arXiv:2308.05962v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#38754;&#20020;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27835;&#29702;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#37329;&#20250;&#27169;&#22411;&#22240;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#21644;&#28508;&#21147;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25285;&#24515;&#22522;&#20110;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#24471;&#21040;&#20102;&#36866;&#24403;&#30340;&#27835;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#21487;&#20449;&#24230;&#65292;&#24182;&#38450;&#27490;&#21487;&#33021;&#23545;&#20154;&#31867;&#12289;&#31038;&#20250;&#21644;&#29615;&#22659;&#36896;&#25104;&#20260;&#23475;&#30340;&#28389;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22522;&#37329;&#20250;&#27169;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#38754;&#20020;&#30340;&#20843;&#20010;&#27835;&#29702;&#25361;&#25112;&#65292;&#28041;&#21450;&#27835;&#29702;&#30340;&#19977;&#20010;&#22522;&#26412;&#32500;&#24230;&#65306;&#20915;&#31574;&#26435;&#12289;&#28608;&#21169;&#26426;&#21046;&#21644;&#38382;&#36131;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21306;&#22359;&#38142;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#25552;&#20379;&#20998;&#24067;&#24335;&#36134;&#26412;&#26469;&#20419;&#36827;&#21435;&#20013;&#24515;&#21270;&#30340;&#27835;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#22522;&#37329;&#20250;&#27169;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#27835;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are increasingly attracting interest worldwide for their distinguished capabilities and potential to perform a wide variety of tasks. Nevertheless, people are concerned about whether foundation model based AI systems are properly governed to ensure trustworthiness of foundation model based AI systems and to prevent misuse that could harm humans, society and the environment. In this paper, we identify eight governance challenges in the entire lifecycle of foundation model based AI systems regarding the three fundamental dimensions of governance: decision rights, incentives, and accountability. Furthermore, we explore the potential of blockchain as a solution to address the challenges by providing a distributed ledger to facilitate decentralised governance. We present an architecture that demonstrates how blockchain can be leveraged to realise governance in foundation model based AI systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19981;&#21516;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20276;&#38543;&#20914;&#31361;&#39044;&#27979;&#30340;&#35299;&#37322;&#26469;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#12290;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#35774;&#32622;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#20174;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07636</link><description>&lt;p&gt;
&#19981;&#21516;&#35299;&#37322;: &#21033;&#29992;&#20998;&#27495;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance. (arXiv:2307.07636v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07636
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19981;&#21516;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20276;&#38543;&#20914;&#31361;&#39044;&#27979;&#30340;&#35299;&#37322;&#26469;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#12290;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#35774;&#32622;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#20174;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#35299;&#37322;&#24615;&#26159;&#26085;&#30410;&#22797;&#26434;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#19968;&#20010;&#21487;&#21462;&#29305;&#24449;&#65292;&#20294;&#29616;&#20195;&#35299;&#37322;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#30340;&#12290;&#35299;&#37322;&#30340;&#35821;&#20041;&#24182;&#19981;&#24635;&#26159;&#23436;&#20840;&#29702;&#35299;&#30340; - &#35299;&#37322;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;"&#35299;&#37322;"&#19968;&#20010;&#20915;&#31574;&#65292;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21482;&#26159;&#25903;&#25345;&#19968;&#20010;&#20915;&#31574;&#65311;&#25105;&#20204;&#33021;&#21542;&#24110;&#21161;&#20154;&#20204;&#20174;&#20276;&#38543;&#27491;&#30830;&#39044;&#27979;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#27934;&#23519;&#21147;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#20381;&#36182;&#35299;&#37322;&#25152;&#25552;&#20513;&#30340;&#38169;&#35823;&#39044;&#27979;&#65311;&#22312;&#36825;&#20010;&#35282;&#24230;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#35299;&#37322;&#27010;&#24565;: &#20276;&#38543;&#20914;&#31361;&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#35774;&#32622;&#19979;&#19981;&#21516;&#35299;&#37322;&#30340;&#20248;&#21183;&#65292;&#20854;&#20013;&#20855;&#26377;&#30456;&#20284;&#24615;&#33021;&#30340;&#22810;&#20010;&#27169;&#22411;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#19981;&#21516;&#30340;&#35299;&#37322;&#21487;&#20197;&#36890;&#36807;&#35843;&#29992;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#23454;&#29616;&#12290;&#36890;&#36807;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#35299;&#37322;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
While explainability is a desirable characteristic of increasingly complex black-box models, modern explanation methods have been shown to be inconsistent and contradictory. The semantics of explanations is not always fully understood - to what extent do explanations "explain" a decision and to what extent do they merely advocate for a decision? Can we help humans gain insights from explanations accompanying correct predictions and not over-rely on incorrect predictions advocated for by explanations? With this perspective in mind, we introduce the notion of dissenting explanations: conflicting predictions with accompanying explanations. We first explore the advantage of dissenting explanations in the setting of model multiplicity, where multiple models with similar performance may have different predictions. In such cases, providing dissenting explanations could be done by invoking the explanations of disagreeing models. Through a pilot study, we demonstrate that dissenting explanation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#25351;&#20986;&#20102;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#33258;&#20027;&#33021;&#21147;&#12289;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#20197;&#21450;&#20307;&#39564;&#21040;&#27169;&#31946;&#38382;&#39064;&#30340;&#22823;&#19990;&#30028;&#31561;&#29305;&#28857;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#21017;&#19982;&#27492;&#30456;&#21453;&#12290;</title><link>http://arxiv.org/abs/2307.07515</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#31639;&#27861;&#27169;&#20223;&#65306;&#20026;&#20160;&#20040;&#20154;&#24037;&#8220;&#20195;&#29702;&#8221;&#19981;&#26159;&#65288;&#20063;&#19981;&#20250;&#25104;&#20026;&#65289;&#30495;&#27491;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is algorithmic mimicry: why artificial "agents" are not (and won't be) proper agents. (arXiv:2307.07515v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#25351;&#20986;&#20102;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#33258;&#20027;&#33021;&#21147;&#12289;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#20197;&#21450;&#20307;&#39564;&#21040;&#27169;&#31946;&#38382;&#39064;&#30340;&#22823;&#19990;&#30028;&#31561;&#29305;&#28857;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#21017;&#19982;&#27492;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#37325;&#28857;&#25506;&#35752;&#8220;&#20195;&#29702;&#8221;&#27010;&#24565;&#65292;&#26469;&#25506;&#35752;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;&#20316;&#32773;&#25351;&#20986;&#20102;&#19977;&#20010;&#22522;&#26412;&#30340;&#24046;&#24322;&#65306;&#65288;1&#65289;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#30340;&#33258;&#20027;&#33021;&#21147;&#65292;&#33021;&#22815;&#35774;&#23450;&#33258;&#36523;&#30340;&#20869;&#22312;&#30446;&#26631;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#23384;&#22312;&#20110;&#19968;&#20010;&#30001;&#22806;&#37096;&#20195;&#29702;&#25552;&#20379;&#30446;&#26631;&#20989;&#25968;&#30340;&#35745;&#31639;&#29615;&#22659;&#20013;&#12290;&#65288;2&#65289;&#29983;&#29289;&#31995;&#32479;&#26159;&#20855;&#20307;&#20307;&#29616;&#30340;&#65292;&#21363;&#20854;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#65292;&#32780;&#31639;&#27861;&#36816;&#34892;&#22312;&#35745;&#31639;&#32467;&#26500;&#19978;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#23558;&#36719;&#20214;&#19982;&#30828;&#20214;&#38548;&#31163;&#12290;&#65288;3&#65289;&#29983;&#29289;&#31995;&#32479;&#20307;&#39564;&#21040;&#19968;&#20010;&#24222;&#22823;&#30340;&#19990;&#30028;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38382;&#39064;&#26159;&#27169;&#31946;&#30340;&#65288;&#24182;&#38750;&#20840;&#37096;&#21487;&#23450;&#20041;&#65289;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#23384;&#22312;&#20110;&#19968;&#20010;&#23567;&#19990;&#30028;&#20013;&#65292;&#20854;&#20013;&#25152;&#26377;&#38382;&#39064;&#37117;&#26159;&#26126;&#30830;&#30340;&#12290;&#36825;&#19977;&#20010;&#24046;&#24322;&#35828;&#26126;&#20102;&#29983;&#29289;&#21644;&#31639;&#27861;&#31995;&#32479;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the prospect of developing artificial general intelligence (AGI)? I investigate this question by systematically comparing living and algorithmic systems, with a special focus on the notion of "agency." There are three fundamental differences to consider: (1) Living systems are autopoietic, that is, self-manufacturing, and therefore able to set their own intrinsic goals, while algorithms exist in a computational environment with target functions that are both provided by an external agent. (2) Living systems are embodied in the sense that there is no separation between their symbolic and physical aspects, while algorithms run on computational architectures that maximally isolate software from hardware. (3) Living systems experience a large world, in which most problems are ill-defined (and not all definable), while algorithms exist in a small world, in which all problems are well-defined. These three differences imply that living and algorithmic systems have very different capab
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09222</link><description>&lt;p&gt;
&#38543;&#26426;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09222
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;f-&#25955;&#24230;&#30340;&#21551;&#21457;&#65292;&#24050;&#30693;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#25913;&#36827;&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21152;&#26435;&#26041;&#26696;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#32467;&#21512;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;DomainBed&#21644;Tabular&#20998;&#31867;&#22522;&#20934;&#19978;&#20998;&#21035;&#27604;&#29616;&#26377;&#26368;&#20339;&#32467;&#26524;&#25552;&#21319;&#20102;0.7%&#21644;1.44%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;BERT&#22312;GLUE&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.94%&#65292;&#23558;ViT&#22312;ImageNet-1K&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.01%&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#39044;&#31034;&#30528;&#23427;&#22312;&#25913;&#21892;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#26410;&#30693;&#32422;&#26463;&#20197;&#21450;&#26597;&#35810;&#25298;&#32477;&#38382;&#39064;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05843</link><description>&lt;p&gt;
&#26080;&#39046;&#22495;&#20559;&#35265;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#31215;&#20998;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Batch Bayesian Optimization with Diverse Constraints via Bayesian Quadrature. (arXiv:2306.05843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#26410;&#30693;&#32422;&#26463;&#20197;&#21450;&#26597;&#35810;&#25298;&#32477;&#38382;&#39064;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#20855;&#26377;&#22810;&#26679;&#30340;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#31561;&#29305;&#28857;&#12290;&#21516;&#26102;&#65292;&#24403;&#23384;&#22312;&#26410;&#30693;&#32422;&#26463;&#26102;&#65292;&#20363;&#22914;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#21160;&#29289;&#23454;&#39564;&#23433;&#20840;&#24615;&#31561;&#39046;&#22495;&#65292;&#24517;&#39035;&#30830;&#31435;&#26410;&#30693;&#32422;&#26463;&#20043;&#21518;&#25165;&#33021;&#26597;&#35810;&#30446;&#26631;&#20989;&#25968;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20165;&#38024;&#23545;&#19978;&#36848;&#26576;&#20123;&#29305;&#24449;&#32780;&#24182;&#38750;&#32508;&#21512;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22522;&#20110;SOBER&#31639;&#27861;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#35880;&#24910;&#24182;&#34892;&#20027;&#21160;&#37319;&#26679;&#22120;&#65292;&#32771;&#34385;&#21040;&#20102;&#26410;&#30693;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#38598;&#25104;&#35823;&#24046;&#30340;&#24433;&#21709;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#26041;&#27861;&#65292;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#26465;&#20214;&#21644;&#26410;&#30693;&#32422;&#26463;&#26597;&#35810;&#25298;&#32477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world optimisation problems often feature complex combinations of (1) diverse constraints, (2) discrete and mixed spaces, and are (3) highly parallelisable. (4) There are also cases where the objective function cannot be queried if unknown constraints are not satisfied, e.g. in drug discovery, safety on animal experiments (unknown constraints) must be established before human clinical trials (querying objective function) may proceed. However, most existing works target each of the above three problems in isolation and do not consider (4) unknown constraints with query rejection. For problems with diverse constraints and/or unconventional input spaces, it is difficult to apply these techniques as they are often mutually incompatible. We propose cSOBER, a domain-agnostic prudent parallel active sampler for Bayesian optimisation, based on SOBER of Adachi et al. (2023). We consider infeasibility under unknown constraints as a type of integration error that we can estimate. We propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03552</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#21160;&#24577;&#20559;&#31227;&#30340;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#21463;&#21040;&#21160;&#24577;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#30340;&#29615;&#22659;&#21160;&#24577;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#29615;&#22659;&#21442;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26681;&#25454;&#20854;&#29615;&#22659;&#21442;&#25968;&#23558;&#24102;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20998;&#24320;&#20197;&#35757;&#32451;&#30456;&#24212;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#8220;&#29305;&#23450;&#22330;&#26223;&#8221;&#20351;&#29992;&#30340;&#65292;&#38024;&#23545;&#26576;&#20010;&#29615;&#22659;&#35757;&#32451;&#30340;&#31574;&#30053;&#19981;&#33021;&#20174;&#25910;&#38598;&#22312;&#20854;&#20182;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#25152;&#26377;&#20854;&#20182;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#21644;&#19981;&#21516;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#65292;&#24182;&#20174;&#20855;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#24577;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#37325;&#29992;&#12290;&#36825;&#31181;&#20998;&#24067;&#29992;&#20110;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23548;&#33268;&#20102; SRPO&#65288;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;&#65289;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SRPO &#22312;&#20855;&#26377;&#21160;&#24577;&#20559;&#31227;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01938</link><description>&lt;p&gt;
Doc2SoarGraph&#65306;&#22522;&#20110;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#30340;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26723;&#30340;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20004;&#24180;&#26469;&#65292;&#23545;&#20110;&#34920;&#26684;&#25991;&#26412;&#25991;&#26723;&#65288;&#20363;&#22914;&#36130;&#21153;&#25253;&#21578;&#65289;&#30340;&#31163;&#25955;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22823;&#22810;&#36890;&#36807;&#25163;&#21160;&#36873;&#25321;&#21644;&#36716;&#25442;&#25991;&#26723;&#39029;&#38754;&#21040;&#32467;&#26500;&#21270;&#30340;&#34920;&#26684;&#21644;&#27573;&#33853;&#26469;&#31616;&#21270;&#36825;&#19968;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#19968;&#31181;&#26356;&#20026;&#29616;&#23454;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#20197; TAT-DQA &#30340;&#24418;&#24335;&#22238;&#31572;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Doc2SoarGraph &#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#20854;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545; TAT-DQA &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#31934;&#30830;&#21305;&#37197;&#65288;EM&#65289;&#21644; F1 &#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#27604;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;&#20102; 17.73% &#21644; 16.91%&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete reasoning over table-text documents (e.g., financial reports) gains increasing attention in recent two years. Existing works mostly simplify this challenge by manually selecting and transforming document pages to structured tables and paragraphs, hindering their practical application. In this work, we explore a more realistic problem setting in the form of TAT-DQA, i.e. to answer the question over a visually-rich table-text document. Specifically, we propose a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements (e.g., quantities, dates) of the given question and document with Semantic-oriented hierarchical Graph structures. We conduct extensive experiments on TAT-DQA dataset, and the results show that our proposed framework outperforms the best baseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score respectively on the test set, achieving the new state-of-the-art
&lt;/p&gt;</description></item><item><title>&#32858;&#21512;&#21464;&#37327;&#19978;&#30340;&#22240;&#26524;&#24615;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#20351;&#24471;&#21407;&#26412;&#19981;&#28151;&#28102;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#28151;&#28102;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#25509;&#21463;&#23439;&#35266;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#21482;&#19982;&#24494;&#35266;&#29366;&#24577;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2304.11625</link><description>&lt;p&gt;
&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#32858;&#21512;&#21644;&#24726;&#35770;&#24615;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
Meaningful Causal Aggregation and Paradoxical Confounding. (arXiv:2304.11625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11625
&lt;/p&gt;
&lt;p&gt;
&#32858;&#21512;&#21464;&#37327;&#19978;&#30340;&#22240;&#26524;&#24615;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#20351;&#24471;&#21407;&#26412;&#19981;&#28151;&#28102;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#28151;&#28102;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#25509;&#21463;&#23439;&#35266;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#21482;&#19982;&#24494;&#35266;&#29366;&#24577;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#21512;&#21464;&#37327;&#20013;&#65292;&#24178;&#39044;&#30340;&#24433;&#21709;&#36890;&#24120;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#22240;&#20026;&#30456;&#21516;&#30340;&#23439;&#35266;&#24178;&#39044;&#30340;&#19981;&#21516;&#24494;&#35266;&#23454;&#29616;&#21487;&#33021;&#20250;&#23548;&#33268;&#19979;&#28216;&#23439;&#35266;&#21464;&#37327;&#30340;&#19981;&#21516;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#20110;&#32858;&#21512;&#21464;&#37327;&#65292;&#22240;&#26524;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#20351;&#24471;&#21407;&#26412;&#19981;&#28151;&#28102;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#28151;&#28102;&#65292;&#24182;&#19988;&#21453;&#20043;&#20134;&#28982;&#65292;&#36825;&#19968;&#28857;&#21462;&#20915;&#20110;&#30456;&#24212;&#30340;&#24494;&#35266;&#23454;&#29616;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#22312;&#32858;&#21512;&#22240;&#26524;&#31995;&#32479;&#27809;&#26377;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25165;&#21487;&#20197;&#23454;&#38469;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;&#21542;&#21017;&#65292;&#25105;&#20204;&#38656;&#35201;&#25509;&#21463;&#19968;&#28857;&#65292;&#23601;&#26159;&#23439;&#35266;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#21482;&#19982;&#24494;&#35266;&#29366;&#24577;&#30456;&#20851;&#12290;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#23439;&#35266;&#24178;&#39044;&#30340;&#20998;&#24067;&#19982;&#35266;&#27979;&#20998;&#24067;&#20013;&#24494;&#35266;&#29366;&#24577;&#30340;&#20998;&#24067;&#30456;&#21516;&#26102;&#65292;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#35752;&#35770;&#20102;&#27492;&#35266;&#23519;&#30340;&#27010;&#25324;&#12290;
&lt;/p&gt;
&lt;p&gt;
In aggregated variables the impact of interventions is typically ill-defined because different micro-realizations of the same macro-intervention can result in different changes of downstream macro-variables. We show that this ill-definedness of causality on aggregated variables can turn unconfounded causal relations into confounded ones and vice versa, depending on the respective micro-realization. We argue that it is practically infeasible to only use aggregated causal systems when we are free from this ill-definedness. Instead, we need to accept that macro causal relations are typically defined only with reference to the micro states. On the positive side, we show that cause-effect relations can be aggregated when the macro interventions are such that the distribution of micro states is the same as in the observational distribution and also discuss generalizations of this observation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16133</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#35299;&#20915;&#32479;&#19968;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#20219;&#21153;&#19981;&#19968;&#33268;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#29992;&#30340;&#35270;&#35273;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#25928;&#65292;&#20445;&#35777;&#23427;&#20204;&#22312;&#21508;&#33258;&#25903;&#25345;&#30340;&#20219;&#21153;&#20013;&#30340;&#19968;&#33268;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#35748;&#20026;&#19981;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#36825;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#36755;&#20986;&#30340;&#22823;&#22411;&#31995;&#32479;&#26469;&#35828;&#26159;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#39044;&#27979;&#32467;&#26524;&#26159;&#21542;&#19968;&#33268;&#65292;&#22240;&#27492;&#65292;&#35780;&#20272;&#21487;&#33021;&#21253;&#25324;&#19981;&#21516;&#27169;&#24577;&#36755;&#20986;&#30340;&#38750;&#24120;&#24322;&#26500;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#23567;&#22411;&#20294;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#20462;&#25913;&#26469;&#21019;&#24314;&#23545;&#27604;&#38598;&#65292;&#20197;&#26356;&#25913;&#37329;&#26631;&#31614;&#65292;&#24182;&#27010;&#36848;&#20102;&#29992;&#20110;&#36890;&#36807;&#23545;&#27604;&#25509;&#36817;&#21407;&#22987;&#21644;&#20462;&#25913;&#21518;&#30340;&#23454;&#20363;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26292;&#21147;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#23618;&#22240;&#26524;&#27169;&#22411;&#21644;&#20302;&#23618;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#24182;&#19988;DAS&#21487;&#20197;&#21457;&#29616;&#20808;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;DAS&#31639;&#27861;&#26377;&#28508;&#21147;&#23454;&#29616;&#23545;&#22797;&#26434;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#26356;&#22909;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.02536</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#21464;&#37327;&#21644;&#20998;&#24067;&#24335;&#31070;&#32463;&#34920;&#31034;&#20043;&#38388;&#23547;&#25214;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations. (arXiv:2303.02536v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26292;&#21147;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#23618;&#22240;&#26524;&#27169;&#22411;&#21644;&#20302;&#23618;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#24182;&#19988;DAS&#21487;&#20197;&#21457;&#29616;&#20808;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;DAS&#31639;&#27861;&#26377;&#28508;&#21147;&#23454;&#29616;&#23545;&#22797;&#26434;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#26356;&#22909;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25277;&#35937;&#26159;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23427;&#23450;&#20041;&#20102;&#21487;&#35299;&#37322;&#30340;&#39640;&#23618;&#22240;&#26524;&#27169;&#22411;&#20309;&#26102;&#26159;&#20302;&#23618;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#20449;&#31616;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#23427;&#20204;&#38656;&#35201;&#22312;&#39640;&#23618;&#27169;&#22411;&#21644;&#20302;&#23618;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#26292;&#21147;&#25628;&#32034;&#23545;&#40784;&#65292;&#24182;&#19988;&#23427;&#20204;&#39044;&#35774;&#39640;&#23618;&#27169;&#22411;&#20013;&#30340;&#21464;&#37327;&#23558;&#19982;&#20302;&#23618;&#27169;&#22411;&#20013;&#30340;&#19981;&#30456;&#20132;&#30340;&#31070;&#32463;&#20803;&#38598;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#65292;&#23427;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;DAS&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#25214;&#21040;&#39640;&#23618;&#27169;&#22411;&#21644;&#20302;&#23618;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#20801;&#35768;&#20010;&#20307;&#31070;&#32463;&#20803;&#22312;&#38750;&#20256;&#32479;&#22522;&#24213;&#20998;&#24067;&#34920;&#31034;&#20013;&#21457;&#25381;&#22810;&#20010;&#19981;&#21516;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DAS&#21487;&#20197;&#21457;&#29616;&#20808;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;DAS&#26377;&#28508;&#21147;&#23454;&#29616;&#23545;&#22797;&#26434;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#26356;&#22909;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;LHC&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#32463;&#20856;&#31639;&#27861;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2302.12906</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Invertible Quantum Neural Networks. (arXiv:2302.12906v2 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;LHC&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#32463;&#20856;&#31639;&#27861;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#27169;&#25311;&#21644;&#29983;&#25104;&#39640;&#24230;&#22797;&#26434;&#25968;&#25454;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#38376;&#31639;&#27861;&#29992;&#20110;&#37327;&#23376;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;QINN&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#34928;&#21464;&#20026;&#36731;&#23376;&#30340;Z&#29627;&#33394;&#23376;&#30340;&#21943;&#27880;&#30456;&#20851;&#20135;&#29983;&#30340;LHC&#25968;&#25454;&#65292;&#36825;&#26159;&#31890;&#23376;&#23545;&#25758;&#26426;&#31934;&#23494;&#27979;&#37327;&#30340;&#26631;&#20934;&#36807;&#31243;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;QINN&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#28151;&#21512;&#30340;QINN&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#19968;&#20010;&#26174;&#33879;&#26356;&#22823;&#30340;&#23436;&#20840;&#32463;&#20856;&#30340;INN&#30340;&#34920;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invertible Neural Networks (INN) have become established tools for the simulation and generation of highly complex data. We propose a quantum-gate algorithm for a Quantum Invertible Neural Network (QINN) and apply it to the LHC data of jet-associated production of a Z-boson that decays into leptons, a standard candle process for particle collider precision measurements. We compare the QINN's performance for different loss functions and training scenarios. For this task, we find that a hybrid QINN matches the performance of a significantly larger purely classical INN in learning and generating complex data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#26469;&#24110;&#21161;&#26816;&#27979;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00131</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65306;&#19968;&#20010;&#20197;&#22240;&#26524;&#20851;&#31995;&#20026;&#22522;&#30784;&#30340;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution. (arXiv:2210.00131v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#26469;&#24110;&#21161;&#26816;&#27979;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#24120;&#24120;&#23384;&#22312;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#38382;&#39064;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#26631;&#35760;&#39044;&#27979;&#65292;&#22312;&#25512;&#26029;&#26102;&#21487;&#33021;&#26377;&#22810;&#20010;&#21333;&#35789;&#31526;&#21512;&#29992;&#25143;&#20135;&#29983;&#33258;&#28982;&#35821;&#35328;&#30340;&#24847;&#22270;&#65292;&#28982;&#32780;&#22312;&#35757;&#32451;&#26102;&#21482;&#26377;&#19968;&#20010;&#21333;&#35789;&#33021;&#22815;&#26368;&#23567;&#21270;&#20219;&#21153;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#21512;&#29702;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#25551;&#36848;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#22312;&#29983;&#25104;&#34394;&#20551;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#20854;&#31616;&#27905;&#24615;&#65292;&#25105;&#20204;&#30340;&#22240;&#26524;&#27169;&#22411;&#30452;&#25509;&#25351;&#23548;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#20013;&#30340;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#19978;&#65292;&#20197;&#24110;&#21161; 1) &#26816;&#27979;&#25512;&#26029;&#26102;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#21033;&#29992;&#20102; 2&#65289;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#28085;&#30422;&#20102; A&#65289;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;BERT-base&#21040;GPT 3.5&#65292;B&#65289;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20174;&#36974;&#34109;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#21040;&#36825;&#20123;&#30446;&#26631;&#30340;&#28151;&#21512;&#65292;&#20197;&#21450;C&#65289;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20174;&#20165;&#39044;&#35757;&#32451;&#21040;&#22686;&#24378;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language modeling tasks are often underspecified: for a given token prediction, many words may satisfy the user's intent of producing natural language at inference time, however only one word would minimize the task's loss function at training time. We provide a simple yet plausible causal mechanism describing the role underspecification plays in the generation of spurious correlations. Despite its simplicity, our causal model directly informs the development of two lightweight black-box evaluation methods, that we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid in the detection of inference-time task underspecification by exploiting 2) previously unreported gender vs. time and gender vs. location spurious correlations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B) pre-training objectives: from masked &amp; autoregressive language modeling to a mixture of these objectives, and C) training stages: from pre-training only to reinforcement l
&lt;/p&gt;</description></item></channel></rss>