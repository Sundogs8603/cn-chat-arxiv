<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35770;&#12289;&#24212;&#29992;&#21644;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#35814;&#32454;&#21644;&#30495;&#23454;&#30340;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15703</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#20174;&#29702;&#35770;&#21040;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Natural Language Generation: From Theory to Applications. (arXiv:2307.15703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35770;&#12289;&#24212;&#29992;&#21644;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#35814;&#32454;&#21644;&#30495;&#23454;&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#23853;&#38706;&#22836;&#35282;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#25191;&#34892;&#20256;&#32479;&#20219;&#21153;&#22914;&#25688;&#35201;&#25110;&#32763;&#35793;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;NLG&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#22312;&#21487;&#33021;&#20986;&#38169;&#26102;&#25351;&#31034;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#35266;&#28857;&#12289;&#32972;&#26223;&#21644;&#20889;&#20316;&#39118;&#26684; - &#21453;&#26144;&#22810;&#20803;&#21270;&#20154;&#31867;&#20122;&#32676;&#20307;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#22788;&#29702;&#33021;&#22815;&#24110;&#21161;&#21019;&#24314;&#19982;&#36825;&#20123;&#30446;&#26631;&#26356;&#22909;&#22320;&#23545;&#40784;&#30340;&#31995;&#32479;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#25152;&#38656;&#30340;&#22522;&#26412;&#29702;&#35770;&#12289;&#26694;&#26550;&#21644;&#35789;&#27719;&#12290;&#28982;&#21518;&#20174;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#25551;&#36848;NLG&#20013;&#30340;&#20027;&#35201;&#19981;&#30830;&#23450;&#24615;&#28304;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#27604;&#27969;&#34892;&#30340;&#38543;&#26426;&#24615;/&#35748;&#35782;&#24615;&#20108;&#20998;&#27861;&#26356;&#35814;&#32454;&#21644;&#30495;&#23454;&#30340;&#20108;&#32500;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances of powerful Language Models have allowed Natural Language Generation (NLG) to emerge as an important technology that can not only perform traditional tasks like summarisation or translation, but also serve as a natural language interface to a variety of applications. As such, it is crucial that NLG systems are trustworthy and reliable, for example by indicating when they are likely to be wrong; and supporting multiple views, backgrounds and writing styles -- reflecting diverse human sub-populations. In this paper, we argue that a principled treatment of uncertainty can assist in creating systems and evaluation protocols better aligned with these goals. We first present the fundamental theory, frameworks and vocabulary required to represent uncertainty. We then characterise the main sources of uncertainty in NLG from a linguistic perspective, and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Final
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#24335;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26469;&#20248;&#21270;&#33258;&#28982;&#28798;&#23475;&#20013;&#27773;&#36710;&#32039;&#24613;&#30095;&#25955;&#35745;&#21010;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#27169;&#25311;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15682</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24212;&#24613;&#36867;&#29983;&#36335;&#24452;&#38382;&#39064;&#30340;&#30417;&#30563;&#24335;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A supervised hybrid quantum machine learning solution to the emergency escape routing problem. (arXiv:2307.15682v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#24335;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26469;&#20248;&#21270;&#33258;&#28982;&#28798;&#23475;&#20013;&#27773;&#36710;&#32039;&#24613;&#30095;&#25955;&#35745;&#21010;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#27169;&#25311;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#31649;&#29702;&#23545;&#33258;&#28982;&#28798;&#23475;&#30340;&#21709;&#24212;&#21487;&#20197;&#22823;&#22823;&#20943;&#36731;&#20854;&#30772;&#22351;&#24615;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#30417;&#30563;&#24335;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26469;&#20248;&#21270;&#33258;&#28982;&#28798;&#23475;&#20013;&#27773;&#36710;&#32039;&#24613;&#30095;&#25955;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#22320;&#38663;&#32039;&#24613;&#24773;&#20917;&#65292;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#21160;&#24577;&#35745;&#31639;&#22270;&#65292;&#22320;&#38663;&#30772;&#22351;&#22478;&#24066;&#30340;&#19968;&#37096;&#20998;&#12290;&#23621;&#27665;&#35797;&#22270;&#36890;&#36807;&#21040;&#36798;&#20132;&#36890;&#25317;&#22581;&#21457;&#29983;&#30340;&#20986;&#21475;&#28857;&#26469;&#25764;&#31163;&#22478;&#24066;&#12290;&#35813;&#24773;&#20917;&#34987;&#24314;&#27169;&#20026;&#22312;&#19981;&#30830;&#23450;&#21644;&#21160;&#24577;&#28436;&#21270;&#30340;&#22320;&#22270;&#19978;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#20855;&#20307;&#22478;&#24066;&#22270;&#19978;&#30340;&#20551;&#35774;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#21151;&#33021;&#32447;&#24615;&#35843;&#21046;(FiLM)&#31070;&#32463;&#32593;&#32476;&#65292;&#19982;&#19968;&#20010;&#32463;&#20856;&#30340;FiLM&#32593;&#32476;&#24182;&#34892;&#65292;&#20197;&#27169;&#20223;&#30830;&#23450;&#24615;&#21160;&#24577;&#22270;&#19978;&#30340;Dijkstra&#33410;&#28857;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12290;&#24182;&#34892;&#28155;&#21152;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Managing the response to natural disasters effectively can considerably mitigate their devastating impact. This work explores the potential of using supervised hybrid quantum machine learning to optimize emergency evacuation plans for cars during natural disasters. The study focuses on earthquake emergencies and models the problem as a dynamic computational graph where an earthquake damages an area of a city. The residents seek to evacuate the city by reaching the exit points where traffic congestion occurs. The situation is modeled as a shortest-path problem on an uncertain and dynamically evolving map. We propose a novel hybrid supervised learning approach and test it on hypothetical situations on a concrete city graph. This approach uses a novel quantum feature-wise linear modulation (FiLM) neural network parallel to a classical FiLM network to imitate Dijkstra's node-wise shortest path algorithm on a deterministic dynamic graph. Adding the quantum neural network in parallel increas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23558;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;IT&#30417;&#25511;&#25968;&#25454;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.15678</link><description>&lt;p&gt;
IT&#30417;&#25511;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#25512;&#26029;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Case Studies of Causal Discovery from IT Monitoring Time Series. (arXiv:2307.15678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23558;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;IT&#30417;&#25511;&#25968;&#25454;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25216;&#26415;&#65288;IT&#65289;&#31995;&#32479;&#23545;&#29616;&#20195;&#20225;&#19994;&#33267;&#20851;&#37325;&#35201;&#65292;&#22788;&#29702;&#25968;&#25454;&#23384;&#20648;&#12289;&#36890;&#20449;&#21644;&#27969;&#31243;&#33258;&#21160;&#21270;&#12290;&#30417;&#25511;&#36825;&#20123;&#31995;&#32479;&#23545;&#20110;&#20854;&#27491;&#24120;&#36816;&#34892;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#25910;&#38598;&#35814;&#32454;&#30340;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;&#38543;&#30528;IT&#30417;&#25511;&#31995;&#32479;&#20013;&#23545;&#22240;&#26524;&#25512;&#26029;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20102;&#35299;IT&#31995;&#32479;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26377;&#21161;&#20110;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#24322;&#24120;&#21644;&#20107;&#25925;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#23427;&#36824;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#20998;&#26512;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#21040;IT&#30417;&#25511;&#25968;&#25454;&#19978;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20363;&#22914;&#65292;IT&#30417;&#25511;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#19981;&#23545;&#40784;&#30340;&#26102;&#38388;&#24207;&#21015;&#12289;&#20241;&#30496;&#26102;&#38388;&#24207;&#21015;&#12289;&#26102;&#38388;&#25139;&#38169;&#35823;&#21644;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#21040;&#19981;&#21516;IT&#30417;&#25511;&#25968;&#25454;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information technology (IT) systems are vital for modern businesses, handling data storage, communication, and process automation. Monitoring these systems is crucial for their proper functioning and efficiency, as it allows collecting extensive observational time series data for analysis. The interest in causal discovery is growing in IT monitoring systems as knowing causal relations between different components of the IT system helps in reducing downtime, enhancing system performance and identifying root causes of anomalies and incidents. It also allows proactive prediction of future issues through historical data analysis. Despite its potential benefits, applying causal discovery algorithms on IT monitoring data poses challenges, due to the complexity of the data. For instance, IT monitoring data often contains misaligned time series, sleeping time series, timestamp errors and missing values. This paper presents case studies on applying causal discovery algorithms to different IT mo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;</title><link>http://arxiv.org/abs/2307.15644</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25968;&#25454;&#29983;&#25104;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#35270;&#35273;&#23548;&#33322;&#30740;&#31350;&#20013;&#65292;&#23545;&#20110;&#36941;&#21382;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#21487;&#27867;&#21270;&#20195;&#29702;&#30340;&#30417;&#30563;&#25968;&#37327;&#26377;&#20102;&#26126;&#26174;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;HM3D&#21644;Gibson&#25968;&#25454;&#38598;&#20013;&#30340;1200&#22810;&#20010;&#36924;&#30495;&#30340;&#29615;&#22659;&#65292;&#24182;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#36164;&#28304;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#33539;&#24335;&#20013;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#24688;&#24403;&#22320;&#24212;&#29992;&#25193;&#22686;&#25968;&#25454;&#26469;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20195;&#29702;&#12290;&#24471;&#30410;&#20110;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#29616;&#26377;&#20195;&#29702;&#30340;&#24615;&#33021;&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#65288;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#32477;&#23545;&#20540;&#22686;&#21152;&#20102;11%&#65289;&#65292;&#22312;R2R&#27979;&#35797;&#38598;&#20013;&#21333;&#27425;&#36816;&#34892;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#20154;&#20010;&#24615;&#19982;&#20010;&#20307;&#20154;&#31867;&#29305;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22806;&#21521;&#20010;&#24615;&#30340;&#26426;&#22120;&#20154;&#22312;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#21654;&#21857;&#24072;&#30340;&#20449;&#20219;&#20013;&#26356;&#21463;&#27426;&#36814;&#21644;&#34987;&#20449;&#20219;&#65292;&#21516;&#26102;&#21457;&#29616;&#20010;&#20307;&#23545;&#26426;&#22120;&#20154;&#30340;&#24577;&#24230;&#21644;&#20559;&#22909;&#23545;&#20449;&#20219;&#20063;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.15568</link><description>&lt;p&gt;
&#25105;&#20204;&#37117;&#26159;&#20010;&#20307;&#65306;&#26426;&#22120;&#20154;&#20010;&#24615;&#21644;&#20154;&#31867;&#29305;&#36136;&#22312;&#21487;&#20449;&#20114;&#21160;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
We are all Individuals: The Role of Robot Personality and Human Traits in Trustworthy Interaction. (arXiv:2307.15568v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#20154;&#20010;&#24615;&#19982;&#20010;&#20307;&#20154;&#31867;&#29305;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22806;&#21521;&#20010;&#24615;&#30340;&#26426;&#22120;&#20154;&#22312;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#21654;&#21857;&#24072;&#30340;&#20449;&#20219;&#20013;&#26356;&#21463;&#27426;&#36814;&#21644;&#34987;&#20449;&#20219;&#65292;&#21516;&#26102;&#21457;&#29616;&#20010;&#20307;&#23545;&#26426;&#22120;&#20154;&#30340;&#24577;&#24230;&#21644;&#20559;&#22909;&#23545;&#20449;&#20219;&#20063;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#20154;&#22312;&#25105;&#20204;&#31038;&#20250;&#20013;&#25198;&#28436;&#35282;&#33394;&#65292;&#37325;&#35201;&#30340;&#26159;&#23427;&#20204;&#30340;&#22806;&#35266;&#12289;&#34892;&#20026;&#21644;&#20010;&#24615;&#26159;&#21542;&#36866;&#21512;&#20854;&#24037;&#20316;&#65292;&#24182;&#19988;&#26159;&#21542;&#21463;&#21040;&#19982;&#20854;&#20114;&#21160;&#30340;&#20154;&#20204;&#30340;&#21916;&#29233;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#20154;&#20010;&#24615;&#19982;&#20010;&#20307;&#20154;&#31867;&#29305;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#22768;&#38899;&#25552;&#31034;&#21644;&#35821;&#35328;&#29305;&#24449;&#26469;&#20934;&#30830;&#25551;&#32472;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#65292;&#21363;&#22806;&#21521;&#24615;&#21644;&#20869;&#21521;&#24615;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#33719;&#21462;&#23545;&#36825;&#20123;&#19981;&#21516;&#26426;&#22120;&#20154;&#20010;&#24615;&#30340;&#20559;&#22909;&#21644;&#20449;&#20219;&#35780;&#32423;&#65292;&#25105;&#20204;&#30830;&#23450;&#23545;&#20110;&#19968;&#21517;&#26426;&#22120;&#20154;&#21654;&#21857;&#24072;&#32780;&#35328;&#65292;&#22806;&#21521;&#20010;&#24615;&#30340;&#26426;&#22120;&#20154;&#27604;&#20869;&#21521;&#20010;&#24615;&#30340;&#26426;&#22120;&#20154;&#26356;&#21463;&#27426;&#36814;&#21644;&#34987;&#20449;&#20219;&#65292;&#19981;&#35770;&#21463;&#35797;&#32773;&#33258;&#36523;&#30340;&#20010;&#24615;&#22914;&#20309;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#21457;&#29616;&#20010;&#20307;&#23545;&#26426;&#22120;&#20154;&#30340;&#24577;&#24230;&#21644;&#20559;&#22909;&#30830;&#23454;&#20250;&#24433;&#21709;&#23545;&#26426;&#22120;&#20154;&#21654;&#21857;&#24072;&#30340;&#20449;&#20219;&#65292;&#22240;&#27492;&#38500;&#20102;&#26426;&#22120;&#20154;&#20010;&#24615;&#12289;&#35282;&#33394;&#21644;&#20114;&#21160;&#20043;&#22806;&#65292;&#23427;&#20204;&#20063;&#26159;&#37325;&#35201;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
As robots take on roles in our society, it is important that their appearance, behaviour and personality are appropriate for the job they are given and are perceived favourably by the people with whom they interact. Here, we provide an extensive quantitative and qualitative study exploring robot personality but, importantly, with respect to individual human traits. Firstly, we show that we can accurately portray personality in a social robot, in terms of extroversion-introversion using vocal cues and linguistic features. Secondly, through garnering preferences and trust ratings for these different robot personalities, we establish that, for a Robo-Barista, an extrovert robot is preferred and trusted more than an introvert robot, regardless of the subject's own personality. Thirdly, we find that individual attitudes and predispositions towards robots do impact trust in the Robo-Baristas, and are therefore important considerations in addition to robot personality, roles and interaction c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#23567;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#36880;&#27493;&#26631;&#35760;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#20027;&#24178;&#32593;&#32476;&#25552;&#21462;&#30340;&#29305;&#24449;&#26500;&#24314;&#22240;&#23376;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#30693;&#35782;&#36801;&#31227;&#21644;&#20998;&#31867;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15524</link><description>&lt;p&gt;
&#22522;&#20110;&#28176;&#36827;&#26426;&#22120;&#23398;&#20064;&#30340;&#23567;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-shot Image Classification based on Gradual Machine Learning. (arXiv:2307.15524v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#23567;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#36880;&#27493;&#26631;&#35760;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#20027;&#24178;&#32593;&#32476;&#25552;&#21462;&#30340;&#29305;&#24449;&#26500;&#24314;&#22240;&#23376;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#30693;&#35782;&#36801;&#31227;&#21644;&#20998;&#31867;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26088;&#22312;&#20165;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#26679;&#26412;&#20934;&#30830;&#22320;&#23545;&#26410;&#26631;&#35760;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#20027;&#35201;&#20851;&#27880;&#35774;&#35745;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#20027;&#24178;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35757;&#32451;&#31867;&#21035;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#38590;&#20197;&#36801;&#31227;&#21040;&#26032;&#31867;&#21035;&#65292;&#35813;&#20219;&#21153;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#26426;&#22120;&#23398;&#20064;&#65288;GML&#65289;&#38750;i.i.d&#33539;&#24335;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#20174;&#20165;&#26377;&#23569;&#37327;&#26377;&#26631;&#31614;&#35266;&#27979;&#24320;&#22987;&#65292;&#28982;&#21518;&#36880;&#28176;&#25353;&#29031;&#38590;&#24230;&#22686;&#21152;&#30340;&#39034;&#24207;&#36890;&#36807;&#36845;&#20195;&#22240;&#23376;&#22270;&#20013;&#30340;&#22240;&#23376;&#25512;&#29702;&#26469;&#26631;&#35760;&#30446;&#26631;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#28145;&#24230;&#20027;&#24178;&#32593;&#32476;&#25552;&#21462;&#20855;&#26377;&#25351;&#31034;&#24615;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#28982;&#21518;&#22522;&#20110;&#25552;&#21462;&#30340;&#29305;&#24449;&#26500;&#24314;&#19968;&#20803;&#22240;&#23376;&#21644;&#20108;&#20803;&#22240;&#23376;&#20197;&#20419;&#36827;&#28176;&#36827;&#23398;&#20064;&#12290;&#19968;&#20803;&#22240;&#23376;&#22522;&#20110;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#31867;&#21035;&#20013;&#24515;&#36317;&#31163;&#26500;&#24314;&#65292;&#32780;&#20108;&#20803;&#22240;&#23376;&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Few-shot image classification aims to accurately classify unlabeled images using only a few labeled samples. The state-of-the-art solutions are built by deep learning, which focuses on designing increasingly complex deep backbones. Unfortunately, the task remains very challenging due to the difficulty of transferring the knowledge learned in training classes to new ones. In this paper, we propose a novel approach based on the non-i.i.d paradigm of gradual machine learning (GML). It begins with only a few labeled observations, and then gradually labels target images in the increasing order of hardness by iterative factor inference in a factor graph. Specifically, our proposed solution extracts indicative feature representations by deep backbones, and then constructs both unary and binary factors based on the extracted features to facilitate gradual learning. The unary factors are constructed based on class center distance in an embedding space, while the binary factors are constructed b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20840;&#21367;&#31215;&#20960;&#20309;&#29305;&#24449;&#65288;FCGF&#65289;&#29992;&#20110;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36880;&#28857;&#21028;&#21035;&#29305;&#24449;&#20197;&#21450;&#36866;&#24403;&#30340;&#20462;&#25913;&#21644;&#35757;&#32451;&#31574;&#30053;&#36229;&#36234;&#20102;&#36817;&#26399;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15514</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20840;&#21367;&#31215;&#20960;&#20309;&#29305;&#24449;&#29992;&#20110;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation. (arXiv:2307.15514v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15514
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20840;&#21367;&#31215;&#20960;&#20309;&#29305;&#24449;&#65288;FCGF&#65289;&#29992;&#20110;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36880;&#28857;&#21028;&#21035;&#29305;&#24449;&#20197;&#21450;&#36866;&#24403;&#30340;&#20462;&#25913;&#21644;&#35757;&#32451;&#31574;&#30053;&#36229;&#36234;&#20102;&#36817;&#26399;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#22270;&#20687;&#21644;&#29289;&#20307;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#38190;&#28857;&#23545;&#24212;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;RANSAC&#30340;&#31639;&#27861;&#25110;&#30452;&#25509;&#36890;&#36807;&#31471;&#21040;&#31471;&#20248;&#21270;&#22238;&#24402;&#23039;&#24577;&#26469;&#30830;&#23450;&#29289;&#20307;&#23039;&#24577;&#12290;&#25105;&#20204;&#35748;&#20026;&#25991;&#29486;&#20013;&#24573;&#35270;&#20102;&#23398;&#20064;&#36880;&#28857;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20840;&#21367;&#31215;&#20960;&#20309;&#29305;&#24449;&#65288;FCGF&#65289;&#65292;&#24182;&#23558;&#20854;&#23450;&#21046;&#20026;&#29289;&#20307;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;FCGF&#37319;&#29992;&#31232;&#30095;&#21367;&#31215;&#65292;&#36890;&#36807;&#20248;&#21270;&#26368;&#38590;&#27604;&#36739;&#25439;&#22833;&#26469;&#23398;&#20064;&#36880;&#28857;&#29305;&#24449;&#12290;&#36890;&#36807;&#23545;&#25439;&#22833;&#21644;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#36827;&#34892;&#20851;&#38190;&#20462;&#25913;&#12289;&#31934;&#24515;&#35843;&#25972;&#35757;&#32451;&#31574;&#30053;&#20197;&#21450;&#37319;&#29992;&#36866;&#21512;&#24213;&#23618;&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#36817;&#26399;&#30340;&#31454;&#20105;&#23545;&#25163;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#27599;&#20010;&#20462;&#25913;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works on 6D object pose estimation focus on learning keypoint correspondences between images and object models, and then determine the object pose through RANSAC-based algorithms or by directly regressing the pose with end-to-end optimisations. We argue that learning point-level discriminative features is overlooked in the literature. To this end, we revisit Fully Convolutional Geometric Features (FCGF) and tailor it for object 6D pose estimation to achieve state-of-the-art performance. FCGF employs sparse convolutions and learns point-level features using a fully-convolutional network by optimising a hardest contrastive loss. We can outperform recent competitors on popular benchmarks by adopting key modifications to the loss and to the input data representations, by carefully tuning the training strategies, and by employing data augmentations suitable for the underlying problem. We carry out a thorough ablation to study the contribution of each modification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#65288;UIT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15504</link><description>&lt;p&gt;
&#25506;&#32034;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#65288;UIT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#33021;&#21147;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#20013;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#21487;&#20197;&#25345;&#32493;&#25552;&#21319;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26368;&#36817;&#30340;&#19968;&#39033;&#21162;&#21147;&#65292;&#21363;&#25910;&#38598;&#21508;&#31181;&#25351;&#20196;&#24182;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#25972;&#21512;&#21040;&#26356;&#22823;&#30340;&#38598;&#21512;&#20013;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#29992;&#25143;&#26377;&#20854;&#29420;&#29305;&#30340;&#34920;&#36798;&#25351;&#20196;&#30340;&#26041;&#24335;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#25351;&#20196;&#39118;&#26684;&#21644;&#26684;&#24335;&#30340;&#21464;&#21270;&#65292;&#21363;&#26684;&#24335;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26684;&#24335;&#19981;&#19968;&#33268;&#24615;&#22914;&#20309;&#24433;&#21709;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#8221;&#65288;UIT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#29992;OpenAI&#30340;API&#23454;&#29616;&#22312;&#19981;&#21516;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;UIT&#25104;&#21151;&#25552;&#39640;&#20102;&#22312;&#26410;&#35265;&#25351;&#20196;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we study how format inconsistency may impact the performance of instruction tuning. We propose a framework called "Unified Instruction Tuning" (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets. We show that UIT successfully improves the generalization performance on unseen instructions, which highlights the importance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ETHER&#65292;&#36890;&#36807;&#23545;&#40784;&#32039;&#24613;&#27807;&#36890;&#26469;&#35299;&#20915;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#20013;&#30340;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26550;&#26500;&#20381;&#36182;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15494</link><description>&lt;p&gt;
ETHER: &#23545;&#20110;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#30340;&#32039;&#23494;&#27807;&#36890;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ETHER: Aligning Emergent Communication for Hindsight Experience Replay. (arXiv:2307.15494v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ETHER&#65292;&#36890;&#36807;&#23545;&#40784;&#32039;&#24613;&#27807;&#36890;&#26469;&#35299;&#20915;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#20013;&#30340;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26550;&#26500;&#20381;&#36182;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#36319;&#38543;&#23545;&#20110;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29305;&#24615;&#65292;&#22914;&#32452;&#21512;&#24615;&#65292;&#33021;&#22815;&#25552;&#20379;&#23398;&#20064;&#22797;&#26434;&#31574;&#30053;&#30340;&#24378;&#24402;&#32435;&#20559;&#22909;&#12290;&#20808;&#21069;&#30340;&#26550;&#26500;&#22914;HIGhER&#32467;&#21512;&#20102;&#35821;&#35328;&#26465;&#20214;&#19982;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#65288;HER&#65289;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#19982;HER&#31867;&#20284;&#65292;HIGhER&#20381;&#36182;&#20110;&#19968;&#20010;&#39044;&#35774;&#30340;&#20989;&#25968;&#26469;&#25552;&#20379;&#21453;&#39304;&#20449;&#21495;&#65292;&#25351;&#31034;&#21738;&#31181;&#35821;&#35328;&#25551;&#36848;&#22312;&#21738;&#31181;&#29366;&#24577;&#19979;&#26377;&#25928;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;HIGhER&#21482;&#21033;&#29992;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#36712;&#36857;&#20013;&#21253;&#21547;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20854;&#26368;&#32456;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#27809;&#26377;&#26089;&#26399;&#25104;&#21151;&#36712;&#36857;&#65292;HIGhER&#24182;&#19981;&#27604;&#20854;&#26500;&#24314;&#20110;&#20043;&#19978;&#30340;DQN&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32039;&#23494;&#25991;&#26412;&#22238;&#39038;&#24615;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language instruction following is paramount to enable collaboration between artificial agents and human beings. Natural language-conditioned reinforcement learning (RL) agents have shown how natural languages' properties, such as compositionality, can provide a strong inductive bias to learn complex policies. Previous architectures like HIGhER combine the benefit of language-conditioning with Hindsight Experience Replay (HER) to deal with sparse rewards environments. Yet, like HER, HIGhER relies on an oracle predicate function to provide a feedback signal highlighting which linguistic description is valid for which state. This reliance on an oracle limits its application. Additionally, HIGhER only leverages the linguistic information contained in successful RL trajectories, thus hurting its final performance and data-efficiency. Without early successful trajectories, HIGhER is no better than DQN upon which it is built. In this paper, we propose the Emergent Textual Hindsight Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#35748;&#30693;&#35268;&#21010;&#20013;&#21487;&#20915;&#23450;&#24615;&#38382;&#39064;&#30340;&#35821;&#20041;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#20132;&#20114;&#20844;&#29702;&#26469;&#25511;&#21046;&#20195;&#29702;&#20154;&#23545;&#20854;&#20182;&#20195;&#29702;&#20154;&#30693;&#35782;&#30340;&#26080;&#38480;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#19981;&#26159;&#24378;&#21152;&#21477;&#27861;&#32422;&#26463;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#38382;&#39064;&#30340;&#26377;&#38480;&#38750;&#19981;&#21160;&#28857;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.15485</link><description>&lt;p&gt;
&#29992;&#35821;&#20041;&#26041;&#27861;&#35299;&#20915;&#35748;&#30693;&#35268;&#21010;&#30340;&#21487;&#20915;&#23450;&#24615;&#38382;&#39064;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
A Semantic Approach to Decidability in Epistemic Planning (Extended Version). (arXiv:2307.15485v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#35748;&#30693;&#35268;&#21010;&#20013;&#21487;&#20915;&#23450;&#24615;&#38382;&#39064;&#30340;&#35821;&#20041;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#20132;&#20114;&#20844;&#29702;&#26469;&#25511;&#21046;&#20195;&#29702;&#20154;&#23545;&#20854;&#20182;&#20195;&#29702;&#20154;&#30693;&#35782;&#30340;&#26080;&#38480;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#19981;&#26159;&#24378;&#21152;&#21477;&#27861;&#32422;&#26463;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#38382;&#39064;&#30340;&#26377;&#38480;&#38750;&#19981;&#21160;&#28857;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20013;&#65292;&#20351;&#29992;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#65288;DEL&#65289;&#24050;&#32463;&#23548;&#33268;&#20102;&#19968;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#34892;&#21160;&#24418;&#24335;&#20307;&#31995;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#30830;&#23450;&#24615;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#20219;&#24847;&#30693;&#35782;&#23884;&#22871;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#34920;&#36798;&#33021;&#21147;&#30340;&#25552;&#39640;&#26159;&#20197;&#19981;&#21487;&#20915;&#23450;&#24615;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#27492;&#24050;&#32463;&#25552;&#21462;&#20986;&#20102;&#20960;&#31181;&#21487;&#20915;&#23450;&#30340;&#29255;&#27573;&#65292;&#20027;&#35201;&#22522;&#20110;&#34892;&#21160;&#24418;&#24335;&#20307;&#31995;&#30340;&#21477;&#27861;&#38480;&#21046;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#26041;&#27861;&#26469;&#23454;&#29616;&#21487;&#20915;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#35748;&#30693;&#35268;&#21010;&#30340;&#36923;&#36753;&#20013;&#22686;&#21152;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#30693;&#35782;&#21487;&#20132;&#25442;&#24615;&#8221;&#30340;&#20132;&#20114;&#20844;&#29702;&#65292;&#35813;&#20844;&#29702;&#25511;&#21046;&#20102;&#20195;&#29702;&#20154;&#22312;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#30693;&#35782;&#19978;&#26080;&#38480;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#26159;&#24378;&#21152;&#21477;&#27861;&#32422;&#26463;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24471;&#21040;&#30340;&#35748;&#30693;&#35268;&#21010;&#38382;&#39064;&#26159;&#21487;&#20915;&#23450;&#30340;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#26377;&#38480;&#30340;&#38750;&#19981;&#21160;&#28857;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Dynamic Epistemic Logic (DEL) in multi-agent planning has led to a widely adopted action formalism that can handle nondeterminism, partial observability and arbitrary knowledge nesting. As such expressive power comes at the cost of undecidability, several decidable fragments have been isolated, mainly based on syntactic restrictions of the action formalism. In this paper, we pursue a novel semantic approach to achieve decidability. Namely, rather than imposing syntactical constraints, the semantic approach focuses on the axioms of the logic for epistemic planning. Specifically, we augment the logic of knowledge S5$_n$ and with an interaction axiom called (knowledge) commutativity, which controls the ability of agents to unboundedly reason on the knowledge of other agents. We then provide a threefold contribution. First, we show that the resulting epistemic planning problem is decidable. In doing so, we prove that our framework admits a finitary non-fixpoint characterization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#31227;&#21160;&#35774;&#22791;&#30456;&#26426;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#30456;&#26426;&#22312;&#38750;&#20405;&#20837;&#24615;&#31958;&#23615;&#30149;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;Gabor&#28388;&#27874;&#22120;&#21644;&#38754;&#37096;&#22359;&#32441;&#29702;&#29305;&#24449;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;96.7%&#30340;&#20934;&#30830;&#24615;&#12289;100%&#30340;&#25935;&#24863;&#24615;&#21644;93%&#30340;&#29305;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15480</link><description>&lt;p&gt;
&#20351;&#29992;Gabor&#28388;&#27874;&#22120;&#30340;&#38750;&#20405;&#20837;&#24335;&#31958;&#23615;&#30149;&#26816;&#27979;&#65306;&#23545;&#19981;&#21516;&#30456;&#26426;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Non-invasive Diabetes Detection using Gabor Filter: A Comparative Analysis of Different Cameras. (arXiv:2307.15480v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#31227;&#21160;&#35774;&#22791;&#30456;&#26426;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#30456;&#26426;&#22312;&#38750;&#20405;&#20837;&#24615;&#31958;&#23615;&#30149;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;Gabor&#28388;&#27874;&#22120;&#21644;&#38754;&#37096;&#22359;&#32441;&#29702;&#29305;&#24449;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;96.7%&#30340;&#20934;&#30830;&#24615;&#12289;100%&#30340;&#25935;&#24863;&#24615;&#21644;93%&#30340;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#21644;&#25506;&#32034;&#20102;&#31227;&#21160;&#35774;&#22791;&#30456;&#26426;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#30456;&#26426;&#20316;&#20026;&#29992;&#20110;&#38750;&#20405;&#20837;&#24615;&#31958;&#23615;&#30149;&#26816;&#27979;&#30340;&#20415;&#21033;&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#38754;&#37096;&#22359;&#32441;&#29702;&#29305;&#24449;&#12290;&#36873;&#25321;20&#33267;79&#23681;&#30340;&#21442;&#19982;&#32773;&#36827;&#34892;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;12mp&#21644;7mp&#31227;&#21160;&#30456;&#26426;&#20197;&#21450;&#31508;&#35760;&#26412;&#30005;&#33041;&#30456;&#26426;&#22312;&#27491;&#24120;&#29031;&#26126;&#26465;&#20214;&#19979;&#25293;&#25668;&#29031;&#29255;&#12290;&#25552;&#21462;&#30340;&#38754;&#37096;&#22359;&#34987;&#20351;&#29992;k-&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#20849;&#25429;&#33719;&#20102;100&#24352;&#22270;&#20687;&#65292;&#32463;&#36807;&#39044;&#22788;&#29702;&#12289;Gabor&#28388;&#27874;&#21644;&#36845;&#20195;&#22788;&#29702;&#12290;&#31995;&#32479;&#30340;&#24615;&#33021;&#20197;&#20934;&#30830;&#24615;&#12289;&#29305;&#24322;&#24615;&#21644;&#25935;&#24863;&#24615;&#26469;&#34913;&#37327;&#12290;&#22312;&#20351;&#29992;SVM&#21644;100&#24352;&#22270;&#20687;&#30340;12mp&#32972;&#38754;&#30456;&#26426;&#19978;&#65292;&#36798;&#21040;&#20102;96.7%&#30340;&#20934;&#30830;&#24615;&#12289;100%&#30340;&#25935;&#24863;&#24615;&#21644;93%&#30340;&#29305;&#24322;&#24615;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper compares and explores the performance of both mobile device camera and laptop camera as convenient tool for capturing images for non-invasive detection of Diabetes Mellitus (DM) using facial block texture features. Participants within age bracket 20 to 79 years old were chosen for the dataset. 12mp and 7mp mobile cameras, and a laptop camera were used to take the photo under normal lighting condition. Extracted facial blocks were classified using k-Nearest Neighbors (k-NN) and Support Vector Machine (SVM). 100 images were captured, preprocessed, filtered using Gabor, and iterated. Performance of the system was measured in terms of accuracy, specificity, and sensitivity. Best performance of 96.7% accuracy, 100% sensitivity, and 93% specificity were achieved from 12mp back camera using SVM with 100 images.
&lt;/p&gt;</description></item><item><title>FeedbackLogs&#26159;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#21453;&#39304;&#35760;&#24405;&#21644;&#26356;&#26032;&#30340;&#34917;&#20805;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#31639;&#27861;&#23457;&#35745;&#21644;&#35760;&#24405;&#21453;&#39304;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2307.15475</link><description>&lt;p&gt;
FeedbackLogs&#65306;&#23558;&#21033;&#30410;&#30456;&#20851;&#32773;&#21453;&#39304;&#35760;&#24405;&#21644;&#32435;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;
&lt;/p&gt;
&lt;p&gt;
FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines. (arXiv:2307.15475v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15475
&lt;/p&gt;
&lt;p&gt;
FeedbackLogs&#26159;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#21453;&#39304;&#35760;&#24405;&#21644;&#26356;&#26032;&#30340;&#34917;&#20805;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#31639;&#27861;&#23457;&#35745;&#21644;&#35760;&#24405;&#21453;&#39304;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#24433;&#21709;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#20294;&#20851;&#20110;&#21033;&#30410;&#30456;&#20851;&#32773;&#36755;&#20837;&#30340;&#35760;&#24405;&#21644;&#21033;&#29992;&#26041;&#38754;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FeedbackLogs&#65292;&#36825;&#26159;&#38024;&#23545;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#25991;&#26723;&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#36861;&#36394;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#36755;&#20837;&#12290;&#27599;&#20010;&#26085;&#24535;&#35760;&#24405;&#20102;&#20851;&#20110;&#21453;&#39304;&#25910;&#38598;&#36807;&#31243;&#12289;&#21453;&#39304;&#20869;&#23481;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#21453;&#39304;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#37325;&#35201;&#32454;&#33410;&#12290;&#26412;&#25991;&#20171;&#32461;&#24182;&#27491;&#24335;&#21270;&#20102;&#25910;&#38598;FeedbackLog&#30340;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#20854;&#20013;FeedbackLogs&#21487;&#20197;&#20316;&#20026;&#31639;&#27861;&#23457;&#35745;&#30340;&#35777;&#25454;&#65292;&#24182;&#29992;&#20316;&#35760;&#24405;&#22522;&#20110;&#21033;&#30410;&#30456;&#20851;&#32773;&#21453;&#39304;&#30340;&#26356;&#26032;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though machine learning (ML) pipelines affect an increasing array of stakeholders, there is little work on how input from stakeholders is recorded and incorporated. We propose FeedbackLogs, addenda to existing documentation of ML pipelines, to track the input of multiple stakeholders. Each log records important details about the feedback collection process, the feedback itself, and how the feedback is used to update the ML pipeline. In this paper, we introduce and formalise a process for collecting a FeedbackLog. We also provide concrete use cases where FeedbackLogs can be employed as evidence for algorithmic auditing and as a tool to record updates based on stakeholder feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#31283;&#20581;&#24615;&#20135;&#29983;&#20102;&#25285;&#24551;&#65292;&#36890;&#36807;&#23545;&#20302;&#31070;&#32463;&#20803;&#21644;&#31526;&#21495;&#25277;&#35937;&#30340;&#25506;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#25511;&#21046;&#22120;&#36798;&#21040;&#39640;&#22238;&#25253;&#65292;&#20173;&#20250;&#20135;&#29983;&#22823;&#37327;&#25345;&#20037;&#20302;&#22238;&#25253;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#25163;&#21487;&#20197;&#36731;&#26131;&#21033;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#31283;&#20581;&#24615;&#30740;&#31350;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25345;&#20037;&#35299;&#20915;&#26041;&#26696;&#30340;&#23384;&#22312;&#24615;&#20197;&#21450;&#21608;&#26399;&#36712;&#36947;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2307.15456</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21450;&#20854;&#31526;&#21495;&#34920;&#31034;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Worrisome Properties of Neural Network Controllers and Their Symbolic Representations. (arXiv:2307.15456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#31283;&#20581;&#24615;&#20135;&#29983;&#20102;&#25285;&#24551;&#65292;&#36890;&#36807;&#23545;&#20302;&#31070;&#32463;&#20803;&#21644;&#31526;&#21495;&#25277;&#35937;&#30340;&#25506;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#25511;&#21046;&#22120;&#36798;&#21040;&#39640;&#22238;&#25253;&#65292;&#20173;&#20250;&#20135;&#29983;&#22823;&#37327;&#25345;&#20037;&#20302;&#22238;&#25253;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#25163;&#21487;&#20197;&#36731;&#26131;&#21033;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#31283;&#20581;&#24615;&#30740;&#31350;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25345;&#20037;&#35299;&#20915;&#26041;&#26696;&#30340;&#23384;&#22312;&#24615;&#20197;&#21450;&#21608;&#26399;&#36712;&#36947;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31616;&#21333;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#38382;&#39064;&#20013;&#25511;&#21046;&#22120;&#30340;&#31283;&#20581;&#24615;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#20999;&#12290;&#25105;&#20204;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21450;&#20854;&#20302;&#31070;&#32463;&#20803;&#21644;&#31526;&#21495;&#25277;&#35937;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#36798;&#21040;&#24456;&#39640;&#30340;&#24179;&#22343;&#22238;&#25253;&#20540;&#65292;&#20294;&#20173;&#20250;&#29983;&#25104;&#22823;&#37327;&#25345;&#20037;&#30340;&#20302;&#22238;&#25253;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#19981;&#21487;&#21462;&#30340;&#29305;&#24615;&#65292;&#23481;&#26131;&#34987;&#23545;&#25163;&#21033;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#25511;&#21046;&#22120;&#20250;&#20135;&#29983;&#26356;&#22810;&#25345;&#20037;&#30340;&#19981;&#33391;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#31283;&#20581;&#24615;&#30740;&#31350;&#30340;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#35745;&#31639;&#26426;&#36741;&#21161;&#35777;&#26126;&#26041;&#27861;&#35777;&#26126;&#20102;&#25345;&#20037;&#35299;&#20915;&#26041;&#26696;&#30340;&#23384;&#22312;&#24615;&#65292;&#20197;&#21450;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#21608;&#26399;&#36712;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We raise concerns about controllers' robustness in simple reinforcement learning benchmark problems. We focus on neural network controllers and their low neuron and symbolic abstractions. A typical controller reaching high mean return values still generates an abundance of persistent low-return solutions, which is a highly undesirable property, easily exploitable by an adversary. We find that the simpler controllers admit more persistent bad solutions. We provide an algorithm for a systematic robustness study and prove existence of persistent solutions and, in some cases, periodic orbits, using a computer-assisted proof methodology.
&lt;/p&gt;</description></item><item><title>CompLog&#26159;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;Kolmogorov&#22797;&#26434;&#24615;&#26367;&#20195;&#27010;&#29575;&#25512;&#29702;&#65292;&#23454;&#29616;&#35745;&#31639;&#26576;&#31181;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#25551;&#36848;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15453</link><description>&lt;p&gt;
&#20174;&#27010;&#29575;&#32534;&#31243;&#21040;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15453
&lt;/p&gt;
&lt;p&gt;
CompLog&#26159;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;Kolmogorov&#22797;&#26434;&#24615;&#26367;&#20195;&#27010;&#29575;&#25512;&#29702;&#65292;&#23454;&#29616;&#35745;&#31639;&#26576;&#31181;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#25551;&#36848;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CompLog&#30340;&#26032;&#22411;&#35745;&#31639;&#26694;&#26550;&#30340;&#20027;&#35201;&#29305;&#28857;&#21644;&#21021;&#27493;&#23454;&#29616;&#12290;CompLog&#20511;&#37492;&#20102;&#27010;&#29575;&#32534;&#31243;&#31995;&#32479;&#65288;&#22914;ProbLog&#65289;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#24182;&#22522;&#20110;Simplicity&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#36890;&#36807;ASP&#31243;&#24207;&#30340;min-path&#25628;&#32034;&#35745;&#31639;&#20004;&#31181;Kolmogorov&#22797;&#26434;&#24615;&#65292;&#32780;&#19981;&#26159;&#27010;&#29575;&#25512;&#29702;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#25143;&#33021;&#22815;&#35745;&#31639;&#26576;&#20010;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;ex-post&#21644;ex-ante&#24230;&#37327;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#21518;&#39564;&#21644;&#20808;&#39564;&#20027;&#35266;&#27010;&#29575;&#12290;&#35745;&#31639;&#22522;&#20110;&#36890;&#36807;&#25551;&#36848;&#24615;&#35859;&#35789;&#20043;&#38388;&#30340;&#22240;&#26524;&#21644;&#25551;&#36848;&#24615;&#20851;&#31995;&#21152;&#26435;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#35268;&#33539;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#20960;&#20010;&#24212;&#29992;&#31034;&#20363;&#65306;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the main characteristics and a preliminary implementation of a novel computational framework named CompLog. Inspired by probabilistic programming systems like ProbLog, CompLog builds upon the inferential mechanisms proposed by Simplicity Theory, relying on the computation of two Kolmogorov complexities (here implemented as min-path searches via ASP programs) rather than probabilistic inference. The proposed system enables users to compute ex-post and ex-ante measures of unexpectedness of a certain situation, mapping respectively to posterior and prior subjective probabilities. The computation is based on the specification of world and mental models by means of causal and descriptive relations between predicates weighted by complexity. The paper illustrates a few examples of application: generating relevant descriptions, and providing alternative approaches to disjunction and to negation.
&lt;/p&gt;</description></item><item><title>DELPHIC&#26159;&#19968;&#20010;&#25193;&#23637;&#20102;&#20256;&#32479;DEL&#30340;&#26694;&#26550;&#65292;&#22312;&#23454;&#29992;DEL&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#33021;&#24615;&#20316;&#20026;&#20027;&#35201;&#26500;&#24314;&#27169;&#22359;&#65292;DELPHIC&#25552;&#20379;&#20102;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.15451</link><description>&lt;p&gt;
DELPHIC: Practical DEL&#35268;&#21010;&#30340;&#21487;&#33021;&#24615;&#36884;&#24452;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
DELPHIC: Practical DEL Planning via Possibilities (Extended Version). (arXiv:2307.15451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15451
&lt;/p&gt;
&lt;p&gt;
DELPHIC&#26159;&#19968;&#20010;&#25193;&#23637;&#20102;&#20256;&#32479;DEL&#30340;&#26694;&#26550;&#65292;&#22312;&#23454;&#29992;DEL&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#33021;&#24615;&#20316;&#20026;&#20027;&#35201;&#26500;&#24314;&#27169;&#22359;&#65292;DELPHIC&#25552;&#20379;&#20102;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#65288;DEL&#65289;&#20026;&#35748;&#30693;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#34920;&#31034;&#38750;&#30830;&#23450;&#24615;&#21160;&#20316;&#12289;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#12289;&#39640;&#38454;&#30693;&#35782;&#20197;&#21450;&#20107;&#23454;&#21644;&#35748;&#30693;&#21464;&#21270;&#12290;DEL&#30340;&#39640;&#34920;&#36798;&#24615;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#35748;&#30693;&#35268;&#21010;&#22120;&#65292;&#36890;&#24120;&#21482;&#33021;&#22788;&#29702;&#25972;&#20010;&#26694;&#26550;&#20013;&#30340;&#21463;&#38480;&#29255;&#27573;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25512;&#21160;&#23454;&#29992;DEL&#35268;&#21010;&#30340;&#21457;&#23637;&#65292;&#26368;&#32456;&#20351;&#35748;&#30693;&#35268;&#21010;&#22120;&#33021;&#22815;&#22788;&#29702;DEL&#25552;&#20379;&#30340;&#25152;&#26377;&#29305;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#23545;DEL&#30340;&#20256;&#32479;&#35821;&#20041;&#36827;&#34892;&#20102;&#36136;&#30097;&#65292;&#35813;&#35821;&#20041;&#26159;&#29992;Kripke&#27169;&#22411;&#26469;&#23450;&#20041;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#20215;&#30340;&#35821;&#20041;&#65292;&#20351;&#29992;&#25152;&#35859;&#30340;&#21487;&#33021;&#24615;&#20316;&#20026;&#20027;&#35201;&#26500;&#24314;&#27169;&#22359;&#65292;&#34920;&#31034;&#19990;&#30028;&#30340;&#20107;&#23454;&#23646;&#24615;&#21644;&#20195;&#29702;&#20154;&#35748;&#20026;&#21487;&#33021;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#32467;&#26524;&#20026;DELPHIC&#26694;&#26550;&#12290;&#25105;&#20204;&#35748;&#20026;DELPHIC&#30830;&#23454;&#25552;&#20379;&#20102;&#23545;&#35748;&#30693;&#29366;&#24577;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Epistemic Logic (DEL) provides a framework for epistemic planning that is capable of representing non-deterministic actions, partial observability, higher-order knowledge and both factual and epistemic change. The high expressivity of DEL challenges existing epistemic planners, which typically can handle only restricted fragments of the whole framework. The goal of this work is to push the envelop of practical DEL planning, ultimately aiming for epistemic planners to be able to deal with the full range of features offered by DEL. Towards this goal, we question the traditional semantics of DEL, defined in terms on Kripke models. In particular, we propose an equivalent semantics defined using, as main building block, so-called possibilities: non well-founded objects representing both factual properties of the world, and what agents consider to be possible. We call the resulting framework DELPHIC. We argue that DELPHIC indeed provides a more compact representation of epistemic sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#26102;&#38388;&#30693;&#35782;&#24211;&#23545;&#40784;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#25913;&#21464;&#24050;&#26377;&#30340;&#30693;&#35782;&#24211;&#65292;&#21516;&#26102;&#28385;&#36275;&#32473;&#23450;&#30340;&#26102;&#38388;&#30456;&#20851;&#26597;&#35810;&#65292;&#24182;&#22312;&#25104;&#26412;&#19978;&#36798;&#21040;&#26368;&#20248;&#12290;&#26041;&#27861;&#22312;ALC TKBs&#21644;&#24102;&#26377;LTL&#36816;&#31639;&#31526;&#30340;&#36830;&#25509;&#26597;&#35810;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25193;&#23637;&#20102;&#21629;&#39064;LTL&#23545;&#40784;&#38382;&#39064;&#19978;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.15439</link><description>&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#24211;&#30340;&#26368;&#20248;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Optimal Alignment of Temporal Knowledge Bases. (arXiv:2307.15439v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#26102;&#38388;&#30693;&#35782;&#24211;&#23545;&#40784;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#25913;&#21464;&#24050;&#26377;&#30340;&#30693;&#35782;&#24211;&#65292;&#21516;&#26102;&#28385;&#36275;&#32473;&#23450;&#30340;&#26102;&#38388;&#30456;&#20851;&#26597;&#35810;&#65292;&#24182;&#22312;&#25104;&#26412;&#19978;&#36798;&#21040;&#26368;&#20248;&#12290;&#26041;&#27861;&#22312;ALC TKBs&#21644;&#24102;&#26377;LTL&#36816;&#31639;&#31526;&#30340;&#36830;&#25509;&#26597;&#35810;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25193;&#23637;&#20102;&#21629;&#39064;LTL&#23545;&#40784;&#38382;&#39064;&#19978;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26412;&#20307;&#30340;&#24773;&#22659;&#35782;&#21035;&#20013;&#65292;&#22238;&#31572;&#26102;&#38388;&#21270;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65288;TKB&#65289;&#19978;&#30340;&#26102;&#38388;CQs&#26159;&#19968;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#30693;&#35782;&#24211;&#20013;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#19981;&#20934;&#30830;&#65292;&#37325;&#35201;&#30340;&#26597;&#35810;&#31572;&#26696;&#21487;&#33021;&#20250;&#34987;&#24573;&#30053;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TKB&#23545;&#40784;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#35745;&#31639;&#19968;&#31181;&#21464;&#20307;&#30340;TKB&#65292;&#26368;&#23567;&#21270;&#22320;&#25913;&#21464;TKB&#65292;&#20294;&#33021;&#25512;&#23548;&#20986;&#32473;&#23450;&#30340;&#26102;&#38388;CQ&#65292;&#24182;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#26159;&#65288;&#20195;&#20215;&#65289;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;ALC TKBs&#21644;&#24102;&#26377;LTL&#36816;&#31639;&#31526;&#30340;&#36830;&#25509;&#26597;&#35810;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26469;&#35745;&#31639;TKB&#30340;&#65288;&#20195;&#20215;&#26368;&#20248;&#30340;&#65289;&#23545;&#40784;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#25193;&#23637;&#20102;&#29992;&#20110;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#21629;&#39064;LTL&#23545;&#40784;&#38382;&#39064;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering temporal CQs over temporalized Description Logic knowledge bases (TKB) is a main technique to realize ontology-based situation recognition. In case the collected data in such a knowledge base is inaccurate, important query answers can be missed. In this paper we introduce the TKB Alignment problem, which computes a variant of the TKB that minimally changes the TKB, but entails the given temporal CQ and is in that sense (cost-)optimal. We investigate this problem for ALC TKBs and conjunctive queries with LTL operators and devise a solution technique to compute (cost-optimal) alignments of TKBs that extends techniques for the alignment problem for propositional LTL over finite traces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#31639;&#27861;&#65288;IGB&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#19968;&#31181;&#37319;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#39318;&#27425;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#36890;&#36807;&#21160;&#24577;&#20998;&#37197;&#20219;&#21153;&#26435;&#37325;&#26469;&#23454;&#29616;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#25439;&#22833;&#24179;&#34913;&#20043;&#21518;&#20173;&#28982;&#23384;&#22312;&#30340;&#24615;&#33021;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15429</link><description>&lt;p&gt;
&#21487;&#25913;&#36827;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24046;&#36317;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improvable Gap Balancing for Multi-Task Learning. (arXiv:2307.15429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#31639;&#27861;&#65288;IGB&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#19968;&#31181;&#37319;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#39318;&#27425;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#36890;&#36807;&#21160;&#24577;&#20998;&#37197;&#20219;&#21153;&#26435;&#37325;&#26469;&#23454;&#29616;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#25439;&#22833;&#24179;&#34913;&#20043;&#21518;&#20173;&#28982;&#23384;&#22312;&#30340;&#24615;&#33021;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#65292;&#26799;&#24230;&#24179;&#34913;&#36817;&#26399;&#27604;&#25439;&#22833;&#24179;&#34913;&#26356;&#21560;&#24341;&#30740;&#31350;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#33021;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#24179;&#34913;&#27604;&#26799;&#24230;&#24179;&#34913;&#26356;&#39640;&#25928;&#65292;&#22240;&#27492;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#20173;&#28982;&#20540;&#24471;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#27880;&#24847;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#24573;&#30053;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#21487;&#25913;&#36827;&#24046;&#36317;&#30340;&#20107;&#23454;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#21487;&#25913;&#36827;&#24046;&#36317;&#23450;&#20041;&#20026;&#24403;&#21069;&#35757;&#32451;&#36827;&#24230;&#19982;&#26399;&#26395;&#30340;&#26368;&#32456;&#35757;&#32451;&#36827;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#22312;&#25439;&#22833;&#24179;&#34913;&#20043;&#21518;&#65292;&#24615;&#33021;&#19981;&#24179;&#34913;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#25439;&#22833;&#24179;&#34913;&#26694;&#26550;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#31639;&#27861;&#65288;IGB&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65306;&#19968;&#31181;&#37319;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#65288;&#39318;&#27425;&#65289;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#29305;&#21035;&#22320;&#65292;&#20004;&#31181;&#31639;&#27861;&#37117;&#36873;&#25321;&#21160;&#24577;&#20998;&#37197;&#20219;&#21153;&#26435;&#37325;&#26469;&#36827;&#34892;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-task learning (MTL), gradient balancing has recently attracted more research interest than loss balancing since it often leads to better performance. However, loss balancing is much more efficient than gradient balancing, and thus it is still worth further exploration in MTL. Note that prior studies typically ignore that there exist varying improvable gaps across multiple tasks, where the improvable gap per task is defined as the distance between the current training progress and desired final training progress. Therefore, after loss balancing, the performance imbalance still arises in many cases. In this paper, following the loss balancing framework, we propose two novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple heuristic, and the other (for the first time) deploys deep reinforcement learning for MTL. Particularly, instead of directly balancing the losses in MTL, both algorithms choose to dynamically assign task weights for improvable gap balancing
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#39044;&#27979;&#26694;&#26550;DSN&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#24086;&#23376;&#20869;&#37096;&#21644;&#24086;&#23376;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15413</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#24086;&#23376;&#20381;&#36182;&#20851;&#31995;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#21463;&#27426;&#36814;&#31243;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Social Media Popularity Prediction with Multiple Post Dependencies. (arXiv:2307.15413v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#39044;&#27979;&#26694;&#26550;DSN&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#24086;&#23376;&#20869;&#37096;&#21644;&#24086;&#23376;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#21463;&#27426;&#36814;&#31243;&#24230;&#39044;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23545;&#35768;&#22810;&#19981;&#21516;&#24212;&#29992;&#26377;&#30528;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#22810;&#23186;&#20307;&#24191;&#21578;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#19968;&#20123;&#21162;&#21147;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#20869;&#23481;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#24086;&#23376;&#20043;&#38388;&#30340;&#22810;&#20010;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#23545;&#20110;&#20840;&#38754;&#25552;&#21462;&#24086;&#23376;&#20869;&#23481;&#20449;&#24687;&#24456;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20381;&#36182;&#24863;&#30693;&#24207;&#21015;&#32593;&#32476;&#65288;DSN&#65289;&#8221;&#30340;&#26032;&#22411;&#39044;&#27979;&#26694;&#26550;&#65292;&#23427;&#21516;&#26102;&#21033;&#29992;&#24086;&#23376;&#20869;&#37096;&#20381;&#36182;&#21644;&#24086;&#23376;&#38388;&#20381;&#36182;&#12290;&#23545;&#20110;&#24086;&#23376;&#20869;&#37096;&#20381;&#36182;&#65292;DSN&#37319;&#29992;&#19968;&#31181;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#65292;&#20174;&#24086;&#23376;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#20013;&#33719;&#21462;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#23545;&#20110;&#24086;&#23376;&#38388;&#20381;&#36182;&#65292;DSN&#20351;&#29992;&#19968;&#31181;&#20998;&#23618;&#20449;&#24687;&#20256;&#25773;&#26041;&#27861;&#26469;&#23398;&#20064;&#33021;&#26356;&#22909;&#25551;&#36848;&#24086;&#23376;&#20043;&#38388;&#24046;&#24322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;DSN&#36824;&#21033;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Social Media Popularity Prediction has drawn a lot of attention because of its profound impact on many different applications, such as recommendation systems and multimedia advertising. Despite recent efforts to leverage the content of social media posts to improve prediction accuracy, many existing models fail to fully exploit the multiple dependencies between posts, which are important to comprehensively extract content information from posts. To tackle this problem, we propose a novel prediction framework named Dependency-aware Sequence Network (DSN) that exploits both intra- and inter-post dependencies. For intra-post dependency, DSN adopts a multimodal feature extractor with an efficient fine-tuning strategy to obtain task-specific representations from images and textual information of posts. For inter-post dependency, DSN uses a hierarchical information propagation method to learn category representations that could better describe the difference between posts. DSN also exploits 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#21516;&#27880;&#24847;&#21147;&#22270;&#27719;&#32858;&#65288;CAGPool&#65289;&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#22270;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22270;&#23545;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15377</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#25104;&#23545;&#22270;&#20132;&#20114;&#23398;&#20064;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#22270;&#27719;&#32858;
&lt;/p&gt;
&lt;p&gt;
Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning. (arXiv:2307.15377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#21516;&#27880;&#24847;&#21147;&#22270;&#27719;&#32858;&#65288;CAGPool&#65289;&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#22270;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22270;&#23545;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#21644;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#29702;&#35299;&#21333;&#19968;&#22270;&#36755;&#20837;&#65292;&#32780;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#38656;&#35201;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25104;&#23545;&#20998;&#26512;&#65288;&#20363;&#22914;&#22330;&#26223;&#22270;&#21305;&#37197;&#12289;&#20195;&#30721;&#25628;&#32034;&#21644;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65289;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#23398;&#20064;&#22270;&#23545;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#21363;&#20132;&#20114;&#26159;&#22312;&#33410;&#28857;&#32423;&#21035;&#36827;&#34892;&#32771;&#34385;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#24615;&#33021;&#20122;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#22270;&#32423;&#26041;&#27861;&#65292;&#21033;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#22312;&#22270;&#27719;&#32858;&#20013;&#25552;&#21462;&#20132;&#20114;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20849;&#21516;&#27880;&#24847;&#21147;&#22270;&#27719;&#32858;&#65288;CAGPool&#65289;&#65292;&#22312;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#23637;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have proven to be effective in processing and learning from graph-structured data. However, previous works mainly focused on understanding single graph inputs while many real-world applications require pair-wise analysis for graph-structured data (e.g., scene graph matching, code searching, and drug-drug interaction prediction). To this end, recent works have shifted their focus to learning the interaction between pairs of graphs. Despite their improved performance, these works were still limited in that the interactions were considered at the node-level, resulting in high computational costs and suboptimal performance. To address this issue, we propose a novel and efficient graph-level approach for extracting interaction representations using co-attention in graph pooling. Our method, Co-Attention Graph Pooling (CAGPool), exhibits competitive performance relative to existing methods in both classification and regression tasks using real-world datasets, whi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#21487;&#20197;&#20135;&#29983;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#19988;&#21487;&#20197;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.15361</link><description>&lt;p&gt;
&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Confident Feature Ranking. (arXiv:2307.15361v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#21487;&#20197;&#20135;&#29983;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#19988;&#21487;&#20197;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#35299;&#37322;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#24449;&#30340;&#30456;&#23545;&#39034;&#24207;&#32780;&#19981;&#26159;&#25968;&#20540;&#26412;&#36523;&#65292;&#20063;&#23601;&#26159;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#37325;&#35201;&#24615;&#20540;&#26102;&#20351;&#29992;&#30340;&#26679;&#26412;&#37327;&#36739;&#23567;&#65292;&#25490;&#24207;&#21487;&#33021;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#22522;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#39640;&#27010;&#29575;&#21253;&#21547;&#8220;&#30495;&#23454;&#8221;&#65288;&#26080;&#38480;&#26679;&#26412;&#65289;&#25490;&#24207;&#65292;&#24182;&#20801;&#35768;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
&lt;/p&gt;</description></item><item><title>Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.15343</link><description>&lt;p&gt;
Med-HALT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15343
&lt;/p&gt;
&lt;p&gt;
Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#12290;&#24187;&#35273;&#25351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#20102;&#21512;&#29702;&#20294;&#26410;&#32463;&#39564;&#35777;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#21307;&#30103;&#24212;&#29992;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;Med-HALT&#65288;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24187;&#35273;&#12290;Med-HALT&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#20803;&#21270;&#30340;&#36328;&#22269;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#21307;&#30103;&#26816;&#26597;&#65292;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#12290;Med-HALT&#21253;&#25324;&#20004;&#31867;&#27979;&#35797;&#65306;&#25512;&#29702;&#21644;&#22522;&#20110;&#35760;&#24518;&#30340;&#24187;&#35273;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#20449;&#24687;&#26816;&#32034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#25991;&#26412;Davinci&#65292;GPT-3.5&#65292;LlaMa-2&#65292;MPT&#21644;Falcon&#31561;&#39046;&#20808;&#30340;LLMs&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#35265;&#35299;&#65292;&#20419;&#36827;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15337</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#39592;&#26550;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24182;&#34892;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#24310;&#36831;&#12290;&#39640;&#29983;&#25104;&#24310;&#36831;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;LLMs&#37117;&#37319;&#29992;&#20102;&#39034;&#24207;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#20154;&#31867;&#30340;&#24605;&#32771;&#21644;&#20889;&#20316;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#23427;&#25351;&#23548;LLMs&#39318;&#20808;&#29983;&#25104;&#31572;&#26696;&#30340;&#39592;&#26550;&#65292;&#28982;&#21518;&#36890;&#36807;&#24182;&#34892;API&#35843;&#29992;&#25110;&#25209;&#37327;&#35299;&#30721;&#26469;&#24182;&#34892;&#23436;&#25104;&#27599;&#20010;&#39592;&#26550;&#28857;&#30340;&#20869;&#23481;&#12290;SoT&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65288;&#22312;11&#20010;&#19981;&#21516;&#30340;LLMs&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;2.39&#20493;&#65289;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#22312;&#22810;&#20010;&#38382;&#39064;&#31867;&#21035;&#19978;&#30340;&#31572;&#26696;&#36136;&#37327;&#65292;&#21253;&#25324;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;SoT&#26159;&#19968;&#31181;&#38024;&#23545;&#25928;&#29575;&#30340;&#25968;&#25454;&#23548;&#21521;&#20248;&#21270;&#30340;&#21021;&#27493;&#23581;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#23558;LLMs&#25512;&#21160;&#26356;&#20687;&#20154;&#31867;&#24605;&#32771;&#20197;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;BERT&#24494;&#35843;&#21644;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;Twitter&#31435;&#22330;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25945;&#31243;&#36890;&#36807;&#23454;&#20363;&#20195;&#30721;&#21644;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23569;&#26679;&#26412;ChatGPT&#21644;FLAN-T5&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#25351;&#23548;&#12290;&#36825;&#20123;&#25945;&#31243;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#25484;&#25569;&#36816;&#29992;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#31435;&#22330;&#35782;&#21035;&#30340;&#23454;&#36341;&#32463;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.15331</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31435;&#22330;&#35782;&#21035;&#30340;&#25945;&#31243;&#65306;BERT&#24494;&#35843;&#21644;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models. (arXiv:2307.15331v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15331
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;BERT&#24494;&#35843;&#21644;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;Twitter&#31435;&#22330;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25945;&#31243;&#36890;&#36807;&#23454;&#20363;&#20195;&#30721;&#21644;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23569;&#26679;&#26412;ChatGPT&#21644;FLAN-T5&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#25351;&#23548;&#12290;&#36825;&#20123;&#25945;&#31243;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#25484;&#25569;&#36816;&#29992;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#31435;&#22330;&#35782;&#21035;&#30340;&#23454;&#36341;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;BERT&#24494;&#35843;&#21644;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;Twitter&#25968;&#25454;&#19978;&#36827;&#34892;&#31435;&#22330;&#35782;&#21035;&#12290;&#31532;&#19968;&#20010;&#25945;&#31243;&#35299;&#37322;&#20102;BERT&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#65292;&#25351;&#23548;&#29992;&#25143;&#36890;&#36807;&#20351;&#29992;HuggingFace transformers&#35757;&#32451;&#12289;&#35843;&#20248;&#21644;&#35780;&#20272;&#26631;&#20934;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;BERT&#27169;&#22411;&#12290;&#31532;&#20108;&#20010;&#25945;&#31243;&#20391;&#37325;&#20110;&#26500;&#24314;&#25552;&#31034;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#20174;ChatGPT&#21644;&#24320;&#28304;FLAN-T5&#20013;&#24341;&#20986;&#31435;&#22330;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#37319;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#28151;&#28102;&#30697;&#38453;&#21644;&#23439;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#25945;&#31243;&#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#21487;&#35270;&#21270;&#21644;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#23569;&#26679;&#26412;ChatGPT&#21644;FLAN-T5&#30340;&#20248;&#21183;&#65292;&#23427;&#20204;&#32988;&#36807;&#20102;&#24494;&#35843;&#30340;BERT&#12290;&#36890;&#36807;&#20197;&#26131;&#20110;&#29702;&#35299;&#12289;&#23454;&#36341;&#20026;&#23548;&#21521;&#30340;&#26041;&#24335;&#21516;&#26102;&#28085;&#30422;&#27169;&#22411;&#24494;&#35843;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#20123;&#25945;&#31243;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#33719;&#24471;&#23545;&#31435;&#22330;&#26816;&#27979;&#30340;&#23574;&#31471;&#26041;&#27861;&#30340;&#23454;&#38469;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two self-contained tutorials on stance detection in Twitter data using BERT fine-tuning and prompting large language models (LLMs). The first tutorial explains BERT architecture and tokenization, guiding users through training, tuning, and evaluating standard and domain-specific BERT models with HuggingFace transformers. The second focuses on constructing prompts and few-shot examples to elicit stances from ChatGPT and open-source FLAN-T5 without fine-tuning. Various prompting strategies are implemented and evaluated using confusion matrices and macro F1 scores. The tutorials provide code, visualizations, and insights revealing the strengths of few-shot ChatGPT and FLAN-T5 which outperform fine-tuned BERTs. By covering both model fine-tuning and prompting-based techniques in an accessible, hands-on manner, these tutorials enable learners to gain applied experience with cutting-edge methods for stance detection.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#27169;&#25311;&#19982;&#23454;&#38469;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#36328;&#30028;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#31163;&#32447;&#20195;&#29702;&#20219;&#21153;&#21644;&#27169;&#25311;&#35757;&#32451;&#65292;&#25104;&#21151;&#22320;&#20248;&#21270;&#20102;&#22495;&#38543;&#26426;&#21270;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.15320</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#35270;&#35273;&#27169;&#25311;&#19982;&#23454;&#38469;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#36328;&#30028;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Robust Visual Sim-to-Real Transfer for Robotic Manipulation. (arXiv:2307.15320v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15320
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#27169;&#25311;&#19982;&#23454;&#38469;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#36328;&#30028;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#31163;&#32447;&#20195;&#29702;&#20219;&#21153;&#21644;&#27169;&#25311;&#35757;&#32451;&#65292;&#25104;&#21151;&#22320;&#20248;&#21270;&#20102;&#22495;&#38543;&#26426;&#21270;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23398;&#20064;&#35270;&#35273;&#21160;&#20316;&#31574;&#30053;&#27604;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#26356;&#23433;&#20840;&#19988;&#26356;&#20415;&#23452;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32463;&#36807;&#27169;&#25311;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#36716;&#31227;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#26102;&#32463;&#24120;&#22833;&#36133;&#12290;&#20026;&#20102;&#24357;&#21512;&#35270;&#35273;&#27169;&#25311;&#19982;&#23454;&#38469;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#22495;&#38543;&#26426;&#21270;(domain randomization, DR)&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#20027;&#35201;&#35780;&#20272;&#20102;DR&#22312;&#23039;&#24577;&#20272;&#35745;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#33073;&#31163;&#21147;&#30340;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35270;&#35273;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#20195;&#29702;&#20219;&#21153;&#65292;&#29992;&#20110;&#36873;&#25321;&#32441;&#29702;&#38543;&#26426;&#21270;&#12289;&#20809;&#29031;&#38543;&#26426;&#21270;&#12289;&#29289;&#20307;&#39068;&#33394;&#21464;&#21270;&#21644;&#30456;&#26426;&#21442;&#25968;&#31561;DR&#21442;&#25968;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DR&#21442;&#25968;&#23545;&#20110;&#31163;&#32447;&#20195;&#29702;&#20219;&#21153;&#21644;&#22312;&#32447;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#31163;&#32447;&#20248;&#21270;&#30340;DR&#21442;&#25968;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#35270;&#35273;&#21160;&#20316;&#31574;&#30053;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#31574;&#30053;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning visuomotor policies in simulation is much safer and cheaper than in the real world. However, due to discrepancies between the simulated and real data, simulator-trained policies often fail when transferred to real robots. One common approach to bridge the visual sim-to-real domain gap is domain randomization (DR). While previous work mainly evaluates DR for disembodied tasks, such as pose estimation and object detection, here we systematically explore visual domain randomization methods and benchmark them on a rich set of challenging robotic manipulation tasks. In particular, we propose an off-line proxy task of cube localization to select DR parameters for texture randomization, lighting randomization, variations of object colors and camera parameters. Notably, we demonstrate that DR parameters have similar impact on our off-line proxy task and on-line policies. We, hence, use off-line optimized DR parameters to train visuomotor policies in simulation and directly apply such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15317</link><description>&lt;p&gt;
DiffKendall:&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation. (arXiv:2307.15317v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#23558;&#22312;&#22522;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#27169;&#22411;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#26032;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#26032;&#31867;&#21035;&#19978;&#36890;&#36947;&#19978;&#29305;&#24449;&#20540;&#30340;&#20998;&#24067;&#30456;&#23545;&#22343;&#21248;&#65292;&#38590;&#20197;&#30830;&#23450;&#26032;&#20219;&#21153;&#20013;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#12290;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36127;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#65292;&#26469;&#34913;&#37327;&#20004;&#20010;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#39640;&#20960;&#20309;&#30456;&#20284;&#24230;&#30340;&#29305;&#24449;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#20041;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning aims to adapt models trained on the base dataset to novel tasks where the categories are not seen by the model before. This often leads to a relatively uniform distribution of feature values across channels on novel classes, posing challenges in determining channel importance for novel tasks. Standard few-shot learning methods employ geometric similarity metrics such as cosine similarity and negative Euclidean distance to gauge the semantic relatedness between two features. However, features with high geometric similarities may carry distinct semantics, especially in the context of few-shot learning. In this paper, we demonstrate that the importance ranking of feature channels is a more reliable indicator for few-shot learning than geometric similarity metrics. We observe that replacing the geometric similarity metric with Kendall's rank correlation only during inference is able to improve the performance of few-shot learning across a wide range of datasets with diffe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#30693;&#35782;&#36827;&#34892;&#21442;&#25968;&#24191;&#25773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;MBA&#30340;&#26694;&#26550;&#65292;&#20197;&#22312;6G&#31227;&#21160;&#32593;&#32476;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#29992;&#25143;AI&#19979;&#36733;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21442;&#25968;&#36873;&#25321;&#21644;&#21151;&#29575;&#25511;&#21046;&#30340;&#32852;&#21512;&#35774;&#35745;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#24182;&#25552;&#39640;&#35774;&#22791;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15316</link><description>&lt;p&gt;
&#39640;&#25928;&#22810;&#29992;&#25143;AI&#19979;&#36733;&#36890;&#36807;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#30693;&#35782;&#24191;&#25773;
&lt;/p&gt;
&lt;p&gt;
Efficient Multiuser AI Downloading via Reusable Knowledge Broadcasting. (arXiv:2307.15316v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15316
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#30693;&#35782;&#36827;&#34892;&#21442;&#25968;&#24191;&#25773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;MBA&#30340;&#26694;&#26550;&#65292;&#20197;&#22312;6G&#31227;&#21160;&#32593;&#32476;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#29992;&#25143;AI&#19979;&#36733;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21442;&#25968;&#36873;&#25321;&#21644;&#21151;&#29575;&#25511;&#21046;&#30340;&#32852;&#21512;&#35774;&#35745;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#24182;&#25552;&#39640;&#35774;&#22791;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;6G&#31227;&#21160;&#32593;&#32476;&#65292;&#21407;&#22320;&#27169;&#22411;&#19979;&#36733;&#24050;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#29992;&#20363;&#65292;&#20197;&#23454;&#29616;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26080;&#32447;&#38142;&#36335;&#23558;&#22810;&#31181;&#39640;&#32500;&#27169;&#22411;&#21516;&#26102;&#19979;&#36733;&#21040;&#22810;&#20010;&#35774;&#22791;&#19978;&#36896;&#25104;&#20102;&#37325;&#22823;&#30340;&#36890;&#20449;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24191;&#25773;&#21644;&#32452;&#21512;(MBA)&#26694;&#26550;&#65292;&#23427;&#26159;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#30693;&#35782;&#65292;&#21363;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#30340;&#21442;&#25968;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#21442;&#25968;&#24191;&#25773;&#12290;MBA&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#31532;&#19968;&#20010;&#26159;MBA&#21327;&#35758;&#65292;&#23427;&#23450;&#20041;&#20102;&#31995;&#32479;&#25805;&#20316;&#65292;&#21253;&#25324;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#21442;&#25968;&#65292;&#24191;&#25773;&#30340;&#21151;&#29575;&#25511;&#21046;&#20197;&#21450;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#32452;&#35013;&#12290;&#31532;&#20108;&#20010;&#32452;&#20214;&#26159;&#21442;&#25968;&#36873;&#25321;&#21644;&#21151;&#29575;&#25511;&#21046;(PS-PC)&#30340;&#32852;&#21512;&#35774;&#35745;&#65292;&#23427;&#20445;&#35777;&#20102;&#35774;&#22791;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#26368;&#23567;&#21270;&#20102;&#19979;&#36733;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the 6G mobile networks, in-situ model downloading has emerged as an important use case to enable real-time adaptive artificial intelligence on edge devices. However, the simultaneous downloading of diverse and high-dimensional models to multiple devices over wireless links presents a significant communication bottleneck. To overcome the bottleneck, we propose the framework of model broadcasting and assembling (MBA), which represents the first attempt on leveraging reusable knowledge, referring to shared parameters among tasks, to enable parameter broadcasting to reduce communication overhead. The MBA framework comprises two key components. The first, the MBA protocol, defines the system operations including parameter selection from a model library, power control for broadcasting, and model assembling at devices. The second component is the joint design of parameter-selection-and-power-control (PS-PC), which provides guarantees on devices' model performance and minimizes the downloa
&lt;/p&gt;</description></item><item><title>WC-SBERT&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SBERT&#21644;&#33258;&#35757;&#32451;&#35299;&#20915;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#35757;&#32451;&#38598;&#21644;&#24314;&#31435;&#31867;&#21035;&#23545;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#23454;&#29616;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.15293</link><description>&lt;p&gt;
WC-SBERT: &#21033;&#29992;SBERT&#21644;&#33258;&#35757;&#32451;&#35299;&#20915;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories. (arXiv:2307.15293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15293
&lt;/p&gt;
&lt;p&gt;
WC-SBERT&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SBERT&#21644;&#33258;&#35757;&#32451;&#35299;&#20915;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#35757;&#32451;&#38598;&#21644;&#24314;&#31435;&#31867;&#21035;&#23545;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#23454;&#29616;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#29305;&#21035;&#24378;&#35843;&#21019;&#26032;&#30340;&#33258;&#35757;&#32451;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;&#26631;&#31614;&#32780;&#38750;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#32500;&#22522;&#30334;&#31185;&#30340;&#31867;&#21035;&#20316;&#20026;&#35757;&#32451;&#38598;&#65292;&#24182;&#21033;&#29992;SBERT&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21516;&#19968;&#25991;&#26412;&#20013;&#30340;&#31867;&#21035;&#23545;&#20043;&#38388;&#24314;&#31435;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#20415;&#20110;&#36827;&#34892;&#32852;&#24819;&#35757;&#32451;&#12290;&#23545;&#20110;&#26032;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21407;&#22987;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#27599;&#20010;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#20808;&#21069;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#32479;&#19968;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#26356;&#22909;&#22320;&#36817;&#20284;&#38646;&#26679;&#26412;&#24773;&#22659;&#12290;&#36825;&#31181;&#20462;&#25913;&#26041;&#24335;&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21644;&#25512;&#26029;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#33258;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research focuses on solving the zero-shot text classification problem in NLP, with a particular emphasis on innovative self-training strategies. To achieve this objective, we propose a novel self-training strategy that uses labels rather than text for training, significantly reducing the model's training time. Specifically, we use categories from Wikipedia as our training set and leverage the SBERT pre-trained model to establish positive correlations between pairs of categories within the same text, facilitating associative training. For new test datasets, we have improved the original self-training approach, eliminating the need for prior training and testing data from each target dataset. Instead, we adopt Wikipedia as a unified training dataset to better approximate the zero-shot scenario. This modification allows for rapid fine-tuning and inference across different datasets, greatly reducing the time required for self-training. Our experimental results demonstrate that this met
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#25513;&#30721;&#22256;&#38590;&#23454;&#20363;&#25366;&#25496;&#30340;&#22810;&#31034;&#20363;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#23398;&#20064;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#32422;&#26463;&#26469;&#25506;&#32034;&#28508;&#22312;&#30340;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#21160;&#37327;&#25945;&#24072;&#38544;&#24335;&#25366;&#25496;&#36825;&#20123;&#23454;&#20363;&#26469;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15254</link><description>&lt;p&gt;
&#24102;&#26377;&#25513;&#30721;&#22256;&#38590;&#23454;&#20363;&#25366;&#25496;&#30340;&#22810;&#31034;&#20363;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification. (arXiv:2307.15254v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15254
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#25513;&#30721;&#22256;&#38590;&#23454;&#20363;&#25366;&#25496;&#30340;&#22810;&#31034;&#20363;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#23398;&#20064;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#32422;&#26463;&#26469;&#25506;&#32034;&#28508;&#22312;&#30340;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#21160;&#37327;&#25945;&#24072;&#38544;&#24335;&#25366;&#25496;&#36825;&#20123;&#23454;&#20363;&#26469;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#31867;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#38382;&#39064;&#12290;&#30001;&#20110;&#38451;&#24615;&#32452;&#32455;&#20165;&#21344;&#20102;&#21513;&#27604;&#20687;&#32032;WSI&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#29616;&#26377;&#30340;MIL&#26041;&#27861;&#30452;&#35266;&#22320;&#20391;&#37325;&#20110;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#35782;&#21035;&#26174;&#33879;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#23548;&#33268;&#20559;&#21521;&#26131;&#20110;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#24573;&#35270;&#20102;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#12290;&#19968;&#20123;&#25991;&#29486;&#25581;&#31034;&#20102;&#22256;&#38590;&#31034;&#20363;&#23545;&#20110;&#20934;&#30830;&#24314;&#27169;&#36793;&#30028;&#26159;&#26377;&#30410;&#30340;&#12290;&#36890;&#36807;&#23558;&#36825;&#19968;&#24605;&#24819;&#24212;&#29992;&#21040;&#23454;&#20363;&#32423;&#21035;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;MIL&#26694;&#26550;&#65292;&#21363;&#24102;&#26377;&#25513;&#30721;&#22256;&#38590;&#23454;&#20363;&#25366;&#25496;&#30340;MIL&#65288;MHIM-MIL&#65289;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#20849;&#20139;&#23398;&#20064;&#32467;&#26500;&#65288;&#25945;&#24072;-&#23398;&#29983;&#65289;&#21644;&#19968;&#33268;&#24615;&#32422;&#26463;&#26469;&#25506;&#32034;&#28508;&#22312;&#30340;&#22256;&#38590;&#23454;&#20363;&#12290;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#22810;&#20010;&#23454;&#20363;&#25513;&#30721;&#31574;&#30053;&#65292;MHIM-MIL&#37319;&#29992;&#21160;&#37327;&#25945;&#24072;&#26469;&#38544;&#24335;&#25366;&#25496;&#29992;&#20110;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#30340;&#22256;&#38590;&#23454;&#20363;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#26159;&#20219;&#20309;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;MIL&#27169;&#22411;&#12290;&#36825;&#20010;&#21453;&#30452;&#35273;&#30340;&#31574;&#30053;&#23545;&#20110;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The whole slide image (WSI) classification is often formulated as a multiple instance learning (MIL) problem. Since the positive tissue is only a small fraction of the gigapixel WSI,existing MIL methods intuitively focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting hard-to-classify instances.Some literature has revealed that hard examples are beneficial for modeling a discriminative boundary accurately.By applying such an idea at the instance level,we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a consistency constraint to explore the potential hard instances. With several instance masking strategies based on attention scores, MHIM-MIL employs a momentum teacher to implicitly mine hard instances for training the student model, which can be any attention-based MIL model.This counter-intuitive strategy essent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#32479;&#35745;&#24322;&#36136;&#23454;&#39564;&#35774;&#35745;&#19979;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#20855;&#26377;&#33391;&#22909;&#28608;&#21169;&#26426;&#21046;&#30340;FL&#23454;&#39564;&#35774;&#32622;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.15245</link><description>&lt;p&gt;
&#12298;&#22312;&#32479;&#35745;&#24322;&#36136;&#23454;&#39564;&#35774;&#35745;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23454;&#29992;&#37197;&#26041;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design. (arXiv:2307.15245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#32479;&#35745;&#24322;&#36136;&#23454;&#39564;&#35774;&#35745;&#19979;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#20855;&#26377;&#33391;&#22909;&#28608;&#21169;&#26426;&#21046;&#30340;FL&#23454;&#39564;&#35774;&#32622;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#28909;&#28857;&#39046;&#22495;&#12290;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#22312;&#22788;&#29702;&#25968;&#25454;&#24322;&#36136;&#24615;&#26102;&#23545;FL&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#35770;&#25991;&#65292;&#20294;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#29366;&#20917;&#26159;&#26410;&#30693;&#30340;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#20102;&#19981;&#19968;&#33268;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#19988;&#27809;&#26377;&#23545;FL&#29305;&#23450;&#23454;&#39564;&#21464;&#37327;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#20063;&#27809;&#26377;&#25552;&#20379;&#19968;&#20010;&#26356;&#21487;&#27604;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;FL&#23454;&#39564;&#35774;&#32622;&#30340;&#23454;&#36341;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#28151;&#26434;&#21464;&#37327;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#19981;&#19968;&#33268;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;FL&#29305;&#23450;&#23454;&#39564;&#21464;&#37327;&#19982;&#24444;&#27492;&#20043;&#38388;&#21644;&#24615;&#33021;&#32467;&#26524;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#21644;&#24314;&#35758;&#65292;&#20026;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#20855;&#26377;&#33391;&#22909;&#28608;&#21169;&#26426;&#21046;&#30340;FL&#23454;&#39564;&#35774;&#32622;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21457;&#24067;FedZoo-Bench&#26469;&#24110;&#21161;&#31038;&#21306;&#65292;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench,
&lt;/p&gt;</description></item><item><title>BOURNE&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#21161;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#20811;&#26381;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#26816;&#27979;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#23616;&#38480;&#20197;&#21450;&#36127;&#23545;&#37319;&#26679;&#24102;&#26469;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15244</link><description>&lt;p&gt;
BOURNE: &#22522;&#20110;&#33258;&#21161;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BOURNE: Bootstrapped Self-supervised Learning Framework for Unified Graph Anomaly Detection. (arXiv:2307.15244v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15244
&lt;/p&gt;
&lt;p&gt;
BOURNE&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#21161;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#20811;&#26381;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#26816;&#27979;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#23616;&#38480;&#20197;&#21450;&#36127;&#23545;&#37319;&#26679;&#24102;&#26469;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#12289;&#37329;&#34701;&#39118;&#38505;&#31649;&#29702;&#21644;&#27969;&#37327;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#22270;&#24322;&#24120;&#26816;&#27979; (GAD) &#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;GAD&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#34987;&#26816;&#27979;&#30340;&#22270;&#23545;&#35937;&#30340;&#31867;&#22411;&#23558;&#20854;&#20998;&#31867;&#20026;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#35270;&#20026;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#20851;&#32852;&#21644;&#39057;&#32321;&#20849;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#21033;&#29992;&#33410;&#28857;&#21644;&#36793;&#24322;&#24120;&#25552;&#20379;&#30340;&#20114;&#30456;&#26816;&#27979;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#26368;&#20808;&#36827;&#30340;GAD&#26041;&#27861;&#65292;&#22914;CoLA&#21644;SL-GAD&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#20005;&#37325;&#20381;&#36182;&#36127;&#23545;&#37319;&#26679;&#65292;&#36825;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21161;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32479;&#19968;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;BOURNE&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) has gained increasing attention in recent years due to its critical application in a wide range of domains, such as social networks, financial risk management, and traffic analysis. Existing GAD methods can be categorized into node and edge anomaly detection models based on the type of graph objects being detected. However, these methods typically treat node and edge anomalies as separate tasks, overlooking their associations and frequent co-occurrences in real-world graphs. As a result, they fail to leverage the complementary information provided by node and edge anomalies for mutual detection. Additionally, state-of-the-art GAD methods, such as CoLA and SL-GAD, heavily rely on negative pair sampling in contrastive learning, which incurs high computational costs, hindering their scalability to large graphs. To address these limitations, we propose a novel unified graph anomaly detection framework based on bootstrapped self-supervised learning (named BOURN
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15220</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25968;&#30334;&#20010;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15220
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#38752;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20027;&#35201;&#20351;&#29992;&#35270;&#35273;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#25163;&#26415;&#35270;&#39057;&#26469;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26410;&#35265;&#25163;&#26415;&#31243;&#24207;&#21644;&#21518;&#32493;&#20219;&#21153;&#19978;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#36890;&#36807;&#24320;&#25918;&#30340;&#25163;&#26415;&#30005;&#23376;&#23398;&#20064;&#24179;&#21488;&#25552;&#20379;&#30340;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#21487;&#20197;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#26377;&#25928;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20114;&#34917;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#29983;&#25104;&#25991;&#26412;&#36716;&#24405;&#26469;&#35299;&#20915;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#23384;&#22312;&#30340;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;SurgVLP - &#25163;&#26415;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;SurgVLP&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#35270;&#39057;&#21098;&#36753;&#23884;&#20837;&#19982;&#30456;&#24212;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#31454;&#20215;&#28216;&#25103;&#30340;&#22270;&#28216;&#25103;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#36139;&#27665;&#31163;&#25955;&#31454;&#20215;&#65292;&#23545;&#20986;&#20215;&#36827;&#34892;&#20102;&#31890;&#24230;&#38480;&#21046;&#65292;&#24182;&#19988;&#36739;&#39640;&#30340;&#20986;&#20215;&#25903;&#20184;&#32473;&#38134;&#34892;&#12290;&#30740;&#31350;&#20851;&#27880;&#38408;&#20540;&#39044;&#31639;&#65292;&#20197;&#30830;&#20445;&#29609;&#23478;1&#23545;&#25239;&#32473;&#23450;&#29609;&#23478;2&#30340;&#39044;&#31639;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.15218</link><description>&lt;p&gt;
&#21487;&#36798;&#24615;&#36139;&#27665;&#31163;&#25955;&#31454;&#20215;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Reachability Poorman Discrete-Bidding Games. (arXiv:2307.15218v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15218
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#31454;&#20215;&#28216;&#25103;&#30340;&#22270;&#28216;&#25103;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#36139;&#27665;&#31163;&#25955;&#31454;&#20215;&#65292;&#23545;&#20986;&#20215;&#36827;&#34892;&#20102;&#31890;&#24230;&#38480;&#21046;&#65292;&#24182;&#19988;&#36739;&#39640;&#30340;&#20986;&#20215;&#25903;&#20184;&#32473;&#38134;&#34892;&#12290;&#30740;&#31350;&#20851;&#27880;&#38408;&#20540;&#39044;&#31639;&#65292;&#20197;&#30830;&#20445;&#29609;&#23478;1&#23545;&#25239;&#32473;&#23450;&#29609;&#23478;2&#30340;&#39044;&#31639;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31867;&#21452;&#20154;&#38646;&#21644;&#22270;&#28216;&#25103;&#65292;&#31216;&#20026;&#31454;&#20215;&#28216;&#25103;&#12290;&#28216;&#25103;&#30340;&#36827;&#34892;&#22914;&#19979;&#65306;&#20004;&#20301;&#29609;&#23478;&#25317;&#26377;&#26377;&#38480;&#30340;&#39044;&#31639;&#65292;&#22312;&#22270;&#30340;&#19968;&#20010;&#39030;&#28857;&#19978;&#25918;&#32622;&#19968;&#20010;&#20196;&#29260;&#65292;&#27599;&#36718;&#29609;&#23478;&#21516;&#26102;&#25552;&#20132;&#31454;&#20215;&#65292;&#20986;&#20215;&#36739;&#39640;&#32773;&#31227;&#21160;&#20196;&#29260;&#65292;&#22914;&#26524;&#20986;&#20215;&#30456;&#21516;&#65292;&#21017;&#26377;&#21033;&#20110;&#29609;&#23478;1&#12290;&#21482;&#26377;&#24403;&#20196;&#29260;&#35775;&#38382;&#21040;&#25351;&#23450;&#30340;&#30446;&#26631;&#39030;&#28857;&#26102;&#65292;&#29609;&#23478;1&#25165;&#33021;&#36194;&#24471;&#28216;&#25103;&#12290;&#25105;&#20204;&#39318;&#27425;&#32771;&#34385;&#20102;&#8220;&#36139;&#27665;&#31163;&#25955;&#31454;&#20215;&#8221;&#65292;&#20854;&#20013;&#31454;&#20215;&#30340;&#31890;&#24230;&#21463;&#38480;&#21046;&#65292;&#36739;&#39640;&#30340;&#20986;&#20215;&#25903;&#20184;&#32473;&#38134;&#34892;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#27809;&#26377;&#26045;&#21152;&#31890;&#24230;&#38480;&#21046;&#65292;&#35201;&#20040;&#32771;&#34385;&#20102;&#8220;&#23500;&#32705;&#8221;&#31454;&#20215;&#65288;&#20986;&#20215;&#25903;&#20184;&#32473;&#23545;&#25163;&#65289;&#12290;&#34429;&#28982;&#21518;&#19968;&#31181;&#26426;&#21046;&#22312;&#25216;&#26415;&#19978;&#26356;&#26131;&#20110;&#29702;&#35299;&#65292;&#20294;&#21069;&#32773;&#22312;&#23454;&#36341;&#35282;&#24230;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#8220;&#38408;&#20540;&#39044;&#31639;&#8221;&#65292;&#21363;&#29609;&#23478;1&#20026;&#30830;&#20445;&#23545;&#25239;&#32473;&#23450;&#29609;&#23478;2&#30340;&#39044;&#31639;&#32780;&#24517;&#38656;&#30340;&#21021;&#22987;&#39044;&#31639;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#23384;&#22312;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider {\em bidding games}, a class of two-player zero-sum {\em graph games}. The game proceeds as follows. Both players have bounded budgets. A token is placed on a vertex of a graph, in each turn the players simultaneously submit bids, and the higher bidder moves the token, where we break bidding ties in favor of Player 1. Player 1 wins the game iff the token visits a designated target vertex. We consider, for the first time, {\em poorman discrete-bidding} in which the granularity of the bids is restricted and the higher bid is paid to the bank. Previous work either did not impose granularity restrictions or considered {\em Richman} bidding (bids are paid to the opponent). While the latter mechanisms are technically more accessible, the former is more appealing from a practical standpoint. Our study focuses on {\em threshold budgets}, which is the necessary and sufficient initial budget required for Player 1 to ensure winning against a given Player 2 budget. We first show existe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.15217</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#30340;&#25216;&#26415;&#12290;RLHF&#24050;&#25104;&#20026;&#24494;&#35843;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26680;&#24515;&#26041;&#27861;&#12290;&#23613;&#31649;&#22914;&#27492;&#21463;&#27426;&#36814;&#65292;&#20294;&#31995;&#32479;&#24615;&#22320;&#31995;&#32479;&#21270;&#20854;&#32570;&#38519;&#30340;&#20844;&#24320;&#24037;&#20316;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#35843;&#26597;&#20102;RLHF&#21450;&#30456;&#20851;&#26041;&#27861;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27010;&#36848;&#20102;&#20102;&#35299;&#12289;&#25913;&#36827;&#21644;&#34917;&#20805;RLHF&#30340;&#23454;&#36341;&#25216;&#26415;&#65307;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20986;&#20102;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#20197;&#25913;&#36827;RLHF&#31995;&#32479;&#30340;&#31038;&#20250;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;RLHF&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20197;&#22810;&#26041;&#38754;&#26041;&#27861;&#24320;&#21457;&#26356;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15199</link><description>&lt;p&gt;
PromptStyler&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#28304;&#22495;&#27867;&#21270;&#39118;&#26684;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#65292;&#25991;&#26412;&#29305;&#24449;&#65288;&#22914;&#8220;&#19968;&#24352;&#29399;&#30340;&#29031;&#29255;&#8221;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20854;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#29399;&#30340;&#29031;&#29255;&#65289;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#26469;&#21512;&#25104;&#21508;&#31181;&#26679;&#24335;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#26469;&#22788;&#29702;&#26080;&#28304;&#22495;&#27867;&#21270;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#26679;&#24335;&#35789;&#21521;&#37327;&#20026;&#20266;&#35789;S*&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a&#8221;&#65289;&#12290;&#20026;&#20102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a [class]&#8221;&#65289;&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#38752;&#36817;&#20854;&#23545;&#24212;&#30340;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;[class]&#8221;&#65289;&#12290;&#22312;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;PromptStyler&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20294;&#22312;PACS&#12289;VLCS&#12289;OfficeHome&#21644;DomainNet&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Inspired by this, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. Our method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not require any images and take
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#21333;&#27425;&#32852;&#21512;&#25552;&#21462;&#12289;&#27880;&#20876;&#21644;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#27169;&#26495;&#22270;&#20687;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#26679;&#26412;&#21644;&#25163;&#21160;&#36136;&#37327;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15198</link><description>&lt;p&gt;
&#21333;&#27425;&#32852;&#21512;&#25552;&#21462;&#12289;&#27880;&#20876;&#21644;&#20998;&#21106;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data. (arXiv:2307.15198v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#21333;&#27425;&#32852;&#21512;&#25552;&#21462;&#12289;&#27880;&#20876;&#21644;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#27169;&#26495;&#22270;&#20687;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#26679;&#26412;&#21644;&#25163;&#21160;&#36136;&#37327;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#25552;&#21462;&#12289;&#27880;&#20876;&#21644;&#20998;&#21106;&#26159;&#31070;&#32463;&#24433;&#20687;&#30740;&#31350;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#20854;&#30446;&#30340;&#26159;&#20174;&#21407;&#22987;&#25104;&#20687;&#25195;&#25551;&#20013;&#25552;&#21462;&#20986;&#22823;&#33041;&#65288;&#25552;&#21462;&#27493;&#39588;&#65289;&#65292;&#23558;&#20854;&#19982;&#30446;&#26631;&#22823;&#33041;&#22270;&#20687;&#23545;&#40784;&#65288;&#27880;&#20876;&#27493;&#39588;&#65289;&#65292;&#24182;&#26631;&#35760;&#35299;&#21078;&#23398;&#22823;&#33041;&#21306;&#22495;&#65288;&#20998;&#21106;&#27493;&#39588;&#65289;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#19987;&#27880;&#20110;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#20026;&#25552;&#21462;&#12289;&#27880;&#20876;&#21644;&#20998;&#21106;&#20219;&#21153;&#24320;&#21457;&#29420;&#31435;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#21644;&#19987;&#23478;&#36827;&#34892;&#35270;&#35273;&#26816;&#26597;&#20197;&#36827;&#34892;&#38169;&#35823;&#26356;&#27491;&#30340;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#21307;&#23398;&#30740;&#31350;&#20013;&#65292;&#25910;&#38598;&#20307;&#32032;&#32423;&#26631;&#31614;&#24182;&#23545;&#39640;&#32500;&#31070;&#32463;&#24433;&#20687;&#65288;&#20363;&#22914;3D MRI&#65289;&#36827;&#34892;&#25163;&#21160;&#36136;&#37327;&#25511;&#21046;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#21333;&#27425;&#32852;&#21512;&#25552;&#21462;&#12289;&#27880;&#20876;&#21644;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20165;&#21033;&#29992;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#27169;&#26495;&#22270;&#20687;&#65288;&#20063;&#31216;&#20026;
&lt;/p&gt;
&lt;p&gt;
Brain extraction, registration and segmentation are indispensable preprocessing steps in neuroimaging studies. The aim is to extract the brain from raw imaging scans (i.e., extraction step), align it with a target brain image (i.e., registration step) and label the anatomical brain regions (i.e., segmentation step). Conventional studies typically focus on developing separate methods for the extraction, registration and segmentation tasks in a supervised setting. The performance of these methods is largely contingent on the quantity of training samples and the extent of visual inspections carried out by experts for error correction. Nevertheless, collecting voxel-level labels and performing manual quality control on high-dimensional neuroimages (e.g., 3D MRI) are expensive and time-consuming in many medical studies. In this paper, we study the problem of one-shot joint extraction, registration and segmentation in neuroimaging data, which exploits only one labeled template image (a.k.a. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15193</link><description>&lt;p&gt;
&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#30899;&#25490;&#25918;&#20132;&#26131;&#26041;&#26696;&#12289;&#22269;&#20538;&#25293;&#21334;&#21644;&#37319;&#36141;&#25293;&#21334;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#37117;&#28041;&#21450;&#25293;&#21334;&#21516;&#36136;&#30340;&#22810;&#20010;&#21333;&#20301;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22914;&#20309;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#25293;&#21334;&#20013;&#65292;&#22823;&#37327;&#65288;&#30456;&#21516;&#30340;&#65289;&#29289;&#21697;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#39640;&#30340;&#20986;&#20215;&#65292;&#27599;&#20010;&#20013;&#26631;&#20215;&#31561;&#20110;&#20986;&#20215;&#26412;&#36523;&#12290;&#30001;&#20110;&#34892;&#21160;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#23398;&#20064;&#22914;&#20309;&#22312;&#20184;&#36153;&#25293;&#21334;&#20013;&#20986;&#20215;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#25237;&#26631;&#20154;&#36890;&#36807;&#21482;&#33021;&#35775;&#38382;&#20854;&#20182;&#25237;&#26631;&#20154;&#36807;&#21435;&#25552;&#20132;&#30340;&#20986;&#20215;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;&#20986;&#20215;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#26041;&#26696;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#21033;&#29992;DP&#26041;&#26696;&#30340;&#32467;&#26500;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under fu
&lt;/p&gt;</description></item><item><title>Med-Flamingo&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#21307;&#23398;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#29983;&#25104;&#24335;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15189</link><description>&lt;p&gt;
Med-Flamingo: &#19968;&#31181;&#22810;&#27169;&#24577;&#21307;&#23398;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Med-Flamingo: a Multimodal Medical Few-shot Learner. (arXiv:2307.15189v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15189
&lt;/p&gt;
&lt;p&gt;
Med-Flamingo&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#21307;&#23398;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#29983;&#25104;&#24335;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#26159;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#39046;&#22495;&#65292;&#38656;&#35201;&#36328;&#22810;&#31181;&#24418;&#24335;&#30340;&#20449;&#24687;&#36827;&#34892;&#32508;&#21512;&#12290;&#21307;&#23398;&#29983;&#25104;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#24182;&#25215;&#35834;&#26377;&#35768;&#22810;&#20196;&#20154;&#20852;&#22859;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#22823;&#38480;&#21046;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#24456;&#23569;&#65292;&#38656;&#35201;&#33021;&#22815;&#20174;&#23569;&#37327;&#23454;&#20363;&#20013;&#23454;&#26102;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Med-Flamingo&#65292;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;&#22312;OpenFlamingo-9B&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#32487;&#32493;&#23545;&#21307;&#23398;&#22270;&#25991;&#25968;&#25454;&#36827;&#34892;&#37197;&#23545;&#21644;&#20132;&#38169;&#30340;&#39044;&#35757;&#32451;&#65292;Med-Flamingo&#35299;&#38145;&#20102;&#23569;&#26679;&#26412;&#29983;&#25104;&#24335;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24335;VQA&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#35270;&#35273;USMLE&#39118;&#26684;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#20010;&#20154;&#24037;&#35780;&#20272;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models (VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering (VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.15176</link><description>&lt;p&gt;
RCT&#25298;&#32477;&#25277;&#26679;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26080;&#20559;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#39640;&#32500;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#22914;&#25991;&#26412;&#25968;&#25454;&#12289;&#22522;&#22240;&#32452;&#23398;&#25110;&#34892;&#20026;&#31038;&#20250;&#31185;&#23398;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#25972;&#26041;&#27861;&#30340;&#32463;&#39564;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32463;&#39564;&#35780;&#20272;&#31574;&#30053;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#65306;&#23545;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#36827;&#34892;&#23376;&#25277;&#26679;&#65292;&#20197;&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#22240;&#26524;&#35782;&#21035;&#25104;&#31435;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#22522;&#20934;RCT&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#23884;&#20837;&#34920;&#31034;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#23545;&#35805;&#20013;&#24773;&#32490;&#34920;&#36798;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23567;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#28151;&#21512;&#30446;&#26631;&#24773;&#32490;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.15164</link><description>&lt;p&gt;
VISU&#21442;&#21152;WASSA 2023&#20849;&#20139;&#20219;&#21153;&#65306;&#21033;&#29992;BERT&#21644;&#22534;&#21472;&#23884;&#20837;&#26816;&#27979;&#23545;&#26032;&#38395;&#25925;&#20107;&#30340;&#24773;&#32490;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings. (arXiv:2307.15164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#23884;&#20837;&#34920;&#31034;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#23545;&#35805;&#20013;&#24773;&#32490;&#34920;&#36798;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23567;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#28151;&#21512;&#30446;&#26631;&#24773;&#32490;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#31995;&#32479;VISU&#21442;&#21152;&#20102;WASSA 2023&#20849;&#20139;&#20219;&#21153;&#65288;3&#65289;&#65292;&#21363;&#20174;&#23545;&#26032;&#38395;&#25991;&#31456;&#30340;&#21453;&#24212;&#20013;&#20889;&#30340;&#25991;&#31456;&#20013;&#36827;&#34892;&#24773;&#32490;&#20998;&#31867;&#12290;&#20174;&#22797;&#26434;&#23545;&#35805;&#20013;&#26816;&#27979;&#24773;&#32490;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#19978;&#19979;&#25991;/&#39046;&#22495;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#19982;&#35789;&#23884;&#20837;&#34920;&#31034;&#30340;&#32452;&#21512;&#26469;&#25429;&#25417;&#34920;&#36798;&#30340;&#24773;&#32490;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#65288;&#21333;&#29420;&#21644;&#22534;&#21472;&#65289;&#19982;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#21644;Transformer&#27169;&#22411;&#12290;&#22312;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21344;&#25454;&#20102;&#31532;&#21313;&#21517;&#65292;&#24471;&#20998;&#20026;0.2717&#30340;&#23439;F1-&#20998;&#25968;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#23454;&#26045;&#30340;&#26041;&#27861;&#22312;&#23567;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#28151;&#21512;&#30446;&#26631;&#24773;&#32490;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion Classification from essays written in reaction to news articles. Emotion detection from complex dialogues is challenging and often requires context/domain understanding. Therefore in this research, we have focused on developing deep learning (DL) models using the combination of word embedding representations with tailored prepossessing strategies to capture the nuances of emotions expressed. Our experiments used static and contextual embeddings (individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and Transformer based models. We occupied rank tenth in the emotion detection task by scoring a Macro F1-Score of 0.2717, validating the efficacy of our implemented approaches for small and imbalanced datasets with mixed categories of target emotions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#20013;&#30340;&#40614;&#26684;&#23572;&#20811;&#25928;&#24212;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#21644;Hebb&#36830;&#25509;&#22312;ReD-SOM&#27169;&#22411;&#20013;&#37325;&#24314;&#20002;&#22833;&#25968;&#25454;&#27169;&#24577;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15095</link><description>&lt;p&gt;
&#21463;&#30382;&#23618;&#21551;&#21457;&#30340;&#20351;&#29992;ReD-SOM&#27169;&#22411;&#24674;&#22797;&#21463;&#25439;&#20449;&#21495;&#27169;&#24577;&#24615;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cortex Inspired Learning to Recover Damaged Signal Modality with ReD-SOM Model. (arXiv:2307.15095v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15095
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#20013;&#30340;&#40614;&#26684;&#23572;&#20811;&#25928;&#24212;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#21644;Hebb&#36830;&#25509;&#22312;ReD-SOM&#27169;&#22411;&#20013;&#37325;&#24314;&#20002;&#22833;&#25968;&#25454;&#27169;&#24577;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#35748;&#30693;&#31185;&#23398;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#24320;&#36767;&#20102;&#20197;&#21069;&#26080;&#27861;&#30740;&#31350;&#30340;&#26032;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#39033;&#29616;&#20195;&#20219;&#21153;&#26159;&#36890;&#36807;&#20351;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#25968;&#25454;&#26469;&#24674;&#22797;&#20002;&#22833;&#30340;&#25968;&#25454;&#27169;&#24577;&#12290;&#20154;&#33041;&#30340;&#21151;&#33021;&#20013;&#23384;&#22312;&#19968;&#31181;&#31867;&#20284;&#25928;&#24212;&#65288;&#31216;&#20026;&#40614;&#26684;&#23572;&#20811;&#25928;&#24212;&#65289;&#65292;&#20854;&#20013;&#19968;&#31181;&#20449;&#24687;&#27169;&#24577;&#24178;&#25200;&#21478;&#19968;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#25913;&#21464;&#20854;&#24863;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#27169;&#25311;&#36825;&#31181;&#25928;&#24212;&#24182;&#23558;&#20854;&#29992;&#20110;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#33258;&#32452;&#32455;&#26144;&#23556;&#21644;Hebb&#36830;&#25509;&#22312;&#32479;&#19968;&#30340;ReD-SOM&#65288;&#37325;&#26032;&#36827;&#20837;&#28145;&#24230;&#33258;&#32452;&#32455;&#26144;&#23556;&#65289;&#27169;&#22411;&#20013;&#37325;&#24314;&#20002;&#22833;&#30340;&#25968;&#25454;&#27169;&#24577;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#20351;&#29992;&#19981;&#21516;&#33041;&#21306;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#21363;&#22312;&#26576;&#19968;&#27169;&#24577;&#20013;&#32570;&#20047;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#27169;&#31946;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#32780;&#19988;&#36824;&#24674;&#22797;&#20102;&#39044;&#26399;&#30340;&#20449;&#21495;&#65281;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in the fields of AI and cognitive sciences opens up new challenges that were previously inaccessible to study. One of such modern tasks is recovering lost data of one modality by using the data from another one. A similar effect (called the McGurk Effect) has been found in the functioning of the human brain. Observing this effect, one modality of information interferes with another, changing its perception. In this paper, we propose a way to simulate such an effect and use it to reconstruct lost data modalities by combining Variational Auto-Encoders, Self-Organizing Maps, and Hebb connections in a unified ReD-SOM (Reentering Deep Self-organizing Map) model. We are inspired by human's capability to use different zones of the brain in different modalities, in case of having a lack of information in one of the modalities. This new approach not only improves the analysis of ambiguous data but also restores the intended signal! The results obtained on the multimodal dataset 
&lt;/p&gt;</description></item><item><title>&#27785;&#31215;&#35745;&#31639;&#26159;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#36830;&#25509;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#21644;&#20016;&#23500;&#30340;&#21160;&#21147;&#23398;&#20351;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33021;&#22815;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#23427;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#21253;&#25324;&#29289;&#29702;&#30828;&#20214;&#23454;&#29616;&#21644;&#29983;&#29289;&#35774;&#22791;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.15092</link><description>&lt;p&gt;
&#20851;&#20110;&#27785;&#31215;&#35745;&#31639;&#21450;&#20854;&#36229;&#36234;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning. (arXiv:2307.15092v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15092
&lt;/p&gt;
&lt;p&gt;
&#27785;&#31215;&#35745;&#31639;&#26159;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#36830;&#25509;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#21644;&#20016;&#23500;&#30340;&#21160;&#21147;&#23398;&#20351;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33021;&#22815;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#23427;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#21253;&#25324;&#29289;&#29702;&#30828;&#20214;&#23454;&#29616;&#21644;&#29983;&#29289;&#35774;&#22791;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27785;&#31215;&#35745;&#31639;&#65288;RC&#65289;&#39318;&#20808;&#29992;&#20110;&#26102;&#38388;&#20449;&#21495;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#36830;&#25509;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#19968;&#26086;&#21021;&#22987;&#21270;&#65292;&#36830;&#25509;&#24378;&#24230;&#23558;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#32467;&#26500;&#23558;RC&#36716;&#21270;&#20026;&#19968;&#20010;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#65292;&#23558;&#20302;&#32500;&#36755;&#20837;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#12290;&#35813;&#27169;&#22411;&#30340;&#20016;&#23500;&#21160;&#21147;&#23398;&#12289;&#32447;&#24615;&#21487;&#20998;&#24615;&#21644;&#35760;&#24518;&#23481;&#37327;&#20351;&#24471;&#31616;&#21333;&#30340;&#32447;&#24615;&#35835;&#20986;&#33021;&#22815;&#20026;&#21508;&#31181;&#24212;&#29992;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;RC&#28085;&#30422;&#20102;&#36828;&#36828;&#36229;&#20986;&#26426;&#22120;&#23398;&#20064;&#33539;&#22260;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#24050;&#32463;&#35777;&#26126;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#21487;&#20197;&#22312;&#21508;&#31181;&#29289;&#29702;&#30828;&#20214;&#23454;&#29616;&#21644;&#29983;&#29289;&#35774;&#22791;&#20013;&#23454;&#29616;&#12290;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#21160;&#21147;&#23398;&#35302;&#21457;&#30340;&#31070;&#32463;&#20803;&#21709;&#24212;&#20063;&#25581;&#31034;&#20102;&#29702;&#35299;&#22823;&#33041;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26426;&#21046;&#20063;&#21033;&#29992;&#31867;&#20284;&#30340;&#21160;&#21147;&#23398;&#36807;&#31243;&#12290;&#34429;&#28982;&#20851;&#20110;RC&#30340;&#25991;&#29486;&#38750;&#24120;&#24222;&#22823;&#19988;&#30862;&#29255;&#21270;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#23545;RC&#30340;&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#32479;&#19968;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing (RC), first applied to temporal signal processing, is a recurrent neural network in which neurons are randomly connected. Once initialized, the connection strengths remain unchanged. Such a simple structure turns RC into a non-linear dynamical system that maps low-dimensional inputs into a high-dimensional space. The model's rich dynamics, linear separability, and memory capacity then enable a simple linear readout to generate adequate responses for various applications. RC spans areas far beyond machine learning, since it has been shown that the complex dynamics can be realized in various physical hardware implementations and biological devices. This yields greater flexibility and shorter computation time. Moreover, the neuronal responses triggered by the model's dynamics shed light on understanding brain mechanisms that also exploit similar dynamical processes. While the literature on RC is vast and fragmented, here we conduct a unified review of RC's recent devel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#23398;&#24037;&#20855;&#23545;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15090</link><description>&lt;p&gt;
&#29702;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Understanding Forward Process of Convolutional Neural Network. (arXiv:2307.15090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#23398;&#24037;&#20855;&#23545;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#12290;&#23427;&#38416;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#20316;&#20026;&#19968;&#20010;&#20998;&#26512;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#26059;&#36716;&#26041;&#38754;&#32479;&#19968;&#37327;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#23450;&#20041;&#30340;&#26041;&#27861;&#35770;&#21453;&#26144;&#20102;&#36827;&#31243;&#32593;&#32476;&#26681;&#25454;&#32479;&#35745;&#25351;&#26631;&#21306;&#20998;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#32467;&#26500;&#21270;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#29702;&#35299;&#25110;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#25581;&#31034;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reveal the selective rotation in the CNNs' forward processing. It elucidates the activation function as a discerning mechanism that unifies and quantizes the rotational aspects of the input data. Experiments show how this defined methodology reflects the progress network distinguish inputs based on statistical indicators, which can be comprehended or analyzed by applying structured mathematical tools. Our findings also unveil the consistency between artificial neural networks and the human brain in their data processing pattern.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#20449;&#24687;&#25552;&#21319;&#30340;&#26041;&#27861;&#36827;&#34892;&#23376;&#32676;&#21457;&#29616;&#12290;&#20855;&#20307;&#38024;&#23545;&#32954;&#30284;&#27835;&#30103;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#21103;&#20316;&#29992;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#65292;&#20020;&#24202;&#25351;&#21335;&#34429;&#28982;&#25552;&#20379;&#20102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#20173;&#26410;&#23558;&#27835;&#30103;&#32467;&#26524;&#32435;&#20837;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15089</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#25552;&#21319;&#23376;&#32676;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Information Gained Subgroup Discovery in Datasets. (arXiv:2307.15089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#20449;&#24687;&#25552;&#21319;&#30340;&#26041;&#27861;&#36827;&#34892;&#23376;&#32676;&#21457;&#29616;&#12290;&#20855;&#20307;&#38024;&#23545;&#32954;&#30284;&#27835;&#30103;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#21103;&#20316;&#29992;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#65292;&#20020;&#24202;&#25351;&#21335;&#34429;&#28982;&#25552;&#20379;&#20102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#20173;&#26410;&#23558;&#27835;&#30103;&#32467;&#26524;&#32435;&#20837;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#39044;&#35745;2023&#24180;&#23558;&#26377;&#36229;&#36807;238,340&#20363;&#26032;&#30340;&#32954;&#30284;&#24739;&#32773;&#65292;&#20854;&#20013;&#26377;&#36229;&#36807;127,070&#20363;&#27515;&#20129;&#12290;&#36873;&#25321;&#27491;&#30830;&#30340;&#27835;&#30103;&#26041;&#26696;&#26159;&#25552;&#39640;&#23384;&#27963;&#29575;&#21644;&#25913;&#21892;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#30284;&#30151;&#27835;&#30103;&#21487;&#33021;&#24341;&#21457;&#21103;&#20316;&#29992;&#65292;&#36825;&#20123;&#27602;&#21103;&#21453;&#24212;&#20250;&#24341;&#36215;&#19981;&#21516;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24433;&#21709;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#27835;&#30103;&#21103;&#20316;&#29992;&#26159;&#20020;&#24202;&#35282;&#24230;&#35201;&#36861;&#27714;&#30340;&#37325;&#35201;&#30446;&#26631;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20020;&#24202;&#25351;&#21335;&#21253;&#25324;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#20197;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#12290;&#23613;&#31649;&#20182;&#20204;&#26681;&#25454;&#30284;&#30151;&#30142;&#30149;&#26041;&#38754;&#21644;&#20010;&#20307;&#24739;&#32773;&#29305;&#24449;&#25552;&#20379;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#24182;&#26410;&#25552;&#20379;&#22522;&#20110;&#27835;&#30103;&#32467;&#26524;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#20020;&#24202;&#25351;&#21335;&#19982;&#27835;&#30103;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is the leading cause of cancer death. More than 238,340 new cases of lung cancer patients are expected in 2023, with an estimation of more than 127,070 deaths. Choosing the correct treatment is an important element to enhance the probability of survival and to improve patient's quality of life. Cancer treatments might provoke secondary effects. These toxicities cause different health problems that impact the patient's quality of life. Hence, reducing treatments toxicities while maintaining or improving their effectivenes is an important goal that aims to be pursued from the clinical perspective. On the other hand, clinical guidelines include general knowledge about cancer treatment recommendations to assist clinicians. Although they provide treatment recommendations based on cancer disease aspects and individual patient features, a statistical analysis taking into account treatment outcomes is not provided here. Therefore, the comparison between clinical guidelines with tre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15051</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#24739;&#32773;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;TrialGPT&#65292;&#37319;&#29992;LLMs&#39044;&#27979;&#22522;&#20110;&#26631;&#20934;&#30340;&#21512;&#26684;&#24615;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#26469;&#23545;&#20505;&#36873;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#25490;&#21517;&#21644;&#25490;&#38500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;184&#21517;&#24739;&#32773;&#21644;18,238&#20010;&#27880;&#37322;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#38431;&#21015;&#19978;&#35780;&#20272;&#20102;TrialGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;TrialGPT&#22312;&#26631;&#20934;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#31532;&#20108;&#65292;TrialGPT&#30340;&#32508;&#21512;&#35797;&#39564;&#32423;&#21035;&#35780;&#20998;&#19982;&#19987;&#23478;&#26631;&#27880;&#30340;&#21512;&#26684;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
&lt;/p&gt;</description></item><item><title>Desbordante&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#26469;&#24110;&#21161;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#36827;&#34892;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;&#29616;&#26377;&#24037;&#20855;&#30340;&#36866;&#24403;&#38598;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#24335;&#32570;&#22833;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2307.14935</link><description>&lt;p&gt;
&#20351;&#29992;Desbordante&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65306;&#19968;&#39033;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14935
&lt;/p&gt;
&lt;p&gt;
Desbordante&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#26469;&#24110;&#21161;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#36827;&#34892;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;&#29616;&#26377;&#24037;&#20855;&#30340;&#36866;&#24403;&#38598;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#24335;&#32570;&#22833;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#26159;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#34892;&#19994;&#20013;&#30340;&#37325;&#35201;&#36807;&#31243;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#65292;&#21253;&#25324;&#20989;&#25968;&#20381;&#36182;&#12289;&#25968;&#25454;&#32422;&#26463;&#12289;&#20851;&#32852;&#35268;&#21017;&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#22797;&#26434;&#32479;&#35745;&#30340;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#31995;&#32479;&#22312;&#19982;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#20351;&#29992;&#30340;&#24037;&#20855;&#36827;&#34892;&#36866;&#24403;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#22312;&#34892;&#19994;&#20013;&#23545;&#36825;&#20123;&#24037;&#20855;&#37319;&#29992;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#31995;&#32479;&#24182;&#19981;&#32771;&#34385;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#19981;&#26088;&#22312;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#65292;&#21363;&#20026;&#20160;&#20040;&#25214;&#19981;&#21040;&#32473;&#23450;&#30340;&#27169;&#24335;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22240;&#20026;&#20102;&#35299;&#29305;&#23450;&#27169;&#24335;&#32570;&#22833;&#30340;&#26681;&#26412;&#21407;&#22240;&#23545;&#22522;&#20110;&#25968;&#25454;&#30340;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#24335;&#23454;&#38469;&#19978;&#26159;&#28040;&#22833;&#22312;&#31354;&#20013;&#20013;&#65306;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#30456;&#23545;&#26377;&#38480;&#65292;&#24456;&#23569;&#34987;&#24191;&#22823;&#20844;&#20247;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.  However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.  Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;ASP&#26041;&#27861;&#35299;&#20915;&#20102;&#23454;&#38469;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#23558;&#20854;&#20855;&#20307;&#35201;&#27714;&#24314;&#27169;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#26426;&#22120;&#21152;&#24037;&#12289;&#35774;&#32622;&#12289;&#25209;&#22788;&#29702;&#21644;&#32500;&#25252;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.14799</link><description>&lt;p&gt;
&#28151;&#21512;ASP&#26041;&#27861;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#22810;&#30446;&#26631;&#35843;&#24230;&#65288;&#25193;&#23637;&#29256;&#26412;&#65289;
&lt;/p&gt;
&lt;p&gt;
Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version). (arXiv:2307.14799v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;ASP&#26041;&#27861;&#35299;&#20915;&#20102;&#23454;&#38469;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#23558;&#20854;&#20855;&#20307;&#35201;&#27714;&#24314;&#27169;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#26426;&#22120;&#21152;&#24037;&#12289;&#35774;&#32622;&#12289;&#25209;&#22788;&#29702;&#21644;&#32500;&#25252;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21322;&#23548;&#20307;&#21046;&#36896;&#28041;&#21450;&#22797;&#26434;&#30340;&#29983;&#20135;&#36807;&#31243;&#65292;&#21253;&#25324;&#25968;&#30334;&#20010;&#25805;&#20316;&#65292;&#20174;&#25209;&#27425;&#21457;&#24067;&#21040;&#23436;&#25104;&#21487;&#33021;&#38656;&#35201;&#25968;&#26376;&#26102;&#38388;&#12290;&#36825;&#20123;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#39640;&#31185;&#25216;&#35774;&#22791;&#22810;&#26679;&#21270;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#38454;&#27573;&#19978;&#23545;&#21333;&#20010;&#26230;&#22278;&#12289;&#25209;&#27425;&#25110;&#25209;&#27425;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#38656;&#35201;&#20135;&#21697;&#29305;&#23450;&#30340;&#35774;&#32622;&#21644;&#19987;&#38376;&#30340;&#32500;&#25252;&#31243;&#24207;&#12290;&#36825;&#31181;&#24773;&#20917;&#19982;&#20256;&#32479;&#30340;&#36710;&#38388;&#35843;&#24230;&#22330;&#26223;&#19981;&#21516;&#65292;&#21518;&#32773;&#20855;&#26377;&#36739;&#19981;&#22797;&#26434;&#30340;&#29983;&#20135;&#36807;&#31243;&#21644;&#35774;&#22791;&#65292;&#20027;&#35201;&#20851;&#27880;&#35299;&#20915;&#39640;&#24230;&#32452;&#21512;&#20294;&#25277;&#35937;&#30340;&#35843;&#24230;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24046;&#24322;&#36923;&#36753;&#30340;&#28151;&#21512;ASP&#27169;&#22411;&#26469;&#23545;&#23454;&#38469;&#30340;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#35843;&#24230;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#21253;&#25324;&#28789;&#27963;&#30340;&#26426;&#22120;&#21152;&#24037;&#12289;&#35774;&#32622;&#12289;&#25209;&#22788;&#29702;&#21644;&#32500;&#25252;&#25805;&#20316;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#36138;&#23146;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#29420;&#31435;&#36827;&#34892;&#35843;&#24230;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
Modern semiconductor manufacturing involves intricate production processes consisting of hundreds of operations, which can take several months from lot release to completion. The high-tech machines used in these processes are diverse, operate on individual wafers, lots, or batches in multiple stages, and necessitate product-specific setups and specialized maintenance procedures. This situation is different from traditional job-shop scheduling scenarios, which have less complex production processes and machines, and mainly focus on solving highly combinatorial but abstract scheduling problems. In this work, we address the scheduling of realistic semiconductor manufacturing processes by modeling their specific requirements using hybrid Answer Set Programming with difference logic, incorporating flexible machine processing, setup, batching and maintenance operations. Unlike existing methods that schedule semiconductor manufacturing processes locally with greedy heuristics or by independen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2307.14750</link><description>&lt;p&gt;
&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#20266;&#21477;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation. (arXiv:2307.14750v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31574;&#30053;&#65306;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#21477;&#23376;&#65292;&#24182;&#23558;&#20854;&#19982;&#32473;&#23450;&#30340;&#22270;&#20687;&#23545;&#40784;&#20316;&#20026;&#20266;&#27880;&#37322;&#65292;&#25110;&#32773;&#20351;&#29992;&#22806;&#37096;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#40784;&#30340;&#35774;&#32622;&#23384;&#22312;&#36136;&#37327;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#20284;&#20046;&#24050;&#32463;&#36798;&#21040;&#20102;&#26497;&#38480;&#65292;&#32780;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#8220;LPM + &#26816;&#32034;&#22686;&#24378;&#23398;&#20064;&#8221;&#65292;&#21033;&#29992;&#26469;&#33258;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#20266;&#21477;&#23376;&#29983;&#25104;&#65288;RaPSG&#65289;&#65292;&#37319;&#29992;&#39640;&#25928;&#30340;&#26041;&#27861;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#20986;&#39640;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training an image captioner without annotated image-sentence pairs has gained traction in recent years. Previous approaches can be categorized into two strategies: crawling sentences from mismatching corpora and aligning them with the given images as pseudo annotations, or pre-training the captioner using external image-text pairs. However, the aligning setting seems to reach its performance limit due to the quality problem of pairs, and pre-training requires significant computational resources. To address these challenges, we propose a new strategy ``LPM + retrieval-augmented learning" where the prior knowledge from large pre-trained models (LPMs) is leveraged as supervision, and a retrieval process is integrated to further reinforce its effectiveness. Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation (RaPSG), which adopts an efficient approach to retrieve highly relevant short region descriptions from the mismatching corpora and use them to generate a variety 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;(MVMR-FS)&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#24230;&#37327;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#20934;&#21017;(MVMR)&#65292;&#20197;&#36741;&#21161;&#29305;&#24449;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.14643</link><description>&lt;p&gt;
MVMR-FS&#65306;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy. (arXiv:2307.14643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;(MVMR-FS)&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#24230;&#37327;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#20934;&#21017;(MVMR)&#65292;&#20197;&#36741;&#21161;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20934;&#30830;&#24230;&#37327;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#21644;&#20887;&#20313;&#26159;&#29305;&#24449;&#36873;&#25321;&#39046;&#22495;&#30340;&#19968;&#20010;&#21476;&#32769;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#36807;&#28388;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24230;&#37327;&#36830;&#32493;&#25968;&#25454;&#30340;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#25351;&#23450;&#29305;&#24449;&#25968;&#37327;&#65292;&#36825;&#22312;&#27809;&#26377;&#19987;&#23478;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#31616;&#31216;&#20026;MVMR-FS&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26680;&#23494;&#24230;&#20272;&#35745;&#65292;&#20197;&#25429;&#25417;&#23427;&#20204;&#22312;&#31867;&#38388;&#21644;&#25972;&#20307;&#20998;&#24067;&#20013;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#65288;MVMR&#65289;&#30340;&#20934;&#21017;&#65292;&#20854;&#20013;&#21033;&#29992;&#31867;&#38388;&#27010;&#29575;&#20998;&#24067;&#26469;&#21453;&#26144;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#25972;&#20307;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#37327;&#21270;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to accurately measure the relevance and redundancy of features is an age-old challenge in the field of feature selection. However, existing filter-based feature selection methods cannot directly measure redundancy for continuous data. In addition, most methods rely on manually specifying the number of features, which may introduce errors in the absence of expert knowledge. In this paper, we propose a non-parametric feature selection algorithm based on maximum inter-class variation and minimum redundancy, abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel density estimation on the features to capture their similarities and differences in inter-class and overall distributions. Subsequently, we present the criteria for maximum inter-class variation and minimum redundancy (MVMR), wherein the inter-class probability distributions are employed to reflect feature relevance and the distances between overall probability distributions are used to quantify redundanc
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EvilEye&#65292;&#21033;&#29992;&#36879;&#26126;&#26174;&#31034;&#22120;&#29983;&#25104;&#21160;&#24577;&#29289;&#29702;&#25932;&#23545;&#31034;&#20363;&#30340;&#24863;&#30693;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25668;&#20687;&#22836;&#30340;&#20809;&#23398;&#29305;&#24615;&#22312;&#22810;&#31181;&#29031;&#26126;&#26465;&#20214;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.13131</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#20320;&#19981;&#28165;&#27905;&#30524;&#38236;&#65311;&#21033;&#29992;&#21160;&#24577;&#20809;&#23398;&#25200;&#21160;&#36827;&#34892;&#24863;&#30693;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Why Don't You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations. (arXiv:2307.13131v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EvilEye&#65292;&#21033;&#29992;&#36879;&#26126;&#26174;&#31034;&#22120;&#29983;&#25104;&#21160;&#24577;&#29289;&#29702;&#25932;&#23545;&#31034;&#20363;&#30340;&#24863;&#30693;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25668;&#20687;&#22836;&#30340;&#20809;&#23398;&#29305;&#24615;&#22312;&#22810;&#31181;&#29031;&#26126;&#26465;&#20214;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25668;&#20687;&#22836;&#20026;&#22522;&#30784;&#30340;&#27169;&#25311;&#20154;&#31867;&#24863;&#30693;&#30340;&#33258;&#20027;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#38598;&#25104;&#21040;&#23433;&#20840;&#20851;&#38190;&#30340;&#24179;&#21488;&#20013;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#25991;&#29486;&#20307;&#31995;&#65292;&#25506;&#32034;&#23545;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25932;&#23545;&#25915;&#20987;&#12290;&#23558;&#25932;&#23545;&#25915;&#20987;&#36866;&#24212;&#20110;&#29616;&#23454;&#19990;&#30028;&#23545;&#25915;&#20987;&#32773;&#26469;&#35828;&#26159;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#36825;&#28040;&#38500;&#20102;&#21361;&#23475;&#25968;&#23383;&#31995;&#32479;&#30340;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#38754;&#20020;&#30528;&#19982;&#24863;&#30693;&#31649;&#36947;&#20013;&#30340;&#29615;&#22659;&#22122;&#22768;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#21160;&#24577;&#24615;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#24863;&#22120;&#20026;&#20808;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EvilEye&#65292;&#19968;&#31181;&#21033;&#29992;&#36879;&#26126;&#26174;&#31034;&#22120;&#29983;&#25104;&#21160;&#24577;&#29289;&#29702;&#25932;&#23545;&#31034;&#20363;&#30340;&#20013;&#38388;&#20154;&#24863;&#30693;&#25915;&#20987;&#12290;EvilEye&#21033;&#29992;&#25668;&#20687;&#22836;&#30340;&#20809;&#23398;&#29305;&#24615;&#22312;&#21508;&#31181;&#29031;&#26126;&#26465;&#20214;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;&#20026;&#20102;&#29983;&#25104;&#21160;&#24577;&#25200;&#21160;&#65292;&#25105;&#20204;&#23558;&#25968;&#23383;&#25915;&#20987;&#30340;&#25237;&#24433;&#24418;&#24335;&#21270;&#20026;&#20809;&#23398;&#35270;&#35273;&#36870;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera-based autonomous systems that emulate human perception are increasingly being integrated into safety-critical platforms. Consequently, an established body of literature has emerged that explores adversarial attacks targeting the underlying machine learning models. Adapting adversarial attacks to the physical world is desirable for the attacker, as this removes the need to compromise digital systems. However, the real world poses challenges related to the "survivability" of adversarial manipulations given environmental noise in perception pipelines and the dynamicity of autonomous systems. In this paper, we take a sensor-first approach. We present EvilEye, a man-in-the-middle perception attack that leverages transparent displays to generate dynamic physical adversarial examples. EvilEye exploits the camera's optics to induce misclassifications under a variety of illumination conditions. To generate dynamic perturbations, we formalize the projection of a digital attack into the ph
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39057;&#29575;&#20559;&#22909;&#25511;&#21046;&#27169;&#22359;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37325;&#26032;&#37197;&#32622;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#30340;&#20302;&#39057;&#21644;&#39640;&#39057;&#25104;&#20998;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#40065;&#26834;&#24615;&#23398;&#20064;&#20013;&#23545;&#39057;&#29575;&#30340;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.09763</link><description>&lt;p&gt;
&#29992;&#39057;&#29575;&#20559;&#24046;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Building More Robust Models with Frequency Bias. (arXiv:2307.09763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39057;&#29575;&#20559;&#22909;&#25511;&#21046;&#27169;&#22359;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37325;&#26032;&#37197;&#32622;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#30340;&#20302;&#39057;&#21644;&#39640;&#39057;&#25104;&#20998;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#40065;&#26834;&#24615;&#23398;&#20064;&#20013;&#23545;&#39057;&#29575;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#20173;&#28982;&#26159;&#23427;&#20204;&#24212;&#29992;&#24191;&#27867;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#35757;&#32451;&#30340;&#27169;&#22411;&#24378;&#35843;&#20302;&#39057;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#21033;&#29992;&#36825;&#31181;&#39057;&#29575;&#29305;&#24615;&#65292;&#20294;&#30452;&#25509;&#23558;&#20302;&#36890;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#36755;&#20837;&#22270;&#20687;&#20250;&#23548;&#33268;&#19981;&#21487;&#36870;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#23545;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39057;&#29575;&#20559;&#22909;&#25511;&#21046;&#27169;&#22359;&#30340;&#25554;&#25300;&#24335;&#27169;&#22359;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#37325;&#26032;&#37197;&#32622;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#30340;&#20302;&#39057;&#21644;&#39640;&#39057;&#25104;&#20998;&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;&#39057;&#29575;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22359;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#20013;&#65292;&#24182;&#36827;&#19968;&#27493;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability of deep neural networks to adversarial samples has been a major impediment to their broad applications, despite their success in various fields. Recently, some works suggested that adversarially-trained models emphasize the importance of low-frequency information to achieve higher robustness. While several attempts have been made to leverage this frequency characteristic, they have all faced the issue that applying low-pass filters directly to input images leads to irreversible loss of discriminative information and poor generalizability to datasets with distinct frequency features. This paper presents a plug-and-play module called the Frequency Preference Control Module that adaptively reconfigures the low- and high-frequency components of intermediate feature representations, providing better utilization of frequency in robust learning. Empirical studies show that our proposed module can be easily incorporated into any adversarial training framework, further improvi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24515;&#29702;&#27979;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37325;&#28857;&#32771;&#23519;&#20102;&#24773;&#24863;&#29702;&#35299;&#65288;EU&#65289;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;LLMs&#22312;&#24773;&#24863;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.09042</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Emotional Intelligence of Large Language Models. (arXiv:2307.09042v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09042
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24515;&#29702;&#27979;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37325;&#28857;&#32771;&#23519;&#20102;&#24773;&#24863;&#29702;&#35299;&#65288;EU&#65289;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;LLMs&#22312;&#24773;&#24863;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20027;&#35201;&#36890;&#36807;&#35821;&#35328;&#29983;&#25104;&#12289;&#30693;&#35782;&#21033;&#29992;&#21644;&#22797;&#26434;&#25512;&#29702;&#31561;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#24773;&#24863;&#21644;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#35780;&#20272;&#65292;&#32780;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#65292;&#21253;&#25324;&#24773;&#24863;&#35782;&#21035;&#12289;&#35299;&#37322;&#21644;&#29702;&#35299;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#30340;&#27807;&#36890;&#21644;&#31038;&#20132;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24515;&#29702;&#27979;&#37327;&#35780;&#20272;&#65292;&#37325;&#28857;&#26159;&#24773;&#24863;&#29702;&#35299;&#65288;EU&#65289;&#65292;&#36825;&#26159;EI&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;LLMs&#12290;&#36825;&#20010;&#27979;&#35797;&#38656;&#35201;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#35780;&#20272;&#22797;&#26434;&#30340;&#24773;&#24863;&#65288;&#20363;&#22914;&#65292;&#24778;&#35766;&#12289;&#24841;&#24555;&#12289;&#22256;&#24785;&#12289;&#33258;&#35946;&#65289;&#65292;&#22914;&#23613;&#31649;&#24863;&#35273;&#34920;&#29616;&#19981;&#20339;&#65292;&#32422;&#32752;&#21364;&#24847;&#22806;&#22320;&#33719;&#24471;&#20102;&#26368;&#39640;&#20998;&#12290;&#36890;&#36807;&#20174;500&#22810;&#21517;&#25104;&#24180;&#20154;&#26500;&#24314;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#20027;&#27969;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs. This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score). With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07686</link><description>&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#25903;&#25345;OpenMP Fortran&#21644;C++&#20195;&#30721;&#30456;&#20114;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#65288;CodeBLEU&#65289;&#21644;&#23450;&#24615;&#65288;&#20154;&#24037;&#35780;&#20272;&#65289;&#26041;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#32534;&#30721;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;5.1&#20493;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#23450;&#32534;&#30721;&#29087;&#24713;&#24230;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;9.9&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#20195;&#30721;&#32763;&#35793;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#22635;&#34917;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#35299;&#37322;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2307.02131</link><description>&lt;p&gt;
&#36229;&#36234;&#24050;&#30693;&#29616;&#23454;&#65306;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#34892;&#21307;&#23398;&#30740;&#31350; (arXiv&#65306;2307.02131v2 [cs.AI] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research. (arXiv:2307.02131v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#22635;&#34917;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#35299;&#37322;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#26088;&#22312;&#25299;&#23637;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#21033;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#29305;&#24449;&#26469;&#35786;&#26029;&#20799;&#31185;&#21518;&#39045;&#31389;&#33041;&#32959;&#30244;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#24050;&#32463;&#35265;&#35777;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#21644;&#23398;&#26415;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#30340;&#20154;&#31867;&#21451;&#22909;&#35299;&#37322;&#30340;&#32570;&#20047;&#26174;&#33879;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#25509;&#21463;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34701;&#20837;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20026;&#26816;&#26597;&#26367;&#20195;&#20915;&#31574;&#22330;&#26223;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#35299;&#37322;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#39564;&#35777;&#39044;&#27979;&#24182;&#28548;&#28165;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#24046;&#24322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#20445;&#25345;&#20102;&#32479;&#35745;&#23398;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study employs counterfactual explanations to explore "what if?" scenarios in medical research, with the aim of expanding our understanding beyond existing boundaries. Specifically, we focus on utilizing MRI features for diagnosing pediatric posterior fossa brain tumors as a case study. The field of artificial intelligence and explainability has witnessed a growing number of studies and increasing scholarly interest. However, the lack of human-friendly interpretations in explaining the outcomes of machine learning algorithms has significantly hindered the acceptance of these methods by clinicians in their clinical practice. To address this, our approach incorporates counterfactual explanations, providing a novel way to examine alternative decision-making scenarios. These explanations offer personalized and context-specific insights, enabling the validation of predictions and clarification of variations under diverse circumstances. Importantly, our approach maintains both statistica
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22238;&#31572;&#22797;&#26434;&#21307;&#23398;&#38382;&#39064;&#19978;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#20854;&#29983;&#25104;&#30340;&#31572;&#26696;&#26356;&#21152;&#27880;&#37325;&#19978;&#19979;&#25991;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00112</link><description>&lt;p&gt;
ChatGPT&#22312;USMLE&#19978;&#30340;&#34920;&#29616;&#65306;&#20026;AI&#36741;&#21161;&#21307;&#23398;&#25945;&#32946;&#24320;&#21551;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education. (arXiv:2307.00112v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00112
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22238;&#31572;&#22797;&#26434;&#21307;&#23398;&#38382;&#39064;&#19978;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#20854;&#29983;&#25104;&#30340;&#31572;&#26696;&#26356;&#21152;&#27880;&#37325;&#19978;&#19979;&#25991;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#33719;&#24471;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#33258;&#20174;OpenAI&#21457;&#24067;ChatGPT&#20197;&#26469;&#65292;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;AI&#30340;&#19994;&#21153;&#30340;&#27969;&#34892;&#24230;&#22823;&#24133;&#22686;&#38271;&#12290;&#20154;&#20204;&#22312;&#32844;&#19994;&#21644;&#20010;&#20154;&#29983;&#27963;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#22320;&#20351;&#29992;ChatGPT&#12290;&#32771;&#34385;&#21040;ChatGPT&#30340;&#24191;&#27867;&#20351;&#29992;&#21644;&#20154;&#20204;&#23545;&#20854;&#30340;&#20381;&#36182;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;ChatGPT&#22312;&#22238;&#31572;&#22797;&#26434;&#21307;&#23398;&#21644;&#20020;&#24202;&#38382;&#39064;&#19978;&#30340;&#21487;&#38752;&#24615;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#21704;&#20315;&#22823;&#23398;&#35299;&#21078;&#23398;&#30693;&#35782;&#21644;&#32654;&#22269;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65288;USMLE&#65289;&#38382;&#21367;&#26469;&#23454;&#29616;&#30446;&#26631;&#12290;&#26412;&#25991;&#20351;&#29992;&#21452;&#22240;&#32032;&#26041;&#24046;&#20998;&#26512;&#21644;&#20107;&#21518;&#20998;&#26512;&#35780;&#20272;&#20102;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;&#20004;&#32773;&#37117;&#26174;&#31034;&#20986;&#26684;&#24335;&#21644;&#25552;&#31034;&#20043;&#38388;&#30340;&#31995;&#32479;&#21327;&#21464;&#12290;&#27492;&#22806;&#65292;&#21307;&#29983;&#35780;&#20272;&#32773;&#36824;&#23545;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#27934;&#23519;&#21147;&#36827;&#34892;&#20102;&#29420;&#31435;&#35780;&#20215;&#12290;&#20998;&#26512;&#32467;&#26524;&#21457;&#29616;&#65292;ChatGPT&#29983;&#25104;&#30340;&#31572;&#26696;&#26356;&#21152;&#27880;&#37325;&#19978;&#19979;&#25991;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is gaining traction in more ways than ever before. The popularity of language models and AI-based businesses has soared since ChatGPT was made available to the general public via OpenAI. It is becoming increasingly common for people to use ChatGPT both professionally and personally. Considering the widespread use of ChatGPT and the reliance people place on it, this study determined how reliable ChatGPT can be for answering complex medical and clinical questions. Harvard University gross anatomy along with the United States Medical Licensing Examination (USMLE) questionnaire were used to accomplish the objective. The paper evaluated the obtained results using a 2-way ANOVA and posthoc analysis. Both showed systematic covariation between format and prompt. Furthermore, the physician adjudicators independently rated the outcome's accuracy, concordance, and insight. As a result of the analysis, ChatGPT-generated answers were found to be more context-oriented and rep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2306.05965</link><description>&lt;p&gt;
&#22312;&#22240;&#23376;&#22270;&#20013;&#33258;&#21160;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#65292;&#36125;&#21494;&#26031;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;&#24050;&#32463;&#34987;&#26377;&#25928;&#33258;&#21160;&#21270;&#65292;&#20294;&#23545;&#20110;&#27169;&#22411;&#27604;&#36739;&#23578;&#26410;&#22914;&#27492;&#65292;&#22240;&#27492;&#20173;&#38656;&#35201;&#23481;&#26131;&#20986;&#38169;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#25512;&#23548;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#27604;&#36739;&#32463;&#24120;&#34987;&#24573;&#35270;&#21644;&#24573;&#30053;&#65292;&#23613;&#31649;&#23427;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;Forney&#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#19978;&#20351;&#29992;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26469;&#39640;&#25928;&#22320;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#12290;&#36827;&#32780;&#21487;&#20351;&#29992;&#32553;&#25918;&#22240;&#23376;&#21516;&#26102;&#25191;&#34892;&#21442;&#25968;&#21644;&#29366;&#24577;&#25512;&#26029;&#20197;&#21450;&#27169;&#22411;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#65292;&#21516;&#26102;&#20801;&#35768;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#20998;&#23618;&#21644;&#26102;&#38388;&#27169;&#22411;&#20808;&#39564;&#65292;&#20197;&#36866;&#24212;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#21464;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IRB-AF&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;IRB&#21644;ArtFlow&#26041;&#27861;&#26469;&#35299;&#20915;&#21475;&#21693;&#22120;&#23448;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#22495;&#24046;&#24322;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10883</link><description>&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21475;&#21693;&#22120;&#23448;sim-to-real&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Sim-to-Real Segmentation of Oropharyngeal Organs. (arXiv:2305.10883v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10883
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IRB-AF&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;IRB&#21644;ArtFlow&#26041;&#27861;&#26469;&#35299;&#20915;&#21475;&#21693;&#22120;&#23448;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#22495;&#24046;&#24322;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#36741;&#21161;&#19979;&#30340;&#32463;&#21475;&#27668;&#31649;&#25554;&#31649;&#38656;&#35201;&#20351;&#29992;&#25903;&#25345;&#21307;&#29983;&#23558;&#27668;&#31649;&#23548;&#31649;&#25554;&#20837;&#22768;&#38376;&#32780;&#19981;&#26159;&#39135;&#31649;&#30340;&#20869;&#31397;&#38236;&#12290;&#36234;&#26469;&#36234;&#22810;&#22522;&#20110;&#26426;&#22120;&#20154;&#30340;&#27668;&#31649;&#25554;&#31649;&#38656;&#35201;&#21307;&#30103;&#26426;&#22120;&#20154;&#20687;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#29983;&#19968;&#26679;&#21306;&#20998;&#35299;&#21078;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#27169;&#20223;&#12290;&#28982;&#32780;&#21475;&#21693;&#22120;&#23448;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#24448;&#24448;&#30001;&#20110;&#24320;&#28304;&#25968;&#25454;&#21463;&#38480;&#21644;&#24739;&#32773;&#38544;&#31169;&#38382;&#39064;&#32780;&#38590;&#20197;&#33719;&#21462;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;Sim-to-Real&#26694;&#26550;&#65292;&#31216;&#20026;IoU-Ranking Blend-ArtFlow (IRB-AF)&#65292;&#29992;&#20110;&#21475;&#21693;&#22120;&#23448;&#30340;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#31181;&#31216;&#20026;IoU-Ranking Blend (IRB)&#30340;&#22270;&#20687;&#34701;&#21512;&#31574;&#30053;&#21644;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;ArtFlow&#12290;&#20854;&#20013;&#65292;IRB&#36890;&#36807;&#20943;&#36731;&#25968;&#25454;&#38598;&#20043;&#38388;&#37325;&#35201;&#30340;&#39046;&#22495;&#24046;&#24322;&#25152;&#23548;&#33268;&#30340;&#23454;&#29616;&#20998;&#21106;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;&#32780;ArtFlow&#21017;&#21487;&#36827;&#19968;&#27493;&#38477;&#20302;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#24615;&#12290; &#26412;&#30740;&#31350;&#37319;&#29992;&#19968;&#31181;&#34394;&#25311;&#21475;&#21693;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#22909;&#30340;&#21475;&#21693;&#22120;&#23448;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-assisted transoral tracheal intubation (TI) necessitates using an endoscope that helps the physician insert a tracheal tube into the glottis instead of the esophagus. The growing trend of robotic-assisted TI would require a medical robot to distinguish anatomical features like an experienced physician which can be imitated by utilizing supervised deep-learning techniques. However, the real datasets of oropharyngeal organs are often inaccessible due to limited open-source data and patient privacy. In this work, we propose a domain adaptive Sim-to-Real framework called IoU-Ranking Blend-ArtFlow (IRB-AF) for image segmentation of oropharyngeal organs. The framework includes an image blending strategy called IoU-Ranking Blend (IRB) and style-transfer method ArtFlow. Here, IRB alleviates the problem of poor segmentation performance caused by significant datasets domain differences; while ArtFlow is introduced to reduce the discrepancies between datasets further. A virtual oropharynx i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;Distantly-Supervised Named Entity Recognition&#20013;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#26500;&#24314;&#26469;&#24212;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2305.04076</link><description>&lt;p&gt;
SANTA&#65306;Distantly-Supervised Named Entity Recognition&#20013;&#22788;&#29702;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition. (arXiv:2305.04076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;Distantly-Supervised Named Entity Recognition&#20013;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#26500;&#24314;&#26469;&#24212;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#30417;&#30563;&#35774;&#32622;&#20013;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#27880;&#37322;&#36127;&#25285;&#65292;&#20294;&#26159;&#26080;&#19978;&#19979;&#25991;&#30340;&#21305;&#37197;&#36807;&#31243;&#21644;&#30693;&#35782;&#24211;&#30340;&#26377;&#38480;&#35206;&#30422;&#24341;&#20837;&#20102;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#30340;&#26631;&#27880;&#22122;&#38899;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#30340;SANTA&#65292;&#20197;&#35299;&#20915;&#30001;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distantly-Supervised Named Entity Recognition effectively alleviates the burden of time-consuming and expensive annotation in the supervised setting. But the context-free matching process and the limited coverage of knowledge bases introduce inaccurate and incomplete annotation noise respectively. Previous studies either considered only incomplete annotation noise or indiscriminately handle two types of noise with the same strategy. In this paper, we argue that the different causes of two types of noise bring up the requirement of different strategies in model architecture. Therefore, we propose the SANTA to handle these two types of noise separately with (1) Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate decision boundary shifting problem caused by incomplete annotation and a noise-tolerant loss to improve the robustness. Benefiting from our separate tailored strategies, we co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;</title><link>http://arxiv.org/abs/2304.10712</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#24858;&#24324;&#28909;&#32418;&#22806;&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fooling Thermal Infrared Detectors in Physical World. (arXiv:2304.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#22312;&#34892;&#20154;&#26816;&#27979;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#26041;&#38754;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#33021;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#20102;&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#23433;&#20840;&#24615;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#24178;&#25200;&#65292;&#22914;&#23567;&#28783;&#27873;&#21644;&#28909;&#8220;QR&#20195;&#30721;&#8221;&#26469;&#25915;&#20987;&#32418;&#22806;&#25104;&#20687;&#25506;&#27979;&#22120;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#34987;&#23519;&#35273;&#65292;&#32570;&#20047;&#38544;&#31192;&#24615;&#12290;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#28909;&#21644;&#20919;&#22359;&#26469;&#27450;&#39575;&#32418;&#22806;&#25104;&#20687;&#25506;&#27979;&#22120;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20174;&#22810;&#20010;&#35282;&#24230;&#25191;&#34892;&#25915;&#20987;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#25105;&#20204;&#26681;&#25454;&#20854;&#26377;&#25928;&#24615;&#12289;&#38544;&#31192;&#24615;&#21644;&#31283;&#20581;&#24615;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AdvIB&#21487;&#20197;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22312;&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#20013;&#25552;&#39640;&#23433;&#20840;&#25514;&#26045;&#30340;&#24517;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infrared imaging systems have a vast array of potential applications in pedestrian detection and autonomous driving, and their safety performance is of great concern. However, few studies have explored the safety of infrared imaging systems in real-world settings. Previous research has used physical perturbations such as small bulbs and thermal "QR codes" to attack infrared imaging detectors, but such methods are highly visible and lack stealthiness. Other researchers have used hot and cold blocks to deceive infrared imaging detectors, but this method is limited in its ability to execute attacks from various angles. To address these shortcomings, we propose a novel physical attack called adversarial infrared blocks (AdvIB). By optimizing the physical parameters of the adversarial infrared blocks, this method can execute a stealthy black-box attack on thermal imaging system from various angles. We evaluate the proposed method based on its effectiveness, stealthiness, and robustness. Our
&lt;/p&gt;</description></item><item><title>VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2304.07810</link><description>&lt;p&gt;
VISAR&#65306;&#19968;&#31181;&#24102;&#26377;&#21487;&#35270;&#21270;&#32534;&#31243;&#21644;&#24555;&#36895;&#33609;&#26696;&#21407;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#35770;&#35777;&#20889;&#20316;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07810
&lt;/p&gt;
&lt;p&gt;
VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#65292;&#20316;&#32773;&#24517;&#39035;&#26500;&#24605;&#20998;&#23618;&#20889;&#20316;&#30446;&#26631;&#65292;&#30830;&#20445;&#20854;&#35770;&#28857;&#30340;&#35828;&#26381;&#21147;&#65292;&#24182;&#36890;&#36807;&#36215;&#33609;&#26469;&#20462;&#35746;&#21644;&#32452;&#32455;&#20182;&#20204;&#30340;&#35745;&#21010;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#65288;&#20363;&#22914;ChatGPT&#65289;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38544;&#21547;&#30340;&#20889;&#20316;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#24847;&#22270;&#65292;&#32570;&#20047;&#29992;&#25143;&#25511;&#21046;&#21644;&#33258;&#20027;&#26435;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#38480;&#30340;&#24110;&#21161;&#26469;&#36827;&#34892;&#24847;&#20041;&#26500;&#24314;&#21644;&#20462;&#35746;&#20889;&#20316;&#35745;&#21010;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VISAR&#65292;&#19968;&#31181;AI&#25903;&#25345;&#30340;&#20889;&#20316;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#22312;&#20854;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#26500;&#24605;&#21644;&#20462;&#35746;&#20998;&#23618;&#30446;&#26631;&#65292;&#36890;&#36807;&#21516;&#27493;&#25991;&#26412;&#32534;&#36753;&#21644;&#21487;&#35270;&#21270;&#32534;&#31243;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#35770;&#35777;&#28779;&#33457;&#25512;&#33616;&#22686;&#24378;&#35828;&#26381;&#21147;&#12290;VISAR&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#25506;&#32034;&#12289;&#23454;&#39564;&#21644;&#39564;&#35777;&#20182;&#20204;&#30340;&#20889;&#20316;&#35745;&#21010;&#12290;&#19968;&#20010;&#21463;&#25511;&#23454;&#39564;&#23460;&#30740;&#31350;&#35777;&#23454;&#65292;VISAR&#21487;&#20197;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Expert-Free Online Transfer Learning (EF-OnTL)&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#38656;&#35201;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#20219;&#21153;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.01170</link><description>&lt;p&gt;
&#26080;&#19987;&#23478;&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning. (arXiv:2303.01170v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Expert-Free Online Transfer Learning (EF-OnTL)&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#38656;&#35201;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#20219;&#21153;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#19987;&#23478;&#26234;&#33021;&#20307;&#36716;&#31227;&#21040;&#26032;&#25163;&#26234;&#33021;&#20307;&#26469;&#35299;&#20915;&#35757;&#32451;&#38382;&#39064;&#65292;&#22914;&#25506;&#32034;&#25104;&#26412;&#12289;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#25910;&#25947;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36801;&#31227;&#38656;&#35201;&#26032;&#25163;&#26234;&#33021;&#20307;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#25165;&#33021;&#26377;&#25928;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#19987;&#23478;&#22312;&#32447;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65288;EF-OnTL&#65289;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#27599;&#19968;&#27425;&#36801;&#31227;&#27493;&#39588;&#20013;&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#26469;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SARS-RND&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#23545;RND&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#12289;&#34892;&#21160;&#12289;&#22870;&#21169;&#21644;&#19979;&#19968;&#29366;&#24577;&#20013;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning in Reinforcement Learning (RL) has been widely studied to overcome training issues of Deep-RL, i.e., exploration cost, data availability and convergence time, by introducing a way to enhance training phase with external knowledge. Generally, knowledge is transferred from expert-agents to novices. While this fixes the issue for a novice agent, a good understanding of the task on expert agent is required for such transfer to be effective. As an alternative, in this paper we propose Expert-Free Online Transfer Learning (EF-OnTL), an algorithm that enables expert-free real-time dynamic transfer learning in multi-agent system. No dedicated expert exists, and transfer source agent and knowledge to be transferred are dynamically selected at each transfer step based on agents' performance and uncertainty. To improve uncertainty estimation, we also propose State Action Reward Next-State Random Network Distillation (sars-RND), an extension of RND that estimates uncertainty from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ESB-CPO&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26089;&#26399;&#38454;&#27573;&#25918;&#23485;&#32422;&#26463;&#24182;&#24341;&#20837;&#39069;&#22806;&#23433;&#20840;&#39044;&#31639;&#65292;&#22312;&#25506;&#32034;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#21463;&#38480;&#31574;&#30053;&#20248;&#21270;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.14339</link><description>&lt;p&gt;
&#21033;&#29992;&#39069;&#22806;&#23433;&#20840;&#39044;&#31639;&#22312;&#21463;&#38480;&#31574;&#30053;&#20248;&#21270;&#20013;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration Using Extra Safety Budget in Constrained Policy Optimization. (arXiv:2302.14339v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ESB-CPO&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26089;&#26399;&#38454;&#27573;&#25918;&#23485;&#32422;&#26463;&#24182;&#24341;&#20837;&#39069;&#22806;&#23433;&#20840;&#39044;&#31639;&#65292;&#22312;&#25506;&#32034;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#21463;&#38480;&#31574;&#30053;&#20248;&#21270;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#22823;&#22810;&#25968;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23398;&#20064;&#39537;&#21160;&#25511;&#21046;&#22120;&#30340;&#23433;&#20840;&#24615;&#26159;&#30830;&#20445;&#25511;&#21046;&#22120;&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#25972;&#20307;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#23548;&#33268;&#26089;&#26399;&#25506;&#32034;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39069;&#22806;&#23433;&#20840;&#39044;&#31639;&#30340;&#21463;&#38480;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65288;ESB-CPO&#65289;&#65292;&#20197;&#22312;&#25506;&#32034;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22312;&#26089;&#26399;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#25918;&#23485;&#19981;&#23433;&#20840;&#36716;&#25442;&#30340;&#23454;&#38469;&#32422;&#26463;&#65288;&#28155;&#21152;&#39069;&#22806;&#23433;&#20840;&#39044;&#31639;&#65289;&#12290;&#38543;&#30528;&#35757;&#32451;&#36807;&#31243;&#30340;&#36827;&#34892;&#65292;&#25105;&#20204;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#32422;&#26463;&#36880;&#28176;&#21464;&#24471;&#26356;&#32039;&#12290;&#21516;&#26102;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36880;&#27493;&#28385;&#36275;&#26368;&#32456;&#35757;&#32451;&#38454;&#27573;&#30340;&#25104;&#26412;&#38480;&#21046;&#35201;&#27714;&#12290;&#22312;Safety-Gym&#21644;Bullet-Safety-Gym&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has achieved promising results on most robotic control tasks. Safety of learning-based controllers is an essential notion of ensuring the effectiveness of the controllers. Current methods adopt whole consistency constraints during the training, thus resulting in inefficient exploration in the early stage. In this paper, we propose an algorithm named Constrained Policy Optimization with Extra Safety Budget (ESB-CPO) to strike a balance between the exploration efficiency and the constraints satisfaction. In the early stage, our method loosens the practical constraints of unsafe transitions (adding extra safety budget) with the aid of a new metric we propose. With the training process, the constraints in our optimization problem become tighter. Meanwhile, theoretical analysis and practical experiments demonstrate that our method gradually meets the cost limit's demand in the final training stage. When evaluated on Safety-Gym and Bullet-Safety-Gym benchmarks, ou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24037;&#31243;&#35774;&#35745;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;MMML&#65289;&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#23545;&#40784;&#12289;&#36716;&#25442;&#21644;&#20849;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20197;&#21450;&#19982;&#24037;&#31243;&#35774;&#35745;&#30456;&#20851;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#21253;&#25324;&#26500;&#24314;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#31639;&#27861;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2302.10909</link><description>&lt;p&gt;
&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65306;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Machine Learning in Engineering Design: A Review and Future Directions. (arXiv:2302.10909v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24037;&#31243;&#35774;&#35745;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;MMML&#65289;&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#23545;&#40784;&#12289;&#36716;&#25442;&#21644;&#20849;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20197;&#21450;&#19982;&#24037;&#31243;&#35774;&#35745;&#30456;&#20851;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#21253;&#25324;&#26500;&#24314;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#31639;&#27861;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;MMML&#65289;&#39046;&#22495;&#20013;&#65292;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#34701;&#21512;&#26377;&#28508;&#21147;&#37325;&#22609;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#24037;&#31243;&#35774;&#35745;&#39046;&#22495;&#20013;MMML&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#32508;&#36848;&#39318;&#20808;&#28145;&#20837;&#25506;&#35752;&#20102;MMML&#30340;&#20116;&#20010;&#22522;&#26412;&#27010;&#24565;&#65306;&#22810;&#27169;&#24577;&#20449;&#24687;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#23545;&#40784;&#12289;&#36716;&#25442;&#21644;&#20849;&#23398;&#20064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MMML&#30340;&#21069;&#27839;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#19982;&#24037;&#31243;&#35774;&#35745;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#22914;&#36328;&#27169;&#24577;&#32508;&#21512;&#12289;&#22810;&#27169;&#24577;&#39044;&#27979;&#21644;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#12290;&#36890;&#36807;&#36825;&#20010;&#32508;&#36848;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#37319;&#29992;MMML&#25152;&#38754;&#20020;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#20026;&#20102;&#25512;&#21160;MMML&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#25345;&#32493;&#28436;&#36827;&#65292;&#25105;&#20204;&#25552;&#20513;&#38598;&#20013;&#21162;&#21147;&#26500;&#24314;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#31639;&#27861;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of multi-modal machine learning (MMML), the convergence of multiple data modalities has the potential to reshape various applications. This paper presents a comprehensive overview of the current state, advancements, and challenges of MMML within the sphere of engineering design. The review begins with a deep dive into five fundamental concepts of MMML:multi-modal information representation, fusion, alignment, translation, and co-learning. Following this, we explore the cutting-edge applications of MMML, placing a particular emphasis on tasks pertinent to engineering design, such as cross-modal synthesis, multi-modal prediction, and cross-modal information retrieval. Through this comprehensive overview, we highlight the inherent challenges in adopting MMML in engineering design, and proffer potential directions for future research. To spur on the continued evolution of MMML in engineering design, we advocate for concentrated efforts to construct extensive 
&lt;/p&gt;</description></item><item><title>3M3D&#26159;&#19968;&#31181;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#35270;&#22270;&#12289;&#22810;&#36335;&#24452;&#12289;&#22810;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#22810;&#35270;&#22270;&#29305;&#24449;&#21644;&#26597;&#35810;&#29305;&#24449;&#65292;&#21516;&#26102;&#22686;&#24378;&#20840;&#26223;&#35270;&#22270;&#21644;&#20840;&#23616;&#35270;&#22270;&#20013;&#30340;&#22330;&#26223;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.08231</link><description>&lt;p&gt;
3M3D: &#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#35270;&#22270;&#12289;&#22810;&#36335;&#24452;&#12289;&#22810;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
3M3D: Multi-view, Multi-path, Multi-representation for 3D Object Detection. (arXiv:2302.08231v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08231
&lt;/p&gt;
&lt;p&gt;
3M3D&#26159;&#19968;&#31181;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#35270;&#22270;&#12289;&#22810;&#36335;&#24452;&#12289;&#22810;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#22810;&#35270;&#22270;&#29305;&#24449;&#21644;&#26597;&#35810;&#29305;&#24449;&#65292;&#21516;&#26102;&#22686;&#24378;&#20840;&#26223;&#35270;&#22270;&#21644;&#20840;&#23616;&#35270;&#22270;&#20013;&#30340;&#22330;&#26223;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#30456;&#26426;&#22270;&#20687;&#30340;&#19977;&#32500;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#26032;&#30340;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#22810;&#35270;&#22270;&#29305;&#24449;&#26469;&#36845;&#20195;&#22686;&#24378;&#29289;&#20307;&#26597;&#35810;&#65288;&#29289;&#20307;&#20505;&#36873;&#65289;&#12290;&#28982;&#32780;&#65292;&#20010;&#20307;&#20027;&#24178;&#29305;&#24449;&#19981;&#20250;&#36890;&#36807;&#22810;&#35270;&#22270;&#29305;&#24449;&#36827;&#34892;&#26356;&#26032;&#65292;&#32780;&#20165;&#20165;&#20445;&#25345;&#20316;&#20026;&#21333;&#19968;&#22270;&#20687;&#20027;&#24178;&#32593;&#32476;&#36755;&#20986;&#30340;&#38598;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3M3D&#65306;&#19968;&#31181;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#35270;&#22270;&#12289;&#22810;&#36335;&#24452;&#12289;&#22810;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#32454;&#31890;&#24230;&#20840;&#26223;&#35270;&#22270;&#21644;&#31895;&#31890;&#24230;&#20840;&#23616;&#35270;&#22270;&#20013;&#21516;&#26102;&#26356;&#26032;&#22810;&#35270;&#22270;&#29305;&#24449;&#21644;&#26597;&#35810;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#22330;&#26223;&#30340;&#34920;&#31034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#35270;&#22270;&#36724;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26356;&#26032;&#22810;&#35270;&#22270;&#29305;&#24449;&#12290;&#23427;&#23558;&#20840;&#26223;&#20449;&#24687;&#34701;&#20837;&#22810;&#35270;&#22270;&#29305;&#24449;&#20013;&#65292;&#22686;&#24378;&#23545;&#20840;&#23616;&#22330;&#26223;&#30340;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#26356;&#26032;&#22810;&#35270;&#22270;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D visual perception tasks based on multi-camera images are essential for autonomous driving systems. Latest work in this field performs 3D object detection by leveraging multi-view images as an input and iteratively enhancing object queries (object proposals) by cross-attending multi-view features. However, individual backbone features are not updated with multi-view features and it stays as a mere collection of the output of the single-image backbone network. Therefore we propose 3M3D: A Multi-view, Multi-path, Multi-representation for 3D Object Detection where we update both multi-view features and query features to enhance the representation of the scene in both fine panoramic view and coarse global view. Firstly, we update multi-view features by multi-view axis self-attention. It will incorporate panoramic information in the multi-view features and enhance understanding of the global scene. Secondly, we update multi-view features by self-attention of the ROI (Region of Interest) w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#27668;&#20505;&#25253;&#21578;&#20013;&#22238;&#31572;&#27668;&#20505;&#38382;&#21367;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#32467;&#26500;&#35757;&#32451;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#20154;&#31867;&#35797;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#27668;&#20505;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2301.04253</link><description>&lt;p&gt;
&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#27668;&#20505;&#25253;&#21578;&#20013;&#22238;&#31572;&#27668;&#20505;&#38382;&#21367;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Answering Climate Questionnaires from Unstructured Climate Reports. (arXiv:2301.04253v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#27668;&#20505;&#25253;&#21578;&#20013;&#22238;&#31572;&#27668;&#20505;&#38382;&#21367;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#32467;&#26500;&#35757;&#32451;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#20154;&#31867;&#35797;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#27668;&#20505;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27668;&#20505;&#21464;&#21270;&#38382;&#39064;&#32039;&#36843;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23545;&#20854;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#34892;&#21160;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#24222;&#22823;&#19988;&#24555;&#36895;&#22686;&#38271;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#27668;&#20505;&#25253;&#21578;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#24418;&#24335;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#20854;&#29616;&#26377;&#32467;&#26500;&#26469;&#35757;&#32451;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#19981;&#21516;&#32452;&#32455;&#31867;&#22411;&#30340;&#27668;&#20505;&#25259;&#38706;&#36827;&#34892;&#27867;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#31867;&#35797;&#39564;&#20013;&#24110;&#21161;&#23558;&#38750;&#32467;&#26500;&#21270;&#27668;&#20505;&#25991;&#26723;&#20013;&#30340;&#25991;&#26412;&#19982;&#21322;&#32467;&#26500;&#21270;&#38382;&#21367;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#25903;&#25345;&#27668;&#20505;&#39046;&#22495;&#36827;&#19968;&#27493;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29616;&#26377;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#21644;&#27604;&#36739;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The topic of Climate Change (CC) has received limited attention in NLP despite its urgency. Activists and policymakers need NLP tools to effectively process the vast and rapidly growing unstructured textual climate reports into structured form. To tackle this challenge we introduce two new large-scale climate questionnaire datasets and use their existing structure to train self-supervised models. We conduct experiments to show that these models can learn to generalize to climate disclosures of different organizations types than seen during training. We then use these models to help align texts from unstructured climate documents to the semi-structured questionnaires in a human pilot study. Finally, to support further NLP research in the climate domain we introduce a benchmark of existing climate text classification datasets to better evaluate and compare existing models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#26377;&#20559;&#25968;&#25454;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#19968;&#33268;&#33539;&#22260;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#22312;&#30446;&#26631;&#20154;&#32676;&#19978;&#26500;&#24314;&#20102;&#21487;&#35777;&#26126;&#20844;&#24179;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.10839</link><description>&lt;p&gt;
&#20844;&#24179;&#39044;&#27979;&#24314;&#27169;&#30340;&#19968;&#31181;&#19968;&#33268;&#33539;&#22260;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Consistent Range Approximation for Fair Predictive Modeling. (arXiv:2212.10839v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#26377;&#20559;&#25968;&#25454;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#19968;&#33268;&#33539;&#22260;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#22312;&#30446;&#26631;&#20154;&#32676;&#19978;&#26500;&#24314;&#20102;&#21487;&#35777;&#26126;&#20844;&#24179;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#26377;&#20559;&#25968;&#25454;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#23427;&#20511;&#37492;&#20102;&#23545;&#19981;&#23436;&#25972;&#21644;&#19981;&#19968;&#33268;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#22238;&#31572;&#65292;&#20197;&#24418;&#24335;&#21270;&#20844;&#24179;&#26597;&#35810;&#22312;&#30446;&#26631;&#20154;&#32676;&#19978;&#30340;&#19968;&#33268;&#33539;&#22260;&#36924;&#36817;&#65288;CRA&#65289;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#21644;&#26377;&#20559;&#25968;&#25454;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#20154;&#32676;&#32479;&#35745;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#20844;&#24179;&#26597;&#35810;&#30340;&#31572;&#26696;&#33539;&#22260;&#12290;&#36890;&#36807;CRA&#65292;&#35813;&#26694;&#26550;&#26500;&#24314;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#22312;&#30446;&#26631;&#20154;&#32676;&#19978;&#33719;&#24471;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;&#65292;&#32780;&#19981;&#21463;&#35757;&#32451;&#36807;&#31243;&#20013;&#22806;&#37096;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#38480;&#21046;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel framework for certifying the fairness of predictive models trained on biased data. It draws from query answering for incomplete and inconsistent databases to formulate the problem of consistent range approximation (CRA) of fairness queries for a predictive model on a target population. The framework employs background knowledge of the data collection process and biased data, working with or without limited statistics about the target population, to compute a range of answers for fairness queries. Using CRA, the framework builds predictive models that are certifiably fair on the target population, regardless of the availability of external data during training. The framework's efficacy is demonstrated through evaluations on real data, showing substantial improvement over existing state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25512;&#24191;&#20102;&#36864;&#28779;&#26391;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#65288;DDM&#31867;&#22411;&#65289;&#29992;&#20110;&#22788;&#29702;&#21463;&#27850;&#26494;&#22122;&#22768;&#24433;&#21709;&#30340;&#20809;&#23398;&#25104;&#20687;&#20013;&#30340;&#22797;&#25968;&#23545;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#22122;&#22768;&#22270;&#20687;&#20013;&#36827;&#34892;&#22797;&#25968;&#26816;&#32034;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.03235</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#22122;&#22768;&#22270;&#20687;&#20013;&#36827;&#34892;&#22797;&#25968;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Complex-valued Retrievals From Noisy Images Using Diffusion Models. (arXiv:2212.03235v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25512;&#24191;&#20102;&#36864;&#28779;&#26391;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#65288;DDM&#31867;&#22411;&#65289;&#29992;&#20110;&#22788;&#29702;&#21463;&#27850;&#26494;&#22122;&#22768;&#24433;&#21709;&#30340;&#20809;&#23398;&#25104;&#20687;&#20013;&#30340;&#22797;&#25968;&#23545;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#22122;&#22768;&#22270;&#20687;&#20013;&#36827;&#34892;&#22797;&#25968;&#26816;&#32034;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#30340;&#26174;&#24494;&#38236;&#27169;&#24335;&#20013;&#65292;&#20256;&#24863;&#22120;&#20165;&#27979;&#37327;&#23454;&#20540;&#24378;&#24230;&#12290;&#27492;&#22806;&#65292;&#20256;&#24863;&#22120;&#35835;&#25968;&#21463;&#27850;&#26494;&#20998;&#24067;&#30340;&#20809;&#23376;&#22122;&#22768;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#24674;&#22797;&#31639;&#27861;&#36890;&#24120;&#26088;&#22312;&#26368;&#23567;&#21270;&#21407;&#22987;&#22270;&#20687;&#21644;&#24674;&#22797;&#22270;&#20687;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#12290;&#36825;&#24448;&#24448;&#23548;&#33268;&#27169;&#31946;&#30340;&#32467;&#26524;&#21644;&#36136;&#37327;&#24046;&#30340;&#35270;&#35273;&#25928;&#26524;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#33021;&#22815;&#20174;&#25152;&#27714;&#21464;&#37327;&#30340;&#21518;&#39564;&#27010;&#29575;&#20013;&#37319;&#26679;&#22270;&#20687;&#65292;&#20174;&#32780;&#24471;&#21040;&#35270;&#35273;&#19978;&#20196;&#20154;&#28385;&#24847;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#29992;&#20110;&#21463;&#21040;&#39640;&#26031;&#22122;&#22768;&#24433;&#21709;&#30340;&#23454;&#20540;&#22270;&#20687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36864;&#28779;&#26391;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#65288;&#19968;&#31181;DDM&#31867;&#22411;&#65289;&#25512;&#24191;&#21040;&#35299;&#20915;&#21463;&#27850;&#26494;&#22122;&#22768;&#24433;&#21709;&#30340;&#20809;&#23398;&#25104;&#20687;&#20013;&#30340;&#22797;&#25968;&#23545;&#35937;&#65288;&#21644;&#23454;&#20540;&#22270;&#20687;&#65289;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#20809;&#23398;&#22330;&#26223;&#65292;&#22914;&#20613;&#31435;&#21494;&#20809;&#23398;&#32455;&#26500;&#22270;&#12289;&#30456;&#20301;&#24674;&#22797;&#21644;&#27850;&#26494;&#21435;&#22122;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;
&lt;/p&gt;
&lt;p&gt;
In diverse microscopy modalities, sensors measure only real-valued intensities. Additionally, the sensor readouts are affected by Poissonian-distributed photon noise. Traditional restoration algorithms typically aim to minimize the mean squared error (MSE) between the original and recovered images. This often leads to blurry outcomes with poor perceptual quality. Recently, deep diffusion models (DDMs) have proven to be highly capable of sampling images from the a-posteriori probability of the sought variables, resulting in visually pleasing high-quality images. These models have mostly been suggested for real-valued images suffering from Gaussian noise. In this study, we generalize annealed Langevin Dynamics, a type of DDM, to tackle the fundamental challenges in optical imaging of complex-valued objects (and real images) affected by Poisson noise. We apply our algorithm to various optical scenarios, such as Fourier Ptychography, Phase Retrieval, and Poisson denoising. Our algorithm is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10890</link><description>&lt;p&gt;
&#32531;&#35299;&#20998;&#23618;&#27880;&#24847;&#21147;&#22810;&#23610;&#24230;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating spectral bias for the multiscale operator learning with hierarchical attention. (arXiv:2210.10890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26080;&#38480;&#32500;&#21442;&#25968;&#21644;&#35299;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#30340;&#22810;&#23610;&#24230;PDE&#65292;&#22914;&#27833;&#34255;&#24314;&#27169;&#21644;&#28237;&#27969;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#36825;&#31181;PDE&#65292;&#23545;&#20302;&#39057;&#20998;&#37327;&#23384;&#22312;&#20809;&#35889;&#20559;&#24046;&#26159;&#29616;&#26377;&#31070;&#32463;&#31639;&#23376;&#30340;&#19968;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#23618;&#27425;&#30697;&#38453;&#26041;&#27861;&#21551;&#21457;&#30340;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#12290;HANO&#20855;&#26377;&#33258;&#36866;&#24212;&#23610;&#24230;&#20132;&#20114;&#33539;&#22260;&#21644;&#23618;&#27425;&#32467;&#26500;&#19978;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#21487;&#25511;&#32447;&#24615;&#25104;&#26412;&#30340;&#23884;&#22871;&#29305;&#24449;&#35745;&#31639;&#21644;&#22810;&#23610;&#24230;&#35299;&#31354;&#38388;&#30340;&#32534;&#30721;/&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#32463;&#39564;H^1&#25439;&#22833;&#20989;&#25968;&#26469;&#22686;&#24378;&#23545;&#39640;&#39057;&#20998;&#37327;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;HANO&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators have emerged as a powerful tool for learning the mapping between infinite-dimensional parameter and solution spaces of partial differential equations (PDEs). In this work, we focus on multiscale PDEs that have important applications such as reservoir modeling and turbulence prediction. We demonstrate that for such PDEs, the spectral bias towards low-frequency components presents a significant challenge for existing neural operators. To address this challenge, we propose a hierarchical attention neural operator (HANO) inspired by the hierarchical matrix approach. HANO features a scale-adaptive interaction range and self-attentions over a hierarchy of levels, enabling nested feature computation with controllable linear cost and encoding/decoding of multiscale solution space. We also incorporate an empirical $H^1$ loss function to enhance the learning of high-frequency components. Our numerical experiments demonstrate that HANO outperforms state-of-the-art (SOTA) methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#31995;&#32479;&#20013;&#30340;&#31070;&#32463;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#27010;&#29575;1&#22312;&#25351;&#23450;&#30340;&#31283;&#23450;&#21306;&#22495;&#20869;&#23454;&#29616;&#31995;&#32479;&#31283;&#23450;&#65292;&#24182;&#24341;&#20837;&#20102;&#31283;&#23450;&#25490;&#24207;&#36229;&#32423;&#38789;(sRSMs)&#30340;&#27010;&#24565;&#26469;&#20811;&#26381;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05304</link><description>&lt;p&gt;
&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#31995;&#32479;&#30340;&#21487;&#35777;&#26126;&#31283;&#23450;&#31070;&#32463;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Provably Stabilizing Neural Controllers for Discrete-Time Stochastic Systems. (arXiv:2210.05304v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#31995;&#32479;&#20013;&#30340;&#31070;&#32463;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#27010;&#29575;1&#22312;&#25351;&#23450;&#30340;&#31283;&#23450;&#21306;&#22495;&#20869;&#23454;&#29616;&#31995;&#32479;&#31283;&#23450;&#65292;&#24182;&#24341;&#20837;&#20102;&#31283;&#23450;&#25490;&#24207;&#36229;&#32423;&#38789;(sRSMs)&#30340;&#27010;&#24565;&#26469;&#20811;&#26381;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#31995;&#32479;&#20013;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#35813;&#31574;&#30053;&#20445;&#35777;&#31995;&#32479;&#20197;&#27010;&#29575;1&#22312;&#26576;&#20010;&#25351;&#23450;&#30340;&#31283;&#23450;&#21306;&#22495;&#20869;&#31283;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#30340;&#21019;&#26032;&#27010;&#24565;&#8212;&#8212;&#31283;&#23450;&#25490;&#24207;&#36229;&#32423;&#38789;(sRSMs)&#12290;&#25105;&#20204;&#30340;sRSMs&#20811;&#26381;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#36827;&#20837;&#31283;&#23450;&#21306;&#22495;&#21518;&#26080;&#27861;&#31163;&#24320;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#23398;&#20064;&#19968;&#20010;&#25511;&#21046;&#31574;&#30053;&#21644;&#19968;&#20010;&#27491;&#24335;&#35777;&#26126;&#27010;&#29575;1&#31283;&#23450;&#24615;&#30340;sRSM&#65292;&#20004;&#32773;&#37117;&#20197;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#36866;&#24212;&#20110;&#22312;&#32473;&#23450;&#30340;Lipschitz&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;&#19979;&#65292;&#39564;&#35777;&#38543;&#26426;&#31995;&#32479;&#20197;&#27010;&#29575;1&#22312;&#26576;&#20010;&#31283;&#23450;&#21306;&#22495;&#20869;&#31283;&#23450;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#36807;&#31243;&#33021;&#22815;&#25104;&#21151;&#22320;&#23398;&#20064;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#31070;&#32463;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning control policies in discrete-time stochastic systems which guarantee that the system stabilizes within some specified stabilization region with probability~$1$. Our approach is based on the novel notion of stabilizing ranking supermartingales (sRSMs) that we introduce in this work. Our sRSMs overcome the limitation of methods proposed in previous works whose applicability is restricted to systems in which the stabilizing region cannot be left once entered under any control policy. We present a learning procedure that learns a control policy together with an sRSM that formally certifies probability~$1$ stability, both learned as neural networks. We show that this procedure can also be adapted to formally verifying that, under a given Lipschitz continuous control policy, the stochastic system stabilizes within some stabilizing region with probability~$1$. Our experimental evaluation shows that our learning procedure can successfully learn provably stab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;AUR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#35823;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23545;&#38271;&#23614;&#29289;&#21697;&#30340;&#21484;&#22238;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.11679</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#32570;&#22833;&#25968;&#25454;&#65306;&#22522;&#20110;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Rethinking Missing Data: Aleatoric Uncertainty-Aware Recommendation. (arXiv:2209.11679v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;AUR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#35823;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23545;&#38271;&#23614;&#29289;&#21697;&#30340;&#21484;&#22238;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#20132;&#20114;&#26159;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#30340;&#40664;&#35748;&#36873;&#25321;&#65292;&#20294;&#36890;&#24120;&#21576;&#29616;&#24456;&#39640;&#30340;&#31232;&#30095;&#24615;&#65292;&#21363;&#22823;&#37096;&#20998;&#29992;&#25143;-&#29289;&#21697;&#23545;&#26159;&#26410;&#35266;&#23519;&#21040;&#30340;&#32570;&#22833;&#25968;&#25454;&#12290;&#26631;&#20934;&#30340;&#20570;&#27861;&#26159;&#23558;&#32570;&#22833;&#25968;&#25454;&#35270;&#20026;&#36127;&#26679;&#26412;&#65292;&#24182;&#22312;&#35266;&#23519;&#21040;&#30340;&#20132;&#20114;&#20013;&#20272;&#35745;&#29992;&#25143;-&#29289;&#21697;&#23545;&#20043;&#38388;&#30340;&#20132;&#20114;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35757;&#32451;&#26041;&#24335;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#23548;&#33268;&#19968;&#20123;&#28508;&#22312;&#30340;&#20132;&#20114;&#34987;&#38169;&#35823;&#26631;&#35760;&#65292;&#25439;&#23475;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38271;&#23614;&#29289;&#21697;&#30340;&#21484;&#22238;&#25928;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#35270;&#35282;&#30740;&#31350;&#20102;&#38169;&#26631;&#38382;&#39064;&#65292;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#25551;&#36848;&#20102;&#32570;&#22833;&#25968;&#25454;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#12290;&#36825;&#31181;&#38543;&#26426;&#24615;&#20419;&#20351;&#25105;&#20204;&#36229;&#36234;&#20165;&#20165;&#32771;&#34385;&#20132;&#20114;&#27010;&#29575;&#65292;&#32780;&#26159;&#37319;&#29992;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;AUR&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#21644;&#19968;&#20010;&#26222;&#36890;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historical interactions are the default choice for recommender model training, which typically exhibit high sparsity, i.e., most user-item pairs are unobserved missing data. A standard choice is treating the missing data as negative training samples and estimating interaction likelihood between user-item pairs along with the observed interactions. In this way, some potential interactions are inevitably mislabeled during training, which will hurt the model fidelity, hindering the model to recall the mislabeled items, especially the long-tail ones. In this work, we investigate the mislabeling issue from a new perspective of aleatoric uncertainty, which describes the inherent randomness of missing data. The randomness pushes us to go beyond merely the interaction likelihood and embrace aleatoric uncertainty modeling. Towards this end, we propose a new Aleatoric Uncertainty-aware Recommendation (AUR) framework that consists of a new uncertainty estimator along with a normal recommender mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SynthA1c&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#25968;&#25454;&#26469;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#39118;&#38505;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#34880;&#28082;&#23454;&#39564;&#23460;&#27979;&#37327;&#65292;&#20854;&#25935;&#24863;&#24615;&#39640;&#36798;87.6%&#12290;</title><link>http://arxiv.org/abs/2209.10043</link><description>&lt;p&gt;
SynthA1c:&#38024;&#23545;&#31958;&#23615;&#30149;&#39118;&#38505;&#20998;&#23618;&#30340;&#20020;&#24202;&#35299;&#37322;&#22411;&#24739;&#32773;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SynthA1c: Towards Clinically Interpretable Patient Representations for Diabetes Risk Stratification. (arXiv:2209.10043v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SynthA1c&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#25968;&#25454;&#26469;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#39118;&#38505;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#34880;&#28082;&#23454;&#39564;&#23460;&#27979;&#37327;&#65292;&#20854;&#25935;&#24863;&#24615;&#39640;&#36798;87.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#35786;&#26029;2&#22411;&#31958;&#23615;&#30149;(T2DM)&#23545;&#20110;&#21551;&#21160;&#21450;&#26102;&#30340;&#27835;&#30103;&#24178;&#39044;&#21644;&#29983;&#27963;&#26041;&#24335;&#25913;&#21464;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#20020;&#24202;&#35786;&#25152;&#35775;&#38382;&#26102;&#38388;&#30340;&#32553;&#30701;&#21644;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#21033;&#29992;&#24739;&#32773;&#22270;&#20687;&#25968;&#25454;&#26426;&#20250;&#24615;&#22320;&#36890;&#36807;&#21307;&#29983;&#23545;T2DM&#36827;&#34892;&#39069;&#22806;&#35786;&#26029;&#24037;&#20316;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#20687;&#34893;&#29983;&#30340;&#34920;&#22411;&#25968;&#25454;&#22312;&#34920;&#26684;&#23398;&#20064;&#20998;&#31867;&#22120;&#27169;&#22411;&#20013;&#39044;&#27979;T2DM&#39118;&#38505;&#65292;&#20197;&#33258;&#21160;&#22320;&#30830;&#23450;&#39640;&#39118;&#38505;&#24739;&#32773;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#34880;&#28082;&#23454;&#39564;&#23460;&#27979;&#35797;&#12290;&#19982;&#20256;&#32479;&#30340;&#20108;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#20915;&#31574;&#26641;&#27169;&#22411;&#23558;&#24739;&#32773;&#25968;&#25454;&#34920;&#31034;&#20026;&#8220;SynthA1c&#8221;&#28508;&#22312;&#21464;&#37327;&#65292;&#36825;&#20123;&#21464;&#37327;&#27169;&#25311;&#20102;&#34880;&#32418;&#34507;&#30333;A1c&#30340;&#32463;&#39564;&#23454;&#39564;&#23460;&#27979;&#37327;&#32467;&#26524;&#65292;&#20854;&#25935;&#24863;&#24615;&#39640;&#36798;87.6%&#12290;&#20026;&#20102;&#35780;&#20272;SynthA1c&#27169;&#22411;&#22312;&#20854;&#20182;&#24739;&#32773;&#32676;&#20307;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25512;&#24191;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of Type 2 Diabetes Mellitus (T2DM) is crucial to enable timely therapeutic interventions and lifestyle modifications. As the time available for clinical office visits shortens and medical imaging data become more widely available, patient image data could be used to opportunistically identify patients for additional T2DM diagnostic workup by physicians. We investigated whether image-derived phenotypic data could be leveraged in tabular learning classifier models to predict T2DM risk in an automated fashion to flag high-risk patients without the need for additional blood laboratory measurements. In contrast to traditional binary classifiers, we leverage neural networks and decision tree models to represent patient data as 'SynthA1c' latent variables, which mimic blood hemoglobin A1c empirical lab measurements, that achieve sensitivities as high as 87.6%. To evaluate how SynthA1c models may generalize to other patient populations, we introduce a novel generalizable metric
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07734</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#65306;&#31934;&#24515;&#31579;&#36873;&#30340;&#33258;&#30417;&#30563;&#23545;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#25104;&#21151;&#20135;&#29983;&#20102;&#24187;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07734
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#21019;&#24314;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#24040;&#22823;&#25104;&#26412;&#12290;&#23545;&#20110;&#26631;&#35760;&#24322;&#24120;&#31232;&#32570;&#25110;&#20960;&#20046;&#19981;&#23384;&#22312;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;SSL&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#36807;&#21435;&#24050;&#32463;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#26469;&#36827;&#34892;&#22522;&#20110;SSL&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#30340;&#31867;&#22411;&#23545;&#20934;&#30830;&#24615;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#26816;&#27979;&#27169;&#22411;&#21644;420&#20010;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25968;&#23383;&#21644;&#21487;&#35270;&#35777;&#25454;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;SSAD&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#32780;&#22312;&#32570;&#20047;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#65292;SSL&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20851;&#20110;&#22270;&#20687;&#22411;SSAD&#30340;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#12290;&#36825;&#19968;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#24674;&#22797;&#22312;&#35813;&#38543;&#26426;&#36712;&#36857;&#20013;&#30340;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#23545;&#27604;&#20854;&#20182;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#65292;&#20351;&#29992;&#20102;&#20135;&#21697;&#22810;&#22270;&#26469;&#20998;&#26512;&#25104;&#26412;&#27169;&#22411;&#23545;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#31639;&#27861;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#31995;&#32479;&#24615;&#33021;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2206.12672</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#12290;&#36825;&#19968;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#24674;&#22797;&#22312;&#35813;&#38543;&#26426;&#36712;&#36857;&#20013;&#30340;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#23545;&#27604;&#20854;&#20182;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#65292;&#20351;&#29992;&#20102;&#20135;&#21697;&#22810;&#22270;&#26469;&#20998;&#26512;&#25104;&#26412;&#27169;&#22411;&#23545;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#31639;&#27861;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#31995;&#32479;&#24615;&#33021;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#21644;&#29983;&#25104;&#19981;&#30830;&#23450;&#25968;&#25454;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#35774;&#32622;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#19982;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#20043;&#38388;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#22312;&#36825;&#20010;&#38543;&#26426;&#36712;&#36857;&#20013;&#24674;&#22797;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#35770;&#25991;&#23545;&#19981;&#21516;&#25104;&#26412;&#27169;&#22411;&#23545;&#36712;&#36857;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21033;&#29992;&#20135;&#21697;&#22810;&#22270;&#26469;&#27604;&#36739;&#26367;&#20195;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#24120;&#35265;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36873;&#25321;&#27599;&#20010;&#19981;&#30830;&#23450;&#27963;&#21160;&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#27491;&#30830;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#26159;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#25913;&#36827;&#31995;&#32479;&#24615;&#33021;&#30340;&#26377;&#21147;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose an algorithm for trace recovery from stochastically known logs, a setting that is becoming more common with the increasing number of sensors and predictive models that generate uncertain data. The suggested approach calculates the conformance between a process model and a stochastically known trace and recovers the best alignment within this stochastic trace as the true trace. The paper offers an analysis of the impact of various cost models on trace recovery accuracy and makes use of a product multi-graph to compare alternative trace recovery options. The average accuracy of our approach, evaluated using two publicly available datasets, is impressive, with an average recovery accuracy score of 90-97%, significantly improving a common heuristic that chooses the most likely value for each uncertain activity. We believe that the effectiveness of the proposed algorithm in recovering correct traces from stochastically known logs may be a powerful aid for developing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Yankee Swap&#36807;&#31243;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#29992;&#20110;&#25311;&#38453;&#31561;&#32423;&#20272;&#20540;&#30340;&#26080;&#20998;&#21106;&#29289;&#21697;&#20844;&#24179;&#20998;&#37197;&#65292;&#36825;&#31181;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#65292;&#24555;&#36895;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.08495</link><description>&lt;p&gt;
Yankee Swap&#65306;&#29992;&#20110;&#25311;&#38453;&#31561;&#32423;&#20272;&#20540;&#30340;&#24555;&#36895;&#20844;&#24179;&#20998;&#37197;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Yankee Swap: a Fast and Simple Fair Allocation Mechanism for Matroid Rank Valuations. (arXiv:2206.08495v5 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Yankee Swap&#36807;&#31243;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#29992;&#20110;&#25311;&#38453;&#31561;&#32423;&#20272;&#20540;&#30340;&#26080;&#20998;&#21106;&#29289;&#21697;&#20844;&#24179;&#20998;&#37197;&#65292;&#36825;&#31181;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#65292;&#24555;&#36895;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21442;&#19982;&#32773;&#20855;&#26377;&#25311;&#38453;&#31561;&#32423;&#20272;&#20540;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#26080;&#20998;&#21106;&#29289;&#21697;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#20442;&#35821;Yankee Swap&#36807;&#31243;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#23427;&#35745;&#31639;&#20986;&#21487;&#35777;&#26126;&#20844;&#24179;&#21644;&#39640;&#25928;&#30340;Lorenz&#25903;&#37197;&#20998;&#37197;&#12290;&#34429;&#28982;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#21487;&#29992;&#20110;&#35745;&#31639;&#27492;&#31867;&#20998;&#37197;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#20004;&#31181;&#25913;&#36827;&#26041;&#24335;&#12290; &#65288;a&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#65292;&#19981;&#20351;&#29992;&#22797;&#26434;&#30340;&#25311;&#38453;&#20248;&#21270;&#31639;&#27861;&#20316;&#20026;&#23376;&#20363;&#31243;&#12290; &#65288;b&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21487;&#25193;&#23637;&#30340;&#65307; &#23427;&#34987;&#35777;&#26126;&#27604;&#25152;&#26377;&#24050;&#30693;&#30340;&#35745;&#31639;Lorenz&#25903;&#37197;&#20998;&#37197;&#30340;&#31639;&#27861;&#26356;&#24555;&#12290;&#36825;&#20004;&#20010;&#23646;&#24615;&#23545;&#20110;&#20219;&#20309;&#30495;&#27491;&#20844;&#24179;&#20998;&#37197;&#29615;&#22659;&#20013;&#31639;&#27861;&#30340;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#65307;&#25105;&#20204;&#30340;&#36129;&#29486;&#20351;&#25105;&#20204;&#31163;&#36825;&#20010;&#30446;&#26631;&#26356;&#36817;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study fair allocation of indivisible goods when agents have matroid rank valuations. Our main contribution is a simple algorithm based on the colloquial Yankee Swap procedure that computes provably fair and efficient Lorenz dominating allocations. While there exist polynomial time algorithms to compute such allocations, our proposed method improves on them in two ways. (a) Our approach is easy to understand and does not use complex matroid optimization algorithms as subroutines. (b) Our approach is scalable; it is provably faster than all known algorithms to compute Lorenz dominating allocations. These two properties are key to the adoption of algorithms in any real fair allocation setting; our contribution brings us one step closer to this goal.
&lt;/p&gt;</description></item></channel></rss>