<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>CONA&#26159;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#20248;&#21270;&#28436;&#31034;&#20869;&#23481;&#24182;&#25552;&#20379;&#19978;&#19979;&#25991;&#24863;&#30693;&#22411;&#31572;&#26696;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#21644;&#26131;&#29702;&#35299;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18620</link><description>&lt;p&gt;
CONA: &#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#33539;&#24335;&#65292;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
CONA: A novel CONtext-Aware instruction paradigm for communication using large language model. (arXiv:2305.18620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18620
&lt;/p&gt;
&lt;p&gt;
CONA&#26159;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33258;&#21160;&#20248;&#21270;&#28436;&#31034;&#20869;&#23481;&#24182;&#25552;&#20379;&#19978;&#19979;&#25991;&#24863;&#30693;&#22411;&#31572;&#26696;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#21644;&#26131;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CONA&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#33539;&#24335;&#65292;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#26377;&#25928;&#36827;&#34892;&#30693;&#35782;&#20256;&#25773;&#12290;CONA&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#24182;&#32467;&#21512;DIKW&#65288;&#25968;&#25454;&#12289;&#20449;&#24687;&#12289;&#30693;&#35782;&#12289;&#26234;&#24935;&#65289;&#23618;&#32423;&#33258;&#21160;&#25351;&#23548;&#21644;&#20248;&#21270;&#28436;&#31034;&#20869;&#23481;&#65292;&#39044;&#27979;&#28508;&#22312;&#30340;&#21548;&#20247;&#38382;&#39064;&#65292;&#24182;&#36866;&#24212;&#21548;&#20247;&#32676;&#20307;&#30340;&#30693;&#35782;&#27700;&#24179;&#25552;&#20379;&#19978;&#19979;&#25991;&#24863;&#30693;&#22411;&#31572;&#26696;&#12290;CONA&#33539;&#24335;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#20854;&#29420;&#31435;&#21672;&#35810;&#26426;&#21046;&#21644;&#26681;&#26893;&#20110;DIKW&#23618;&#32423;&#30340;&#36882;&#24402;&#21453;&#39304;&#24490;&#29615;&#30340;&#32452;&#21512;&#12290;&#36825;&#31181;&#21327;&#21516;&#20316;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#20869;&#23481;&#65292;&#30830;&#20445;&#21548;&#20247;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#21644;&#33719;&#21462;&#12290;&#36825;&#31181;&#33539;&#24335;&#26159;&#25506;&#32034;LLM&#26102;&#20195;&#30340;&#30693;&#35782;&#20256;&#25773;&#21644;&#27807;&#36890;&#30340;&#26032;&#26041;&#27861;&#30340;&#26089;&#26399;&#20808;&#39537;&#65292;&#20026;&#26085;&#24120;&#30693;&#35782;&#20849;&#20139;&#22330;&#26223;&#25552;&#20379;&#26377;&#25928;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CONA, a novel context-aware instruction paradigm for effective knowledge dissemination using generative pre-trained transformer (GPT) models. CONA is a flexible framework designed to leverage the capabilities of Large Language Models (LLMs) and incorporate DIKW (Data, Information, Knowledge, Wisdom) hierarchy to automatically instruct and optimise presentation content, anticipate potential audience inquiries, and provide context-aware answers that adaptive to the knowledge level of the audience group. The unique aspect of the CONA paradigm lies in its combination of an independent advisory mechanism and a recursive feedback loop rooted on the DIKW hierarchy. This synergy significantly enhances context-aware contents, ensuring they are accessible and easily comprehended by the audience. This paradigm is an early pioneer to explore new methods for knowledge dissemination and communication in the LLM era, offering effective support for everyday knowledge sharing scenarios. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(ChatGPT-3.5&#12289;ChatGPT-4&#21644;Google Bard)&#22312;&#35299;&#20915;&#25968;&#23398;&#21644;&#36923;&#36753;&#38382;&#39064;&#19978;&#30340;&#27491;&#30830;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#20294;&#22312;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#20013;&#38656;&#35201;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.18618</link><description>&lt;p&gt;
&#25968;&#23398;&#19982;&#36923;&#36753;&#38382;&#39064;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;: ChatGPT-3.5&#12289;ChatGPT-4&#21644;Google Bard&#30340;&#21021;&#27493;&#27604;&#36739;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard. (arXiv:2305.18618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(ChatGPT-3.5&#12289;ChatGPT-4&#21644;Google Bard)&#22312;&#35299;&#20915;&#25968;&#23398;&#21644;&#36923;&#36753;&#38382;&#39064;&#19978;&#30340;&#27491;&#30830;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#20294;&#22312;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#20013;&#38656;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(ChatGPT-3.5&#12289;ChatGPT-4&#21644;Google Bard)&#22312;&#35299;&#20915;&#25968;&#23398;&#21644;&#36923;&#36753;&#38382;&#39064;&#26102;&#30340;&#27491;&#30830;&#24615;&#23545;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;30&#20010;&#28165;&#26224;&#12289;&#26080;&#20108;&#20041;&#24615;&#12289;&#20165;&#20351;&#29992;&#32431;&#25991;&#26412;&#19988;&#20855;&#26377;&#29420;&#29305;&#23450;&#20041;&#30340;&#27491;&#30830;&#31572;&#26696;&#30340;&#38382;&#39064;&#65292;&#20998;&#20026;&#20004;&#32452;&#24182;&#20998;&#21035;&#21521;&#27599;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20986;&#19977;&#36941;&#12290;&#36890;&#36807;&#35760;&#24405;&#24182;&#35752;&#35770;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#31616;&#21333;&#30340;&#31639;&#26415;&#21644;&#20195;&#25968;&#34920;&#36798;&#24335;...
&lt;/p&gt;
&lt;p&gt;
A comparison between three chatbots which are based on large language models, namely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their ability to give correct answers to mathematics and logic problems. In particular, we check their ability to Understand the problem at hand; Apply appropriate algorithms or methods for its solution; and Generate a coherent response and a correct answer. We use 30 questions that are clear, without any ambiguities, fully described with plain text only, and have a unique, well defined correct answer. The questions are divided into two sets of 15 each. The questions of Set A are 15 "Original" problems that cannot be found online, while Set B contains 15 "Published" problems that one can find online, usually with their solution. Each question is posed three times to each chatbot. The answers are recorded and discussed, highlighting their strengths and weaknesses. It has been found that for straightforward arithmetic, algebraic expressions
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#26412;&#31185;&#29983;&#36328;&#23398;&#31185;&#21512;&#20316;&#23398;&#20064;&#20013;&#20351;&#29992;ChatGPT&#65292;&#21487;&#20197;&#20419;&#36827;&#20854;&#36328;&#23398;&#31185;&#38382;&#39064;&#35299;&#20915;&#12289;&#36523;&#20307;&#21644;&#35748;&#30693;&#25237;&#20837;&#65292;&#23588;&#20854;&#23545;STEM&#23398;&#29983;&#20135;&#29983;&#31215;&#26497;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18616</link><description>&lt;p&gt;
&#25235;&#20303;&#26426;&#36935;&#65292;&#36814;&#25509;&#25361;&#25112;&#65306;&#22312;&#26412;&#31185;&#36328;&#23398;&#31185;&#21512;&#20316;&#23398;&#20064;&#20013;&#20351;&#29992;ChatGPT
&lt;/p&gt;
&lt;p&gt;
Embrace Opportunities and Face Challenges: Using ChatGPT in Undergraduate Students' Collaborative Interdisciplinary Learning. (arXiv:2305.18616v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18616
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#26412;&#31185;&#29983;&#36328;&#23398;&#31185;&#21512;&#20316;&#23398;&#20064;&#20013;&#20351;&#29992;ChatGPT&#65292;&#21487;&#20197;&#20419;&#36827;&#20854;&#36328;&#23398;&#31185;&#38382;&#39064;&#35299;&#20915;&#12289;&#36523;&#20307;&#21644;&#35748;&#30693;&#25237;&#20837;&#65292;&#23588;&#20854;&#23545;STEM&#23398;&#29983;&#20135;&#29983;&#31215;&#26497;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#21518;&#65292;&#21463;&#21040;&#20840;&#29699;&#23398;&#29983;&#21644;&#25945;&#32946;&#24037;&#20316;&#32773;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#21326;&#12298;2023&#12299;&#30340;&#19968;&#20221;&#22312;&#32447;&#25253;&#21578;&#31216;&#20854;&#26159;&#21382;&#21490;&#19978;&#22686;&#38271;&#26368;&#24555;&#30340;&#28040;&#36153;&#24212;&#29992;&#31243;&#24207;&#12290;&#34429;&#28982;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#35752;&#35770;&#20351;&#29992;ChatGPT&#30340;&#35805;&#39064;&#24456;&#22810;&#65292;&#20294;&#20854;&#23545;&#36328;&#23398;&#31185;&#21512;&#20316;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#23454;&#35777;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#25506;&#31350;&#20854;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25311;&#23454;&#39564;&#24615;&#30740;&#31350;&#65292;&#25307;&#21215;&#20102;130&#21517;&#26412;&#31185;&#29983;&#65288;STEM&#21644;&#38750;STEM&#65289;&#65292;&#20182;&#20204;&#22312;&#20004;&#21608;&#20869;&#23398;&#20064;&#25968;&#23383;&#32032;&#20859;&#65292;&#26377;&#20123;&#20351;&#29992;ChatGPT&#65292;&#26377;&#20123;&#19981;&#20351;&#29992;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#26377;&#20851;&#36328;&#23398;&#31185;&#38382;&#39064;&#35299;&#20915;&#12289;&#36523;&#20307;&#21644;&#35748;&#30693;&#25237;&#20837;&#20197;&#21450;&#23545;Chat GPT&#20351;&#29992;&#30340;&#20010;&#20154;&#21453;&#24605;&#30340;&#21608;&#24230;&#35843;&#26597;&#12290;&#23545;&#35843;&#26597;&#32467;&#26524;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#35805;&#39064;&#23545;&#36328;&#23398;&#31185;&#38382;&#39064;&#35299;&#20915;&#21644;&#36523;&#20307;&#12289;&#35748;&#30693;&#25237;&#20837;&#26377;&#26174;&#33879;&#30340;&#20027;&#25928;&#24212;&#65292;&#23398;&#31185;&#32972;&#26223;&#21644;ChatGPT&#26465;&#20214;&#23545;&#35748;&#30693;&#25237;&#20837;&#26377;&#36739;&#23567;&#30340;&#20132;&#20114;&#25928;&#24212;&#65292; ChatGPT&#23545;STEM&#23398;&#29983;&#20855;&#26377;&#31215;&#26497;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, launched in November 2022, has gained widespread attention from students and educators globally, with an online report by Hu (2023) stating it as the fastest-growing consumer application in history. While discussions on the use of ChatGPT in higher education are abundant, empirical studies on its impact on collaborative interdisciplinary learning are rare. To investigate its potential, we conducted a quasi-experimental study with 130 undergraduate students (STEM and non-STEM) learning digital literacy with or without ChatGPT over two weeks. Weekly surveys were conducted on collaborative interdisciplinary problem-solving, physical and cognitive engagement, and individual reflections on ChatGPT use. Analysis of survey responses showed significant main effects of topics on collaborative interdisciplinary problem-solving and physical and cognitive engagement, a marginal interaction effect between disciplinary backgrounds and ChatGPT conditions for cognitive engagement, and a signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#26469;&#22788;&#29702;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;NTS&#25968;&#25454;&#20013;&#30340;&#22270;&#21160;&#24577;&#21644;&#32570;&#22833;&#36793;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18612</link><description>&lt;p&gt;
&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders. (arXiv:2305.18612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#26469;&#22788;&#29702;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;NTS&#25968;&#25454;&#20013;&#30340;&#22270;&#21160;&#24577;&#21644;&#32570;&#22833;&#36793;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26159;&#36817;&#24180;&#26469;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#22823;&#31867;&#65292;&#21253;&#25324;&#65288;1&#65289;&#20027;&#35201;&#20851;&#27880;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#30340;&#28145;&#24230;&#36882;&#24402;&#25110;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;MTS&#22266;&#26377;&#22270;&#32467;&#26500;&#30340;&#25299;&#25169;&#20449;&#24687;&#20316;&#20026;&#25554;&#34917;&#30340;&#20851;&#31995;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#25299;&#25169;&#20449;&#24687;&#65292;&#35201;&#20040;&#20551;&#23450;&#22270;&#32467;&#26500;&#22266;&#23450;&#19988;&#20934;&#30830;&#24050;&#30693;&#12290;&#22240;&#27492;&#65292;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NTS&#65289;&#25968;&#25454;&#20013;&#65292;&#23427;&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22270;&#21160;&#24577;&#36827;&#34892;&#31934;&#30830;&#30340;&#25554;&#34917;&#65292;&#20854;&#20013;&#24213;&#23618;&#22270;&#19981;&#26029;&#21464;&#21270;&#24182;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#36793;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21253;&#21547;&#33410;&#28857;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20013;&#32570;&#22833;&#20540;&#30340;NTS&#25554;&#34917;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PGE-VAE&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#23450;&#20301;&#32534;&#30721;&#25216;&#26415;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#21512;&#24182;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#22270;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#21644;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22270;&#29983;&#25104;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#30340;&#36793;&#24182;&#36866;&#24212;&#22270;&#21160;&#24577;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new mod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#25216;&#26415;&#20462;&#22797;Java&#28431;&#27934;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;Java&#28431;&#27934;&#20462;&#22797;&#22522;&#20934;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;Java&#28431;&#27934;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.18607</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#23433;&#20840;&#28431;&#27934;&#30340;&#26377;&#25928;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Effective Are Neural Networks for Fixing Security Vulnerabilities. (arXiv:2305.18607v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18607
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#25216;&#26415;&#20462;&#22797;Java&#28431;&#27934;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;Java&#28431;&#27934;&#20462;&#22797;&#22522;&#20934;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;Java&#28431;&#27934;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#28431;&#27934;&#20462;&#22797;&#26159;&#19968;&#39033;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#22256;&#38590;&#20219;&#21153;&#12290;&#20004;&#32452;&#25216;&#26415;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#65306;&#65288;1&#65289;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23427;&#20204;&#24050;&#32463;&#38024;&#23545;&#20195;&#30721;&#23436;&#25104;&#31561;&#20219;&#21153;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#33258;&#21160;&#20462;&#22797;&#36719;&#20214;&#38169;&#35823;&#30340;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#25216;&#26415;&#12290;&#26412;&#25991;&#26159;&#39318;&#27425;&#30740;&#31350;&#21644;&#27604;&#36739;LLMs&#21644;DL-based APR&#27169;&#22411;&#22312;Java&#28431;&#27934;&#20462;&#22797;&#33021;&#21147;&#26041;&#38754;&#30340;&#35770;&#25991;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23558;&#24182;&#35780;&#20272;&#20116;&#20010;LLMs&#65288;Codex&#12289;CodeGen&#12289;CodeT5&#12289;PLBART&#21644;InCoder&#65289;&#12289;&#22235;&#20010;&#31934;&#32454;&#35843;&#25972;&#30340;LLMs&#21644;&#22235;&#31181;&#22522;&#20110;DL&#30340;APR&#25216;&#26415;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;Java&#28431;&#27934;&#22522;&#20934;&#65288;Vul4J&#21644;VJBench&#65289;&#19978;&#65292;&#65288;2&#65289;&#35774;&#35745;&#20195;&#30721;&#36716;&#25442;&#26469;&#35299;&#20915;&#23545;Codex&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#37325;&#21472;&#23041;&#32961;&#65292;&#65288;3&#65289;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;Java&#28431;&#27934;&#20462;&#22797;&#22522;&#20934;VJBench&#65292;&#20197;&#21450;&#23427;&#30340;&#36716;&#21270;&#29256;&#26412;VJBench-trans&#65292;&#65288;4&#65289;&#35780;&#20272;LLMs&#21644;APR&#25216;&#26415;&#23545;&#36716;&#21270;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs.  This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#30740;&#31350;&#20102;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.18585</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#35774;&#35745;&#23545;&#25239;&#25915;&#20987;&#21644;&#35780;&#20272;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models. (arXiv:2305.18585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#30740;&#31350;&#20102;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#35768;&#22810;&#20262;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#20167;&#24680;&#35328;&#35770;&#26159;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#23457;&#26597;&#20869;&#23481;&#24182;&#20419;&#36827;&#25991;&#26126;&#35752;&#35770;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#21487;&#33021;&#20250;&#34987;&#23545;&#25239;&#25915;&#20987;&#25152;&#35823;&#23548;&#65292;&#24341;&#36215;&#20854;&#40065;&#26834;&#24615;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#21333;&#29420;&#35752;&#35770;&#20102;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#27809;&#26377;&#20840;&#38754;&#30740;&#31350;&#20182;&#20204;&#30340;&#20132;&#38598;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#21019;&#26032;&#22312;&#20110;&#32467;&#21512;&#36825;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#35782;&#21035;&#28508;&#22312;&#30340;&#28431;&#27934;&#65292;&#20174;&#32780;&#35774;&#35745;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#21644;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of social media has given rise to numerous ethical challenges, with hate speech among the most significant concerns. Researchers are attempting to tackle this problem by leveraging hate-speech detection and employing language models to automatically moderate content and promote civil discourse. Unfortunately, recent studies have revealed that hate-speech detection systems can be misled by adversarial attacks, raising concerns about their resilience. While previous research has separately addressed the robustness of these models under adversarial attacks and their interpretability, there has been no comprehensive study exploring their intersection. The novelty of our work lies in combining these two critical aspects, leveraging interpretability to identify potential vulnerabilities and enabling the design of targeted adversarial attacks. We present a comprehensive and comparative analysis of adversarial robustness exhibited by various hate-speech detection models. Our study e
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;GPT-4&#32534;&#20889;TikZ&#20195;&#30721;&#21487;&#25552;&#39640;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.18583</link><description>&lt;p&gt;
&#21033;&#29992;GPT-4&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable Text-to-Image Generation with GPT-4. (arXiv:2305.18583v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18583
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;GPT-4&#32534;&#20889;TikZ&#20195;&#30721;&#21487;&#25552;&#39640;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#25353;&#29031;&#25991;&#26412;&#35828;&#26126;&#36827;&#34892;&#25805;&#20316;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#38656;&#35201;&#31354;&#38388;&#25512;&#29702;&#30340;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#29983;&#25104;&#29992;&#20110;&#36890;&#36807;TikZ&#22270;&#35299;&#25991;&#26412;&#36755;&#20837;&#30340;&#20195;&#30721;&#29255;&#27573;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Control-GPT&#26469;&#20026;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#31649;&#36947;&#25552;&#20379;&#25351;&#23548;&#65292;GPT-4&#29983;&#25104;&#32534;&#31243;&#33609;&#22270;&#65292;&#22686;&#24378;&#20102;&#20182;&#20204;&#36981;&#24490;&#25351;&#31034;&#30340;&#33021;&#21147;&#12290;Control-GPT&#36890;&#36807;&#26597;&#35810;GPT-4&#26469;&#32534;&#20889;TikZ&#20195;&#30721;&#65292;&#29983;&#25104;&#30340;&#33609;&#22270;&#19982;&#25991;&#26412;&#35828;&#26126;&#19968;&#36215;&#29992;&#20316;&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;ControlNet&#65289;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;&#35757;&#32451;&#25105;&#20204;&#30340;&#31649;&#36947;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#21253;&#21547;&#23545;&#20934;&#25991;&#26412;&#65292;&#22270;&#20687;&#21644;&#33609;&#22270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#25513;&#30721;&#36716;&#25442;&#20026;&#22810;&#36793;&#24418;&#26469;&#27169;&#20223;&#27979;&#35797;&#26102;&#20351;&#29992;&#30340;&#33609;&#22270;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#65292;Control-GPT&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#21462;&#24471;&#20102;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24778;&#20154;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly
&lt;/p&gt;</description></item><item><title>&#27492;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#31354;&#38388;&#30697;&#38453;&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#26029;&#65292;&#26159;&#20855;&#26377;&#26102;&#38388;&#25442;&#31354;&#38388;&#30340;&#26522;&#20030;&#31639;&#27861;&#65307;&#20013;&#32512;&#38381;&#21253;&#20351;&#25805;&#20316;&#26356;&#20026;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.18575</link><description>&lt;p&gt;
&#22312;GPU&#19978;&#36827;&#34892;&#22522;&#20110;&#25628;&#32034;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Search-Based Regular Expression Inference on a GPU. (arXiv:2305.18575v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18575
&lt;/p&gt;
&lt;p&gt;
&#27492;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#31354;&#38388;&#30697;&#38453;&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#26029;&#65292;&#26159;&#20855;&#26377;&#26102;&#38388;&#25442;&#31354;&#38388;&#30340;&#26522;&#20030;&#31639;&#27861;&#65307;&#20013;&#32512;&#38381;&#21253;&#20351;&#25805;&#20316;&#26356;&#20026;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#34920;&#36798;&#24335;&#25512;&#26029;(REI)&#26159;&#19968;&#20010;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#31243;&#24207;&#32508;&#21512;&#38382;&#39064;&#65292;&#23427;&#25509;&#21463; &#27491;&#12289;&#21453;&#26679;&#20363; &#21644; &#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#25104;&#26412;&#24230;&#37327; &#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#19968;&#31181;&#31934;&#30830;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;(&#21363;&#25509;&#21463;&#25152;&#26377;&#27491;&#26679;&#20363;&#21644;&#25298;&#32477;&#25152;&#26377;&#21453;&#26679;&#20363;)&#65292;&#24182;&#30456;&#23545;&#20110;&#25104;&#26412;&#24230;&#37327;&#26159;&#26368;&#23567;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;REI&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#20219;&#24847;&#23383;&#27597;&#34920;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26522;&#20030;&#24615;&#65292;&#24182;&#20197;&#26102;&#38388;&#25442;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#31639;&#27861;&#24605;&#24819;&#26159;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#25628;&#32034;&#31354;&#38388;&#31616;&#27905;&#22320;&#23454;&#29616;&#20026;&#20301;&#21521;&#37327;&#30697;&#38453;&#30340;&#36830;&#32493;&#24418;&#24335;&#12290;&#24635;&#20307;&#19978;&#65292;&#22312;&#32467;&#26524;&#30697;&#38453;&#20013;&#65292;&#25152;&#26377;&#23376;&#35821;&#35328;&#37117;&#34987;&#34920;&#31034;&#20026;&#29305;&#24449;&#24207;&#21015;&#65292;&#23545;&#20110;&#27491;&#12289;&#21453;&#20363;&#23376;&#30340;&#24182;&#38598;&#20351;&#29992; infix-closure&#12290;&#20854;&#25968;&#23398;&#26412;&#36136;&#19978;&#26159;(&#27491;&#24335;&#24130;&#32423;&#25968;&#30340;&#21464;&#20307;)&#30340;&#21322;&#29615;&#12290;&#20013;&#32512;&#38381;&#21253;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#21322;&#29615;&#30340;&#25805;&#20316;&#65292;&#20174;&#24213;&#37096;&#21521;&#19978;&#22320;&#36890;&#36807;&#26500;&#25104;&#26356;&#22823;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#65292;&#26368;&#23567;&#21270;&#25968;&#25454;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular expression inference (REI) is a supervised machine learning and program synthesis problem that takes a cost metric for regular expressions, and positive and negative examples of strings as input. It outputs a regular expression that is precise (i.e., accepts all positive and rejects all negative examples), and minimal w.r.t. to the cost metric. We present a novel algorithm for REI over arbitrary alphabets that is enumerative and trades off time for space. Our main algorithmic idea is to implement the search space of regular expressions succinctly as a contiguous matrix of bitvectors. Collectively, the bitvectors represent, as characteristic sequences, all sub-languages of the infix-closure of the union of positive and negative examples. Mathematically, this is a semiring of (a variant of) formal power series. Infix-closure enables bottom-up compositional construction of larger from smaller regular expressions using the operations of our semiring. This minimises data movement an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.18569</link><description>&lt;p&gt;
ChatGPT&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#35299;&#20915;LLM&#20013;&#19981;&#20844;&#24179;&#30340;&#38382;&#39064;&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;LLM&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#39046;&#22495;&#26102;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20844;&#24179;&#35780;&#20272;&#26041;&#38754;&#65292;&#25968;&#37327;&#20998;&#26512;&#21644;&#28145;&#20837;&#30740;&#31350;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;ChatGPT&#22312;&#21253;&#25324;&#25945;&#32946;&#12289;&#29359;&#32618;&#23398;&#12289;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#21644;&#20010;&#20154;&#20844;&#24179;&#24615;&#65292;&#24182;&#35266;&#23519;&#20102;&#22312;&#19968;&#31995;&#21015;&#26377;&#20559;&#25110;&#26080;&#20559;&#25552;&#31034;&#19979;ChatGPT&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#20415;&#20110;&#20559;&#35265;&#32531;&#35299;&#65292;&#20419;&#36827;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18553</link><description>&lt;p&gt;
&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Controllable Path of Destruction. (arXiv:2305.18553v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27585;&#28781;&#36335;&#24452;&#65288;PoD&#65289;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#30772;&#22351;&#19968;&#32452;&#29289;&#21697;&#26469;&#20135;&#29983;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#20026;&#27599;&#20010;&#30772;&#22351;&#27493;&#39588;&#21019;&#24314;&#19968;&#20010;&#19982;&#30456;&#24212;&#20462;&#22797;&#21160;&#20316;&#30456;&#20851;&#30340;&#35757;&#32451;&#23454;&#20363;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#36890;&#36807;&#20174;&#20219;&#24847;&#29366;&#24577;&#8220;&#20462;&#22797;&#8221;&#26469;&#29983;&#25104;&#26032;&#30340;&#29289;&#21697;&#12290;PoD&#26041;&#27861;&#22312;&#21407;&#22987;&#35757;&#32451;&#31034;&#20363;&#26041;&#38754;&#38750;&#24120;&#33410;&#30465;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#21512;&#30001;&#20998;&#31867;&#25968;&#25454;&#32452;&#25104;&#30340;&#21151;&#33021;&#37096;&#20214;&#65292;&#20363;&#22914;&#28216;&#25103;&#20851;&#21345;&#21644;&#31163;&#25955;&#30340;3D&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#25193;&#23637;&#21040;&#20801;&#35768;&#35774;&#35745;&#24072;&#25511;&#21046;&#29983;&#25104;&#30340;&#29289;&#21697;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#21521;&#26500;&#25104;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#24341;&#20837;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#22312;2D&#22320;&#29282;&#35774;&#32622;&#20197;&#21450;&#23567;&#22411;3D&#20048;&#39640;&#27773;&#36710;&#39046;&#22495;&#27979;&#35797;&#20102;&#21487;&#25511;PoD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path of Destruction (PoD) is a self-supervised method for learning iterative generators. The core idea is to produce a training set by destroying a set of artifacts, and for each destructive step create a training instance based on the corresponding repair action. A generator trained on this dataset can then generate new artifacts by ``repairing'' from arbitrary states. The PoD method is very data-efficient in terms of original training examples and well-suited to functional artifacts composed of categorical data, such as game levels and discrete 3D structures. In this paper, we extend the Path of Destruction method to allow designer control over aspects of the generated artifacts. Controllability is introduced by adding conditional inputs to the state-action pairs that make up the repair trajectories. We test the controllable PoD method in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.
&lt;/p&gt;</description></item><item><title>RLAD&#26159;&#39318;&#20010;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#24212;&#29992;&#22522;&#20110;&#20687;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#36335;&#24452;&#28857;&#32534;&#30721;&#22120;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18510</link><description>&lt;p&gt;
RLAD&#65306;&#22522;&#20110;&#20687;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban Environments. (arXiv:2305.18510v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18510
&lt;/p&gt;
&lt;p&gt;
RLAD&#26159;&#39318;&#20010;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#24212;&#29992;&#22522;&#20110;&#20687;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#36335;&#24452;&#28857;&#32534;&#30721;&#22120;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#24212;&#29992;&#20110;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#24863;&#30693;&#35757;&#32451;&#19982;&#39550;&#39542;&#31574;&#30053;&#35757;&#32451;&#20998;&#31163;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#20026;&#20102;&#36991;&#20813;&#22312;&#31574;&#30053;&#32593;&#32476;&#26049;&#36793;&#35757;&#32451;&#21367;&#31215;&#32534;&#30721;&#22120;&#65292;&#22240;&#20026;&#36825;&#31181;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#29305;&#24449;&#34920;&#31034;&#36864;&#21270;&#21644;&#33258;&#25105;&#36807;&#24230;&#25311;&#21512;&#31561;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#29615;&#22659;&#34920;&#31034;&#19982;&#19979;&#28216;&#20219;&#21153;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RLAD&#65292;&#36825;&#26159;&#39318;&#20010;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#24212;&#29992;&#22522;&#20110;&#20687;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLfP&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#22312;&#27492;&#39046;&#22495;&#20013;RLfP&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#65306;i&#65289;&#19968;&#31181;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#22270;&#20687;&#22686;&#24378;&#21644;&#33258;&#36866;&#24212;&#23616;&#37096;&#20449;&#21495;&#28151;&#21512;&#65288;A-LIX&#65289;&#23618;&#30340;&#20248;&#21183;&#65307;ii&#65289;WayConv1D&#65292;&#19968;&#31181;&#21033;&#29992;2D&#20960;&#20309;&#20449;&#24687;&#30340;&#36335;&#24452;&#28857;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches of Reinforcement Learning (RL) applied in urban Autonomous Driving (AD) focus on decoupling the perception training from the driving policy training. The main reason is to avoid training a convolution encoder alongside a policy network, which is known to have issues related to sample efficiency, degenerated feature representations, and catastrophic self-overfitting. However, this paradigm can lead to representations of the environment that are not aligned with the downstream task, which may result in suboptimal performances. To address this limitation, this paper proposes RLAD, the first Reinforcement Learning from Pixels (RLfP) method applied in the urban AD domain. We propose several techniques to enhance the performance of an RLfP algorithm in this domain, including: i) an image encoder that leverages both image augmentations and Adaptive Local Signal Mixing (A-LIX) layers; ii) WayConv1D, which is a waypoint encoder that harnesses the 2D geometrical information of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#8212;&#8212;&#20195;&#30721;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#27604;&#65292;&#20195;&#30721;&#25552;&#31034;&#26377;&#30528;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#65292;&#33021;&#22815;&#25552;&#39640;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18507</link><description>&lt;p&gt;
&#20195;&#30721;&#25552;&#31034;&#65306;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#8212;&#8212;&#20195;&#30721;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#27604;&#65292;&#20195;&#30721;&#25552;&#31034;&#26377;&#30528;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#65292;&#33021;&#22815;&#25552;&#39640;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#25552;&#31034;&#26041;&#27861;&#25193;&#22823;&#20102;&#35268;&#27169;&#65292;&#20197;&#35299;&#38145;&#24191;&#27867;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#31034;&#26041;&#27861;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#20013;&#38388;&#27493;&#39588;&#20197;&#24110;&#21161;&#25512;&#29702;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#23436;&#21892;&#30340;&#20219;&#21153;&#32553;&#20943;&#21644;&#28151;&#28102;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#26679;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20195;&#30721;&#25552;&#31034;&#65292;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#65292;&#20855;&#26377;&#38646;-shot&#21644;&#23569;-shot&#29256;&#26412;&#65292;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;7&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20195;&#30721;&#25552;&#31034;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#20195;&#30721;&#25552;&#31034;&#30340;&#24615;&#33021;&#21644;&#38480;&#21046;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#21644;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#20351;&#29992;&#31526;&#21495;&#25552;&#31034;&#30456;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#20195;&#30721;&#25552;&#31034;&#21644;&#24605;&#36335;&#38142;&#25552;&#31034;&#30340;&#38598;&#21512;&#65292;&#20197;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show throu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;</title><link>http://arxiv.org/abs/2305.18505</link><description>&lt;p&gt;
&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#33539;&#20363;&#65292;&#22312;&#27492;&#33539;&#20363;&#19979;&#65292;RL&#20195;&#29702;&#23398;&#20064;&#20351;&#29992;&#23545;&#36712;&#36857;&#30340;&#25104;&#23545;&#20248;&#20808;&#32423;&#21453;&#39304;&#26469;&#26368;&#20248;&#21270;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#26126;&#30830;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#23613;&#31649;RLHF&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#29992;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#23454;&#35777;&#30740;&#31350;&#24182;&#26410;&#35299;&#20915;&#22914;&#20309;&#39640;&#25928;&#37319;&#26679;&#36712;&#36857;&#23545;&#20197;&#26597;&#35810;&#20154;&#31867;&#21453;&#39304;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#25506;&#32034;&#24615;&#36712;&#36857;&#65292;&#22312;&#25910;&#38598;&#20219;&#20309;&#20154;&#31867;&#21453;&#39304;&#20043;&#21069;&#65292;&#20351;&#23398;&#20064;&#38544;&#34255;&#30340;&#22870;&#21169;&#20989;&#25968;&#26356;&#21152;&#20934;&#30830;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#22522;&#20110;&#20559;&#22909;&#27169;&#22411;&#19979;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#20154;&#31867;&#21453;&#39304;&#26356;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#32435;&#20837;&#32447;&#24615;&#21644;&#20302;&#31209;MDPs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#30340;&#21453;&#39304;&#30340;RLHF&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#20248;&#21270;&#20855;&#26377;&#26377;&#38480;&#21453;&#39304;&#30340;&#20219;&#21153;&#26102;&#33719;&#24471;&#25506;&#32034;&#24615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18500</link><description>&lt;p&gt;
VAST&#65306;&#19968;&#31181;&#35270;&#21548;&#23383;&#24149;&#25991;&#26412;&#20840;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23436;&#20840;&#25506;&#32034;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#65292;&#32780;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#20840;&#27169;&#24577;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;VAST-27M&#65292;&#24314;&#31435;&#22810;&#27169;&#24577;&#35270;&#39057;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#24182;&#19982;&#25991;&#26412;&#36827;&#34892;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;2700&#19975;&#20010;&#24320;&#25918;&#39046;&#22495;&#35270;&#39057;&#29255;&#27573;&#65292;&#24182;&#20998;&#21035;&#35757;&#32451;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#22120;&#20197;&#29983;&#25104;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29983;&#25104;&#30340;&#23383;&#24149;&#12289;&#23383;&#24149;&#21644;&#25351;&#23548;&#25552;&#31034;&#38598;&#25104;&#21040;&#20840;&#27169;&#24577;&#23383;&#24149;&#20013;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;VAST-27M&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#65292;&#23427;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#24182;&#26356;&#22909;&#22320;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vis
&lt;/p&gt;</description></item><item><title>ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.18498</link><description>&lt;p&gt;
ANPL&#65306;&#20351;&#29992;&#20132;&#20114;&#24335;&#20998;&#35299;&#32534;&#35793;&#33258;&#28982;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18498
&lt;/p&gt;
&lt;p&gt;
ANPL&#26159;&#19968;&#20010;&#32534;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#22312;&#36890;&#36807;&#33258;&#28982;&#20132;&#20114;&#22686;&#24378;&#32534;&#31243;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#23558;&#24120;&#35265;&#30340;&#20351;&#29992;&#27169;&#24335;&#32534;&#35793;&#20026;&#32534;&#31243;&#35821;&#35328;&#65292;&#20363;&#22914;Python&#65292;&#20294;&#22914;&#20309;&#32534;&#36753;&#21644;&#35843;&#35797;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31243;&#24207;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ANPL&#65292;&#19968;&#31181;&#32534;&#31243;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#20998;&#35299;&#29305;&#23450;&#20110;&#29992;&#25143;&#30340;&#20219;&#21153;&#12290;&#22312;ANPL&#31243;&#24207;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#30452;&#25509;&#25805;&#20316;&#33609;&#22270;&#65292;&#35813;&#33609;&#22270;&#25351;&#23450;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#25968;&#25454;&#27969;&#12290;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27880;&#37322;&#27169;&#22359;&#25110;&#23380;&#65292;&#23558;&#29983;&#25104;&#21151;&#33021;&#30340;&#26114;&#36149;&#20219;&#21153;&#21368;&#36733;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#32473;&#23450;&#19968;&#20010;ANPL&#31243;&#24207;&#65292;ANPL&#32534;&#35793;&#22120;&#20250;&#29983;&#25104;&#19968;&#20010;&#26377;&#26426;&#30340;Python&#31243;&#24207;&#65292;&#23454;&#29616;&#23380;&#20013;&#30340;&#21151;&#33021;&#65292;&#24182;&#36981;&#23432;&#33609;&#22270;&#20013;&#25351;&#23450;&#30340;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#23558;ANPL&#37096;&#32626;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#65292;&#23427;&#26159;&#19968;&#32452;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#29305;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advents of Large Language Models (LLMs) have shown promise in augmenting programming using natural interactions. However, while LLMs are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an LLM-generated program. We introduce ANPL, a programming system that allows users to decompose user-specific tasks. In an ANPL program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the LLM. Given an ANPL program, the ANPL compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#37327;&#25104;&#23545;&#26679;&#20363;&#30340;&#20391;&#20449;&#24687;&#30452;&#25509;&#23398;&#20064;&#25968;&#25454;&#32858;&#31867;&#30340;&#26041;&#27861;DMS&#65292;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26080;&#38656;&#30693;&#36947;&#31867;&#21035;&#25968;&#12289;&#31867;&#20013;&#24515;&#25110;&#32773;&#20219;&#20309;&#30456;&#20284;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20391;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#27714;&#23558;&#30456;&#21516;&#30340;&#25968;&#25454;&#28857;&#20998;&#25104;&#19981;&#21516;&#30340;&#32858;&#31867;&#65292;&#19988;&#22312;&#22266;&#26377;&#30340;&#21644;&#38750;&#22266;&#26377;&#30340;&#25968;&#25454;&#38598;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18492</link><description>&lt;p&gt;
DMS&#65306;&#22522;&#20110;&#20391;&#20449;&#24687;&#30340;&#26080;&#38656;&#30693;&#36947;&#31867;&#21035;&#25968;&#21644;&#36317;&#31163;&#24230;&#37327;&#30340;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific Clustering Using Side Information. (arXiv:2305.18492v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#37327;&#25104;&#23545;&#26679;&#20363;&#30340;&#20391;&#20449;&#24687;&#30452;&#25509;&#23398;&#20064;&#25968;&#25454;&#32858;&#31867;&#30340;&#26041;&#27861;DMS&#65292;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26080;&#38656;&#30693;&#36947;&#31867;&#21035;&#25968;&#12289;&#31867;&#20013;&#24515;&#25110;&#32773;&#20219;&#20309;&#30456;&#20284;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20391;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#27714;&#23558;&#30456;&#21516;&#30340;&#25968;&#25454;&#28857;&#20998;&#25104;&#19981;&#21516;&#30340;&#32858;&#31867;&#65292;&#19988;&#22312;&#22266;&#26377;&#30340;&#21644;&#38750;&#22266;&#26377;&#30340;&#25968;&#25454;&#38598;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#25104;&#23545;&#26679;&#20363;&#30340;&#20391;&#20449;&#24687;&#30452;&#25509;&#23398;&#20064;&#25968;&#25454;&#32858;&#31867;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26080;&#38656;&#30693;&#36947;&#31867;&#21035;&#25968;&#12289;&#31867;&#20013;&#24515;&#25110;&#32773;&#20219;&#20309;&#30456;&#20284;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20391;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#27714;&#23558;&#30456;&#21516;&#30340;&#25968;&#25454;&#28857;&#20998;&#25104;&#19981;&#21516;&#30340;&#32858;&#31867;&#12290;&#21463;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#33258;&#23450;&#20041;&#30340;&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#32858;&#31867;&#26041;&#27861;&#8212;&#8212;Differentiable Mean Shift (DMS)&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#26080;&#20851;&#32858;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#24378;&#22823;&#30340;&#32858;&#31867;&#23450;&#20041;&#65292;&#32780;&#19981;&#24517;&#24378;&#21046;&#35201;&#27714;&#27599;&#20010;&#31751;&#22312;&#35757;&#32451;&#26399;&#38388;&#21576;&#29616;&#12290;DMS&#22312;&#22266;&#26377;&#30340;&#21644;&#38750;&#22266;&#26377;&#30340;&#25968;&#25454;&#38598;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach, in which we learn to cluster data directly from side information, in the form of a small set of pairwise examples. Unlike previous methods, with or without side information, we do not need to know the number of clusters, their centers or any kind of distance metric for similarity. Our method is able to divide the same data points in various ways dependant on the needs of a specific task, defined by the side information. Contrastingly, other work generally finds only the intrinsic, most obvious, clusters. Inspired by the mean shift algorithm, we implement our new clustering approach using a custom iterative neural network to create Differentiable Mean Shift (DMS), a state of the art, dataset agnostic, clustering method. We found that it was possible to train a strong cluster definition without enforcing a constraint that each cluster must be presented during training. DMS outperforms current methods in both the intrinsic and non-intrinsic dataset tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#19982;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#31995;&#21015;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18485</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Autoencoding Conditional Neural Processes for Representation Learning. (arXiv:2305.18485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#19982;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#31995;&#21015;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;(CNPs)&#26159;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#27169;&#22411;&#26063;&#32676;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#20540;&#20013;&#23398;&#20064;&#20986;&#19968;&#20010;&#38543;&#26426;&#36807;&#31243;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;CNPs &#22312;&#19978;&#19979;&#25991;&#22270;&#20687;&#34917;&#20840;&#20013;&#24471;&#21040;&#20102;&#29305;&#21035;&#30340;&#24212;&#29992;&#65292;&#21363;&#36890;&#36807;&#35266;&#23519;&#26576;&#20123;&#20301;&#32622;&#30340;&#20687;&#32032;&#20540;&#26469;&#39044;&#27979;&#20854;&#20182;&#26410;&#35266;&#23519;&#20301;&#32622;&#19978;&#30340;&#20540;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#26679;&#19968;&#20010; CNP &#30340;&#20687;&#32032;&#36873;&#25321;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#25110;&#32773;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;(&#20363;&#22914;&#20687;&#32032;&#26041;&#24046;)&#23548;&#20986;&#30340;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#36716;&#21464;&#19968;&#19979;&#65306;&#19968;&#20010; CNP &#24819;&#35201;&#35266;&#23519;&#21738;&#20123;&#20687;&#32032;&#65311;&#20063;&#23601;&#26159;&#35828;&#65292;&#21738;&#20123;&#20687;&#32032;&#20801;&#35768;&#25311;&#21512; CNP&#65292;&#36825;&#26679;&#30340;&#20687;&#32032;&#33021;&#21578;&#35785;&#25105;&#20204;&#19968;&#20123;&#20851;&#20110;&#28508;&#22312;&#22270;&#20687;&#30340;&#20449;&#24687;&#21527;&#65311;&#23558;&#25552;&#20379;&#32473; CNP &#30340;&#19978;&#19979;&#25991;&#35270;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#19968;&#27425;&#24615;&#21464;&#20998;&#26694;&#26550;&#65292;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(Partical Pixel Space VAE, PPS-VAE)&#65292;&#21516;&#26102;&#39044;&#27979;&#36825;&#20010;&#19978;&#19979;&#25991;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010; CNP&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102; PPS-VAE&#65292;&#21457;&#29616;&#36890;&#36807;&#30456;&#23545;&#22823;&#23567;&#25110;&#21464;&#21270;&#39044;&#27979;&#20687;&#32032;&#30340;&#36873;&#25321;&#21487;&#20197;&#23433;&#25490;&#23398;&#20064;&#65292;&#19988;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#20102;&#19978;&#19979;&#25991;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional neural processes (CNPs) are a flexible and efficient family of models that learn to learn a stochastic process from observations. In the visual domain, they have seen particular application in contextual image completion - observing pixel values at some locations to predict a distribution over values at other unobserved locations. However, the choice of pixels in learning such a CNP is typically either random or derived from a simple statistical measure (e.g. pixel variance). Here, we turn the problem on its head and ask: which pixels would a CNP like to observe? That is, which pixels allow fitting CNP, and do such pixels tell us something about the underlying image? Viewing the context provided to the CNP as fixed-size latent representations, we construct an amortised variational framework, Partial Pixel Space Variational Autoencoder (PPS-VAE), for predicting this context simultaneously with learning a CNP. We evaluate PPS-VAE on a set of vision datasets, and find that not
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#20154;&#26426;&#36741;&#21161;&#30340;&#20803;&#23431;&#23449;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#36164;&#28304;&#20998;&#37197;&#21644;&#36712;&#36857;&#25511;&#21046;&#38598;&#25104;&#21040;&#31995;&#32479;&#27169;&#22411;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#21644;&#20984;&#20248;&#21270;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#8220;&#21512;&#20316;&#8221;&#26041;&#24335;&#35299;&#20915;&#26102;&#38388;&#39034;&#24207;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#24471;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.18481</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#20984;&#20248;&#21270;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;UAV&#30340;&#33258;&#20027;&#20803;&#23431;&#23449;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Framework of Reinforcement Learning and Convex Optimization for UAV-Based Autonomous Metaverse Data Collection. (arXiv:2305.18481v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#20154;&#26426;&#36741;&#21161;&#30340;&#20803;&#23431;&#23449;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#36164;&#28304;&#20998;&#37197;&#21644;&#36712;&#36857;&#25511;&#21046;&#38598;&#25104;&#21040;&#31995;&#32479;&#27169;&#22411;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#21644;&#20984;&#20248;&#21270;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#8220;&#21512;&#20316;&#8221;&#26041;&#24335;&#35299;&#20915;&#26102;&#38388;&#39034;&#24207;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#24471;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#30001;&#20110;&#25104;&#26412;&#21644;&#28789;&#27963;&#24615;&#30340;&#20248;&#21183;&#65292;&#22312;&#25552;&#20379;&#36890;&#20449;&#26381;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20852;&#20803;&#23431;&#23449;&#21644;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#32972;&#26223;&#19979;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;UAV&#36741;&#21161;&#30340;&#20803;&#23431;&#23449;&#32593;&#32476;&#65292;&#20854;&#20013;UAV&#25193;&#23637;&#20102;&#22522;&#31449;&#65288;BS&#65289;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#25910;&#38598;&#36335;&#36793;&#21333;&#20803;&#65288;RSU&#65289;&#20135;&#29983;&#30340;&#20803;&#23431;&#23449;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#65292;&#31995;&#32479;&#27169;&#22411;&#20013;&#38598;&#25104;&#20102;&#36164;&#28304;&#20998;&#37197;&#21644;&#36712;&#36857;&#25511;&#21046;&#12290;&#20248;&#21270;&#38382;&#39064;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#20351;&#20256;&#32479;&#20984;&#20248;&#21270;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;UAV&#36741;&#21161;&#30340;&#20803;&#23431;&#23449;&#32593;&#32476;&#31995;&#32479;&#27169;&#22411;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#21644;&#20984;&#20248;&#21270;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#8220;&#21512;&#20316;&#8221;&#26041;&#24335;&#35299;&#20915;&#26102;&#38388;&#39034;&#24207;&#20248;&#21270;&#38382;&#39064;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#22312;&#20445;&#35777;&#25910;&#38598;&#25968;&#25454;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned aerial vehicles (UAVs) are promising for providing communication services due to their advantages in cost and mobility, especially in the context of the emerging Metaverse and Internet of Things (IoT). This paper considers a UAV-assisted Metaverse network, in which UAVs extend the coverage of the base station (BS) to collect the Metaverse data generated at roadside units (RSUs). Specifically, to improve the data collection efficiency, resource allocation and trajectory control are integrated into the system model. The time-dependent nature of the optimization problem makes it non-trivial to be solved by traditional convex optimization methods. Based on the proposed UAV-assisted Metaverse network system model, we design a hybrid framework with reinforcement learning and convex optimization to {cooperatively} solve the time-sequential optimization problem. Simulation results show that the proposed framework is able to reduce the mission completion time with a given transmission 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23558;Human Action Recognition&#20013;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20043;&#19968;&#8212;&#8212;X3D&#26144;&#23556;&#21040;&#20219;&#20309;FPGA&#35774;&#22791;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#23454;&#29616;&#20102;&#38024;&#23545;&#36825;&#31181;&#22797;&#26434;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;Human Action Recognition&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.18479</link><description>&lt;p&gt;
FMM-X3D&#65306;&#22522;&#20110;FPGA&#30340;X3D&#24314;&#27169;&#21644;&#26144;&#23556;&#29992;&#20110;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition. (arXiv:2305.18479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23558;Human Action Recognition&#20013;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20043;&#19968;&#8212;&#8212;X3D&#26144;&#23556;&#21040;&#20219;&#20309;FPGA&#35774;&#22791;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#23454;&#29616;&#20102;&#38024;&#23545;&#36825;&#31181;&#22797;&#26434;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;Human Action Recognition&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#30340;&#20851;&#27880;&#65292;&#22312;&#30417;&#25511;&#31995;&#32479;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#20154;&#20307;&#30417;&#27979;&#31995;&#32479;&#21644;&#35270;&#39057;&#26816;&#32034;&#31561;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#31995;&#32479;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#23558;Human Action Recognition&#20013;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20043;&#19968;&#8212;&#8212;X3D&#26144;&#23556;&#21040;&#20219;&#20309;FPGA&#35774;&#22791;&#19978;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#24037;&#20855;&#27969;&#29983;&#25104;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#22522;&#20110;&#27969;&#30340;&#30828;&#20214;&#31995;&#32479;&#65292;&#32771;&#34385;&#20102;FPGA&#35774;&#22791;&#30340;&#21487;&#29992;&#36164;&#28304;&#21644;&#22806;&#37096;&#23384;&#20648;&#22120;&#29305;&#24615;&#12290;&#25152;&#29983;&#25104;&#30340;&#35774;&#35745;&#25512;&#36827;&#20102;&#24403;&#21069;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#24182;&#39318;&#27425;&#23454;&#29616;&#20102;&#38024;&#23545;&#36825;&#31181;&#22797;&#26434;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;Human Action Recognition&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D Convolutional Neural Networks are gaining increasing attention from researchers and practitioners and have found applications in many domains, such as surveillance systems, autonomous vehicles, human monitoring systems, and video retrieval. However, their widespread adoption is hindered by their high computational and memory requirements, especially when resource-constrained systems are targeted. This paper addresses the problem of mapping X3D, a state-of-the-art model in Human Action Recognition that achieves accuracy of 95.5\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflow generates an optimised stream-based hardware system, taking into account the available resources and off-chip memory characteristics of the FPGA device. The generated designs push further the current performance-accuracy pareto front, and enable for the first time the targeting of such complex model architectures for the Human Action Recognition task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18477</link><description>&lt;p&gt;
&#36229;&#36234;&#20803;&#25968;&#25454;&#65306;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#36827;&#34892;&#36328;&#29256;&#26412;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#26159;&#20840;&#29699;&#28216;&#25103;&#24066;&#22330;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#26159;&#22686;&#38271;&#26368;&#24555;&#30340;&#28216;&#25103;&#32454;&#20998;&#39046;&#22495;&#12290;&#36825;&#23548;&#33268;&#20102;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#30340;&#39046;&#22495;&#20135;&#29983;&#65292;&#20854;&#20351;&#29992;&#28216;&#25103;&#25552;&#21462;&#30340;&#36965;&#27979;&#25968;&#25454;&#26469;&#20026;&#29609;&#23478;&#12289;&#25945;&#32451;&#12289;&#25773;&#38899;&#21592;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#30340;&#20307;&#32946;&#27604;&#36187;&#30456;&#27604;&#65292;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#30340;&#26426;&#21046;&#21644;&#35268;&#21017;&#32463;&#24120;&#21457;&#29983;&#24555;&#36895;&#21464;&#21270;&#12290;&#30001;&#20110;&#28216;&#25103;&#21442;&#25968;&#30340;&#39057;&#32321;&#26356;&#25913;&#65292;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#27169;&#22411;&#30340;&#20351;&#29992;&#23551;&#21629;&#21487;&#33021;&#24456;&#30701;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#30053;&#20102;&#12290;&#26412;&#25991;&#25552;&#21462;&#28216;&#25103;&#35774;&#35745;&#20449;&#24687;&#65288;&#21363;&#34917;&#19969;&#35828;&#26126;&#65289;&#65292;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#12290;&#20197;Dota 2&#28216;&#25103;&#20013;&#20987;&#26432;&#27425;&#25968;&#30340;&#39044;&#27979;&#20026;&#26696;&#20363;&#65292;&#21033;&#29992;&#36825;&#31181;&#21019;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#27492;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21253;&#25324;&#24120;&#35268;&#25216;&#26415;&#22312;&#20869;&#30340;&#20004;&#20010;&#19981;&#21516;&#22522;&#32447;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#36824;&#20811;&#26381;&#20102;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#20013;&#29256;&#26412;&#26356;&#36845;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;Multi-Task Diffusion Model (MTDiff)&#65292;&#32467;&#21512;&#20102;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22914;&#27492;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#29615;&#22659;&#19979;&#21462;&#24471;&#30456;&#24403;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18459</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#35268;&#21010;&#22120;&#21644;&#25968;&#25454;&#21512;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. (arXiv:2305.18459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;Multi-Task Diffusion Model (MTDiff)&#65292;&#32467;&#21512;&#20102;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22914;&#27492;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#29615;&#22659;&#19979;&#21462;&#24471;&#30456;&#24403;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;NLP&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#20063;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#22797;&#26434;&#31574;&#30053;&#25110;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#35774;&#32622;&#65292;&#27809;&#26377;&#32771;&#34385;&#22810;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#22810;&#26679;&#21270;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Task Diffusion Model&#65288;MTDiff&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#29983;&#25104;&#35268;&#21010;&#21644;&#25968;&#25454;&#21512;&#25104;&#12290;MTDiff&#21033;&#29992;&#22823;&#37327;&#21487;&#29992;&#20110;&#22810;&#20219;&#21153;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#25191;&#34892;&#38544;&#24335;&#30693;&#35782;&#20849;&#20139;&#20197;&#36827;&#34892;&#34394;&#25311;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#22522;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24102;&#26377;&#27700;&#21360;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#31639;&#27861;&#20998;&#26512;&#20102;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#25152;&#20135;&#29983;&#30340;&#36755;&#20986;&#20998;&#24067;&#21644;&#36923;&#36753;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18456</link><description>&lt;p&gt;
&#35782;&#21035;&#27700;&#21360;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Baselines for Identifying Watermarked Large Language Models. (arXiv:2305.18456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#22522;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24102;&#26377;&#27700;&#21360;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#31639;&#27861;&#20998;&#26512;&#20102;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#25152;&#20135;&#29983;&#30340;&#36755;&#20986;&#20998;&#24067;&#21644;&#36923;&#36753;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26032;&#20852;&#38382;&#39064;&#65306;&#22914;&#20309;&#35782;&#21035;&#24191;&#27867;&#20351;&#29992;&#30340;&#65292;&#20844;&#24320;&#25176;&#31649;&#30340;&#65292;&#38381;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#27700;&#21360;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#22522;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;LLM&#27700;&#21360;&#65292;&#36825;&#20123;&#31639;&#27861;&#20381;&#36182;&#20110;&#20998;&#26512;&#30001;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20196;&#29260;&#21644;&#36923;&#36753;&#20998;&#24067;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24102;&#26377;&#27700;&#21360;&#30340;LLMs tend to&#20135;&#29983;&#20998;&#24067;&#19982;&#26631;&#20934;&#27169;&#22411; qualitatively and identifiably&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#24378;&#24230;&#19979;&#35782;&#21035;&#27700;&#21360;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#32771;&#34385;&#20102;&#27599;&#20010;&#35782;&#21035;&#26426;&#21046;&#22312;&#27700;&#21360;&#22330;&#26223;&#20013;&#30340;&#26435;&#34913;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#21270;&#20102;&#35782;&#21035;LLMs&#20013;&#30340;&#27700;&#21360;&#30340;&#20855;&#20307;&#38382;&#39064;&#65292;&#20197;&#21450;LLM&#27700;&#21360;&#21644;&#27700;&#21360;&#26816;&#27979;&#30340;&#19968;&#33324;&#24615;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#23427;&#20204;&#25552;&#20379;&#20102;&#26694;&#26550;&#21644;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the emerging problem of identifying the presence and use of watermarking schemes in widely used, publicly hosted, closed source large language models (LLMs). We introduce a suite of baseline algorithms for identifying watermarks in LLMs that rely on analyzing distributions of output tokens and logits generated by watermarked and unmarked LLMs. Notably, watermarked LLMs tend to produce distributions that diverge qualitatively and identifiably from standard models. Furthermore, we investigate the identifiability of watermarks at varying strengths and consider the tradeoffs of each of our identification mechanisms with respect to watermarking scenario. Along the way, we formalize the specific problem of identifying watermarks in LLMs, as well as LLM watermarks and watermark detection in general, providing a framework and foundations for studying them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18451</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#20122;&#32467;&#26500;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#24341;&#36215;&#20102;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#39044;&#27979;&#20998;&#23376;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#24615;&#24378;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;CMRL&#65292;&#23427;&#36890;&#36807;&#26816;&#27979;&#19982;&#21270;&#23398;&#21453;&#24212;&#22240;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#20122;&#32467;&#26500;&#26469;&#24212;&#23545;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20551;&#23450;&#22522;&#20110;&#20998;&#23376;&#31185;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#26500;&#24314;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#25581;&#31034;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;SCM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#20854;&#24178;&#39044;&#26159;&#22522;&#20110;&#25104;&#23545;&#20998;&#23376;&#26465;&#20214;&#30340;&#12290;&#20351;&#29992;&#26465;&#20214;&#24178;&#39044;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#20174;&#22240;&#26524;&#20122;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#20102;&#19982;&#21270;&#23398;&#21453;&#24212;&#34394;&#20551;&#30456;&#20851;&#30340;&#24555;&#25463;&#20122;&#32467;&#26500;&#30340;&#28151;&#28102;&#25928;&#24212;&#12290;&#26412;&#25991;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25552;&#31034;&#65292;&#25511;&#21046;AI bot&#21040;&#36798;&#20219;&#20309;&#29366;&#24577;&#65292;&#32780;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;Bot&#33021;&#20960;&#20046;&#30830;&#23450;&#22320;&#21040;&#36798;&#20219;&#20309;&#24847;&#20041;&#23376;&#38598;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18449</link><description>&lt;p&gt;
&#39535;&#26381;AI Bot&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Taming AI Bots: Controllability of Neural States in Large Language Models. (arXiv:2305.18449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25552;&#31034;&#65292;&#25511;&#21046;AI bot&#21040;&#36798;&#20219;&#20309;&#29366;&#24577;&#65292;&#32780;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;Bot&#33021;&#20960;&#20046;&#30830;&#23450;&#22320;&#21040;&#36798;&#20219;&#20309;&#24847;&#20041;&#23376;&#38598;&#65292;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25552;&#31034;&#65292;&#23558;AI bot &#25511;&#21046;&#21040;&#20219;&#20309;&#29366;&#24577;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#24847;&#20041;&#8221;&#23450;&#20041;&#65292;&#20415;&#20110;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#26465;&#20214;&#65292;&#34920;&#24449;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25152;&#26174;&#28982;&#35757;&#32451;&#30340;&#8220;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#8221;&#21644;&#8220;&#35757;&#32451;&#33391;&#22909;&#30340;LLM&#8221;&#12290;&#34429;&#28982;&#35757;&#32451;&#33391;&#22909;&#30340;LLM&#26500;&#24314;&#20102;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24847;&#20041;&#23884;&#20837;&#31354;&#38388;&#65292;&#20294;&#26159;&#24847;&#20041;&#26412;&#36523;&#24182;&#19981;&#24418;&#25104;&#19968;&#20010;&#21521;&#37327;&#65288;&#32447;&#24615;&#65289;&#23376;&#31354;&#38388;&#65292;&#32780;&#26159;&#19968;&#20010;&#21830;&#31354;&#38388;&#12290;&#25105;&#20204;&#28982;&#21518;&#34920;&#24449;&#20102;&#26576;&#20010;&#36755;&#20837;&#25552;&#31034;&#30340;&#29366;&#24577;&#25152;&#33021;&#21040;&#36798;&#30340;&#24847;&#20041;&#23376;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#33391;&#22909;&#30340;Bot&#33021;&#22815;&#21040;&#36798;&#20219;&#20309;&#24847;&#20041;&#65292;&#23613;&#31649;&#27010;&#29575;&#24456;&#23567;&#12290;&#25105;&#20204;&#25509;&#30528;&#24341;&#20837;&#20102;&#26356;&#24378;&#30340;&#21487;&#25511;&#24615;&#27010;&#24565;&#65292;&#21363;&#8220;&#20960;&#20046;&#30830;&#23450;&#21487;&#36798;&#24615;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#38480;&#21046;&#21040;&#24847;&#20041;&#31354;&#38388;&#26102;&#65292;AI bot&#26159;&#21487;&#25511;&#30340;&#12290;&#25105;&#20204;&#22312;&#24341;&#20837;&#21151;&#33021;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#36825;&#26679;&#20570;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the question of whether an agent can, by suitable choice of prompts, control an AI bot to any state. To that end, we first introduce a formal definition of ``meaning'' that is amenable to analysis. Then, we characterize ``meaningful data'' on which large language models (LLMs) are ostensibly trained, and ``well-trained LLMs'' through conditions that are largely met by today's LLMs. While a well-trained LLM constructs an embedding space of meanings that is Euclidean, meanings themselves do not form a vector (linear) subspace, but rather a quotient space within. We then characterize the subset of meanings that can be reached by the state of the LLMs for some input prompt, and show that a well-trained bot can reach any meaning albeit with small probability. We then introduce a stronger notion of controllability as {\em almost certain reachability}, and show that, when restricted to the space of meanings, an AI bot is controllable. We do so after introducing a functional characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;CoTASP&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20043;&#21069;&#20219;&#21153;&#30340;&#20849;&#21516;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.18444</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#25552;&#31034;&#30340;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#30340;&#25345;&#32493;&#20219;&#21153;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Continual Task Allocation in Meta-Policy Network via Sparse Prompting. (arXiv:2305.18444v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;CoTASP&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20043;&#21069;&#20219;&#21153;&#30340;&#20849;&#21516;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#19981;&#26029;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#26469;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#19968;&#33324;&#21270;&#33021;&#21147;&#30340;&#20803;&#31574;&#30053;&#65292;&#26159;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36830;&#32493;&#20219;&#21153;&#20998;&#37197;&#30340;&#31232;&#30095;&#25552;&#31034;&#65288;CoTASP&#65289;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#36807;&#23436;&#22791;&#23383;&#20856;&#26469;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#20316;&#20026;&#25552;&#31034;&#65292;&#20174;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#25552;&#21462;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#12290;&#36890;&#36807;&#20132;&#26367;&#20248;&#21270;&#23376;&#32593;&#32476;&#21644;&#25552;&#31034;&#65292;CoTASP&#26356;&#26032;&#20102;&#20803;&#31574;&#30053;&#65292;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#28982;&#21518;&#26356;&#26032;&#23383;&#20856;&#65292;&#20197;&#20351;&#20248;&#21270;&#21518;&#30340;&#25552;&#31034;&#19982;&#20219;&#21153;&#23884;&#20837;&#30456;&#21305;&#37197;&#65292;&#20174;&#32780;&#25429;&#25417;&#20854;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#30456;&#20851;&#20219;&#21153;&#36890;&#36807;&#30456;&#20284;&#30340;&#25552;&#31034;&#22312;&#20803;&#31574;&#30053;&#32593;&#32476;&#20013;&#20849;&#20139;&#26356;&#22810;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#36328;&#20219;&#21153;&#24178;&#25200;&#23548;&#33268;&#36951;&#24536;&#34987;&#26377;&#25928;&#22320;&#32422;&#26463;&#12290;&#32473;&#23450;&#32463;&#36807;&#35757;&#32451;&#30340;&#20803;&#31574;&#30053;&#21644;&#26356;&#26032;&#21518;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;&#25552;&#31034;&#26469;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#20174;&#20803;&#31574;&#30053;&#20013;&#25552;&#21462;&#30456;&#20851;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23548;&#33322;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;CoTASP&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20219;&#21153;&#23436;&#25104;&#24230;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by "Continual Task Allocation via Sparse Prompting (CoTASP)", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;Journey Ranker&#65292;&#26469;&#35299;&#20915;Airbnb&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#21807;&#19968;&#25361;&#25112;&#65292;&#21363;&#23458;&#25143;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#65292;&#35813;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.18431</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20248;&#21270;Airbnb&#25628;&#32034;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
Optimizing Airbnb Search Journey with Multi-task Learning. (arXiv:2305.18431v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;Journey Ranker&#65292;&#26469;&#35299;&#20915;Airbnb&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#21807;&#19968;&#25361;&#25112;&#65292;&#21363;&#23458;&#25143;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#65292;&#35813;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Airbnb&#26159;&#19968;&#20010;&#22312;&#32447;&#20303;&#23487;&#21644;&#20307;&#39564;&#24066;&#22330;&#65292;&#23458;&#20154;&#36890;&#24120;&#38656;&#35201;&#33457;&#36153;&#25968;&#21608;&#26469;&#25506;&#32034;&#21644;&#27604;&#36739;&#22810;&#20010;&#29289;&#21697;&#65292;&#24182;&#22312;&#20570;&#20986;&#26368;&#21518;&#30340;&#39044;&#35746;&#35831;&#27714;&#20043;&#21069;&#24179;&#34913;&#23458;&#20154;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#12290;&#25628;&#32034;&#36807;&#31243;&#30340;&#38271;&#26399;&#24615;&#36136;&#20197;&#21450;&#38656;&#35201;&#24179;&#34913;&#23458;&#20154;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#65292;&#36825;&#20123;&#37117;&#20026;Airbnb&#30340;&#25628;&#32034;&#25490;&#21517;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;Journey Ranker&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;Journey Ranker&#21033;&#29992;&#20013;&#38388;&#30340;&#23458;&#25143;&#25805;&#20316;&#20316;&#20026;&#37324;&#31243;&#30865;&#65288;&#26080;&#35770;&#26159;&#31215;&#26497;&#30340;&#36824;&#26159;&#28040;&#26497;&#30340;&#65289;&#26469;&#26356;&#22909;&#22320;&#23558;&#23458;&#25143;&#25512;&#21521;&#25104;&#21151;&#30340;&#39044;&#35746;&#12290;&#23427;&#36824;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#23458;&#25143;&#29366;&#24577;&#21644;&#25628;&#32034;&#26597;&#35810;&#65289;&#26469;&#24179;&#34913;&#23458;&#20154;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#12290;&#20854;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35774;&#35745;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65292;&#20998;&#31163;&#26126;&#30830;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;Airbnb&#25628;&#32034;&#25490;&#21517;&#20197;&#22806;&#30340;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
At Airbnb, an online marketplace for stays and experiences, guests often spend weeks exploring and comparing multiple items before making a final reservation request. Each reservation request may then potentially be rejected or cancelled by the host prior to check-in. The long and exploratory nature of the search journey, as well as the need to balance both guest and host preferences, present unique challenges for Airbnb search ranking. In this paper, we present Journey Ranker, a new multi-task deep learning model architecture that addresses these challenges. Journey Ranker leverages intermediate guest actions as milestones, both positive and negative, to better progress the guest towards a successful booking. It also uses contextual information such as guest state and search query to balance guest and host preferences. Its modular and extensible design, consisting of four modules with clear separation of concerns, allows for easy application to use cases beyond the Airbnb search ranki
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#22686;&#26448;&#21046;&#36896;&#20013;&#36755;&#20837;&#21464;&#37327;&#21644;&#25289;&#20280;&#24378;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#21457;&#29616;Infill&#30334;&#20998;&#27604;&#21644;&#25380;&#20986;&#28201;&#24230;&#23545;&#20110;&#25289;&#20280;&#24378;&#24230;&#20855;&#26377;&#26368;&#39640;&#30340;&#27491;&#30456;&#20851;&#21644;&#36127;&#30456;&#20851;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18426</link><description>&lt;p&gt;
&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#25506;&#31350;&#22686;&#26448;&#21046;&#36896;&#26679;&#21697;&#20013;&#36755;&#20837;&#21464;&#37327;&#21644;&#25289;&#20280;&#24378;&#24230;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Employing Explainable Artificial Intelligence (XAI) Methodologies to Analyze the Correlation between Input Variables and Tensile Strength in Additively Manufactured Samples. (arXiv:2305.18426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#22686;&#26448;&#21046;&#36896;&#20013;&#36755;&#20837;&#21464;&#37327;&#21644;&#25289;&#20280;&#24378;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#21457;&#29616;Infill&#30334;&#20998;&#27604;&#21644;&#25380;&#20986;&#28201;&#24230;&#23545;&#20110;&#25289;&#20280;&#24378;&#24230;&#20855;&#26377;&#26368;&#39640;&#30340;&#27491;&#30456;&#20851;&#21644;&#36127;&#30456;&#20851;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21253;&#25324;Infill&#30334;&#20998;&#27604;&#12289;&#23618;&#39640;&#12289;&#25380;&#20986;&#28201;&#24230;&#21644;&#25171;&#21360;&#36895;&#24230;&#22312;&#20869;&#30340;&#21508;&#31181;&#36755;&#20837;&#21442;&#25968;&#23545;&#20110;&#20135;&#29983;&#30340;&#22686;&#26448;&#21046;&#36896;&#29289;&#20307;&#30340;&#25289;&#20280;&#24378;&#24230;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#25105;&#20204;&#23545;&#36755;&#20837;&#21442;&#25968;&#21644;&#25289;&#20280;&#24378;&#24230;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#29702;&#35299;&#65292;&#20197;&#21450;&#30830;&#23450;&#24433;&#21709;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;SHAP&#65288;Shapley Additive Explanations&#65289;&#26041;&#27861;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#34892;&#20026;&#65292;&#20197;&#20998;&#26512;&#25968;&#25454;&#24182;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;Infill&#30334;&#20998;&#27604;&#21644;&#25380;&#20986;&#28201;&#24230;&#19982;&#25289;&#20280;&#24378;&#24230;&#20855;&#26377;&#26368;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20998;&#21035;&#20026;&#27491;&#30456;&#20851;&#21644;&#36127;&#30456;&#20851;&#12290;&#20351;&#29992;XAI&#25216;&#26415;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36755;&#20837;&#21442;&#25968;&#21644;&#22686;&#26448;&#21046;&#36896;&#20013;&#25289;&#20280;&#24378;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper explores the impact of various input parameters, including Infill percentage, Layer Height, Extrusion Temperature, and Print Speed, on the resulting Tensile Strength in objects produced through additive manufacturing. The main objective of this study is to enhance our understanding of the correlation between the input parameters and Tensile Strength, as well as to identify the key factors influencing the performance of the additive manufacturing process. To achieve this objective, we introduced the utilization of Explainable Artificial Intelligence (XAI) techniques for the first time, which allowed us to analyze the data and gain valuable insights into the system's behavior. Specifically, we employed SHAP (SHapley Additive exPlanations), a widely adopted framework for interpreting machine learning model predictions, to provide explanations for the behavior of a machine learning model trained on the data. Our findings reveal that the Infill percentage and Extrusion T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#27531;&#24046;&#20302;&#31209;&#29305;&#24615;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#39640;&#25928;&#23384;&#20648;&#30340;&#26032;&#26041;&#27861;ERE&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#37327;&#21270;&#21644;&#20998;&#23618;&#31209;&#20998;&#37197;&#26469;&#25552;&#39640;&#23384;&#20648;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18425</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#26435;&#37325;&#27531;&#24046;&#30340;&#20302;&#31209;&#36924;&#36817;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#39640;&#25928;&#23384;&#20648;
&lt;/p&gt;
&lt;p&gt;
Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals. (arXiv:2305.18425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#27531;&#24046;&#20302;&#31209;&#29305;&#24615;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#39640;&#25928;&#23384;&#20648;&#30340;&#26032;&#26041;&#27861;ERE&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#37327;&#21270;&#21644;&#20998;&#23618;&#31209;&#20998;&#37197;&#26469;&#25552;&#39640;&#23384;&#20648;&#25928;&#29575;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#27531;&#24046;&#30340;&#20302;&#31209;&#29305;&#24615;&#26469;&#39640;&#25928;&#23384;&#20648;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#22823;&#22411;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#27531;&#24046;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#20302;&#31209;&#29305;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#25928;&#27531;&#24046;&#32534;&#30721;&#65288;ERE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#20302;&#31209;&#26435;&#37325;&#27531;&#24046;&#26469;&#23454;&#29616;&#23545;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#30340;&#39640;&#25928;&#23384;&#20648;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26435;&#37325;&#27531;&#24046;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#37327;&#21270;&#21644;&#20998;&#23618;&#31209;&#20998;&#37197;&#26469;&#25512;&#21160;&#23384;&#20648;&#25928;&#29575;&#30340;&#26497;&#38480;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#24577;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an efficient method for storing fine-tuned models by leveraging the low-rank properties of weight residuals. Our key observation is that weight residuals in large overparameterized models exhibit even stronger low-rank characteristics. Based on this insight, we propose Efficient Residual Encoding (ERE), a novel approach that achieves efficient storage of fine-tuned model weights by approximating the low-rank weight residuals. Furthermore, we analyze the robustness of weight residuals and push the limit of storage efficiency by utilizing additional quantization and layer-wise rank allocation. Our experimental results demonstrate that our method significantly reduces memory footprint while preserving performance in various tasks and modalities. We release our code.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperTime&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#26102;&#38388;&#19978;&#40065;&#26834;&#30340;&#39044;&#27979;&#24615;&#33021;&#36229;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#21382;&#21490;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#23545;&#24179;&#22343;&#39564;&#35777;&#25439;&#22833;&#21644;&#26368;&#22351;&#24773;&#20917;&#39564;&#35777;&#25439;&#22833;&#35774;&#32622;&#20102;&#35789;&#20856;&#20248;&#20808;&#32423;&#39034;&#24207;&#65292;&#24182;&#22312;&#22810;&#20010;&#24102;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#24378;&#21170;&#12290;</title><link>http://arxiv.org/abs/2305.18421</link><description>&lt;p&gt;
HyperTime: &#24212;&#23545;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperTime: Hyperparameter Optimization for Combating Temporal Distribution Shifts. (arXiv:2305.18421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperTime&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#26102;&#38388;&#19978;&#40065;&#26834;&#30340;&#39044;&#27979;&#24615;&#33021;&#36229;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#21382;&#21490;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#23545;&#24179;&#22343;&#39564;&#35777;&#25439;&#22833;&#21644;&#26368;&#22351;&#24773;&#20917;&#39564;&#35777;&#25439;&#22833;&#35774;&#32622;&#20102;&#35789;&#20856;&#20248;&#20808;&#32423;&#39034;&#24207;&#65292;&#24182;&#22312;&#22810;&#20010;&#24102;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#24378;&#21170;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;HyperTime&#8221;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#23547;&#25214;&#23545;&#26410;&#30693;&#27979;&#35797;&#25968;&#25454;&#30340;&#28508;&#22312;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24471;&#21040;&#20102;&#19968;&#20010;&#37325;&#35201;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#21363;&#36890;&#36807;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26102;&#38388;&#19978;&#30340;&#40065;&#26834;&#39044;&#27979;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#40065;&#26834;&#20248;&#21270;&#25991;&#29486;&#20013;&#30340;&#26368;&#22351;&#24773;&#20917;&#23548;&#21521;&#21746;&#23398;&#65292;&#24110;&#21161;&#25214;&#21040;&#36825;&#26679;&#30340;&#40065;&#26834;&#24615;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;HyperTime&#22312;&#21382;&#21490;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#23545;&#24179;&#22343;&#39564;&#35777;&#25439;&#22833;&#21644;&#26368;&#22351;&#24773;&#20917;&#39564;&#35777;&#25439;&#22833;&#35774;&#32622;&#20102;&#35789;&#20856;&#20248;&#20808;&#32423;&#39034;&#24207;&#12290;&#25105;&#20204;&#23545;&#39044;&#26399;&#27979;&#35797;&#25439;&#22833;&#30340;&#19978;&#38480;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#22312;&#22810;&#20010;&#20855;&#26377;&#26102;&#38388;&#20998;&#24067;&#20559;&#31227;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24378;&#22823;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a hyperparameter optimization method named \emph{HyperTime} to find hyperparameters robust to potential temporal distribution shifts in the unseen test data. Our work is motivated by an important observation that it is, in many cases, possible to achieve temporally robust predictive performance via hyperparameter optimization. Based on this observation, we leverage the `worst-case-oriented' philosophy from the robust optimization literature to help find such robust hyperparameter configurations. HyperTime imposes a lexicographic priority order on average validation loss and worst-case validation loss over chronological validation sets. We perform a theoretical analysis on the upper bound of the expected test loss, which reveals the unique advantages of our approach. We also demonstrate the strong empirical performance of the proposed method on multiple machine learning tasks with temporal distribution shifts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20869;&#23384;&#21463;&#21040;&#26497;&#31471;&#38480;&#21046;&#26102;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.18418</link><description>&lt;p&gt;
&#19968;&#30629;&#65306;&#37325;&#26032;&#24605;&#32771;&#35270;&#39057;&#19981;&#26029;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Just a Glimpse: Rethinking Temporal Information for Video Continual Learning. (arXiv:2305.18418v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20869;&#23384;&#21463;&#21040;&#26497;&#31471;&#38480;&#21046;&#26102;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26159;&#36830;&#32493;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#37325;&#35201;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#23494;&#20999;&#30456;&#20851;&#12290;&#38543;&#30528;&#31867;&#21035;/&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#21463;&#21040;&#20869;&#23384;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#20250;&#20986;&#29616;&#12290;&#22312;&#35270;&#39057;&#39046;&#22495;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35270;&#39057;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#24103;&#65292;&#36825;&#20250;&#20351;&#22238;&#25918;&#35760;&#24518;&#36127;&#25285;&#26356;&#37325;&#12290;&#30446;&#21069;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#20174;&#35270;&#39057;&#27969;&#20013;&#23545;&#24103;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#22238;&#25918;&#35760;&#24518;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26497;&#31471;&#20869;&#23384;&#38480;&#21046;&#19979;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20174;&#20195;&#34920;&#22823;&#37327;&#29420;&#29305;&#35270;&#39057;&#30340;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#35270;&#39057;&#25968;&#25454;&#38598;Kin&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning is one of the most important settings for the study of Continual Learning, as it closely resembles real-world application scenarios. With constrained memory sizes, catastrophic forgetting arises as the number of classes/tasks increases. Studying continual learning in the video domain poses even more challenges, as video data contains a large number of frames, which places a higher burden on the replay memory. The current common practice is to sub-sample frames from the video stream and store them in the replay memory. In this paper, we propose SMILE a novel replay mechanism for effective video continual learning based on individual/single frames. Through extensive experimentation, we show that under extreme memory constraints, video diversity plays a more significant role than temporal information. Therefore, our method focuses on learning from a small number of frames that represent a large number of unique videos. On three representative video datasets, Kin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.18413</link><description>&lt;p&gt;
&#20174;API&#23398;&#20064;&#23398;&#20064;&#65306;&#40657;&#30418;&#25968;&#25454;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#65288;DFML&#65289;&#26088;&#22312;&#36890;&#36807;&#20174;&#19968;&#32452;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20803;&#23398;&#20064;&#32780;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;DFML&#24037;&#20316;&#20165;&#33021;&#20174;&#65288;i&#65289;&#30333;&#30418;&#21644;&#65288;ii&#65289;&#23567;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;iii&#65289;&#30456;&#21516;&#30340;&#26550;&#26500;&#20013;&#20803;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#21363;&#29992;&#25143;&#20165;&#33021;&#36890;&#36807;&#20219;&#24847;&#27169;&#22411;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;API&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25968;&#25454;&#26080;&#20851;&#20803;&#30693;&#35782;&#33976;&#39311;&#65288;BiDf-MKD&#65289;&#26694;&#26550;&#65292;&#23558;&#26356;&#36890;&#29992;&#30340;&#20803;&#30693;&#35782;&#20174;&#19968;&#32452;&#40657;&#30418;API&#36716;&#31227;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
&lt;/p&gt;</description></item><item><title>MoleculeSDE&#26159;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20960;&#20309;&#19982;2D&#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.18407</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining. (arXiv:2305.18407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18407
&lt;/p&gt;
&lt;p&gt;
MoleculeSDE&#26159;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20960;&#20309;&#19982;2D&#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#25552;&#39640;&#22522;&#20110; AI &#30340;&#33647;&#29289;&#21457;&#29616;&#24615;&#33021;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#20851;&#27880;&#21333;&#19968;&#30340;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#22823;&#21270;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#21487;&#20197;&#22686;&#24378;&#20998;&#23376;&#34920;&#31034;&#33021;&#21147;&#12290;&#32780;&#29616;&#26377;&#30340;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20174;&#25299;&#25169;&#21644;&#20960;&#20309;&#32534;&#30721;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#20272;&#35745; MI&#65292;&#22240;&#27492;&#20002;&#22833;&#20102;&#20998;&#23376;&#30340;&#20851;&#38190;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MoleculeSDE&#12290;MoleculeSDE&#21033;&#29992;&#32676;&#23545;&#31216;&#65288;&#22914; SE&#65288;3&#65289;-&#31561;&#21464;&#21644;&#21453;&#23556;-&#21453;&#23545;&#31216;&#65289;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104; 3D &#20960;&#20309;&#24418;&#29366;&#19982; 2D &#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#23427;&#19981;&#20165;&#33719;&#24471;&#26356;&#32039;&#30340;MI&#30028;&#38480;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecule pretraining has quickly become the go-to schema to boost the performance of AI-based drug discovery. Naturally, molecules can be represented as 2D topological graphs or 3D geometric point clouds. Although most existing pertaining methods focus on merely the single modality, recent research has shown that maximizing the mutual information (MI) between such two modalities enhances the molecule representation ability. Meanwhile, existing molecule multi-modal pretraining approaches approximate MI based on the representation space encoded from the topology and geometry, thus resulting in the loss of critical structural information of molecules. To address this issue, we propose MoleculeSDE. MoleculeSDE leverages group symmetric (e.g., SE(3)-equivariant and reflection-antisymmetric) stochastic differential equation models to generate the 3D geometries from 2D topologies, and vice versa, directly in the input space. It not only obtains tighter MI bound but also enables prosperous dow
&lt;/p&gt;</description></item><item><title>Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18405</link><description>&lt;p&gt;
Dink-Net: &#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18405
&lt;/p&gt;
&lt;p&gt;
Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#24418;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#32452;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Dink-Net&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21306;&#20998;&#24102;&#22686;&#24378;&#30340;&#36319;&#19981;&#24102;&#22686;&#24378;&#30340;&#33410;&#28857;&#65292;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#24418;&#24335;&#12290;&#21516;&#26102;&#65292;&#23558;&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#26041;&#24335;&#26368;&#23567;&#21270;&#25552;&#20986;&#30340;&#38598;&#32676;&#33192;&#32960;&#25439;&#22833;&#21644;&#38598;&#32676;&#25910;&#32553;&#25439;&#22833;&#65292;&#20248;&#21270;&#32858;&#31867;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#23558;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#20004;&#20010;&#27493;&#39588;&#32479;&#19968;&#20026;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24341;&#23548;&#32593;&#32476;&#23398;&#20064;&#32858;&#31867;&#21451;&#22909;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;Dink-Net&#33021;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#22270;&#24418;&#19978;&#65292;&#22240;&#20026;&#35774;&#35745;&#30340;&#33192;&#32960;&#25910;&#32553;&#25805;&#20316;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Dink-Net&#22312;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#22270;&#24418;&#30340;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#22270;&#32858;&#31867;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.18400</link><description>&lt;p&gt;
&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18400
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#65288;TFL&#65289;&#36890;&#24120;&#21033;&#29992;&#20445;&#25252;&#26426;&#21046;&#26469;&#20445;&#35777;&#38544;&#31169;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20445;&#25252;&#26426;&#21046;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24341;&#20837;&#25928;&#29992;&#25439;&#22833;&#25110;&#25928;&#29575;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#20445;&#25252;&#26426;&#21046;&#21450;&#20854;&#21442;&#25968;&#24212;&#35813;&#20180;&#32454;&#36873;&#25321;&#65292;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#20174;&#19994;&#32773;&#38656;&#35201;&#24037;&#20855;&#26469;&#34913;&#37327;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#24182;&#20248;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#25163;&#22836;&#24212;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20010;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;(1)&#23558;TFL&#23450;&#20041;&#20026;&#25214;&#21040;&#20445;&#25252;&#26426;&#21046;&#26469;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#19977;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#38382;&#39064;&#65307;(2)&#27491;&#24335;&#23450;&#20041;&#20102;&#36825;&#19977;&#20010;&#22240;&#32032;&#30340;&#26377;&#30028;&#27979;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#27492;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.18399</link><description>&lt;p&gt;
&#20851;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#35268;&#33539;&#21270;&#23545;&#21021;&#22987;&#21270;&#31561;&#36317;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010; Gram &#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#35813;&#30697;&#38453;&#21253;&#21547;&#19982;&#19968;&#25209;&#36755;&#20837;&#23545;&#24212;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#25104;&#23545;&#20869;&#31215;&#12290;&#22312;&#20960;&#31181;&#26550;&#26500;&#20013;&#65292;&#35266;&#23519;&#21040;&#22312;&#21021;&#22987;&#21270;&#26102;&#35813; Gram &#30697;&#38453;&#20250;&#38543;&#30528;&#28145;&#24230;&#21464;&#24471;&#36864;&#21270;&#65292;&#20174;&#32780;&#20005;&#37325;&#20943;&#32531;&#35757;&#32451;&#36895;&#24230;&#12290;&#35268;&#33539;&#21270;&#23618;&#22914;&#25209;&#22788;&#29702;&#35268;&#33539;&#21270;&#25110;&#23618;&#35268;&#33539;&#21270;&#65292;&#22312;&#38450;&#27490;&#31209;&#23849;&#28291;&#38382;&#39064;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#26080;&#27861;&#20840;&#38754;&#35206;&#30422;&#24191;&#27867;&#29992;&#20110; transformer &#20013;&#30340;&#23618;&#35268;&#33539;&#21270;&#21644;&#26377;&#38480;&#28145;&#24230;&#19979;&#35268;&#33539;&#21270;&#30340;&#37327;&#21270;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#32467;&#21512;&#28608;&#27963;&#20989;&#25968;&#23618;&#20351;&#29992;&#30340;&#23618;&#35268;&#33539;&#21270;&#21487;&#20197;&#20351;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340; Gram &#30697;&#38453;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#24182;&#20351;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340; Hermite &#23637;&#24320;&#26469;&#37327;&#21270;&#36825;&#20010;&#36895;&#24230;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22797;&#21046;&#19981;&#36866;&#24403;&#20154;&#31867;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25233;&#21046;&#29983;&#25104;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#27169;&#22411;&#23545;&#19990;&#30028;&#19985;&#38475;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.18398</link><description>&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#19981;&#24403;&#34892;&#20026;&#32531;&#35299;&#65306;&#21453;&#26144;&#19990;&#30028;&#19985;&#38475;&#26159;&#21542;&#26377;&#20215;&#20540;&#65311;
&lt;/p&gt;
&lt;p&gt;
Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?. (arXiv:2305.18398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22797;&#21046;&#19981;&#36866;&#24403;&#20154;&#31867;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25233;&#21046;&#29983;&#25104;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#27169;&#22411;&#23545;&#19990;&#30028;&#19985;&#38475;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#32593;&#32476;&#19978;&#38543;&#26426;&#25235;&#21462;&#30340;&#25968;&#21313;&#20159;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#36824;&#20250;&#22797;&#21046;&#19981;&#36866;&#24403;&#30340;&#20154;&#31867;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#19981;&#24403;&#36864;&#21270;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#37096;&#32626;&#26102;&#23545;&#20854;&#36827;&#34892;&#30417;&#35270;&#21644;&#35843;&#33410;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25512;&#29702;&#26102;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#25233;&#21046;&#19981;&#21512;&#36866;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27169;&#22411;&#23545;&#19990;&#30028;&#19985;&#38475;&#30340;&#34920;&#29616;&#26469;&#23558;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also reproduce inappropriate human behavior. Specifically, we demonstrate inappropriate degeneration on a large-scale for various generative text-to-image models, thus motivating the need for monitoring and moderating them at deployment. To this end, we evaluate mitigation strategies at inference to suppress the generation of inappropriate content. Our findings show that we can use models' representations of the world's ugliness to align them with human preferences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18395</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311;&#65306;&#38754;&#21521;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#21512;&#30693;&#35782;&#29702;&#35299;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#39640;&#19988;&#28041;&#21450;&#25968;&#25454;&#38544;&#31169;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;&#26631;&#35760;&#25968;&#25454;&#25110;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#26159;&#30001;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#25152;&#38656;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311; (KARD) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#30340;&#20381;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#37325;&#25490;&#22120;&#65292;&#29992;&#20110;&#33719;&#24471;&#19982;&#20381;&#25454;&#29983;&#25104;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;KARD&#22312;&#19977;&#39033;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;LLMs&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#37327;&#23398;&#20064;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#21487;&#33021;&#23384;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#23454;&#35777;&#25581;&#31034;&#20102;11&#20010;&#20856;&#22411;&#30340;&#22686;&#37327;&#23398;&#20064;&#22120;&#23545;&#22522;&#20110;&#20013;&#27602;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#24230;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18384</link><description>&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#65306;&#19968;&#39033;&#23454;&#35777;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study. (arXiv:2305.18384v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#37327;&#23398;&#20064;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#21487;&#33021;&#23384;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#23454;&#35777;&#25581;&#31034;&#20102;11&#20010;&#20856;&#22411;&#30340;&#22686;&#37327;&#23398;&#20064;&#22120;&#23545;&#22522;&#20110;&#20013;&#27602;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#24230;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#32531;&#35299;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#22788;&#29702;&#39034;&#24207;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22686;&#37327;&#23398;&#20064;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#65292;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#22522;&#20110;&#20013;&#27602;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#25105;&#20204;&#35748;&#20026;&#22686;&#37327;&#23398;&#20064;&#20013;&#27969;&#24335;&#25968;&#25454;&#30340;&#26412;&#36136;&#20026;&#23545;&#25163;&#25552;&#20379;&#20102;&#24456;&#22823;&#30340;&#20415;&#21033;&#65292;&#36890;&#36807;&#25968;&#25454;&#27745;&#26579;&#65292;&#23545;&#25163;&#21487;&#20197;&#22312;&#20219;&#20309;&#26102;&#38388;&#25110;&#26102;&#38388;&#24207;&#21015;&#19978;&#21019;&#24314;&#20998;&#24067;&#24335;&#21644;&#36328;&#20219;&#21153;&#25915;&#20987;&#65292;&#20174;&#32780;&#24433;&#21709;&#20219;&#20309;&#26410;&#30693;&#30340;&#20808;&#21069;&#25110;&#21518;&#32493;&#20219;&#21153;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#27880;&#20837;&#26497;&#23567;&#25968;&#37327;&#30340;&#21518;&#38376;&#26679;&#26412; (&#20363;&#22914;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#20165;&#38656;&#35201;&#27880;&#20837;0.1%&#65289;&#12290;&#20026;&#20102;&#21560;&#24341;&#30740;&#31350;&#31038;&#21306;&#30340;&#20851;&#27880;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#23398;&#20064;&#22330;&#26223;&#20986;&#21457;&#65292;&#23454;&#35777;&#25581;&#31034;&#20102;11&#20010;&#20856;&#22411;&#30340;&#22686;&#37327;&#23398;&#20064;&#22120;&#23545;&#22522;&#20110;&#20013;&#27602;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#24230;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#36328;&#20219;&#21153;&#27867;&#21270;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large amounts of incremental learning algorithms have been proposed to alleviate the catastrophic forgetting issue arises while dealing with sequential data on a time series. However, the adversarial robustness of incremental learners has not been widely verified, leaving potential security risks. Specifically, for poisoning-based backdoor attacks, we argue that the nature of streaming data in IL provides great convenience to the adversary by creating the possibility of distributed and cross-task attacks -- an adversary can affect \textbf{any unknown} previous or subsequent task by data poisoning \textbf{at any time or time series} with extremely small amount of backdoor samples injected (e.g., $0.1\%$ based on our observations). To attract the attention of the research community, in this paper, we empirically reveal the high vulnerability of 11 typical incremental learners against poisoning-based backdoor attack on 3 learning scenarios, especially the cross-task generalization effect 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#30719;&#30707;&#20013;&#25552;&#28860;&#40644;&#37329;: &#22522;&#20110;&#20851;&#38190;&#26679;&#26412;&#36873;&#25321;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#22810;&#27169;&#22411;&#30340;&#29616;&#22312;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#20173;&#28982;&#38750;&#24120;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#29992;&#20449;&#24687;&#29702;&#35770;&#26469;&#24314;&#27169;&#33976;&#39311;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#31181;&#26032;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#25193;&#22823;&#29616;&#26377;&#31639;&#27861;&#33539;&#22260;&#20197;&#23545;&#26356;&#24222;&#22823;&#21644;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;0.04&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#20445;&#25345;&#21487;&#27604;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20854;&#36129;&#29486;&#21487;&#33021;&#20026;&#33976;&#39311;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#23433;&#20840;&#20445;&#35777;&#25552;&#20379;&#20551;&#35774;&#30340;&#20570;&#27861;&#65292;&#20197;&#29992;&#20110;&#39564;&#35777;&#20855;&#26377;&#22797;&#26434;&#29615;&#22659;&#21644;&#23398;&#20064;&#22686;&#24378;&#32452;&#20214;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;DNN&#34892;&#20026;&#20551;&#35774;&#65292;&#26469;&#28385;&#36275;&#35201;&#27714;&#30340;&#23433;&#20840;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18372</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#33258;&#20027;&#31995;&#32479;&#39564;&#35777;&#20013;&#30340;&#20551;&#35774;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Assumption Generation for the Verification of Learning-Enabled Autonomous Systems. (arXiv:2305.18372v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#23433;&#20840;&#20445;&#35777;&#25552;&#20379;&#20551;&#35774;&#30340;&#20570;&#27861;&#65292;&#20197;&#29992;&#20110;&#39564;&#35777;&#20855;&#26377;&#22797;&#26434;&#29615;&#22659;&#21644;&#23398;&#20064;&#22686;&#24378;&#32452;&#20214;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#30340;DNN&#34892;&#20026;&#20551;&#35774;&#65292;&#26469;&#28385;&#36275;&#35201;&#27714;&#30340;&#23433;&#20840;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#30340;&#23433;&#20840;&#20445;&#35777;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#22312;&#38656;&#35201;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#31561;&#23398;&#20064;&#22686;&#24378;&#32452;&#20214;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#29992;&#20110;&#35270;&#35273;&#24863;&#30693;&#12290; DNN&#30001;&#20110;&#35268;&#27169;&#22823;&#65288;&#21487;&#33021;&#20855;&#26377;&#25104;&#21315;&#19978;&#19975;&#20010;&#21442;&#25968;&#65289;&#12289;&#32570;&#20047;&#27491;&#24335;&#35268;&#33539;&#65288;DNN&#36890;&#24120;&#26159;&#20174;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#32570;&#20047;&#20219;&#20309;&#27491;&#24335;&#35201;&#27714;&#65289;&#20197;&#21450;&#23545;&#29615;&#22659;&#20013;&#24494;&#23567;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#32780;&#38590;&#20197;&#20998;&#26512;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#20445;&#35777;&#24335;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#36825;&#31181;&#33258;&#20027;&#31995;&#32479;&#30340;&#31995;&#32479;&#32423;&#23433;&#20840;&#23646;&#24615;&#12290; &#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#22312;&#20110;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21512;&#25104;&#26377;&#20445;&#35777;&#30340;DNN&#34892;&#20026;&#30340;&#20551;&#35774;&#65292;&#22312;&#27809;&#26377;DNN&#24863;&#30693;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#20998;&#26512;&#31995;&#32479;&#65292;&#20197;&#20445;&#35777;&#28385;&#36275;&#25152;&#38656;&#30340;&#23433;&#20840;&#23646;&#24615;&#12290; &#21512;&#25104;&#30340;&#20551;&#35774;&#26159;&#26368;&#24369;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#24449;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;DNN&#30340;&#36755;&#20986;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing safety guarantees for autonomous systems is difficult as these systems operate in complex environments that require the use of learning-enabled components, such as deep neural networks (DNNs) for visual perception. DNNs are hard to analyze due to their size (they can have thousands or millions of parameters), lack of formal specifications (DNNs are typically learnt from labeled data, in the absence of any formal requirements), and sensitivity to small changes in the environment. We present an assume-guarantee style compositional approach for the formal verification of system-level safety properties of such autonomous systems. Our insight is that we can analyze the system in the absence of the DNN perception components by automatically synthesizing assumptions on the DNN behaviour that guarantee the satisfaction of the required safety properties. The synthesized assumptions are the weakest in the sense that they characterize the output sequences of all the possible DNNs that, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#24103;&#21644;&#20107;&#20214;&#30456;&#26426;&#25509;&#21475;&#30340;UAV&#24179;&#21488;ColibriUAV&#65292;&#20854;&#22260;&#32469;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#21151;&#32791;RISC-V SoC Kraken&#35774;&#35745;&#65292;Kraken&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;DVS&#30456;&#26426;&#30340;&#20107;&#20214;&#25968;&#25454;&#21644;&#26469;&#33258;RGB&#30456;&#26426;&#30340;&#24103;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.18371</link><description>&lt;p&gt;
ColibriUAV: &#19968;&#31181;&#20855;&#26377;&#20107;&#20214;&#21644;&#24103;&#30456;&#26426;&#25509;&#21475;&#30340;&#36229;&#24555;&#12289;&#33021;&#25928;&#21452;&#20248;&#30340;&#31070;&#32463;&#22411;&#36793;&#32536;&#22788;&#29702;&#26080;&#20154;&#26426;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
ColibriUAV: An Ultra-Fast, Energy-Efficient Neuromorphic Edge Processing UAV-Platform with Event-Based and Frame-Based Cameras. (arXiv:2305.18371v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#24103;&#21644;&#20107;&#20214;&#30456;&#26426;&#25509;&#21475;&#30340;UAV&#24179;&#21488;ColibriUAV&#65292;&#20854;&#22260;&#32469;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#21151;&#32791;RISC-V SoC Kraken&#35774;&#35745;&#65292;Kraken&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;DVS&#30456;&#26426;&#30340;&#20107;&#20214;&#25968;&#25454;&#21644;&#26469;&#33258;RGB&#30456;&#26426;&#30340;&#24103;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#35270;&#35273;&#20256;&#24863;&#22120;&#65288;DVS&#65289;&#39537;&#21160;&#30340;&#26080;&#20154;&#26426;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#21463;&#21040;&#29983;&#29289;&#21551;&#21457;&#30340;&#20107;&#20214;&#20256;&#24863;&#22120;&#24494;&#31186;&#32423;&#21453;&#24212;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#30456;&#27604;RGB&#30456;&#26426;&#65292;&#22686;&#21152;&#20102;&#24863;&#30693;&#20219;&#21153;&#30340;&#31283;&#20581;&#24615;&#65292;&#38477;&#20302;&#20102;&#24310;&#36831;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ColibriUAV&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#24103;&#21644;&#20107;&#20214;&#30456;&#26426;&#25509;&#21475;&#30340;UAV&#24179;&#21488;&#65292;&#21487;&#36827;&#34892;&#39640;&#25928;&#24863;&#30693;&#21644;&#36817;&#20256;&#24863;&#22120;&#22788;&#29702;&#12290;&#25152;&#25552;&#35758;&#30340;&#24179;&#21488;&#22260;&#32469;Kraken&#35774;&#35745;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#21151;&#32791;RISC-V SoC&#65292;&#20854;&#20855;&#26377;&#20004;&#20010;&#23450;&#20301;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#19977;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;Kraken&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;DVS&#30456;&#26426;&#30340;&#20107;&#20214;&#25968;&#25454;&#21644;&#26469;&#33258;RGB&#30456;&#26426;&#30340;&#24103;&#25968;&#25454;&#12290;Kraken&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#20854;&#19982;DVS&#30456;&#26426;&#30340;&#38598;&#25104;&#22411;&#19987;&#29992;&#25509;&#21475;&#12290;&#26412;&#25991;&#23545;&#31070;&#32463;&#22411;&#20107;&#20214;UAV&#23376;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#33021;&#37327;&#25928;&#29575;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#29616;&#20102;72&#30340;&#20107;&#20214;&#25968;&#25454;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interest in dynamic vision sensor (DVS)-powered unmanned aerial vehicles (UAV) is raising, especially due to the microsecond-level reaction time of the bio-inspired event sensor, which increases robustness and reduces latency of the perception tasks compared to a RGB camera. This work presents ColibriUAV, a UAV platform with both frame-based and event-based cameras interfaces for efficient perception and near-sensor processing. The proposed platform is designed around Kraken, a novel low-power RISC-V System on Chip with two hardware accelerators targeting spiking neural networks and deep ternary neural networks.Kraken is capable of efficiently processing both event data from a DVS camera and frame data from an RGB camera. A key feature of Kraken is its integrated, dedicated interface with a DVS camera. This paper benchmarks the end-to-end latency and power efficiency of the neuromorphic and event-based UAV subsystem, demonstrating state-of-the-art event data with a throughput of 72
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18365</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#21040;&#24213;&#26377;&#24590;&#26679;&#30340;&#24212;&#29992;&#65311;&#20843;&#20010;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#36719;&#20214;&#24037;&#31243;&#31561;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;LLM &#26159;&#21542;&#26377;&#33021;&#21147;&#25512;&#21160;&#21270;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#21547; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21517;&#31216;&#39044;&#27979;&#12289;&#23646;&#24615;&#39044;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#12289;&#21453;&#24212;&#39044;&#27979;&#12289;&#21453;&#21512;&#25104;&#65288;&#20174;&#20135;&#29289;&#39044;&#27979;&#21453;&#24212;&#29289;&#65289;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#23376;&#35774;&#35745;&#12289;&#20998;&#23376;&#23383;&#24149;&#21644;&#35797;&#21058;&#36873;&#25321;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; BBBP&#12289;Tox21&#12289;PubChem&#12289;USPTO &#21644; ChEBI&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#31034;&#20363;&#20013;&#65292;&#23545;&#19977;&#31181; GPT &#27169;&#22411;&#65288;GPT-4&#12289;GPT-3.5 &#21644; DaVinci-003&#65289;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been rapidly applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper,we establish a comprehensive benchmark containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6)text-based molecule design, 7) molecule captioning, and 8) reagent selection. Our analysis draws on widely recognized datasets including BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Three GPT models (GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21313;&#31181;&#35780;&#20272;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.18363</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Conversational Recommender Systems. (arXiv:2305.18363v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21313;&#31181;&#35780;&#20272;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35299;&#37322;&#24050;&#32463;&#35777;&#26126;&#22312;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#25512;&#33616;&#21512;&#29702;&#24615;&#20197;&#21450;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#20855;&#26377;&#30410;&#22788;&#12290;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#65292;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#19978;&#19979;&#25991;&#21270;&#35299;&#37322;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34913;&#37327;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21313;&#31181;&#35780;&#20272;&#35270;&#35282;&#65292;&#32467;&#21512;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#30340;&#27010;&#24565;&#21644;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#35780;&#20272;&#20102;&#20116;&#20010;&#24050;&#26377;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#35266;&#23519;&#21040;&#38656;&#35201;&#25552;&#39640;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#35299;&#37322;&#36136;&#37327;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#21160;&#21644;&#33258;&#21160;&#26041;&#27861;&#25193;&#23637;&#20102;&#36825;&#20123;&#23545;&#35805;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;&#21487;&#35299;&#37322;&#25512;&#33616;&#23545;&#35805;&#8221;&#65288;E-ReDial&#65289;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;756&#20010;&#23545;&#35805;&#65292;&#36229;&#36807;2000&#26465;&#39640;&#36136;&#37327;&#30340;&#37325;&#20889;&#35299;&#37322;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations in conventional recommender systems have demonstrated benefits in helping the user understand the rationality of the recommendations and improving the system's efficiency, transparency, and trustworthiness. In the conversational environment, multiple contextualized explanations need to be generated, which poses further challenges for explanations. To better measure explainability in conversational recommender systems (CRS), we propose ten evaluation perspectives based on concepts from conventional recommender systems together with the characteristics of CRS. We assess five existing CRS benchmark datasets using these metrics and observe the necessity of improving the explanation quality of CRS. To achieve this, we conduct manual and automatic approaches to extend these dialogues and construct a new CRS dataset, namely Explainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with over 2,000 high-quality rewritten explanations. We compare two baseline approa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#27169;&#22411;&#25970;&#38376;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25214;&#21040;&#26174;&#33879;&#30340;&#27010;&#24565;&#20197;&#36991;&#20813;&#35823;&#35299;&#65292;&#24182;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#22312;&#26576;&#20010;&#20540;&#19979;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18362</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#27169;&#22411;&#25970;&#38376;&#25216;&#26415;&#26816;&#39564;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26174;&#33879;&#32479;&#35745;&#32467;&#35770;
&lt;/p&gt;
&lt;p&gt;
Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs. (arXiv:2305.18362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#27169;&#22411;&#25970;&#38376;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25214;&#21040;&#26174;&#33879;&#30340;&#27010;&#24565;&#20197;&#36991;&#20813;&#35823;&#35299;&#65292;&#24182;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#22312;&#26576;&#20010;&#20540;&#19979;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#27010;&#24565;&#35299;&#37322;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#25253;&#65292;&#23558;&#19981;&#30456;&#20851;&#30340;&#27010;&#24565;&#35823;&#35748;&#20026;&#26159;&#39044;&#27979;&#20219;&#21153;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#29992;&#20110;&#20998;&#31867;&#30340;&#26174;&#33879;&#27010;&#24565;&#20197;&#38450;&#27490;&#35823;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#22270;&#20687;&#27010;&#24565;&#65292;&#28982;&#21518;&#20351;&#29992;Knockoff&#26679;&#26412;&#36873;&#25321;&#37325;&#35201;&#27010;&#24565;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#22312;&#26576;&#20010;&#20540;&#19979;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36866;&#24403;&#22320;&#25511;&#21046;FDR&#65292;&#21516;&#26102;&#36873;&#25321;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A concept-based classifier can explain the decision process of a deep learning model by human-understandable concepts in image classification problems. However, sometimes concept-based explanations may cause false positives, which misregards unrelated concepts as important for the prediction task. Our goal is to find the statistically significant concept for classification to prevent misinterpretation. In this study, we propose a method using a deep learning model to learn the image concept and then using the Knockoff samples to select the important concepts for prediction by controlling the False Discovery Rate (FDR) under a certain value. We evaluate the proposed method in our synthetic and real data experiments. Also, it shows that our method can control the FDR properly while selecting highly interpretable concepts to improve the trustworthiness of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#25913;&#21892;&#35270;&#35273;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.18357</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepSI: Interactive Deep Learning for Semantic Interaction. (arXiv:2305.18357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#25913;&#21892;&#35270;&#35273;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#35270;&#35273;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#20132;&#20114;&#12290;&#35821;&#20041;&#20132;&#20114;&#25512;&#26029;&#20998;&#26512;&#20154;&#21592;&#22312;&#24863;&#30693;&#36807;&#31243;&#20013;&#30340;&#31934;&#30830;&#24847;&#22270;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\text{DeepSI}_{\text{finetune}}$&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#21040;&#20154;&#22312;&#20132;&#20114;&#24335;&#24863;&#30693;&#31649;&#36947;&#20013;&#65292;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#28145;&#24230;&#23398;&#20064;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#35821;&#20041;&#20132;&#20114;&#25512;&#26029;&#30340;&#36136;&#37327;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#35821;&#20041;&#20132;&#20114;&#26469;&#24494;&#35843;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35821;&#20041;&#20132;&#20114;&#25512;&#26029;&#30340;&#36136;&#37327;&#12290;&#20154;&#26426;&#20132;&#20114;&#21644;&#28145;&#24230;&#23398;&#20064;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#20351;&#24471;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35780;&#20272;&#23558;&#28145;&#24230;&#23398;&#20064;&#23884;&#20837;&#21040;&#35821;&#20041;&#20132;&#20114;&#24490;&#29615;&#20013;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;$\text{DeepSI}_{\te
&lt;/p&gt;
&lt;p&gt;
In this paper, we design novel interactive deep learning methods to improve semantic interactions in visual analytics applications. The ability of semantic interaction to infer analysts' precise intents during sensemaking is dependent on the quality of the underlying data representation. We propose the $\text{DeepSI}_{\text{finetune}}$ framework that integrates deep learning into the human-in-the-loop interactive sensemaking pipeline, with two important properties. First, deep learning extracts meaningful representations from raw data, which improves semantic interaction inference. Second, semantic interactions are exploited to fine-tune the deep learning representations, which then further improves semantic interaction inference. This feedback loop between human interaction and deep learning enables efficient learning of userand task-specific representations. To evaluate the advantage of embedding the deep learning within the semantic interaction loop, we compare $\text{DeepSI}_{\te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;GPT&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#38656;&#35201;&#26680;&#24515;&#27010;&#24565;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#26032;&#30340;1D-ARC&#22522;&#20934;&#65292;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18354</link><description>&lt;p&gt;
LLM&#21644;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65306;&#25104;&#21151;&#12289;&#22833;&#36133;&#19982;&#22522;&#20110;&#23545;&#35937;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. (arXiv:2305.18354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;GPT&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#38656;&#35201;&#26680;&#24515;&#27010;&#24565;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#26032;&#30340;1D-ARC&#22522;&#20934;&#65292;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21542;&#35299;&#20915;&#31616;&#21333;&#30340;&#25277;&#35937;&#25512;&#29702;&#38382;&#39064;&#65311;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#20998;&#26512;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#30340;&#34920;&#29616;&#26469;&#25506;&#32034;&#36825;&#20010;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#65292;&#20174;&#26377;&#38480;&#30340;&#20363;&#23376;&#20013;&#35201;&#27714;&#25105;&#20204;&#26377;&#20123;&#20851;&#20110;&#27010;&#24565;&#65288;&#22914;&#23545;&#35937;&#12289;&#30446;&#26631;&#29366;&#24577;&#12289;&#35745;&#25968;&#21644;&#22522;&#26412;&#20960;&#20309;&#65289;&#30340;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#20197;&#35299;&#20915;&#38382;&#39064;&#12290;&#24403;&#20351;&#29992;&#25991;&#26412;&#32534;&#30721;&#23545;&#20108;&#32500;&#36755;&#20837;&#36755;&#20986;&#32593;&#26684;&#30340;ARC&#20219;&#21153;&#36827;&#34892;&#32534;&#30721;&#26102;&#65292;GPT-4&#20165;&#35299;&#20915;&#20102;50&#20010;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#30340;13&#20010;&#12290;&#25105;&#20204;&#30340;&#22833;&#36133;&#20998;&#26512;&#26174;&#31034;&#65292;GPT-4&#35782;&#21035;&#23545;&#35937;&#24182;&#25512;&#29702;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#34920;&#31034;&#20219;&#21153;&#20013;&#23545;&#35937;&#30340;&#25991;&#26412;&#30340;&#39034;&#24207;&#24615;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;1D-ARC&#65292;&#23427;&#30001;&#26356;&#26377;&#21033;&#20110;&#22522;&#20110;GPT&#30340;&#25512;&#29702;&#30340;&#19968;&#32500;&#65288;&#31867;&#20284;&#25968;&#32452;&#65289;&#20219;&#21153;&#32452;&#25104;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#65292;GPT&#30340;&#34920;&#29616;&#30830;&#23454;&#27604;&#22312;&#65288;2D&#65289;ARC&#19978;&#26356;&#22909;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#23427;&#20445;&#30041;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#22522;&#26412;&#31354;&#38388;&#20851;&#31995;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#12290;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#65292;GPT-4&#22312;ARC&#19978;&#30340;&#25104;&#21151;&#29575;&#22823;&#22823;&#25552;&#39640;&#65292;&#36798;&#21040;&#20102;45/50&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;LLM&#22522;&#20110;&#25512;&#29702;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some "core knowledge" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we prop
&lt;/p&gt;</description></item><item><title>&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#39640;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#20351;&#24471;&#39640;&#32500;&#25968;&#25454;&#22686;&#21152;&#65292;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#65292;&#29992;&#20110;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18352</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#22810;&#35270;&#35282;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Genetic Algorithm for Multi-View Feature Selection. (arXiv:2305.18352v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18352
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#39640;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#20351;&#24471;&#39640;&#32500;&#25968;&#25454;&#22686;&#21152;&#65292;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#65292;&#29992;&#20110;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#34917;&#20805;&#20449;&#24687;&#26469;&#22686;&#24378;&#39044;&#27979;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20250;&#23548;&#33268;&#39640;&#32500;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;&#36825;&#23545;&#21487;&#20197;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#39044;&#27979;&#27169;&#22411;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#19981;&#20165;&#21487;&#20197;&#35299;&#20915;&#19981;&#33391;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23613;&#31649;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#21033;&#29992;&#36328;&#27169;&#24577;&#30340;&#20869;&#22312;&#20449;&#24687;&#12289;&#32570;&#20047;&#27867;&#21270;&#24615;&#21644;&#36866;&#29992;&#20110;&#29305;&#23450;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#19978;&#30340;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view datasets offer diverse forms of data that can enhance prediction models by providing complementary information. However, the use of multi-view data leads to an increase in high-dimensional data, which poses significant challenges for the prediction models that can lead to poor generalization. Therefore, relevant feature selection from multi-view datasets is important as it not only addresses the poor generalization but also enhances the interpretability of the models. Despite the success of traditional feature selection methods, they have limitations in leveraging intrinsic information across modalities, lacking generalizability, and being tailored to specific classification tasks. We propose a novel genetic algorithm strategy to overcome these limitations of traditional feature selection methods for multi-view data. Our proposed approach, called the multi-view multi-objective feature selection genetic algorithm (MMFS-GA), simultaneously selects the optimal subset of feature
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#29983;&#25104; AI &#23545;&#25968;&#23383;&#38452;&#24433;&#20135;&#19994;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20174;&#40657;&#33394;/&#38452;&#24433;&#20135;&#19994;&#30340;&#19978;&#28216;&#12289;&#20013;&#28216;&#21644;&#19979;&#28216;&#36335;&#24452;&#36827;&#34892;&#20102;&#25216;&#26415;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#29616;&#26377;&#39118;&#38505;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18346</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#35770;&#25991;&#65306;&#29983;&#25104; AI &#22914;&#20309;&#37325;&#22609;&#25968;&#23383;&#38452;&#24433;&#20135;&#19994;&#65311;
&lt;/p&gt;
&lt;p&gt;
Attention Paper: How Generative AI Reshapes Digital Shadow Industry?. (arXiv:2305.18346v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#29983;&#25104; AI &#23545;&#25968;&#23383;&#38452;&#24433;&#20135;&#19994;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20174;&#40657;&#33394;/&#38452;&#24433;&#20135;&#19994;&#30340;&#19978;&#28216;&#12289;&#20013;&#28216;&#21644;&#19979;&#28216;&#36335;&#24452;&#36827;&#34892;&#20102;&#25216;&#26415;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#29616;&#26377;&#39118;&#38505;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#32463;&#27982;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#21508;&#31181;&#40657;&#33394;&#21644;&#38452;&#24433;&#20114;&#32852;&#32593;&#20135;&#19994;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#20135;&#19994;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21487;&#20197;&#36890;&#36807;&#25968;&#23383;&#39118;&#38505;&#31649;&#29702;&#65288;DRM&#65289;&#26469;&#35782;&#21035;&#21644;&#31649;&#29702;&#65292;&#20854;&#20013;&#21253;&#25324;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31561;&#19981;&#21516;&#25216;&#26415;&#12290;DRM &#26550;&#26500;&#30340;&#21457;&#23637;&#24050;&#32463;&#21463;&#21040;&#25968;&#25454;&#24418;&#24335;&#21464;&#21270;&#30340;&#39537;&#21160;&#12290;&#28982;&#32780;&#65292;AI &#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#22914; ChatGPT &#21644; Stable Diffusion&#65292;&#20026;&#40657;&#33394;&#21644;&#38452;&#24433;&#20135;&#19994;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20010;&#24615;&#21270;&#25968;&#25454;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#29255;&#21644;&#23545;&#35805;&#20197;&#36827;&#34892;&#27450;&#35784;&#27963;&#21160;&#12290;&#36825;&#23545;&#20110; DRM &#31995;&#32479;&#26469;&#25511;&#21046;&#26469;&#33258;&#25968;&#25454;&#29983;&#25104;&#28304;&#30340;&#39118;&#38505;&#24182;&#36805;&#36895;&#24212;&#23545;&#24555;&#36895;&#21464;&#21270;&#30340;&#39118;&#38505;&#29615;&#22659;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#40657;&#33394;/&#38452;&#24433;&#20135;&#19994;&#30340;&#19978;&#28216;&#12289;&#20013;&#28216;&#21644;&#19979;&#28216;&#36335;&#24452;&#25552;&#20379; AIGC &#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#30340;&#25216;&#26415;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;&#29616;&#26377;&#39118;&#38505;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of digital economy has led to the emergence of various black and shadow internet industries, which pose potential risks that can be identified and managed through digital risk management (DRM) that uses different techniques such as machine learning and deep learning. The evolution of DRM architecture has been driven by changes in data forms. However, the development of AI-generated content (AIGC) technology, such as ChatGPT and Stable Diffusion, has given black and shadow industries powerful tools to personalize data and generate realistic images and conversations for fraudulent activities. This poses a challenge for DRM systems to control risks from the source of data generation and to respond quickly to the fast-changing risk environment. This paper aims to provide a technical analysis of the challenges and opportunities of AIGC from upstream, midstream, and downstream paths of black/shadow industries and suggest future directions for improving existing risk con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18342</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#32534;&#31243;&#20013;&#31070;&#32463;&#20219;&#21153;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#19979;&#32473;&#23450;&#30340;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20687; GPT-4 &#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21512;&#25104;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#36923;&#36753;&#21644;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415; NeurTaskSyn&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#21512;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;NeurTaskSyn &#30001;&#20004;&#20010;&#37096;&#20998;&#26500;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#25351;&#23548;&#24213;&#23618;&#31526;&#21495;&#25191;&#34892;&#24341;&#25806;&#29983;&#25104;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RLCF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18341</link><description>&lt;p&gt;
&#20351;&#29992;&#32534;&#35793;&#22120;&#29983;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#35843;&#25972;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback. (arXiv:2305.18341v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RLCF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20195;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31243;&#24207;&#21512;&#25104;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#21487;&#33021;&#36829;&#21453;&#22522;&#26412;&#30340;&#35821;&#35328;&#32423;&#21035;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RLCF&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#30340;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290; RLCF&#23558;LLM&#35270;&#20026;&#36890;&#36807;RL&#20195;&#29702;&#36880;&#27493;&#29983;&#25104;&#20195;&#30721;&#65292;&#24182;&#25509;&#25910;&#20197;&#19979;&#21453;&#39304;&#65306;&#65288;i&#65289;&#32534;&#35793;&#22120;&#27966;&#29983;&#30340;&#21453;&#39304;&#19982;&#25152;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#36890;&#36807;&#19968;&#32452;&#27491;&#30830;&#24615;&#26816;&#26597;&#26377;&#20851;; &#65288;ii&#65289;&#19981;&#21516;LLM&#30340;&#21453;&#39304;&#65292;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#19968;&#32452;&#21442;&#32771;&#31243;&#24207;&#30456;&#20284;&#12290;&#36825;&#20123;&#21453;&#39304;&#26426;&#21046;&#24110;&#21161;&#25152;&#29983;&#25104;&#30340;&#20195;&#30721;&#22312;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#30340;&#21516;&#26102;&#20445;&#25345;&#22312;&#30446;&#26631;&#20998;&#24067;&#20013;&#12290;RLCF&#26159;&#27169;&#22411;&#21644;&#35821;&#35328;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#22312;Java&#30340;MBJP&#21644;MathQA&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLCF&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, the code that these models produce can violate basic language-level invariants, leading to lower performance in downstream tasks. We address this issue through an approach, called RLCF, that further trains a pre-trained LLM using feedback from a code compiler. RLCF views the LLM as an RL agent that generates code step by step and receives: (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM on whether the generated code is similar to a set of reference programs in the training corpus. Together, these feedback mechanisms help the generated code remain within the target distribution while passing all static correctness checks. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF significantly raise
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#23041;&#32961;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.18339</link><description>&lt;p&gt;
ChatGPT&#65306;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#25361;&#25112;&#19982;&#35299;&#20915;&#26041;&#26696;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. (arXiv:2305.18339v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#23041;&#32961;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27604;&#22914;ChatGPT&#30340;&#26222;&#21450;&#20351;&#29992;&#65292;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#27491;&#22312;&#24341;&#39046;&#20869;&#23481;&#21019;&#20316;&#21644;&#30693;&#35782;&#34920;&#31034;&#26041;&#24335;&#23454;&#29616;&#33539;&#24335;&#36716;&#21464;&#12290;AIGC&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;AI&#31639;&#27861;&#26469;&#36741;&#21161;&#25110;&#26367;&#20195;&#20154;&#31867;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#21019;&#24314;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#21644;&#31867;&#20154;&#30340;&#20869;&#23481;&#12290;&#23613;&#31649;&#22312;AIGC&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#25361;&#25112;&#20173;&#38656;&#35299;&#20915;&#12290;&#26412;&#25991;&#28145;&#20837;&#35843;&#26597;&#20102;AIGC&#33539;&#24335;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#12289;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;AIGC&#30340;&#25216;&#26415;&#23454;&#29616;&#12289;&#24635;&#20307;&#26550;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24037;&#20316;&#27169;&#24335;&#21644;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#38024;&#23545;AIGC&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#20998;&#31867;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;GPT&#21644;AIGC&#25216;&#26415;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#24050;&#30830;&#23450;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;AIGC&#39046;&#22495;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#21457;&#23637;AIGC&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#25104;&#22270;&#20687;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#21512;&#25104;&#22270;&#20687;&#35780;&#20272;&#20307;&#31995;&#65292;&#35777;&#26126;&#20102;&#21512;&#25104;&#22270;&#20687;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18337</link><description>&lt;p&gt;
&#26080;&#38656;&#23436;&#32654;&#65292;&#20063;&#33021;&#38378;&#32768;&#65306;&#25581;&#31034;&#21512;&#25104;&#22270;&#20687;&#30340;&#24212;&#29992;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
You Don't Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images. (arXiv:2305.18337v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#25104;&#22270;&#20687;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#21512;&#25104;&#22270;&#20687;&#35780;&#20272;&#20307;&#31995;&#65292;&#35777;&#26126;&#20102;&#21512;&#25104;&#22270;&#20687;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#21512;&#25104;&#27169;&#22411;&#30340;&#36873;&#25321;&#20027;&#35201;&#22522;&#20110;&#22270;&#20687;&#36136;&#37327;&#30340;&#24230;&#37327;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#32773;&#20559;&#29233;&#20135;&#29983;&#36924;&#30495;&#22270;&#20687;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#21363;&#20855;&#26377;&#33391;&#22909;&#20445;&#30495;&#24230;&#24471;&#20998;&#65288;&#22914;&#20302;&#30340;FID&#21644;&#39640;&#30340;PSNR&#65289;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#19981;&#20165;&#23616;&#38480;&#20110;&#20445;&#30495;&#24230;&#65292;&#24212;&#35813;&#35780;&#20272;&#19968;&#31995;&#21015;&#25351;&#26631;&#20197;&#20840;&#38754;&#34913;&#37327;&#20854;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#24182;&#19981;&#26159;&#21512;&#25104;&#22270;&#20687;&#23454;&#29992;&#24615;&#30340;&#20934;&#30830;&#39044;&#20272;&#22120;&#65292;&#36825;&#20123;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#20063;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#12289;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#31561;&#22810;&#39033;&#25351;&#26631;&#30340;&#21512;&#25104;&#22270;&#20687;&#20840;&#38754;&#35780;&#20272;&#20307;&#31995;&#12290;&#36890;&#36807;&#23545;&#36229;&#36807;10&#19975;&#24352;&#33016;&#37096;X&#32447;&#22270;&#29255;&#21450;&#20854;&#21512;&#25104;&#21103;&#26412;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21512;&#25104;&#22270;&#20687;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic images generated from deep generative models have the potential to address data scarcity and data privacy issues. The selection of synthesis models is mostly based on image quality measurements, and most researchers favor synthetic images that produce realistic images, i.e., images with good fidelity scores, such as low Fr\'echet Inception Distance (FID) and high Peak Signal-To-Noise Ratio (PSNR). However, the quality of synthetic images is not limited to fidelity, and a wide spectrum of metrics should be evaluated to comprehensively measure the quality of synthetic images. In addition, quality metrics are not truthful predictors of the utility of synthetic images, and the relations between these evaluation metrics are not yet clear. In this work, we have established a comprehensive set of evaluators for synthetic images, including fidelity, variety, privacy, and utility. By analyzing more than 100k chest X-ray images and their synthetic copies, we have demonstrated that ther
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18333</link><description>&lt;p&gt;
&#20855;&#26377;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#25490;&#21517;&#65306;&#33258;&#22686;&#24378;&#21160;&#24577;&#19979;&#30340;&#29992;&#25143;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics. (arXiv:2305.18333v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18333
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#30830;&#35748;&#27969;&#34892;&#24230;&#20559;&#35265;&#22312;&#25512;&#33616;&#65288;&#21644;&#20854;&#20182;&#22522;&#20110;&#25490;&#21517;&#30340;&#65289;&#31995;&#32479;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#20854;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#30340;&#35814;&#32454;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#36890;&#36807;&#23427;&#65292;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#21487;&#20197;&#24433;&#21709;&#29992;&#25143;&#36873;&#25321;&#65292;&#24182;&#19988;&#21487;&#20197;&#36127;&#38754;&#24433;&#21709;&#21508;&#31181;&#25512;&#33616;&#31574;&#30053;&#30340;&#38598;&#20307;&#29992;&#25143;&#25928;&#29992;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#38750;&#24179;&#31283;&#19978;&#19979;&#25991;&#33073;&#38774;&#26426;&#65292;&#24378;&#35843;&#19981;&#26159;&#20026;&#20102;&#28040;&#38500;&#27969;&#34892;&#24230;&#20559;&#35265;&#32780;&#26159;&#20026;&#20102;&#20943;&#36731;&#20854;&#36127;&#38754;&#24433;&#21709;&#32780;&#36827;&#34892;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#39318;&#20808;&#65292;&#26222;&#36890;&#30340;&#26377;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#25512;&#33616;&#31995;&#32479;&#20250;&#36890;&#36807;&#28151;&#28102;&#29289;&#21697;&#36136;&#37327;&#21644;&#27969;&#34892;&#24230;&#32780;&#24341;&#21457;&#32447;&#24615;&#36951;&#25022;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#32447;&#24615;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#29289;&#21697;&#36136;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#20063;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#36275;&#22815;&#21464;&#24322;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31867;UCB&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#23454;&#20102;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While popularity bias is recognized to play a role in recommmender (and other ranking-based) systems, detailed analyses of its impact on user welfare have largely been lacking. We propose a general mechanism by which item popularity, item quality, and position bias can impact user choice, and how it can negatively impact the collective user utility of various recommender policies. Formulating the problem as a non-stationary contextual bandit, we highlight the importance of exploration, not to eliminate popularity bias, but to mitigate its negative effects. First, naive popularity-biased recommenders are shown to induce linear regret by conflating item quality and popularity. More generally, we show that, even in linear settings, identifiability of item quality may not be possible due to the confounding effects of popularity bias. However, under sufficient variability assumptions, we develop an efficient UCB-style algorithm and prove efficient regret guarantees. We complement our analys
&lt;/p&gt;</description></item><item><title>#REVAL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#35821;&#20041;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;#REVAL&#22312;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.18330</link><description>&lt;p&gt;
#REVAL&#65306;&#19968;&#31181;&#29992;&#20110;hashtag&#25512;&#33616;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
#REVAL: a semantic evaluation framework for hashtag recommendation. (arXiv:2305.18330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18330
&lt;/p&gt;
&lt;p&gt;
#REVAL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#35821;&#20041;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;#REVAL&#22312;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#31995;&#32479;&#20013;&#65292;&#33258;&#21160;&#35780;&#20272;hashtag&#25512;&#33616;&#27169;&#22411;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#39318;&#20808;&#27604;&#36739;&#31639;&#27861;&#25512;&#33616;&#30340;hashtag&#19982;&#23454;&#38469;&#30340;hashtag&#30340;&#31934;&#30830;&#23545;&#24212;&#20851;&#31995;&#65292;&#28982;&#21518;&#20351;&#29992;&#31934;&#30830;&#21305;&#37197;&#30340;&#25968;&#37327;&#35745;&#31639;&#21629;&#20013;&#29575;&#12289;&#21629;&#20013;&#27604;&#29575;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#25110;F1&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35780;&#20272;&#26041;&#24335;&#24573;&#30053;&#20102;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;#REval&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#31216;&#20026;BERTag&#30340;&#20869;&#37096;&#27169;&#22359;&#65292;&#21487;&#33258;&#21160;&#23398;&#20064;hashtag&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;#REval-hit-ratio&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#20102;#REval&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#21644;&#25512;&#33616;&#20013;&#30340;&#21516;&#20041;&#35789;&#21644;hashtag&#25968;&#37327;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25429;&#25417;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation of hashtag recommendation models is a fundamental task in many online social network systems. In the traditional evaluation method, the recommended hashtags from an algorithm are firstly compared with the ground truth hashtags for exact correspondences. The number of exact matches is then used to calculate the hit rate, hit ratio, precision, recall, or F1-score. This way of evaluating hashtag similarities is inadequate as it ignores the semantic correlation between the recommended and ground truth hashtags. To tackle this problem, we propose a novel semantic evaluation framework for hashtag recommendation, called #REval. This framework includes an internal module referred to as BERTag, which automatically learns the hashtag embeddings. We investigate on how the #REval framework performs under different word embedding methods and different numbers of synonyms and hashtags in the recommendation using our proposed #REval-hit-ratio measure. Our experiments of the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#28608;&#20809;&#36229;&#22768;&#21487;&#35270;&#21270;&#27979;&#35797;&#22270;&#20687;&#20013;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#27604;&#36890;&#29992;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#26356;&#26377;&#25928;&#19988;&#35745;&#31639;&#26102;&#38388;&#26356;&#30701;&#12290;</title><link>http://arxiv.org/abs/2305.18327</link><description>&lt;p&gt;
&#28608;&#20809;&#36229;&#22768;&#21487;&#35270;&#21270;&#27979;&#35797;&#22270;&#20687;&#30340;&#32570;&#38519;&#26816;&#27979;&#28145;&#24230;CNN&#32467;&#26500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Deep CNN Structures for Defect Detection From Laser Ultrasonic Visualization Testing Images. (arXiv:2305.18327v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#28608;&#20809;&#36229;&#22768;&#21487;&#35270;&#21270;&#27979;&#35797;&#22270;&#20687;&#20013;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#27604;&#36890;&#29992;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#26356;&#26377;&#25928;&#19988;&#35745;&#31639;&#26102;&#38388;&#26356;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36229;&#22768;&#26080;&#25439;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#12290;&#23558;&#28608;&#20809;&#36229;&#22768;&#26816;&#27979;&#19982;&#25955;&#27874;&#21487;&#35270;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#28608;&#20809;&#36229;&#22768;&#21487;&#35270;&#21270;&#27979;&#35797;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#25955;&#27874;&#34987;&#21487;&#35270;&#21270;&#65292;&#26816;&#39564;&#21592;&#20173;&#38656;&#35201;&#20180;&#32454;&#26816;&#26597;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#33258;&#21160;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LUVT&#22270;&#20687;&#20013;&#30340;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;LUVT&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#19982;&#36890;&#29992;&#23545;&#35937;&#26816;&#27979;&#38382;&#39064;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#25506;&#32034;&#36866;&#21512;&#35813;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#20351;&#29992;&#26469;&#33258;SUS304&#24179;&#26495;&#30340;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#27604;&#36890;&#29992;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25152;&#38656;&#30340;&#39044;&#27979;&#35745;&#31639;&#26102;&#38388;&#27604;&#36890;&#29992;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of ultrasonic nondestructive testing has been increasing in recent years, and there are high expectations for the potential of laser ultrasonic visualization testing, which combines laser ultrasonic testing with scattered wave visualization technology. Even if scattered waves are visualized, inspectors still need to carefully inspect the images. To automate this, this paper proposes a deep neural network for automatic defect detection and localization in LUVT images. To explore the structure of a neural network suitable to this task, we compared the LUVT image analysis problem with the generic object detection problem. Numerical experiments using real-world data from a SUS304 flat plate showed that the proposed method is more effective than the general object detection model in terms of prediction performance. We also show that the computational time required for prediction is faster than that of the general object detection model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;BigVideo&#65292; &#38598;&#25104;&#20102;4.5 million&#21477;&#23376;&#23545;&#21644;9981&#23567;&#26102;&#35270;&#39057;&#65292;&#35774;&#35745;&#20102;&#26377;&#27495;&#20041;&#21644;&#26126;&#30830;&#30340;&#27979;&#35797;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;NMT&#27169;&#22411;&#30340;BLEU&#12289;BLEURT&#21644;COMET&#24471;&#20998;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#65292;&#25968;&#25454;&#38598;&#21644;&#32763;&#35793;&#27169;&#22411;&#20844;&#24320;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18326</link><description>&lt;p&gt;
BigVideo:&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation. (arXiv:2305.18326v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;BigVideo&#65292; &#38598;&#25104;&#20102;4.5 million&#21477;&#23376;&#23545;&#21644;9981&#23567;&#26102;&#35270;&#39057;&#65292;&#35774;&#35745;&#20102;&#26377;&#27495;&#20041;&#21644;&#26126;&#30830;&#30340;&#27979;&#35797;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;NMT&#27169;&#22411;&#30340;BLEU&#12289;BLEURT&#21644;COMET&#24471;&#20998;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#65292;&#25968;&#25454;&#38598;&#21644;&#32763;&#35793;&#27169;&#22411;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35270;&#39057;&#23383;&#24149;&#32763;&#35793;&#25968;&#25454;&#38598;BigVideo&#65292;&#20197;&#20419;&#36827;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;How2&#21644;VaTeX&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;BigVideo&#36229;&#36807;10&#20493;&#65292;&#21253;&#25324;450&#19975;&#20010;&#21477;&#23376;&#23545;&#21644;9981&#23567;&#26102;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#26377;&#24847;&#35774;&#35745;&#30340;&#27979;&#35797;&#38598;&#26469;&#39564;&#35777;&#35270;&#35273;&#20449;&#24687;&#30340;&#24517;&#35201;&#24615;&#65306;&#19968;&#20010;&#26159;&#19968;&#20010;&#26377;&#27495;&#20041;&#35789;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#20854;&#20013;&#25991;&#26412;&#19978;&#19979;&#25991;&#23545;&#20110;&#32763;&#35793;&#26159;&#33258;&#21253;&#21547;&#30340;&#26126;&#30830;&#38598;&#21512;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24314;&#27169;&#25991;&#26412;&#21644;&#35270;&#39057;&#20849;&#20139;&#30340;&#20849;&#21516;&#35821;&#20041;&#65292;&#25105;&#20204;&#22312;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;BigVideo&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65306;a&#65289;&#35270;&#35273;&#20449;&#24687;&#22312;&#27495;&#20041;&#21644;&#26126;&#30830;&#30340;&#27979;&#35797;&#38598;&#19978;&#22987;&#32456;&#25552;&#39640;NMT&#27169;&#22411;&#30340;BLEU&#12289;BLEURT&#21644;COMET&#24471;&#20998;&#12290;b&#65289;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#26415;&#35821;&#23450;&#20301;&#24471;&#20998;&#21644;&#20154;&#24037;&#35780;&#20272;&#32780;&#35328;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#65292;&#19982;&#24378;&#25991;&#26412;&#22522;&#32447;&#30456;&#27604;&#12290;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#32763;&#35793;&#27169;&#22411;&#37117;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a large-scale video subtitle translation dataset, BigVideo, to facilitate the study of multi-modality machine translation. Compared with the widely used How2 and VaTeX datasets, BigVideo is more than 10 times larger, consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also introduce two deliberately designed test sets to verify the necessity of visual information: Ambiguous with the presence of ambiguous words, and Unambiguous in which the text context is self-contained for translation. To better model the common semantics shared across texts and videos, we introduce a contrastive learning method in the cross-modal encoder. Extensive experiments on the BigVideo show that: a) Visual information consistently improves the NMT model in terms of BLEU, BLEURT, and COMET on both Ambiguous and Unambiguous test sets. b) Visual information helps disambiguation, compared to the strong text baseline on terminology-targeted scores and human evaluation. Dataset and our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#27169;&#24335;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#29305;&#24449;&#19982;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#19968;&#36215;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#39564;&#65292;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18324</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#22686;&#24378;&#30340;&#39046;&#22495;&#36801;&#31227;&#20027;&#39064;&#20998;&#31867;&#65306;&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain. (arXiv:2305.18324v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#27169;&#24335;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#29305;&#24449;&#19982;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#19968;&#36215;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#39564;&#65292;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#39069;&#22806;&#30340;&#23618;&#36827;&#34892;&#24494;&#35843;&#12290;&#20294;&#24403;&#19979;&#28216;&#39046;&#22495;&#26159;&#19968;&#20010;&#19987;&#19994;&#39046;&#22495;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#36890;&#29992;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#25928;&#26524;&#19981;&#20339;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#27169;&#24335;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#29983;&#20135;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20165;&#22312;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#19978;&#36827;&#34892;&#24494;&#35843;&#30456;&#27604;&#65292;&#36825;&#31181;&#24494;&#35843;&#26041;&#27861;&#25913;&#21892;&#20102;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#27604;&#31616;&#21333;&#30340;&#32447;&#24615;&#23618;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common way to use large pre-trained language models for downstream tasks is to fine tune them using additional layers. This may not work well if downstream domain is a specialized domain whereas the large language model has been pre-trained on a generic corpus. In this paper, we discuss the use of regular expression patterns employed as features for domain knowledge during the process of fine tuning, in addition to domain specific text. Our experiments on real scenario production data show that this method of fine tuning improves the downstream text classification tasks as compared to fine tuning only on domain specific text. We also show that the use of attention network for fine tuning improves results compared to simple linear layers.
&lt;/p&gt;</description></item><item><title>ReWOO&#26159;&#19968;&#31181;&#23558;&#25512;&#29702;&#36807;&#31243;&#19982;&#22806;&#37096;&#35266;&#23519;&#20998;&#31163;&#30340;&#27169;&#22359;&#21270;&#33539;&#24335;&#65292;&#20174;&#32780;&#21487;&#20197;&#20943;&#23569;&#26631;&#35760;&#28040;&#32791;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18323</link><description>&lt;p&gt;
ReWOO&#65306;&#23558;&#25512;&#29702;&#19982;&#35266;&#23519;&#20998;&#31163;&#65292;&#23454;&#29616;&#39640;&#25928;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. (arXiv:2305.18323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18323
&lt;/p&gt;
&lt;p&gt;
ReWOO&#26159;&#19968;&#31181;&#23558;&#25512;&#29702;&#36807;&#31243;&#19982;&#22806;&#37096;&#35266;&#23519;&#20998;&#31163;&#30340;&#27169;&#22359;&#21270;&#33539;&#24335;&#65292;&#20174;&#32780;&#21487;&#20197;&#20943;&#23569;&#26631;&#35760;&#28040;&#32791;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20801;&#35768;&#30693;&#35782;&#26816;&#32034;&#21644;&#34892;&#21160;&#25191;&#34892;&#30340;&#24037;&#20855;&#30456;&#32467;&#21512;&#12290;&#29616;&#26377;&#30340;ALM&#31995;&#32479;&#20197;&#20132;&#38169;&#26041;&#24335;&#35302;&#21457;LLM&#24605;&#32771;&#36807;&#31243;&#65292;&#21516;&#26102;&#20174;&#36825;&#20123;&#24037;&#20855;&#20013;&#25552;&#21462;&#35266;&#23519;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#33539;&#24335;ReWOO&#65288;Without Observation Reasoning&#65289;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#19982;&#22806;&#37096;&#35266;&#23519;&#20998;&#31163;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#26631;&#35760;&#28040;&#32791;&#12290;&#22312;&#20845;&#20010;&#20844;&#20849;NLP&#22522;&#20934;&#27979;&#35797;&#21644;&#19968;&#20010;&#31574;&#21010;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology.
&lt;/p&gt;</description></item><item><title>REFinD&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#22522;&#20110;&#37329;&#34701;&#25991;&#26723;&#29983;&#25104;&#30340;&#20851;&#31995;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#22312;&#37329;&#34701;&#39046;&#22495;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#30340;&#29305;&#23450;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.18322</link><description>&lt;p&gt;
REFinD: &#37329;&#34701;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
REFinD: Relation Extraction Financial Dataset. (arXiv:2305.18322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18322
&lt;/p&gt;
&lt;p&gt;
REFinD&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#22522;&#20110;&#37329;&#34701;&#25991;&#26723;&#29983;&#25104;&#30340;&#20851;&#31995;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#22312;&#37329;&#34701;&#39046;&#22495;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#30340;&#29305;&#23450;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#20102;&#35768;&#22810;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#25628;&#32034;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#34164;&#21547;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26410;&#33021;&#25429;&#25417;&#21040;&#37329;&#34701;&#39046;&#22495;&#30340;&#29305;&#23450;&#25361;&#25112;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#12289;&#22522;&#20110;&#32593;&#32476;&#30340;&#25991;&#26412;&#21644;&#26032;&#38395;&#25991;&#31456;&#31561;&#19968;&#33324;&#30693;&#35782;&#26469;&#28304;&#32534;&#21046;&#65292;&#38459;&#30861;&#20102;&#22312;&#37329;&#34701;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#36827;&#23637;&#21644;&#37319;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REFinD&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#22522;&#20110;&#37329;&#34701;&#25991;&#26723;&#29983;&#25104;&#30340;&#20851;&#31995;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;8&#31181;&#31867;&#22411;&#30340;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;22&#20010;&#20851;&#31995;&#21644;&#32422;29K&#20010;&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19982;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#20316;&#20026;RE&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#24182;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#20851;&#31995;&#21644;&#26041;&#21521;&#27169;&#31946;&#31561;&#26041;&#38754;&#37117;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources such as Wikipedia, web-based text and news articles, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with $\sim$29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34892;&#20026;forma mentis&#32593;&#32476;(BFMNs)&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT-3&#65292;Chat-GPT&#21644;GPT-4)&#23545;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#30340;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#20542;&#21521;&#20110;&#23558;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#35270;&#20026;&#22256;&#38590;&#12289;&#32039;&#24352;&#21644;&#24341;&#36215;&#28966;&#34385;&#12290;</title><link>http://arxiv.org/abs/2305.18320</link><description>&lt;p&gt;
&#35748;&#30693;&#32593;&#32476;&#31185;&#23398;&#25581;&#31034;GPT-3&#12289;ChatGPT&#21644;GPT-4&#20013;&#30340;&#20559;&#35265;&#65306;&#21453;&#26144;&#39640;&#20013;&#23398;&#29983;&#25968;&#23398;&#28966;&#34385;
&lt;/p&gt;
&lt;p&gt;
Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students. (arXiv:2305.18320v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34892;&#20026;forma mentis&#32593;&#32476;(BFMNs)&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT-3&#65292;Chat-GPT&#21644;GPT-4)&#23545;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#30340;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#20542;&#21521;&#20110;&#23558;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#35270;&#20026;&#22256;&#38590;&#12289;&#32039;&#24352;&#21644;&#24341;&#36215;&#28966;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#36880;&#28176;&#34701;&#20837;&#25105;&#20204;&#30340;&#29983;&#27963;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#23427;&#20204;&#36755;&#20986;&#20013;&#30340;&#20559;&#35265;&#26159;&#24456;&#37325;&#35201;&#30340;&#65292;&#20197;&#36991;&#20813;&#25345;&#32493;&#27969;&#20256;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#36825;&#31181;&#25361;&#25112;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#26469;&#37327;&#21270;&#24773;&#24863;&#21644;&#35821;&#20041;&#20559;&#24046;&#65292;&#21516;&#26102;&#38125;&#35760;LLMs&#26159;&#21453;&#26144;&#31038;&#20250;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#35266;&#28857;&#21644;&#20542;&#21521;&#30340;&#24515;&#29702;&#31038;&#20250;&#38236;&#23376;&#12290;&#20854;&#20013;&#19968;&#31181;&#26377;&#23475;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#38738;&#23569;&#24180;&#20840;&#29699;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#30340;&#28966;&#34385;&#29616;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36816;&#29992;&#32593;&#32476;&#31185;&#23398;&#21644;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#34892;&#20026;forma mentis&#32593;&#32476;(BFMNs)&#65292;&#30740;&#31350;&#20102;&#20808;&#36827;&#30340;LLMs&#65292;&#21363;GPT-3&#12289;Chat-GPT&#21644;GPT-4&#65292;&#25552;&#20379;&#30340;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;&#36825;&#20123;LLMs&#22914;&#20309;&#23558;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#19982;&#20854;&#20182;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#25968;&#23398;&#21644;STEM&#30456;&#20851;&#30340;&#25552;&#31034;&#25506;&#27979;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#21457;&#29616;GPT-3&#12289;Chat-GPT&#21644;GPT-4&#37117;&#20542;&#21521;&#20110;&#23558;&#25968;&#23398;&#21644;STEM&#39046;&#22495;&#35270;&#20026;&#22256;&#38590;&#12289;&#32039;&#24352;&#21644;&#24341;&#36215;&#28966;&#34385;&#65292;&#21453;&#26144;&#20102;&#39640;&#20013;&#23398;&#29983;&#25968;&#23398;&#28966;&#34385;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#32771;&#34385;LLMs&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#21033;&#29992;&#32593;&#32476;&#31185;&#23398;&#21644;&#35748;&#30693;&#24515;&#29702;&#23398;&#24320;&#21457;&#37327;&#21270;&#36825;&#20123;&#20559;&#35265;&#30340;&#26032;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly integrated into our lives. Hence, it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes, which originate in our own flawed ways of thinking. This challenge requires developing new benchmarks and methods for quantifying affective and semantic bias, keeping in mind that LLMs act as psycho-social mirrors that reflect the views and tendencies that are prevalent in society. One such tendency that has harmful negative effects is the global phenomenon of anxiety toward math and STEM subjects. Here, we investigate perceptions of math and STEM fields provided by cutting-edge language models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach from network science and cognitive psychology. Specifically, we use behavioral forma mentis networks (BFMNs) to understand how these LLMs frame math and STEM disciplines in relation to other concepts. We use data obtained by probing the thr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#65292;&#23545;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#25688;&#35201;&#32451;&#20064;&#30340;&#31572;&#26696;&#32467;&#26500;&#36827;&#34892;&#21453;&#39304;&#65292;&#35813;&#27169;&#22411;&#22312;&#23398;&#29983;&#25552;&#20132;&#30340;&#21477;&#23376;&#20013;&#23558;&#20854;&#24402;&#20026;&#19977;&#31867;&#65292;&#21363;&#32972;&#26223;&#12289;&#25216;&#26415;&#21644;&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#21270;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2305.18319</link><description>&lt;p&gt;
&#21270;&#23398;&#25968;&#25454;&#24211;&#21644;&#25688;&#35201;&#32451;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Automated Feedback Generation for a Chemistry Database and Abstracting Exercise. (arXiv:2305.18319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#65292;&#23545;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#25688;&#35201;&#32451;&#20064;&#30340;&#31572;&#26696;&#32467;&#26500;&#36827;&#34892;&#21453;&#39304;&#65292;&#35813;&#27169;&#22411;&#22312;&#23398;&#29983;&#25552;&#20132;&#30340;&#21477;&#23376;&#20013;&#23558;&#20854;&#24402;&#20026;&#19977;&#31867;&#65292;&#21363;&#32972;&#26223;&#12289;&#25216;&#26415;&#21644;&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#21270;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#30340;&#21453;&#39304;&#23545;&#20110;&#25945;&#23398;&#21644;&#23398;&#20064;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#29616;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#21464;&#25442;&#22120;&#65288;&#26426;&#22120;&#23398;&#20064;&#65289;&#27169;&#22411;&#65288;BERT&#65289;&#26469;&#23545;&#25688;&#35201;&#32451;&#20064;&#30340;&#31572;&#26696;&#32467;&#26500;&#36827;&#34892;&#21453;&#39304;&#12290;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#65292;&#35201;&#27714;&#23398;&#29983;&#20204;&#20174;&#20986;&#29256;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#19968;&#31687;&#25991;&#31456;&#24182;&#23545;&#20854;&#20869;&#23481;&#36827;&#34892;&#24635;&#32467;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;207&#20010;&#25552;&#20132;&#21697;&#65292;&#24635;&#20849;&#25688;&#35201;&#20102;21&#31687;&#26469;&#33258;&#20027;&#35201;&#25991;&#29486;&#30340;&#25991;&#31456;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65288;&#32422;15,000&#20010;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;80%&#30340;&#24050;&#25552;&#20132;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#36825;&#19968;&#27493;&#39588;&#34987;&#35748;&#20026;&#26159;&#37325;&#35201;&#30340;&#12290;&#23398;&#29983;&#25552;&#20132;&#30340;&#21477;&#23376;&#34987;&#24402;&#20026;&#19977;&#31867;&#8212;&#8212;&#32972;&#26223;&#12289;&#25216;&#26415;&#21644;&#35266;&#23519;&#8212;&#8212;&#36825;&#20351;&#24471;&#21487;&#20197;&#23558;&#27599;&#20010;&#25552;&#20132;&#21697;&#30340;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#23398;&#29983;&#30340;&#25688;&#35201;&#32467;&#26500;&#20197;&#21450;&#26469;&#33258;PubMed&#25968;&#25454;&#24211;&#30340;&#22823;&#37327;&#25688;&#35201;&#65292;&#21487;&#20197;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Timely feedback is an important part of teaching and learning. Here we describe how a readily available neural network transformer (machine-learning) model (BERT) can be used to give feedback on the structure of the response to an abstracting exercise where students are asked to summarise the contents of a published article after finding it from a publication database. The dataset contained 207 submissions from two consecutive years of the course, summarising a total of 21 different papers from the primary literature. The model was pre-trained using an available dataset (approx. 15,000 samples) and then fine-tuned on 80% of the submitted dataset. This fine tuning was seen to be important. The sentences in the student submissions are characterised into three classes - background, technique and observation - which allows a comparison of how each submission is structured. Comparing the structure of the students' abstract a large collection of those from the PubMed database shows that stud
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#20844;&#20849;&#26381;&#21153;&#39046;&#22495;&#20013;&#24212;&#29992;&#25991;&#26412;&#20998;&#26512;&#25216;&#33021;&#20197;&#25903;&#25345;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#30340;&#30740;&#31350;&#65292;&#21457;&#25496;&#20102;TA&#25216;&#26415;&#21644;&#20844;&#20849;&#26381;&#21153;&#30340;&#23545;&#20844;&#20849;&#20215;&#20540;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22312;&#20844;&#20849;&#26381;&#21153;&#39046;&#22495;&#20013;TA&#25216;&#26415;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.18316</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#26512;&#22312;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#20013;&#30340;&#24212;&#29992;&#65306;&#25991;&#29486;&#32508;&#36848;&#21644;&#30740;&#31350;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Application of Text Analytics in Public Service Co-Creation: Literature Review and Research Framework. (arXiv:2305.18316v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#20844;&#20849;&#26381;&#21153;&#39046;&#22495;&#20013;&#24212;&#29992;&#25991;&#26412;&#20998;&#26512;&#25216;&#33021;&#20197;&#25903;&#25345;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#30340;&#30740;&#31350;&#65292;&#21457;&#25496;&#20102;TA&#25216;&#26415;&#21644;&#20844;&#20849;&#26381;&#21153;&#30340;&#23545;&#20844;&#20849;&#20215;&#20540;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22312;&#20844;&#20849;&#26381;&#21153;&#39046;&#22495;&#20013;TA&#25216;&#26415;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#37096;&#38376;&#38754;&#20020;&#30528;&#22810;&#20010;&#25361;&#25112;&#65292;&#20363;&#22914;&#26469;&#33258;&#20869;&#37096;&#22806;&#37096;&#30340;&#38656;&#27714;&#21464;&#21270;&#65292;&#20844;&#27665;&#23545;&#20844;&#20849;&#37096;&#38376;&#32452;&#32455;&#30340;&#19981;&#28385;&#21644;&#25387;&#36133;&#24863;&#65292;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#24471;&#21040;&#35299;&#20915;&#12290;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#26159;&#20256;&#32479;&#33258;&#19978;&#32780;&#19979;&#30340;&#20844;&#20849;&#26381;&#21153;&#24320;&#21457;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#20419;&#36827;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#26088;&#22312;&#21019;&#24314;&#26356;&#22909;&#30340;&#20844;&#20849;&#26381;&#21153;&#24182;&#23454;&#29616;&#20844;&#20849;&#20215;&#20540;&#12290;&#21516;&#26102;&#65292;&#25991;&#26412;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25512;&#21160;&#20102;&#25968;&#25454;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#21327;&#21516;&#21019;&#20316;&#21644;TA&#22312;&#31169;&#33829;&#37096;&#38376;&#20013;&#37117;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20294;&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#30340;&#20851;&#20110;&#24212;&#29992;&#25991;&#26412;&#20998;&#26512;&#25216;&#26415;&#25903;&#25345;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#30340;&#30740;&#31350;&#65292;&#24182;&#23545;975&#31687;&#25991;&#31456;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;TA&#25216;&#26415;&#21644;&#20844;&#20849;&#26381;&#21153;&#23545;&#20844;&#20849;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The public sector faces several challenges, such as a number of external and internal demands for change, citizens' dissatisfaction and frustration with public sector organizations, that need to be addressed. An alternative to the traditional top-down development of public services is co-creation of public services. Co-creation promotes collaboration between stakeholders with the aim to create better public services and achieve public values. At the same time, data analytics has been fuelled by the availability of immense amounts of textual data. Whilst both co-creation and TA have been used in the private sector, we study existing works on the application of Text Analytics (TA) techniques on text data to support public service co-creation. We systematically review 75 of the 979 papers that focus directly or indirectly on the application of TA in the context of public service development. In our review, we analyze the TA techniques, the public service they support, public value outcome
&lt;/p&gt;</description></item><item><title>CDJUR-BR&#26159;&#19968;&#20221;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#21253;&#21547;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#20013;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#65292;&#35813;&#25910;&#34255;&#28085;&#30422;&#21508;&#31181;&#27861;&#24459;&#31243;&#24207;&#25991;&#20214;&#65292;&#24182;&#26377;&#21161;&#20110;&#35299;&#20915;&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26080;&#27861;&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#27861;&#24459;&#23454;&#36341;&#25991;&#26412;&#20013;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18315</link><description>&lt;p&gt;
CDJUR-BR -- &#24102;&#26377;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#30340;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#40644;&#37329;&#25910;&#34255;
&lt;/p&gt;
&lt;p&gt;
CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities. (arXiv:2305.18315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18315
&lt;/p&gt;
&lt;p&gt;
CDJUR-BR&#26159;&#19968;&#20221;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#21253;&#21547;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#20013;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#65292;&#35813;&#25910;&#34255;&#28085;&#30422;&#21508;&#31181;&#27861;&#24459;&#31243;&#24207;&#25991;&#20214;&#65292;&#24182;&#26377;&#21161;&#20110;&#35299;&#20915;&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26080;&#27861;&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#27861;&#24459;&#23454;&#36341;&#25991;&#26412;&#20013;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22810;&#25968;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#65288;Legal AI&#65289;&#24212;&#29992;&#31243;&#24207;&#32780;&#35328;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27861;&#24459;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#25991;&#26412;&#28041;&#21450;&#21040;&#30340;&#23454;&#20307;&#24182;&#38750;&#24403;&#21069;&#21487;&#29992;&#30340;NER&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#12290;&#32570;&#20047;&#27861;&#35268;&#12289;&#21028;&#20363;&#12289;&#35777;&#25454;&#12289;&#24809;&#32602;&#12289;&#27861;&#24459;&#31243;&#24207;&#20013;&#20154;&#20204;&#30340;&#35282;&#33394;&#65288;&#27861;&#23448;&#12289;&#24459;&#24072;&#12289;&#21463;&#23475;&#32773;&#12289;&#34987;&#21578;&#12289;&#35777;&#20154;&#65289;&#12289;&#20301;&#32622;&#31867;&#22411;&#65288;&#29359;&#32618;&#22320;&#28857;&#12289;&#34987;&#21578;&#22320;&#22336;&#65289;&#31561;&#30340;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#20173;&#38656;&#35201;&#19968;&#20010;&#29992;&#27861;&#24459;&#39046;&#22495;&#30340;&#31934;&#32454;&#23454;&#20307;&#36827;&#34892;&#27880;&#37322;&#30340;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#28085;&#30422;&#27861;&#24459;&#31243;&#24207;&#30340;&#21508;&#31181;&#25991;&#20214;&#65292;&#20363;&#22914;&#35831;&#24895;&#20070;&#12289;&#35843;&#26597;&#12289;&#25237;&#35785;&#12289;&#20915;&#23450;&#21644;&#21028;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24052;&#35199;&#21496;&#27861;&#40644;&#37329;&#25910;&#34255;&#65288;CDJUR-BR&#65289;&#30340;&#24320;&#21457;&#65292;&#35813;&#25910;&#34255;&#21253;&#21547;&#19968;&#32452;&#30001;&#27861;&#24459;&#25991;&#29486;&#19987;&#23478;&#27880;&#37322;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#12290;&#21019;&#24314;CDJUR-BR&#36981;&#24490;&#20102;&#33258;&#24049;&#30340;
&lt;/p&gt;
&lt;p&gt;
A basic task for most Legal Artificial Intelligence (Legal AI) applications is Named Entity Recognition (NER). However, texts produced in the context of legal practice make references to entities that are not trivially recognized by the currently available NERs. There is a lack of categorization of legislation, jurisprudence, evidence, penalties, the roles of people in a legal process (judge, lawyer, victim, defendant, witness), types of locations (crime location, defendant's address), etc. In this sense, there is still a need for a robust golden collection, annotated with fine-grained entities of the legal domain, and which covers various documents of a legal process, such as petitions, inquiries, complaints, decisions and sentences. In this article, we describe the development of the Golden Collection of the Brazilian Judiciary (CDJUR-BR) contemplating a set of fine-grained named entities that have been annotated by experts in legal documents. The creation of CDJUR-BR followed its ow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;(CAT)&#26694;&#26550;&#30340;&#32422;&#26463;&#29256;&#26412;C-BOBCAT&#65292;&#36890;&#36807;&#26435;&#34913;&#27979;&#35797;&#20934;&#30830;&#24615;&#21644;&#38382;&#39064;&#26292;&#38706;&#29575;&#21450;&#27979;&#35797;&#37325;&#21472;&#29575;&#65292;&#35299;&#20915;&#20102;BOBCAT&#23384;&#22312;&#30340;&#39640;&#38382;&#39064;&#26292;&#38706;&#29575;&#21644;&#27979;&#35797;&#37325;&#21472;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18312</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#20013;&#24179;&#34913;&#27979;&#35797;&#20934;&#30830;&#24615;&#19982;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Balancing Test Accuracy and Security in Computerized Adaptive Testing. (arXiv:2305.18312v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;(CAT)&#26694;&#26550;&#30340;&#32422;&#26463;&#29256;&#26412;C-BOBCAT&#65292;&#36890;&#36807;&#26435;&#34913;&#27979;&#35797;&#20934;&#30830;&#24615;&#21644;&#38382;&#39064;&#26292;&#38706;&#29575;&#21450;&#27979;&#35797;&#37325;&#21472;&#29575;&#65292;&#35299;&#20915;&#20102;BOBCAT&#23384;&#22312;&#30340;&#39640;&#38382;&#39064;&#26292;&#38706;&#29575;&#21644;&#27979;&#35797;&#37325;&#21472;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;(CAT)&#26159;&#19968;&#31181;&#21487;&#20197;&#20934;&#30830;&#27979;&#37327;&#23398;&#29983;&#30693;&#35782;&#27700;&#24179;&#19988;&#32553;&#30701;&#27979;&#35797;&#26102;&#38388;&#30340;&#20010;&#24615;&#21270;&#27979;&#35797;&#24418;&#24335;&#12290;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;CAT(BOBCAT)&#26159;&#19968;&#20010;&#26368;&#36817;&#30340;&#26694;&#26550;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#32553;&#30701;&#20102;&#27979;&#35797;&#26102;&#38388;&#24182;&#25552;&#39640;&#20102;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#23384;&#22312;&#39640;&#38382;&#39064;&#26292;&#38706;&#29575;&#21644;&#27979;&#35797;&#37325;&#21472;&#29575;&#30340;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#27979;&#35797;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BOBCAT&#30340;&#19968;&#31181;&#32422;&#26463;&#29256;&#26412;&#65292;&#36890;&#36807;&#26356;&#25913;&#20854;&#20248;&#21270;&#35774;&#32622;&#20351;&#25105;&#20204;&#33021;&#22815;&#26435;&#34913;&#27979;&#35797;&#20934;&#30830;&#24615;&#21644;&#38382;&#39064;&#26292;&#38706;&#29575;&#21450;&#27979;&#35797;&#37325;&#21472;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;&#25104;&#20154;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computerized adaptive testing (CAT) is a form of personalized testing that accurately measures students' knowledge levels while reducing test length. Bilevel optimization-based CAT (BOBCAT) is a recent framework that learns a data-driven question selection algorithm to effectively reduce test length and improve test accuracy. However, it suffers from high question exposure and test overlap rates, which potentially affects test security. This paper introduces a constrained version of BOBCAT to address these problems by changing its optimization setup and enabling us to trade off test accuracy for question exposure and test overlap rates. We show that C-BOBCAT is effective through extensive experiments on two real-world adult testing datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35748;&#35777;&#26631;&#31614;&#22312;&#20256;&#36798;&#21487;&#20449;AI&#19978;&#30340;&#20316;&#29992;&#12290;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;&#35748;&#35777;&#26631;&#31614;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#32456;&#31471;&#29992;&#25143;&#22312;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#22330;&#26223;&#19979;&#20351;&#29992;AI&#30340;&#20449;&#20219;&#21644;&#20351;&#29992;&#24847;&#24895;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#20854;&#25928;&#26524;&#26356;&#20026;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2305.18307</link><description>&lt;p&gt;
&#21487;&#20449;AI&#35748;&#35777;&#26631;&#31614;&#65306;&#22522;&#20110;&#32463;&#39564;&#26434;&#20132;&#26041;&#27861;&#30740;&#31350;&#30340;&#28145;&#21051;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study. (arXiv:2305.18307v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35748;&#35777;&#26631;&#31614;&#22312;&#20256;&#36798;&#21487;&#20449;AI&#19978;&#30340;&#20316;&#29992;&#12290;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;&#35748;&#35777;&#26631;&#31614;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#32456;&#31471;&#29992;&#25143;&#22312;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#22330;&#26223;&#19979;&#20351;&#29992;AI&#30340;&#20449;&#20219;&#21644;&#20351;&#29992;&#24847;&#24895;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#20854;&#25928;&#26524;&#26356;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23457;&#35745;&#22312;&#21487;&#20449;AI&#30340;&#21457;&#23637;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21019;&#24314;&#21487;&#23457;&#35745;&#30340;AI&#25991;&#20214;&#26041;&#38754;&#65292;&#36825;&#20123;&#25991;&#20214;&#26159;&#38024;&#23545;&#30417;&#31649;&#26426;&#26500;&#21644;&#19987;&#23478;&#32780;&#19981;&#26159;&#21463;AI&#20915;&#31574;&#24433;&#21709;&#30340;&#32456;&#31471;&#29992;&#25143;&#32780;&#35774;&#35745;&#30340;&#12290;&#22914;&#20309;&#21521;&#20844;&#20247;&#20256;&#36798;&#19968;&#20010;&#32463;&#36807;&#23457;&#35745;&#24182;&#34987;&#35270;&#20026;&#21487;&#20449;&#30340;AI&#30340;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#23454;&#35777;&#35843;&#26597;&#20102;&#35748;&#35777;&#26631;&#31614;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35775;&#35848;&#65288;N = 12&#65289;&#21644;&#26222;&#26597;&#20195;&#34920;&#24615;&#35843;&#26597;&#65288;N = 302&#65289;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32456;&#31471;&#29992;&#25143;&#23545;&#35748;&#35777;&#26631;&#31614;&#30340;&#24577;&#24230;&#20197;&#21450;&#22312;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;AI&#22330;&#26223;&#20013;&#26377;&#25928;&#20256;&#36798;&#21487;&#20449;&#24615;&#30340;&#33021;&#21147;&#12290;&#26681;&#25454;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#26631;&#31614;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#32456;&#31471;&#29992;&#25143;&#22312;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#22330;&#26223;&#19979;&#20351;&#29992;AI&#30340;&#20449;&#20219;&#21644;&#20351;&#29992;&#24847;&#24895;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#32456;&#31471;&#29992;&#25143;&#23545;&#35748;&#35777;&#26631;&#31614;&#30340;&#20559;&#22909;&#21450;&#20854;&#23545;&#20449;&#20219;&#21644;&#20351;&#29992;AI&#30340;&#24433;&#21709;&#26356;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users' attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users' trust and willingness to use AI in both lowand high-stakes scenarios. However, end-users' preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake sce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#35270;&#35282;&#20132;&#20114;&#20027;&#39064;&#22238;&#24402;&#31639;&#27861;&#65288;MV-ICTR&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#21516;&#26102;&#32435;&#20837;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#29289;&#21697;&#29305;&#23450;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#30340;&#20010;&#20154;&#20559;&#22909;&#65292;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#31574;&#30053;&#36827;&#34892;&#25345;&#32493;&#30340;&#22312;&#32447;&#20010;&#24615;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18306</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20132;&#20114;&#24335;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Multi-View Interactive Collaborative Filtering. (arXiv:2305.18306v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#35270;&#35282;&#20132;&#20114;&#20027;&#39064;&#22238;&#24402;&#31639;&#27861;&#65288;MV-ICTR&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#21516;&#26102;&#32435;&#20837;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#29289;&#21697;&#29305;&#23450;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#30340;&#20010;&#20154;&#20559;&#22909;&#65292;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#31574;&#30053;&#36827;&#34892;&#25345;&#32493;&#30340;&#22312;&#32447;&#20010;&#24615;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65288;&#22914;&#28857;&#20987;&#25110;&#35780;&#20998;&#65289;&#24448;&#24448;&#24456;&#23569;&#65292;&#29289;&#21697;&#30340;&#25442;&#25163;&#29575;&#65288;&#20363;&#22914;&#26032;&#25991;&#31456;&#12289;&#25307;&#32856;&#20449;&#24687;&#65289;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#29992;&#25143;-&#29289;&#21697;&#35780;&#20998;&#22806;&#65292;&#38598;&#25104;&#19978;&#19979;&#25991;&#8220;&#36793;&#8221;&#20449;&#24687;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#12290;&#34429;&#28982;&#23384;&#22312;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20165;&#33021;&#36827;&#34892;&#26679;&#26412;&#20869;&#25512;&#33616;&#65292;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#38480;&#21046;&#65292;&#24182;&#19981;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#31574;&#30053;&#36827;&#34892;&#38271;&#26399;&#32047;&#31215;&#25910;&#30410;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#20132;&#20114;&#20027;&#39064;&#22238;&#24402;&#65288;MV-ICTR&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#37096;&#20998;&#22312;&#32447;&#28508;&#22312;&#22240;&#23376;&#25512;&#33616;&#31639;&#27861;&#65292;&#21516;&#26102;&#32435;&#20837;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#29289;&#21697;&#29305;&#23450;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#30340;&#20010;&#20154;&#20559;&#22909;&#65292;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#31574;&#30053;&#36827;&#34892;&#25345;&#32493;&#22312;&#32447;&#20010;&#24615;&#21270;&#12290;&#35813;&#31639;&#27861;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, recommender system user interaction data such as clicks or ratings is sparse, and item turnover rates (e.g., new articles, job postings) high. Given this, the integration of contextual "side" information in addition to user-item ratings is highly desirable. Whilst there are algorithms that can handle both rating and contextual data simultaneously, these algorithms are typically limited to making only in-sample recommendations, suffer from the curse of dimensionality, and do not incorporate multi-armed bandit (MAB) policies for long-term cumulative reward optimization. We propose multi-view interactive topic regression (MV-ICTR) a novel partially online latent factor recommender algorithm that incorporates both rating and contextual information to model item-specific feature dependencies and users' personal preferences simultaneously, with multi-armed bandit policies for continued online personalization. The result is significantly increased performance on datasets wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#21644;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25913;&#21892;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#38656;&#35201;&#25919;&#24220;&#12289;&#31169;&#33829;&#37096;&#38376;&#12289;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#22269;&#38469;&#32452;&#32455;&#21327;&#35843;&#19968;&#33268;&#22320;&#21162;&#21147;&#65292;&#21019;&#24314;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#28385;&#36275;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#29420;&#29305;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.18302</link><description>&lt;p&gt;
&#30446;&#21069;&#20026;&#27490;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
What We Know So Far: Artificial Intelligence in African Healthcare. (arXiv:2305.18302v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#21644;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25913;&#21892;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#38656;&#35201;&#25919;&#24220;&#12289;&#31169;&#33829;&#37096;&#38376;&#12289;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#22269;&#38469;&#32452;&#32455;&#21327;&#35843;&#19968;&#33268;&#22320;&#21162;&#21147;&#65292;&#21019;&#24314;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#28385;&#36275;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#29420;&#29305;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#30340;&#21307;&#30103;&#20445;&#20581;&#38382;&#39064;&#21463;&#21040;&#35768;&#22810;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#36139;&#22256;&#12289;&#22522;&#30784;&#35774;&#26045;&#32570;&#20047;&#21644;&#36164;&#37329;&#19981;&#36275;&#31561;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20855;&#26377;&#28508;&#21147;&#36890;&#36807;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12289;&#20351;&#30142;&#30149;&#26356;&#26089;&#22320;&#34987;&#21457;&#29616;&#12289;&#25903;&#25345;&#20010;&#24615;&#21270;&#33647;&#29289;&#30340;&#21457;&#24067;&#26469;&#25913;&#21464;&#38750;&#27954;&#30340;&#21307;&#30103;&#20445;&#20581;&#29366;&#20917;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#30446;&#21069;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#25913;&#21892;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#30142;&#30149;&#30417;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25913;&#21892;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#25919;&#24220;&#12289;&#31169;&#33829;&#37096;&#38376;&#12289;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#22269;&#38469;&#32452;&#32455;&#21327;&#35843;&#19968;&#33268;&#22320;&#21162;&#21147;&#65292;&#21019;&#24314;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#28385;&#36275;&#38750;&#27954;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#29420;&#29305;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare in Africa is a complex issue influenced by many factors including poverty, lack of infrastructure, and inadequate funding. However, Artificial intelligence (AI) applied to healthcare, has the potential to transform healthcare in Africa by improving the accuracy and efficiency of diagnosis, enabling earlier detection of diseases, and supporting the delivery of personalized medicine. This paper reviews the current state of how AI Algorithms can be used to improve diagnostics, treatment, and disease monitoring, as well as how AI can be used to improve access to healthcare in Africa as a low-resource setting and discusses some of the critical challenges and opportunities for its adoption. As such, there is a need for a well-coordinated effort by the governments, private sector, healthcare providers, and international organizations to create sustainable AI solutions that meet the unique needs of the African healthcare system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21629;&#21517;&#20026;OOD&#35821;&#20041;&#20462;&#21098;&#65288;OSP&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#20174;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#29305;&#24449;&#20013;&#20462;&#21098;&#25481;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18158</link><description>&lt;p&gt;
&#40065;&#26834;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#24067;&#22806;&#35821;&#20041;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning. (arXiv:2305.18158v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21629;&#21517;&#20026;OOD&#35821;&#20041;&#20462;&#21098;&#65288;OSP&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#20174;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#29305;&#24449;&#20013;&#20462;&#21098;&#25481;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#40065;&#26834;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20013;&#30340;&#36827;&#23637;&#36890;&#24120;&#22312;&#26679;&#26412;&#32423;&#21035;&#36807;&#28388;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#20449;&#24687;&#12290;&#25105;&#20204;&#35748;&#20026;&#40065;&#26834;SSL&#30340;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#26159;&#20854;&#22312;&#35821;&#20041;&#32423;&#21035;&#19978;&#30340;&#25439;&#22351;&#20449;&#24687;&#65292;&#23454;&#38469;&#19978;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#21021;&#27493;&#25514;&#26045;&#65292;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;OOD&#35821;&#20041;&#20462;&#21098;&#65288;OSP&#65289;&#65292;&#26088;&#22312;&#20174;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#29305;&#24449;&#20013;&#20462;&#21098;OOD&#35821;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;(i)&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21035;&#21517;OOD&#21305;&#37197;&#27169;&#22359;&#65292;&#23558;&#27599;&#20010;ID&#26679;&#26412;&#19982;&#19968;&#20010;OOD&#26679;&#26412;&#36827;&#34892;&#35821;&#20041;&#37325;&#21472;&#21305;&#37197;&#12290;(ii)&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36719;&#27491;&#20132;&#27491;&#21017;&#21270;&#65292;&#23427;&#39318;&#20808;&#36890;&#36807;&#25233;&#21046;&#19982;&#37197;&#23545;OOD&#26679;&#26412;&#20849;&#32447;&#30340;&#35821;&#20041;&#25104;&#20998;&#26469;&#36716;&#25442;&#27599;&#20010;ID&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23427;&#24378;&#21046;&#22312;&#36719;&#27491;&#20132;&#20998;&#35299;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#39044;&#27979;&#32467;&#26524;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#36827;&#34892;OOD&#26816;&#27979;&#21644;ID&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in robust semi-supervised learning (SSL) typically filter out-of-distribution (OOD) information at the sample level. We argue that an overlooked problem of robust SSL is its corrupted information on semantic level, practically limiting the development of the field. In this paper, we take an initial step to explore and propose a unified framework termed OOD Semantic Pruning (OSP), which aims at pruning OOD semantics out from in-distribution (ID) features. Specifically, (i) we propose an aliasing OOD matching module to pair each ID sample with an OOD sample with semantic overlap. (ii) We design a soft orthogonality regularization, which first transforms each ID feature by suppressing its semantic component that is collinear with paired OOD sample. It then forces the predictions before and after soft orthogonality decomposition to be consistent. Being practically simple, our method shows a strong performance in OOD detection and ID classification on challenging benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#38169;&#35273;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;InDL&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#33021;&#21147;&#12290;&#21033;&#29992;&#20960;&#20309;&#20809;&#23398;&#35270;&#38169;&#35273;&#65292;&#24314;&#31435;&#21487;&#27604;&#24615;&#26694;&#26550;&#29992;&#20110;&#38416;&#26126;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#21644;&#25552;&#20379;&#25913;&#36827;&#27169;&#22411;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17716</link><description>&lt;p&gt;
InDL: &#22522;&#20110;&#35270;&#38169;&#35273;&#30340;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#26032;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion. (arXiv:2305.17716v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#38169;&#35273;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;InDL&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#33021;&#21147;&#12290;&#21033;&#29992;&#20960;&#20309;&#20809;&#23398;&#35270;&#38169;&#35273;&#65292;&#24314;&#31435;&#21487;&#27604;&#24615;&#26694;&#26550;&#29992;&#20110;&#38416;&#26126;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#21644;&#25552;&#20379;&#25913;&#36827;&#27169;&#22411;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#36259;&#39046;&#22495;&#30340;&#35270;&#38169;&#35273;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;InDL&#65292;&#26088;&#22312;&#20005;&#26684;&#27979;&#35797;&#21644;&#22522;&#20934;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#20845;&#20010;&#32463;&#20856;&#30340;&#20960;&#20309;&#35270;&#35273;&#38169;&#35273;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#20154;&#31867;&#21644;&#26426;&#22120;&#35270;&#35273;&#24863;&#30693;&#30340;&#21487;&#27604;&#24615;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#37327;&#21270;&#30340;&#34913;&#37327;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#34892;&#21160;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to evaluating deep learning models' capacity for in-diagram logic interpretation. Leveraging the intriguing realm of visual illusions, we establish a unique dataset, InDL, designed to rigorously test and benchmark these models. Deep learning has witnessed remarkable progress in domains such as computer vision and natural language processing. However, models often stumble in tasks requiring logical reasoning due to their inherent 'black box' characteristics, which obscure the decision-making process. Our work presents a new lens to understand these models better by focusing on their handling of visual illusions -- a complex interplay of perception and logic. We utilize six classic geometric optical illusions to create a comparative framework between human and machine visual perception. This methodology offers a quantifiable measure to rank models, elucidating potential weaknesses and providing actionable insights for model improvements. Our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24403;&#39044;&#27979;&#21487;&#20197;&#24433;&#21709;&#32467;&#26524;&#26102;&#22914;&#20309;&#21033;&#29992;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#28608;&#21169;&#19987;&#23478;&#20316;&#20986;&#20934;&#30830;&#21453;&#26144;&#20449;&#24565;&#30340;&#39044;&#27979;&#65292;&#24182;&#32473;&#20986;&#20102;&#38024;&#23545;&#20108;&#20803;&#39044;&#27979;&#30340;&#35780;&#20998;&#35268;&#21017;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.17601</link><description>&lt;p&gt;
&#21033;&#29992;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#28608;&#21169;&#35802;&#23454;&#30340;&#34920;&#29616;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Incentivizing honest performative predictions with proper scoring rules. (arXiv:2305.17601v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24403;&#39044;&#27979;&#21487;&#20197;&#24433;&#21709;&#32467;&#26524;&#26102;&#22914;&#20309;&#21033;&#29992;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#28608;&#21169;&#19987;&#23478;&#20316;&#20986;&#20934;&#30830;&#21453;&#26144;&#20449;&#24565;&#30340;&#39044;&#27979;&#65292;&#24182;&#32473;&#20986;&#20102;&#38024;&#23545;&#20108;&#20803;&#39044;&#27979;&#30340;&#35780;&#20998;&#35268;&#21017;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#21487;&#20197;&#28608;&#21169;&#19987;&#23478;&#20934;&#30830;&#25253;&#21578;&#20449;&#24565;&#65292;&#20294;&#25105;&#20204;&#25918;&#23485;&#20102;&#27492;&#20551;&#35774;&#65292;&#30740;&#31350;&#20102;&#24403;&#39044;&#27979;&#21487;&#20197;&#24433;&#21709;&#32467;&#26524;&#26102;&#65288;&#20363;&#22914;&#22312;&#32929;&#24066;&#19978;&#20844;&#24320;&#39044;&#27979;&#26102;&#65289;&#65292;&#39044;&#27979;&#23545;&#19987;&#23478;&#20449;&#24565;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27492;&#35774;&#23450;&#19979;&#65292;&#26368;&#22823;&#21270;&#39044;&#26399;&#24471;&#20998;&#30340;&#25253;&#21578;&#36890;&#24120;&#19981;&#21453;&#26144;&#19987;&#23478;&#30340;&#20449;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#36825;&#20123;&#25253;&#21578;&#30340;&#19981;&#20934;&#30830;&#24230;&#30028;&#38480;&#12290;&#23545;&#20110;&#20108;&#20803;&#39044;&#27979;&#65292;&#22914;&#26524;&#19987;&#23478;&#39044;&#27979;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#26377;&#30028;&#65292;&#21487;&#20197;&#23450;&#20041;&#35780;&#20998;&#35268;&#21017;&#65292;&#20351;&#24471;&#26368;&#20248;&#25253;&#21578;&#19982;&#22266;&#23450;&#28857;&#38750;&#24120;&#25509;&#36817;&#12290;&#20294;&#23545;&#20110;&#36229;&#36807;&#20004;&#20010;&#32467;&#26524;&#30340;&#39044;&#27979;&#65292;&#36825;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#36824;&#22312;&#29609;&#20855;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#65292;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proper scoring rules incentivize experts to accurately report beliefs, assuming predictions cannot influence outcomes. We relax this assumption and investigate incentives when predictions are performative, i.e., when they can influence the outcome of the prediction, such as when making public predictions about the stock market. We say a prediction is a fixed point if it accurately reflects the expert's beliefs after that prediction has been made. We show that in this setting, reports maximizing expected score generally do not reflect an expert's beliefs, and we give bounds on the inaccuracy of such reports. We show that, for binary predictions, if the influence of the expert's prediction on outcomes is bounded, it is possible to define scoring rules under which optimal reports are arbitrarily close to fixed points. However, this is impossible for predictions over more than two outcomes. We also perform numerical simulations in a toy setting, showing that our bounds are tight in some si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#26816;&#27979;&#22840;&#24352;&#21644;&#38544;&#21947;&#12290;&#20351;&#29992;&#38544;&#21947;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#22840;&#24352;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22840;&#24352;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#38544;&#21947;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26816;&#27979;&#22840;&#24352;&#30340;&#24615;&#33021;&#27604;&#20808;&#21069;&#26041;&#27861;&#36827;&#27493;&#20102;12%&#12290;&#27492;&#22806;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;17%&#12290;</title><link>http://arxiv.org/abs/2305.17480</link><description>&lt;p&gt;
&#19968;&#26729;&#22825;&#20316;&#20043;&#21512;&#65306;&#29992;&#20110;&#22840;&#24352;&#21644;&#38544;&#21947;&#26816;&#27979;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection. (arXiv:2305.17480v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#26816;&#27979;&#22840;&#24352;&#21644;&#38544;&#21947;&#12290;&#20351;&#29992;&#38544;&#21947;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#22840;&#24352;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22840;&#24352;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#38544;&#21947;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26816;&#27979;&#22840;&#24352;&#30340;&#24615;&#33021;&#27604;&#20808;&#21069;&#26041;&#27861;&#36827;&#27493;&#20102;12%&#12290;&#27492;&#22806;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;17%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22840;&#24352;&#21644;&#38544;&#21947;&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#24456;&#24120;&#35265;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#38519;&#20837;&#20102;&#40635;&#28902;&#20043;&#20013;&#8221;&#65306;&#40635;&#28902;&#24590;&#20040;&#21487;&#33021;&#26377;&#28145;&#24230;&#65311;&#65289;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#26816;&#27979;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35805; AI &#35774;&#32622;&#20013;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#26816;&#27979;&#22840;&#24352;&#21644;&#38544;&#21947;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#30740;&#31350;&#20102;&#36825;&#20123;&#35821;&#35328;&#29616;&#35937;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20174;&#26410;&#34987;&#35745;&#31639;&#21270;&#25506;&#32034;&#36807;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#26816;&#27979;&#22840;&#24352;&#21644;&#38544;&#21947;&#12290;&#25105;&#20204;&#20551;&#35774;&#38544;&#21947;&#26377;&#21161;&#20110;&#22840;&#24352;&#26816;&#27979;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#20026;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#20351;&#29992;&#38544;&#21947;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#22840;&#24352;&#25968;&#25454;&#38598;-HYPO &#21644; HYPO-L-&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#22840;&#24352;&#26631;&#31614;&#27880;&#37322;&#20102;&#20004;&#20010;&#38544;&#21947;&#25968;&#25454;&#38598;-TroFi &#21644; LCC&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20808;&#21069;&#30340;&#22840;&#24352;&#26816;&#27979;&#26041;&#27861;&#36827;&#27493;&#20102; 12%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#30456;&#23545;&#20110;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798; 17%&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbole and metaphor are common in day-to-day communication (e.g., "I am in deep trouble": how does trouble have depth?), which makes their detection important, especially in a conversational AI setting. Existing approaches to automatically detect metaphor and hyperbole have studied these language phenomena independently, but their relationship has hardly, if ever, been explored computationally. In this paper, we propose a multi-task deep learning framework to detect hyperbole and metaphor simultaneously. We hypothesize that metaphors help in hyperbole detection, and vice-versa. To test this hypothesis, we annotate two hyperbole datasets- HYPO and HYPO-L- with metaphor labels. Simultaneously, we annotate two metaphor datasets- TroFi and LCC- with hyperbole labels. Experiments using these datasets give an improvement of the state of the art of hyperbole detection by 12%. Additionally, our multi-task learning (MTL) approach shows an improvement of up to 17% over single-task learning (S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>mldr.resampling&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#26088;&#22312;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17152</link><description>&lt;p&gt;
mldr.resampling: &#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#31639;&#27861;&#26377;&#25928;&#30340;&#21442;&#32771;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms. (arXiv:2305.17152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17152
&lt;/p&gt;
&lt;p&gt;
mldr.resampling&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#26088;&#22312;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#37319;&#26679;&#31639;&#27861;&#26159;&#24212;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#24773;&#20917;&#30340;&#26377;&#29992;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#24517;&#39035;&#22788;&#29702;&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#30340;&#22855;&#24322;&#24615;&#65292;&#20363;&#22914;&#21516;&#19968;&#23454;&#20363;&#20013;&#39057;&#32321;&#21644;&#19981;&#39057;&#32321;&#26631;&#31614;&#30340;&#20986;&#29616;&#12290;&#36825;&#31687;&#21407;&#21019;&#36719;&#20214;&#21457;&#34920;&#20171;&#32461;&#20102; mldr.resampling&#65292;&#36825;&#26159;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;&#20102;11&#31181;&#22810;&#26631;&#31614;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#24378;&#35843;&#25928;&#29575;&#65292;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#32791;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resampling algorithms are a useful approach to deal with imbalanced learning in multilabel scenarios. These methods have to deal with singularities in the multilabel data, such as the occurrence of frequent and infrequent labels in the same instance. Implementations of these methods are sometimes limited to the pseudocode provided by their authors in a paper. This Original Software Publication presents mldr.resampling, a software package that provides reference implementations for eleven multilabel resampling methods, with an emphasis on efficiency since these algorithms are usually time-consuming.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Retrieval Augmentation&#65288;RetA&#65289;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;OpenAI&#30340;GPT-3&#12289;GPT-4&#12289;Bing&#30340;Prometheus&#20197;&#21450;&#23450;&#21046;&#30340;RetA&#27169;&#22411;&#22312;&#22238;&#31572;19&#20010;&#24357;&#28459;&#22823;B&#32454;&#32990;&#28107;&#24052;&#30244;&#65288;DLBCL&#65289;&#30142;&#30149;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;RetA&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.17116</link><description>&lt;p&gt;
&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;GPT-3/4&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model. (arXiv:2305.17116v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Retrieval Augmentation&#65288;RetA&#65289;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;OpenAI&#30340;GPT-3&#12289;GPT-4&#12289;Bing&#30340;Prometheus&#20197;&#21450;&#23450;&#21046;&#30340;RetA&#27169;&#22411;&#22312;&#22238;&#31572;19&#20010;&#24357;&#28459;&#22823;B&#32454;&#32990;&#28107;&#24052;&#30244;&#65288;DLBCL&#65289;&#30142;&#30149;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;RetA&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#24191;&#27867;&#30340;&#35821;&#26009;&#24211;&#25429;&#33719;&#20102;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#20294;&#20063;&#21487;&#33021;&#24341;&#20837;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#32780;&#19987;&#27880;&#30340;&#35821;&#26009;&#24211;&#36890;&#36807;&#20943;&#23569;&#35823;&#23548;&#24615;&#20449;&#24687;&#26469;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;Retrieval Augmentation&#65288;RetA&#65289;&#26041;&#27861;&#26159;&#20351;&#29992;&#19987;&#27880;&#35821;&#26009;&#24211;&#35757;&#32451;LLM&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#38024;&#23545;&#24357;&#28459;&#22823;B&#32454;&#32990;&#28107;&#24052;&#30244;&#65288;DLBCL&#65289;&#30142;&#30149;&#25552;&#20986;&#20102;19&#20010;&#38382;&#39064;&#24182;&#27604;&#36739;&#20102;OpenAI&#30340;GPT-3&#12289;GPT-4&#12289;Bing&#30340;Prometheus&#20197;&#21450;&#23450;&#21046;&#30340;RetA&#27169;&#22411;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290; 8&#21517;&#29420;&#31435;&#30340;&#35780;&#23457;&#26681;&#25454;&#20934;&#30830;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#65288;&#35780;&#20998;1-3&#65289;&#23545;&#22238;&#31572;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;RetA&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#65288;19&#39033;&#20013;12&#39033;&#33719;&#24471;3&#20998;&#65292;&#24635;&#35745;47&#20998;&#65289;&#21644;&#30456;&#20851;&#24615;&#65288;19&#39033;&#20013;13&#39033;&#65292;50&#20998;&#65289;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#27425;&#26159;GPT-4&#65288;19&#39033;&#20013;8&#39033;&#65292;43&#20998;&#65307;11&#39033;&#20013;49&#20998;&#65289;&#12290;GPT-4&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#21487;&#35835;&#24615;&#35780;&#20998;&#65288;19&#39033;&#20013;17&#39033;&#65292;55&#20998;&#65289;&#65292;&#20854;&#27425;&#26159;GPT-3&#65288;19&#39033;&#20013;15&#39033;&#65292;53&#20998;&#65289;&#21644;RetA&#27169;&#22411;&#65288;19&#39033;&#20013;11&#39033;&#65292;47&#20998;&#65289;&#12290;Prometheus&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in natural language processing (NLP). Broad corpora capture diverse patterns but can introduce irrelevance, while focused corpora enhance reliability by reducing misleading information. Training LLMs on focused corpora poses computational challenges. An alternative approach is to use a retrieval-augmentation (RetA) method tested in a specific domain.  To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and a custom RetA model were compared using 19 questions on diffuse large B-cell lymphoma (DLBCL) disease. Eight independent reviewers assessed responses based on accuracy, relevance, and readability (rated 1-3).  The RetA model performed best in accuracy (12/19 3-point scores, total=47) and relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4 received the highest readability scores (17/19, 55), followed by GPT-3 (15/19, 53) and the RetA model (11/19, 47). Prometheus underperformed in accu
&lt;/p&gt;</description></item><item><title>&#22312;Lace&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#21644;&#32422;&#26463;&#35782;&#21035;&#34920;&#31034;&#21516;&#19968;&#23454;&#20307;&#30340;&#23454;&#20307;&#24341;&#29992;&#23545;&#65292;&#35813;&#35782;&#21035;&#32467;&#21512;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#21512;&#24182;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.16926</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#36923;&#36753;&#30340;&#23454;&#20307;&#28040;&#35299;&#20013;&#32467;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Combining Global and Local Merges in Logic-based Entity Resolution. (arXiv:2305.16926v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16926
&lt;/p&gt;
&lt;p&gt;
&#22312;Lace&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#21644;&#32422;&#26463;&#35782;&#21035;&#34920;&#31034;&#21516;&#19968;&#23454;&#20307;&#30340;&#23454;&#20307;&#24341;&#29992;&#23545;&#65292;&#35813;&#35782;&#21035;&#32467;&#21512;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#21512;&#24182;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#38598;&#20307;&#23454;&#20307;&#28040;&#35299;&#26694;&#26550;Lace&#20013;&#65292;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#21644;&#32422;&#26463;&#26469;&#35782;&#21035;&#34920;&#31034;&#21516;&#19968;&#23454;&#20307;&#30340;&#23454;&#20307;&#24341;&#29992;&#23545;&#65288;&#20363;&#22914;&#20316;&#32773;&#25110;&#35770;&#25991;ID&#65289;&#12290;&#36825;&#31181;&#35782;&#21035;&#26159;&#20840;&#23616;&#30340;&#65306;&#36825;&#20123;&#23454;&#20307;&#24341;&#29992;&#30340;&#25152;&#26377;&#20986;&#29616;&#65288;&#21487;&#33021;&#36328;&#36234;&#22810;&#20010;&#25968;&#25454;&#24211;&#20803;&#32452;&#65289;&#37117;&#34987;&#35270;&#20026;&#30456;&#31561;&#24182;&#19988;&#21487;&#20197;&#21512;&#24182;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#35782;&#21035;&#25968;&#25454;&#20540;&#23545;&#26102;&#65292;&#26412;&#22320;&#21512;&#24182;&#24418;&#24335;&#36890;&#24120;&#26356;&#33258;&#28982;&#65292;&#20363;&#22914;&#65292;&#19968;&#20123;&#20986;&#29616;&#8220;J.Smith&#8221;&#30340;&#23454;&#20307;&#21487;&#20197;&#19982;&#8220;Joe Smith&#8221;&#31561;&#21516;&#65292;&#32780;&#20854;&#20182;&#23454;&#20307;&#24212;&#21512;&#24182;&#21040;&#8220;Jane Smith&#8221;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25193;&#23637;Lace&#20540;&#30340;&#26412;&#22320;&#21512;&#24182;&#65292;&#24182;&#25506;&#32034;&#25152;&#24471;&#21040;&#30340;&#24418;&#24335;&#21270;&#30340;&#35745;&#31639;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recently proposed Lace framework for collective entity resolution, logical rules and constraints are used to identify pairs of entity references (e.g. author or paper ids) that denote the same entity. This identification is global: all occurrences of those entity references (possibly across multiple database tuples) are deemed equal and can be merged. By contrast, a local form of merge is often more natural when identifying pairs of data values, e.g. some occurrences of 'J. Smith' may be equated with 'Joe Smith', while others should merge with 'Jane Smith'. This motivates us to extend Lace with local merges of values and explore the computational properties of the resulting formalism.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16809</link><description>&lt;p&gt;
GenQ&#65306;&#33258;&#21160;&#21270;&#38382;&#31572;&#29983;&#25104;&#22120;&#20197;&#24110;&#21161;&#29031;&#39038;&#32773;&#19982;&#23401;&#23376;&#20849;&#35835;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29031;&#39038;&#32773;&#35810;&#38382;&#24320;&#25918;&#24335;&#38382;&#39064;&#20197;&#28608;&#21457;&#19982;&#23401;&#23376;&#30340;&#23545;&#35805;&#26102;&#65292;&#21487;&#20197;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;&#34429;&#28982;&#26377;&#21033;&#29992;&#25216;&#26415;&#24037;&#20855;&#26469;&#25903;&#25345;&#36825;&#20010;&#36807;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#8221;&#30340;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#29616;&#26377;&#30340;&#29983;&#25104;&#31867;&#20154;&#35821;&#35328;&#38382;&#39064;&#30340;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#24320;&#21457;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#31995;&#32479;&#30340;&#22521;&#35757;&#25968;&#25454;&#36890;&#24120;&#27809;&#26377;&#32771;&#34385;&#21040;&#20154;&#21475;&#32479;&#35745;&#23398;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20154;&#21487;&#33021;&#20250;&#25552;&#20986;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#20026;&#25289;&#19969;&#35028;&#20799;&#31461;&#35774;&#35745;&#26234;&#33021;&#38405;&#35835;&#25903;&#25345;&#24212;&#29992;&#31243;&#24207;&#30340;&#24191;&#27867;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20174;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#25289;&#19969;&#35028;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20197;&#21450;&#20854;&#20182;&#20154;&#21475;&#32479;&#35745;&#23398;&#32972;&#26223;&#30340;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20013;&#32676;&#38598;&#22823;&#37327;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#20010;&#20307;&#12289;&#25991;&#21270;&#21644;&#29615;&#22659;&#22240;&#32032;&#20013;&#20171;&#30340;&#38382;&#39064;&#25552;&#38382;&#30340;&#21464;&#21270;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#33258;&#21160;&#20135;&#29983;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When caregivers ask open--ended questions to motivate dialogue with children, it facilitates the child's reading comprehension skills.Although there is scope for use of technological tools, referred here as "intelligent tutoring systems", to scaffold this process, it is currently unclear whether existing intelligent systems that generate human--language like questions is beneficial. Additionally, training data used in the development of these automated question generation systems is typically sourced without attention to demographics, but people with different cultural backgrounds may ask different questions. As a part of a broader project to design an intelligent reading support app for Latinx children, we crowdsourced questions from Latinx caregivers and noncaregivers as well as caregivers and noncaregivers from other demographics. We examine variations in question--asking within this dataset mediated by individual, cultural, and contextual factors. We then design a system that autom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16806</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#20250;&#20135;&#29983;&#26356;&#19981;&#20934;&#30830;&#30340;&#32763;&#35793;?
&lt;/p&gt;
&lt;p&gt;
Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#65292;&#24050;&#32463;&#25104;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25110;&#29702;&#35299;&#20219;&#21153;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#65292;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;few-shot&#25552;&#31034;&#26426;&#21046;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#30456;&#23545;&#36739;&#23569;&#22320;&#20851;&#27880;&#36825;&#31181;&#32763;&#35793;&#19982;&#26631;&#20934;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#29983;&#25104;&#32763;&#35793;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#20174;&#25991;&#23383;&#23545;&#40784;&#21644;&#21333;&#35843;&#24615;&#31561;&#26041;&#38754;&#65292;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#26412;&#25991;&#23383;&#31215;&#26497;&#24230;&#65292;&#21457;&#29616;GPT&#20174;&#33521;&#35821;&#65288;E-X&#65289;&#32763;&#35793;&#30340;&#25991;&#26412;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#24403;&#32763;&#35793;&#21477;&#23376;&#38271;&#24230;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24046;&#21035;&#23601;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16358</link><description>&lt;p&gt;
&#24102;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16358
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26435;&#37325;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#26641;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#22810;&#20010;&#36830;&#36890;&#20998;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20197;&#23454;&#29616;&#24179;&#28369;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#27969;&#27700;&#32447;&#20013;&#21253;&#21547;&#32858;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20960;&#20309;&#29615;&#22659;&#19979;&#20063;&#33021;&#33391;&#22909;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#21046;&#23450;&#20102;&#19968;&#20010;&#29305;&#21035;&#30340;&#25439;&#22833;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#37096;&#20998;&#32858;&#31867;&#25968;&#25454;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20013;&#20064;&#24471;&#26032;&#30340;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#26102;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15937</link><description>&lt;p&gt;
&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35270;&#35273;&#19978;&#26377;&#20381;&#25454;&#30340;&#23569;&#26679;&#26412;&#35789;&#27719;&#20064;&#24471;
&lt;/p&gt;
&lt;p&gt;
Visually grounded few-shot word acquisition with fewer shots. (arXiv:2305.15937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20013;&#20064;&#24471;&#26032;&#30340;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#26102;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20013;&#20064;&#24471;&#26032;&#30340;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#12290;&#32473;&#23450;&#19968;&#32452;&#27979;&#35797;&#22270;&#20687;&#21644;&#19968;&#20010;&#21475;&#22836;&#26597;&#35810;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#25351;&#20986;&#21738;&#20010;&#22270;&#20687;&#23637;&#31034;&#20102;&#26597;&#35810;&#35789;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#20351;&#29992;&#25968;&#23383;&#35789;-&#22270;&#20687;&#23545;&#30340;&#20154;&#36896;&#29615;&#22659;&#26469;&#31616;&#21270;&#35813;&#38382;&#39064;&#65292;&#35201;&#20040;&#20351;&#29992;&#27599;&#20010;&#31867;&#21035;&#22823;&#37327;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#33258;&#28982;&#30340;&#35789;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#65292;&#20294;&#21482;&#38656;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#21363;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#32473;&#23450;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#22270;&#20687;&#20013;&#25366;&#25496;&#26032;&#30340;&#26080;&#30417;&#30563;&#35789;-&#22270;&#20687;&#35757;&#32451;&#23545;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35789;&#21040;&#22270;&#20687;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#30830;&#23450;&#35789;-&#22270;&#20687;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#20219;&#20309;&#29616;&#26377;&#26041;&#27861;&#37117;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#21482;&#38656;&#26356;&#23569;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a visually grounded speech model that acquires new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelled speech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new model, we achieve better performance with fewer shots than any existing approach.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15703</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#22909;&#22788;&#65306;&#23567;&#25439;&#22833;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15703
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#26524;&#65292;&#20294;&#20854;&#20309;&#26102;&#20309;&#22320;&#26377;&#30410;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#22238;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#25105;&#20204;&#30340;&#36793;&#30028;&#20250;&#27604;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#26356;&#24378;&#12290;&#20316;&#20026;&#28909;&#36523;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#25104;&#26412;&#20998;&#24067;&#20250;&#22312;&#24773;&#22659;&#23637;&#24320;&#65288;CB&#65289;&#20013;&#23548;&#33268;&#23567;&#25439;&#22833;&#21518;&#24724;&#36793;&#30028;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#24335;CB&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#22312;&#23454;&#35777;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#22312;&#32447;RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#29256;&#26412;&#31354;&#38388;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#34920;&#26684;MDP&#20013;&#23454;&#29616;&#20102;&#23567;&#25439;&#22833;&#21518;&#24724;&#65292;&#21516;&#26102;&#22312;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#20139;&#26377;&#23567;&#25439;&#22833;PAC&#36793;&#30028;&#12290;&#20197;&#31867;&#20284;&#30340;&#35265;&#35299;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#31163;&#32447;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#36719;&#20214;&#20195;&#30721;&#21644;&#37197;&#32622;&#29983;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#33258;&#21160;&#21512;&#25104;&#31243;&#24207;&#65292;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#23436;&#25104;&#31243;&#24207;&#21512;&#25104;&#65292;&#24182;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#37197;&#32622;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.15642</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#36719;&#20214;&#20195;&#30721;&#21644;&#37197;&#32622;&#33258;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Automatic Synthesis of Software Code and Configuration. (arXiv:2305.15642v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#36719;&#20214;&#20195;&#30721;&#21644;&#37197;&#32622;&#29983;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#33258;&#21160;&#21512;&#25104;&#31243;&#24207;&#65292;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#23436;&#25104;&#31243;&#24207;&#21512;&#25104;&#65292;&#24182;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#37197;&#32622;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#34892;&#19994;&#30340;&#38656;&#27714;&#22686;&#21152;&#21644;&#36719;&#20214;&#24037;&#31243;&#24072;&#30340;&#31232;&#32570;&#24615;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#33258;&#21160;&#21270;&#36719;&#20214;&#29983;&#25104;&#21644;&#37197;&#32622;&#30340;&#36807;&#31243;&#12290;&#22823;&#35268;&#27169;&#30340;&#33258;&#21160;&#36719;&#20214;&#20195;&#30721;&#29983;&#25104;&#21644;&#37197;&#32622;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#23558;&#33258;&#21160;&#36719;&#20214;&#29983;&#25104;&#21644;&#37197;&#32622;&#25286;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#35268;&#33539;&#33258;&#21160;&#21512;&#25104;&#36719;&#20214;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#20219;&#21153;&#34987;&#36827;&#19968;&#27493;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#26159;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21512;&#25104;&#31243;&#24207;&#65292;&#20854;&#39537;&#21160;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#31243;&#24207;&#36319;&#36394;&#21644;&#35268;&#33539;&#35757;&#32451;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#31243;&#24207;&#21512;&#25104;&#24418;&#24335;&#21270;&#20026;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#65288;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#65289;&#26469;&#21512;&#25104;&#31243;&#24207;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#37197;&#32622;&#29983;&#25104;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increasing demands in software industry and scarcity of software engineers motivates researchers and practitioners to automate the process of software generation and configuration. Large scale automatic software generation and configuration is a very complex and challenging task. In this proposal, we set out to investigate this problem by breaking down automatic software generation and configuration into two different tasks. In first task, we propose to synthesize software automatically with input output specifications. This task is further broken down into two sub-tasks. The first sub-task is about synthesizing programs with a genetic algorithm which is driven by a neural network based fitness function trained with program traces and specifications. For the second sub-task, we formulate program synthesis as a continuous optimization problem and synthesize programs with covariance matrix adaption evolutionary strategy (a state-of-the-art continuous optimization method). Finally, for th
&lt;/p&gt;</description></item><item><title>SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;</title><link>http://arxiv.org/abs/2305.15486</link><description>&lt;p&gt;
SPRING: GPT-4&#36890;&#36807;&#23398;&#20064;&#35770;&#25991;&#21644;&#25512;&#29702;&#22312;&#28216;&#25103;&#20013;&#34920;&#29616;&#36229;&#36807;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. (arXiv:2305.15486v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15486
&lt;/p&gt;
&lt;p&gt;
SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#30001;&#20110;&#20854;&#22810;&#20219;&#21153;&#12289;&#28145;&#24230;&#25506;&#32034;&#21644;&#30446;&#26631;&#20248;&#20808;&#32423;&#35201;&#27714;&#65292;&#23545;AI&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#28216;&#25103;&#26041;&#38754;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#22312;&#20687;Crafter&#25110;Minecraft&#36825;&#26679;&#22797;&#26434;&#30340;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SPRING&#65292;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;LaTeX&#28304;&#20316;&#20026;&#28216;&#25103;&#35821;&#22659;&#21644;&#20195;&#29702;&#24403;&#21069;&#35266;&#23519;&#30340;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;SPRING&#26694;&#26550;&#21033;&#29992;&#20855;&#26377;&#28216;&#25103;&#30456;&#20851;&#38382;&#39064;&#30340;&#23450;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20316;&#20026;&#33410;&#28857;&#21644;&#20381;&#36182;&#20851;&#31995;&#20316;&#20026;&#36793;&#12290;&#36890;&#36807;&#25353;&#25299;&#25169;&#39034;&#24207;&#36941;&#21382;DAG&#24182;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#30340;LLM&#21709;&#24212;&#26469;&#30830;&#23450;&#22312;&#29615;&#22659;&#20013;&#37319;&#21462;&#30340;&#26368;&#20248;&#34892;&#21160;&#65292;LLM&#23545;&#26368;&#32456;&#33410;&#28857;&#30340;&#31572;&#26696;&#30452;&#25509;&#36716;&#21270;&#20026;&#29615;&#22659;&#34892;&#21160;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;
&lt;/p&gt;
&lt;p&gt;
Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.15151</link><description>&lt;p&gt;
&#30693;&#35782;&#35774;&#35745;&#65306;&#36890;&#36807;&#30693;&#35782;&#25552;&#28860;&#25512;&#21160;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#65292;&#23547;&#25214;&#25240;&#21472;&#20026;&#25152;&#26399;&#26395;&#32467;&#26500;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#24050;&#32463;&#21462;&#24471;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#30053;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#37325;&#35201;&#24615;&#65292;&#26410;&#33021;&#35206;&#30422;&#24191;&#27867;&#30340;&#34507;&#30333;&#36136;&#31354;&#38388;&#65292;&#24182;&#19988;&#27809;&#26377;&#34701;&#20837;&#24120;&#35265;&#30340;&#34507;&#30333;&#36136;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#26469;&#33410;&#30465;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;CATH&#12289;TS50&#21644;TS500&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#30693;&#35782;&#35774;&#35745;&#26041;&#27861;&#22312;CATH&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;PiFold&#26041;&#27861;&#32422;9&#65285;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30693;&#35782;&#35774;&#35745;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VIPER&#31639;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#30340;&#19987;&#23478;&#32423;&#25511;&#21046;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14343</link><description>&lt;p&gt;
&#20316;&#20026;&#22870;&#21169;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Video Prediction Models as Rewards for Reinforcement Learning. (arXiv:2305.14343v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VIPER&#31639;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#30340;&#19987;&#23478;&#32423;&#25511;&#21046;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21046;&#23450;&#35753;&#20195;&#29702;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#30340;&#22870;&#21169;&#20449;&#21495;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#20174;&#24191;&#27867;&#21487;&#29992;&#20110;&#20114;&#32852;&#32593;&#19978;&#30340;&#26080;&#26631;&#27880;&#35270;&#39057;&#20013;&#25552;&#21462;&#34892;&#20026;&#20559;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Video Prediction Rewards (VIPER)&#65292;&#36825;&#31181;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#20316;&#20026;&#19981;&#38656;&#35201;&#34892;&#20026;&#24178;&#39044;&#30340;&#24378;&#21270;&#23398;&#20064;&#22870;&#21169;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#19987;&#23478;&#35270;&#39057;&#19978;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;Transformer&#65292;&#28982;&#21518;&#23558;&#35270;&#39057;&#39044;&#27979;&#21487;&#33021;&#24615;&#29992;&#20316;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;VIPER&#20351;&#24471;&#22312;DMC&#12289;Atari&#21644;RLBench&#20219;&#21153;&#31561;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#20869;&#65292;&#22312;&#27809;&#26377;&#32534;&#31243;&#20219;&#21153;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19987;&#23478;&#32423;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20026;&#27809;&#26377;&#19987;&#23478;&#25968;&#25454;&#21487;&#29992;&#30340;&#20998;&#24067;&#22806;&#29615;&#22659;&#23548;&#20986;&#22870;&#21169;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#26700;&#38754;&#25805;&#32437;&#30340;&#36328;&#20307;&#29616;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#20855;&#26377;&#20280;&#32553;&#24615;&#30340;&#22870;&#21169;&#21046;&#23450;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning. A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13551</link><description>&lt;p&gt;
EntRED: &#29992;&#26356;&#23569;&#30340;&#25463;&#24452;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EntRED: Benchmarking Relation Extraction with Fewer Shortcuts. (arXiv:2305.13551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#31216;&#26356;&#20026;&#22810;&#26679;&#12289;&#27809;&#26377;&#25463;&#24452;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;EntRed&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#23454;&#20307;&#27880;&#37322;&#38169;&#35823;&#12289;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#12289;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21517;&#31216;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#36215;&#30528;&#26377;&#25928;&#30340;&#20316;&#29992;&#65292;&#24182;&#24120;&#24120;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#38598;&#20013;&#30340;&#23454;&#20307;&#21517;&#31216;&#26174;&#33879;&#24433;&#21709;&#20102;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26631;&#20934;&#30340;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23384;&#22312;&#22823;&#37327;&#38169;&#35823;&#30340;&#23454;&#20307;&#27880;&#37322;&#65292;&#23454;&#20307;&#21517;&#31216;&#22810;&#26679;&#24615;&#36739;&#20302;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#20174;&#23454;&#20307;&#21517;&#31216;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19982;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EntRED&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36739;&#23569;&#25463;&#24452;&#21644;&#26356;&#39640;&#23454;&#20307;&#22810;&#26679;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#26500;&#24314;EntRED&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#65288;CI&#65289;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26367;&#25442;&#31649;&#36947;&#65306;ERIC&#12290;ERIC&#23545;&#23454;&#20307;&#36827;&#34892;&#31867;&#22411;&#32422;&#26463;&#26367;&#25442;&#65292;&#20197;&#20943;&#23569;&#20174;&#23454;&#20307;&#20559;&#24046;&#21040;&#22522;&#26412;&#20107;&#23454;&#20851;&#31995;&#30340;&#25463;&#24452;&#12290;ERIC&#22312;&#20004;&#20010;&#26041;&#38754;&#24212;&#29992;CI&#65306;1&#65289;&#38024;&#23545;&#38656;&#35201;&#23454;&#20307;&#26367;&#25442;&#30340;&#23454;&#20363;&#65292;2&#65289;&#30830;&#23450;&#20505;&#36873;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity names play an effective role in relation extraction (RE) and often influence model performance. As a result, the entity names in the benchmarks' test sets significantly influence the evaluation of RE models. In this work, we find that the standard RE benchmarks' datasets have a large portion of incorrect entity annotations, low entity name diversity, and are prone to have shortcuts from entity names to ground-truth relations. These issues make the standard benchmarks far from reflecting the real-world scenarios. Hence, in this work, we present EntRED, a challenging RE benchmark with reduced shortcuts and higher diversity of entities. To build EntRED, we propose an end-to-end entity replacement pipeline based on causal inference (CI): ERIC. ERIC performs type-constrained replacements on entities to reduce the shortcuts from entity bias to ground-truth relations. ERIC applies CI in two aspects: 1) targeting the instances that need entity replacements, and 2) determining the candid
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11541</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#24212;&#29992;&#21644;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#29305;&#23450;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#24456;&#24179;&#24248;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;MSQA&#30340;&#22522;&#20934;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28041;&#21450;Microsoft&#20135;&#21697;&#21644;&#23458;&#25143;&#36935;&#21040;&#30340;IT&#25216;&#26415;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#34892;&#19994;&#20113;&#30340;&#29305;&#23450;QA&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#19968;&#33324;&#30340;LLM&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;LLM&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;LLM&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36981;&#24490;&#25105;&#20204;&#30340;&#27169;&#22411;&#34701;&#21512;&#26694;&#26550;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#30340;&#24120;&#29992;LLM&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average since there is no specific knowledge in it. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, which is not available for general LLM, so it is well suited for evaluating methods aimed at improving domain-specific capabilities of LLM. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our model fusion framework outperforms the commonly used LLM with retrieval methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11461</link><description>&lt;p&gt;
&#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#20174;&#35821;&#20041;&#32423;&#21035;&#21040;&#20195;&#30721;&#32423;&#21035;&#30340; SelfzCoT&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs
&lt;/p&gt;
&lt;p&gt;
SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs. (arXiv:2305.11461v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558; SelfzCoT &#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#65292;&#20854;&#20934;&#30830;&#24615;&#20174;GSM8K&#30340;40.50%&#25552;&#39640;&#33267;82.34%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;94.7%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;94.10%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;91.30%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;82.33%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;79.70%&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20351;&#29992;&#21069;&#20004;&#20010;&#25345;&#20037;&#36335;&#24452;&#28608;&#27963;&#21040;LLM&#65292;&#29305;&#21035;&#26159;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#20351; SelfzCoT &#22312;&#25152;&#26377;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#22312;GSM8K&#20013;&#65292;MzCoT&#30340;&#20934;&#30830;&#24615;&#20174;40.50%&#25552;&#39640;&#33267;76.32%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;96.97%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;92.39%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;94.60%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;79.90%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;81.50%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper show a work on better use of LLMs with SelfzCoT a self-prompt zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%, with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP from 63.70% to 79.70%. Totally, using the first two lasting path activations to LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our modified zero-shot CoT (MzCoT) also achieves remarkable performance in the reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from 40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70% to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to 79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.09782</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35270;&#35273;&#38382;&#31572;&#31639;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29702;&#35299;&#24182;&#22238;&#31572;&#38382;&#39064;&#12290; VQA &#23545;&#35270;&#35273;&#21463;&#25439;&#32773;&#26377;&#24110;&#21161;&#65292;&#21487;&#29992;&#20110;&#23433;&#20840;&#30417;&#25511;&#31995;&#32479;&#21644;&#20174;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#22312;&#32447;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290; &#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23398;&#20064;&#38382;&#39064;&#30340;&#35821;&#20041;&#24182;&#25552;&#21462;&#25991;&#26412;&#29305;&#24449;&#12290; &#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#29992;&#20110;&#20197;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25152;&#38382;&#38382;&#39064;&#28041;&#21450;&#30340;&#29289;&#20307;&#30340;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#34920;&#31034;&#12290; &#27880;&#24847;&#21147;&#27169;&#22411;&#35797;&#22270;&#27169;&#20223;&#20154;&#31867;&#26681;&#25454;&#35821;&#22659;&#20851;&#27880;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#34892;&#20026;&#12290; &#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340; VQA &#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#29983;&#25104;&#25991;&#26412;&#35821;&#20041;&#65292;&#35782;&#21035;&#23545;&#35937;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) usesimage processing algorithms to process the image and natural language processing methods to understand and answer the question. VQA is helpful to a visually impaired person, can be used for the security surveillance system and online chatbots that learn from the web. It uses NLP methods to learn the semantic of the question and to derive the textual features. Computer vision techniques are used for generating image representation in such a way that they can identify the objects about which question is asked. The Attention model tries to mimic the human behavior of giving attention to a different region of an image according to our understanding of its context. This paper critically examines and reviews methods of VQA algorithm such as generation of semantics of text, identification of objects and answer classification techniques that use the co-attention approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05368</link><description>&lt;p&gt;
GNNs: &#21487;&#20197;&#26356;&#24378;&#12289;&#26356;&#26032;&#12289;&#26356;&#24555;
&lt;/p&gt;
&lt;p&gt;
GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#20174;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#36890;&#36807;&#38598;&#25104;&#37051;&#23621;&#33410;&#28857;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#34920;&#29616;&#20986;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#24615;&#33021;&#20250;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#32780;&#36880;&#28176;&#38477;&#20302;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;k&#36339;&#23376;&#22270;&#32858;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;&#28145;&#23618;GNN&#34920;&#29616;&#36880;&#28176;&#36864;&#21270;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#32858;&#21512;&#23376;&#22270;&#30340;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#27531;&#24046;&#30340;GNN&#23454;&#38469;&#19978;&#21033;&#29992;&#20102;1&#21040;k&#36339;&#23376;&#22270;&#32858;&#21512;&#32467;&#26524;&#26469;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#35777;&#26126;&#20854;&#27604;&#20043;&#21069;&#30340;&#27531;&#24046;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#21487;&#20197;&#21033;&#29992;1&#21040;k&#36339;&#36291;&#23376;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#21152;&#26435;&#22240;&#26524; DAGs&#30340;&#26032;&#24230;&#37327;&#21644;&#25628;&#32034;&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#30340;&#22240;&#26524;&#22270;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#25429;&#25417;&#25628;&#32034;&#31639;&#27861;&#30340;&#26368;&#22351;&#24178;&#39044;&#25104;&#26412;&#65292;&#24182;&#25552;&#20379;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#23454;&#29616;&#23545;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2305.04445</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#26435;&#22240;&#26524; DAG &#30340;&#26032;&#24230;&#37327;&#21644;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
New metrics and search algorithms for weighted causal DAGs. (arXiv:2305.04445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#21152;&#26435;&#22240;&#26524; DAGs&#30340;&#26032;&#24230;&#37327;&#21644;&#25628;&#32034;&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#30340;&#22240;&#26524;&#22270;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#25429;&#25417;&#25628;&#32034;&#31639;&#27861;&#30340;&#26368;&#22351;&#24178;&#39044;&#25104;&#26412;&#65292;&#24182;&#25552;&#20379;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#23454;&#29616;&#23545;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#26102;&#65292;&#21482;&#33021;&#24674;&#22797;&#21040;&#19968;&#20010;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#30340;&#22240;&#26524;&#22270;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#20551;&#35774;&#25110;&#24178;&#39044;&#25968;&#25454;&#26469;&#23436;&#25104;&#24674;&#22797;&#12290;&#26412;&#25991;&#22312;&#19968;&#20123;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#36890;&#36807;&#33410;&#28857;&#30456;&#20851;&#24178;&#39044;&#25104;&#26412;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#65292;&#30740;&#31350;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#27809;&#26377;&#31639;&#27861;&#33021;&#22815;&#27604;&#39564;&#35777;&#27425;&#25968;&#30340;&#39034;&#24207;&#26356;&#22909;&#22320;&#23454;&#29616;&#28176;&#36817;&#20445;&#35777;&#65292;&#39564;&#35777;&#27425;&#25968;&#26159;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#30340;&#19968;&#20010;&#25104;&#29087;&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#36127;&#38754;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#25429;&#25417;&#20219;&#20309;&#25628;&#32034;&#31639;&#27861;&#26368;&#22351;&#24178;&#39044;&#25104;&#26412;&#30340;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#36825;&#20010;&#26032;&#22522;&#20934;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#37117;&#33021;&#23454;&#29616;&#23545;&#25968;&#36924;&#36817;&#65306;&#21407;&#23376;&#12289;&#26377;&#30028;&#22823;&#23567;&#30340;&#24178;&#39044;&#21644;&#24191;&#20041;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recovering causal relationships from data is an important problem. Using observational data, one can typically only recover causal graphs up to a Markov equivalence class and additional assumptions or interventional data are needed for complete recovery. In this work, under some standard assumptions, we study causal graph discovery via adaptive interventions with node-dependent interventional costs. For this setting, we show that no algorithm can achieve an approximation guarantee that is asymptotically better than linear in the number of vertices with respect to the verification number; a well-established benchmark for adaptive search algorithms. Motivated by this negative result, we define a new benchmark that captures the worst-case interventional cost for any search algorithm. Furthermore, with respect to this new benchmark, we provide adaptive search algorithms that achieve logarithmic approximations under various settings: atomic, bounded size interventions and generalized cost o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.04111</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#24314;&#27169;&#19979;&#30340;&#39640;&#25928;&#21644;&#24230;&#25968;&#24341;&#23548;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23567;&#22270;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#29983;&#25104;&#21253;&#21547;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22270;&#24182;&#28385;&#36275;&#22270;&#32479;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#38543;&#26426;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#26368;&#32456;&#33719;&#24471;&#19968;&#24352;&#31354;&#30333;&#22270;&#12290;EDGE&#20165;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#12290;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#26356;&#23569;&#22320;&#36827;&#34892;&#36793;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;EDGE&#26126;&#30830;&#22320;&#20801;&#35768;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;&#23427;&#36824;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02637</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#25968;&#25454;&#38598;&#30340;&#24369;&#30417;&#30563;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#22810;&#20301;&#23398;&#32773;&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#24403;&#21069;&#38024;&#23545;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#35782;&#21035;&#30340;&#30740;&#31350;&#29305;&#28857;&#26159;&#19981;&#31995;&#32479;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#27880;&#37322;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#23545;&#23427;&#20204;&#26410;&#34987;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#24046;&#65292;&#24182;&#19988;&#19981;&#21516;HS&#20998;&#31867;&#27861;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26080;&#27861;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20869;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;HS&#20998;&#31867;&#27169;&#22411;&#36890;&#29992;&#24615;&#36739;&#24046;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00909</link><description>&lt;p&gt;
&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;&#65306;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#22797;&#26434;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#20570;&#27861;&#36890;&#24120;&#26159;&#20808;&#27010;&#36848;&#19968;&#19979;&#25511;&#21046;&#27969;&#31243;&#65292;&#28982;&#21518;&#36845;&#20195;&#36827;&#34892;&#20016;&#23500;&#65292;&#26368;&#32456;&#29983;&#25104;&#19968;&#20123;&#31934;&#24515;&#21152;&#24037;&#30340;&#35821;&#27861;&#32467;&#26500;&#21644;&#23618;&#27425;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#20195;&#30721;&#65292;&#27809;&#26377;&#20013;&#38388;&#29615;&#33410;&#65292;&#20197;&#21453;&#26144;"&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;"&#30340;&#32467;&#26500;&#21270;&#24605;&#32500;&#36807;&#31243;&#12290;&#21463;&#21040;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChainCoder&#65292;&#36825;&#26159;&#19968;&#31181;&#31243;&#24207;&#32508;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36880;&#27493;&#29983;&#25104;Python&#20195;&#30721;&#65292;&#21363;&#20174;&#31895;&#21040;&#32454;&#36827;&#34892;&#22810;&#27425;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#35821;&#27861;&#26641;&#35299;&#26512;&#23558;&#28304;&#20195;&#30721;&#20998;&#35299;&#20026;&#24067;&#23616;&#26694;&#26550;&#32452;&#20214;&#21644;&#38468;&#20214;&#32452;&#20214;&#65292;&#20197;&#26500;&#24314;&#23618;&#27425;&#34920;&#31034;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#39044;&#27979;&#30446;&#26631;&#37325;&#26032;&#21551;&#21160;&#65292;&#24418;&#25104;&#22810;&#27425;&#36890;&#36807;&#30446;&#26631;&#65292;&#27599;&#27425;&#29983;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#36825;&#20123;&#23376;&#24207;&#21015;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#20018;&#32852;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>"&#27169;&#25311;&#32593;&#32476;"&#27169;&#22411;&#22312;3D&#29289;&#20307;&#22330;&#26223;&#20998;&#21106;&#20013;&#37319;&#29992;&#31867;&#27604;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#24182;&#39044;&#27979;&#31867;&#20284;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#33021;&#22815;&#22312;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#20013;&#24471;&#20986;&#30456;&#20284;&#30340;&#35299;&#26512;&#65292;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2304.14382</link><description>&lt;p&gt;
&#27169;&#25311;&#24418;&#24335;&#36716;&#25442;&#22120;&#29992;&#20110;&#23569;&#26679;&#26412;3D&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analogy-Forming Transformers for Few-Shot 3D Parsing. (arXiv:2304.14382v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14382
&lt;/p&gt;
&lt;p&gt;
"&#27169;&#25311;&#32593;&#32476;"&#27169;&#22411;&#22312;3D&#29289;&#20307;&#22330;&#26223;&#20998;&#21106;&#20013;&#37319;&#29992;&#31867;&#27604;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#24182;&#39044;&#27979;&#31867;&#20284;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#33021;&#22815;&#22312;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#20013;&#24471;&#20986;&#30456;&#20284;&#30340;&#35299;&#26512;&#65292;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#19968;&#32452;&#26377;&#26631;&#35760;&#30340;&#32467;&#26500;&#21270;3D&#22330;&#26223;&#20013;&#26174;&#24335;&#22320;&#32534;&#30721;&#39046;&#22495;&#30693;&#35782;&#65288;&#20316;&#20026;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#37096;&#20998;&#65289;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#23545;3D&#29289;&#20307;&#22330;&#26223;&#36827;&#34892;&#20998;&#21106;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20174;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#21450;&#20854;&#30456;&#24212;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#35843;&#21046;&#26426;&#21046;&#20026;&#36755;&#20837;&#22330;&#26223;&#39044;&#27979;&#31867;&#20284;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23558;&#22330;&#26223;&#26144;&#23556;&#21040;&#37096;&#20998;&#20998;&#21106;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26816;&#32034;&#30340;&#35760;&#24518;&#36827;&#34892;&#26465;&#20214;&#25511;&#21046;&#65292;&#39044;&#27979;&#28151;&#21512;&#21305;&#37197;&#26816;&#32034;&#35760;&#24518;&#30340;&#32467;&#26500;&#21512;&#25104;&#12290;&#22312;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#20013;&#65292;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#34987;&#19968;&#33268;&#22320;&#22788;&#29702;&#65292;&#36890;&#36807;&#23545;&#36866;&#24403;&#30340;&#35760;&#24518;&#38598;&#36827;&#34892;&#26465;&#20214;&#35859;&#35789;&#65292;&#26080;&#35770;&#26159;&#20174;&#21333;&#20010;&#12289;&#23569;&#25968;&#36824;&#26159;&#35768;&#22810;&#23384;&#20648;&#23454;&#20363;&#20013;&#32487;&#25215;&#30456;&#20284;&#30340;&#35299;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#22312;&#35768;&#22810;&#26679;&#26412;&#24773;&#20917;&#19979;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Analogical Networks, a model that encodes domain knowledge explicitly, in a collection of structured labelled 3D scenes, in addition to implicitly, as model parameters, and segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures for the input scene, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformers in many-shot settings, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.15103</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#30456;&#20284;&#24615;&#22270;&#35889;&#19978;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#36816;&#20316;&#21407;&#29702;&#21644;&#21407;&#22240;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#12290;&#21033;&#29992;&#36825;&#31181;&#31561;&#20215;&#24615;&#20316;&#20026;&#22522;&#30707;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#24182;&#20005;&#26684;&#25551;&#36848;&#22810;&#27169;&#24577;&#23545;&#35937;&#22914;&#20309;&#34987;&#23884;&#20837;&#21040;&#19968;&#36215;&#12290;&#22312;&#29702;&#35770;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#28151;&#21512;&#25439;&#22833;&#65292;&#32467;&#21512;&#26032;&#39062;&#30340;&#26680;&#20989;&#25968;&#65292;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26631;&#20934;&#39640;&#26031;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;&#65288;RN-Net&#65289;&#12290;RN-Net&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#22788;&#29702;&#24322;&#27493;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;DVS128&#25163;&#21183;&#19978;&#23454;&#29616;&#20102;&#36804;&#20170;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;99.2&#65285;&#65292;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#23610;&#23544;&#19979;&#23454;&#29616;&#20102;DVS Lip&#25968;&#25454;&#38598;&#30340;67.5&#65285;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.10770</link><description>&lt;p&gt;
RN-Net: &#22522;&#20110;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network. (arXiv:2303.10770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;&#65288;RN-Net&#65289;&#12290;RN-Net&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#22788;&#29702;&#24322;&#27493;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;DVS128&#25163;&#21183;&#19978;&#23454;&#29616;&#20102;&#36804;&#20170;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;99.2&#65285;&#65292;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#23610;&#23544;&#19979;&#23454;&#29616;&#20102;DVS Lip&#25968;&#25454;&#38598;&#30340;67.5&#65285;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#21463;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#31232;&#30095;&#21644;&#24322;&#27493;&#33033;&#20914;&#34920;&#31034;&#30340;&#21551;&#21457;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#20107;&#20214;&#25968;&#25454;&#35201;&#20040;&#38656;&#35201;&#20351;&#29992;&#26114;&#36149;&#30340;&#29305;&#24449;&#25551;&#36848;&#31526;&#23558;&#33033;&#20914;&#36716;&#25442;&#25104;&#24103;&#65292;&#35201;&#20040;&#20351;&#29992;&#38590;&#20197;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#21367;&#31215;&#23618;&#21644;&#21160;&#24577;&#26102;&#38388;&#32534;&#30721;&#20648;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20855;&#26377;&#20302;&#30828;&#20214;&#21644;&#35757;&#32451;&#25104;&#26412;&#12290;&#20351;&#29992;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;&#65288;RN-Net&#65289;&#20351;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24322;&#27493;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#23454;&#29616;&#20102;DVS128&#25163;&#21183;&#30340;&#36804;&#20170;&#26368;&#39640;&#20934;&#30830;&#24230;99.2&#65285;&#65292;&#21516;&#26102;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#23610;&#23544;&#19979;&#23454;&#29616;&#20102;DVS Lip&#25968;&#25454;&#38598;&#30340;67.5&#65285;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#35760;&#24518;&#30005;&#38459;&#22120;&#30340;&#20869;&#37096;&#21160;&#24577;&#65292;&#21487;&#20197;&#20197;&#38750;&#24120;&#20302;&#30340;&#30828;&#20214;&#25104;&#26412;&#23454;&#29616;&#24322;&#27493;&#26102;&#38388;&#29305;&#24449;&#32534;&#30721;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#22788;&#29702;&#25110;&#19987;&#29992;&#30340;&#23384;&#20648;&#22120;&#21644;&#31639;&#26415;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-based cameras are inspired by the sparse and asynchronous spike representation of the biological visual system. However, processing the even data requires either using expensive feature descriptors to transform spikes into frames, or using spiking neural networks that are difficult to train. In this work, we propose a neural network architecture based on simple convolution layers integrated with dynamic temporal encoding reservoirs with low hardware and training costs. The Reservoir Nodes-enabled neuromorphic vision sensing Network (RN-Net) allows the network to efficiently process asynchronous temporal features, and achieves the highest accuracy of 99.2% for DVS128 Gesture reported to date, and one of the highest accuracy of 67.5% for DVS Lip dataset at a much smaller network size. By leveraging the internal dynamics of memristors, asynchronous temporal feature encoding can be implemented at very low hardware cost without preprocessing or dedicated memory and arithmetic units. T
&lt;/p&gt;</description></item><item><title>CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08127</link><description>&lt;p&gt;
CB2&#65306;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08127
&lt;/p&gt;
&lt;p&gt;
CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CB2 &#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010; 3D &#28216;&#25103;&#29615;&#22659;&#12289;&#19968;&#20010;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#21487;&#20026;&#20154;&#31867;&#26234;&#33021;&#20307;&#25552;&#20379;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312; https://cb2.ai &#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#31995;&#32479;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06833</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#20989;&#25968;&#20540;&#26597;&#25214;&#20854;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;&#20110;&#29983;&#25104;&#26041;&#31243;&#24207;&#21015;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#33719;&#30410;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#27604;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20851;&#27880;&#30340;&#26159;&#20511;&#37492;&#25991;&#26412;&#29983;&#25104;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#24573;&#30053;&#20102;&#26041;&#31243;&#30340;&#29305;&#23450;&#30446;&#26631;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TPSR&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#34701;&#20837;&#21040;Transformer&#35299;&#30721;&#36807;&#31243;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#21516;&#65292;TPSR&#20801;&#35768;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#65288;&#22914;&#25311;&#21512;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65289;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#30340;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13221</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65306;&#36890;&#36807;&#31163;&#25955;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#23454;&#29616;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Data-Centric AI: Deep Generative Differentiable Feature Selection via Discrete Subsetting as Continuous Embedding Space Optimization. (arXiv:2302.13221v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#30340;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#65288;FS&#65289;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#23376;&#38598;&#65292;&#20363;&#22914;&#36807;&#28388;&#22120;&#12289;&#21253;&#35013;&#22120;&#21644;&#23884;&#20837;&#24335;&#26041;&#27861;&#12290;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;FS&#30340;&#26631;&#20934;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21464;&#21270;&#65292;&#24182;&#19988;&#24403;&#25968;&#25454;&#26159;&#39640;&#32500;&#21644;&#23567;&#26679;&#26412;&#26102;&#65292;FS&#23481;&#26131;&#20986;&#29616;&#38382;&#39064;&#12290;&#36873;&#25321;&#30340;&#29305;&#24449;&#23376;&#38598;&#26159;&#21542;&#21487;&#20197;&#26356;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#27867;&#21270;&#20026;&#19968;&#20010;&#28145;&#24230;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65306;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#21644;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#28145;&#24230;&#29305;&#24449;&#23376;&#38598;&#32534;&#30721;&#22120;&#12289;&#20934;&#30830;&#24615;&#35780;&#20272;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#26799;&#24230;&#19978;&#21319;&#20248;&#21270;&#22120;&#12290;&#36825;&#20010;&#26694;&#26550;&#23454;&#29616;&#20102;&#22235;&#20010;&#27493;&#39588;&#65306;1) &#29305;&#24449;-&#20934;&#30830;&#24615;&#35757;&#32451;&#25968;&#25454;&#20934;&#22791;&#65307;2) &#28145;&#24230;&#29305;&#24449;&#23376;&#38598;&#23884;&#20837;&#65307;3) &#26799;&#24230;&#20248;&#21270;&#25628;&#32034;&#65307;4) &#29305;&#24449;&#23376;&#38598;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#27934;&#35265;&#65306;&#23558;&#24378;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#27169;&#22411;&#35270;&#20026;&#25628;&#32034;&#21152;&#36895;&#22120;&#12289;&#22810;&#23610;&#24230;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#36880;&#28176;&#22686;&#24378;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Feature Selection (FS), such as filter, wrapper, and embedded methods, aims to find the optimal feature subset for a given downstream task. However, in many real-world practices, 1) the criteria of FS vary across domains; 2) FS is brittle when data is a high-dimensional and small sample size. Can selected feature subsets be more generalized, accurate, and input dimensionality agnostic? We generalize this problem into a deep differentiable feature selection task and propose a new perspective: discrete feature subsetting as continuous embedding space optimization. We develop a generic and principled framework including a deep feature subset encoder, accuracy evaluator, decoder, and gradient ascent optimizer. This framework implements four steps: 1) features-accuracy training data preparation; 2) deep feature subset embedding; 3) gradient-optimized search; 4) feature subset reconstruction. We develop new technical insights: reinforcement as a training data generator, ensembles of diverse 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#26500;&#36896;&#24207;&#21015;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20844;&#24335;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.13186</link><description>&lt;p&gt;
&#26500;&#36896;&#25968;&#65306;&#22914;&#20309;&#24314;&#31435;&#19968;&#20010;&#22270;&#24418;&#65311;
&lt;/p&gt;
&lt;p&gt;
Construction numbers: How to build a graph?. (arXiv:2302.13186v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13186
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#26500;&#36896;&#24207;&#21015;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20844;&#24335;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;50&#24180;&#21069;&#65292;&#26031;&#22374;&#21033;&#32771;&#34385;&#20102;&#35745;&#31639;&#20559;&#24207;&#30340;&#32447;&#24615;&#25193;&#23637;&#25968;&#37327;&#38382;&#39064;&#12290;&#23545;&#20110;&#30001;&#21253;&#21547;&#20851;&#31995;&#30830;&#23450;&#30340;&#22270;&#24418;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#20559;&#24207;&#65292;&#25105;&#20204;&#31216;&#36825;&#26679;&#30340;&#32447;&#24615;&#25193;&#23637;&#20026;&#22270;&#24418;&#30340;&#8220;&#26500;&#36896;&#24207;&#21015;&#8221;&#65292;&#22240;&#20026;&#27599;&#20010;&#36793;&#37117;&#36981;&#24490;&#20854;&#20004;&#20010;&#31471;&#28857;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#36335;&#24452;&#12289;&#29615;&#12289;&#26143;&#24418;&#22270;&#12289;&#21452;&#26143;&#24418;&#22270;&#21644;&#23436;&#20840;&#22270;&#30340;&#27492;&#31867;&#24207;&#21015;&#25968;&#37327;&#12290;&#23545;&#20110;&#36335;&#24452;&#65292;&#25105;&#20204;&#35748;&#21516;&#26031;&#22374;&#21033;&#30340;&#24819;&#27861;&#65288;&#20999;&#32447;&#25968;&#65289;&#65292;&#24182;&#24471;&#21040;&#20102;&#20854;&#20182;&#31867;&#22411;&#30340;&#20844;&#24335;&#12290;&#27492;&#22806;&#36824;&#30740;&#31350;&#20102;&#32467;&#26500;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counting the number of linear extensions of a partial order was considered by Stanley about 50 years ago. For the partial order on the vertices and edges of a graph determined by inclusion, we call such linear extensions {\it construction sequences} for the graph as each edge follows both of its endpoints. The number of such sequences for paths, cycles, stars, double-stars, and complete graphs is found. For paths, we agree with Stanley (the Tangent numbers) and get formulas for the other classes. Structure and applications are also studied.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#37322;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#20013;&#20851;&#38190;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65306;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#19981;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.12537</link><description>&lt;p&gt;
&#30446;&#26631;&#32593;&#32476;&#22914;&#20309;&#31283;&#23450;&#26102;&#38388;&#24046;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Why Target Networks Stabilise Temporal Difference Methods. (arXiv:2302.12537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#37322;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#20013;&#20851;&#38190;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65306;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#19981;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36817;&#26399;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31867;&#20351;&#29992;&#19981;&#39057;&#32321;&#26356;&#26032;&#30446;&#26631;&#20540;&#36827;&#34892;&#31574;&#30053;&#35780;&#20272;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;&#30446;&#26631;&#32593;&#32476;&#26377;&#25928;&#24615;&#30340;&#23436;&#25972;&#29702;&#35770;&#35299;&#37322;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31181;&#27969;&#34892;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26368;&#32456;&#22238;&#31572;&#20102;&#8220;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#21487;&#20197;&#31283;&#23450;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#37096;&#20998;&#25311;&#21512;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#25324;&#30446;&#26631;&#32593;&#32476;&#30340;&#20351;&#29992;&#65292;&#24182;&#19988;&#22635;&#34917;&#20102;&#25311;&#21512;&#26041;&#27861;&#21644;&#21322;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#29420;&#29305;&#22320;&#25551;&#36848;&#25152;&#35859;&#30340;&#33268;&#21629;&#19977;&#20803;&#32452;&#65292;&#21363;&#20351;&#29992;&#26102;&#24207;&#24046;&#20998;&#26356;&#26032;&#65292;&#32467;&#21512;&#65288;&#38750;&#32447;&#24615;&#65289;&#20989;&#25968;&#36924;&#36817;&#21644;&#22788;&#20110;&#31163;&#32447;&#29366;&#24577;&#30340;&#25968;&#25454;&#65292;&#36825;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#25910;&#25947;&#30340;&#31639;&#27861;&#12290;&#36825;&#19968;&#35748;&#35782;&#20351;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#30446;&#26631;&#32593;&#32476;&#30340;&#20351;&#29992;&#21487;&#20197;&#20943;&#36731;&#26465;&#20214;&#24046;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integral to recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. Yet a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: `why do target networks stabilise TD learning'? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad - the use of TD updates with (nonlinear) function approximation and off-policy data - which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditionin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#32858;&#21512;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#26412;OOD&#26816;&#27979;&#12290;&#20854;&#33021;&#21457;&#25496;&#19981;&#21516;&#23618;&#36755;&#20986;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#26356;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#24182;&#25193;&#23637;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#20197;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2302.09852</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#25991;&#26412;OOD&#26816;&#27979;&#24471;&#20998;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Layer-wise Score Aggregation for Textual OOD Detection. (arXiv:2302.09852v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09852
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#32858;&#21512;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#26412;OOD&#26816;&#27979;&#12290;&#20854;&#33021;&#21457;&#25496;&#19981;&#21516;&#23618;&#36755;&#20986;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#26356;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#24182;&#25193;&#23637;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#20197;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#22686;&#21152;&#65292;OOD&#26816;&#27979;&#26159;&#19968;&#20010;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#30001;&#20110;&#26032;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#35201;&#27714;&#12290;&#29616;&#26377;&#30340;OOD&#25991;&#26412;&#26816;&#27979;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#32534;&#30721;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#36755;&#20986;&#19978;&#35745;&#31639;&#30340;&#24322;&#24120;&#24471;&#20998;&#65288;&#20363;&#22914;&#39532;&#27663;&#36317;&#31163;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;OOD&#26816;&#27979;&#24615;&#33021;&#22240;&#20219;&#21153;&#21644;&#23618;&#36755;&#20986;&#32780;&#24322;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#24120;&#30340;&#36873;&#25321;&#65288;&#26368;&#21518;&#19968;&#23618;&#65289;&#24456;&#23569;&#26159;OOD&#26816;&#27979;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#22914;&#26524;&#36873;&#25321;&#26368;&#20339;&#23618;&#65292;&#21017;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#32467;&#21512;&#36880;&#23618;&#30340;&#24322;&#24120;&#24471;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#25324;&#26356;&#22810;&#31867;&#21035;&#30340;&#20998;&#31867;&#20219;&#21153;&#65288;&#39640;&#36798;77&#65289;&#25193;&#23637;&#20102;&#32463;&#20856;&#25991;&#26412;OOD&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#32780;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;&#22312;&#36825;&#20010;&#22686;&#24378;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21518;&#32858;&#21512;&#26041;&#27861;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;OOD&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems. Existing OOD textual detectors often rely on an anomaly score (e.g., Mahalanobis distance) computed on the embedding output of the last layer of the encoder. In this work, we observe that OOD detection performance varies greatly depending on the task and layer output. More importantly, we show that the usual choice (the last layer) is rarely the best one for OOD detection and that far better results could be achieved if the best layer were picked. To leverage this observation, we propose a data-driven, unsupervised method to combine layer-wise anomaly scores. In addition, we extend classical textual OOD benchmarks by including classification tasks with a greater number of classes (up to 77), which reflects more realistic settings. On this augmented benchmark, we show that the proposed post-aggregation methods achieve robust an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;</title><link>http://arxiv.org/abs/2302.07849</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36866;&#24212;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#35843;&#25972;&#65292;&#29305;&#21035;&#26159;&#24403;&#27809;&#26377;&#38024;&#23545;&#8220;&#26032;&#27491;&#24120;&#8221;&#36827;&#34892;&#35757;&#32451;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23548;&#33268;&#20135;&#29983;&#20102;&#38646;&#26679;&#26412;AD&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#65288;ACR&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;AD&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;SVDD&#65289;&#26469;&#36866;&#24212;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#21152;&#20803;&#35757;&#32451;&#65292;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#21644;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#38646;&#26679;&#26412;AD&#32467;&#26524;&#65292;&#24182;&#22312;&#26469;&#33258;&#19987;&#19994;&#39046;&#22495;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#27573;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26131;&#21463;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#29616;&#26377;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#23545;&#25193;&#25955;&#27169;&#22411;&#22522;&#26412;&#26080;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.01316</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26131;&#21463;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Diffusion Models Vulnerable to Membership Inference Attacks?. (arXiv:2302.01316v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26131;&#21463;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#29616;&#26377;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#23545;&#25193;&#25955;&#27169;&#22411;&#22522;&#26412;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#23427;&#20204;&#21487;&#33021;&#24102;&#26469;&#23433;&#20840;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;GANs&#25110;VAE&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#26469;&#35828;&#22522;&#26412;&#26080;&#25928;&#65292;&#35201;&#20040;&#26159;&#22240;&#20026;&#36866;&#29992;&#22330;&#26223;&#19981;&#21516;&#65288;&#20363;&#22914;&#38656;&#35201;GANs&#30340;&#21028;&#21035;&#22120;&#65289;&#65292;&#35201;&#20040;&#26159;&#22240;&#20026;&#20551;&#35774;&#19981;&#24403;&#65288;&#20363;&#22914;&#65292;&#21512;&#25104;&#26679;&#26412;&#21644;&#25104;&#21592;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#26356;&#36817;&#65289;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;Step-wise Error Comparing Membership Inference&#65288;SecMI&#65289;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#20013;&#21069;&#21521;&#36807;&#31243;&#21518;&#39564;&#20272;&#35745;&#30340;&#21305;&#37197;&#24773;&#20917;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;SecMI&#36981;&#24490;MIA&#20013;&#30340;&#24120;&#35265;&#36807;&#25311;&#21512;&#20551;&#35774;&#65292;&#21363;&#25104;&#21592;&#26679;&#26412;&#36890;&#24120;&#20855;&#26377;&#36739;&#23567;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#32780;&#27604;&#36739;&#20445;&#25345;&#26679;&#26412;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;&#26631;&#20934;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#20999;&#27604;&#38634;&#22827;&#25193;&#23637;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#20998;&#21106;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2301.12171</link><description>&lt;p&gt;
ZegOT:&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#30340;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#38646;&#26679;&#26412;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts. (arXiv:2301.12171v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12171
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#20998;&#21106;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#23545;&#27604;&#24615;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#30340;&#25104;&#21151;&#20026;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#24076;&#26395;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#25110;&#23545;CLIP&#27169;&#22359;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ZegOT&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23558;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#19982;&#20923;&#32467;&#30340;&#22270;&#20687;&#23884;&#20837;&#21305;&#37197;&#65292;&#20174;&#32780;&#23454;&#29616;&#38646;&#26679;&#26412;&#20998;&#21106;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22810;&#25552;&#31034;&#26368;&#20248;&#20256;&#36755;&#27714;&#35299;&#22120;&#65288;MPOT&#65289;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#19982;&#20923;&#32467;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#34255;&#23618;&#30340;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#20043;&#38388;&#23398;&#20064;&#20102;&#19968;&#31181;&#26368;&#20248;&#26144;&#23556;&#12290;&#36825;&#31181;&#29420;&#29305;&#30340;&#26144;&#23556;&#26041;&#27861;&#26377;&#25928;&#22320;&#20351;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#20851;&#27880;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#20041;&#23646;&#24615;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent success of large-scale Contrastive Language-Image Pre-training (CLIP) has led to great promise in zero-shot semantic segmentation by transferring image-text aligned knowledge to pixel-level classification. However, existing methods usually require an additional image encoder or retraining/tuning the CLIP module. Here, we propose a novel Zero-shot segmentation with Optimal Transport (ZegOT) method that matches multiple text prompts with frozen image embeddings through optimal transport. In particular, we introduce a novel Multiple Prompt Optimal Transport Solver (MPOT), which is designed to learn an optimal mapping between multiple text prompts and visual feature maps of the frozen image encoder hidden layers. This unique mapping method facilitates each of the multiple text prompts to effectively focus on distinct visual semantic attributes. Through extensive experiments on benchmark datasets, we show that our method achieves the state-of-the-art (SOTA) performance over existing 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#36719;&#25552;&#31034;&#20196;&#29260;&#23884;&#20837;&#26469;&#23398;&#20064;&#20219;&#21153;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65292;&#21516;&#26102;&#22312;&#19981;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.10915</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning. (arXiv:2301.10915v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#36719;&#25552;&#31034;&#20196;&#29260;&#23884;&#20837;&#26469;&#23398;&#20064;&#20219;&#21153;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65292;&#21516;&#26102;&#22312;&#19981;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#26159;&#23545;&#35805;&#31649;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#65292;&#38656;&#35201;&#36319;&#36394;&#29992;&#25143;&#30340;&#20449;&#24565;&#29366;&#24577;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#23545;&#25152;&#26377;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#26469;&#24212;&#23545;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#20351;&#29992;&#20960;&#21313;&#20010;&#24494;&#35843;&#20102;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25152;&#38656;&#30340;&#25104;&#26412;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20026;&#20102;&#38477;&#20302;&#21442;&#25968;&#22823;&#23567;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#36328;&#20219;&#21153;&#20849;&#20139;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36719;&#25552;&#31034;&#20196;&#29260;&#23884;&#20837;&#26469;&#23398;&#20064;&#20219;&#21153;&#23646;&#24615;&#12290;&#22312;&#19981;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21442;&#25968;&#25968;&#37327;&#22823;&#24133;&#20943;&#23569;&#21040;&#23569;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;0.5&#65285;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue state tracking (DST) is an important step in dialogue management to keep track of users' beliefs. Existing works fine-tune all language model (LM) parameters to tackle the DST task, which requires significant data and computing resources for training and hosting. The cost grows exponentially in the real-world deployment where dozens of fine-tuned LM are used for different domains and tasks. To reduce parameter size and better utilize cross-task shared information, we propose to use soft prompt token embeddings to learn task properties. Without tuning LM parameters, our method drastically reduces the number of parameters needed to less than 0.5% of prior works while achieves better low-resource DST performance.
&lt;/p&gt;</description></item><item><title>&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#38656;&#27714;&#27169;&#22411;&#30340;&#22825;&#28982;&#20559;&#35265;&#23548;&#33268;&#20102;&#35266;&#27979;&#21040;&#30340;&#38656;&#27714;&#19982;&#23454;&#38469;&#38656;&#27714;&#26377;&#24046;&#24322;&#65292;&#20351;&#29992;&#26377;&#20851;&#23457;&#26597;&#30340;&#27169;&#22411;&#26469;&#27169;&#25311;&#20805;&#30005;&#38656;&#27714;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#30495;&#23454;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2301.06418</link><description>&lt;p&gt;
&#27880;&#24847;&#24046;&#36317;&#65306;&#24314;&#27169;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#38656;&#27714;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mind the Gap: Modelling Difference Between Censored and Uncensored Electric Vehicle Charging Demand. (arXiv:2301.06418v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06418
&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#38656;&#27714;&#27169;&#22411;&#30340;&#22825;&#28982;&#20559;&#35265;&#23548;&#33268;&#20102;&#35266;&#27979;&#21040;&#30340;&#38656;&#27714;&#19982;&#23454;&#38469;&#38656;&#27714;&#26377;&#24046;&#24322;&#65292;&#20351;&#29992;&#26377;&#20851;&#23457;&#26597;&#30340;&#27169;&#22411;&#26469;&#27169;&#25311;&#20805;&#30005;&#38656;&#27714;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#30495;&#23454;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20805;&#30005;&#35760;&#24405;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#38656;&#27714;&#27169;&#22411;&#20250;&#22825;&#28982;&#22320;&#23545;&#21487;&#29992;&#20805;&#30005;&#22120;&#30340;&#20379;&#24212;&#20135;&#29983;&#20559;&#35265;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#32771;&#34385;&#21040;&#34987;&#21344;&#29992;&#20805;&#30005;&#31449;&#21644;&#31454;&#20105;&#23545;&#20805;&#30005;&#38656;&#27714;&#30340;&#25439;&#22833;&#12290;&#36825;&#20123;&#25439;&#22833;&#34920;&#26126;&#23454;&#38469;&#38656;&#27714;&#24456;&#21487;&#33021;&#27604;&#20805;&#30005;&#35760;&#24405;&#21453;&#26144;&#30340;&#38656;&#27714;&#26356;&#39640;&#65292;&#21363;&#30495;&#23454;&#38656;&#27714;&#26159;&#28508;&#22312;&#30340;&#65288;&#26410;&#35266;&#23519;&#21040;&#30340;&#65289;&#65292;&#32780;&#35266;&#23519;&#32467;&#26524;&#21017;&#26159;&#34987;&#23457;&#26597;&#30340;&#12290;&#22240;&#27492;&#65292;&#20381;&#36182;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#20805;&#30005;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#22312;&#26410;&#26469;&#22522;&#30784;&#35774;&#26045;&#25193;&#23637;&#21644;&#20379;&#24212;&#31649;&#29702;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#20272;&#35745;&#20805;&#30005;&#30340;&#30495;&#23454;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26377;&#20851;&#23457;&#26597;&#30340;&#27169;&#22411;&#26469;&#27169;&#25311;&#20805;&#30005;&#38656;&#27714;&#20197;&#35299;&#20915;&#27492;&#38480;&#21046;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#23457;&#26597;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#24182;&#20174;&#35266;&#23519;&#21040;&#30340;&#20805;&#30005;&#35760;&#24405;&#20013;&#23398;&#20064;&#30495;&#23454;&#30340;&#28508;&#22312;&#38656;&#27714;&#20998;&#24067;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#34987;&#21344;&#29992;&#20805;&#30005;&#31449;&#21644;&#31454;&#20105;&#26381;&#21153;&#22914;&#20309;&#20351;&#29992;GPS&#36712;&#36857;&#23457;&#26597;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electric vehicle charging demand models, with charging records as input, will inherently be biased toward the supply of available chargers. These models often fail to account for demand lost from occupied charging stations and competitors. The lost demand suggests that the actual demand is likely higher than the charging records reflect, i.e., the true demand is latent (unobserved), and the observations are censored. As a result, machine learning models that rely on these observed records for forecasting charging demand may be limited in their application in future infrastructure expansion and supply management, as they do not estimate the true demand for charging. We propose using censorship-aware models to model charging demand to address this limitation. These models incorporate censorship in their loss functions and learn the true latent demand distribution from observed charging records. We study how occupied charging stations and competing services censor demand using GPS traject
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;SPTS v2&#26694;&#26550;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#26497;&#20302;&#25104;&#26412;&#30340;&#21333;&#28857;&#27880;&#37322;&#26469;&#35757;&#32451;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33258;&#22238;&#24402;Transformer&#19982;&#23454;&#20363;&#20998;&#37197;&#35299;&#30721;&#22120;&#65288;IAD&#65289;&#30340;&#20248;&#21183;&#65292;&#24182;&#20351;&#29992;&#24182;&#34892;&#35782;&#21035;&#35299;&#30721;&#22120;&#65288;PRD&#65289;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2301.01635</link><description>&lt;p&gt;
SPTS v2: &#21333;&#28857;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
SPTS v2: Single-Point Scene Text Spotting. (arXiv:2301.01635v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;SPTS v2&#26694;&#26550;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#26497;&#20302;&#25104;&#26412;&#30340;&#21333;&#28857;&#27880;&#37322;&#26469;&#35757;&#32451;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33258;&#22238;&#24402;Transformer&#19982;&#23454;&#20363;&#20998;&#37197;&#35299;&#30721;&#22120;&#65288;IAD&#65289;&#30340;&#20248;&#21183;&#65292;&#24182;&#20351;&#29992;&#24182;&#34892;&#35782;&#21035;&#35299;&#30721;&#22120;&#65288;PRD&#65289;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#20043;&#38388;&#30340;&#20869;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#31471;&#21040;&#31471;&#30340;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#25163;&#21160;&#26631;&#27880;&#65288;&#22914;&#27700;&#24179;&#30697;&#24418;&#12289;&#26059;&#36716;&#30697;&#24418;&#12289;&#22235;&#36793;&#24418;&#21644;&#22810;&#36793;&#24418;&#65289;&#35270;&#20026;&#24517;&#35201;&#26465;&#20214;&#65292;&#32780;&#36825;&#27604;&#20351;&#29992;&#21333;&#28857;&#35201;&#26114;&#36149;&#24471;&#22810;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36890;&#36807;&#25552;&#20986;&#30340;&#21517;&#20026;SPTS v2&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#26497;&#20302;&#25104;&#26412;&#30340;&#21333;&#28857;&#27880;&#37322;&#26469;&#35757;&#32451;&#22330;&#26223;&#25991;&#26412;&#23450;&#20301;&#27169;&#22411;&#12290;SPTS v2&#36890;&#36807;&#20197;&#39034;&#24207;&#39044;&#27979;&#21516;&#19968;&#39044;&#27979;&#24207;&#21015;&#20013;&#25152;&#26377;&#25991;&#26412;&#23454;&#20363;&#30340;&#20013;&#24515;&#28857;&#65292;&#20445;&#30041;&#20102;&#33258;&#22238;&#24402;Transformer&#19982;&#23454;&#20363;&#20998;&#37197;&#35299;&#30721;&#22120;&#65288;IAD&#65289;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20351;&#29992;&#24182;&#34892;&#35782;&#21035;&#35299;&#30721;&#22120;&#65288;PRD&#65289;&#36827;&#34892;&#25991;&#26412;&#35782;&#21035;&#12290;&#36825;&#20004;&#20010;&#35299;&#30721;&#22120;&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#36807;&#31243;&#36827;&#34892;&#20132;&#20114;&#36830;&#25509;&#65292;&#20197;&#20256;&#36882;&#26799;&#24230;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end scene text spotting has made significant progress due to its intrinsic synergy between text detection and recognition. Previous methods commonly regard manual annotations such as horizontal rectangles, rotated rectangles, quadrangles, and polygons as a prerequisite, which are much more expensive than using single-point. For the first time, we demonstrate that training scene text spotting models can be achieved with an extremely low-cost single-point annotation by the proposed framework, termed SPTS v2. SPTS v2 reserves the advantage of the auto-regressive Transformer with an Instance Assignment Decoder (IAD) through sequentially predicting the center points of all text instances inside the same predicting sequence, while with a Parallel Recognition Decoder (PRD) for text recognition in parallel. These two decoders share the same parameters and are interactively connected with a simple but effective information transmission process to pass the gradient and information. Compre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10823</link><description>&lt;p&gt;
&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#20381;&#36182;&#32467;&#26500;&#21270;&#27880;&#37322;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#26399;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#30340;RE&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;RE&#30446;&#26631;&#39044;&#35757;&#32451;&#20851;&#31995;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23427;&#38459;&#27490;RE&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20851;&#31995;&#21487;&#33021;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36731;&#26494;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#65292;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#25991;&#26723;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#26174;&#30528;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
&lt;/p&gt;</description></item><item><title>ORCA &#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#21508;&#31181;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#19968;&#31995;&#21015;&#26377;&#25361;&#25112;&#24615;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#26500;&#24314;&#12290;&#24403;&#21069;&#20351;&#29992; ORCA &#23545; 18 &#20010;&#22810;&#35821;&#35328;&#21644;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2212.10758</link><description>&lt;p&gt;
ORCA: &#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ORCA: A Challenging Benchmark for Arabic Language Understanding. (arXiv:2212.10758v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10758
&lt;/p&gt;
&lt;p&gt;
ORCA &#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#21508;&#31181;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#19968;&#31995;&#21015;&#26377;&#25361;&#25112;&#24615;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#26500;&#24314;&#12290;&#24403;&#21069;&#20351;&#29992; ORCA &#23545; 18 &#20010;&#22810;&#35821;&#35328;&#21644;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#25152;&#26377; NLP &#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24050;&#25552;&#20986;&#20102;&#22810;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#30446;&#21069;&#23578;&#19981;&#23384;&#22312;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30340;&#22810;&#26679;&#21270;&#20844;&#20849;&#22522;&#20934;&#12290;&#36825;&#20351;&#24471;&#21516;&#26102;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20010;&#25361;&#25112;&#36824;&#22240;&#38463;&#25289;&#20271;&#35821;&#19981;&#26159;&#21333;&#19968;&#35821;&#35328;&#32780;&#26159;&#19968;&#31995;&#21015;&#35821;&#35328;&#21644;&#26041;&#35328;&#32780;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; ORCA&#65292;&#19968;&#39033;&#20844;&#24320;&#21487;&#29992;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#12290;ORCA &#34987;&#31934;&#24515;&#26500;&#24314;&#65292;&#20197;&#35206;&#30422;&#22810;&#31181;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#19968;&#31995;&#21015;&#26377;&#25361;&#25112;&#24615;&#30340;&#38463;&#25289;&#20271;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#21033;&#29992;&#19971;&#20010; NLU &#20219;&#21153;&#38598;&#32676;&#20013;&#30340; 60 &#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#34913;&#37327;&#24403;&#21069;&#38463;&#25289;&#20271;&#35821; NLU &#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#20351;&#29992; ORCA &#22312; 18 &#20010;&#22810;&#35821;&#35328;&#21644;&#38463;&#25289;&#20271;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#20102;&#20840;&#38754;&#23545;&#27604;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#25490;&#34892;&#27036;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their crucial role in all NLP, several benchmarks have been proposed to evaluate pretrained language models. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluation of Arabic. This makes it challenging to measure progress for both Arabic and multilingual language models. This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but rather a collection of languages and varieties. In this work, we introduce ORCA, a publicly available benchmark for Arabic language understanding evaluation. ORCA is carefully constructed to cover diverse Arabic varieties and a wide range of challenging Arabic understanding tasks exploiting 60 different datasets across seven NLU task clusters. To measure current progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between 18 multilingual and Arabic language models. We also provide a public leaderboard 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10511</link><description>&lt;p&gt;
&#20309;&#26102;&#19981;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;&#65306;&#25506;&#32034;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#35760;&#24518;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#26263;&#31034;&#20102;&#20165;&#20381;&#38752;&#20854;&#21442;&#25968;&#26469;&#32534;&#30721;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#22312;PopQA&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30693;&#35782;&#25506;&#27979;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38271;&#23614;&#20013;&#65292;&#25193;&#23637;&#35268;&#27169;&#26080;&#27861;&#26126;&#26174;&#25913;&#21892;&#35760;&#24518;&#23454;&#38469;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32988;&#36807;&#32423;&#21035;&#22823;&#24471;&#22810;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26410;&#32463;&#21327;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#39640;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#38656;&#35201;&#26102;&#26816;&#32034;&#38750;&#21442;&#25968;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
&lt;/p&gt;</description></item><item><title>LMP&#23558;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20174;&#32431;&#25991;&#26412;&#25552;&#31034;&#25193;&#23637;&#20026;&#25991;&#26412;&#25552;&#31034;&#21644;&#33050;&#26412;&#30340;&#30452;&#35266;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#32534;&#31243;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2212.06094</link><description>&lt;p&gt;
Prompting&#23601;&#26159;&#32534;&#31243;: &#19968;&#31181;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Prompting Is Programming: A Query Language for Large Language Models. (arXiv:2212.06094v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06094
&lt;/p&gt;
&lt;p&gt;
LMP&#23558;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20174;&#32431;&#25991;&#26412;&#25552;&#31034;&#25193;&#23637;&#20026;&#25991;&#26412;&#25552;&#31034;&#21644;&#33050;&#26412;&#30340;&#30452;&#35266;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#32534;&#31243;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#20174;&#39640;&#23618;&#27425;&#19978;&#35762;&#65292;&#32473;&#23450;&#36755;&#20837;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#32479;&#35745;&#19978;&#30340;&#21487;&#33021;&#24615;&#33258;&#21160;&#23436;&#25104;&#24207;&#21015;&#12290;&#22522;&#20110;&#27492;&#65292;&#29992;&#25143;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#25110;&#31034;&#20363;&#26469;&#25552;&#31034;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#39640;&#32423;&#25552;&#31034;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#26263;&#31034;&#27169;&#22411;&#12289;&#29992;&#25143;&#21644;&#35745;&#31639;&#22120;&#31561;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#25110;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24517;&#39035;&#23454;&#29616;&#22797;&#26434;&#30340;&#20219;&#21153;-&#21644;&#27169;&#22411;&#29305;&#23450;&#30340;&#31243;&#24207;&#65292;&#36825;&#20173;&#28982;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#30340;&#20132;&#20114;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#32534;&#31243;&#65288;LMP&#65289;&#30340;&#26032;&#27010;&#24565;&#12290;LMP&#23558;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20174;&#32431;&#25991;&#26412;&#25552;&#31034;&#25193;&#23637;&#20026;&#25991;&#26412;&#25552;&#31034;&#21644;&#33050;&#26412;&#30340;&#30452;&#35266;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;LMP&#20801;&#35768;&#25351;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20013;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#31995;&#32479;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#27700;&#24179;&#65292;&#24110;&#21161;&#27979;&#35797;&#35774;&#35745;&#20154;&#21592;&#30830;&#20445;&#20351;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#38382;&#39064;&#26159;&#21487;&#25509;&#21463;&#30340;&#65292;&#21516;&#26102;&#21487;&#20197;&#27979;&#35797;&#38382;&#39064;&#30340;&#36136;&#37327;&#24182;&#21457;&#29616;&#35774;&#35745;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.07040</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#19990;&#30028;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
World Knowledge in Multiple Choice Reading Comprehension. (arXiv:2211.07040v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20013;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#35780;&#20272;&#31995;&#32479;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#27700;&#24179;&#65292;&#24110;&#21161;&#27979;&#35797;&#35774;&#35745;&#20154;&#21592;&#30830;&#20445;&#20351;&#29992;&#19990;&#30028;&#30693;&#35782;&#30340;&#38382;&#39064;&#26159;&#21487;&#25509;&#21463;&#30340;&#65292;&#21516;&#26102;&#21487;&#20197;&#27979;&#35797;&#38382;&#39064;&#30340;&#36136;&#37327;&#24182;&#21457;&#29616;&#35774;&#35745;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#35821;&#22659;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#65288;MCRC&#65289;&#31995;&#32479;&#33021;&#22815;&#27604;&#38543;&#26426;&#22238;&#31572;&#38382;&#39064;&#35201;&#22909;&#24471;&#22810;&#12290;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#20182;&#20204;&#31215;&#32047;&#30340;&#8220;&#19990;&#30028;&#30693;&#35782;&#8221;&#26469;&#30452;&#25509;&#22238;&#31572;&#38382;&#39064;&#65292;&#32780;&#19981;&#20351;&#29992;&#26469;&#33258;&#27573;&#33853;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#20316;&#20026;&#27979;&#35797;&#35774;&#35745;&#24037;&#20855;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#30830;&#20445;&#22312;&#29305;&#23450;&#30340;&#19968;&#32452;&#38382;&#39064;&#20013;&#20351;&#29992;&#8220;&#19990;&#30028;&#30693;&#35782;&#8221;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#31995;&#32479;&#21033;&#29992;"&#19990;&#30028;&#30693;&#35782;"&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#65306;&#26399;&#26395;&#36873;&#39033;&#25968;&#65292;&#23427;&#27979;&#37327;&#20102;&#26080;&#38656;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#31995;&#32479;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#26469;&#30830;&#23450;&#19968;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#65307;&#35789;&#27719;&#20114;&#20449;&#24687;&#65292;&#23427;&#27979;&#37327;&#20102;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#26469;&#35828;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#37027;&#20123;&#26399;&#26395;&#36873;&#39033;&#25968;&#36739;&#20302;&#30340;&#38382;&#39064;&#65292;&#21363;&#21487;&#20197;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#26469;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#34987;&#20934;&#30830;&#22320;&#37492;&#23450;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#27979;&#35797;&#38382;&#39064;&#30340;&#36136;&#37327;&#24182;&#30830;&#23450;&#27979;&#35797;&#35774;&#35745;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently it has been shown that without any access to the contextual passage, multiple choice reading comprehension (MCRC) systems are able to answer questions significantly better than random on average. These systems use their accumulated "world knowledge" to directly answer questions, rather than using information from the passage. This paper examines the possibility of exploiting this observation as a tool for test designers to ensure that the use of "world knowledge" is acceptable for a particular set of questions. We propose information-theory based metrics that enable the level of "world knowledge" exploited by systems to be assessed. Two metrics are described: the expected number of options, which measures whether a passage-free system can identify the answer a question using world knowledge; and the contextual mutual information, which measures the importance of context for a given question. We demonstrate that questions with low expected number of options, and hence answerabl
&lt;/p&gt;</description></item><item><title>FedGen&#20026;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#21487;&#25512;&#24191;&#24615;&#65292;&#20801;&#35768;&#20998;&#24067;&#24335;&#35774;&#22791;&#20849;&#21516;&#35782;&#21035;&#21644;&#21306;&#20998;&#20266;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#24067;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2211.01914</link><description>&lt;p&gt;
FedGen: &#36866;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#30340;&#21487;&#25512;&#24191;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedGen: Generalizable Federated Learning for Sequential Data. (arXiv:2211.01914v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01914
&lt;/p&gt;
&lt;p&gt;
FedGen&#20026;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#21487;&#25512;&#24191;&#24615;&#65292;&#20801;&#35768;&#20998;&#24067;&#24335;&#35774;&#22791;&#20849;&#21516;&#35782;&#21035;&#21644;&#21306;&#20998;&#20266;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#24067;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36981;&#24490;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#23384;&#22312;&#20266;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#24448;&#24448;&#38590;&#20197;&#25512;&#24191;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20998;&#24067;&#24335;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#20998;&#24067;&#24335;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#30340;&#20559;&#24046;&#21644;&#25968;&#25454;&#37319;&#26679;&#38382;&#39064;&#65292;&#20250;&#20135;&#29983;&#20266;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#38169;&#35823;&#22320;&#24433;&#21709;&#27169;&#22411;&#12290;&#24403;&#21069;&#30340;&#25512;&#24191;&#26041;&#27861;&#26159;&#20026;&#38598;&#20013;&#24335;&#35757;&#32451;&#35774;&#35745;&#30340;&#65292;&#35797;&#22270;&#35782;&#21035;&#20855;&#26377;&#19982;&#30446;&#26631;&#19981;&#21464;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20943;&#23569;&#20266;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38590;&#20197;&#33719;&#24471;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedGen&#30340;&#21487;&#25512;&#24191;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20801;&#35768;&#23458;&#25143;&#31471;&#20197;&#21327;&#20316;&#30340;&#26041;&#24335;&#35782;&#21035;&#21644;&#21306;&#20998;&#20266;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#24067;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing federated learning models that follow the standard risk minimization paradigm of machine learning often fail to generalize in the presence of spurious correlations in the training data. In many real-world distributed settings, spurious correlations exist due to biases and data sampling issues on distributed devices or clients that can erroneously influence models. Current generalization approaches are designed for centralized training and attempt to identify features that have an invariant causal relationship with the target, thereby reducing the effect of spurious features. However, such invariant risk minimization approaches rely on apriori knowledge of training data distributions which is hard to obtain in many applications. In this work, we present a generalizable federated learning framework called FedGen, which allows clients to identify and distinguish between spurious and invariant features in a collaborative manner without prior knowledge of training distributions. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#24418;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#33258;&#28982;&#21644;&#30452;&#35266;&#30340;&#25512;&#29702;&#36807;&#21435;&#26102;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00472</link><description>&lt;p&gt;
&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Backtracking Counterfactuals. (arXiv:2211.00472v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#24418;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#33258;&#28982;&#21644;&#30452;&#35266;&#30340;&#25512;&#29702;&#36807;&#21435;&#26102;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#26159;&#20154;&#31867;&#24605;&#32500;&#20013;&#24191;&#27867;&#23384;&#22312;&#30340;&#19968;&#31181;&#25512;&#29702;&#26041;&#24335;&#8212;&#8212;&#35774;&#24819;&#19968;&#20123;&#20551;&#35774;&#22330;&#26223;&#25110;&#21487;&#33021;&#23384;&#22312;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#24773;&#20917;&#19982;&#23454;&#38469;&#24773;&#20917;&#19981;&#21516;&#12290;&#20256;&#32479;&#19978;&#65292;&#21453;&#20107;&#23454;&#24773;&#26223;&#34987;&#35270;&#20026;&#23616;&#37096;&#36829;&#21453;&#33258;&#28982;&#35268;&#24459;&#30340;&#8220;&#23567;&#22855;&#36857;&#8221;&#65292;&#20294;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#32780;&#22312;Pearl&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#26694;&#26550;&#20013;&#65292;&#36825;&#36890;&#36807;&#20462;&#25913;&#22240;&#26524;&#23450;&#24459;&#30340;&#24178;&#39044;&#32780;&#20351;&#24471;&#22806;&#29983;&#21464;&#37327;&#30340;&#20540;&#20849;&#20139;&#24471;&#21040;&#20102;&#25968;&#23398;&#19978;&#30340;&#20005;&#26684;&#21270;&#12290;&#20294;&#36817;&#24180;&#26469;&#65292;&#21746;&#23398;&#23478;&#21644;&#24515;&#29702;&#23398;&#23478;&#23545;&#36825;&#31181;&#21333;&#32431;&#30340;&#24178;&#39044;&#20027;&#20041;&#21453;&#20107;&#23454;&#35266;&#28857;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#36136;&#30097;&#12290;&#30456;&#21453;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#35266;&#28857;&#65292;&#21363;&#22312;&#21453;&#20107;&#23454;&#19990;&#30028;&#20013;&#22240;&#26524;&#23450;&#24459;&#20445;&#25345;&#19981;&#21464;&#65292;&#23558;&#19982;&#23454;&#38469;&#24773;&#20917;&#30340;&#24046;&#24322;&#8220;&#22238;&#28335;&#8221;&#21040;&#25913;&#21464;&#30340;&#21021;&#22987;&#26465;&#20214;(&#22806;&#29983;&#21464;&#37327;)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#20294;&#28789;&#27963;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#22238;&#28335;&#24335;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#29702;&#36807;&#21435;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#12289;&#26356;&#30452;&#35266;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning -- envisioning hypothetical scenarios, or possible worlds, where some circumstances are different from what (f)actually occurred (counter-to-fact) -- is ubiquitous in human cognition. Conventionally, counterfactually-altered circumstances have been treated as "small miracles" that locally violate the laws of nature while sharing the same initial conditions. In Pearl's structural causal model (SCM) framework this is made mathematically rigorous via interventions that modify the causal laws while the values of exogenous variables are shared. In recent years, however, this purely interventionist account of counterfactuals has increasingly come under scrutiny from both philosophers and psychologists. Instead, they suggest a backtracking account of counterfactuals, according to which the causal laws remain unchanged in the counterfactual world; differences to the factual world are instead "backtracked" to altered initial conditions (exogenous variables). In the pres
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.17426</link><description>&lt;p&gt;
&#20613;&#31435;&#21494;&#20998;&#26512;&#23454;&#29616;&#19968;&#33268;&#19988;&#30495;&#23454;&#30340;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#36328;&#23398;&#31185;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#37322;&#38656;&#35201;&#19982;&#24403;&#21069;&#26696;&#20363;&#30456;&#20851;&#30340;&#20551;&#35774;&#24773;&#26223;&#19968;&#33268;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#22240;&#32032;&#25913;&#21464;&#65292;&#27169;&#22411;&#20250;&#22914;&#20309;&#21453;&#24212;&#65311;&#23613;&#31649;&#24402;&#22240;&#26041;&#27861;&#30001;&#20248;&#38597;&#30340;&#20844;&#29702;&#31995;&#32479;&#25903;&#25345;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20837;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#19968;&#33268;&#12290;&#20026;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#24067;&#23572;&#20989;&#25968;&#30340;&#20613;&#31435;&#21494;&#20998;&#26512;&#26469;&#33719;&#24471;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#21508;&#31181;&#21322;&#24452;&#30340;&#37051;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#23454;&#29616;2&#20493;&#33267;50&#20493;&#26356;&#20302;&#30340;&#35299;&#37322;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22810;&#23646;&#24615;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#24230;&#37327;&#21487;&#33021;&#20250;&#24573;&#30053;&#22810;&#20010;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21487;&#33021;&#38169;&#35823;&#22320;&#32473;&#20986;&#24230;&#37327;&#32467;&#26524;&#65292;&#20351;&#24471;&#26368;&#23567;&#25110;&#27809;&#26377;&#20559;&#35265;&#25918;&#22823;&#30340;&#24773;&#20917;&#34987;&#35823;&#21028;&#12290;</title><link>http://arxiv.org/abs/2210.11924</link><description>&lt;p&gt;
&#30007;&#24615;&#20063;&#27927;&#34915;&#26381;&#65306;&#22810;&#23646;&#24615;&#20559;&#35265;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Men Also Do Laundry: Multi-Attribute Bias Amplification. (arXiv:2210.11924v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22810;&#23646;&#24615;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#24230;&#37327;&#21487;&#33021;&#20250;&#24573;&#30053;&#22810;&#20010;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21487;&#33021;&#38169;&#35823;&#22320;&#32473;&#20986;&#24230;&#37327;&#32467;&#26524;&#65292;&#20351;&#24471;&#26368;&#23567;&#25110;&#27809;&#26377;&#20559;&#35265;&#25918;&#22823;&#30340;&#24773;&#20917;&#34987;&#35823;&#21028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#30740;&#31350;&#30028;&#21644;&#20844;&#20247;&#23545;&#36825;&#20123;&#31995;&#32479;&#19981;&#20165;&#20250;&#22797;&#21046;&#65292;&#32780;&#19988;&#20250;&#25918;&#22823;&#26377;&#23475;&#31038;&#20250;&#20559;&#35265;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#25991;&#25152;&#20851;&#27880;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26159;&#25351;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#25918;&#22823;&#22266;&#26377;&#30340;&#35757;&#32451;&#38598;&#20559;&#35265;&#12290;&#29616;&#26377;&#30340;&#25351;&#26631;&#38024;&#23545;&#21333;&#20010;&#27880;&#37322;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#8220;&#30005;&#33041;&#8221;&#65289;&#27979;&#37327;&#20559;&#35265;&#25918;&#22823;&#12290;&#28982;&#32780;&#65292;&#20960;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#30001;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#27880;&#37322;&#30340;&#22270;&#20687;&#32452;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21033;&#29992;&#22810;&#20010;&#23646;&#24615;&#65288;&#20363;&#22914;{&#8220;&#30005;&#33041;&#8221;&#65292;&#8220;&#38190;&#30424;&#8221;}&#65289;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#36825;&#20123;&#30456;&#20851;&#24615;&#19981;&#34987;&#24403;&#21069;&#30340;&#24230;&#37327;&#25152;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#21069;&#30340;&#24230;&#37327;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#32473;&#20986;&#26368;&#23567;&#25110;&#27809;&#26377;&#20559;&#35265;&#25918;&#22823;&#30340;&#21360;&#35937;&#65292;&#22240;&#20026;&#23427;&#20204;&#28041;&#21450;&#23545;&#27491;&#20540;&#21644;&#36127;&#20540;&#36827;&#34892;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25351;&#26631;&#32570;&#20047;&#26126;&#30830;&#30340;&#26399;&#26395;&#20540;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
As computer vision systems become more widely deployed, there is increasing concern from both the research community and the public that these systems are not only reproducing but amplifying harmful social biases. The phenomenon of bias amplification, which is the focus of this work, refers to models amplifying inherent training set biases at test time. Existing metrics measure bias amplification with respect to single annotated attributes (e.g., $\texttt{computer}$). However, several visual datasets consist of images with multiple attribute annotations. We show models can learn to exploit correlations with respect to multiple attributes (e.g., {$\texttt{computer}$, $\texttt{keyboard}$}), which are not accounted for by current metrics. In addition, we show current metrics can give the erroneous impression that minimal or no bias amplification has occurred as they involve aggregating over positive and negative values. Further, these metrics lack a clear desired value, making them diffic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Call Graph Evolution Analytics&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36719;&#20214;&#31995;&#32479;&#29256;&#26412;&#24207;&#21015;&#30340;&#28436;&#21270;&#35843;&#29992;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36890;&#36807;&#20998;&#26512;&#28436;&#21270;&#35843;&#29992;&#22270;&#30340;&#20381;&#36182;&#20851;&#31995;&#27169;&#24335;&#30340;&#28436;&#21270;&#65292;&#24110;&#21161;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#29256;&#26412;&#28436;&#21270;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2210.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#36719;&#20214;&#31995;&#32479;&#29256;&#26412;&#24207;&#21015;&#19978;&#30340;&#35843;&#29992;&#22270;&#28436;&#21270;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Call Graph Evolution Analytics over a Version Series of an Evolving Software System. (arXiv:2210.08316v1 [cs.SE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08316
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Call Graph Evolution Analytics&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36719;&#20214;&#31995;&#32479;&#29256;&#26412;&#24207;&#21015;&#30340;&#28436;&#21270;&#35843;&#29992;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36890;&#36807;&#20998;&#26512;&#28436;&#21270;&#35843;&#29992;&#22270;&#30340;&#20381;&#36182;&#20851;&#31995;&#27169;&#24335;&#30340;&#28436;&#21270;&#65292;&#24110;&#21161;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#29256;&#26412;&#28436;&#21270;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#29992;&#22270;&#28436;&#21270;&#20998;&#26512;&#21487;&#20197;&#24110;&#21161;&#36719;&#20214;&#24037;&#31243;&#24072;&#22312;&#32500;&#25252;&#25110;&#28436;&#36827;&#36719;&#20214;&#31995;&#32479;&#26102;&#36827;&#34892;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Call Graph Evolution Analytics&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36719;&#20214;&#31995;&#32479;&#29256;&#26412;&#24207;&#21015; VS = V_1, V_2, &#8230; V_N &#30340;&#28436;&#21270;&#35843;&#29992;&#22270; ECG = CG_1, CG_2, &#8230; CG_N &#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;Call Graph Evolution Rules&#65288;CGERs&#65289;&#21644;Call Graph Evolution Subgraphs&#65288;CGESs&#65289;&#23436;&#25104;&#30340;&#12290;&#31867;&#20284;&#20110;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#65292;CGERs&#29992;&#20110;&#25429;&#33719;&#31995;&#32479;&#20013;&#20381;&#36182;&#20851;&#31995;&#30340;&#20849;&#29616;&#12290;&#19982;&#35843;&#29992;&#22270;&#20013;&#30340;&#23376;&#22270;&#27169;&#24335;&#31867;&#20284;&#65292;CGESs&#29992;&#20110;&#25429;&#33719;&#28436;&#21270;&#35843;&#29992;&#22270;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#27169;&#24335;&#30340;&#28436;&#21270;&#12290;&#23545;&#36825;&#20123;&#27169;&#24335;&#30340;&#28436;&#21270;&#36827;&#34892;&#35843;&#29992;&#22270;&#20998;&#26512;&#21487;&#20197;&#35782;&#21035;&#20986;&#38656;&#35201;&#20851;&#27880;&#30340;&#28508;&#22312;&#21463;&#24433;&#21709;&#30340;&#20381;&#36182;&#20851;&#31995;&#65288;&#25110;&#36807;&#31243;&#35843;&#29992;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Call Graph evolution analytics can aid a software engineer when maintaining or evolving a software system. This paper proposes Call Graph Evolution Analytics to extract information from an evolving call graph ECG = CG_1, CG_2,... CG_N for their version series VS = V_1, V_2, ... V_N of an evolving software system. This is done using Call Graph Evolution Rules (CGERs) and Call Graph Evolution Subgraphs (CGESs). Similar to association rule mining, the CGERs are used to capture co-occurrences of dependencies in the system. Like subgraph patterns in a call graph, the CGESs are used to capture evolution of dependency patterns in evolving call graphs. Call graph analytics on the evolution in these patterns can identify potentially affected dependencies (or procedure calls) that need attention. The experiments are done on the evolving call graphs of 10 large evolving systems to support dependency evolution management. We also consider results from a detailed study for evolving call graphs of M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26680;&#22343;&#20540;&#27744;&#65288;KAP&#65289;&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#33258;&#28982;&#22320;&#20135;&#29983;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#26680;&#38598;&#21512;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00062</link><description>&lt;p&gt;
&#20351;&#29992;&#26680;&#22343;&#20540;&#27744;&#21270;&#23398;&#20064;&#40065;&#26834;&#30340;&#26680;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Kernel Ensembles with Kernel Average Pooling. (arXiv:2210.00062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26680;&#22343;&#20540;&#27744;&#65288;KAP&#65289;&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#33258;&#28982;&#22320;&#20135;&#29983;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#26680;&#38598;&#21512;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#38598;&#25104;&#19968;&#30452;&#34987;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20197;&#20943;&#23569;&#20010;&#21035;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#24046;&#65292;&#20351;&#20854;&#23545;&#36755;&#20837;&#25200;&#21160;&#26356;&#21152;&#40065;&#26834;&#12290;&#20551;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;dropout&#65289;&#20063;&#24120;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#22343;&#20540;&#27744;&#65288;KAP&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27169;&#22359;&#65292;&#21487;&#27839;&#30528;&#23618;&#28608;&#27963;&#24352;&#37327;&#30340;&#26680;&#32500;&#24230;&#24212;&#29992;&#22343;&#20540;&#28388;&#27874;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;KAP&#20197;&#21450;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#26680;&#38598;&#21512;&#33258;&#28982;&#22320;&#20135;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#30340;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;KAP&#27169;&#22411;&#23545;&#21508;&#31181;&#24418;&#24335;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;CIFAR10&#12289;CIFAR100&#12289;TinyImagenet&#21644;Imagenet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce Kernel Average Pooling (KAP), a neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when trained on inputs perturbed with additive Gaussian noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#25277;&#21462;&#24335;&#24635;&#32467;&#20013;&#23384;&#22312;&#30340;&#20116;&#31181;&#24191;&#27867;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;30%&#30340;&#25277;&#21462;&#24335;&#25688;&#35201;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#38382;&#39064;&#12290;&#20026;&#20102;&#33258;&#21160;&#26816;&#27979;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; ExtEval&#12290;</title><link>http://arxiv.org/abs/2209.03549</link><description>&lt;p&gt;
&#25277;&#21462;&#24335;&#24635;&#32467;&#30340;&#19981;&#24544;&#23454;&#24615;&#65306;&#24191;&#27867;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization. (arXiv:2209.03549v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#25277;&#21462;&#24335;&#24635;&#32467;&#20013;&#23384;&#22312;&#30340;&#20116;&#31181;&#24191;&#27867;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;30%&#30340;&#25277;&#21462;&#24335;&#25688;&#35201;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#38382;&#39064;&#12290;&#20026;&#20102;&#33258;&#21160;&#26816;&#27979;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; ExtEval&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25277;&#35937;&#24335;&#24635;&#32467;&#30340;&#32972;&#26223;&#19979;&#65292;&#19981;&#24544;&#23454;&#24635;&#32467;&#30340;&#38382;&#39064;&#24050;&#32463;&#34987;&#24191;&#27867;&#35752;&#35770;&#12290;&#34429;&#28982;&#30456;&#36739;&#20110;&#25277;&#35937;&#24335;&#24635;&#32467;&#65292;&#25277;&#21462;&#24335;&#24635;&#32467;&#26356;&#23569;&#20542;&#21521;&#20110;&#26222;&#36941;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#65292;&#20294;&#36825;&#26159;&#21542;&#24847;&#21619;&#30528;&#25277;&#21462;&#24335;&#24635;&#32467;&#31561;&#21516;&#20110;&#24544;&#23454;&#21602;&#65311;&#32467;&#26524;&#35777;&#26126;&#24182;&#38750;&#22914;&#27492;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#20116;&#31181;&#24191;&#27867;&#30340;&#19981;&#24544;&#23454;&#38382;&#39064;&#65288;&#21253;&#25324;&#21644;&#36229;&#20986;&#38750;&#34164;&#21547;&#65289;&#30340;&#20998;&#31867;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#20986;&#29616;&#22312;&#25277;&#21462;&#24335;&#24635;&#32467;&#20013;&#65292;&#21253;&#25324;&#19981;&#27491;&#30830;&#30340;&#20849;&#25351;&#12289;&#19981;&#23436;&#25972;&#30340;&#20849;&#25351;&#12289;&#19981;&#27491;&#30830;&#30340;&#35805;&#35821;&#12289;&#19981;&#23436;&#25972;&#30340;&#35805;&#35821;&#65292;&#20197;&#21450;&#20854;&#20182;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35201;&#27714;&#20154;&#31867;&#23545;&#30001;16&#20010;&#19981;&#21516;&#30340;&#25277;&#21462;&#24335;&#31995;&#32479;&#20135;&#29983;&#30340;1600&#31687;&#33521;&#25991;&#25688;&#35201;&#36827;&#34892;&#26631;&#27880;&#65292;&#25105;&#20204;&#21457;&#29616;30%&#30340;&#25688;&#35201;&#20013;&#33267;&#23569;&#23384;&#22312;&#20116;&#20010;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#12290;&#20026;&#20102;&#33258;&#21160;&#26816;&#27979;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#29616;5&#31181;&#29616;&#26377;&#30340;&#24635;&#32467;&#24544;&#23454;&#24230;&#35780;&#20272;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#21028;&#30340;&#30456;&#20851;&#24615;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; ExtEval&#12290;
&lt;/p&gt;
&lt;p&gt;
The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is no. In this work, we define a typology with five types of broad unfaithfulness problems (including and beyond not-entailment) that can appear in extractive summaries, including incorrect coreference, incomplete coreference, incorrect discourse, incomplete discourse, as well as other misleading information. We ask humans to label these problems out of 1600 English summaries produced by 16 diverse extractive systems. We find that 30% of the summaries have at least one of the five issues. To automatically detect these problems, we find that 5 existing faithfulness evaluation metrics for summarization have poor correlations with human judgment. To remedy this, we propose a new metric, ExtEval, that
&lt;/p&gt;</description></item><item><title>PD-MORL&#26159;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#37319;&#29992;&#20559;&#22909;&#20316;&#20026;&#25351;&#23548;&#65292;&#36866;&#24212;&#20110;&#21508;&#31181;&#20559;&#22909;&#31354;&#38388;&#30340;&#30446;&#26631;&#65292;&#21487;&#25193;&#23637;&#21040;&#36830;&#32493;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;MORL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.07914</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;PD-MORL
&lt;/p&gt;
&lt;p&gt;
PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm. (arXiv:2208.07914v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07914
&lt;/p&gt;
&lt;p&gt;
PD-MORL&#26159;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#37319;&#29992;&#20559;&#22909;&#20316;&#20026;&#25351;&#23548;&#65292;&#36866;&#24212;&#20110;&#21508;&#31181;&#20559;&#22909;&#31354;&#38388;&#30340;&#30446;&#26631;&#65292;&#21487;&#25193;&#23637;&#21040;&#36830;&#32493;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;MORL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#30001;&#20559;&#22909;&#21521;&#37327;&#21152;&#26435;&#30340;&#32852;&#21512;&#30446;&#26631;&#20989;&#25968;&#65292;&#38024;&#23545;&#22810;&#20010;&#20914;&#31361;&#30446;&#26631;&#30340;&#23454;&#38469;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#35774;&#35745;&#32422;&#26463;&#21644;&#30446;&#26631;&#36890;&#24120;&#20250;&#21160;&#24577;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#23384;&#20648;&#27599;&#20010;&#28508;&#22312;&#20559;&#22909;&#30340;&#31574;&#30053;&#26159;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#32473;&#23450;&#22495;&#20013;&#20351;&#29992;&#21333;&#27425;&#35757;&#32451;&#33719;&#21462;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#35299;&#38598;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MORL&#31639;&#27861;&#65292;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#36890;&#29992;&#32593;&#32476;&#20197;&#35206;&#30422;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#36830;&#32493;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#8220;&#22522;&#20110;&#20559;&#22909;&#30340;MORL&#65288;PD-MORL&#65289;&#8221;&#21033;&#29992;&#20559;&#22909;&#20197;&#26356;&#26032;&#32593;&#32476;&#21442;&#25968;&#65292;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#21270;&#26041;&#27861;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#36866;&#24212;&#24403;&#21069;&#24773;&#20917;&#19979;&#30340;&#21508;&#31181;&#20559;&#22909;&#31354;&#38388;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#35780;&#20272;PD-MORL&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#24085;&#32047;&#25176;&#21069;&#27839;&#35206;&#30422;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;MORL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective reinforcement learning (MORL) approaches have emerged to tackle many real-world problems with multiple conflicting objectives by maximizing a joint objective function weighted by a preference vector. These approaches find fixed customized policies corresponding to preference vectors specified during training. However, the design constraints and objectives typically change dynamically in real-life scenarios. Furthermore, storing a policy for each potential preference is not scalable. Hence, obtaining a set of Pareto front solutions for the entire preference space in a given domain with a single training is critical. To this end, we propose a novel MORL algorithm that trains a single universal network to cover the entire preference space scalable to continuous robotic tasks. The proposed approach, Preference-Driven MORL (PD-MORL), utilizes the preferences as guidance to update the network parameters. It also employs a novel parallelization approach to increase sample effi
&lt;/p&gt;</description></item><item><title>RLang&#26159;&#19968;&#31181;&#22768;&#26126;&#24615;&#35821;&#35328;&#65292;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#37096;&#20998;&#19990;&#30028;&#27169;&#22411;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#34920;&#26684;&#31639;&#27861;&#12289;&#31574;&#30053;&#26799;&#24230;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#12289;&#20998;&#23618;&#26041;&#27861;&#21644;&#28145;&#24230;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.06448</link><description>&lt;p&gt;
RLang&#65306;&#19968;&#31181;&#25551;&#36848;&#37096;&#20998;&#39046;&#22495;&#30693;&#35782;&#32473;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#22768;&#26126;&#24615;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents. (arXiv:2208.06448v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06448
&lt;/p&gt;
&lt;p&gt;
RLang&#26159;&#19968;&#31181;&#22768;&#26126;&#24615;&#35821;&#35328;&#65292;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#37096;&#20998;&#19990;&#30028;&#27169;&#22411;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#34920;&#26684;&#31639;&#27861;&#12289;&#31574;&#30053;&#26799;&#24230;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#12289;&#20998;&#23618;&#26041;&#27861;&#21644;&#28145;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; RLang &#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;(DSL)&#65292;&#29992;&#20110;&#19982;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#36890;&#20449;&#12290;&#19982;&#29616;&#26377;&#30340;&#21482;&#19982;&#20915;&#31574;&#21046;&#23450;&#24418;&#24335;&#20013;&#30340;&#19968;&#20010;&#20803;&#32032;(&#22914;&#22870;&#21169;&#20989;&#25968;&#25110;&#31574;&#30053;)&#30456;&#20851;&#30340; DSL &#19981;&#21516;&#65292;RLang &#21487;&#20197;&#25351;&#23450;&#20851;&#20110;Markov&#20915;&#31574;&#36807;&#31243;&#30340;&#27599;&#20010;&#20803;&#32032;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20026;RLang&#23450;&#20041;&#20102;&#31934;&#30830;&#30340;&#35821;&#27861;&#21644;&#22522;&#30784;&#35821;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#26512;&#22120;&#65292;&#23558;RLang&#31243;&#24207;&#22522;&#20110;&#31639;&#27861;&#26102;&#19981;&#21464;&#30340;&#37096;&#20998;&#19990;&#30028;&#27169;&#22411;&#21644;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28436;&#31034;&#19981;&#21516;RL&#26041;&#27861;&#22914;&#20309;&#21033;&#29992;&#25152;&#24471;&#30693;&#35782;&#30340;RLang&#31243;&#24207;&#65292;&#21253;&#25324;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#34920;&#26684;&#31639;&#27861;&#12289;&#31574;&#30053;&#26799;&#24230;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#12289;&#20998;&#23618;&#26041;&#27861;&#21644;&#28145;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RLang, a domain-specific language (DSL) for communicating domain knowledge to an RL agent. Unlike existing RL DSLs that ground to \textit{single} elements of a decision-making formalism (e.g., the reward function or policy), RLang can specify information about every element of a Markov decision process. We define precise syntax and grounding semantics for RLang, and provide a parser that grounds RLang programs to an algorithm-agnostic \textit{partial} world model and policy that can be exploited by an RL agent. We provide a series of example RLang programs demonstrating how different RL methods can exploit the resulting knowledge, encompassing model-free and model-based tabular algorithms, policy gradient and value-based methods, hierarchical approaches, and deep methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#33647;&#29289;&#27835;&#30103;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#23519;&#30149;&#20154;&#20010;&#20307;&#30340;&#21382;&#21490;&#35760;&#24405;&#12289;&#20351;&#29992; Transformer &#27169;&#22411;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#23458;&#35266;&#39044;&#27979;&#32467;&#26524;&#65292;&#20026;&#20010;&#24615;&#21270;&#36828;&#31243;&#20581;&#24247;&#20256;&#24863;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.13700</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36828;&#31243;&#29992;&#33647;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Remote Medication Status Prediction for Individuals with Parkinson's Disease using Time-series Data from Smartphones. (arXiv:2207.13700v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#33647;&#29289;&#27835;&#30103;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#23519;&#30149;&#20154;&#20010;&#20307;&#30340;&#21382;&#21490;&#35760;&#24405;&#12289;&#20351;&#29992; Transformer &#27169;&#22411;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#23458;&#35266;&#39044;&#27979;&#32467;&#26524;&#65292;&#20026;&#20010;&#24615;&#21270;&#36828;&#31243;&#20581;&#24247;&#20256;&#24863;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#30340;&#33647;&#29289;&#27835;&#30103;&#36890;&#24120;&#26159;&#36828;&#31243;&#36827;&#34892;&#30340;&#65292;&#36828;&#31163;&#21307;&#38498;&#30340;&#29615;&#22659;&#23545;&#21450;&#26102;&#20934;&#30830;&#22320;&#25910;&#38598;&#20581;&#24247;&#29366;&#24577;&#25968;&#25454;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#31359;&#25140;&#24335;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#34892;&#20026;&#20449;&#21495;&#30340;&#20010;&#20307;&#24046;&#24322;&#20063;&#23548;&#33268;&#37319;&#29992;&#24403;&#21069;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#27969;&#31243;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20844;&#20849; mPower &#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258; 487 &#21517;&#24739;&#32773;&#30340;&#26234;&#33021;&#25163;&#26426;&#25910;&#38598;&#30340;62,182&#20010;&#36828;&#31243;&#22810;&#27169;&#24335;&#27979;&#35797;&#35760;&#24405;&#65292;&#29992;&#20110;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#29992;&#33647;&#29366;&#24577;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#23519;&#30149;&#20154;&#20010;&#20307;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#36890;&#36807; Transformer &#27169;&#22411;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#65292;&#26377;&#26395;&#22312;&#23458;&#35266;&#19978;&#39044;&#27979;&#19977;&#31181;&#19981;&#21516;&#30340;&#29992;&#33647;&#29366;&#24577;&#65306;&#29992;&#33647;&#21069;&#65288;AUC = 0.95&#65289;&#12289;&#29992;&#33647;&#21518;&#65288;AUC = 0.958&#65289;&#21644;&#20854;&#20182;&#26102;&#21051;&#65288;AUC = 0.976&#65289;&#12290;&#26412;&#26041;&#27861;&#20026;&#20010;&#24615;&#21270;&#36828;&#31243;&#20581;&#24247;&#20256;&#24863;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication for neurological diseases such as the Parkinson's disease usually happens remotely away from hospitals. Such out-of-lab environments pose challenges in collecting timely and accurate health status data. Individual differences in behavioral signals collected from wearable sensors also lead to difficulties in adopting current general machine learning analysis pipelines. To address these challenges, we present a method for predicting the medication status of Parkinson's disease patients using the public mPower dataset, which contains 62,182 remote multi-modal test records collected on smartphones from 487 patients. The proposed method shows promising results in predicting three medication statuses objectively: Before Medication (AUC=0.95), After Medication (AUC=0.958), and Another Time (AUC=0.976) by examining patient-wise historical records with the attention weights learned through a Transformer model. Our method provides an innovative way for personalized remote health sensi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#21322;&#38544;&#31169;&#22330;&#26223;&#19979;&#22914;&#20309;&#36890;&#36807;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#29992;&#25143;&#25935;&#24863;&#23646;&#24615;&#26102;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.08336</link><description>&lt;p&gt;
&#24403;&#20844;&#24179;&#36935;&#21040;&#38544;&#31169;&#65306;&#21322;&#38544;&#31169;&#25935;&#24863;&#23646;&#24615;&#19979;&#30340;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes. (arXiv:2207.08336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08336
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#21322;&#38544;&#31169;&#22330;&#26223;&#19979;&#22914;&#20309;&#36890;&#36807;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#29992;&#25143;&#25935;&#24863;&#23646;&#24615;&#26102;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#24050;&#32463;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#38459;&#30861;&#23427;&#20204;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30830;&#20445;&#20844;&#24179;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#21162;&#21147;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#20197;&#20943;&#36731;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#23545;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#38544;&#31169;&#30340;&#25285;&#24551;&#65292;&#36890;&#24120;&#38590;&#20197;&#33719;&#21462;&#22823;&#35268;&#27169;&#29992;&#25143;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;&#30001;&#20110;&#27861;&#24459;&#21512;&#35268;&#24615;&#21644;&#20154;&#20204;&#23545;&#38544;&#31169;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#38544;&#31169;&#26426;&#21046;&#20363;&#22914;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#34987;&#24191;&#27867;&#24378;&#21046;&#25191;&#34892;&#20110;&#25935;&#24863;&#20449;&#24687;&#30340;&#25968;&#25454;&#25910;&#38598;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#22312;&#38544;&#31169;&#20445;&#25252;&#19979;&#36827;&#34892;&#20844;&#24179;&#39044;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21322;&#38544;&#31169;&#22330;&#26223;&#19979;&#30340;&#20844;&#24179;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#25935;&#24863;&#23646;&#24615;&#26159;&#31169;&#26377;&#30340;&#65292;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#26159;&#21487;&#29992;&#30340;&#25935;&#24863;&#23646;&#24615;&#26159;&#24178;&#20928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models have demonstrated promising performance in many areas. However, the concerns that they can be biased against specific demographic groups hinder their adoption in high-stake applications. Thus, it is essential to ensure fairness in machine learning models. Most previous efforts require direct access to sensitive attributes for mitigating bias. Nonetheless, it is often infeasible to obtain large-scale users' sensitive attributes considering users' concerns about privacy in the data collection process. Privacy mechanisms such as local differential privacy (LDP) are widely enforced on sensitive information in the data collection stage due to legal compliance and people's increasing awareness of privacy. Therefore, a critical problem is how to make fair predictions under privacy. We study a novel and practical problem of fair classification in a semi-private setting, where most of the sensitive attributes are private and only a small amount of clean ones are availabl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21487;&#35270;&#21270;&#26041;&#27861;&#26816;&#27979;&#27169;&#22411;&#24322;&#24120;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#35782;&#21035;&#24494;&#22937;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#26080;&#27861;&#35782;&#21035;&#23548;&#33268;&#24322;&#24120;&#34892;&#20026;&#30340;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#36879;&#26126;&#24230;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.13498</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#23457;&#35745;&#65306;&#36879;&#26126;&#26041;&#27861;&#38590;&#20197;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior. (arXiv:2206.13498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21487;&#35270;&#21270;&#26041;&#27861;&#26816;&#27979;&#27169;&#22411;&#24322;&#24120;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#35782;&#21035;&#24494;&#22937;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#26080;&#27861;&#35782;&#21035;&#23548;&#33268;&#24322;&#24120;&#34892;&#20026;&#30340;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#36879;&#26126;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21487;&#35270;&#21270;&#25552;&#20379;&#20102;&#20165;&#26377;&#36755;&#20986;&#21487;&#33021;&#20250;&#24573;&#30053;&#30340;&#20449;&#24687;&#12290;&#20294;&#25105;&#20204;&#33021;&#30456;&#20449;&#27169;&#22411;&#21487;&#35270;&#21270;&#21453;&#26144;&#20102;&#27169;&#22411;&#34892;&#20026;&#21527;&#65311;&#20363;&#22914;&#65292;&#23427;&#20204;&#33021;&#21542;&#35786;&#26029;&#20986;&#31181;&#26893;&#30340;&#21518;&#38376;&#25110;&#36807;&#24230;&#27491;&#21017;&#21270;&#31561;&#24322;&#24120;&#34892;&#20026;&#65311;&#20026;&#20102;&#35780;&#20272;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#23427;&#20204;&#26159;&#21542;&#23558;&#19981;&#27491;&#24120;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#27491;&#24120;&#27169;&#22411;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21040;&#26126;&#26174;&#24322;&#24120;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#35782;&#21035;&#26356;&#24494;&#22937;&#30340;&#24322;&#24120;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#35782;&#21035;&#23548;&#33268;&#24322;&#24120;&#34892;&#20026;&#30340;&#36755;&#20837;&#65292;&#20363;&#22914;&#21253;&#21547;&#34394;&#20551;&#25552;&#31034;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20123;&#27969;&#34892;&#27169;&#22411;&#21487;&#35270;&#21270;&#30340;&#30450;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#36879;&#26126;&#24230;&#26041;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model visualizations provide information that outputs alone might miss. But can we trust that model visualizations reflect model behavior? For instance, can they diagnose abnormal behavior such as planted backdoors or overregularization? To evaluate visualization methods, we test whether they assign different visualizations to anomalously trained models and normal models. We find that while existing methods can detect models with starkly anomalous behavior, they struggle to identify more subtle anomalies. Moreover, they often fail to recognize the inputs that induce anomalous behavior, e.g. images containing a spurious cue. These results reveal blind spots and limitations of some popular model visualizations. By introducing a novel evaluation framework for visualizations, our work paves the way for developing more reliable model transparency methods in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#37329;&#23383;&#22612;&#34701;&#21512;&#26041;&#27861;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#26469;&#25366;&#25496;&#36328;&#29305;&#24449;&#37329;&#23383;&#22612;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2201.04019</link><description>&lt;p&gt;
&#37329;&#23383;&#22612;&#34701;&#21512;&#21464;&#25442;&#22120;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#37329;&#23383;&#22612;&#34701;&#21512;&#26041;&#27861;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#26469;&#25366;&#25496;&#36328;&#29305;&#24449;&#37329;&#23383;&#22612;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;MaskFormer&#20026;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65306;&#23427;&#20174;&#27969;&#34892;&#30340;&#20687;&#32032;&#32423;&#20998;&#31867;&#33539;&#24335;&#36716;&#31227;&#21040;&#20102;&#19968;&#31181;&#38754;&#21521;&#25513;&#30721;&#32423;&#21035;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#26412;&#36136;&#19978;&#65292;&#23427;&#29983;&#25104;&#19982;&#31867;&#21035;&#29255;&#27573;&#23545;&#24212;&#30340;&#25104;&#23545;&#27010;&#29575;&#21644;&#25513;&#30721;&#65292;&#24182;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#20998;&#21106;&#22270;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#22522;&#20110;&#21333;&#23610;&#24230;&#29305;&#24449;&#30340;&#27599;&#20010;&#25513;&#30721;&#20998;&#31867;&#35299;&#30721;&#22120;&#19981;&#36275;&#20197;&#25552;&#21462;&#21487;&#38752;&#30340;&#27010;&#29575;&#25110;&#25513;&#30721;&#12290;&#20026;&#20102;&#25366;&#25496;&#36328;&#29305;&#24449;&#37329;&#23383;&#22612;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#29992;&#20110;&#21333;&#23610;&#24230;&#35821;&#20041;&#20998;&#21106;&#30340;&#37329;&#23383;&#22612;&#34701;&#21512;&#21464;&#25442;&#22120;&#65288;PFT&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#22312;&#27599;&#20010;&#31354;&#38388;&#29305;&#24449;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#20043;&#38388;&#24182;&#34892;&#25191;&#34892;&#20132;&#21449;&#27880;&#24847;&#65292;&#24182;&#20351;&#29992;&#36328;&#23610;&#24230;&#38388;&#26597;&#35810;&#27880;&#24847;&#26469;&#20132;&#25442;&#34917;&#20805;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed MaskFormer gives a refreshed perspective on the task of semantic segmentation: it shifts from the popular pixel-level classification paradigm to a mask-level classification method. In essence, it generates paired probabilities and masks corresponding to category segments and combines them during inference for the segmentation maps. In our study, we find that per-mask classification decoder on top of a single-scale feature is not effective enough to extract reliable probability or mask. To mine for rich semantic information across the feature pyramid, we propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask approach semantic segmentation with multi-scale features. The proposed transformer decoder performs cross-attention between the learnable queries and each spatial feature from the feature pyramid in parallel and uses cross-scale inter-query attention to exchange complimentary information. We achieve competitive performance on three widely use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#31867;&#22411;&#22806;&#37096;&#25968;&#25454;&#30340;&#25968;&#25454;&#35821;&#20041;&#34701;&#21512;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#22788;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2201.02732</link><description>&lt;p&gt;
C2-CRS&#65306;&#38754;&#21521;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#31867;&#22411;&#22806;&#37096;&#25968;&#25454;&#30340;&#25968;&#25454;&#35821;&#20041;&#34701;&#21512;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#22788;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21521;&#29992;&#25143;&#25512;&#33616;&#36866;&#21512;&#30340;&#29289;&#21697;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#25928;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25216;&#26415;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#38750;&#24120;&#26377;&#38480;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#29992;&#25143;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#32467;&#21512;&#22806;&#37096;&#25968;&#25454;&#26469;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#19968;&#20123;&#29305;&#23450;&#31867;&#22411;&#30340;&#22806;&#37096;&#25968;&#25454;&#35774;&#35745;&#34701;&#21512;&#27169;&#22411;&#65292;&#36825;&#19981;&#36866;&#29992;&#20110;&#27169;&#22411;&#21644;&#21033;&#29992;&#22810;&#31867;&#22411;&#30340;&#22806;&#37096;&#25968;&#25454;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#22810;&#31867;&#22411;&#30340;&#22806;&#37096;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25913;&#21892;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#35821;&#20041;&#34701;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#19981;&#21516;&#30340;&#25968;&#25454;&#20449;&#21495;&#20013;&#25552;&#21462;&#21644;&#34920;&#31034;&#22810;&#31181;&#31890;&#24230;&#30340;&#35821;&#20041;&#21333;&#20803;&#65292;&#28982;&#21518;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#23545;&#40784;&#30456;&#20851;&#30340;&#22810;&#31181;&#35821;&#20041;&#21333;&#20803;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#36807;&#31243;&#26469;&#23545;&#29992;&#25143;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRS) aim to recommend suitable items to users through natural language conversations. For developing effective CRSs, a major technical issue is how to accurately infer user preference from very limited conversation context. To address issue, a promising solution is to incorporate external data for enriching the context information. However, prior studies mainly focus on designing fusion models tailored for some specific type of external data, which is not general to model and utilize multi-type external data.  To effectively leverage multi-type external data, we propose a novel coarse-to-fine contrastive learning framework to improve data semantic fusion for CRS. In our approach, we first extract and represent multi-grained semantic units from different data signals, and then align the associated multi-type semantic units in a coarse-to-fine way. To implement this framework, we design both coarse-grained and fine-grained procedures for modeling user 
&lt;/p&gt;</description></item><item><title>&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#22797;&#26434; AI &#27169;&#22411;&#38656;&#35201;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#35774;&#35745;&#20915;&#31574;&#23545;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2109.15284</link><description>&lt;p&gt;
AI-Enabled &#31227;&#21160;&#24212;&#29992;&#20013;&#30340;&#21738;&#20123;&#35774;&#35745;&#20915;&#31574;&#26377;&#21161;&#20110;&#26356;&#29615;&#20445;&#30340; AI&#65311;
&lt;/p&gt;
&lt;p&gt;
Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener AI?. (arXiv:2109.15284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.15284
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#22797;&#26434; AI &#27169;&#22411;&#38656;&#35201;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#35774;&#35745;&#20915;&#31574;&#23545;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26500;&#24314;&#12289;&#21457;&#23637;&#21644;&#20351;&#29992;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#38656;&#35201;&#26114;&#36149;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#34429;&#28982;&#24403;&#21069;&#21487;&#29992;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#29615;&#22659;&#25903;&#25345;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20294;&#23558; AI &#27169;&#22411;&#37096;&#32626;&#21040;&#31227;&#21160;&#35774;&#22791;&#19978;&#65288;&#36825;&#26159;&#19968;&#31181;&#19981;&#26029;&#22686;&#38271;&#30340;&#36235;&#21183;&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#21253;&#25324;&#35745;&#31639;&#36164;&#28304;&#32570;&#20047;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#24847;&#21619;&#30528;&#22312; AI &#21551;&#29992;&#36719;&#20214;&#24037;&#31243;&#29983;&#21629;&#21608;&#26399;&#20013;&#36827;&#34892;&#35774;&#35745;&#20915;&#31574;&#30340;&#26102;&#20505;&#38656;&#35201;&#24179;&#34913;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#26435;&#34913;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#31995;&#32479;&#22320;&#35780;&#20272;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#22797;&#26434;&#30340; AI &#27169;&#22411;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#26102;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23427;&#20204;&#20855;&#26377;&#38544;&#24335;&#30340;&#36164;&#28304;&#38480;&#21046;&#12290;&#25105;&#20204;&#26088;&#22312;&#28085;&#30422;&#65288;i&#65289;&#35774;&#35745;&#20915;&#31574;&#23545;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#39564;&#35777;&#21078;&#26512;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#38024;&#23545;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340; AI&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The construction, evolution and usage of complex artificial intelligence (AI) models demand expensive computational resources. While currently available high-performance computing environments support well this complexity, the deployment of AI models in mobile devices, which is an increasing trend, is challenging. Mobile applications consist of environments with low computational resources and hence imply limitations in the design decisions during the AI-enabled software engineering lifecycle that balance the trade-off between the accuracy and the complexity of the mobile applications.  Objective: Our objective is to systematically assess the trade-off between accuracy and complexity when deploying complex AI models (e.g. neural networks) to mobile devices, which have an implicit resource limitation. We aim to cover (i) the impact of the design decisions on the achievement of high-accuracy and low resource-consumption implementations; and (ii) the validation of profiling to
&lt;/p&gt;</description></item></channel></rss>