<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#27880;&#24847;&#21147;&#20027;&#39064;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#24773;&#24863;&#35782;&#21035;&#22120;&#22312;&#22788;&#29702;&#23545;&#35805;&#20013;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#26102;&#33719;&#24471;&#24773;&#24863;&#30456;&#20851;&#30340;&#20840;&#23616;&#35270;&#35282;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.12221</link><description>&lt;p&gt;
FATRER: &#29992;&#20110;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#20840;&#27880;&#24847;&#21147;&#20027;&#39064;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition. (arXiv:2307.12221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#27880;&#24847;&#21147;&#20027;&#39064;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#24773;&#24863;&#35782;&#21035;&#22120;&#22312;&#22788;&#29702;&#23545;&#35805;&#20013;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#26102;&#33719;&#24471;&#24773;&#24863;&#30456;&#20851;&#30340;&#20840;&#23616;&#35270;&#35282;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#29702;&#35299;&#20250;&#35805;&#35805;&#35821;&#20013;&#24341;&#21457;&#30340;&#23545;&#35805;&#32773;&#24773;&#32490;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#26356;&#20934;&#30830;&#30340;&#24773;&#24863;&#39044;&#27979;&#65292;&#32780;&#24573;&#35270;&#20102;&#24403;&#23616;&#37096;&#19978;&#19979;&#25991;&#34987;&#23545;&#25239;&#24615;&#25915;&#20987;&#30772;&#22351;&#26102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#32500;&#25345;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#20840;&#27880;&#24847;&#21147;&#20027;&#39064;&#27491;&#21017;&#21270;&#22120;&#22686;&#24378;&#30340;&#24773;&#24863;&#35782;&#21035;&#22120;&#65292;&#22312;&#24314;&#27169;&#23545;&#35805;&#20013;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#26102;&#23454;&#29616;&#24773;&#24863;&#30456;&#20851;&#30340;&#20840;&#23616;&#35270;&#35282;&#12290;&#24341;&#20837;&#32852;&#21512;&#20027;&#39064;&#24314;&#27169;&#31574;&#30053;&#65292;&#20174;&#34920;&#31034;&#21644;&#25439;&#22833;&#30340;&#35282;&#24230;&#23454;&#29616;&#27491;&#21017;&#21270;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#24230;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25918;&#24323;&#20102;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#20013;&#23384;&#22312;&#30340;&#20851;&#20110;&#20808;&#39564;&#20998;&#24067;&#30340;&#38480;&#21046;&#65292;&#24182;&#23436;&#20840;&#20381;&#38752;&#27880;&#24847;&#21147;&#23545;&#40784;&#36827;&#34892;&#27010;&#29575;&#36817;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#33719;&#24471;&#20102;&#26356;&#26377;&#21033;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#33719;&#24471;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concentrates on the understanding of interlocutors' emotions evoked in conversational utterances. Previous studies in this literature mainly focus on more accurate emotional predictions, while ignoring model robustness when the local context is corrupted by adversarial attacks. To maintain robustness while ensuring accuracy, we propose an emotion recognizer augmented by a full-attention topic regularizer, which enables an emotion-related global view when modeling the local context in a conversation. A joint topic modeling strategy is introduced to implement regularization from both representation and loss perspectives. To avoid over-regularization, we drop the constraints on prior distributions that exist in traditional topic modeling and perform probabilistic approximations based entirely on attention alignment. Experiments show that our models obtain more favorable results than state-of-the-art models, and gain convincing robustness under three types of adversarial attacks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#24314;&#31569;&#29289;&#20998;&#21106;&#20013;&#30340;&#27169;&#22411;&#20256;&#36882;&#25928;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFSeg&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#23485;&#26494;&#30417;&#30563;&#26469;&#22686;&#24378;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12220</link><description>&lt;p&gt;
&#36890;&#36807;&#28176;&#36827;&#23485;&#26494;&#30417;&#30563;&#21152;&#24555;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#24314;&#31569;&#29289;&#20998;&#21106;&#30340;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision. (arXiv:2307.12220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#24314;&#31569;&#29289;&#20998;&#21106;&#20013;&#30340;&#27169;&#22411;&#20256;&#36882;&#25928;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFSeg&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#23485;&#26494;&#30417;&#30563;&#26469;&#22686;&#24378;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#30340;&#26377;&#25928;&#24615;&#19968;&#30452;&#21463;&#21040;&#27169;&#22411;&#20256;&#36882;&#25928;&#26524;&#30340;&#38459;&#30861;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;U-Net&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#24320;&#21457;&#30340;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#26159;&#20174;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;&#26032;&#24320;&#21457;&#30340;&#39592;&#24178;&#32593;&#32476;&#24494;&#35843;&#32780;&#26469;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35299;&#30721;&#22120;&#35774;&#35745;&#30340;&#22823;&#37327;&#35745;&#31639;&#36127;&#25285;&#38459;&#30861;&#20102;&#36825;&#20123;&#29616;&#20195;&#32534;&#30721;&#22120;&#32593;&#32476;&#25104;&#21151;&#36716;&#31227;&#21040;&#36965;&#24863;&#20219;&#21153;&#19978;&#12290;&#21363;&#20351;&#26159;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#30417;&#30563;&#31574;&#30053;&#20063;&#26080;&#27861;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#28151;&#21512;&#21306;&#22495;&#65292;&#21069;&#26223;&#21644;&#32972;&#26223;&#20687;&#32032;&#26159;&#20132;&#38169;&#30340;&#65292;&#23548;&#33268;&#20854;&#25439;&#22833;&#26080;&#25928;&#12290;&#26412;&#25991;&#23545;&#20110;&#29616;&#26377;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#35774;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFSeg&#30340;&#39640;&#25928;&#26694;&#26550;&#26469;&#22686;&#24378;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#36830;&#25509;&#30340;&#31895;&#21040;&#32454;&#29305;&#24449;&#34701;&#21512;&#35299;&#30721;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
The efficacy of building footprint segmentation from remotely sensed images has been hindered by model transfer effectiveness. Many existing building segmentation methods were developed upon the encoder-decoder architecture of U-Net, in which the encoder is finetuned from the newly developed backbone networks that are pre-trained on ImageNet. However, the heavy computational burden of the existing decoder designs hampers the successful transfer of these modern encoder networks to remote sensing tasks. Even the widely-adopted deep supervision strategy fails to mitigate these challenges due to its invalid loss in hybrid regions where foreground and background pixels are intermixed. In this paper, we conduct a comprehensive evaluation of existing decoder network designs for building footprint segmentation and propose an efficient framework denoted as BFSeg to enhance learning efficiency and effectiveness. Specifically, a densely-connected coarse-to-fine feature fusion decoder network that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#20840;&#29699;&#19981;&#21516;&#22320;&#21306;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#20154;&#24037;&#26234;&#33021;&#27861;&#35268;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#21382;&#21490;&#25945;&#35757;&#21644;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#24110;&#21161;&#27835;&#29702;&#26426;&#26500;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#30340;&#28151;&#20081;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2307.12218</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27861;&#35268;&#25919;&#31574;&#30340;&#32508;&#21512;&#35780;&#20272;&#19982;&#31995;&#32479;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review and Systematic Analysis of Artificial Intelligence Regulation Policies. (arXiv:2307.12218v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#20840;&#29699;&#19981;&#21516;&#22320;&#21306;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#20154;&#24037;&#26234;&#33021;&#27861;&#35268;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#21382;&#21490;&#25945;&#35757;&#21644;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#24110;&#21161;&#27835;&#29702;&#26426;&#26500;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#30340;&#28151;&#20081;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#19990;&#30028;&#21508;&#22320;&#30340;&#25991;&#21270;&#21644;&#27835;&#29702;&#24046;&#24322;&#65292;&#30446;&#21069;&#23384;&#22312;&#19968;&#31995;&#21015;&#20154;&#24037;&#26234;&#33021;&#27861;&#35268;&#25919;&#31574;&#24314;&#35758;&#65292;&#20174;&#32780;&#22312;&#20840;&#29699;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#39046;&#22495;&#36896;&#25104;&#20102;&#28151;&#20081;&#12290;&#36866;&#24403;&#22320;&#30417;&#31649;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#22312;&#27861;&#24459;&#38480;&#21046;&#19982;&#25216;&#26415;&#21457;&#23637;&#20043;&#38388;&#20445;&#25345;&#24494;&#22937;&#30340;&#24179;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#26469;&#33258;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#25991;&#21270;&#32972;&#26223;&#30340;&#20154;&#24037;&#26234;&#33021;&#27861;&#35268;&#24314;&#35758;&#36827;&#34892;&#20840;&#38754;&#22238;&#39038;&#12290;&#28982;&#21518;&#65292;&#20511;&#37492;&#21382;&#21490;&#25945;&#35757;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#23545;&#20154;&#24037;&#26234;&#33021;&#27861;&#35268;&#24314;&#35758;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#27861;&#35268;&#24314;&#35758;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#27599;&#20010;&#24314;&#35758;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#21253;&#21547;&#21382;&#21490;&#32463;&#39564;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#32780;&#27835;&#20043;&#30340;&#26041;&#24335;&#24110;&#21161;&#27835;&#29702;&#26426;&#26500;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#30340;&#28151;&#20081;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the cultural and governance differences of countries around the world, there currently exists a wide spectrum of AI regulation policy proposals that have created a chaos in the global AI regulatory space. Properly regulating AI technologies is extremely challenging, as it requires a delicate balance between legal restrictions and technological developments. In this article, we first present a comprehensive review of AI regulation proposals from different geographical locations and cultural backgrounds. Then, drawing from historical lessons, we develop a framework to facilitate a thorough analysis of AI regulation proposals. Finally, we perform a systematic analysis of these AI regulation proposals to understand how each proposal may fail. This study, containing historical lessons and analysis methods, aims to help governing bodies untangling the AI regulatory chaos through a divide-and-conquer manner.
&lt;/p&gt;</description></item><item><title>DeepCL&#26159;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24230;&#37327;&#23398;&#20064;&#21644;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#20851;&#31995;&#24314;&#27169;&#21644;&#20266;&#21464;&#21270;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.12208</link><description>&lt;p&gt;
DeepCL: &#24212;&#29992;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#28145;&#24230;&#21464;&#21270;&#29305;&#24449;&#23398;&#20064;&#20110;&#36965;&#24863;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DeepCL: Deep Change Feature Learning on Remote Sensing Images in the Metric Space. (arXiv:2307.12208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12208
&lt;/p&gt;
&lt;p&gt;
DeepCL&#26159;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24230;&#37327;&#23398;&#20064;&#21644;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#20851;&#31995;&#24314;&#27169;&#21644;&#20266;&#21464;&#21270;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21270;&#26816;&#27979;&#26159;&#22320;&#29699;&#35266;&#27979;&#39046;&#22495;&#20013;&#30417;&#27979;&#22320;&#29699;&#34920;&#38754;&#21160;&#24577;&#30340;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#26368;&#36817;&#25512;&#21160;&#20102;&#33258;&#21160;&#21464;&#21270;&#26816;&#27979;&#30340;&#25216;&#26415;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;1&#65289;&#19981;&#20805;&#20998;&#30340;&#26102;&#38388;&#20851;&#31995;&#24314;&#27169;&#21644;2&#65289;&#20266;&#21464;&#21270;&#35823;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#24230;&#37327;&#23398;&#20064;&#30340;&#24378;&#22823;&#26102;&#38388;&#24314;&#27169;&#33021;&#21147;&#19982;&#20998;&#21106;&#30340;&#26174;&#33879;&#25311;&#21512;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;&#21464;&#21270;&#26816;&#27979;&#30340;&#28145;&#24230;&#21464;&#21270;&#29305;&#24449;&#23398;&#20064;&#65288;DeepCL&#65289;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#26679;&#26412;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#25439;&#22833;&#37325;&#26032;&#26435;&#34913;&#20102;&#30828;&#26679;&#26412;&#21644;&#31616;&#21333;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#31181;&#25439;&#22833;&#20801;&#35768;&#26174;&#24335;&#22320;&#24314;&#27169;&#21452;&#26102;&#30456;&#36965;&#24863;&#22270;&#20687;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#24314;&#27169;&#30340;&#26102;&#38388;&#20851;&#31995;&#34987;&#29992;&#20316;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20998;&#21106;&#36807;&#31243;&#65292;&#20197;&#20415;&#26816;&#27979;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection (CD) is an important yet challenging task in the Earth observation field for monitoring Earth surface dynamics. The advent of deep learning techniques has recently propelled automatic CD into a technological revolution. Nevertheless, deep learning-based CD methods are still plagued by two primary issues: 1) insufficient temporal relationship modeling and 2) pseudo-change misclassification. To address these issues, we complement the strong temporal modeling ability of metric learning with the prominent fitting ability of segmentation and propose a deep change feature learning (DeepCL) framework for robust and explainable CD. Firstly, we designed a hard sample-aware contrastive loss, which reweights the importance of hard and simple samples. This loss allows for explicit modeling of the temporal correlation between bi-temporal remote sensing images. Furthermore, the modeled temporal relations are utilized as knowledge prior to guide the segmentation process for detecting
&lt;/p&gt;</description></item><item><title>DeepLearning.scala 2&#35299;&#20915;&#20102;&#22312;&#38745;&#24577;&#31867;&#22411;&#35821;&#35328;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#33258;&#21160;&#27714;&#23548;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#21333;&#23376;&#21644;&#21333;&#23376;&#21464;&#25442;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.12187</link><description>&lt;p&gt;
&#21333;&#23376;&#21270;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Monadic Deep Learning. (arXiv:2307.12187v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12187
&lt;/p&gt;
&lt;p&gt;
DeepLearning.scala 2&#35299;&#20915;&#20102;&#22312;&#38745;&#24577;&#31867;&#22411;&#35821;&#35328;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#33258;&#21160;&#27714;&#23548;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#21333;&#23376;&#21644;&#21333;&#23376;&#21464;&#25442;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Java&#21644;Scala&#31038;&#21306;&#24314;&#31435;&#20102;&#19968;&#20010;&#38750;&#24120;&#25104;&#21151;&#30340;&#22823;&#25968;&#25454;&#29983;&#24577;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#22312;&#20854;&#19978;&#36816;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#21160;&#24577;&#31867;&#22411;&#32534;&#31243;&#35821;&#35328;&#24314;&#27169;&#30340;&#12290;&#36825;&#20123;&#21160;&#24577;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#21253;&#21547;&#35768;&#22810;&#21487;&#35757;&#32451;&#21464;&#37327;&#30340;&#21487;&#24494;&#20998;&#34920;&#36798;&#24335;&#65292;&#24182;&#22312;&#35757;&#32451;&#26102;&#23545;&#36825;&#20123;&#34920;&#36798;&#24335;&#36827;&#34892;&#33258;&#21160;&#27714;&#23548;&#12290;&#30452;&#21040;2019&#24180;&#65292;&#38745;&#24577;&#31867;&#22411;&#35821;&#35328;&#30340;&#23398;&#20064;&#26694;&#26550;&#27809;&#26377;&#25552;&#20379;&#20256;&#32479;&#26694;&#26550;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#38500;&#38750;&#21019;&#24314;&#22823;&#37327;&#26679;&#26495;&#20195;&#30721;&#36827;&#34892;&#30828;&#32534;&#30721;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#21542;&#21017;&#29992;&#25143;&#26080;&#27861;&#20351;&#29992;&#33258;&#23450;&#20041;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;DeepLearning.scala 2&#20013;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#65306;1.&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#21253;&#21547;&#22810;&#20010;&#21487;&#35757;&#32451;&#21464;&#37327;&#30340;&#38745;&#24577;&#31867;&#22411;&#20989;&#25968;&#36827;&#34892;&#21453;&#21521;&#27169;&#24335;&#30340;&#33258;&#21160;&#27714;&#23548;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#30001;&#22320;&#19982;&#20803;&#35821;&#35328;&#20114;&#25805;&#20316;&#12290;2.&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#21333;&#23376;&#21644;&#21333;&#23376;&#21464;&#25442;&#22120;&#65292;
&lt;/p&gt;
&lt;p&gt;
The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.  Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.  We solved this problem in DeepLearning.scala 2. Our contributions are:  1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.  2. We designed a set of monads and monad transformers, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#36779;&#23376;&#21644;&#24179;&#38754;&#36779;&#23376;&#36827;&#34892;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#20415;&#21033;&#19981;&#21464;&#37327;&#65292;&#21253;&#25324;&#24179;&#38754;&#36779;&#23376;&#30340;&#23436;&#20840;&#19981;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.12185</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#36779;&#23376;&#21644;&#24179;&#38754;&#36779;&#23376;&#30340;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Machine learning discovers invariants of braids and flat braids. (arXiv:2307.12185v1 [math.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#36779;&#23376;&#21644;&#24179;&#38754;&#36779;&#23376;&#36827;&#34892;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#20415;&#21033;&#19981;&#21464;&#37327;&#65292;&#21253;&#25324;&#24179;&#38754;&#36779;&#23376;&#30340;&#23436;&#20840;&#19981;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23558;&#36779;&#23376;&#65288;&#25110;&#24179;&#38754;&#36779;&#23376;&#65289;&#30340;&#31034;&#20363;&#20998;&#31867;&#20026;&#24179;&#20961;&#25110;&#38750;&#24179;&#20961;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#37319;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;&#22810;&#23618;&#24863;&#30693;&#22120;&#65289;&#30340;&#30417;&#30563;&#23398;&#20064;&#24418;&#24335;&#12290;&#24403;&#23427;&#20204;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#23427;&#20204;&#30340;&#32467;&#26500;&#35299;&#37322;&#20026;&#25968;&#23398;&#29468;&#24819;&#65292;&#28982;&#21518;&#35777;&#26126;&#36825;&#20123;&#29468;&#24819;&#25104;&#20026;&#23450;&#29702;&#12290;&#32467;&#26524;&#65292;&#22312;&#36779;&#23376;&#20013;&#25214;&#21040;&#20102;&#26032;&#30340;&#20415;&#21033;&#19981;&#21464;&#37327;&#65292;&#21253;&#25324;&#24179;&#38754;&#36779;&#23376;&#30340;&#23436;&#20840;&#19981;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use machine learning to classify examples of braids (or flat braids) as trivial or non-trivial. Our ML takes form of supervised learning using neural networks (multilayer perceptrons). When they achieve good results in classification, we are able to interpret their structure as mathematical conjectures and then prove these conjectures as theorems. As a result, we find new convenient invariants of braids, including a complete invariant of flat braids.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32473;&#20986;&#20102;&#23384;&#22312;&#36825;&#26679;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20102;&#27599;&#20010;&#38750;&#36864;&#21270;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#38598;&#21512;&#37117;&#21487;&#20197;&#29992;&#22810;&#32500;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#26469;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.12184</link><description>&lt;p&gt;
&#22810;&#32500;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Expressivity of Multidimensional Markov Reward. (arXiv:2307.12184v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32473;&#20986;&#20102;&#23384;&#22312;&#36825;&#26679;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20102;&#27599;&#20010;&#38750;&#36864;&#21270;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#38598;&#21512;&#37117;&#21487;&#20197;&#29992;&#22810;&#32500;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#26469;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#35270;&#20026;&#25551;&#36848;&#20195;&#29702;&#34892;&#20026;&#30340;&#25163;&#27573;&#12290;&#20551;&#35774;&#26399;&#26395;&#34892;&#20026;&#34987;&#25351;&#23450;&#20026;&#19968;&#32452;&#21487;&#25509;&#21463;&#31574;&#30053;&#65292;&#25105;&#20204;&#30740;&#31350;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#26631;&#37327;&#25110;&#22810;&#32500;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#20351;&#24471;&#35813;&#38598;&#21512;&#20013;&#30340;&#31574;&#30053;&#27604;&#20854;&#20182;&#31574;&#30053;&#26356;&#29702;&#24819;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#38472;&#36848;&#20102;&#23384;&#22312;&#36825;&#26679;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#20110;&#27599;&#20010;&#38750;&#36864;&#21270;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#38598;&#21512;&#65292;&#37117;&#23384;&#22312;&#19968;&#20010;&#22810;&#32500;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#26469;&#25551;&#36848;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the expressivity of Markov rewards in sequential decision making under uncertainty. We view reward functions in Markov Decision Processes (MDPs) as a means to characterize desired behaviors of agents. Assuming desired behaviors are specified as a set of acceptable policies, we investigate if there exists a scalar or multidimensional Markov reward function that makes the policies in the set more desirable than the other policies. Our main result states both necessary and sufficient conditions for the existence of such reward functions. We also show that for every non-degenerate set of deterministic policies, there exists a multidimensional Markov reward function that characterizes it
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#36827;&#34892;&#20840;&#38754;&#20998;&#31867;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#21152;&#24378;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23545;&#26032;&#20852;&#23433;&#20840;&#39118;&#38505;&#30340;&#38450;&#33539;&#12290;</title><link>http://arxiv.org/abs/2307.12181</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#19982;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Security and Privacy Issues of Federated Learning. (arXiv:2307.12181v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#36827;&#34892;&#20840;&#38754;&#20998;&#31867;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#21152;&#24378;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23545;&#26032;&#20852;&#23433;&#20840;&#39118;&#38505;&#30340;&#38450;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#26395;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#20445;&#23494;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#32773;&#26500;&#24314;&#20849;&#20139;&#27169;&#22411;&#32780;&#19981;&#38598;&#20013;&#25935;&#24863;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#25955;&#30340;&#27169;&#24335;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#38656;&#35201;&#20840;&#38754;&#35782;&#21035;&#21644;&#20998;&#31867;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#20197;&#30830;&#20445;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#32852;&#37030;&#23398;&#20064;&#20013;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#30340;&#20840;&#38754;&#20998;&#31867;&#12290;&#25105;&#20204;&#29305;&#21035;&#23545;&#32858;&#21512;&#22120;&#21644;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#25915;&#20987;&#30340;&#20998;&#31867;&#65292;&#37325;&#28857;&#20851;&#27880;&#27602;&#21270;&#25915;&#20987;&#12289;&#21518;&#38376;&#25915;&#20987;&#12289;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#25915;&#20987;&#21644;&#24046;&#20998;&#38544;&#31169;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#65292;&#23547;&#27714;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#20197;&#21152;&#24378;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23545;&#26032;&#20852;&#23433;&#20840;&#39118;&#38505;&#30340;&#38450;&#33539;&#65292;&#32500;&#25252;&#25935;&#24863;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising approach to address data privacy and confidentiality concerns by allowing multiple participants to construct a shared model without centralizing sensitive data. However, this decentralized paradigm introduces new security challenges, necessitating a comprehensive identification and classification of potential risks to ensure FL's security guarantees. This paper presents a comprehensive taxonomy of security and privacy challenges in Federated Learning (FL) across various machine learning models, including large language models. We specifically categorize attacks performed by the aggregator and participants, focusing on poisoning attacks, backdoor attacks, membership inference attacks, generative adversarial network (GAN) based attacks, and differential privacy attacks. Additionally, we propose new directions for future research, seeking innovative solutions to fortify FL systems against emerging security risks and uphold sensitive data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#20013;&#21629;&#21517;&#23454;&#20307;&#28040;&#35299;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#24418;&#24335;&#21270;&#23450;&#20041;&#12289;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#35299;&#20915;&#32452;&#20214;&#20197;&#21450;&#24212;&#29992;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.12173</link><description>&lt;p&gt;
&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
Named Entity Resolution in Personal Knowledge Graphs. (arXiv:2307.12173v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#20013;&#21629;&#21517;&#23454;&#20307;&#28040;&#35299;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#24418;&#24335;&#21270;&#23450;&#20041;&#12289;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#35299;&#20915;&#32452;&#20214;&#20197;&#21450;&#24212;&#29992;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#28040;&#35299;&#65288;ER&#65289;&#26159;&#30830;&#23450;&#20004;&#20010;&#23454;&#20307;&#26159;&#21542;&#25351;&#21521;&#30456;&#21516;&#24213;&#23618;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#30740;&#31350;&#20102;50&#22810;&#24180;&#65292;&#26368;&#36817;&#22312;&#20114;&#32852;&#32593;&#19978;&#21457;&#24067;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#25628;&#32034;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#30340;&#8220;&#30693;&#35782;&#22270;&#35889;&#8221;&#26102;&#20195;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#26412;&#31456;&#23558;&#35752;&#35770;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#32972;&#26223;&#19979;&#21629;&#21517;&#23454;&#20307;&#28040;&#35299;&#30340;&#20855;&#20307;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#23450;&#20041;&#20197;&#21450;&#36827;&#34892;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#23454;&#20307;&#28040;&#35299;&#25152;&#38656;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;Web&#35268;&#27169;&#25968;&#25454;&#20013;&#39044;&#35745;&#20250;&#20986;&#29616;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#25216;&#26415;&#22914;&#20309;&#28508;&#22312;&#22320;&#24212;&#29992;&#20110;PKG&#36827;&#34892;&#20102;&#31616;&#35201;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35206;&#30422;&#20102;&#19968;&#20123;&#24212;&#29992;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Resolution (ER) is the problem of determining when two entities refer to the same underlying entity. The problem has been studied for over 50 years, and most recently, has taken on new importance in an era of large, heterogeneous 'knowledge graphs' published on the Web and used widely in domains as wide ranging as social media, e-commerce and search. This chapter will discuss the specific problem of named ER in the context of personal knowledge graphs (PKGs). We begin with a formal definition of the problem, and the components necessary for doing high-quality and efficient ER. We also discuss some challenges that are expected to arise for Web-scale data. Next, we provide a brief literature review, with a special focus on how existing techniques can potentially apply to PKGs. We conclude the chapter by covering some applications, as well as promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>&#24187;&#35273;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20026;&#23545;&#27604;&#23398;&#20064;&#25552;&#20379;&#39069;&#22806;&#30340;&#27491;&#26679;&#26412;&#65292;&#25913;&#36827;&#20102;&#26080;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12168</link><description>&lt;p&gt;
&#24187;&#35273;&#25913;&#36827;&#20102;&#26080;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Hallucination Improves the Performance of Unsupervised Visual Representation Learning. (arXiv:2307.12168v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12168
&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20026;&#23545;&#27604;&#23398;&#20064;&#25552;&#20379;&#39069;&#22806;&#30340;&#27491;&#26679;&#26412;&#65292;&#25913;&#36827;&#20102;&#26080;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36830;&#20307;&#32467;&#26500;&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#20004;&#20010;&#26465;&#20214;&#65306;&#36275;&#22815;&#25968;&#37327;&#30340;&#27491;&#23545;&#21644;&#36866;&#24403;&#30340;&#21464;&#21270;&#12290;&#22914;&#26524;&#36825;&#20123;&#26465;&#20214;&#19981;&#28385;&#36275;&#65292;&#36825;&#20123;&#26694;&#26550;&#23558;&#32570;&#20047;&#35821;&#20041;&#23545;&#27604;&#65292;&#24182;&#19988;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hallucinator&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#39069;&#22806;&#30340;&#27491;&#26679;&#26412;&#20197;&#36827;&#19968;&#27493;&#36827;&#34892;&#23545;&#27604;&#12290;Hallucinator&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#19982;&#39044;&#35757;&#32451;&#20219;&#21153;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#24341;&#20837;&#20960;&#20046;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#38750;&#32447;&#24615;&#25805;&#20316;&#20943;&#23569;&#20102;&#24187;&#35273;&#23545;&#30340;&#20114;&#20449;&#24687;&#24182;&#20351;&#20854;&#24179;&#28369;&#12290;&#36825;&#20010;&#36807;&#31243;&#26377;&#21161;&#20110;&#36991;&#20813;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#65292;&#24182;&#23454;&#29616;&#26356;&#20855;&#21464;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning models based on Siamese structure have demonstrated remarkable performance in self-supervised learning. Such a success of contrastive learning relies on two conditions, a sufficient number of positive pairs and adequate variations between them. If the conditions are not met, these frameworks will lack semantic contrast and be fragile on overfitting. To address these two issues, we propose Hallucinator that could efficiently generate additional positive samples for further contrast. The Hallucinator is differentiable and creates new data in the feature space. Thus, it is optimized directly with the pre-training task and introduces nearly negligible computation. Moreover, we reduce the mutual information of hallucinated pairs and smooth them through non-linear operations. This process helps avoid over-confident contrastive learning models during the training and achieves more transformation-invariant feature embeddings. Remarkably, we empirically prove that the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#20307;&#35009;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#21147;&#65292;&#23613;&#31649;&#22312;&#21306;&#20998;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.12166</link><description>&lt;p&gt;
&#27169;&#20223;&#28216;&#25103;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#26816;&#27979;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models. (arXiv:2307.12166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#20307;&#35009;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#21147;&#65292;&#23613;&#31649;&#22312;&#21306;&#20998;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#38761;&#26032;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#20154;&#31867;&#20889;&#20316;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19981;&#21516;&#20307;&#35009;&#30340;&#20154;&#31867;&#20889;&#20316;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65306;&#35770;&#25991;&#12289;&#25925;&#20107;&#12289;&#35799;&#27468;&#21644;Python&#20195;&#30721;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#36825;&#20123;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#20998;&#31867;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26102;&#65292;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25925;&#20107;&#20889;&#20316;&#26041;&#38754;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26356;&#22797;&#26434;&#30340;&#22810;&#31867;&#21035;&#20219;&#21153;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#21644;&#29305;&#23450;LLM&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of artificial intelligence (AI)-based large language models (LLMs) holds considerable promise in revolutionizing education, research, and practice. However, distinguishing between human-written and AI-generated text has become a significant task. This paper presents a comparative study, introducing a novel dataset of human-written and LLM-generated texts in different genres: essays, stories, poetry, and Python code. We employ several machine learning models to classify the texts. Results demonstrate the efficacy of these models in discerning between human and AI-generated text, despite the dataset's limited sample size. However, the task becomes more challenging when classifying GPT-generated text, particularly in story writing. The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared to the more complex multiclass tasks that involve discerning among human-ge
&lt;/p&gt;</description></item><item><title>DIP-RL&#26159;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#30340;&#31639;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#25512;&#23548;&#20559;&#22909;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#22312;Minecraft&#20013;&#30340;&#30733;&#26641;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12158</link><description>&lt;p&gt;
DIP-RL&#65306;&#22312;Minecraft&#20013;&#30340;&#28436;&#31034;&#25512;&#23548;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIP-RL: Demonstration-Inferred Preference Learning in Minecraft. (arXiv:2307.12158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12158
&lt;/p&gt;
&lt;p&gt;
DIP-RL&#26159;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#30340;&#31639;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#25512;&#23548;&#20559;&#22909;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#22312;Minecraft&#20013;&#30340;&#30733;&#26641;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#31639;&#27861;&#20195;&#29702;&#36890;&#36807;&#25509;&#25910;&#22870;&#21169;&#20449;&#21495;&#30340;&#21453;&#39304;&#26469;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#38750;&#32467;&#26500;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#36825;&#26679;&#30340;&#22870;&#21169;&#20449;&#21495;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#26080;&#27861;&#21487;&#38752;&#22320;&#26500;&#24314;&#19968;&#20010;&#27491;&#30830;&#25429;&#25417;&#25152;&#38656;&#34892;&#20026;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#20026;&#20102;&#22312;&#36825;&#26679;&#30340;&#38750;&#32467;&#26500;&#21270;&#21644;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Demonstration-Inferred Preference Reinforcement Learning (DIP-RL)&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#28436;&#31034;&#25968;&#25454;&#31181;&#23376;&#24378;&#21270;&#23398;&#20064; (RL)&#35757;&#32451;&#25209;&#27425;&#65292;&#24182;&#25512;&#23548;&#20986;&#20559;&#22909;&#20197;&#23398;&#20064;&#24341;&#23548;RL&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;Minecraft&#20013;&#30340;&#30733;&#26641;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;DIP-RL&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25351;&#23548;RL&#20195;&#29702;&#23398;&#20064;&#19968;&#20010;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;DIP-RL&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#33258;&#36866;&#24212;&#26172;&#22812;&#33410;&#24459;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#22312;&#21608;&#26399;&#24615;&#21464;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#35269;&#39135;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#26234;&#33021;&#20307;&#33021;&#22815;&#20869;&#21270;&#29615;&#22659;&#20449;&#21495;&#24182;&#36866;&#24212;&#30456;&#20301;&#21464;&#21270;&#65292;&#36825;&#19968;&#33258;&#36866;&#24212;&#36807;&#31243;&#26159;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#20803;&#30340;&#21160;&#21147;&#23398;&#26469;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12143</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#36866;&#24212;&#26172;&#22812;&#33410;&#24459;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning. (arXiv:2307.12143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#33258;&#36866;&#24212;&#26172;&#22812;&#33410;&#24459;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#22312;&#21608;&#26399;&#24615;&#21464;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#35269;&#39135;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#26234;&#33021;&#20307;&#33021;&#22815;&#20869;&#21270;&#29615;&#22659;&#20449;&#21495;&#24182;&#36866;&#24212;&#30456;&#20301;&#21464;&#21270;&#65292;&#36825;&#19968;&#33258;&#36866;&#24212;&#36807;&#31243;&#26159;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#20803;&#30340;&#21160;&#21147;&#23398;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#29615;&#22659;&#30340;&#35268;&#24459;&#23545;&#29983;&#29289;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20351;&#20854;&#33021;&#22815;&#39044;&#27979;&#20107;&#20214;&#21644;&#21046;&#23450;&#35745;&#21010;&#12290;&#20854;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;&#29983;&#29289;&#23545;&#22320;&#29699;&#33258;&#36716;24&#23567;&#26102;&#21608;&#26399;&#30340;&#20869;&#21270;&#65292;&#21363;&#26172;&#22812;&#33410;&#24459;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#31867;&#20284;&#26172;&#22812;&#33410;&#24459;&#30340;&#20986;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21608;&#26399;&#24615;&#21464;&#21270;&#21487;&#38752;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#20102;&#26234;&#33021;&#20307;&#65292;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#35269;&#39135;&#20219;&#21153;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#34920;&#24449;&#20102;&#26234;&#33021;&#20307;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#20869;&#28304;&#24615;&#19988;&#21487;&#35843;&#21305;&#30340;&#33410;&#24459;&#30340;&#20986;&#29616;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20869;&#37096;&#33410;&#24459;&#36866;&#24212;&#20102;&#29615;&#22659;&#20449;&#21495;&#30456;&#20301;&#30340;&#21464;&#21270;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23700;&#21644;&#30456;&#20301;&#21709;&#24212;&#26354;&#32447;&#20998;&#26512;&#23637;&#31034;&#20102;&#20154;&#24037;&#31070;&#32463;&#20803;&#22914;&#20309;&#21457;&#23637;&#20986;&#25903;&#25345;&#29615;&#22659;&#33410;&#24459;&#20869;&#21270;&#30340;&#21160;&#21147;&#23398;&#12290;&#20174;&#21160;&#21147;&#23398;&#31995;&#32479;&#35270;&#35282;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#33258;&#36866;&#24212;&#36807;&#31243;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the $24$-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents. In particular, we deployed agents in an environment with a reliable periodic variation while solving a foraging task. We systematically characterize the agent's behavior during learning and demonstrate the emergence of a rhythm that is endogenous and entrainable. Interestingly, the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training. Furthermore, we show via bifurcation and phase response curve analyses how artificial neurons develop dynamics to support the internalization of the environmental rhythm. From a dynamical systems view, we demonstrate that the adaptation proceeds by the emergenc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#65288;NIAs&#65289;&#21450;&#20854;&#22312;&#36335;&#32447;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;NIAs&#26159;&#19968;&#31867;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21463;&#33258;&#28982;&#29616;&#35937;&#21551;&#21457;&#65292;&#36890;&#36807;&#25910;&#25947;&#21644;&#38543;&#26426;&#29305;&#24615;&#32473;&#20986;&#26368;&#20248;&#32467;&#26524;&#12290;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#36335;&#32447;&#35268;&#21010;&#38382;&#39064;&#20013;&#65292;NIAs&#33021;&#22815;&#26377;&#25928;&#22320;&#24110;&#21161;&#26426;&#22120;&#20154;&#36991;&#20813;&#29615;&#22659;&#20013;&#30340;&#38556;&#30861;&#29289;&#12290;</title><link>http://arxiv.org/abs/2307.12133</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#36827;&#34892;&#36335;&#32447;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Route Planning Using Nature-Inspired Algorithms. (arXiv:2307.12133v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#65288;NIAs&#65289;&#21450;&#20854;&#22312;&#36335;&#32447;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;NIAs&#26159;&#19968;&#31867;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21463;&#33258;&#28982;&#29616;&#35937;&#21551;&#21457;&#65292;&#36890;&#36807;&#25910;&#25947;&#21644;&#38543;&#26426;&#29305;&#24615;&#32473;&#20986;&#26368;&#20248;&#32467;&#26524;&#12290;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#36335;&#32447;&#35268;&#21010;&#38382;&#39064;&#20013;&#65292;NIAs&#33021;&#22815;&#26377;&#25928;&#22320;&#24110;&#21161;&#26426;&#22120;&#20154;&#36991;&#20813;&#29615;&#22659;&#20013;&#30340;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#24456;&#22810;&#19981;&#21516;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#65288;NIAs&#65289;&#12290;&#23427;&#20204;&#36890;&#24120;&#21463;&#26576;&#20123;&#33258;&#28982;&#29616;&#35937;&#30340;&#21551;&#21457;&#65292;&#24182;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#25910;&#25947;&#21644;&#38543;&#26426;&#29305;&#24615;&#32780;&#34987;&#35748;&#20026;&#22312;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#26102;&#32473;&#20986;&#26368;&#20248;&#32467;&#26524;&#12290;NIAs&#26377;&#24456;&#22810;&#24212;&#29992;&#65292;&#20854;&#20013;&#26368;&#27969;&#34892;&#30340;&#21487;&#33021;&#26159;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#36335;&#32447;&#35268;&#21010;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#20174;&#36215;&#28857;&#21040;&#30446;&#26631;&#20197;&#20248;&#21270;&#30340;&#26041;&#24335;&#36991;&#24320;&#29615;&#22659;&#20013;&#30340;&#38556;&#30861;&#29289;&#26102;&#30340;&#19968;&#31995;&#21015;&#24179;&#31227;&#21644;&#26059;&#36716;&#27493;&#39588;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#23558;&#39318;&#20808;&#27010;&#36848;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#65292;&#28982;&#21518;&#20171;&#32461;&#23427;&#20204;&#30340;&#20998;&#31867;&#21644;&#24120;&#35265;&#31034;&#20363;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#35752;&#35770;&#22914;&#20309;&#24212;&#29992;NIAs&#26469;&#35299;&#20915;&#36335;&#32447;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are many different heuristic algorithms for solving combinatorial optimization problems that are commonly described as Nature-Inspired Algorithms (NIAs). Generally, they are inspired by some natural phenomenon, and due to their inherent converging and stochastic nature, they are known to give optimal results when compared to classical approaches. There are a large number of applications of NIAs, perhaps the most popular being route planning problems in robotics - problems that require a sequence of translation and rotation steps from the start to the goal in an optimized manner while avoiding obstacles in the environment. In this chapter, we will first give an overview of Nature-Inspired Algorithms, followed by their classification and common examples. We will then discuss how the NIAs have applied to solve the route planning problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26234;&#33021;&#22478;&#24066;&#20013;&#20132;&#36890;&#20107;&#25925;&#21450;&#20107;&#25925;&#26816;&#27979;&#31995;&#32479;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20132;&#36890;&#30417;&#25511;&#25668;&#20687;&#22836;&#21644;&#34892;&#20026;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#21644;&#21709;&#24212;&#20132;&#36890;&#20107;&#25925;&#65292;&#24182;&#23558;&#20854;&#19982;&#32039;&#24613;&#26381;&#21153;&#25972;&#21512;&#65292;&#20197;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#21644;&#25913;&#21892;&#20132;&#36890;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2307.12128</link><description>&lt;p&gt;
AI&#22312;&#36947;&#36335;&#19978;&#30340;&#24212;&#29992;&#65306;&#26234;&#33021;&#22478;&#24066;&#20013;&#20132;&#36890;&#20107;&#25925;&#21450;&#20107;&#25925;&#26816;&#27979;&#31995;&#32479;&#30340;&#32508;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities. (arXiv:2307.12128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26234;&#33021;&#22478;&#24066;&#20013;&#20132;&#36890;&#20107;&#25925;&#21450;&#20107;&#25925;&#26816;&#27979;&#31995;&#32479;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20132;&#36890;&#30417;&#25511;&#25668;&#20687;&#22836;&#21644;&#34892;&#20026;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#21644;&#21709;&#24212;&#20132;&#36890;&#20107;&#25925;&#65292;&#24182;&#23558;&#20854;&#19982;&#32039;&#24613;&#26381;&#21153;&#25972;&#21512;&#65292;&#20197;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#21644;&#25913;&#21892;&#20132;&#36890;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#25925;&#26816;&#27979;&#21644;&#20132;&#36890;&#20998;&#26512;&#26159;&#26234;&#33021;&#22478;&#24066;&#21644;&#33258;&#21160;&#21270;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21487;&#20197;&#20943;&#23569;&#20107;&#25925;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#65292;&#25913;&#21892;&#20132;&#36890;&#31649;&#29702;&#12290;&#26412;&#25991;&#21033;&#29992;&#32654;&#22269;&#22269;&#23478;&#20844;&#36335;&#20132;&#36890;&#23433;&#20840;&#31649;&#29702;&#23616;&#65288;NHTSA&#65289;&#30340;Crash Report Sampling System&#65288;CRSS&#65289;&#30340;&#25968;&#25454;&#65292;&#23545;&#32654;&#22269;&#19981;&#21516;&#22320;&#21306;&#30340;&#20132;&#36890;&#20107;&#25925;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#20026;&#20102;&#24212;&#23545;&#20107;&#25925;&#26816;&#27979;&#21644;&#20132;&#36890;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#20132;&#36890;&#30417;&#25511;&#25668;&#20687;&#22836;&#21644;&#34892;&#20026;&#35782;&#21035;&#31995;&#32479;&#26469;&#21363;&#26102;&#26816;&#27979;&#21644;&#21709;&#24212;&#20132;&#36890;&#20107;&#25925;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19982;&#32039;&#24613;&#26381;&#21153;&#25972;&#21512;&#65292;&#23558;&#20132;&#36890;&#25668;&#20687;&#22836;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21147;&#37327;&#29992;&#20110;&#21709;&#24212;&#20132;&#36890;&#20107;&#25925;&#21644;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#12290;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#20107;&#25925;&#26816;&#27979;&#31995;&#32479;&#31561;&#20808;&#36827;&#26234;&#33021;&#25216;&#26415;&#23558;&#25913;&#21892;&#20132;&#36890;&#23433;&#20840;&#65292;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve tra
&lt;/p&gt;</description></item><item><title>&#31227;&#21160;AIGC&#25216;&#26415;&#21487;&#20197;&#25512;&#21160;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#29983;&#25104;&#32597;&#35265;&#30142;&#30149;&#25968;&#25454;&#12289;&#24314;&#27169;&#39640;&#20445;&#30495;&#25968;&#23383;&#23402;&#29983;&#20197;&#21450;&#25552;&#20379;24/7&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12115</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#21307;&#30103;&#38761;&#21629;&#65306;&#21033;&#29992;&#31227;&#21160;AIGC&#23454;&#29616;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;
&lt;/p&gt;
&lt;p&gt;
A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC. (arXiv:2307.12115v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12115
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;AIGC&#25216;&#26415;&#21487;&#20197;&#25512;&#21160;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#29983;&#25104;&#32597;&#35265;&#30142;&#30149;&#25968;&#25454;&#12289;&#24314;&#27169;&#39640;&#20445;&#30495;&#25968;&#23383;&#23402;&#29983;&#20197;&#21450;&#25552;&#20379;24/7&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#25216;&#26415;&#25351;&#30340;&#26159;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#19978;&#37319;&#29992;AI&#31639;&#27861;&#33258;&#21160;&#21270;&#20449;&#24687;&#21019;&#24314;&#36807;&#31243;&#65292;&#21516;&#26102;&#28385;&#36275;&#32456;&#31471;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;&#31227;&#21160;AIGC&#26368;&#36817;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#20026;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#65288;HDT&#65289;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#31227;&#21160;AIGC&#39537;&#21160;&#30340;HDT&#26377;&#26395;&#36890;&#36807;&#29983;&#25104;&#32597;&#35265;&#30142;&#30149;&#25968;&#25454;&#12289;&#24314;&#27169;&#39640;&#20445;&#30495;&#25968;&#23383;&#23402;&#29983;&#12289;&#26500;&#24314;&#22810;&#21151;&#33021;&#35797;&#39564;&#24179;&#21488;&#21644;&#25552;&#20379;&#20840;&#22825;&#20505;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26469;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20010;&#24615;&#21270;&#21307;&#30103;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#26032;&#22411;&#33539;&#24335;&#30340;&#21457;&#23637;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31227;&#21160;AIGC&#39537;&#21160;&#30340;HDT&#31995;&#32479;&#26550;&#26500;&#65292;&#24182;&#24378;&#35843;&#20102;&#30456;&#24212;&#30340;&#35774;&#35745;&#35201;&#27714;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20030;&#20363;&#35828;&#26126;&#20102;&#20004;&#31181;&#29992;&#20363;&#65292;&#21363;&#22312;&#23450;&#21046;&#25163;&#26415;&#35268;&#21010;&#21644;&#20010;&#24615;&#21270;&#33647;&#29289;&#27835;&#30103;&#20013;&#20351;&#29992;&#31227;&#21160;AIGC&#39537;&#21160;&#30340;HDT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile Artificial Intelligence-Generated Content (AIGC) technology refers to the adoption of AI algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling high-fidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental stud
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12114</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;ChatGPT&#12289;Flan-T5 UL2&#12289;Tk-Instruct&#21644;Alpaca&#8212;&#8212;&#22312;13&#20010;&#23454;&#38469;&#19990;&#30028;&#30340;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#38382;&#31572;&#65288;QA&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31561;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#30340;LLM&#24320;&#22987;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;QA&#20219;&#21153;&#34920;&#29616;&#24471;&#29305;&#21035;&#22909;&#65292;&#21363;&#20351;&#23427;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#34920;&#29616;&#20302;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#65288;&#22914;PubMedBERT&#65289;&#21487;&#20197;&#36798;&#21040;&#30340;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#27809;&#26377;&#19968;&#20010;LLM&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#20004;&#20154;&#40635;&#23558;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#31574;&#30053;&#25277;&#35937;&#23454;&#29616;&#20102;&#28216;&#25103;&#35770;&#20998;&#26512;&#65292;&#35813;&#26694;&#26550;&#21487;&#25512;&#24191;&#21040;&#20854;&#20182;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2307.12087</link><description>&lt;p&gt;
CFR-p: &#24102;&#26377;&#20998;&#23618;&#31574;&#30053;&#25277;&#35937;&#30340;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#21450;&#20854;&#22312;&#20004;&#20154;&#40635;&#23558;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CFR-p: Counterfactual Regret Minimization with Hierarchical Policy Abstraction, and its Application to Two-player Mahjong. (arXiv:2307.12087v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#20004;&#20154;&#40635;&#23558;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#31574;&#30053;&#25277;&#35937;&#23454;&#29616;&#20102;&#28216;&#25103;&#35770;&#20998;&#26512;&#65292;&#35813;&#26694;&#26550;&#21487;&#25512;&#24191;&#21040;&#20854;&#20182;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;(CFR)&#31639;&#27861;&#22312;&#24503;&#24030;&#25169;&#20811;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#21478;&#19968;&#31181;&#27969;&#34892;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#65292;&#40635;&#23558;&#12290;&#19982;&#25169;&#20811;&#28216;&#25103;&#30456;&#27604;&#65292;&#40635;&#23558;&#26356;&#21152;&#22797;&#26434;&#65292;&#26377;&#35768;&#22810;&#21464;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#21338;&#24328;&#35770;&#20998;&#26512;&#24182;&#22522;&#20110;&#33719;&#32988;&#31574;&#30053;&#23545;CFR&#36827;&#34892;&#20998;&#23618;&#25277;&#35937;&#65292;&#30740;&#31350;&#20102;&#20004;&#20154;&#40635;&#23558;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Regret Minimization(CFR) has shown its success in Texas Hold'em poker. We apply this algorithm to another popular incomplete information game, Mahjong. Compared to the poker game, Mahjong is much more complex with many variants. We study two-player Mahjong by conducting game theoretical analysis and making a hierarchical abstraction to CFR based on winning policies. This framework can be generalized to other imperfect information games.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#39034;&#24207;&#26102;&#38388;&#23439;&#21160;&#20316;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;&#26102;&#38388;&#35268;&#21010;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#21644;&#22788;&#29702;&#22797;&#26434;&#30340;&#36164;&#28304;&#20914;&#31361;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.12081</link><description>&lt;p&gt;
&#22686;&#24378;&#26102;&#38388;&#35268;&#21010;&#39046;&#22495;&#30340;&#36830;&#32493;&#23439;&#21160;&#20316;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Enhancing Temporal Planning Domains by Sequential Macro-actions (Extended Version). (arXiv:2307.12081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#39034;&#24207;&#26102;&#38388;&#23439;&#21160;&#20316;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;&#26102;&#38388;&#35268;&#21010;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#21644;&#22788;&#29702;&#22797;&#26434;&#30340;&#36164;&#28304;&#20914;&#31361;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#35268;&#21010;&#26159;&#32463;&#20856;&#35268;&#21010;&#30340;&#25193;&#23637;&#65292;&#28041;&#21450;&#34892;&#21160;&#30340;&#24182;&#21457;&#25191;&#34892;&#21644;&#19982;&#26102;&#38388;&#32422;&#26463;&#30340;&#23545;&#40784;&#12290;&#25345;&#32493;&#21160;&#20316;&#21644;&#19981;&#21464;&#37327;&#20351;&#24471;&#33021;&#22815;&#24314;&#27169;&#22810;&#20010;&#20195;&#29702;&#22312;&#20849;&#20139;&#36164;&#28304;&#19978;&#24182;&#34892;&#25805;&#20316;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#22312;&#36991;&#20813;&#36164;&#28304;&#20914;&#31361;&#26041;&#38754;&#36890;&#24120;&#24456;&#37325;&#35201;&#65292;&#20854;&#20013;&#26102;&#38388;&#32422;&#26463;&#30830;&#31435;&#20102;&#24182;&#21457;&#34892;&#21160;&#21644;&#20107;&#20214;&#30340;&#19968;&#33268;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#39046;&#22495;&#20013;&#30340;&#20195;&#29702;&#21644;&#23545;&#35937;&#25968;&#37327;&#21464;&#22823;&#26102;&#65292;&#26102;&#38388;&#35268;&#21010;&#24341;&#25806;&#30340;&#24615;&#33021;&#24448;&#24448;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#20351;&#29992;&#22312;&#32463;&#20856;&#35268;&#21010;&#30340;&#19978;&#19979;&#25991;&#20013;&#24050;&#32463;&#30740;&#31350;&#36807;&#30340;&#23439;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#35268;&#21010;&#35774;&#32622;&#20013;&#65292;&#24341;&#20837;&#23439;&#21160;&#20316;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#22240;&#20026;&#19981;&#24212;&#23436;&#20840;&#25233;&#21046;&#34892;&#21160;&#30340;&#24182;&#21457;&#25191;&#34892;&#21644;&#20849;&#20139;&#36164;&#28304;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#35201;&#28385;&#36275;&#26102;&#38388;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#39034;&#24207;&#26102;&#38388;&#23439;&#21160;&#20316;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal planning is an extension of classical planning involving concurrent execution of actions and alignment with temporal constraints. Durative actions along with invariants allow for modeling domains in which multiple agents operate in parallel on shared resources. Hence, it is often important to avoid resource conflicts, where temporal constraints establish the consistency of concurrent actions and events. Unfortunately, the performance of temporal planning engines tends to sharply deteriorate when the number of agents and objects in a domain gets large. A possible remedy is to use macro-actions that are well-studied in the context of classical planning. In temporal planning settings, however, introducing macro-actions is significantly more challenging when the concurrent execution of actions and shared use of resources, provided the compliance to temporal constraints, should not be suppressed entirely. Our work contributes a general concept of sequential temporal macro-actions t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#32806;&#21512;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#26469;&#30830;&#20445;&#20195;&#29702;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.12062</link><description>&lt;p&gt;
&#28216;&#25103;&#29702;&#35770;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26102;&#38388;&#32806;&#21512;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations. (arXiv:2307.12062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12062
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#32806;&#21512;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#26469;&#30830;&#20445;&#20195;&#29702;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#33021;&#22815;&#22312;&#29615;&#22659;&#24178;&#25200;&#25110;&#23545;&#25239;&#25915;&#20987;&#19979;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21487;&#33021;&#24178;&#25200;&#30340;&#31354;&#38388;&#22312;&#21508;&#20010;&#26102;&#38388;&#27493;&#39588;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#21487;&#33021;&#24178;&#25200;&#30340;&#31354;&#38388;&#21462;&#20915;&#20110;&#36807;&#21435;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#27491;&#24335;&#24341;&#20837;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#65292;&#23545;&#29616;&#26377;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRAD&#65292;&#19968;&#31181;&#26032;&#30340;&#28216;&#25103;&#29702;&#35770;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#32806;&#21512;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#12290;&#36890;&#36807;&#22312;&#36825;&#20010;&#28216;&#25103;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#22343;&#34913;&#65292;GRAD&#30830;&#20445;&#20102;&#20195;&#29702;&#30340;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23545;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#22312;&#26631;&#20934;&#21644;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#19979;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GPU&#19978;&#39640;&#25928;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30690;&#37327;&#26469;&#28155;&#21152;&#26032;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#38382;&#39064;&#36716;&#21270;&#20026;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#22788;&#29702;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.12059</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#24555;&#36895;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Fast Knowledge Graph Completion using Graphics Processing Units. (arXiv:2307.12059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GPU&#19978;&#39640;&#25928;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30690;&#37327;&#26469;&#28155;&#21152;&#26032;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#38382;&#39064;&#36716;&#21270;&#20026;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#22788;&#29702;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#22312;&#19982;&#25968;&#25454;&#35821;&#20041;&#30456;&#20851;&#30340;&#22810;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#65292;&#20363;&#22914;&#38382;&#31572;&#31995;&#32479;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26500;&#24314;&#30340;&#30693;&#35782;&#22270;&#35889;&#38656;&#35201;&#36890;&#36807;&#34917;&#20805;&#20851;&#31995;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#30693;&#35782;&#12290;&#36825;&#34987;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#20026;&#20102;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#21521;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#28155;&#21152;&#26032;&#20851;&#31995;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;N&#215;N&#215;R&#20010;&#30690;&#37327;&#25805;&#20316;&#65292;&#20854;&#20013;N&#26159;&#23454;&#20307;&#25968;&#37327;&#65292;R&#26159;&#20851;&#31995;&#31867;&#22411;&#25968;&#37327;&#12290;&#36825;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;GPU&#19978;&#39640;&#25928;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#20197;&#33719;&#21462;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30690;&#37327;&#30340;&#26032;&#20851;&#31995;&#12290;&#22312;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#8220;&#21487;&#20197;&#36716;&#25442;&#20026;&#24230;&#37327;&#31354;&#38388;&#8221;&#65292;&#28982;&#21518;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#38382;&#39064;&#36716;&#21270;&#20026;&#8220;&#21487;&#20197;&#36716;&#25442;&#20026;&#24230;&#37327;&#31354;&#38388;&#8221;&#30340;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20043;&#21518;&#65292;&#20026;&#20102;&#39640;&#25928;&#22788;&#29702;&#30456;&#20284;&#24615;&#36830;&#25509;&#38382;&#39064;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate $N\times N \times R$ vector operations, where $N$ is the number of entities and $R$ is the number of relation types. It is very costly.  In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define "transformable to a metric space" and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is "transformable to a metric space". After that, to efficiently process the similarity join problem, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;</title><link>http://arxiv.org/abs/2307.12057</link><description>&lt;p&gt;
&#22806;&#37096;&#25512;&#29702;&#65306;&#26397;&#30528;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20114;&#25442;&#36741;&#21161;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback. (arXiv:2307.12057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#34987;&#35748;&#20026;&#26159;&#20351;&#28023;&#39532;&#20307;&#21644;&#33041;&#31070;&#32463;&#20803;&#20869;&#20445;&#25345;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12289;&#38543;&#21518;&#29992;&#20110;&#35299;&#20915;&#36890;&#36807;&#23398;&#20064;&#19968;&#29983;&#20013;&#36935;&#21040;&#30340;&#29616;&#23454;&#25361;&#25112;&#30340;&#20851;&#38190;&#20154;&#31867;&#33021;&#21147;&#12290;&#36890;&#36807;&#24212;&#29992;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#35299;&#20915;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19968;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20687;GPT-3.5&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#20132;&#20114;&#21644;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#24191;&#27867;&#12289;&#19981;&#26029;&#28436;&#21464;&#30340;&#30693;&#35782;&#24211;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#27493;&#34880;&#31958;&#39044;&#27979;&#22120;&#19982;&#32447;&#24615;&#26102;&#21464;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#30456;&#32467;&#21512;&#30340;&#38381;&#29615;&#33008;&#23707;&#32032;&#36755;&#36865;&#31639;&#27861;&#35774;&#35745;&#65292;&#29992;&#20110;&#27835;&#30103;1&#22411;&#31958;&#23615;&#30149;&#12290;&#20316;&#32773;&#36890;&#36807;&#30452;&#25509;&#25311;&#21512;&#25972;&#20010;&#34880;&#31958;&#39044;&#27979;&#26354;&#32447;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#32447;&#24615;MPC&#36827;&#34892;&#23545;&#27604;&#65292;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#12290;</title><link>http://arxiv.org/abs/2307.12015</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#27493;&#39044;&#27979;&#34880;&#31958;&#39044;&#27979;&#22120;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#20154;&#24037;&#33008;&#33146;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors. (arXiv:2307.12015v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#27493;&#34880;&#31958;&#39044;&#27979;&#22120;&#19982;&#32447;&#24615;&#26102;&#21464;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#30456;&#32467;&#21512;&#30340;&#38381;&#29615;&#33008;&#23707;&#32032;&#36755;&#36865;&#31639;&#27861;&#35774;&#35745;&#65292;&#29992;&#20110;&#27835;&#30103;1&#22411;&#31958;&#23615;&#30149;&#12290;&#20316;&#32773;&#36890;&#36807;&#30452;&#25509;&#25311;&#21512;&#25972;&#20010;&#34880;&#31958;&#39044;&#27979;&#26354;&#32447;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#32447;&#24615;MPC&#36827;&#34892;&#23545;&#27604;&#65292;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#33008;&#23707;&#32032;&#36755;&#36865;&#31639;&#27861;&#30340;&#35774;&#35745;&#21644;\textit{in-silico}&#35780;&#20272;&#65292;&#29992;&#20110;&#27835;&#30103;1&#22411;&#31958;&#23615;&#30149;&#65288;T1D&#65289;&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#27493;&#21069;&#30651;&#34880;&#31958;(BG)&#39044;&#27979;&#22120;&#65292;&#19982;&#19968;&#20010;&#32447;&#24615;&#26102;&#21464;&#65288;LTV&#65289;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26694;&#26550;&#38598;&#25104;&#12290;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#25311;&#21512;&#25972;&#20010;BG&#39044;&#27979;&#26354;&#32447;&#65292;&#20316;&#20026;MPC&#20013;&#39044;&#23450;&#20041;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#36825;&#20010;&#38750;&#32447;&#24615;&#20989;&#25968;&#30001;&#36807;&#21435;&#30340;&#36755;&#20837;-&#36755;&#20986;&#25968;&#25454;&#21644;&#26410;&#26469;&#30340;&#33008;&#23707;&#32032;&#25511;&#21046;&#36755;&#20837;&#30340;&#32447;&#24615;&#20989;&#25968;&#32452;&#25104;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#32780;&#23545;&#20110;&#32447;&#24615;&#37096;&#20998;&#65292;&#36873;&#25321;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#19982;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#30340;&#20256;&#32479;&#32447;&#24615;MPC&#65288;&#22522;&#20110;&#33258;&#22238;&#24402;&#22806;&#29983;&#65288;ARX&#65289;&#36755;&#20837;&#27169;&#22411;&#65289;&#30456;&#27604;&#30340;&#20248;&#21155;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#27169;&#25311;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#25552;&#35758;&#30340;LSTM-MPC&#25511;&#21046;&#22120;&#65306;&#19968;&#20010;&#27491;&#24120;&#24773;&#20917;wit
&lt;/p&gt;
&lt;p&gt;
We present the design and \textit{in-silico} evaluation of a closed-loop insulin delivery algorithm to treat type 1 diabetes (T1D) consisting in a data-driven multi-step-ahead blood glucose (BG) predictor integrated into a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework. Instead of identifying an open-loop model of the glucoregulatory system from available data, we propose to directly fit the entire BG prediction over a predefined prediction horizon to be used in the MPC, as a nonlinear function of past input-ouput data and an affine function of future insulin control inputs. For the nonlinear part, a Long Short-Term Memory (LSTM) network is proposed, while for the affine component a linear regression model is chosen. To assess benefits and drawbacks when compared to a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case wit
&lt;/p&gt;</description></item><item><title>Psy-LLM&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25552;&#20379;&#38382;&#31572;&#26381;&#21153;&#65292;&#21069;&#31471;&#24037;&#20855;&#21487;&#35753;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#65292;&#21516;&#26102;&#36824;&#21487;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#36741;&#21161;&#35782;&#21035;&#32039;&#24613;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.11991</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#24515;&#29702;&#26381;&#21153;&#30340;Psy-LLM
&lt;/p&gt;
&lt;p&gt;
Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models. (arXiv:2307.11991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11991
&lt;/p&gt;
&lt;p&gt;
Psy-LLM&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25552;&#20379;&#38382;&#31572;&#26381;&#21153;&#65292;&#21069;&#31471;&#24037;&#20855;&#21487;&#35753;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#65292;&#21516;&#26102;&#36824;&#21487;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#36741;&#21161;&#35782;&#21035;&#32039;&#24613;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24515;&#29702;&#21672;&#35810;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#20840;&#29699;COVID-19&#30340;&#29190;&#21457;&#65292;&#36825;&#21152;&#24378;&#20102;&#21450;&#26102;&#21644;&#19987;&#19994;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#38656;&#27714;&#12290;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25104;&#20026;&#24212;&#23545;&#36825;&#19968;&#38656;&#27714;&#30340;&#20027;&#35201;&#26381;&#21153;&#26041;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Psy-LLM&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#20013;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;LLMs&#21644;&#20174;&#24515;&#29702;&#23398;&#23478;&#21644;&#24191;&#27867;&#25910;&#38598;&#30340;&#24515;&#29702;&#25991;&#31456;&#20013;&#33719;&#21462;&#30340;&#30495;&#23454;&#19990;&#30028;&#19987;&#19994;&#38382;&#31572;&#12290;Psy-LLM&#26694;&#26550;&#20316;&#20026;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#21069;&#31471;&#24037;&#20855;&#65292;&#20801;&#35768;&#20182;&#20204;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#26469;&#32531;&#35299;&#24739;&#32773;&#21387;&#21147;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#65292;&#35782;&#21035;&#38656;&#35201;&#36827;&#19968;&#27493;&#21327;&#21161;&#30340;&#32039;&#24613;&#26696;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#22256;&#24785;&#24230;&#31561;&#20869;&#22312;&#24230;&#37327;&#26631;&#20934;&#21644;&#22806;&#37096;&#24230;&#37327;&#26631;&#20934;&#23545;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for psychological counseling has grown significantly in recent years, particularly with the global outbreak of COVID-19, which has heightened the need for timely and professional mental health support. Online psychological counseling has emerged as the predominant mode of providing services in response to this demand. In this study, we propose the Psy-LLM framework, an AI-based system leveraging Large Language Models (LLMs) for question-answering in online psychological consultation. Our framework combines pre-trained LLMs with real-world professional Q&amp;A from psychologists and extensively crawled psychological articles. The Psy-LLM framework serves as a front-end tool for healthcare professionals, allowing them to provide immediate responses and mindfulness activities to alleviate patient stress. Additionally, it functions as a screening tool to identify urgent cases requiring further assistance. We evaluated the framework using intrinsic metrics, such as perplexity, and ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35270;&#35273;Transformer&#27169;&#22411;&#19978;&#24212;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#21644;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20043;&#21518;&#36827;&#34892;&#21098;&#26525;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#35745;&#31639;&#36127;&#25285;&#32780;&#19981;&#25439;&#22833;&#22826;&#22810;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11988</link><description>&lt;p&gt;
&#31232;&#30095;&#21270;&#20877;&#21098;&#26525;&#65306;&#21521;&#39640;&#25928;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
Sparse then Prune: Toward Efficient Vision Transformers. (arXiv:2307.11988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35270;&#35273;Transformer&#27169;&#22411;&#19978;&#24212;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#21644;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20043;&#21518;&#36827;&#34892;&#21098;&#26525;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#35745;&#31639;&#36127;&#25285;&#32780;&#19981;&#25439;&#22833;&#22826;&#22810;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#27169;&#22411;&#26159;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;Transformer&#27169;&#22411;&#25104;&#21151;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#33258;&#27880;&#24847;&#26426;&#21046;&#12289;&#22823;&#37327;&#21442;&#25968;&#20197;&#21450;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#20173;&#20351;&#24471;&#35270;&#35273;Transformer&#35745;&#31639;&#19978;&#30340;&#36127;&#25285;&#36739;&#37325;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35270;&#35273;Transformer&#19978;&#24212;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#19978;&#65292;&#26080;&#35770;&#26159;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20043;&#21518;&#36824;&#26159;&#20043;&#21069;&#36827;&#34892;&#21098;&#26525;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;&#27491;&#21017;&#21270;&#21644;&#21098;&#26525;&#26041;&#27861;&#24212;&#29992;&#20110;&#29992;&#20110;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-100&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#12290;&#35270;&#35273;Transformer&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#65306;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#39044;&#35757;&#32451;&#20351;&#29992;ImageNet21K&#25968;&#25454;&#65292;&#28982;&#21518;&#36827;&#34892;20&#20010;epoch&#30340;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22312;CIFAR-100&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#26368;&#20339;&#25928;&#26524;&#26159;&#36890;&#36807;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20043;&#21518;&#36827;&#34892;&#21098;&#26525;&#24471;&#21040;&#30340;&#65292;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#22826;&#22810;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Vision Transformer architecture is a deep learning model inspired by the success of the Transformer model in Natural Language Processing. However, the self-attention mechanism, large number of parameters, and the requirement for a substantial amount of training data still make Vision Transformers computationally burdensome. In this research, we investigate the possibility of applying Sparse Regularization to Vision Transformers and the impact of Pruning, either after Sparse Regularization or without it, on the trade-off between performance and efficiency. To accomplish this, we apply Sparse Regularization and Pruning methods to the Vision Transformer architecture for image classification tasks on the CIFAR-10, CIFAR-100, and ImageNet-100 datasets. The training process for the Vision Transformer model consists of two parts: pre-training and fine-tuning. Pre-training utilizes ImageNet21K data, followed by fine-tuning for 20 epochs. The results show that when testing with CIFAR-100 an
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#21442;&#30340;&#26041;&#24335;&#36866;&#24212;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#19988;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20851;&#38190;&#21407;&#22240;&#21253;&#25324;&#22266;&#23450;&#30340;&#31867;&#21517;&#26631;&#35760;&#23545;&#27169;&#22411;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#20316;&#29992;&#20197;&#21450;&#20174;&#22810;&#26679;&#19988;&#36890;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#24378;&#22823;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#25552;&#20379;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.11978</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#35843;&#21442;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?. (arXiv:2307.11978v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11978
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#21442;&#30340;&#26041;&#24335;&#36866;&#24212;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#19988;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20851;&#38190;&#21407;&#22240;&#21253;&#25324;&#22266;&#23450;&#30340;&#31867;&#21517;&#26631;&#35760;&#23545;&#27169;&#22411;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#20316;&#29992;&#20197;&#21450;&#20174;&#22810;&#26679;&#19988;&#36890;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#24378;&#22823;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#25552;&#20379;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#36890;&#36807;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#20102;&#36890;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23884;&#20837;&#12290;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#21442;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20351;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#25552;&#31034;&#35843;&#21442;&#36807;&#31243;&#23545;&#20110;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#30740;&#31350;&#25552;&#31034;&#35843;&#21442;&#33539;&#24335;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#21407;&#22240;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#20851;&#38190;&#22240;&#32032;&#21253;&#25324;&#65306;1&#65289;&#22266;&#23450;&#30340;&#31867;&#21517;&#26631;&#35760;&#23545;&#27169;&#22411;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#27491;&#21017;&#21270;&#20316;&#29992;&#65292;&#20943;&#23569;&#20102;&#22122;&#22768;&#26679;&#26412;&#24341;&#36215;&#30340;&#26799;&#24230;&#65307;2&#65289;&#20174;&#22810;&#26679;&#19988;&#36890;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#20026;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#21033;&#29992;CLIP&#20013;&#30340;&#22122;&#22768;&#38646;&#26679;&#26412;&#39044;&#27979;&#26469;&#35843;&#25972;&#20854;&#33258;&#36523;&#30340;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/CE&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CE
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30693;&#35782;&#25429;&#25417;&#24182;&#24418;&#24335;&#21270;&#20026;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20351;&#29992;&#22522;&#20110;&#26679;&#26412;&#30340;&#22312;&#32447;&#35299;&#20915;&#26041;&#27861;&#26469;&#25512;&#21160;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.11954</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On-Robot Bayesian Reinforcement Learning for POMDPs. (arXiv:2307.11954v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30693;&#35782;&#25429;&#25417;&#24182;&#24418;&#24335;&#21270;&#20026;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20351;&#29992;&#22522;&#20110;&#26679;&#26412;&#30340;&#22312;&#32447;&#35299;&#20915;&#26041;&#27861;&#26469;&#25512;&#21160;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#36739;&#39640;&#65292;&#26426;&#22120;&#20154;&#23398;&#20064;&#24448;&#24448;&#22256;&#38590;&#37325;&#37325;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#20805;&#20998;&#21033;&#29992;&#19987;&#23478;&#23545;&#26426;&#22120;&#20154;&#21160;&#24577;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#20915;&#22823;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22522;&#20110;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#65288;BRL&#65289;&#30001;&#20110;&#20854;&#26679;&#26412;&#25928;&#29575;&#21644;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#21033;&#29992;&#33021;&#21147;&#65292;&#22312;&#36825;&#19968;&#38382;&#39064;&#19978;&#29420;&#20855;&#20248;&#21183;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#34920;&#36798;&#19987;&#23478;&#30693;&#35782;&#21644;&#35299;&#20915;&#21518;&#32493;&#25512;&#29702;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;BRL&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#29289;&#29702;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#25512;&#21160;&#20102;&#26426;&#22120;&#20154;&#20013;&#30340;BRL&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20197;&#20998;&#35299;&#34920;&#31034;&#30340;&#24418;&#24335;&#25429;&#25417;&#36825;&#20123;&#30693;&#35782;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#21518;&#39564;&#27010;&#29575;&#30340;&#20998;&#35299;&#24418;&#24335;&#65292;&#24182;&#26368;&#32456;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#23558;&#27169;&#22411;&#24418;&#24335;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#31890;&#23376;&#28388;&#27874;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#22312;&#32447;&#35299;&#20915;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30149;&#29702;&#23398;&#21644;&#22522;&#22240;&#32452;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#65288;PathOmics&#65289;&#65292;&#29992;&#20110;&#32467;&#32928;&#30456;&#20851;&#30284;&#30151;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#25429;&#25417;&#30149;&#29702;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#24494;&#35843;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;TCGA&#32467;&#32928;&#21644;&#30452;&#32928;&#30284;&#38431;&#21015;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11952</link><description>&lt;p&gt;
&#30149;&#29702;&#23398;&#21644;&#22522;&#22240;&#32452;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#29992;&#20110;&#29983;&#23384;&#32467;&#23616;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction. (arXiv:2307.11952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11952
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30149;&#29702;&#23398;&#21644;&#22522;&#22240;&#32452;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#65288;PathOmics&#65289;&#65292;&#29992;&#20110;&#32467;&#32928;&#30456;&#20851;&#30284;&#30151;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#25429;&#25417;&#30149;&#29702;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#24494;&#35843;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;TCGA&#32467;&#32928;&#21644;&#30452;&#32928;&#30284;&#38431;&#21015;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#32467;&#23616;&#35780;&#20272;&#22312;&#30284;&#30151;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19982;&#22810;&#31181;&#20020;&#24202;&#22240;&#32032;&#65288;&#22914;&#25104;&#20687;&#21644;&#22522;&#22240;&#32452;&#29983;&#29289;&#26631;&#24535;&#29289;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#23454;&#29616;&#22810;&#27169;&#24577;&#20998;&#26512;&#26377;&#26395;&#25581;&#31034;&#24739;&#32773;&#32467;&#23616;&#30340;&#26032;&#30340;&#39044;&#27979;&#27169;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#65288;PathOmics&#65289;&#65292;&#23558;&#30149;&#29702;&#23398;&#21644;&#22522;&#22240;&#32452;&#27934;&#23519;&#21147;&#25972;&#21512;&#21040;&#32467;&#32928;&#30456;&#20851;&#30284;&#30151;&#29983;&#23384;&#39044;&#27979;&#20013;&#12290;&#25105;&#20204;&#24378;&#35843;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20197;&#25429;&#33719;&#21513;&#27604;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#21508;&#31181;&#22522;&#22240;&#32452;&#25968;&#25454;&#65288;&#22914;mRNA&#24207;&#21015;&#12289;&#25335;&#36125;&#25968;&#21464;&#24322;&#21644;&#30002;&#22522;&#21270;&#65289;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#39044;&#35757;&#32451;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#30693;&#35782;&#32858;&#21512;&#21518;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#24494;&#35843;&#21487;&#20197;&#25193;&#23637;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#25968;&#25454;&#65288;&#22914;&#20165;&#22270;&#20687;&#25110;&#22522;&#22240;&#32452;&#65289;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#33539;&#22260;&#12290;&#25105;&#20204;&#22312;TCGA&#32467;&#32928;&#21644;&#30452;&#32928;&#30284;&#38431;&#21015;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival outcome assessment is challenging and inherently associated with multiple clinical factors (e.g., imaging and genomics biomarkers) in cancer. Enabling multimodal analytics promises to reveal novel predictive patterns of patient outcomes. In this study, we propose a multimodal transformer (PathOmics) integrating pathology and genomics insights into colon-related cancer survival prediction. We emphasize the unsupervised pretraining to capture the intrinsic interaction between tissue microenvironments in gigapixel whole slide images (WSIs) and a wide range of genomics data (e.g., mRNA-sequence, copy number variant, and methylation). After the multimodal knowledge aggregation in pretraining, our task-specific model finetuning could expand the scope of data utility applicable to both multi- and single-modal data (e.g., image- or genomics-only). We evaluate our approach on both TCGA colon and rectum cancer cohorts, showing that the proposed approach is competitive and outperforms st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.11949</link><description>&lt;p&gt;
HIQL: &#20197;&#28508;&#22312;&#29366;&#24577;&#20316;&#20026;&#21160;&#20316;&#30340;&#31163;&#32447;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26368;&#36817;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#65288;&#26080;&#22870;&#21169;&#65289;&#25968;&#25454;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#30452;&#25509;&#20174;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20934;&#30830;&#20272;&#35745;&#36828;&#26399;&#30446;&#26631;&#30340;&#20215;&#20540;&#20989;&#25968;&#24456;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#21363;&#36798;&#21040;&#36828;&#26399;&#30446;&#26631;&#38656;&#35201;&#39318;&#20808;&#36890;&#36807;&#36739;&#36817;&#23376;&#30446;&#26631;&#12290;&#36825;&#31181;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35780;&#20272;&#37051;&#36817;&#30446;&#26631;&#30340;&#21160;&#20316;&#36136;&#37327;&#36890;&#24120;&#27604;&#26356;&#36828;&#30446;&#26631;&#23481;&#26131;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;&#21033;&#29992;&#19968;&#20010;&#27809;&#26377;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65306;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLINDER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#26465;&#20214;&#19979;&#29366;&#24577;&#25551;&#36848;&#30340;&#20540;&#20989;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#31616;&#26126;&#30340;&#29366;&#24577;&#25551;&#36848;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#28436;&#21592;&#22312;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.11922</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#24863;&#30693;&#65306;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20026;&#35821;&#35328;&#27169;&#22411;&#28436;&#21592;&#20248;&#21270;&#29366;&#24577;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors. (arXiv:2307.11922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLINDER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#26465;&#20214;&#19979;&#29366;&#24577;&#25551;&#36848;&#30340;&#20540;&#20989;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#31616;&#26126;&#30340;&#29366;&#24577;&#25551;&#36848;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#28436;&#21592;&#22312;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#21644;&#28216;&#25103;&#31561;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#30340;&#28436;&#21592;&#20013;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24456;&#23569;&#25506;&#32034;&#36890;&#36807;&#35821;&#35328;&#21521;LLM&#28436;&#21592;&#25552;&#20379;&#20160;&#20040;&#29615;&#22659;&#29366;&#24577;&#20449;&#24687;&#12290;&#35814;&#23613;&#25551;&#36848;&#39640;&#32500;&#29366;&#24577;&#21487;&#33021;&#20250;&#24433;&#21709;&#24615;&#33021;&#24182;&#22686;&#21152;LLM&#28436;&#21592;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#20197;&#21069;&#30340;LLM&#28436;&#21592;&#36890;&#36807;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#21327;&#35758;&#26469;&#30830;&#23450;&#35813;&#29366;&#24577;&#30340;&#21738;&#20123;&#29305;&#24449;&#38656;&#35201;&#36827;&#34892;&#20256;&#36882;&#65292;&#21738;&#20123;&#19981;&#38656;&#35201;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLINDER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#26465;&#20214;&#19979;&#29366;&#24577;&#25551;&#36848;&#30340;&#20540;&#20989;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#31616;&#26126;&#30340;&#29366;&#24577;&#25551;&#36848;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;NetHack&#21644;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;BLINDER&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#20943;&#23569;&#20102;&#36755;&#20837;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and gen
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35745;&#37327;&#20998;&#26512;&#20102;&#39030;&#32423;100&#23478;&#23398;&#26415;&#20986;&#29256;&#21830;&#21644;&#26399;&#21002;&#23545;&#20110;&#20316;&#32773;&#22312;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#65288;GAI&#65289;&#12289;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24037;&#20855;&#26041;&#38754;&#30340;&#25351;&#23548;&#31243;&#24230;&#21644;&#20869;&#23481;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26377;17%&#30340;&#20986;&#29256;&#21830;&#21644;70%&#30340;&#26399;&#21002;&#25552;&#20379;&#20102;&#26377;&#20851;GAI&#30340;&#25351;&#23548;&#65292;&#24182;&#19988;&#22823;&#37096;&#20998;&#31105;&#27490;&#23558;GAI&#20316;&#20026;&#20316;&#32773;&#30340;&#19968;&#37096;&#20998;&#12290;&#21516;&#26102;&#20063;&#21457;&#29616;&#20102;&#19968;&#23450;&#30340;&#21464;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11918</link><description>&lt;p&gt;
&#23398;&#26415;&#21644;&#31185;&#23398;&#20986;&#29256;&#20013;&#20851;&#20110;&#29983;&#25104;&#24335;AI&#30340;&#20986;&#29256;&#21830;&#21644;&#26399;&#21002;&#20316;&#32773;&#25351;&#21335;&#30340;&#35745;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bibliometric Analysis of Publisher and Journal Instructions to Authors on Generative-AI in Academic and Scientific Publishing. (arXiv:2307.11918v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35745;&#37327;&#20998;&#26512;&#20102;&#39030;&#32423;100&#23478;&#23398;&#26415;&#20986;&#29256;&#21830;&#21644;&#26399;&#21002;&#23545;&#20110;&#20316;&#32773;&#22312;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#65288;GAI&#65289;&#12289;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24037;&#20855;&#26041;&#38754;&#30340;&#25351;&#23548;&#31243;&#24230;&#21644;&#20869;&#23481;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26377;17%&#30340;&#20986;&#29256;&#21830;&#21644;70%&#30340;&#26399;&#21002;&#25552;&#20379;&#20102;&#26377;&#20851;GAI&#30340;&#25351;&#23548;&#65292;&#24182;&#19988;&#22823;&#37096;&#20998;&#31105;&#27490;&#23558;GAI&#20316;&#20026;&#20316;&#32773;&#30340;&#19968;&#37096;&#20998;&#12290;&#21516;&#26102;&#20063;&#21457;&#29616;&#20102;&#19968;&#23450;&#30340;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#20851;&#20110;&#23398;&#26415;&#20986;&#29256;&#21830;&#21644;&#26399;&#21002;&#30340;&#39030;&#32423;100&#21517;&#20986;&#29256;&#21830;&#21644;&#26399;&#21002;&#23545;&#20110;&#20316;&#32773;&#22312;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#65288;GAI&#65289;&#12289;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24037;&#20855;&#26041;&#38754;&#30340;&#25351;&#23548;&#31243;&#24230;&#21644;&#20869;&#23481;&#12290;&#23545;&#36825;&#20123;&#20986;&#29256;&#21830;&#21644;&#26399;&#21002;&#30340;&#32593;&#31449;&#36827;&#34892;&#20102;2023&#24180;5&#26376;19&#26085;&#33267;20&#26085;&#30340;&#31579;&#36873;&#12290;&#22312;&#26368;&#22823;&#30340;100&#23478;&#20986;&#29256;&#21830;&#20013;&#65292;&#26377;17%&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;GAI&#30340;&#25351;&#23548;&#65292;&#20854;&#20013;12&#23478;&#65288;70.6%&#65289;&#23646;&#20110;&#21069;25&#22823;&#20986;&#29256;&#21830;&#12290;&#22312;&#21069;100&#21517;&#26399;&#21002;&#20013;&#65292;&#26377;70%&#25552;&#20379;&#20102;&#26377;&#20851;GAI&#30340;&#25351;&#23548;&#12290;&#22312;&#26377;&#25351;&#23548;&#30340;&#26399;&#21002;&#20013;&#65292;94.1%&#30340;&#20986;&#29256;&#21830;&#21644;95.7%&#30340;&#26399;&#21002;&#31105;&#27490;&#20316;&#32773;&#23558;GAI&#20316;&#20026;&#20316;&#32773;&#30340;&#19968;&#37096;&#20998;&#12290;&#20854;&#20013;&#26377;&#22235;&#20010;&#26399;&#21002;&#65288;5.7%&#65289;&#26126;&#30830;&#31105;&#27490;&#22312;&#25776;&#20889;&#31295;&#20214;&#26102;&#20351;&#29992;GAI&#65292;&#32780;3&#20010;&#20986;&#29256;&#21830;&#65288;17.6%&#65289;&#21644;15&#20010;&#26399;&#21002;&#65288;21.4%&#65289;&#34920;&#31034;&#20182;&#20204;&#30340;&#25351;&#23548;&#21482;&#36866;&#29992;&#20110;&#25776;&#20889;&#36807;&#31243;&#12290;&#22312;&#25259;&#38706;&#20351;&#29992;GAI&#26102;&#65292;42.8%&#30340;&#20986;&#29256;&#21830;&#21644;44.3%&#30340;&#26399;&#21002;&#21253;&#21547;&#20102;&#20855;&#20307;&#30340;&#25259;&#38706;&#26631;&#20934;&#12290;&#23384;&#22312;&#19968;&#23450;&#30340;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to determine the extent and content of guidance for authors regarding the use of generative-AI (GAI), Generative Pretrained models (GPTs) and Large Language Models (LLMs) powered tools among the top 100 academic publishers and journals in science. The websites of these publishers and journals were screened from between 19th and 20th May 2023. Among the largest 100 publishers, 17% provided guidance on the use of GAI, of which 12 (70.6%) were among the top 25 publishers. Among the top 100 journals, 70% have provided guidance on GAI. Of those with guidance, 94.1% of publishers and 95.7% of journals prohibited the inclusion of GAI as an author. Four journals (5.7%) explicitly prohibit the use of GAI in the generation of a manuscript, while 3 (17.6%) publishers and 15 (21.4%) journals indicated their guidance exclusively applies to the writing process. When disclosing the use of GAI, 42.8% of publishers and 44.3% of journals included specific disclosure criteria. There was variabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hindsight-DICE&#31639;&#27861;&#65292;&#21033;&#29992;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#25913;&#21892;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11897</link><description>&lt;p&gt;
Hindsight-DICE&#65306;&#31283;&#23450;&#20449;&#29992;&#20998;&#37197;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning. (arXiv:2307.11897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hindsight-DICE&#31639;&#27861;&#65292;&#21033;&#29992;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#25913;&#21892;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#29615;&#22659;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#35780;&#20272;&#21453;&#39304;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#34892;&#20026;&#30340;&#38271;&#26102;&#38388;&#36712;&#36857;&#20165;&#20197;&#19968;&#20010;&#32456;&#27490;&#20449;&#21495;&#26631;&#35760;&#65292;&#23548;&#33268;&#35266;&#23519;&#21040;&#38750;&#24179;&#20961;&#22870;&#21169;&#21644;&#35302;&#21457;&#27492;&#31867;&#21453;&#39304;&#30340;&#20010;&#20307;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#26102;&#38388;&#24310;&#36831;&#12290;&#35299;&#20915;&#36825;&#31181;&#20449;&#29992;&#20998;&#37197;&#25361;&#25112;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#26469;&#26174;&#33879;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#22788;&#29702;&#12290;&#34429;&#28982;&#20351;&#29992;&#25152;&#35859;&#30340;&#20107;&#21518;&#31574;&#30053;&#20026;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#36820;&#22238;&#36820;&#22238;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26426;&#21046;&#65292;&#20294;&#26159;&#31616;&#21333;&#22320;&#24212;&#29992;&#37325;&#35201;&#25277;&#26679;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#25110;&#36807;&#24230;&#28382;&#21518;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, engendering a significant temporal delay between the observation of non-trivial reward and the individual steps of behavior culpable for eliciting such feedback. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning and, in this work, we capitalize on existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the handling of credit assignment with policy-gradient methods. While the use of so-called hindsight policies offers a principled mechanism for reweighting on-policy data by saliency to the observed trajectory return, naively applying importance sampling results in unstable or excessively lagged learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11892</link><description>&lt;p&gt;
&#20851;&#20110;&#21463;&#24694;&#24847;&#22122;&#22768;&#24433;&#21709;&#30340;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11892
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#24494;&#23567;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#12290;Konstantinov&#21644;Lampert (2021)&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#36127;&#38754;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#19981;&#24179;&#34913;&#30340;&#32676;&#32452;&#22823;&#23567;&#19979;&#23384;&#22312;&#19968;&#20123;&#25968;&#25454;&#20998;&#24067;&#65292;&#20219;&#20309;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#37117;&#20250;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#20048;&#35266;&#30340;&#35266;&#28857;&#65292;&#22914;&#26524;&#20801;&#35768;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#21017;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$\Theta(\alpha)$&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#20854;&#20013;$\alpha$&#26159;&#24694;&#24847;&#22122;&#22768;&#29575;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#27809;&#26377;&#20844;&#27491;&#32422;&#26463;&#30340;&#24773;&#20917;&#23436;&#20840;&#21305;&#37197;&#12290;&#23545;&#20110;&#26426;&#20250;&#22343;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$O(\sqrt{\alpha})$&#30340;&#25439;&#22833;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21305;&#37197;&#30340;$\Omega(\sqrt{\alpha})$&#30340;&#19979;&#30028;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Konstantinov&#21644;Lampert (2021)&#31034;&#33539;&#20102;&#23545;&#20110;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#65292;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#31934;&#24230;&#25439;&#22833;&#37117;&#26159;$\Omega(1)$&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24212;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#21644;&#25552;&#39640;&#38134;&#34892;&#19994;&#21153;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38134;&#34892;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;LayoutXLM&#31561;&#27169;&#22411;&#22312;&#20998;&#26512;&#38134;&#34892;&#25991;&#26723;&#20013;&#30340;&#28508;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11845</link><description>&lt;p&gt;
&#38754;&#21521;&#38134;&#34892;&#27969;&#31243;&#33258;&#21160;&#21270;&#30340;&#22810;&#27169;&#24335;&#25991;&#26723;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multimodal Document Analytics for Banking Process Automation. (arXiv:2307.11845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24212;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#21644;&#25552;&#39640;&#38134;&#34892;&#19994;&#21153;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38134;&#34892;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;LayoutXLM&#31561;&#27169;&#22411;&#22312;&#20998;&#26512;&#38134;&#34892;&#25991;&#26723;&#20013;&#30340;&#28508;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#30340;&#22686;&#38271;&#21644;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#20110;&#29702;&#35299;&#22312;&#38134;&#34892;&#27969;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23545;&#22810;&#26679;&#21270;&#30340;&#38134;&#34892;&#25991;&#26723;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20808;&#36827;&#30340;&#20998;&#26512;&#25216;&#26415;&#22312;&#23458;&#25143;&#19994;&#21153;&#20013;&#25552;&#39640;&#25928;&#29575;&#30340;&#26426;&#20250;&#12290;&#22522;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35832;&#22914;LayoutXLM&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#23427;&#26159;&#19968;&#31181;&#36328;&#35821;&#35328;&#12289;&#22810;&#27169;&#24335;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#38134;&#34892;&#19994;&#20013;&#21508;&#31181;&#19981;&#21516;&#30340;&#25991;&#26723;&#12290;&#35813;&#27169;&#22411;&#23545;&#24503;&#22269;&#20844;&#21496;&#30331;&#35760;&#25552;&#21462;&#30340;&#25991;&#26412;&#26631;&#35760;&#20998;&#31867;&#20855;&#26377;&#22823;&#32422;80%&#30340;F1&#24471;&#20998;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#35777;&#23454;&#20102;&#24067;&#23616;&#20449;&#24687;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#25972;&#21512;&#22270;&#20687;&#20449;&#24687;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to growing FinTech competition and the need for improved operational efficiency, this research focuses on understanding the potential of advanced document analytics, particularly using multimodal models, in banking processes. We perform a comprehensive analysis of the diverse banking document landscape, highlighting the opportunities for efficiency gains through automation and advanced analytics techniques in the customer business. Building on the rapidly evolving field of natural language processing (NLP), we illustrate the potential of models such as LayoutXLM, a cross-lingual, multimodal, pre-trained model, for analyzing diverse documents in the banking sector. This model performs a text token classification on German company register extracts with an overall F1 score performance of around 80\%. Our empirical evidence confirms the critical role of layout information in improving model performance and further underscores the benefits of integrating image information. Inte
&lt;/p&gt;</description></item><item><title>HybridAugment++&#26159;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#39057;&#25104;&#20998;&#30340;&#20381;&#36182;&#24182;&#20419;&#36827;&#30456;&#20301;&#20449;&#24687;&#24212;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23427;&#36890;&#36807;&#32479;&#19968;&#21508;&#31181;&#39057;&#35889;&#22686;&#24378;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11823</link><description>&lt;p&gt;
HybridAugment++&#65306;&#29992;&#20110;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#32479;&#19968;&#39057;&#35889;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness. (arXiv:2307.11823v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11823
&lt;/p&gt;
&lt;p&gt;
HybridAugment++&#26159;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#39057;&#25104;&#20998;&#30340;&#20381;&#36182;&#24182;&#20419;&#36827;&#30456;&#20301;&#20449;&#24687;&#24212;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23427;&#36890;&#36807;&#32479;&#19968;&#21508;&#31181;&#39057;&#35889;&#22686;&#24378;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#25968;&#25454;&#20998;&#24067;&#36716;&#21464;&#19979;&#34920;&#29616;&#20986;&#24046;&#21170;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#24050;&#32463;&#23545;&#23427;&#20204;&#30340;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#32780;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#20174;&#39057;&#29575;&#35282;&#24230;&#32771;&#34385;&#38382;&#39064;&#12290;&#36825;&#20123;&#30740;&#31350;&#25351;&#20986;&#65292;&#20154;&#31867;&#21644;CNN&#21487;&#33021;&#20250;&#20851;&#27880;&#22270;&#20687;&#30340;&#19981;&#21516;&#39057;&#29575;&#25104;&#20998;&#12290;&#39318;&#20808;&#65292;&#21463;&#21040;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;HybridAugment&#65292;&#23427;&#20943;&#23569;&#20102;CNN&#23545;&#39640;&#39057;&#25104;&#20998;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#20445;&#25345;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HierarchicalAugment++&#65292;&#36825;&#26159;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#32479;&#19968;&#21508;&#31181;&#39057;&#35889;&#22686;&#24378;&#26041;&#27861;&#12290;HierarchicalAugment++&#22522;&#20110;HybridAugment&#65292;&#20943;&#23569;&#20102;CNN&#23545;&#22270;&#20687;&#25391;&#24133;&#20998;&#37327;&#30340;&#20381;&#36182;&#65292;&#24182;&#25512;&#21160;&#30456;&#20301;&#20449;&#24687;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#32479;&#19968;&#32467;&#26524;&#21477;&#23545;&#25239;&#29616;&#26377;&#25216;&#26415;&#30340;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNN) are known to exhibit poor generalization performance under distribution shifts. Their generalization have been studied extensively, and one line of work approaches the problem from a frequency-centric perspective. These studies highlight the fact that humans and CNNs might focus on different frequency components of an image. First, inspired by these observations, we propose a simple yet effective data augmentation method HybridAugment that reduces the reliance of CNNs on high-frequency components, and thus improves their robustness while keeping their clean accuracy high. Second, we propose HybridAugment++, which is a hierarchical augmentation method that attempts to unify various frequency-spectrum augmentations. HybridAugment++ builds on HybridAugment, and also reduces the reliance of CNNs on the amplitude component of images, and promotes phase information instead. This unification results in competitive to or better than state-of-the-art results 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#20102;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#25193;&#23637;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#21487;&#20197;&#20923;&#32467;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#21407;&#26377;&#21151;&#33021;&#65292;&#24182;&#19988;&#25552;&#21319;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#35268;&#27169;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11795</link><description>&lt;p&gt;
&#29992;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Speech Recognition Abilities. (arXiv:2307.11795v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#20102;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#25193;&#23637;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#21487;&#20197;&#20923;&#32467;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#21407;&#26377;&#21151;&#33021;&#65292;&#24182;&#19988;&#25552;&#21319;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#35268;&#27169;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#35777;&#26126;&#20854;&#39640;&#24230;&#28789;&#27963;&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#27010;&#25324;&#24615;&#25688;&#35201;&#21644;&#24320;&#25918;&#24615;&#38382;&#31572;&#12290;&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#38468;&#21152;&#19968;&#20010;&#23567;&#22411;&#38899;&#39057;&#32534;&#30721;&#22120;&#26469;&#25193;&#23637;LLM&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#25191;&#34892;&#35821;&#38899;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;&#19968;&#31995;&#21015;&#22768;&#38899;&#23884;&#20837;&#30452;&#25509;&#39044;&#32622;&#21040;&#25991;&#26412;&#20196;&#29260;&#23884;&#20837;&#20043;&#21069;&#65292;LLM&#21487;&#20197;&#36716;&#25442;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20854;&#25991;&#26412;&#23545;&#24212;&#29289;&#20197;&#23436;&#20840;&#30456;&#21516;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#22312;&#22810;&#35821;&#35328;LibriSpeech&#65288;MLS&#65289;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#19968;&#20010;conformer&#32534;&#30721;&#22120;&#34701;&#20837;&#21040;&#24320;&#28304;&#30340;LLaMA-7B&#20013;&#65292;&#20351;&#20854;&#22312;&#21333;&#19968;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;18%&#65292;&#24182;&#33021;&#22815;&#25191;&#34892;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#65292;&#23613;&#31649;LLaMA&#30340;&#35757;&#32451;&#20027;&#35201;&#20381;&#36182;&#20110;&#33521;&#25991;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;LLM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#21487;&#20197;&#23436;&#20840;&#20923;&#32467;&#20197;&#20445;&#25345;&#20854;&#21407;&#26377;&#21151;&#33021;&#65292;&#20197;&#21450;&#25552;&#21319;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11788</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#36136;&#37327;&#25913;&#36827;&#20063;&#33021;&#20135;&#29983;&#24040;&#22823;&#20215;&#20540;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#37329;&#34701;&#26159;&#26089;&#26399;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#32773;&#12290;&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;DisCoCat&#21644;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QNLP)&#36825;&#20004;&#31181;&#20013;&#24515;&#26041;&#27861;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ChatGPT&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1000&#22810;&#20010;&#30495;&#23454;&#21477;&#23376;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;QLSTM&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;DisCoCat&#24555;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#20063;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an application domain where the slightest qualitative improvements can yield immense value, finance is a promising candidate for early quantum advantage. Focusing on the rapidly advancing field of Quantum Natural Language Processing (QNLP), we explore the practical applicability of the two central approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data generation approach, we conduct a case study with more than 1000 realistic sentences and find that QLSTMs can be trained substantially faster than DisCoCat while also achieving close to classical results for their available software implementations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.11787</link><description>&lt;p&gt;
LLM&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#26377;&#25152;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#34429;&#28982;&#36825;&#31867;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#23427;&#20204;&#20316;&#20026;&#35748;&#30693;&#20027;&#20307;&#30340;&#35843;&#26597;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#23545;GPT-3&#21644;ChatGPT&#22312;&#19968;&#20010;&#26469;&#33258;&#35748;&#30693;&#31185;&#23398;&#25991;&#29486;&#30340;&#26377;&#38480;&#25968;&#25454;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#23398;&#20064;&#26426;&#21046;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#39564;&#35777;&#26041;&#27861;&#26469;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#32479;&#35745;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2307.11784</link><description>&lt;p&gt;
&#31350;&#31455;&#20160;&#20040;&#26159;&#21487;&#23454;&#29616;&#30340;&#21487;&#35777;&#26126;&#20445;&#38556;&#30340;&#23398;&#20064;&#26426;&#21046;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems. (arXiv:2307.11784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23398;&#20064;&#26426;&#21046;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#39564;&#35777;&#26041;&#27861;&#26469;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#32479;&#35745;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#33258;&#20449;&#22320;&#21033;&#29992;&#23398;&#20064;&#26426;&#21046;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#25361;&#25112;&#20013;&#65292;&#23454;&#29616;&#23433;&#20840;&#20445;&#35777;&#30340;&#19968;&#31181;&#20005;&#35880;&#20294;&#23454;&#29992;&#30340;&#26041;&#27861;&#26159;&#26368;&#20026;&#31361;&#20986;&#30340;&#12290;&#26412;&#25991;&#39318;&#20808;&#35752;&#35770;&#20102;&#35774;&#35745;&#21644;&#39564;&#35777;&#36825;&#31181;&#31995;&#32479;&#25152;&#28041;&#21450;&#30340;&#24037;&#31243;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#23545;&#29616;&#26377;&#24037;&#20316;&#26080;&#27861;&#23454;&#29616;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#39564;&#35777;&#26041;&#27861;&#65292;&#20197;&#26368;&#32456;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#32479;&#35745;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has made remarkable advancements, but confidently utilising learning-enabled components in safety-critical domains still poses challenges. Among the challenges, it is known that a rigorous, yet practical, way of achieving safety guarantees is one of the most prominent. In this paper, we first discuss the engineering and research challenges associated with the design and verification of such systems. Then, based on the observation that existing works cannot actually achieve provable guarantees, we promote a two-step verification method for the ultimate achievement of provable statistical guarantees.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25552;&#21462;-&#25688;&#35201;&#36724;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#23481;&#30340;"&#20511;&#29992;"&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#30456;&#24212;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#25351;&#21335;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11779</link><description>&lt;p&gt;
Generate&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#21462;-&#25688;&#35201;&#36724;:&#27979;&#37327;&#20869;&#23481;"&#20511;&#29992;"
&lt;/p&gt;
&lt;p&gt;
The Extractive-Abstractive Axis: Measuring Content "Borrowing" in Generative Language Models. (arXiv:2307.11779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25552;&#21462;-&#25688;&#35201;&#36724;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#23481;&#30340;"&#20511;&#29992;"&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#30456;&#24212;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#25351;&#21335;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35774;&#35745;&#20135;&#29983;&#39640;&#24230;&#25688;&#35201;&#30340;&#36755;&#20986;&#65292;&#19982;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#25552;&#21462;&#24335;&#21709;&#24212;&#24418;&#25104;&#23545;&#27604;&#12290;&#37492;&#20110;LLMs&#30340;&#36825;&#19968;&#29305;&#28857;&#21450;&#20854;&#23545;&#20869;&#23481;&#35768;&#21487;&#21644;&#24402;&#23646;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;&#25552;&#21462;-&#25688;&#35201;&#36724;&#65292;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#24378;&#35843;&#24320;&#21457;&#30456;&#24212;&#30340;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#25351;&#21335;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#38480;&#21046;&#22312;&#25991;&#26412;&#24418;&#24335;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models produce highly abstractive outputs by design, in contrast to extractive responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing &amp; Attribution, we propose the the so-called Extractive-Abstractive axis for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.11768</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#35299;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;
&lt;/p&gt;
&lt;p&gt;
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11768
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#39564;&#35777;&#20854;&#34892;&#20026;&#30340;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#26159;&#35201;&#27714;LLM&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#20197;&#36880;&#27493;&#25512;&#29702;&#30340;&#26041;&#24335;&#22806;&#21270;&#20854;&#25512;&#29702;&#36807;&#31243;&#65288;&#24605;&#32500;&#38142;&#65307;CoT&#65289;&#12290;&#25512;&#29702;&#36807;&#31243;&#21487;&#20197;&#35753;&#25105;&#20204;&#26816;&#26597;&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#25152;&#38472;&#36848;&#30340;&#25512;&#29702;&#33021;&#22815;&#24544;&#23454;&#22320;&#21453;&#26144;&#27169;&#22411;&#30340;&#23454;&#38469;&#25512;&#29702;&#65292;&#32780;&#36825;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#25552;&#39640;CoT&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#26469;&#29983;&#25104;&#25512;&#29702;&#12290;&#22522;&#20110;&#20998;&#35299;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#25509;&#36817;CoT&#65292;&#24182;&#22312;&#20960;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#25152;&#38472;&#36848;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#22312;&#21333;&#29420;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#31572;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#22823;&#22823;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35789;&#27719;&#25512;&#29702;&#20219;&#21153;MPC&#65292;&#36890;&#36807;&#24494;&#35843;BERT&#27169;&#22411;&#21644;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#31616;&#21270;&#26631;&#27880;&#36164;&#28304;&#30340;&#21516;&#26102;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#19982;SentiWordNet&#30340;&#27604;&#36739;&#65292;&#36824;&#21457;&#29616;&#20102;MPC&#19982;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20027;&#35266;&#24615;&#20998;&#31867;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.11767</link><description>&lt;p&gt;
&#22312;&#39640;&#25928;&#33258;&#21160;&#21270;&#30340;&#39118;&#26684;&#20013;&#35782;&#21035;&#24515;&#29702;&#24418;&#23481;&#35789;
&lt;/p&gt;
&lt;p&gt;
Recognition of Mental Adjectives in An Efficient and Automatic Style. (arXiv:2307.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35789;&#27719;&#25512;&#29702;&#20219;&#21153;MPC&#65292;&#36890;&#36807;&#24494;&#35843;BERT&#27169;&#22411;&#21644;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#31616;&#21270;&#26631;&#27880;&#36164;&#28304;&#30340;&#21516;&#26102;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#19982;SentiWordNet&#30340;&#27604;&#36739;&#65292;&#36824;&#21457;&#29616;&#20102;MPC&#19982;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20027;&#35266;&#24615;&#20998;&#31867;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#24120;&#35782;&#25512;&#29702;&#22312;&#23398;&#26415;&#30028;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35789;&#27719;&#25512;&#29702;&#20219;&#21153;&#65292;&#24515;&#29702;&#19982;&#29289;&#29702;&#20998;&#31867;&#65288;MPC&#65289;&#65292;&#20197;&#22788;&#29702;&#24120;&#35782;&#25512;&#29702;&#12290;&#24515;&#29702;&#35789;&#35821;&#19982;&#24515;&#29702;&#27963;&#21160;&#30456;&#20851;&#65292;&#21487;&#20998;&#20026;&#20845;&#20010;&#31867;&#21035;&#65306;&#24773;&#24863;&#12289;&#38656;&#27714;&#12289;&#24863;&#30693;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#21644;&#20010;&#24615;&#12290;&#29289;&#29702;&#35789;&#35821;&#25551;&#36848;&#29289;&#20307;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#22914;&#39068;&#33394;&#12289;&#30828;&#24230;&#12289;&#36895;&#24230;&#21644;&#21487;&#22609;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;BERT&#27169;&#22411;&#23545;&#36825;&#20010;&#20219;&#21153;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#22312;&#35757;&#32451;&#26694;&#26550;&#20013;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#20351;&#29992;ENTROPY&#31574;&#30053;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#65292;&#20165;&#38656;&#35201;&#32422;300&#20010;&#26631;&#27880;&#30340;&#35789;&#35821;&#12290;&#25105;&#20204;&#36824;&#23558;&#32467;&#26524;&#19982;SentiWordNet&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#26816;&#26597;MPC&#19982;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20027;&#35266;&#24615;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, commonsense reasoning has received more and more attention from academic community. We propose a new lexical inference task, Mental and Physical Classification (MPC), to handle commonsense reasoning in a reasoning graph. Mental words relate to mental activities, which fall into six categories: Emotion, Need, Perceiving, Reasoning, Planning and Personality. Physical words describe physical attributes of an object, like color, hardness, speed and malleability. A BERT model is fine-tuned for this task and active learning algorithm is adopted in the training framework to reduce the required annotation resources. The model using ENTROPY strategy achieves satisfactory accuracy and requires only about 300 labeled words. We also compare our result with SentiWordNet to check the difference between MPC and subjectivity classification task in sentiment analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#21462;&#29992;&#25143;&#30340;&#24515;&#26234;&#27169;&#22411;&#26469;&#27979;&#37327;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#35748;&#30693;&#22270;&#26469;&#35780;&#20272;&#35299;&#37322;&#23545;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20026;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#21442;&#32771;&#21644;&#25913;&#36827;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.11765</link><description>&lt;p&gt;
&#20351;&#29992;&#35843;&#21462;&#24515;&#26234;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#27979;&#37327;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#30340;&#24863;&#30693;&#20449;&#20219;&#24230;
&lt;/p&gt;
&lt;p&gt;
Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model. (arXiv:2307.11765v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#21462;&#29992;&#25143;&#30340;&#24515;&#26234;&#27169;&#22411;&#26469;&#27979;&#37327;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#35748;&#30693;&#22270;&#26469;&#35780;&#20272;&#35299;&#37322;&#23545;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20026;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#21442;&#32771;&#21644;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#23454;&#35777;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27979;&#37327;&#29992;&#25143;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24863;&#30693;&#20449;&#20219;&#24230;&#12290;&#21033;&#29992;&#27169;&#31946;&#35748;&#30693;&#22270;&#26469;&#33719;&#21462;&#29992;&#25143;&#30340;&#24515;&#26234;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#30097;&#20284;COVID-19&#24739;&#32773;&#20998;&#31867;&#20026;&#38451;&#24615;&#25110;&#38452;&#24615;&#12290;&#28982;&#21518;&#65292;&#21307;&#30103;&#19987;&#23478;&#26681;&#25454;&#20182;&#20204;&#30340;&#19987;&#19994;&#30693;&#35782;&#20197;&#21450;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25552;&#20379;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#36827;&#34892;&#35786;&#26029;&#20915;&#31574;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#35299;&#37322;&#23545;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#65292;&#21307;&#30103;&#19987;&#23478;&#36890;&#36807;&#35843;&#26597;&#23545;&#35299;&#37322;&#28385;&#24847;&#24230;&#36827;&#34892;&#35780;&#20998;&#12290;&#28982;&#21518;&#65292;&#23558;&#20854;&#20316;&#20026;&#27169;&#31946;&#35748;&#30693;&#22270;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#24444;&#27492;&#20197;&#21450;&#26368;&#32456;&#23545;&#24863;&#30693;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32771;&#34385;&#21307;&#30103;&#19987;&#23478;&#30340;&#20027;&#35266;&#24615;&#65292;&#20351;&#29992;&#27169;&#31946;&#35821;&#35328;&#21464;&#37327;&#26469;&#30830;&#23450;&#24433;&#21709;&#30340;&#24378;&#24230;&#12290;&#22312;&#27169;&#31946;&#35748;&#30693;&#22270;&#36798;&#21040;&#31283;&#23450;&#29366;&#24577;&#21518;&#65292;&#23558;&#33719;&#24471;&#19968;&#20010;&#37327;&#21270;&#20540;&#26469;&#34913;&#37327;&#24863;&#30693;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This empirical study proposes a novel methodology to measure users' perceived trust in an Explainable Artificial Intelligence (XAI) model. To do so, users' mental models are elicited using Fuzzy Cognitive Maps (FCMs). First, we exploit an interpretable Machine Learning (ML) model to classify suspected COVID-19 patients into positive or negative cases. Then, Medical Experts' (MEs) conduct a diagnostic decision-making task based on their knowledge and then prediction and interpretations provided by the XAI model. In order to evaluate the impact of interpretations on perceived trust, explanation satisfaction attributes are rated by MEs through a survey. Then, they are considered as FCM's concepts to determine their influences on each other and, ultimately, on the perceived trust. Moreover, to consider MEs' mental subjectivity, fuzzy linguistic variables are used to determine the strength of influences. After reaching the steady state of FCMs, a quantified value is obtained to measure the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20449;&#20219;&#20462;&#22797;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#26377;&#25928;&#31574;&#30053;&#24182;&#25506;&#32034;&#25104;&#21151;&#28508;&#22312;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.11763</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20449;&#20219;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Rethinking Trust Repair in Human-Robot Interaction. (arXiv:2307.11763v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20449;&#20219;&#20462;&#22797;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#26377;&#25928;&#31574;&#30053;&#24182;&#25506;&#32034;&#25104;&#21151;&#28508;&#22312;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#20154;&#22312;&#24037;&#20316;&#21512;&#20316;&#20013;&#30340;&#26222;&#21450;&#65292;&#20449;&#20219;&#24050;&#25104;&#20026;&#23427;&#20204;&#34987;&#25509;&#21463;&#21644;&#21457;&#25381;&#25928;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#20449;&#20219;&#26159;&#21160;&#24577;&#30340;&#65292;&#24403;&#20986;&#29616;&#38169;&#35823;&#26102;&#65292;&#20449;&#20219;&#20250;&#34987;&#20405;&#34432;&#12290;&#23613;&#31649;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20449;&#20219;&#20462;&#22797;&#19978;&#26377;&#19981;&#26029;&#28044;&#29616;&#30340;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20851;&#20110;&#22914;&#20309;&#22312;&#26426;&#22120;&#20154;&#36829;&#32972;&#20449;&#20219;&#21518;&#24674;&#22797;&#20449;&#20219;&#30340;&#21487;&#38752;&#26041;&#27861;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#30340;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#35774;&#35745;&#33021;&#22815;&#20462;&#22797;&#20449;&#20219;&#30340;&#26426;&#22120;&#20154;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#24182;&#25506;&#32034;&#36825;&#20123;&#31574;&#30053;&#25104;&#21151;&#30340;&#28508;&#22312;&#26426;&#21046;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#20449;&#20219;&#20462;&#22797;&#36807;&#31243;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#21450;&#25105;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#24403;&#21069;&#24050;&#21457;&#34920;&#24037;&#20316;&#30340;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#25105;&#36824;&#35752;&#35770;&#20102;&#23558;&#25351;&#23548;&#25105;&#26410;&#26469;&#24037;&#20316;&#30340;&#30740;&#31350;&#38382;&#39064;&#20197;&#21450;&#36825;&#39033;&#30740;&#31350;&#21487;&#33021;&#23545;&#35813;&#39046;&#22495;&#20570;&#20986;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As robots become increasingly prevalent in work-oriented collaborations, trust has emerged as a critical factor in their acceptance and effectiveness. However, trust is dynamic and can erode when mistakes are made. Despite emerging research on trust repair in human-robot interaction, significant questions remain about identifying reliable approaches to restoring trust in robots after trust violations occur. To address this problem, my research aims to identify effective strategies for designing robots capable of trust repair in human-robot interaction (HRI) and to explore the underlying mechanisms that make these strategies successful. This paper provides an overview of the fundamental concepts and key components of the trust repair process in HRI, as well as a summary of my current published work in this area. Additionally, I discuss the research questions that will guide my future work and the potential contributions that this research could make to the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25351;&#23548;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#34917;&#20805;&#65292;ChatGPT&#33021;&#22815;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#23569;&#33267;&#20256;&#32479;&#27169;&#22411;&#30340;1/40&#65292;&#34920;&#29616;&#20248;&#24322;&#65292;&#23588;&#20854;&#25797;&#38271;&#20943;&#23567;&#35823;&#25253;&#24182;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#22312;&#20854;&#20182;&#31867;&#20284;&#20219;&#21153;&#20013;&#20805;&#20998;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.11761</link><description>&lt;p&gt;
ChatGPT&#30340;&#20844;&#24179;&#24615;&#21450;&#21487;&#35299;&#37322;&#24341;&#23548;&#25552;&#31034;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fairness of ChatGPT and the Role Of Explainable-Guided Prompts. (arXiv:2307.11761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25351;&#23548;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#34917;&#20805;&#65292;ChatGPT&#33021;&#22815;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#23569;&#33267;&#20256;&#32479;&#27169;&#22411;&#30340;1/40&#65292;&#34920;&#29616;&#20248;&#24322;&#65292;&#23588;&#20854;&#25797;&#38271;&#20943;&#23567;&#35823;&#25253;&#24182;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#22312;&#20854;&#20182;&#31867;&#20284;&#20219;&#21153;&#20013;&#20805;&#20998;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#28508;&#21147;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;OpenAI&#30340;GPT&#65292;&#22312;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;LLMs&#21463;&#21040;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25351;&#23548;&#24182;&#34917;&#20805;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26102;&#65292;&#20854;&#34920;&#29616;&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20182;&#20204;&#21482;&#20351;&#29992;&#20102;&#23569;&#24471;&#22810;&#30340;&#25968;&#25454;-&#20165;&#20165;20&#20010;&#25968;&#25454;&#28857;&#65292;&#32780;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;800&#20010;&#25968;&#25454;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;LLMs&#22312;&#20943;&#23567;&#35823;&#25253;&#25552;&#21319;&#20844;&#24179;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#22312;&#39118;&#38505;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#32467;&#26524;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#24378;&#35843;&#20102;LLMs&#22312;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#20026;&#26410;&#26469;&#22312;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research investigates the potential of Large-scale Language Models (LLMs), specifically OpenAI's GPT, in credit risk assessment-a binary classification task. Our findings suggest that LLMs, when directed by judiciously designed prompts and supplemented with domain-specific knowledge, can parallel the performance of traditional Machine Learning (ML) models. Intriguingly, they achieve this with significantly less data-40 times less, utilizing merely 20 data points compared to the ML's 800. LLMs particularly excel in minimizing false positives and enhancing fairness, both being vital aspects of risk analysis. While our results did not surpass those of classical ML models, they underscore the potential of LLMs in analogous tasks, laying a groundwork for future explorations into harnessing the capabilities of LLMs in diverse ML tasks.
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#25512;&#29702;&#22120;&#30340;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#32570;&#38519;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20915;&#31574;&#26641;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#25991;&#23383;&#35828;&#26126;&#35299;&#37322;&#22522;&#20110;&#25513;&#27169;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#20197;&#25552;&#21319;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11643</link><description>&lt;p&gt;
&#22522;&#20110;&#24418;&#24577;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;AI&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.11643v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#25512;&#29702;&#22120;&#30340;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#32570;&#38519;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20915;&#31574;&#26641;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#25991;&#23383;&#35828;&#26126;&#35299;&#37322;&#22522;&#20110;&#25513;&#27169;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#20197;&#25552;&#21319;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#24037;&#31243;&#21644;&#21046;&#36896;&#31561;&#34892;&#19994;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#36879;&#26126;&#30340;&#25512;&#29702;&#20197;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AI&#25512;&#29702;&#22120;&#65292;&#35813;&#25512;&#29702;&#22120;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#32570;&#38519;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20915;&#31574;&#26641;&#23545;&#32570;&#38519;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;AI&#25512;&#29702;&#22120;&#36890;&#36807;&#21487;&#35270;&#21270;&#22270;&#34920;&#21644;&#25991;&#23383;&#35828;&#26126;&#25552;&#20379;&#23545;&#22522;&#20110;&#25513;&#27169;&#30340;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#35299;&#37322;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;AI&#25512;&#29702;&#22120;&#22312;&#20351;&#29992;366&#24352;&#21547;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#38598;&#21512;&#19978;&#27979;&#35797;&#20102;&#35299;&#37322;IE Mask R-CNN&#27169;&#22411;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#25512;&#29702;&#22120;&#22312;&#35299;&#37322;IE Mask R-CNN&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25152;&#25552;&#20986;&#30340;AI&#25512;&#29702;&#22120;&#20026;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of artificial intelligent (AI) models becomes more prevalent in industries such as engineering and manufacturing, it is essential that these models provide transparent reasoning behind their predictions. This paper proposes the AI-Reasoner, which extracts the morphological characteristics of defects (DefChars) from images and utilises decision trees to reason with the DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e. charts) and textual explanations to provide insights into outputs made by masked-based defect detection and classification models. It also provides effective mitigation strategies to enhance data pre-processing and overall model performance. The AI-Reasoner was tested on explaining the outputs of an IE Mask R-CNN model using a set of 366 images containing defects. The results demonstrated its effectiveness in explaining the IE Mask R-CNN model's predictions. Overall, the proposed AI-Reasoner provides a solution for improving the performanc
&lt;/p&gt;</description></item><item><title>CausE&#26159;&#19968;&#20010;&#37319;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#23884;&#20837;&#35299;&#32544;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11610</link><description>&lt;p&gt;
CausE: &#26397;&#21521;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
CausE: Towards Causal Knowledge Graph Embedding. (arXiv:2307.11610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11610
&lt;/p&gt;
&lt;p&gt;
CausE&#26159;&#19968;&#20010;&#37319;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#23884;&#20837;&#35299;&#32544;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#30340;&#37325;&#28857;&#26159;&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#36830;&#32493;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#19977;&#20803;&#32452;&#20197;&#23454;&#29616;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#65288;KGC&#65289;&#12290;&#28982;&#32780;&#65292;KGE&#27169;&#22411;&#36890;&#24120;&#21482;&#26159;&#31616;&#21333;&#22320;&#23398;&#20064;&#19977;&#20803;&#32452;&#25968;&#25454;&#30340;&#32467;&#26500;&#20851;&#32852;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;KG&#20013;&#65292;&#23884;&#20837;&#21487;&#33021;&#20250;&#34987;&#24494;&#19981;&#36275;&#36947;&#30340;&#27169;&#24335;&#21644;&#22122;&#22768;&#38142;&#25509;&#25152;&#35823;&#23548;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22240;&#26524;&#24615;&#21644;&#23884;&#20837;&#35299;&#32544;&#26041;&#38754;&#24314;&#31435;&#20102;KGE&#30340;&#26032;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Causality-enhanced knowledge graph Embedding&#65288;CausE&#65289;&#26694;&#26550;&#12290;CausE&#20351;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#20272;&#35745;&#28151;&#26434;&#23884;&#20837;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CausE&#21487;&#20197;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;KGC&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;https://github.com/zjukg/CausE&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) focuses on representing the entities and relations of a knowledge graph (KG) into the continuous vector spaces, which can be employed to predict the missing triples to achieve knowledge graph completion (KGC). However, KGE models often only briefly learn structural correlations of triple data and embeddings would be misled by the trivial patterns and noisy links in real-world KGs. To address this issue, we build the new paradigm of KGE in the context of causality and embedding disentanglement. We further propose a Causality-enhanced knowledge graph Embedding (CausE) framework. CausE employs causal intervention to estimate the causal effect of the confounder embeddings and design new training objectives to make stable predictions. Experimental results demonstrate that CausE could outperform the baseline models and achieve state-of-the-art KGC performance. We release our code in https://github.com/zjukg/CausE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AIGC&#65288;GPT&#65289;&#22312;&#30005;&#20449;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30005;&#20449;&#22686;&#24378;&#35748;&#30693;&#33021;&#21147;&#31995;&#32479;&#65292;&#20026;&#30005;&#20449;&#26381;&#21153;&#30340;&#26500;&#24314;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11449</link><description>&lt;p&gt;
AIGC&#36171;&#33021;&#30005;&#20449;&#34892;&#19994;&#30333;&#30382;&#20070;
&lt;/p&gt;
&lt;p&gt;
AIGC Empowering Telecom Sector White Paper. (arXiv:2307.11449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AIGC&#65288;GPT&#65289;&#22312;&#30005;&#20449;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30005;&#20449;&#22686;&#24378;&#35748;&#30693;&#33021;&#21147;&#31995;&#32479;&#65292;&#20026;&#30005;&#20449;&#26381;&#21153;&#30340;&#26500;&#24314;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;GPT&#28909;&#28526;&#20013;&#65292;&#20154;&#20204;&#28145;&#20999;&#24847;&#35782;&#21040;&#20316;&#20026;&#19968;&#39033;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#25216;&#26415;&#21644;&#32463;&#27982;&#31038;&#20250;&#21457;&#23637;&#30340;&#20851;&#38190;&#21147;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#23558;&#32473;&#20840;&#29699;&#20135;&#19994;&#24102;&#26469;&#24040;&#22823;&#30340;&#39134;&#36291;&#21644;&#31361;&#30772;&#65292;&#24182;&#28145;&#21051;&#24433;&#21709;&#26410;&#26469;&#30340;&#31454;&#20105;&#26684;&#23616;&#12290;&#20316;&#20026;&#20449;&#24687;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#30340;&#24314;&#35774;&#32773;&#21644;&#36816;&#33829;&#21830;&#65292;&#30005;&#20449;&#34892;&#19994;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#22522;&#30784;&#25903;&#25345;&#65292;&#29978;&#33267;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#12290;&#22914;&#20309;&#23454;&#29616;AIGC&#65288;GPT&#65289;&#24212;&#29992;&#24182;&#22312;&#30005;&#20449;&#34892;&#19994;&#20013;&#23454;&#26045;AIGC&#26159;&#30005;&#20449;&#20174;&#19994;&#32773;&#24517;&#39035;&#24605;&#32771;&#21644;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;GPT&#20316;&#20026;AIGC&#30340;&#20856;&#22411;&#20195;&#34920;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;GPT&#22914;&#20309;&#36890;&#36807;&#22330;&#26223;&#36171;&#33021;&#30005;&#20449;&#34892;&#19994;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;GPT&#36890;&#29992;&#27169;&#22411;&#19982;&#30005;&#20449;&#26381;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#30005;&#20449;&#22686;&#24378;&#35748;&#30693;&#33021;&#21147;&#31995;&#32479;&#65292;&#22238;&#31572;&#20102;&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#30005;&#20449;&#26381;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the global craze of GPT, people have deeply realized that AI, as a transformative technology and key force in economic and social development, will bring great leaps and breakthroughs to the global industry and profoundly influence the future world competition pattern. As the builder and operator of information and communication infrastructure, the telecom sector provides infrastructure support for the development of AI, and even takes the lead in the implementation of AI applications. How to enable the application of AIGC (GPT) and implement AIGC in the telecom sector are questions that telecom practitioners must ponder and answer. Through the study of GPT, a typical representative of AIGC, the authors have analyzed how GPT empowers the telecom sector in the form of scenarios, discussed the gap between the current GPT general model and telecom services, proposed for the first time a Telco Augmented Cognition capability system, provided answers to how to construct a telecom service 
&lt;/p&gt;</description></item><item><title>EMS-YOLO&#26159;&#19968;&#31181;&#30452;&#25509;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#32780;&#19981;&#26159;ANN-SNN&#36716;&#25442;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#28145;&#24230;SNN&#30340;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11411</link><description>&lt;p&gt;
&#28145;&#24230;&#30452;&#25509;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Directly-Trained Spiking Neural Networks for Object Detection. (arXiv:2307.11411v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11411
&lt;/p&gt;
&lt;p&gt;
EMS-YOLO&#26159;&#19968;&#31181;&#30452;&#25509;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#32780;&#19981;&#26159;ANN-SNN&#36716;&#25442;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#28145;&#24230;SNN&#30340;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#39640;&#33021;&#25928;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26102;&#31354;&#21160;&#24577;&#26469;&#32534;&#30721;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#35757;&#32451;&#30340;&#28145;&#24230;SNN&#22312;&#23569;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#26469;&#35299;&#20915;&#30446;&#26631;&#26816;&#27979;&#22238;&#24402;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMS-YOLO&#65292;&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#26032;&#22411;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#26694;&#26550;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26367;&#20195;&#26799;&#24230;&#32780;&#19981;&#26159;ANN-SNN&#36716;&#25442;&#31574;&#30053;&#26469;&#35757;&#32451;&#28145;&#24230;SNN&#30340;&#23581;&#35797;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#33033;&#20914;&#27531;&#24046;&#22359;EMS-ResNet&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#25193;&#23637;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#30340;&#28145;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20855;&#26377;&#20302;&#21151;&#32791;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35777;&#26126;&#20102;EMS-ResNet&#21487;&#20197;&#36991;&#20813;&#26799;&#24230;&#28040;&#22833;&#25110;&#26799;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;ANN-SNN&#36716;&#25442;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) are brain-inspired energy-efficient models that encode information in spatiotemporal dynamics. Recently, deep SNNs trained directly have shown great success in achieving high performance on classification tasks with very few time steps. However, how to design a directly-trained SNN for the regression task of object detection still remains a challenging problem. To address this problem, we propose EMS-YOLO, a novel directly-trained SNN framework for object detection, which is the first trial to train a deep SNN with surrogate gradients for object detection rather than ANN-SNN conversion strategies. Specifically, we design a full-spike residual block, EMS-ResNet, which can effectively extend the depth of the directly-trained SNN with low power consumption. Furthermore, we theoretically analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding. The results demonstrate that our approach outperforms the state-of-the-art ANN-SNN conversion me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#23457;&#26597;&#38382;&#39064;&#65292;&#25351;&#20986;&#29616;&#26377;&#30340;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#23384;&#22312;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.10719</link><description>&lt;p&gt;
LLM&#23457;&#26597;&#65306;&#26426;&#22120;&#23398;&#20064;&#25361;&#25112;&#36824;&#26159;&#35745;&#31639;&#26426;&#23433;&#20840;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?. (arXiv:2307.10719v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#23457;&#26597;&#38382;&#39064;&#65292;&#25351;&#20986;&#29616;&#26377;&#30340;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#23384;&#22312;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#25552;&#20379;&#30340;&#25351;&#20196;&#30340;&#30450;&#30446;&#36981;&#24490;&#24341;&#21457;&#20102;&#23545;&#24694;&#24847;&#20351;&#29992;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;LLM&#30340;&#27169;&#22411;&#24494;&#35843;&#25110;&#20351;&#29992;LLM&#36827;&#34892;&#36755;&#20986;&#23457;&#26597;&#65292;&#24050;&#35777;&#26126;&#26159;&#26377;&#32570;&#38519;&#30340;&#65292;&#22240;&#20026;LLM&#20173;&#28982;&#21487;&#20197;&#29983;&#25104;&#26377;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#24120;&#29992;&#30340;&#23457;&#26597;&#26041;&#27861;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20381;&#36182;&#20110;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;LLM&#36755;&#20986;&#20013;&#30340;&#19981;&#33391;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#36825;&#31181;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#30340;&#23457;&#26597;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#38480;&#20110;&#35821;&#20041;&#23457;&#26597;&#65292;&#22240;&#20026;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10617</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26816;&#27979;&#34394;&#20551;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#32447;&#35780;&#35770;&#22312;&#25512;&#24191;&#20219;&#20309;&#20135;&#21697;&#25110;&#26381;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20225;&#19994;&#21487;&#33021;&#20250;&#23884;&#20837;&#34394;&#20551;&#35780;&#35770;&#20197;&#21560;&#24341;&#23458;&#25143;&#36141;&#20080;&#20182;&#20204;&#30340;&#20135;&#21697;&#12290;&#20182;&#20204;&#29978;&#33267;&#21487;&#33021;&#31361;&#20986;&#24378;&#35843;&#33258;&#24049;&#20135;&#21697;&#30340;&#20248;&#28857;&#25110;&#25209;&#35780;&#31454;&#20105;&#23545;&#25163;&#30340;&#20135;&#21697;&#12290;&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#22312;&#32447;&#21830;&#19994;&#29992;&#25143;&#26377;&#21160;&#26426;&#20026;&#20182;&#20204;&#24819;&#35201;&#25512;&#24191;&#30340;&#20135;&#21697;&#32534;&#20889;&#34394;&#20551;&#30340;&#27491;&#38754;&#35780;&#35770;&#65292;&#25110;&#32773;&#20026;&#20182;&#20204;&#30495;&#27491;&#19981;&#21916;&#27426;&#30340;&#20135;&#21697;&#25552;&#20379;&#34394;&#20551;&#30340;&#36127;&#38754;&#35780;&#35770;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#25345;&#32493;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#19968;&#20010;&#39184;&#39302;&#35780;&#35770;&#30340;&#34394;&#20551;&#24847;&#35265;&#22403;&#22334;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22810;&#27425;&#23454;&#39564;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;n-gram&#27169;&#22411;&#21644;&#26368;&#22823;&#29305;&#24449;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, online reviews play a vital role for promoting any kind of product or services. Businesses may embed fake reviews in order to attract customers to purchase their products. They may even highlight the benefits of their own product or criticize the competition's product. Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like. So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation. Thus, identifying deceptive reviews is an intense and on-going research area. This research paper proposes machine learning model approach to identify deceptive reviews. The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews. We developed a n-gram model and max features to identify 
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20013;&#20351;&#29992;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#20960;&#20046;&#21487;&#20197;&#25552;&#20379;&#19982;&#36923;&#36753;&#19978;&#26377;&#25928;&#30340;&#25552;&#31034;&#30456;&#20284;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#32780;&#19988;&#22312;&#26368;&#22256;&#38590;&#30340;&#20219;&#21153;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;</title><link>http://arxiv.org/abs/2307.10573</link><description>&lt;p&gt;
&#26080;&#25928;&#36923;&#36753;&#65292;&#31561;&#25928;&#25910;&#30410;&#65306;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20013;&#30340;&#22855;&#24618;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting. (arXiv:2307.10573v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10573
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20013;&#20351;&#29992;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#20960;&#20046;&#21487;&#20197;&#25552;&#20379;&#19982;&#36923;&#36753;&#19978;&#26377;&#25928;&#30340;&#25552;&#31034;&#30456;&#20284;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#32780;&#19988;&#22312;&#26368;&#22256;&#38590;&#30340;&#20219;&#21153;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#25552;&#31034;&#20197;&#19968;&#31181;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#36825;&#26679;&#30340;&#25552;&#31034;&#20250;&#25552;&#39640;&#24615;&#33021;&#36824;&#19981;&#28165;&#26970;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;CoT&#25552;&#31034;&#20960;&#20046;&#21487;&#20197;&#20687;&#36923;&#36753;&#19978;&#26377;&#25928;&#30340;CoT&#25552;&#31034;&#19968;&#26679;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#23558;CoT&#25552;&#31034;&#20013;&#30340;&#29305;&#23450;&#38382;&#39064;&#20449;&#24687;&#26367;&#25442;&#20026;&#25277;&#35937;&#20449;&#24687;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#20449;&#24687;&#36890;&#24120;&#19981;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#25209;&#35780;&#20154;&#22763;&#22238;&#24212;&#35828;&#65292;&#36825;&#20123;&#21457;&#29616;&#26159;&#22522;&#20110;&#22826;&#23569;&#12289;&#22826;&#31616;&#21333;&#30340;&#20219;&#21153;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20105;&#35758;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#22312;BIG-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#22256;&#38590;&#30340;&#20219;&#21153;&#19978;&#65292;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;CoT&#25552;&#31034;&#26159;&#21542;&#25552;&#20379;&#19982;&#36923;&#36753;&#19978;&#26377;&#25928;&#30340;&#25552;&#31034;&#30456;&#21516;&#27700;&#24179;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36825;&#20123;&#20219;&#21153;&#34987;&#31216;&#20026;BIG-Bench Hard&#65288;BBH&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;BBH&#20219;&#21153;&#19978;&#65292;&#36923;&#36753;&#19978;&#26080;&#25928;&#30340;&#25512;&#29702;&#25552;&#31034;&#30830;&#23454;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models can be prompted to reason through problems in a manner that significantly improves performance. However, \textit{why} such prompting improves performance is unclear. Recent work showed that using logically \textit{invalid} Chain-of-Thought (CoT) prompting improves performance almost as much as logically \textit{valid} CoT prompting, and that editing CoT prompts to replace problem-specific information with abstract information or out-of-distribution information typically doesn't harm performance. Critics have responded that these findings are based on too few and too easy tasks to draw meaningful conclusions. To resolve this dispute, we test whether logically invalid CoT prompts offer the same level of performance gains as logically valid prompts on the hardest tasks in the BIG-Bench benchmark, termed BIG-Bench Hard (BBH). We find that the logically \textit{invalid} reasoning prompts do indeed achieve similar performance gains on BBH tasks as logically valid reasoning pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36864;&#20241;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#65292;&#21033;&#29992;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#36827;&#34892;&#39044;&#27979;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.10559</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning. (arXiv:2307.10559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36864;&#20241;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#65292;&#21033;&#29992;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#36827;&#34892;&#39044;&#27979;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#26159;&#19968;&#20010;&#23433;&#20840;&#20851;&#38190;&#30340;&#26381;&#21153;&#31995;&#32479;&#65292;&#35201;&#27714;&#22320;&#38754;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#65288;ATCo&#65289;&#26102;&#21051;&#20851;&#27880;&#20197;&#32500;&#25345;&#26085;&#24120;&#33322;&#31354;&#36816;&#33829;&#12290;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#21487;&#33021;&#23545;&#36816;&#33829;&#23433;&#20840;&#21644;&#31354;&#22495;&#20351;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#36733;&#24182;&#30830;&#20445;ATCo&#30340;&#21487;&#25509;&#21463;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#65292;&#20934;&#30830;&#39044;&#27979;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#23545;&#20110;&#37319;&#21462;&#32531;&#35299;&#25514;&#26045;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;ATCo&#24037;&#20316;&#36127;&#33655;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#20027;&#35201;&#20174;&#31354;&#20013;&#20132;&#36890;&#30340;&#35282;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#19982;&#36864;&#20241;ATCo&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#33719;&#24471;&#20102;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#12290;&#27169;&#25311;&#22312;&#19977;&#31181;&#33778;&#23612;&#20811;&#26031;&#25509;&#36817;&#22330;&#26223;&#19979;&#36827;&#34892;&#65292;&#35201;&#27714;&#20154;&#31867;ATCo&#33258;&#25105;&#35780;&#20272;&#20854;&#24037;&#20316;&#36127;&#33655;&#35780;&#32423;&#65288;&#21363;&#65292;&#20302;-1&#21040;&#39640;-7&#65289;&#12290;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#21512;&#35268;&#21270;&#39044;&#27979;&#65292;&#26469;&#23545;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to ide
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#20102;&#20854;&#29305;&#24449;&#21644;&#20316;&#29992;&#12290;&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37325;&#22823;&#30340;&#24433;&#21709;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.10460</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65306;&#25968;&#25454;&#31185;&#23398;&#30340;&#26412;&#36136;&#12289;&#20215;&#20540;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
A data science axiology: the nature, value, and risks of data science. (arXiv:2307.10460v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#20102;&#20854;&#29305;&#24449;&#21644;&#20316;&#29992;&#12290;&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37325;&#22823;&#30340;&#24433;&#21709;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#26080;&#27861;&#39044;&#27979;&#30340;&#33539;&#22260;&#12289;&#35268;&#27169;&#12289;&#22797;&#26434;&#24615;&#21644;&#30693;&#35782;&#21457;&#29616;&#33021;&#21147;&#65292;&#36825;&#26159;&#20854;&#20182;&#26041;&#24335;&#26080;&#27861;&#23454;&#29616;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#36229;&#20986;&#20154;&#31867;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#23427;&#24050;&#32463;&#22312;AI&#20891;&#22791;&#31454;&#36187;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#25968;&#20197;&#19975;&#35745;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#24050;&#32463;&#23454;&#36136;&#24615;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#30340;&#19990;&#30028;&#65292;&#20294;&#30001;&#20110;&#20854;&#19981;&#21487;&#24605;&#35758;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#33021;&#24102;&#26469;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#21644;&#35780;&#20272;&#20102;&#20854;&#26174;&#33879;&#32780;&#20915;&#23450;&#24615;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#20102;&#35299;&#21644;&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65292;&#35748;&#35782;&#21040;&#20854;&#28508;&#22312;&#30340;&#30410;&#22788;&#12289;&#39118;&#38505;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#12290;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#31185;&#23398;&#26412;&#36136;&#19978;&#28041;&#21450;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#27604;&#25105;&#20204;&#23545;&#31185;&#23398;&#30830;&#23450;&#24615;&#30340;&#20559;&#22909;&#26356;&#21152;&#29616;&#23454;&#12290;&#25968;&#25454;&#31185;&#23398;&#23558;&#20135;&#29983;&#36828;&#36828;&#36229;&#20986;&#30693;&#35782;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;</title><link>http://arxiv.org/abs/2307.10234</link><description>&lt;p&gt;
SentimentGPT&#65306;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#21450;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24773;&#24863;&#20998;&#26512;&#20013;&#21508;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#29305;&#21035;&#26159;&#22312;SemEval 2017&#25968;&#25454;&#38598;&#30340;&#20219;&#21153;4&#20013;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#20027;&#35201;&#31574;&#30053;&#65306;1&#65289;&#20351;&#29992;GPT-3.5 Turbo&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;2&#65289;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;3&#65289;&#37319;&#29992;&#21019;&#26032;&#30340;&#23884;&#20837;&#20998;&#31867;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#31574;&#30053;&#21644;&#20010;&#21035;GPT&#27169;&#22411;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#35265;&#35299;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#29420;&#29305;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#23558;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#21516;&#26102;&#20195;&#12289;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;22%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;GPT&#26041;&#27861;&#30340;&#37325;&#35201;&#20215;&#20540;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other contemporary, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22% in F1-score compared to the state-of-the-art. Further, the paper addresses common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25216;&#26415;&#21644;&#21382;&#21490;&#25968;&#25454;&#65292;&#20026;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#25552;&#20379;&#31185;&#23398;&#20915;&#31574;&#24037;&#20855;&#21644;&#35777;&#25454;&#65292;&#20197;&#35299;&#20915;&#36947;&#36335;&#32500;&#25252;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10085</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#36947;&#36335;&#27573;&#25512;&#33616;&#32500;&#25252;&#30340;&#20915;&#31574;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A decision making framework for recommended maintenance of road segments. (arXiv:2307.10085v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10085
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25216;&#26415;&#21644;&#21382;&#21490;&#25968;&#25454;&#65292;&#20026;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#25552;&#20379;&#31185;&#23398;&#20915;&#31574;&#24037;&#20855;&#21644;&#35777;&#25454;&#65292;&#20197;&#35299;&#20915;&#36947;&#36335;&#32500;&#25252;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#36947;&#36335;&#20132;&#36890;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21508;&#22269;&#24050;&#23436;&#25104;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#24314;&#35774;&#12290;&#28982;&#32780;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#25361;&#25112;&#22312;&#20110;&#29616;&#26377;&#36947;&#36335;&#30340;&#32500;&#25252;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#21508;&#22269;&#22312;&#36947;&#36335;&#32500;&#25252;&#39033;&#30446;&#19978;&#30340;&#39044;&#31639;&#26377;&#38480;&#65292;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#22312;&#36827;&#34892;&#31185;&#23398;&#20915;&#31574;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#23558;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#25216;&#26415;&#19982;&#21382;&#21490;&#32500;&#25252;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#36866;&#24212;&#36947;&#36335;&#32500;&#25252;&#31185;&#23398;&#20915;&#31574;&#30340;&#32972;&#26223;&#65292;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#25972;&#21512;&#26088;&#22312;&#20026;&#36947;&#36335;&#31649;&#29702;&#37096;&#38376;&#25552;&#20379;&#26356;&#31185;&#23398;&#30340;&#24037;&#20855;&#21644;&#35777;&#25454;&#65292;&#20197;&#36827;&#34892;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26694;&#26550;&#20027;&#35201;&#35299;&#20915;&#20197;&#19979;&#22235;&#20010;&#38382;&#39064;&#65306;1&#65289;&#39044;&#27979;&#21508;&#36335;&#32447;&#30340;&#36335;&#38754;&#24615;&#33021;&#65292;2&#65289;&#30830;&#23450;&#32500;&#25252;&#36335;&#32447;&#30340;&#20248;&#20808;&#32423;&#65292;3&#65289;&#22522;&#20110;&#35780;&#20272;&#26631;&#20934;&#21046;&#23450;&#32500;&#25252;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of global road transportation, countries worldwide have completed the construction of road networks. However, the ensuing challenge lies in the maintenance of existing roads. It is well-known that countries allocate limited budgets to road maintenance projects, and road management departments face difficulties in making scientifically informed maintenance decisions. Therefore, integrating various artificial intelligence decision-making techniques to thoroughly explore historical maintenance data and adapt them to the context of road maintenance scientific decision-making has become an urgent issue. This integration aims to provide road management departments with more scientific tools and evidence for decision-making. The framework proposed in this paper primarily addresses the following four issues: 1) predicting the pavement performance of various routes, 2) determining the prioritization of maintenance routes, 3) making maintenance decisions based on the e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10025</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#29983;&#32946;&#25919;&#31574;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods. (arXiv:2307.10025v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#32946;&#38382;&#39064;&#19982;&#20154;&#21475;&#23433;&#20840;&#23494;&#20999;&#30456;&#20851;&#65292;&#20013;&#22269;60&#24180;&#26469;&#39318;&#27425;&#20986;&#29616;&#20154;&#21475;&#36127;&#22686;&#38271;&#36235;&#21183;&#65292;&#29983;&#32946;&#25919;&#31574;&#30340;&#21464;&#21270;&#24341;&#36215;&#20102;&#31038;&#20250;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#37319;&#29992;&#20849;&#29616;&#35821;&#20041;&#20998;&#26512;&#12289;&#20027;&#39064;&#20998;&#26512;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#22810;&#31890;&#24230;&#30340;&#35821;&#20041;&#20998;&#26512;&#12290;&#21457;&#29616;&#20851;&#20110;&#8220;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#8221;&#30340;&#25552;&#26696;&#35752;&#35770;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#24182;&#35814;&#32454;&#25506;&#35752;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 ``two sessions" proposal ``suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and ``unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of ``removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behaviour, social ethics and law, and national policy, with people's s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#28385;&#36275;&#20854;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.09683</link><description>&lt;p&gt;
PubMed&#21450;&#20854;&#20182;&#65306;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
PubMed and Beyond: Recent Advances and Best Practices in Biomedical Literature Search. (arXiv:2307.09683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#28385;&#36275;&#20854;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20135;&#29983;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20854;&#20013;&#24456;&#22810;&#21482;&#33021;&#36890;&#36807;&#25991;&#29486;&#33719;&#21462;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#26816;&#32034;&#26159;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#24314;&#31435;&#22312;&#20808;&#21069;&#30693;&#35782;&#22522;&#30784;&#19978;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23558;&#21151;&#33021;&#25193;&#23637;&#21040;&#20102;&#36229;&#36234;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25628;&#32034;&#65292;&#20294;&#36825;&#20123;&#36827;&#23637;&#21487;&#33021;&#23545;&#20020;&#24202;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#36824;&#27604;&#36739;&#38476;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#29305;&#23450;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#22320;&#28385;&#36275;&#20182;&#20204;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;PubMed&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#25913;&#36827;&#21644;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20116;&#31181;&#29305;&#23450;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65306;1.&#20026;&#24490;&#35777;&#21307;&#23398;&#23547;&#25214;&#39640;&#36136;&#37327;&#20020;&#24202;&#30740;&#31350;&#12290;2.&#20026;&#31934;&#20934;&#21307;&#23398;&#21644;&#22522;&#22240;&#32452;&#23398;&#26816;&#32034;&#22522;&#22240;&#30456;&#20851;&#20449;&#24687;&#12290;3.&#26681;&#25454;&#24847;&#20041;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical research yields a wealth of information, much of which is only accessible through the literature. Consequently, literature search is an essential tool for building on prior knowledge in clinical and biomedical research. Although recent improvements in artificial intelligence have expanded functionality beyond keyword-based search, these advances may be unfamiliar to clinicians and researchers. In response, we present a survey of literature search tools tailored to both general and specific information needs in biomedicine, with the objective of helping readers efficiently fulfill their information needs. We first examine the widely used PubMed search engine, discussing recent improvements and continued challenges. We then describe literature search tools catering to five specific information needs: 1. Identifying high-quality clinical research for evidence-based medicine. 2. Retrieving gene-related information for precision medicine and genomics. 3. Searching by meaning, inc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21452;&#22495;&#32593;&#32476;&#29992;&#20110;&#23569;&#35270;&#35282;&#24515;&#33039;SPECT&#22270;&#20687;&#37325;&#24314;&#65292;&#36890;&#36807;&#23450;&#21046;&#30340;&#25237;&#24433;&#21040;&#22270;&#20687;&#22495;&#36716;&#25442;&#22120;&#30452;&#25509;&#20174;&#25237;&#24433;&#25968;&#25454;&#20013;&#37325;&#24314;&#19977;&#32500;&#24515;&#33039;SPECT&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.09624</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21452;&#22495;&#32593;&#32476;&#29992;&#20110;&#23569;&#35270;&#35282;&#24515;&#33039;SPECT&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions. (arXiv:2307.09624v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21452;&#22495;&#32593;&#32476;&#29992;&#20110;&#23569;&#35270;&#35282;&#24515;&#33039;SPECT&#22270;&#20687;&#37325;&#24314;&#65292;&#36890;&#36807;&#23450;&#21046;&#30340;&#25237;&#24433;&#21040;&#22270;&#20687;&#22495;&#36716;&#25442;&#22120;&#30452;&#25509;&#20174;&#25237;&#24433;&#25968;&#25454;&#20013;&#37325;&#24314;&#19977;&#32500;&#24515;&#33039;SPECT&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;(CVD)&#26159;&#20840;&#29699;&#27515;&#20129;&#21407;&#22240;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#24515;&#32908;&#28748;&#27880;&#25104;&#20687;&#20351;&#29992;SPECT&#22312;CVD&#30340;&#35786;&#26029;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;GE 530/570c&#19987;&#29992;&#24515;&#33039;SPECT&#25195;&#25551;&#20202;&#37319;&#29992;&#38745;&#24577;&#20960;&#20309;&#24418;&#24335;&#65292;&#21516;&#26102;&#33719;&#21462;19&#20010;&#25237;&#24433;&#20197;&#25552;&#39640;&#28789;&#25935;&#24230;&#21644;&#23454;&#29616;&#21160;&#24577;&#25104;&#20687;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#35282;&#24230;&#37319;&#26679;&#23545;&#22270;&#20687;&#36136;&#37327;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20174;&#38745;&#24577;&#25968;&#25454;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#36825;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#23569;&#35270;&#35282;&#25104;&#20687;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TIP-Net&#30340;&#26032;&#22411;&#19977;&#32500;Transformer&#21452;&#22495;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#30340;&#19977;&#32500;&#24515;&#33039;SPECT&#22270;&#20687;&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#23450;&#21046;&#30340;&#25237;&#24433;&#21040;&#22270;&#20687;&#22495;&#36716;&#25442;&#22120;&#65292;&#30452;&#25509;&#20174;&#25237;&#24433;&#25968;&#25454;&#20013;&#37325;&#24314;&#19977;&#32500;&#24515;&#33039;SPECT&#22270;&#20687;&#65292;&#32780;&#19981;&#38656;&#35201;&#36845;&#20195;&#37325;&#24314;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#32473;&#23450;&#20854;&#37325;&#24314;&#36755;&#20986;&#21644;&#21407;&#22987;&#30340;&#23569;&#35270;&#35282;&#37325;&#24314;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular disease (CVD) is the leading cause of death worldwide, and myocardial perfusion imaging using SPECT has been widely used in the diagnosis of CVDs. The GE 530/570c dedicated cardiac SPECT scanners adopt a stationary geometry to simultaneously acquire 19 projections to increase sensitivity and achieve dynamic imaging. However, the limited amount of angular sampling negatively affects image quality. Deep learning methods can be implemented to produce higher-quality images from stationary data. This is essentially a few-view imaging problem. In this work, we propose a novel 3D transformer-based dual-domain network, called TIP-Net, for high-quality 3D cardiac SPECT image reconstructions. Our method aims to first reconstruct 3D cardiac SPECT images directly from projection data without the iterative reconstruction process by proposing a customized projection-to-image domain transformer. Then, given its reconstruction output and the original few-view reconstruction, we further 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.08930</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31232;&#30095;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24212;&#29992;&#20110;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#28857;&#21305;&#37197;&#12290;&#19982;&#26631;&#20934;&#30340;&#8220;&#30417;&#30563;&#8221;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20851;&#38190;&#28857;&#23545;&#20043;&#38388;&#30340;&#30495;&#23454;&#23545;&#24212;&#12290;&#30456;&#21453;&#65292;&#23427;&#36890;&#36807;&#24378;&#21046;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#12290;&#30001;&#20110;&#21305;&#37197;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#26159;&#31163;&#25955;&#30340;&#65292;&#23427;&#20204;&#30340;&#23548;&#25968;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#40657;&#30418;&#24494;&#20998;&#30340;&#26368;&#26032;&#32467;&#26524;&#22522;&#30784;&#19978;&#26500;&#24314;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#19982;&#20219;&#24847;&#32593;&#32476;&#26550;&#26500;&#21644;&#32452;&#21512;&#27714;&#35299;&#22120;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>Disco-Bench&#26159;&#19968;&#20010;&#38754;&#21521;&#35821;&#35328;&#24314;&#27169;&#30340;&#35770;&#36848;&#24863;&#30693;&#35780;&#20272;&#22522;&#20934;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;NLP&#20219;&#21153;&#35780;&#20272;&#21477;&#20869;&#35770;&#36848;&#23646;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25991;&#29486;&#39046;&#22495;&#30340;9&#20010;&#27979;&#35797;&#38598;&#21644;&#19968;&#20010;&#35786;&#26029;&#27979;&#35797;&#22871;&#20214;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#35770;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;20&#20010;&#19981;&#21516;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.08074</link><description>&lt;p&gt;
Disco-Bench: &#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#24314;&#27169;&#30340;&#35770;&#36848;&#24863;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language Modelling. (arXiv:2307.08074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08074
&lt;/p&gt;
&lt;p&gt;
Disco-Bench&#26159;&#19968;&#20010;&#38754;&#21521;&#35821;&#35328;&#24314;&#27169;&#30340;&#35770;&#36848;&#24863;&#30693;&#35780;&#20272;&#22522;&#20934;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;NLP&#20219;&#21153;&#35780;&#20272;&#21477;&#20869;&#35770;&#36848;&#23646;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25991;&#29486;&#39046;&#22495;&#30340;9&#20010;&#27979;&#35797;&#38598;&#21644;&#19968;&#20010;&#35786;&#26029;&#27979;&#35797;&#22871;&#20214;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#35770;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;20&#20010;&#19981;&#21516;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#36848;&#24314;&#27169;&#65292;&#21363;&#36229;&#36234;&#20010;&#21035;&#21477;&#23376;&#30340;&#35821;&#35328;&#29616;&#35937;&#65292;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#19968;&#20010;&#22522;&#26412;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#21477;&#38388;&#23646;&#24615;&#30340;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#36328;&#21477;&#23376;&#30340;&#20851;&#38190;&#35770;&#36848;&#29616;&#35937;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Disco-Bench&#65292;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21477;&#20869;&#35770;&#36848;&#23646;&#24615;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#29702;&#35299;&#12289;&#32763;&#35793;&#21644;&#29983;&#25104;&#12290;Disco-Bench&#21253;&#25324;&#20102;&#25991;&#29486;&#39046;&#22495;&#30340;9&#20010;&#25991;&#26723;&#32423;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20013;&#25991;&#21644;/&#25110;&#33521;&#25991;&#20013;&#20016;&#23500;&#30340;&#35770;&#36848;&#29616;&#35937;&#65288;&#22914;&#36830;&#36143;&#24615;&#21644;&#36830;&#36143;&#24615;&#65289;&#12290;&#20026;&#20102;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#22871;&#35786;&#26029;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20197;&#26816;&#26597;&#30446;&#26631;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#21040;&#20102;&#35770;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#24635;&#20849;&#35780;&#20272;&#20102;20&#20010;&#22522;&#20110;Transformer&#12289;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26550;&#26500;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36890;&#29992;&#22411;&#12289;&#39046;&#22495;&#20869;&#21644;&#21830;&#19994;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling discourse -- the linguistic phenomena that go beyond individual sentences, is a fundamental yet challenging aspect of natural language processing (NLP). However, existing evaluation benchmarks primarily focus on the evaluation of inter-sentence properties and overlook critical discourse phenomena that cross sentences. To bridge the gap, we propose Disco-Bench, a benchmark that can evaluate intra-sentence discourse properties across a diverse set of NLP tasks, covering understanding, translation, and generation. Disco-Bench consists of 9 document-level testsets in the literature domain, which contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese and/or English. For linguistic analysis, we also design a diagnostic test suite that can examine whether the target models learn discourse knowledge. We totally evaluate 20 general-, in-domain and commercial models based on Transformer, advanced pretraining architectures and large language models (LLMs). Our results 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#24182;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2307.08044</link><description>&lt;p&gt;
&#26580;&#24615;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65306;&#36890;&#36807;&#25490;&#21517;&#22238;&#24402;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression. (arXiv:2307.08044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#24182;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20107;&#20214;&#20998;&#26512;&#65292;&#20063;&#34987;&#31216;&#20026;&#29983;&#23384;&#20998;&#26512;&#65292;&#26088;&#22312;&#26681;&#25454;&#19968;&#32452;&#29305;&#24449;&#39044;&#27979;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#34987;&#25130;&#23614;&#30340;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#20351;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#22797;&#26434;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#21644;&#21152;&#36895;&#22833;&#25928;&#26102;&#38388;&#65288;AFT&#65289;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#19968;&#20123;&#20551;&#35774;&#65292;&#22914;&#27604;&#20363;&#39118;&#38505;&#21644;&#32447;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;AFT&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#30340;&#21442;&#25968;&#20998;&#24067;&#20551;&#35774;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21361;&#38505;&#27169;&#22411;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#25991;&#29486;&#20013;&#23545;&#20110;AFT&#30340;&#34920;&#31034;&#23398;&#20064;&#23578;&#26410;&#24191;&#27867;&#25506;&#32034;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#20197;&#21361;&#38505;&#20026;&#37325;&#28857;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#26356;&#21152;&#31616;&#21333;&#21644;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#26469;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-to-event analysis, also known as survival analysis, aims to predict the time of occurrence of an event, given a set of features. One of the major challenges in this area is dealing with censored data, which can make learning algorithms more complex. Traditional methods such as Cox's proportional hazards model and the accelerated failure time (AFT) model have been popular in this field, but they often require assumptions such as proportional hazards and linearity. In particular, the AFT models often require pre-specified parametric distributional assumptions. To improve predictive performance and alleviate strict assumptions, there have been many deep learning approaches for hazard-based models in recent years. However, representation learning for AFT has not been widely explored in the neural network literature, despite its simplicity and interpretability in comparison to hazard-focused methods. In this work, we introduce the Deep AFT Rank-regression model for Time-to-event predic
&lt;/p&gt;</description></item><item><title>AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.07851</link><description>&lt;p&gt;
AspectCSE: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07851
&lt;/p&gt;
&lt;p&gt;
AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#31895;&#30053;&#36817;&#20284;&#65292;&#20294;&#24573;&#30053;&#20102;&#20351;&#25991;&#26412;&#30456;&#20284;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#26041;&#38754;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#39044;&#27979;&#26356;&#21152;&#38024;&#23545;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AspectCSE&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;AspectCSE&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#21892;3.97%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;Wikidata&#30693;&#35782;&#22270;&#23646;&#24615;&#26469;&#35757;&#32451;&#22810;&#26041;&#38754;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;&#30456;&#20284;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#26041;&#38754;&#23884;&#20837;&#22312;&#29305;&#23450;&#26041;&#38754;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#36827;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#36890;&#36807;&#30830;&#23450;&#26367;&#20195;&#36335;&#24452;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#35266;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#24182;&#24110;&#21161;&#35782;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.07764</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI with counterfactual paths. (arXiv:2307.07764v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#36890;&#36807;&#30830;&#23450;&#26367;&#20195;&#36335;&#24452;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#35266;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#24182;&#24110;&#21161;&#35782;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#21407;&#21017;&#19978;&#26088;&#22312;&#20351;&#40657;&#30418;&#27169;&#22411;&#36879;&#26126;&#21487;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26465;&#20214;&#32622;&#25442;&#29983;&#25104;&#20102;&#21453;&#20107;&#23454;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#30830;&#23450;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#32467;&#26524;&#30340;&#26367;&#20195;&#36335;&#24452;&#26469;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#35299;&#37322;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#26816;&#26597;&#30693;&#35782;&#22270;&#35889;&#20013;&#36755;&#20837;&#25968;&#25454;&#30340;&#20551;&#35774;&#24615;&#21464;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#31995;&#32479;&#22320;&#39564;&#35777;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#26816;&#26597;&#23545;&#27169;&#22411;&#39044;&#27979;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#25110;&#29305;&#24449;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#30340;&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#26356;&#30452;&#35266;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) is an increasingly important area of research in machine learning, which in principle aims to make black-box models transparent and interpretable. In this paper, we propose a novel approach to XAI that uses counterfactual paths generated by conditional permutations. Our method provides counterfactual explanations by identifying alternative paths that could have led to different outcomes. The proposed method is particularly suitable for generating explanations based on counterfactual paths in knowledge graphs. By examining hypothetical changes to the input data in the knowledge graph, we can systematically validate the behaviour of the model and examine the features or combination of features that are most important to the model's predictions. Our approach provides a more intuitive and interpretable explanation for the model's behaviour than traditional feature weighting methods and can help identify and mitigate biases in the model.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#30340;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#25552;&#20379;&#23398;&#20064;&#21644;&#29702;&#35299;&#25163;&#26415;&#35270;&#39057;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.05182</link><description>&lt;p&gt;
Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery&#65288;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#30340;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#65289;
&lt;/p&gt;
&lt;p&gt;
Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.05182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#30340;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#25552;&#20379;&#23398;&#20064;&#21644;&#29702;&#35299;&#25163;&#26415;&#35270;&#39057;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#22312;&#23398;&#20064;&#25163;&#26415;&#26102;&#36890;&#24120;&#20381;&#36182;&#20110;&#39640;&#32423;&#22806;&#31185;&#21307;&#29983;&#21644;&#19987;&#23478;&#22238;&#31572;&#20182;&#20204;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#20204;&#32463;&#24120;&#24537;&#20110;&#20020;&#24202;&#21644;&#23398;&#26415;&#24037;&#20316;&#65292;&#27809;&#26377;&#22810;&#23569;&#26102;&#38388;&#25552;&#20379;&#25351;&#23548;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22806;&#31185;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#21482;&#33021;&#25552;&#20379;&#31616;&#21333;&#31572;&#26696;&#65292;&#32780;&#27809;&#26377;&#31572;&#26696;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#65292;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#23569;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#22806;&#31185;&#35270;&#35273;&#38382;&#31572;&#23450;&#20301;&#31995;&#32479;&#23545;&#20110;&#21307;&#23398;&#29983;&#21644;&#21021;&#32423;&#22806;&#31185;&#21307;&#29983;&#20174;&#24405;&#21046;&#30340;&#25163;&#26415;&#35270;&#39057;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22806;&#31185;&#22330;&#26223;&#30340;&#31471;&#21040;&#31471;Transformer&#19982;&#20849;&#21516;&#20851;&#27880;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#65288;CAT-ViL&#65289;&#30340;VQLA&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#36890;&#36807;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#12290;CAT-ViL&#23884;&#20837;&#27169;&#22359;&#30340;&#35774;&#35745;&#26088;&#22312;&#34701;&#21512;&#26469;&#33258;&#35270;&#35273;&#21644;&#25991;&#26412;&#26469;&#28304;&#30340;&#24322;&#26500;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery. However, experts are often busy with clinical and academic work, and have little time to give guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers. In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks. Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos. We propose an end-to-end Transformer with Co-Attention gaTed Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not require feature extraction through detection models. The CAT-ViL embedding module is designed to fuse heterogeneous features from visual and textual sources. The fused embedding 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;2L&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#24341;&#23548;&#21512;&#25104;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21442;&#32771;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#21644;&#22312;MicroRTS&#38182;&#26631;&#36187;&#20013;&#30340;&#32988;&#21033;&#65292;&#35777;&#26126;&#20102;2L&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.04893</link><description>&lt;p&gt;
&#36873;&#25321;&#22909;&#23545;&#25163;&#65306;&#22914;&#20309;&#25351;&#23548;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies. (arXiv:2307.04893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;2L&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#24341;&#23548;&#21512;&#25104;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#21442;&#32771;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#21644;&#22312;MicroRTS&#38182;&#26631;&#36187;&#20013;&#30340;&#32988;&#21033;&#65292;&#35777;&#26126;&#20102;2L&#31639;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Local Learner (2L)&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#20379;&#19968;&#32452;&#21442;&#32771;&#31574;&#30053;&#65292;&#20197;&#25351;&#23548;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25628;&#32034;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;&#20043;&#21069;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#36845;&#20195;&#26368;&#20339;&#21709;&#24212;&#31639;&#27861;(IBR)&#65292;&#34394;&#26500;&#28216;&#25103;&#31639;&#27861;(FP)&#21644;&#21452;&#27491;&#20132;&#31639;&#27861;(DO)&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#25110;&#20250;&#28431;&#25481;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;2L&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#21442;&#32771;&#31574;&#30053;&#20197;&#25552;&#39640;&#25628;&#32034;&#20449;&#21495;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#28216;&#25103;&#20013;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#23454;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#21183;&#65292;&#20854;&#20013;&#21253;&#25324;MicroRTS&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#26102;&#25112;&#30053;&#28216;&#25103;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;2L&#23398;&#20064;&#21040;&#30340;&#21442;&#32771;&#31574;&#30053;&#25552;&#20379;&#20102;&#27604;IBR&#65292;FP&#21644;DO&#26356;&#24378;&#30340;&#25628;&#32034;&#20449;&#21495;&#12290;&#25105;&#20204;&#36824;&#27169;&#25311;&#20102;&#19968;&#22330;MicroRTS&#38182;&#26631;&#36187;&#65292;&#20854;&#20013;&#20351;&#29992;2L&#21512;&#25104;&#22120;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20004;&#20010;&#26368;&#26032;MicroRTS&#27604;&#36187;&#30340;&#32988;&#32773;&#65292;&#36825;&#20123;&#32988;&#32773;&#22343;&#20026;&#20154;&#31867;&#32534;&#31243;&#21592;&#32534;&#20889;&#30340;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Local Learner (2L), an algorithm for providing a set of reference strategies to guide the search for programmatic strategies in two-player zero-sum games. Previous learning algorithms, such as Iterated Best Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be computationally expensive or miss important information for guiding search algorithms. 2L actively selects a set of reference strategies to improve the search signal. We empirically demonstrate the advantages of our approach while guiding a local search algorithm for synthesizing strategies in three games, including MicroRTS, a challenging real-time strategy game. Results show that 2L learns reference strategies that provide a stronger search signal than IBR, FP, and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L outperformed the winners of the two latest MicroRTS competitions, which were programmatic strategies written by human programmers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#35780;&#35770;&#21644;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#29983;&#25104;&#23545;&#27604;&#35780;&#20215;&#21477;&#23376;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#12289;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#29983;&#25104;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03691</link><description>&lt;p&gt;
&#23558;&#33529;&#26524;&#19982;&#33529;&#26524;&#36827;&#34892;&#27604;&#36739;&#65306;&#20174;&#29992;&#25143;&#35780;&#35770;&#29983;&#25104;&#32437;&#21521;&#24863;&#30693;&#30340;&#27604;&#36739;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Review. (arXiv:2307.03691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#35780;&#35770;&#21644;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#29983;&#25104;&#23545;&#27604;&#35780;&#20215;&#21477;&#23376;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#12289;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#29983;&#25104;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#30456;&#20284;&#30340;&#36873;&#25321;&#20013;&#25214;&#21040;&#26368;&#20339;&#20135;&#21697;&#26159;&#38750;&#24120;&#32791;&#26102;&#30340;&#12290;&#27604;&#36739;&#21477;&#23376;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20197;&#31361;&#20986;&#30340;&#26041;&#24335;&#23545;&#27604;&#19968;&#20010;&#39033;&#30446;&#19982;&#20854;&#20182;&#39033;&#30446;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#24378;&#35843;&#20986;&#37325;&#35201;&#29305;&#24449;&#12290;&#22522;&#20110;&#29992;&#25143;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#39033;&#30446;&#30340;&#35780;&#35770;&#21450;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#65292;&#25105;&#20204;&#29983;&#25104;&#27604;&#36739;&#35780;&#35770;&#21477;&#23376;&#26469;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19977;&#20010;&#36830;&#32493;&#32452;&#20214;&#65306;&#65288;i&#65289;&#19968;&#20010;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#29992;&#20110;&#23545;&#39033;&#30446;&#36827;&#34892;&#32534;&#30721;&#27604;&#36739;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#29983;&#25104;&#27604;&#36739;&#21477;&#23376;&#65292;&#65288;iii&#65289;&#19968;&#31181;&#29992;&#20110;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#26032;&#22411;&#35299;&#30721;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#31243;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#19988;&#22810;&#26679;&#30340;&#27604;&#36739;&#21477;&#23376;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#26469;&#39564;&#35777;&#25105;&#20204;&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#19988;&#30495;&#23454;&#30340;&#27604;&#36739;&#35780;&#35770;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is time-consuming to find the best product among many similar alternatives. Comparative sentences can help to contrast one item from others in a way that highlights important features of an item that stand out. Given reviews of one or multiple items and relevant item features, we generate comparative review sentences to aid users to find the best fit. Specifically, our model consists of three successive components in a transformer: (i) an item encoding module to encode an item for comparison, (ii) a comparison generation module that generates comparative sentences in an autoregressive manner, (iii) a novel decoding method for user personalization. We show that our pipeline generates fluent and diverse comparative sentences. We run experiments on the relevance and fidelity of our generated sentences in a human evaluation study and find that our algorithm creates comparative review sentences that are relevant and truthful.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02620</link><description>&lt;p&gt;
&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#35266;&#27979;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#21253;&#25324;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#12289;&#20379;&#26262;&#19982;&#21046;&#20919;&#31995;&#32479;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#20316;-&#24863;&#30693;&#24490;&#29615;&#36890;&#24120;&#20551;&#35774;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#37117;&#21487;&#20197;&#33719;&#24471;&#23545;&#29615;&#22659;&#29366;&#24577;&#30340;&#27979;&#37327;&#65292;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#28023;&#21644;&#34892;&#26143;&#26426;&#22120;&#20154;&#25506;&#32034;&#12289;&#26448;&#26009;&#35774;&#35745;&#21644;&#21307;&#23398;&#31561;&#24212;&#29992;&#20013;&#65292;&#27979;&#37327;&#25110;&#32773;&#36817;&#20284;&#29615;&#22659;&#29366;&#24577;&#21487;&#33021;&#20250;&#20135;&#29983;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36817;&#26469;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#37319;&#21462;&#20102;RL&#20195;&#29702;&#21487;&#33021;&#19981;&#38656;&#35201;&#25110;&#32773;&#19981;&#24819;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#36827;&#34892;&#26114;&#36149;&#27979;&#37327;&#30340;&#35266;&#28857;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Dynamic Multi-Step Observationless Agent (DMSOA)&#65292;&#24182;&#23558;&#20854;&#19982;&#25991;&#29486;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#22312;OpenAI gym&#21644;Atari Pong&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as deep-sea and planetary robot exploration, materials design and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and meas
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02591</link><description>&lt;p&gt;
ODD: &#19968;&#20221;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#65288;ORAB&#65289;&#26159;&#38450;&#27490;&#33647;&#29289;&#36807;&#37327;&#30340;&#26032;&#39118;&#38505;&#22240;&#32032;&#12290;&#20197;&#24448;&#65292;ORAB&#20027;&#35201;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#21644;&#33647;&#29289;&#32473;&#20104;&#30417;&#27979;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19981;&#33021;&#28085;&#30422;&#25152;&#26377;&#24322;&#24120;&#34892;&#20026;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;ORAB&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#24191;&#27867;&#26377;&#35760;&#24405;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;ORAB&#26816;&#27979;&#12290;ODD&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;750&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#12290;ODD&#26088;&#22312;&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#35782;&#21035;ORAB&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20061;&#20010;&#31867;&#21035;&#65306;1&#65289;&#24050;&#30830;&#35748;&#24322;&#24120;&#34892;&#20026;&#65292;2&#65289;&#26263;&#31034;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;3&#65289;&#38463;&#29255;&#31867;&#33647;&#29289;&#65292;4&#65289;&#36866;&#24212;&#30151;&#65292;5&#65289;&#24050;&#35786;&#26029;&#30340;&#38463;&#29255;&#21046;&#21058;&#20381;&#36182;&#65292;6&#65289;&#33519;&#20108;&#27694;&#24179;&#31867;&#33647;&#29289;&#65292;7&#65289;&#33647;&#29289;&#21464;&#21270;&#65292;8&#65289;&#19982;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30456;&#20851;&#65292;9&#65289;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioid related aberrant behaviors (ORAB) present novel risk factors for opioid overdose. Previously, ORAB have been mainly assessed by survey results and by monitoring drug administrations. Such methods however, cannot scale up and do not cover the entire spectrum of aberrant behaviors. On the other hand, ORAB are widely documented in electronic health record notes. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset comprising of more than 750 publicly available EHR notes. ODD has been designed to identify ORAB from patients' EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing (NLP) mode
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16177</link><description>&lt;p&gt;
&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65306;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defining data science: a new field of inquiry. (arXiv:2306.16177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16177
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#12290;&#23427;&#30340;&#21147;&#37327;&#12289;&#33539;&#22260;&#21644;&#35268;&#27169;&#23558;&#36229;&#36234;&#31185;&#23398;&#65292;&#25104;&#20026;&#20419;&#20351;&#30693;&#35782;&#21457;&#29616;&#24182;&#25913;&#21464;&#19990;&#30028;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#25105;&#20204;&#23578;&#26410;&#29702;&#35299;&#21644;&#23450;&#20041;&#23427;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#20854;&#28508;&#21147;&#21644;&#31649;&#29702;&#20854;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#33258;1962&#24180;&#20197;&#26469;&#32531;&#24930;&#21457;&#23637;&#65292;&#24182;&#19988;&#33258;2000&#24180;&#20197;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#23427;&#26159;&#19968;&#31181;&#26681;&#26412;&#24615;&#30340;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26159;21&#19990;&#32426;&#26368;&#27963;&#36291;&#12289;&#26368;&#24378;&#22823;&#21644;&#21457;&#23637;&#26368;&#24555;&#30340;&#21019;&#26032;&#20043;&#19968;&#12290;&#30001;&#20110;&#20854;&#20215;&#20540;&#12289;&#21147;&#37327;&#21644;&#36866;&#29992;&#24615;&#65292;&#23427;&#27491;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#25968;&#25454;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#21253;&#21547;&#20102;&#26080;&#25968;&#20851;&#20110;&#25968;&#25454;&#31185;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#23450;&#20041;&#12290;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#35768;&#22810;&#23450;&#20041;&#26159;&#29420;&#31435;&#30340;&#12289;&#24212;&#29992;&#29305;&#23450;&#30340;&#12289;&#30456;&#20114;&#19981;&#23436;&#25972;&#30340;&#12289;&#20887;&#20313;&#30340;&#25110;&#19981;&#19968;&#33268;&#30340;&#65292;&#22240;&#27492;&#25968;&#25454;&#31185;&#23398;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#22810;&#37325;&#23450;&#20041;&#25361;&#25112;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in 40+ disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application-specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#30340;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#65292;&#22312;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#27604;&#36739;&#20013;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15880</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards Open Vocabulary Learning: A Survey. (arXiv:2306.15880v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#30340;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#65292;&#22312;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#27604;&#36739;&#20013;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#26816;&#27979;&#31561;&#21508;&#31181;&#26680;&#24515;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22522;&#20110;&#23553;&#38381;&#38598;&#30340;&#20551;&#35774;&#65292;&#21363;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#24050;&#23450;&#20041;&#30340;&#31867;&#21035;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#12290;&#36825;&#20123;&#26032;&#26041;&#27861;&#26088;&#22312;&#23450;&#20301;&#21644;&#35782;&#21035;&#36229;&#20986;&#27880;&#37322;&#26631;&#31614;&#31354;&#38388;&#30340;&#31867;&#21035;&#12290;&#19982;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#30456;&#27604;&#65292;&#24320;&#25918;&#35789;&#27719;&#26041;&#27861;&#26356;&#21152;&#36890;&#29992;&#12289;&#23454;&#29992;&#21644;&#26377;&#25928;&#12290;&#26412;&#25991;&#23545;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#36817;&#26399;&#22312;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#24320;&#25918;&#38598;&#35782;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#31561;&#30456;&#20851;&#27010;&#24565;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#22312;&#20998;&#21106;&#20219;&#21153;&#30340;&#20960;&#20010;&#32039;&#23494;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of visual scene understanding, deep neural networks have made impressive advancements in various core tasks like segmentation, tracking, and detection. However, most approaches operate on the close-set assumption, meaning that the model can only identify pre-defined categories that are present in the training set. Recently, open vocabulary settings were proposed due to the rapid progress of vision language pre-training. These new approaches seek to locate and recognize categories beyond the annotated label space. The open vocabulary approach is more general, practical, and effective compared to weakly supervised and zero-shot settings. This paper provides a thorough review of open vocabulary learning, summarizing and analyzing recent developments in the field. In particular, we begin by comparing it to related concepts such as zero-shot learning, open-set recognition, and out-of-distribution detection. Then, we review several closely related tasks in the case of segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20687;&#32032;&#35266;&#27979;&#20013;&#36827;&#34892;&#31232;&#30095;&#22870;&#21169;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#20316;&#20026;&#30446;&#26631;&#26465;&#20214;&#65292;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25913;&#36827;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#19987;&#23478;&#34892;&#21160;&#27425;&#25968;&#20943;&#23569;&#20102;4-20&#20493;&#65292;&#24182;&#19988;&#20248;&#20110;&#20998;&#23618;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.13872</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#20687;&#32032;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Pixels with Expert Observations. (arXiv:2306.13872v2 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20687;&#32032;&#35266;&#27979;&#20013;&#36827;&#34892;&#31232;&#30095;&#22870;&#21169;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#20316;&#20026;&#30446;&#26631;&#26465;&#20214;&#65292;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25913;&#36827;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#19987;&#23478;&#34892;&#21160;&#27425;&#25968;&#20943;&#23569;&#20102;4-20&#20493;&#65292;&#24182;&#19988;&#20248;&#20110;&#20998;&#23618;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31232;&#30095;&#22870;&#21169;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#21487;&#20197;&#21033;&#29992;&#19987;&#23478;&#34892;&#20026;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#26126;&#30830;&#30340;&#19987;&#23478;&#34892;&#21160;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#65292;&#32780;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#36890;&#24120;&#26356;&#23481;&#26131;&#33719;&#24471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#20687;&#32032;&#35266;&#27979;&#20013;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23558;&#19987;&#23478;&#35266;&#23519;&#25968;&#25454;&#20316;&#20026;&#30446;&#26631;&#26465;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20013;&#38388;&#35270;&#35273;&#30446;&#26631;&#65292;&#20351;&#20854;&#36890;&#36807;&#36830;&#32493;&#36798;&#21040;&#19968;&#31995;&#21015;&#30446;&#26631;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22359;&#22534;&#21472;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#19982;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26234;&#33021;&#20307;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#30340;&#19987;&#23478;&#34892;&#21160;&#27425;&#25968;&#20943;&#23569;&#20102;4-20&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20248;&#20110;&#19968;&#20010;&#20998;&#23618;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), sparse rewards can present a significant challenge. Fortunately, expert actions can be utilized to overcome this issue. However, acquiring explicit expert actions can be costly, and expert observations are often more readily available. This paper presents a new approach that uses expert observations for learning in robot manipulation tasks with sparse rewards from pixel observations. Specifically, our technique involves using expert observations as intermediate visual goals for a goal-conditioned RL agent, enabling it to complete a task by successively reaching a series of goals. We demonstrate the efficacy of our method in five challenging block construction tasks in simulation and show that when combined with two state-of-the-art agents, our approach can significantly improve their performance while requiring 4-20 times fewer expert actions during training. Moreover, our method is also superior to a hierarchical baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13264</link><description>&lt;p&gt;
FedSelect: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#25968;&#33258;&#23450;&#20041;&#36873;&#25321;&#30340;&#32454;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#24494;&#35843;&#23458;&#25143;&#31471;&#21442;&#25968;&#25110;&#38024;&#23545;&#26412;&#22320;&#20219;&#21153;&#20010;&#24615;&#21270;&#26550;&#26500;&#26469;&#25552;&#39640;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#22312;&#29306;&#29298;&#37325;&#35201;&#30340;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#35201;&#20040;&#22312;&#39044;&#20808;&#30830;&#23450;&#32593;&#32476;&#23618;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#20840;&#23616;&#30693;&#35782;&#20648;&#23384;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedSelect&#65292;&#36890;&#36807;&#21516;&#26102;&#25628;&#32034;&#24182;&#33719;&#24471;&#20010;&#24615;&#21270;&#26368;&#20339;&#21442;&#25968;&#21644;&#29992;&#20110;&#20840;&#23616;&#32858;&#21512;&#30340;&#20854;&#20313;&#21442;&#25968;&#65292;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
&lt;/p&gt;</description></item><item><title>&#36716;&#25442;&#22120;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20351;&#29992;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#27604;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.10891</link><description>&lt;p&gt;
&#36716;&#25442;&#22120;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#39044;&#27979;&#22810;&#20010;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Transformer Training Strategies for Forecasting Multiple Load Time Series. (arXiv:2306.10891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10891
&lt;/p&gt;
&lt;p&gt;
&#36716;&#25442;&#22120;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20351;&#29992;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#27604;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;&#26234;&#33021;&#30005;&#32593;&#20013;&#65292;&#20934;&#30830;&#30340;&#36127;&#36733;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;&#22312;&#26412;&#22320;&#24179;&#34913;&#20379;&#38656;&#65292;&#24182;&#38450;&#27490;&#30005;&#32593;&#25925;&#38556;&#12290;&#23613;&#31649;&#34987;&#30417;&#27979;&#30340;&#23458;&#25143;&#25968;&#37327;&#23558;&#38543;&#30528;&#19981;&#26029;&#25512;&#36827;&#30340;&#26234;&#33021;&#30005;&#34920;&#23433;&#35013;&#32780;&#22686;&#21152;&#65292;&#20294;&#27599;&#20010;&#23458;&#25143;&#30340;&#25968;&#25454;&#37327;&#22987;&#32456;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#25442;&#22120;&#36127;&#36733;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#21463;&#30410;&#20110;&#36716;&#31227;&#23398;&#20064;&#31574;&#30053;&#65292;&#21363;&#22312;&#22810;&#20010;&#23458;&#25143;&#30340;&#36127;&#36733;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#20840;&#23616;&#30340;&#21333;&#21464;&#37327;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;&#20004;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#23458;&#25143;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#20248;&#20110;&#30456;&#20851;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#22810;&#21464;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#19982;&#20854;&#20182;&#20004;&#31181;&#31574;&#30053;&#30456;&#27604;&#65292;&#20840;&#23616;&#35757;&#32451;&#31574;&#30053;&#22312;&#20174;&#26410;&#26469;&#19968;&#22825;&#21040;&#19968;&#20010;&#26376;&#30340;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#39044;&#27979;&#35823;&#24046;&#38477;&#20302;&#20102;21.8%&#21644;12.8%&#12290;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;LSTM&#27169;&#22411;&#30340;&#27604;&#36739;&#26174;&#31034;&#65292;&#36716;&#25442;&#22120;&#35757;&#32451;&#31574;&#30053;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the smart grid of the future, accurate load forecasts on the level of individual clients can help to balance supply and demand locally and to prevent grid outages. While the number of monitored clients will increase with the ongoing smart meter rollout, the amount of data per client will always be limited. We evaluate whether a Transformer load forecasting model benefits from a transfer learning strategy, where a global univariate model is trained on the load time series from multiple clients. In experiments with two datasets containing load time series from several hundred clients, we find that the global training strategy is superior to the multivariate and local training strategies used in related work. On average, the global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies, measured across forecasting horizons from one day to one month into the future. A comparison to linear models, multi-layer perceptrons and LSTMs shows that T
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10125</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10125
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;SSL&#26368;&#31361;&#20986;&#30340;&#20248;&#21183;&#26159;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#19982;&#35768;&#22810;&#20851;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#32508;&#36848;&#30456;&#27604;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;SSL&#30340;&#32508;&#36848;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#22238;&#39038;&#20102;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#29616;&#26377;&#32508;&#36848;&#65292;&#28982;&#21518;&#36890;&#36807;&#24635;&#32467;&#20174;&#29983;&#25104;&#22411;&#12289;&#23545;&#27604;&#22411;&#21644;&#23545;&#25239;&#22411;&#19977;&#20010;&#35282;&#24230;&#23545;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#21313;&#20010;&#23376;&#31867;&#65292;&#35814;&#32454;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#30452;&#35273;&#12289;&#20027;&#35201;&#26694;&#26550;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Clausal Tableaux&#35777;&#26126;&#31995;&#32479;&#23454;&#29616;&#21487;&#34892;&#30340;&#33539;&#22260;&#38480;&#21046;&#25554;&#20540;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03572</link><description>&lt;p&gt;
&#36890;&#36807;Clausal Tableaux&#23454;&#29616;&#33539;&#22260;&#38480;&#21046;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Range-Restricted Interpolation through Clausal Tableaux. (arXiv:2306.03572v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03572
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Clausal Tableaux&#35777;&#26126;&#31995;&#32479;&#23454;&#29616;&#21487;&#34892;&#30340;&#33539;&#22260;&#38480;&#21046;&#25554;&#20540;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#38454;&#36923;&#36753;&#30340;Clausal Tableaux&#35777;&#26126;&#31995;&#32479;&#65292;&#20174;&#36755;&#20837;&#21040;&#36755;&#20986;&#20256;&#36882;&#21464;&#21270;&#30340;&#33539;&#22260;&#38480;&#21046;&#21644;Horn&#24615;&#36136;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#23558;&#35777;&#26126;&#32467;&#26500;&#30340;&#25805;&#20316;&#19982;&#39640;&#24230;&#20248;&#21270;&#30340;&#19968;&#38454;&#35777;&#26126;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#21487;&#34892;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;&#20027;&#35201;&#24212;&#29992;&#20110;&#26597;&#35810;&#21512;&#25104;&#21644;&#25554;&#20540;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how variations of range-restriction and also the Horn property can be passed from inputs to outputs of Craig interpolation in first-order logic. The proof system is clausal tableaux, which stems from first-order ATP. Our results are induced by a restriction of the clausal tableau structure, which can be achieved in general by a proof transformation, also if the source proof is by resolution/paramodulation. Primarily addressed applications are query synthesis and reformulation with interpolation. Our methodical approach combines operations on proof structures with the immediate perspective of feasible implementation through incorporating highly optimized first-order provers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.16259</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#29616;&#29366;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37319;&#29992;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38271;&#25991;&#26412;&#20998;&#26512;&#30340;&#38656;&#27714;&#19982;&#30701;&#25991;&#26412;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#32780;&#32593;&#32476;&#19978;&#20256;&#36755;&#30340;&#25991;&#26723;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#20351;&#38271;&#25991;&#26412;&#30340;&#33258;&#21160;&#29702;&#35299;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;a&#65289;&#27010;&#36848;&#30456;&#20851;&#30340;&#31070;&#32463;&#26500;&#24314;&#27169;&#22359;&#65292;&#20316;&#20026;&#30701;&#25945;&#31243;&#65307;b&#65289;&#24635;&#32467;&#38271;&#25991;&#26412;NLP&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#26680;&#24515;&#20219;&#21153;&#65306;&#25991;&#26723;&#20998;&#31867;&#21644;&#25991;&#26723;&#25688;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#20063;&#28085;&#30422;&#22312;&#20869;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26723;&#20998;&#31867;&#30340;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;NLP&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#20844;&#24320;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#21644;2&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#12290;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#27867;&#21270;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2305.11435</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Mode. (arXiv:2305.11435v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#21644;2&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#12290;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#27867;&#21270;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#35757;&#32451;&#30446;&#26631;&#35757;&#32451;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#26102;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#34920;&#31034;&#38899;&#33410;&#30340;&#21333;&#20803;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20960;&#20046;&#30456;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#65288;HuBERT&#65289;&#65292;&#22312;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#27809;&#26377;&#34920;&#29616;&#20986;&#36825;&#31181;&#33021;&#21147;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#24341;&#23548;&#30446;&#26631;&#23548;&#33268;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#65292;&#28982;&#21518;&#20351;&#29992;&#20004;&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#23558;&#30456;&#21516;&#30340;&#38899;&#33410;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#35757;&#32451;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821;&#65289;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21516;&#30340;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;4&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#21333;&#35789;&#20998;&#21106;&#20219;&#21153;&#27867;&#21270;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20987;&#36133;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show that representations capturing syllabic units emerge when training a self-supervised speech model with a visually-grounded training objective. We demonstrate that a nearly identical model architecture (HuBERT) trained with a masked language modeling loss does not exhibit this same ability, suggesting that the visual grounding objective is responsible for the emergence of this phenomenon. We propose the use of a minimum cut algorithm to automatically predict syllable boundaries in speech, followed by a 2-stage clustering method to group identical syllables together. We show that our model not only outperforms a state-of-the-art syllabic segmentation method on the language it was trained on (English), but also generalizes in a zero-shot fashion to Estonian. Finally, we show that the same model is capable of zero-shot generalization for a word segmentation task on 4 other languages from the Zerospeech Challenge, in some cases beating the previous state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01788</link><description>&lt;p&gt;
&#35270;&#35273;&#19982;&#23450;&#20041;&#30456;&#36935;&#65306;&#34701;&#21512;&#35789;&#20041;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information. (arXiv:2305.01788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20934;&#30830;&#22320;&#25551;&#36848;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30446;&#26631;&#35789;&#27491;&#30830;&#24847;&#20041;&#30340;&#22270;&#20687;&#12290;&#20197;&#24448;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#35789;&#20041;&#22810;&#20041;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#27719;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#35789;&#20041;&#23450;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#30340;&#35789;&#20041;&#20449;&#24687;&#26102;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25913;&#36827;&#35789;&#20856;&#22806;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;GPT-3&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26126;&#26174;&#25552;&#39640;&#20102;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#22312;&#35789;&#20856;&#22806;&#20363;&#23376;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method. We will publish source 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.14760</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Class of Explanations for Classifiers with Non-Binary Features. (arXiv:2304.14760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24403;&#20998;&#26512;&#20998;&#31867;&#22120;&#20915;&#31574;&#26102;&#65292;&#24050;&#32463;&#26377;&#20004;&#31181;&#31867;&#22411;&#30340;&#35299;&#37322;&#21463;&#21040;&#20102;&#25991;&#29486;&#20013;&#30340;&#37325;&#35270;&#12290;&#31532;&#19968;&#31181;&#35299;&#37322;&#26159;&#20026;&#20915;&#31574;&#25552;&#20379;&#20805;&#20998;&#29702;&#30001;&#30340;&#35299;&#37322;&#65292;&#21363;&#32553;&#20889;&#20026;PI&#35299;&#37322;&#30340;&#35825;&#23548;&#24335;&#35299;&#37322;&#65307;&#31532;&#20108;&#31181;&#35299;&#37322;&#26159;&#20026;&#20309;&#19981;&#20570;&#20986;&#20854;&#20182;&#20915;&#31574;&#30340;&#35299;&#37322;&#65292;&#21363;&#23545;&#29031;&#24335;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24517;&#35201;&#29702;&#30001;&#12290;&#36825;&#20123;&#35299;&#37322;&#26159;&#20026;&#20108;&#20803;&#12289;&#31163;&#25955;&#21644;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20026;&#36830;&#32493;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#23450;&#20041;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23384;&#22312;&#38750;&#20108;&#20803;&#29305;&#24449;&#26102;&#65292;&#36825;&#20123;&#35299;&#37322;&#21487;&#20197;&#24471;&#21040;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31867;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#20063;&#34987;&#35777;&#26126;&#26159;&#23436;&#25972;&#21407;&#22240;&#30340;&#20027;&#35201;&#34164;&#21547;&#39033;&#21644;&#34987;&#34164;&#21547;&#39033;&#65292;&#21487;&#20197;&#20351;&#29992;&#37327;&#21270;&#31639;&#23376;&#33719;&#24471;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25913;&#36827;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#24565;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two types of explanations have received significant attention in the literature recently when analyzing the decisions made by classifiers. The first type explains why a decision was made and is known as a sufficient reason for the decision, also an abductive or PI-explanation. The second type explains why some other decision was not made and is known as a necessary reason for the decision, also a contrastive or counterfactual explanation. These explanations were defined for classifiers with binary, discrete and, in some cases, continuous features. We show that these explanations can be significantly improved in the presence of non-binary features, leading to a new class of explanations that relay more information about decisions and the underlying classifiers. Necessary and sufficient reasons were also shown to be the prime implicates and implicants of the complete reason for a decision, which can be obtained using a quantification operator. We show that our improved notions of necessa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.13671</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#30340;&#22810;&#30446;&#26631;&#29289;&#27969;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Logistics Optimization for Automated ATM Cash Replenishment Process. (arXiv:2304.13671v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#23558;&#25968;&#23383;&#25216;&#26415;&#25972;&#21512;&#21040;&#38134;&#34892;&#36816;&#33829;&#30340;&#21508;&#20010;&#26041;&#38754;&#21487;&#20197;&#25913;&#21892;&#27969;&#31243;&#33258;&#21160;&#21270;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#26381;&#21153;&#27700;&#24179;&#25552;&#21319;&#12290;&#34429;&#28982;ATM&#29616;&#37329;&#29289;&#27969;&#26159;&#24433;&#21709;&#36816;&#33829;&#25104;&#26412;&#21644;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20294;&#21364;&#24456;&#23569;&#26377;&#21162;&#21147;&#26469;&#21152;&#20197;&#25913;&#36827;&#12290;&#29305;&#21035;&#26159;&#22312;&#36234;&#21335;&#65292;&#25317;&#26377;&#36229;&#36807;2&#19975;&#21488;ATM&#30340;&#24066;&#22330;&#19978;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#21644;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ATM&#29616;&#37329;&#34917;&#20805;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#36827;&#34892;&#20102;&#27010;&#25324;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the digital transformation era, integrating digital technology into every aspect of banking operations improves process automation, cost efficiency, and service level improvement. Although logistics for ATM cash is a crucial task that impacts operating costs and consumer satisfaction, there has been little effort to enhance it. Specifically, in Vietnam, with a market of more than 20,000 ATMs nationally, research and technological solutions that can resolve this issue remain scarce. In this paper, we generalized the vehicle routing problem for ATM cash replenishment, suggested a mathematical model and then offered a tool to evaluate various situations. When being evaluated on the simulated dataset, our proposed model and method produced encouraging results with the benefits of cutting ATM cash operating costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35821;&#35328;-&#35270;&#35273;GPT&#27169;&#22411;&#65292;&#22686;&#24378;GPT2&#27169;&#22411;&#20197;&#21253;&#25324;&#35270;&#35273;&#36755;&#20837;&#65292;&#28982;&#21518;&#24494;&#35843;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#25163;&#26415;VQA&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#35270;&#35273;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2304.09974</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;SurgicalGPT&#65306;&#31471;&#21040;&#31471;&#30340;&#35821;&#35328;-&#35270;&#35273;GPT&#27169;&#22411;&#29992;&#20110;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery. (arXiv:2304.09974v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35821;&#35328;-&#35270;&#35273;GPT&#27169;&#22411;&#65292;&#22686;&#24378;GPT2&#27169;&#22411;&#20197;&#21253;&#25324;&#35270;&#35273;&#36755;&#20837;&#65292;&#28982;&#21518;&#24494;&#35843;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#25163;&#26415;VQA&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#35270;&#35273;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20351;&#29992;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36825;&#20123;&#33258;&#22238;&#24402;LLM&#21487;&#20197;&#29983;&#25104;&#38271;&#32780;&#36830;&#36143;&#30340;&#27573;&#33853;&#65292;&#20294;&#26159;&#22312;&#38656;&#35201;&#21516;&#26102;&#22788;&#29702;&#35270;&#35273;&#21450;&#35821;&#35328;&#20449;&#24687;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#38656;&#35201;&#21452;&#21521;&#27880;&#24847;&#21147;&#25110;&#34701;&#21512;&#25216;&#26415;&#26469;&#25429;&#33719;&#22810;&#31181;&#27169;&#24577;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;&#30001;&#20110;GPT&#26412;&#36523;&#19981;&#25903;&#25345;&#35270;&#35273;&#26631;&#35760;&#22788;&#29702;&#65292;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;GPT&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#25163;&#26415;VQA&#20013;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#35821;&#35328;-&#35270;&#35273;GPT&#65288;LV-GPT&#65289;&#27169;&#22411;&#65292;&#23558;GPT2&#27169;&#22411;&#25193;&#23637;&#20026;&#21253;&#25324;&#35270;&#35273;&#36755;&#20837;&#65288;&#22270;&#20687;&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;LV-GPT&#21253;&#25324;&#21151;&#33021;&#25552;&#21462;&#22120;&#65288;&#35270;&#35273;&#26631;&#35760;&#22120;&#65289;&#21644;&#35270;&#35273;&#26631;&#35760;&#23884;&#20837;&#65288;&#26631;&#35760;&#31867;&#22411;&#21644;&#23039;&#24577;&#65289;&#12290;&#37492;&#20110;GPT&#27169;&#22411;&#20013;&#21333;&#21521;&#20851;&#27880;&#30340;&#38480;&#21046;&#20197;&#21450;&#29983;&#25104;&#36830;&#36143;&#38271;&#27573;&#33853;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;GPT&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21152;&#20837;&#21452;&#21521;&#20851;&#27880;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25163;&#26415;VQA&#20219;&#21153;&#20013;&#30340;&#35270;&#35273;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in GPT-based large language models (LLMs) are revolutionizing natural language processing, exponentially increasing its use across various domains. Incorporating uni-directional attention, these autoregressive LLMs can generate long and coherent paragraphs. However, for visual question answering (VQA) tasks that require both vision and language processing, models with bi-directional attention or models employing fusion techniques are often employed to capture the context of multiple modalities all at once. As GPT does not natively process vision tokens, to exploit the advancements in GPT models for VQA in robotic surgery, we design an end-to-end trainable Language-Vision GPT (LV-GPT) model that expands the GPT2 model to include vision input (image). The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and vision token embedding (token type and pose). Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.08649</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;BERT&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#35789;&#24615;&#65288;POS&#65289;&#26631;&#35760;&#31561;&#65289;&#19978;&#20135;&#29983;&#20102;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;&#32467;&#26524;&#12290;&#24403;&#20998;&#31867;&#38271;&#25991;&#26723;&#65288;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#30340;&#25991;&#26723;&#65289;&#26102;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#21487;&#33021;&#27604;&#36739;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#20915;&#23450;&#25110;&#26368;&#39640;&#27861;&#38498;&#25968;&#25454;&#24211;&#65288;SCDB&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;SOTA&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#38024;&#23545;&#38271;&#25991;&#26723;&#30340;SOTA&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#65288;1&#65289;&#24191;&#27867;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;15&#20010;&#31867;&#21035;&#65307;&#65288;2&#65289;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;279&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#32467;&#26524;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#20135;&#29983;80&#65285;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#20135;&#29983;60&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#23460;&#20869;&#29615;&#22659;&#19979;&#36827;&#34892;&#36816;&#21160;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#20351;&#29992;&#20102;&#32467;&#26500;&#36816;&#21160;&#19982;&#27169;&#25311;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#30740;&#31350;&#32773;&#25972;&#21512;&#20809;&#27969;&#21644;&#30456;&#23545;&#23039;&#24577;&#22238;&#24402;&#26041;&#27861;&#24110;&#21161;&#35299;&#20915;&#20102;&#22240;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#32467;&#26500;&#31561;&#38382;&#39064;&#32780;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#20026;&#23460;&#20869;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.07250</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#20809;&#27969;&#20449;&#24687;&#20013;&#34701;&#21512;&#36816;&#21160;&#32467;&#26500;&#19982;&#27169;&#25311;&#25968;&#25454;&#30340;&#32477;&#23545;&#20301;&#32622;&#22238;&#24402;&#65292;&#35299;&#20915;&#23460;&#20869;&#29615;&#22659;&#23450;&#20301;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments. (arXiv:2304.07250v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#23460;&#20869;&#29615;&#22659;&#19979;&#36827;&#34892;&#36816;&#21160;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#20351;&#29992;&#20102;&#32467;&#26500;&#36816;&#21160;&#19982;&#27169;&#25311;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#30740;&#31350;&#32773;&#25972;&#21512;&#20809;&#27969;&#21644;&#30456;&#23545;&#23039;&#24577;&#22238;&#24402;&#26041;&#27861;&#24110;&#21161;&#35299;&#20915;&#20102;&#22240;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#32467;&#26500;&#31561;&#38382;&#39064;&#32780;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#20026;&#23460;&#20869;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#30340;&#23450;&#20301;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#27604;&#22914;&#26426;&#22120;&#20154;&#12289;&#34394;&#25311;&#21644;&#22686;&#24378;&#29616;&#23454;&#12289;&#21644;&#22312;&#20179;&#24211;&#20013;&#36816;&#36865;&#36135;&#29289;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#20808;&#36827;&#21457;&#23637;&#24050;&#32463;&#20351;&#24471;&#20351;&#29992;&#21333;&#30446;&#35270;&#35273;&#30456;&#26426;&#36827;&#34892;&#23450;&#20301;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#30001;&#20110;&#29615;&#22659;&#26412;&#36523;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#38468;&#21152;&#20449;&#24687;&#21644;&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#22238;&#24402;&#65288;RPR&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20351;&#29992;Lucas-Kanade&#31639;&#27861;&#35745;&#31639;&#36830;&#32493;&#22270;&#20687;&#20043;&#38388;&#30340;&#20809;&#27969;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#23567;&#22411;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#30456;&#23545;&#23039;&#24577;&#12290;&#23558;&#32477;&#23545;&#23039;&#24577;&#21644;&#30456;&#23545;&#23039;&#24577;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is
&lt;/p&gt;</description></item><item><title>&#29992;&#29289;&#29702;&#24341;&#25806;&#24378;&#21046;&#23454;&#29616;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#29289;&#29702;&#21512;&#29702;&#24615;&#22312;&#23454;&#36341;&#20013;&#26377;&#24456;&#22823;&#22256;&#38590;&#12290;&#36825;&#31687;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#35273;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#21387;&#21147;&#28909;&#22270;&#12289;&#21387;&#21147;&#20013;&#24515;&#21644;&#36523;&#20307;&#36136;&#24515;&#31561;&#26415;&#35821;&#65292;&#22312;&#20272;&#35745;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.18246</link><description>&lt;p&gt;
&#22522;&#20110;&#30452;&#35273;&#29289;&#29702;&#30340;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
3D Human Pose Estimation via Intuitive Physics. (arXiv:2303.18246v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18246
&lt;/p&gt;
&lt;p&gt;
&#29992;&#29289;&#29702;&#24341;&#25806;&#24378;&#21046;&#23454;&#29616;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#29289;&#29702;&#21512;&#29702;&#24615;&#22312;&#23454;&#36341;&#20013;&#26377;&#24456;&#22823;&#22256;&#38590;&#12290;&#36825;&#31687;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#35273;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#21387;&#21147;&#28909;&#22270;&#12289;&#21387;&#21147;&#20013;&#24515;&#21644;&#36523;&#20307;&#36136;&#24515;&#31561;&#26415;&#35821;&#65292;&#22312;&#20272;&#35745;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20272;&#35745;&#20154;&#20307;&#23039;&#24577;&#26102;&#24448;&#24448;&#20250;&#20986;&#29616;&#19981;&#21512;&#29702;&#30340;&#36523;&#20307;&#20542;&#26012;&#12289;&#28014;&#21160;&#25110;&#31359;&#36879;&#22320;&#26495;&#30340;&#24773;&#20917;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#36523;&#20307;&#36890;&#24120;&#30001;&#22330;&#26223;&#25903;&#25745;&#30340;&#20107;&#23454;&#12290;&#29289;&#29702;&#24341;&#25806;&#21487;&#20197;&#29992;&#26469;&#24378;&#21046;&#23454;&#29616;&#29289;&#29702;&#21512;&#29702;&#24615;&#65292;&#20294;&#36825;&#20123;&#24341;&#25806;&#19981;&#21487;&#24494;&#20998;&#65292;&#20381;&#36182;&#20110;&#19981;&#29616;&#23454;&#30340;&#20195;&#29702;&#29289;&#20307;&#65292;&#24182;&#19988;&#38590;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#21644;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#26032;&#39062;&#30340;&#30452;&#35273;&#29289;&#29702;&#65288;IP&#65289;&#26415;&#35821;&#65292;&#36825;&#20123;&#26415;&#35821;&#21487;&#20197;&#20174;&#19968;&#20010;&#19982;&#22330;&#26223;&#30456;&#20114;&#20316;&#29992;&#30340;3D SMPL&#36523;&#20307;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#21463;&#29983;&#29289;&#21147;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#36523;&#20307;&#19978;&#30340;&#21387;&#21147;&#28909;&#22270;&#12289;&#28909;&#22270;&#19978;&#30340;&#21387;&#21147;&#20013;&#24515;&#65288;CoP&#65289;&#20197;&#21450;SMPL&#36523;&#20307;&#30340;&#36136;&#24515;&#12290;&#20511;&#21161;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;IPMAN&#65292;&#36890;&#36807;&#40723;&#21169;&#21512;&#29702;&#30340;&#22320;&#26495;&#25509;&#35302;&#21644;&#37325;&#21472;&#30340;CoP&#21644;CoM&#65292;&#22312;&#24425;&#33394;&#22270;&#20687;&#20013;&#20272;&#35745;&#19968;&#20010;&#8220;&#31283;&#23450;&#8221;&#30340;3D&#36523;&#20307;&#12290;&#25105;&#20204;&#30340;IP&#26415;&#35821;&#30452;&#35266;&#26131;&#25026;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#35745;&#31639;&#36895;&#24230;&#24555;&#65292;&#21487;&#24494;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#21644;&#22238;&#24402;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating 3D humans from images often produces implausible bodies that lean, float, or penetrate the floor. Such methods ignore the fact that bodies are typically supported by the scene. A physics engine can be used to enforce physical plausibility, but these are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. In contrast, we exploit novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Inspired by biomechanics, we infer the pressure heatmap on the body, the Center of Pressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). With these, we develop IPMAN, to estimate a 3D body from a color image in a "stable" configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, differentiable, and can be integrated into existing optimization and regression m
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#25945;&#32946;&#20013;&#26377;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#65292;&#38656;&#35201;&#32771;&#34385;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#31561;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.13379</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#20013;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#25361;&#25112;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review. (arXiv:2303.13379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13379
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#25945;&#32946;&#20013;&#26377;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#65292;&#38656;&#35201;&#32771;&#34385;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#30340;&#25945;&#32946;&#25216;&#26415;&#21019;&#26032;&#26174;&#31034;&#20986;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21019;&#26032;&#26469;&#33258;&#21160;&#21270;&#21508;&#31181;&#25945;&#32946;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#38382;&#39064;&#12289;&#25552;&#20379;&#21453;&#39304;&#21644;&#35780;&#20998;&#65289;&#65292;&#20294;&#23545;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#12290;&#36825;&#20123;&#25285;&#24551;&#21487;&#33021;&#20250;&#38459;&#30861;&#26410;&#26469;&#30740;&#31350;&#21644;&#22312;&#30495;&#23454;&#25945;&#32946;&#29615;&#22659;&#20013;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#21019;&#26032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;118&#31687;&#33258;2017&#24180;&#20197;&#26469;&#21457;&#34920;&#30340;&#21516;&#34892;&#35780;&#35758;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;LLMs&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#25945;&#32946;&#20219;&#21153;&#30340;&#24403;&#21069;&#30740;&#31350;&#29366;&#24577;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#31995;&#32479;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#65292;&#36824;&#30830;&#23450;&#20102;LLMs&#21019;&#26032;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational technology innovations that have been developed based on large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (e.g., question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts. To address this, we conducted a systematic literature review of 118 peer-reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The practical and ethical challenges of LLMs-based innovations were also identified by assessing their technological readiness, model performance, replicability, system transparency, privacy, equality, and beneficence. The findings 
&lt;/p&gt;</description></item><item><title>BoxSnake&#26159;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26694;&#27880;&#37322;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#25513;&#33180;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;BoxSnake&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11630</link><description>&lt;p&gt;
BoxSnake&#65306;&#20351;&#29992;&#26694;&#27880;&#37322;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
BoxSnake: Polygonal Instance Segmentation with Box Supervision. (arXiv:2303.11630v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11630
&lt;/p&gt;
&lt;p&gt;
BoxSnake&#26159;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26694;&#27880;&#37322;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#25513;&#33180;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;BoxSnake&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26694;&#27880;&#37322;&#30340;&#23454;&#20363;&#20998;&#21106;&#22240;&#21482;&#38656;&#35201;&#31616;&#21333;&#30340;&#26694;&#26631;&#27880;&#32780;&#38750;&#26114;&#36149;&#30340;&#25513;&#33180;&#25110;&#22810;&#36793;&#24418;&#26631;&#27880;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24102;&#26694;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#25513;&#33180;&#30340;&#26694;&#26550;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25216;&#26415;BoxSnake&#65292;&#39318;&#27425;&#20165;&#20351;&#29992;&#26694;&#27880;&#37322;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#65306;&#65288;1&#65289;&#22522;&#20110;&#28857;&#30340;&#21333;&#20803;&#25439;&#22833;&#65292;&#32422;&#26463;&#39044;&#27979;&#22810;&#36793;&#24418;&#30340;&#36793;&#30028;&#26694;&#20197;&#23454;&#29616;&#31895;&#30053;&#20998;&#21106;&#65307;&#65288;2&#65289;&#36317;&#31163;&#24863;&#30693;&#30340;&#25104;&#23545;&#25439;&#22833;&#65292;&#20419;&#20351;&#39044;&#27979;&#30340;&#22810;&#36793;&#24418;&#36148;&#21512;&#29289;&#20307;&#36793;&#30028;&#12290;&#19982;&#22522;&#20110;&#25513;&#33180;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;BoxSnake&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#39044;&#27979;&#20998;&#21106;&#19982;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#22312;Cityscapes&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weakly-supervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2303.11413</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25391;&#21160;&#20449;&#21495;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#33050;&#27493;&#24341;&#36215;&#30340;&#32467;&#26500;&#25391;&#21160;&#20449;&#21495;&#34987;&#24191;&#27867;&#29992;&#20110;&#20154;&#21592;&#35782;&#21035;&#12289;&#23450;&#20301;&#12289;&#20154;&#31867;&#27963;&#21160;&#25512;&#26029;&#12289;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#22122;&#22768;&#12289;&#30005;&#30913;&#24178;&#25200;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23454;&#38469;&#37319;&#38598;&#30340;&#20449;&#21495;&#36890;&#24120;&#20250;&#24102;&#26377;&#22122;&#22768;&#12290;&#22122;&#22768;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;&#20449;&#21495;&#22788;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26368;&#32456;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#35823;&#24046;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.06516</link><description>&lt;p&gt;
&#25171;&#24320;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20197;&#35745;&#31639;Shap&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#39640;&#25928;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36716;&#25442;&#20026;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#12290;&#25152;&#24471;&#21040;&#30340;&#30005;&#36335;&#34987;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#30456;&#27604;&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24341;&#29702;&#22312;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#32467;&#21512;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#31995;&#32479;&#65292;&#33021;&#22815;&#29983;&#25104;&#23545;&#20110;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#26377;&#29992;&#30340;&#24341;&#29702;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#26410;&#33021;&#35299;&#20915;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05854</link><description>&lt;p&gt;
&#24341;&#29702;&#65306;&#29983;&#25104;&#12289;&#36873;&#25321;&#12289;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Lemmas: Generation, Selection, Application. (arXiv:2303.05854v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24341;&#29702;&#22312;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#32467;&#21512;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#31995;&#32479;&#65292;&#33021;&#22815;&#29983;&#25104;&#23545;&#20110;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#26377;&#29992;&#30340;&#24341;&#29702;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#26410;&#33021;&#35299;&#20915;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#24341;&#29702;&#26159;&#25968;&#23398;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#65292;&#25105;&#20204;&#23545;&#24341;&#29702;&#22312;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#28041;&#21450;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#31995;&#32479;&#30340;&#23454;&#39564;&#65292;&#35813;&#31995;&#32479;&#33021;&#29983;&#25104;&#23545;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#26377;&#29992;&#30340;&#24341;&#29702;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20960;&#20010;&#20195;&#34920;&#24615;&#31995;&#32479;&#30340;&#25913;&#36827;&#65292;&#24182;&#35299;&#20915;&#20102;&#20108;&#21313;&#24180;&#26469;&#20219;&#20309;&#31995;&#32479;&#37117;&#26410;&#33021;&#35299;&#20915;&#30340;&#38590;&#39064;&#12290;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#31616;&#21270;&#25512;&#26029;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#22823;&#22823;&#31616;&#21270;&#20102;&#35774;&#32622;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25235;&#20303;&#24341;&#29702;&#30340;&#26412;&#36136;&#21450;&#20854;&#22312;&#35777;&#26126;&#25628;&#32034;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noting that lemmas are a key feature of mathematics, we engage in an investigation of the role of lemmas in automated theorem proving. The paper describes experiments with a combined system involving learning technology that generates useful lemmas for automated theorem provers, demonstrating improvement for several representative systems and solving a hard problem not solved by any system for twenty years. By focusing on condensed detachment problems we simplify the setting considerably, allowing us to get at the essence of lemmas and their role in proof search.
&lt;/p&gt;</description></item><item><title>MenuCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#19982;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.04496</link><description>&lt;p&gt;
MenuCraft: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#33756;&#21333;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
MenuCraft: Interactive Menu System Design with Large Language Models. (arXiv:2303.04496v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04496
&lt;/p&gt;
&lt;p&gt;
MenuCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#19982;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33756;&#21333;&#31995;&#32479;&#35774;&#35745;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#35768;&#22810;&#35774;&#35745;&#36873;&#39033;&#21644;&#21508;&#31181;&#20154;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MenuCraft&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#32454;&#21270;&#33756;&#21333;&#31995;&#32479;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#23454;&#29616;&#35774;&#35745;&#24072;&#19982;&#23545;&#35805;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;MenuCraft&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#35774;&#35745;&#36873;&#39033;&#30340;&#36731;&#26494;&#23450;&#21046;&#12290;MenuCraft&#36890;&#36807;&#23545;&#35805;&#25903;&#25345;&#21508;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Menu system design is a challenging task involving many design options and various human factors. For example, one crucial factor that designers need to consider is the semantic and systematic relation of menu commands. However, capturing these relations can be challenging due to limited available resources. With the advancement of neural language models, large language models can utilize their vast pre-existing knowledge in designing and refining menu systems. In this paper, we propose MenuCraft, an AI-assisted designer for menu design that enables collaboration between the designer and a dialogue system to design menus. MenuCraft offers an interactive language-based menu design tool that simplifies the menu design process and enables easy customization of design options. MenuCraft supports a variety of interactions through dialog that allows performing zero/few-shot learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;3D&#28857;&#20113;&#20013;&#36827;&#34892;&#26080;&#38480;&#25968;&#37327;&#25903;&#25745;&#26816;&#27979;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#26469;&#21033;&#29992;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#35774;&#32622;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02401</link><description>&lt;p&gt;
&#22312;3D&#28857;&#20113;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;3D&#28857;&#20113;&#20013;&#36827;&#34892;&#26080;&#38480;&#25968;&#37327;&#25903;&#25745;&#26816;&#27979;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#26469;&#21033;&#29992;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#35774;&#32622;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25745;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#23616;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#25903;&#25745;&#26631;&#31614;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#65288;OpenAD&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#28857;&#20113;&#20013;&#26816;&#27979;&#26080;&#38480;&#25968;&#37327;&#30340;&#25903;&#25745;&#12290;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#65292;OpenAD&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#24182;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#20309;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#25903;&#25745;&#26816;&#27979;&#35774;&#32622;&#19978;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;OpenAD&#22312;&#23454;&#38469;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affordance detection is a challenging problem with a wide variety of robotic applications. Traditional affordance detection methods are limited to a predefined set of affordance labels, hence potentially restricting the adaptability of intelligent robots in complex and dynamic environments. In this paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method, which is capable of detecting an unbounded number of affordances in 3D point clouds. By simultaneously learning the affordance text and the point feature, OpenAD successfully exploits the semantic relationships between affordances. Therefore, our proposed method enables zero-shot detection and can be able to detect previously unseen affordances without a single annotation example. Intensive experimental results show that OpenAD works effectively on a wide range of affordance detection setups and outperforms other baselines by a large margin. Additionally, we demonstrate the practicality of the proposed OpenAD in real
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.13757</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#25928;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Toward Efficient Gradient-Based Value Estimation. (arXiv:2301.13757v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#23450;&#24615;&#65292;&#20294;&#36890;&#24120;&#27604;&#26102;&#38388;&#24046;&#24322;&#65288;TD&#65289;&#23398;&#20064;&#26041;&#27861;&#24930;&#24471;&#22810;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#32531;&#24930;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#34920;&#26126;&#22343;&#26041;&#36125;&#23572;&#26364;&#35823;&#24046;&#65288;MSBE&#65289;&#26159;&#19968;&#31181;&#30149;&#24577;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#40657;&#22622;&#30697;&#38453;&#20855;&#26377;&#36739;&#22823;&#30340;&#26465;&#20214;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;MSBE&#30340;&#19981;&#33391;&#26465;&#20214;&#23545;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#26080;&#25209;&#22788;&#29702;&#36817;&#31471;&#26041;&#27861;&#65292;&#23427;&#36817;&#20284;&#36981;&#24490;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;&#65292;&#24182;&#22312;&#21442;&#25968;&#21270;&#26041;&#38754;&#28176;&#36817;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#31639;&#27861;&#31216;&#20026;RANS&#65292;&#23427;&#22312;&#25928;&#29575;&#19978;&#27604;&#21097;&#20313;&#26799;&#24230;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#32463;&#20856;&#38382;&#39064;&#19978;&#19982;TD&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based methods for value estimation in reinforcement learning have favorable stability properties, but they are typically much slower than Temporal Difference (TD) learning methods. We study the root causes of this slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned loss function in the sense that its Hessian has large condition-number. To resolve the adverse effect of poor conditioning of MSBE on gradient based methods, we propose a low complexity batch-free proximal method that approximately follows the Gauss-Newton direction and is asymptotically robust to parameterization. Our main algorithm, called RANS, is efficient in the sense that it is significantly faster than the residual gradient methods while having almost the same computational complexity, and is competitive with TD on the classic problems that we tested.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectGPT&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#26354;&#29575;&#26469;&#21028;&#26029;&#25991;&#26412;&#26159;&#21542;&#30001;&#19968;&#20010;&#32473;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#31867;&#22120;&#12289;&#25910;&#38598;&#25968;&#25454;&#38598;&#25110;&#26126;&#30830;&#21152;&#27700;&#21360;&#65292;&#21482;&#20351;&#29992;&#27169;&#22411;&#35745;&#31639;&#30340;&#23545;&#25968;&#27010;&#29575;&#21644;&#21478;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#25200;&#21160;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DetectGPT&#22312;&#27169;&#22411;&#37319;&#26679;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#26356;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11305</link><description>&lt;p&gt;
DetectGPT&#65306;&#20351;&#29992;&#27010;&#29575;&#26354;&#29575;&#36827;&#34892;&#38646;&#26679;&#26412;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. (arXiv:2301.11305v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectGPT&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#26354;&#29575;&#26469;&#21028;&#26029;&#25991;&#26412;&#26159;&#21542;&#30001;&#19968;&#20010;&#32473;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#31867;&#22120;&#12289;&#25910;&#38598;&#25968;&#25454;&#38598;&#25110;&#26126;&#30830;&#21152;&#27700;&#21360;&#65292;&#21482;&#20351;&#29992;&#27169;&#22411;&#35745;&#31639;&#30340;&#23545;&#25968;&#27010;&#29575;&#21644;&#21478;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#25200;&#21160;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DetectGPT&#22312;&#27169;&#22411;&#37319;&#26679;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#26356;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27969;&#30021;&#24230;&#21644;&#24191;&#27867;&#20351;&#29992;&#31361;&#26174;&#20102;&#24076;&#26395;&#26377;&#30456;&#24212;&#30340;&#24037;&#20855;&#26469;&#24110;&#21161;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LLM&#27010;&#29575;&#20989;&#25968;&#32467;&#26500;&#30340;&#19968;&#20010;&#26377;&#29992;&#23646;&#24615;&#65292;&#23545;&#20110;&#36825;&#31181;&#26816;&#27979;&#38750;&#24120;&#26377;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20174;LLM&#20013;&#37319;&#26679;&#30340;&#25991;&#26412;&#20542;&#21521;&#20110;&#21344;&#25454;&#27169;&#22411;&#30340;&#23545;&#25968;&#27010;&#29575;&#20989;&#25968;&#30340;&#36127;&#26354;&#29575;&#21306;&#22495;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26354;&#29575;&#30340;&#20934;&#21017;&#65292;&#29992;&#20110;&#21028;&#26029;&#19968;&#20010;&#27573;&#33853;&#26159;&#21542;&#26159;&#30001;&#32473;&#23450;&#30340;LLM&#29983;&#25104;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#31216;&#20026;DetectGPT&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#20998;&#31867;&#22120;&#65292;&#25910;&#38598;&#30495;&#23454;&#25110;&#29983;&#25104;&#27573;&#33853;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#19981;&#38656;&#35201;&#26126;&#30830;&#22320;&#32473;&#29983;&#25104;&#30340;&#25991;&#26412;&#21152;&#27700;&#21360;&#12290;&#23427;&#21482;&#20351;&#29992;&#25152;&#20851;&#27880;&#27169;&#22411;&#35745;&#31639;&#30340;&#23545;&#25968;&#27010;&#29575;&#21644;&#26469;&#33258;&#21478;&#19968;&#20010;&#36890;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;T5&#65289;&#30340;&#27573;&#33853;&#30340;&#38543;&#26426;&#25200;&#21160;&#12290;&#25105;&#20204;&#21457;&#29616;DetectGPT&#27604;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#26356;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#65292;&#29992;&#20110;&#27169;&#22411;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model samp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#24494;&#23567;&#30340;&#31639;&#27861;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2212.14214</link><description>&lt;p&gt;
&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Backward Curriculum Reinforcement Learning. (arXiv:2212.14214v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#24494;&#23567;&#30340;&#31639;&#27861;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#21069;&#21521;&#29983;&#25104;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#30340;&#25351;&#23548;&#19981;&#36275;&#20197;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#23613;&#21487;&#33021;&#22810;&#30340;&#25506;&#32034;&#12290;&#23613;&#31649;&#25105;&#20204;&#35748;&#35782;&#21040;&#24378;&#21270;&#23398;&#20064;&#32467;&#26524;&#26469;&#33258;&#20805;&#20998;&#30340;&#25506;&#32034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#23384;&#22312;&#25240;&#34935;&#65292;&#36825;&#26159;&#24433;&#21709;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21644;&#32593;&#32476;&#32467;&#26500;&#20462;&#25913;&#26469;&#22686;&#21152;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#24456;&#22810;&#27493;&#39588;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21521;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#22238;&#25918;&#36712;&#36857;&#32780;&#19981;&#26159;&#21407;&#22987;&#30340;&#21069;&#21521;&#36712;&#36857;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#35757;&#32451;&#20043;&#21069;&#23545;&#36712;&#36857;&#30340;&#39034;&#24207;&#36827;&#34892;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#20351;&#24471;&#23454;&#29616;&#36215;&#26469;&#26356;&#21152;&#30452;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current reinforcement learning algorithms train an agent using forward-generated trajectories, which provide little guidance so that the agent can explore as much as possible. While realizing the value of reinforcement learning results from sufficient exploration, this approach leads to a trade-off in losing sample efficiency, an essential factor impacting algorithm performance. Previous tasks use reward-shaping techniques and network structure modification to increase sample efficiency. However, these methods require many steps to implement. In this work, we propose novel backward curriculum reinforcement learning that begins training the agent using the backward trajectory of the episode instead of the original forward trajectory. This approach provides the agent with a strong reward signal, enabling more sample-efficient learning. Moreover, our method only requires a minor change in the algorithm of reversing the order of the trajectory before agent training, allowing a straightforw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#22810;&#27169;&#24577;&#19977;&#31181;&#22270;&#31867;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2212.05767</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31867;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#32508;&#36848;&#65306;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#22810;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
A Survey of Knowledge Graph Reasoning on Graph Types: Static, Dynamic, and Multimodal. (arXiv:2212.05767v7 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#22810;&#27169;&#24577;&#19977;&#31181;&#22270;&#31867;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65288;KGR&#65289;&#26088;&#22312;&#26681;&#25454;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#25512;&#26029;&#20986;&#26032;&#30340;&#20107;&#23454;&#65292;&#24050;&#25104;&#20026;&#24555;&#36895;&#22686;&#38271;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26497;&#22823;&#22320;&#26377;&#30410;&#65292;&#22914;&#38382;&#39064;&#22238;&#31572;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#12290;&#26681;&#25454;&#22270;&#31867;&#22411;&#65292;&#29616;&#26377;&#30340;KGR&#27169;&#22411;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#19977;&#31867;&#65292;&#21363;&#38745;&#24577;&#27169;&#22411;&#12289;&#26102;&#24577;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#35813;&#39046;&#22495;&#30340;&#26089;&#26399;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;KGR&#19978;&#65292;&#32780;&#26368;&#36817;&#30340;&#24037;&#20316;&#23581;&#35797;&#21033;&#29992;&#26356;&#23454;&#38469;&#21644;&#26356;&#25509;&#36817;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#24577;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26080;&#32508;&#21512;&#24635;&#32467;&#21644;&#35752;&#35770;&#36825;&#19968;&#37325;&#35201;&#26041;&#21521;&#20013;&#30340;&#27169;&#22411;&#30340;&#35843;&#26597;&#35770;&#25991;&#21644;&#24320;&#28304;&#23384;&#20648;&#24211;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#38024;&#23545;&#20174;&#38745;&#24577;&#21040;&#26102;&#24577;&#20877;&#21040;&#22810;&#27169;&#24577;KG&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#22522;&#20110;&#21452;&#23618;&#20998;&#31867;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph reasoning (KGR), aiming to deduce new facts from existing facts based on mined logic rules underlying knowledge graphs (KGs), has become a fast-growing research direction. It has been proven to significantly benefit the usage of KGs in many AI applications, such as question answering, recommendation systems, and etc. According to the graph types, existing KGR models can be roughly divided into three categories, i.e., static models, temporal models, and multi-modal models. Early works in this domain mainly focus on static KGR, and recent works try to leverage the temporal and multi-modal information, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for knowledge graph reasoning tracing from static to temporal and then to multi-modal KGs. Concretely, the models are reviewed based on bi-level taxonomy,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#36827;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#22823;&#23398;&#35838;&#31243;&#20998;&#37197;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20462;&#22797;&#31639;&#27861;&#21644;&#25913;&#36827;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29983;&#25104;&#20102;&#26368;&#20339;&#30340;&#35838;&#31243;&#20998;&#37197;&#26041;&#26696;&#65292;&#21516;&#26102;&#28385;&#36275;&#21508;&#31181;&#32422;&#26463;&#65292;&#25552;&#39640;&#26102;&#38388;&#25928;&#29575;&#24182;&#20943;&#23569;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2212.02230</link><description>&lt;p&gt;
&#19968;&#20010;&#28151;&#21512;&#36827;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#23398;&#35838;&#31243;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Evolutionary Approach to Solve University Course Allocation Problem. (arXiv:2212.02230v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#36827;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#22823;&#23398;&#35838;&#31243;&#20998;&#37197;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20462;&#22797;&#31639;&#27861;&#21644;&#25913;&#36827;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29983;&#25104;&#20102;&#26368;&#20339;&#30340;&#35838;&#31243;&#20998;&#37197;&#26041;&#26696;&#65292;&#21516;&#26102;&#28385;&#36275;&#21508;&#31181;&#32422;&#26463;&#65292;&#25552;&#39640;&#26102;&#38388;&#25928;&#29575;&#24182;&#20943;&#23569;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#23398;&#35838;&#31243;&#20998;&#37197;&#38382;&#39064;&#30340;&#21508;&#31181;&#32422;&#26463;&#12289;&#22256;&#38590;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36827;&#21270;&#31639;&#27861;&#65292;&#23558;&#23616;&#37096;&#20462;&#22797;&#31639;&#27861;&#21644;&#25913;&#36827;&#30340;&#36951;&#20256;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#29983;&#25104;&#26368;&#20339;&#30340;&#35838;&#31243;&#20998;&#37197;&#26041;&#26696;&#12290;&#22312;&#20998;&#26512;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#38598;&#21518;&#65292;&#21046;&#23450;&#20102;&#25152;&#26377;&#24517;&#35201;&#30340;&#32422;&#26463;&#12290;&#36825;&#20123;&#32422;&#26463;&#22312;&#32534;&#25490;&#27599;&#20301;&#25945;&#24072;&#30340;&#26080;&#20914;&#31361;&#21644;&#39640;&#25928;&#30340;&#29677;&#32423;&#35838;&#34920;&#26102;&#38656;&#35201;&#32771;&#34385;&#21040;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#20248;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#21516;&#26102;&#20445;&#25345;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#25163;&#21160;&#22788;&#29702;&#36825;&#39033;&#20219;&#21153;&#30340;&#24037;&#20316;&#37327;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#19968;&#20123;&#22522;&#20934;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#26174;&#31034;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#26041;&#38754;&#30340;&#26356;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses various types of constraints, difficulties and solutions to overcome the challenges regarding university course allocation problem. A hybrid evolutionary algorithm has been defined combining Local Repair Algorithm and Modified Genetic Algorithm to generate the best course assignment. After analyzing the collected dataset, all the necessary constraints were formulated. These constraints manage to cover the aspects needed to be kept in mind while preparing clash free and efficient class schedules for every faculty member. The goal is to generate an optimized solution which will fulfill those constraints while maintaining time efficiency and also reduce the workload of handling this task manually. The proposed algorithm was compared with some base level optimization algorithms to show the better efficiency in terms of accuracy and time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34892;&#20026;&#32422;&#26463;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#20248;&#21270;&#22256;&#38590;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#26500;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.15956</link><description>&lt;p&gt;
&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Closed-Form Policy Improvement Operators. (arXiv:2211.15956v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34892;&#20026;&#32422;&#26463;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#20248;&#21270;&#22256;&#38590;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#26500;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34892;&#20026;&#32422;&#26463;&#33258;&#28982;&#22320;&#28608;&#21169;&#20102;&#20351;&#29992;&#19968;&#38454;&#27888;&#21202;&#36817;&#20284;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#31574;&#30053;&#30446;&#26631;&#30340;&#32447;&#24615;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#24322;&#26500;&#31574;&#30053;&#25910;&#38598;&#32780;&#26469;&#65292;&#25105;&#20204;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#24341;&#36215;&#20248;&#21270;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#38381;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#26469;&#23454;&#20363;&#21270;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate offline RL algorithms with our novel policy improvement operators and empirically demonstrate their e
&lt;/p&gt;</description></item><item><title>FsaNet&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#39057;&#27573;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36793;&#32536;&#30340;&#21516;&#26102;&#20419;&#36827;&#23545;&#35937;&#20869;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#65292;&#20302;&#39057;&#33258;&#27880;&#24847;&#21147;&#20063;&#21487;&#20197;&#36798;&#21040;&#25509;&#36817;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39057;&#29575;&#33258;&#27880;&#24847;&#21147;&#36824;&#31616;&#21270;&#20102;&#20196;&#29260;&#26144;&#23556;&#21644;&#20196;&#29260;&#28151;&#21512;&#38454;&#27573;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15595</link><description>&lt;p&gt;
FsaNet: &#39057;&#29575;&#33258;&#27880;&#24847;&#21147;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
FsaNet: Frequency Self-attention for Semantic Segmentation. (arXiv:2211.15595v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15595
&lt;/p&gt;
&lt;p&gt;
FsaNet&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#39057;&#27573;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36793;&#32536;&#30340;&#21516;&#26102;&#20419;&#36827;&#23545;&#35937;&#20869;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#65292;&#20302;&#39057;&#33258;&#27880;&#24847;&#21147;&#20063;&#21487;&#20197;&#36798;&#21040;&#25509;&#36817;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39057;&#29575;&#33258;&#27880;&#24847;&#21147;&#36824;&#31616;&#21270;&#20102;&#20196;&#29260;&#26144;&#23556;&#21644;&#20196;&#29260;&#28151;&#21512;&#38454;&#27573;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#22270;&#20687;&#30340;&#39057;&#35889;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#39640;&#24230;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20445;&#30041;&#36793;&#32536;&#24182;&#20419;&#36827;&#23545;&#35937;&#20869;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#19981;&#21516;&#39057;&#27573;&#19978;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20165;&#22312;&#20302;&#39057;&#20998;&#37327;&#19978;&#36827;&#34892;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#20302;&#39057;&#33258;&#27880;&#24847;&#21147;&#20063;&#21487;&#20197;&#36798;&#21040;&#38750;&#24120;&#25509;&#36817;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23884;&#20837;&#20102;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#21040;CNN&#32593;&#32476;&#30340;&#22836;&#37096;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;FsaNet&#12290;&#39057;&#29575;&#33258;&#27880;&#24847;&#21147;1&#65289;&#20165;&#38656;&#35201;&#20960;&#20010;&#20302;&#39057;&#31995;&#25968;&#20316;&#20026;&#36755;&#20837;&#65292;2&#65289;&#22312;&#25968;&#23398;&#19978;&#21487;&#20197;&#31561;&#25928;&#20110;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#30340;&#31354;&#38388;&#22495;&#33258;&#27880;&#24847;&#21147;&#65292;3&#65289;&#21516;&#26102;&#31616;&#21270;&#20102;&#20196;&#29260;&#26144;&#23556;&#65288;$1\times1$&#21367;&#31215;&#65289;&#38454;&#27573;&#21644;&#20196;&#29260;&#28151;&#21512;&#38454;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39057;&#29575;&#33258;&#27880;&#24847;&#21147;&#21482;&#38656;&#35201;87
&lt;/p&gt;
&lt;p&gt;
Considering the spectral properties of images, we propose a new self-attention mechanism with highly reduced computational complexity, up to a linear rate. To better preserve edges while promoting similarity within objects, we propose individualized processes over different frequency bands. In particular, we study a case where the process is merely over low-frequency components. By ablation study, we show that low frequency self-attention can achieve very close or better performance relative to full frequency even without retraining the network. Accordingly, we design and embed novel plug-and-play modules to the head of a CNN network that we refer to as FsaNet. The frequency self-attention 1) requires only a few low frequency coefficients as input, 2) can be mathematically equivalent to spatial domain self-attention with linear structures, 3) simplifies token mapping ($1\times1$ convolution) stage and token mixing stage simultaneously. We show that frequency self-attention requires $87
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#24322;&#26041;&#24046;&#20998;&#24067;&#19978;&#36827;&#34892;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#22833;&#36133;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24494;&#35843;&#26469;&#20943;&#36731;&#36825;&#31181;&#22833;&#36133;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00928</link><description>&lt;p&gt;
&#24322;&#26041;&#24046;&#20998;&#24067;&#19978;&#30340;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Active Learning on Heteroskedastic Distributions. (arXiv:2211.00928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#24322;&#26041;&#24046;&#20998;&#24067;&#19978;&#36827;&#34892;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#22833;&#36133;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24494;&#35843;&#26469;&#20943;&#36731;&#36825;&#31181;&#22833;&#36133;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20027;&#21160;&#23547;&#25214;&#26368;&#20339;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#27169;&#22411;&#25215;&#35834;&#30528;&#26356;&#20934;&#30830;&#12289;&#36866;&#24212;&#24615;&#24378;&#21644;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#20542;&#21521;&#20110;&#36873;&#25321;&#26368;&#38590;&#20998;&#31867;&#30340;&#20363;&#23376;&#12290;&#23613;&#31649;&#36825;&#22312;&#21516;&#36136;&#25968;&#25454;&#38598;&#19978;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#26631;&#31614;&#22122;&#22768;&#25110;&#24322;&#26041;&#24046;&#24615;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#36825;&#20123;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#24378;&#28872;&#20542;&#21521;&#20110;&#20174;&#22122;&#22768;&#26356;&#22823;&#30340;&#20998;&#24067;&#20013;&#36873;&#25321;&#65292;&#21363;&#20351;&#36825;&#20123;&#20363;&#23376;&#27809;&#26377;&#20449;&#24687;&#32467;&#26500;&#65288;&#20363;&#22914;&#20855;&#26377;&#38543;&#26426;&#26631;&#31614;&#30340;&#32431;&#33394;&#22270;&#20687;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#24322;&#26041;&#24046;&#20998;&#24067;&#19978;&#30340;&#28798;&#38590;&#24615;&#22833;&#36133;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#24341;&#20837;&#20102;&#27169;&#22411;&#24046;&#24322;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#21435;&#38500;&#22122;&#22768;&#20363;&#23376;&#24182;&#37319;&#26679;&#28165;&#26224;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models that can actively seek out the best quality training data hold the promise of more accurate, adaptable, and efficient machine learning. Active learning techniques often tend to prefer examples that are the most difficult to classify. While this works well on homogeneous datasets, we find that it can lead to catastrophic failures when performed on multiple distributions with different degrees of label noise or heteroskedasticity. These active learning algorithms strongly prefer to draw from the distribution with more noise, even if their examples have no informative structure (such as solid color images with random labels). To this end, we demonstrate the catastrophic failure of these active learning algorithms on heteroskedastic distributions and propose a fine-tuning-based approach to mitigate these failures. Further, we propose a new algorithm that incorporates a model difference scoring function for each data point to filter out the noisy examples and sample clean examples th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31867;&#22686;&#37327;NER&#20013;&#30340;&#38544;&#34255;&#23454;&#20307;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#23454;&#20307;&#31867;&#21035;&#21644;"O"&#36827;&#34892;&#21028;&#21035;&#24335;&#34920;&#31034;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;NER&#27169;&#22411;&#23545;&#20110;&#26032;&#26087;&#31867;&#21035;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.04676</link><description>&lt;p&gt;
&#23398;&#20064;"O"&#26377;&#21161;&#20110;&#26356;&#22810;&#30340;&#23398;&#20064;&#65306;&#35299;&#20915;&#31867;&#22686;&#37327;NER&#30340;&#38544;&#34255;&#23454;&#20307;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning "O" Helps for Learning More: Handling the Concealed Entity Problem for Class-incremental NER. (arXiv:2210.04676v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31867;&#22686;&#37327;NER&#20013;&#30340;&#38544;&#34255;&#23454;&#20307;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#23454;&#20307;&#31867;&#21035;&#21644;"O"&#36827;&#34892;&#21028;&#21035;&#24335;&#34920;&#31034;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;NER&#27169;&#22411;&#23545;&#20110;&#26032;&#26087;&#31867;&#21035;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21629;&#21517;&#23454;&#20307;&#30340;&#31867;&#21035;&#36805;&#36895;&#22686;&#21152;&#65292;&#37096;&#32626;&#30340;NER&#27169;&#22411;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#20197;&#35782;&#21035;&#26356;&#22810;&#30340;&#23454;&#20307;&#31867;&#22411;&#65292;&#36825;&#23601;&#38656;&#35201;&#23545;NER&#36827;&#34892;&#31867;&#22686;&#37327;&#23398;&#20064;&#12290;&#32771;&#34385;&#21040;&#38544;&#31169;&#38382;&#39064;&#21644;&#23384;&#20648;&#32422;&#26463;&#65292;&#31867;&#22686;&#37327;NER&#30340;&#26631;&#20934;&#33539;&#24335;&#20165;&#20351;&#29992;&#24102;&#26377;&#26032;&#31867;&#21035;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#65292;&#32780;&#26469;&#33258;&#20854;&#20182;&#23454;&#20307;&#31867;&#21035;&#30340;&#23454;&#20307;&#34987;&#26631;&#35760;&#20026;"&#38750;&#23454;&#20307;"&#65288;&#25110;"O"&#65289;&#12290;&#26412;&#25991;&#23545;"&#26410;&#26631;&#35760;&#23454;&#20307;&#38382;&#39064;"&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#23427;&#23548;&#33268;"O"&#21644;&#23454;&#20307;&#20043;&#38388;&#20005;&#37325;&#28151;&#28102;&#65292;&#38477;&#20302;&#20102;&#26087;&#31867;&#21035;&#30340;&#20998;&#31867;&#33021;&#21147;&#65292;&#24182;&#38477;&#20302;&#20102;&#27169;&#22411;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#26410;&#26631;&#35760;&#23454;&#20307;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#23454;&#20307;&#31867;&#21035;&#21644;"O"&#30340;&#21028;&#21035;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20307;&#24863;&#30693;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;representation learning&#26041;&#38754;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the categories of named entities rapidly increase, the deployed NER models are required to keep updating toward recognizing more entity types, creating a demand for class-incremental learning for NER. Considering the privacy concerns and storage constraints, the standard paradigm for class-incremental NER updates the models with training data only annotated with the new classes, yet the entities from other entity classes are unlabeled, regarded as "Non-entity" (or "O"). In this work, we conduct an empirical study on the "Unlabeled Entity Problem" and find that it leads to severe confusion between "O" and entities, decreasing class discrimination of old classes and declining the model's ability to learn new classes. To solve the Unlabeled Entity Problem, we propose a novel representation learning method to learn discriminative representations for the entity classes and "O". Specifically, we propose an entity-aware contrastive learning method that adaptively detects entity clusters in
&lt;/p&gt;</description></item><item><title>GMA3D&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#26694;&#26550;&#30340;&#27169;&#22359;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#22330;&#26223;&#27969;&#21160;&#20013;&#30340;&#36974;&#25377;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#21033;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#35821;&#20041;&#30456;&#20284;&#24615;&#25512;&#26029;&#34987;&#36974;&#25377;&#28857;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20559;&#31227;&#32858;&#21512;&#22120;&#36827;&#34892;&#27719;&#32858;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GMA3D&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#22330;&#26223;&#27969;&#21160;&#20013;&#30340;&#36974;&#25377;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.03296</link><description>&lt;p&gt;
GMA3D: &#26412;&#22320;-&#20840;&#23616;&#27880;&#24847;&#21147;&#23398;&#20064;&#29992;&#20110;&#20272;&#35745;&#34987;&#36974;&#25377;&#30340;&#22330;&#26223;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
GMA3D: Local-Global Attention Learning to Estimate Occluded Motions of Scene Flow. (arXiv:2210.03296v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03296
&lt;/p&gt;
&lt;p&gt;
GMA3D&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#26694;&#26550;&#30340;&#27169;&#22359;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#22330;&#26223;&#27969;&#21160;&#20013;&#30340;&#36974;&#25377;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#21033;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#35821;&#20041;&#30456;&#20284;&#24615;&#25512;&#26029;&#34987;&#36974;&#25377;&#28857;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20559;&#31227;&#32858;&#21512;&#22120;&#36827;&#34892;&#27719;&#32858;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GMA3D&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#22330;&#26223;&#27969;&#21160;&#20013;&#30340;&#36974;&#25377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#21160;&#34920;&#31034;3D&#28857;&#20113;&#20013;&#27599;&#20010;&#28857;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;&#23427;&#26159;&#24212;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#65288;&#22914;&#36816;&#21160;&#20998;&#21106;&#21644;&#29289;&#20307;&#36319;&#36394;&#65289;&#30340;&#37325;&#35201;&#19979;&#28216;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#20004;&#20010;&#36830;&#32493;&#30340;&#28857;&#20113;&#20043;&#38388;&#24635;&#20250;&#23384;&#22312;&#36974;&#25377;&#28857;&#65292;&#26080;&#35770;&#26159;&#26469;&#33258;&#31232;&#30095;&#25968;&#25454;&#37319;&#26679;&#36824;&#26159;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#36974;&#25377;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35299;&#20915;&#22330;&#26223;&#27969;&#21160;&#20013;&#30340;&#36974;&#25377;&#38382;&#39064;&#65292;&#36890;&#36807;&#36816;&#21160;&#29289;&#20307;&#30340;&#35821;&#20041;&#33258;&#30456;&#20284;&#24615;&#21644;&#36816;&#21160;&#19968;&#33268;&#24615;&#26469;&#25512;&#26029;&#34987;&#36974;&#25377;&#28857;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#26694;&#26550;&#30340;GMA3D&#27169;&#22359;&#65292;&#23427;&#21033;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#35821;&#20041;&#30456;&#20284;&#24615;&#20998;&#21035;&#20174;&#38750;&#36974;&#25377;&#28857;&#30340;&#36816;&#21160;&#20449;&#24687;&#20013;&#25512;&#26029;&#20986;&#34987;&#36974;&#25377;&#28857;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#28982;&#21518;&#20351;&#29992;&#20559;&#31227;&#32858;&#21512;&#22120;&#36827;&#34892;&#27719;&#32858;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#26159;&#31532;&#19968;&#20010;&#22312;&#28857;&#20113;&#19978;&#24212;&#29992;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#22330;&#26223;&#27969;&#21160;&#36974;&#25377;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;GMA3D&#21487;&#20197;&#35299;&#20915;&#22330;&#26223;&#27969;&#21160;&#20013;&#30340;&#36974;&#25377;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Scene flow represents the motion information of each point in the 3D point clouds. It is a vital downstream method applied to many tasks, such as motion segmentation and object tracking. However, there are always occlusion points between two consecutive point clouds, whether from the sparsity data sampling or real-world occlusion. In this paper, we focus on addressing occlusion issues in scene flow by the semantic self-similarity and motion consistency of the moving objects. We propose a GMA3D module based on the transformer framework, which utilizes local and global semantic similarity to infer the motion information of occluded points from the motion information of local and global non-occluded points respectively, and then uses an offset aggregator to aggregate them. Our module is the first to apply the transformer-based architecture to gauge the scene flow occlusion problem on point clouds. Experiments show that our GMA3D can solve the occlusion problem in the scene flow, especiall
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#25991;&#38024;&#23545;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#24191;&#20041;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#26088;&#22312;&#21516;&#26102;&#26377;&#25928;&#35782;&#21035;&#35757;&#32451;&#39046;&#22495;&#22270;&#21644;&#26410;&#35757;&#32451;&#39046;&#22495;&#22270;&#19978;&#30340;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2209.10168</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalizability of Graph Anomaly Detection Models via Data Augmentation. (arXiv:2209.10168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#25991;&#38024;&#23545;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#24191;&#20041;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#26088;&#22312;&#21516;&#26102;&#26377;&#25928;&#35782;&#21035;&#35757;&#32451;&#39046;&#22495;&#22270;&#21644;&#26410;&#35757;&#32451;&#39046;&#22495;&#22270;&#19978;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#21363;&#20351;&#23569;&#25968;&#24322;&#24120;&#37117;&#21487;&#33021;&#23545;&#33391;&#24615;&#29992;&#25143;&#36896;&#25104;&#24040;&#22823;&#23041;&#32961;&#12290;&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;GAD&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21487;&#29992;&#30340;&#26631;&#31614;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#27604;&#26080;&#30417;&#30563;&#26041;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20154;&#20204;&#36890;&#24120;&#38656;&#35201;&#22312;&#26032;&#30340;&#65288;&#23376;&#65289;&#22270;&#19978;&#35782;&#21035;&#24322;&#24120;&#20197;&#30830;&#20445;&#19994;&#21153;&#23433;&#20840;&#65292;&#20294;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#26631;&#31614;&#26469;&#35757;&#32451;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#30452;&#25509;&#23558;&#35757;&#32451;&#22909;&#30340;GAD&#27169;&#22411;&#24212;&#29992;&#20110;&#26032;&#30340;&#65288;&#23376;&#65289;&#22270;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;GAD&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#33391;&#22909;&#30340;&#27169;&#22411;&#22312;&#21516;&#19968;&#22270;&#30340;&#26410;&#35757;&#32451;&#21306;&#22495;&#65288;&#21363;&#35757;&#32451;&#20013;&#26080;&#27861;&#35775;&#38382;&#30340;&#21306;&#22495;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#21487;&#33021;&#20250;&#24102;&#26469;&#24456;&#22823;&#30340;&#40635;&#28902;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#24191;&#20041;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#35782;&#21035;&#35757;&#32451;&#39046;&#22495;&#22270;&#21644;&#26410;&#35757;&#32451;&#39046;&#22495;&#22270;&#19978;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) is a vital task since even a few anomalies can pose huge threats to benign users. Recent semi-supervised GAD methods, which can effectively leverage the available labels as prior knowledge, have achieved superior performances than unsupervised methods. In practice, people usually need to identify anomalies on new (sub)graphs to secure their business, but they may lack labels to train an effective detection model. One natural idea is to directly adopt a trained GAD model to the new (sub)graph for testing. However, we find that existing semi-supervised GAD methods suffer from poor generalization issue, i.e., well-trained models could not perform well on an unseen area (i.e., not accessible in training) of the same graph. It may cause great troubles. In this paper, we base on the phenomenon and propose a general and novel research problem of generalized graph anomaly detection that aims to effectively identify anomalies on both the training-domain graph and u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2209.05732</link><description>&lt;p&gt;
R\'{e}nyi&#25955;&#24230;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#8212;&#8212;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#65288;DML&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#36825;&#31181;&#20570;&#27861;&#26356;&#21152;&#28789;&#27963;&#12289;&#21487;&#35843;&#65292;&#20197;&#25913;&#21892;vanilla DML&#12290;&#36825;&#31181;&#20462;&#25913;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#38468;&#21152;&#22797;&#26434;&#24615;&#19979;&#19981;&#26029;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#33539;&#20363;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#19988;&#34920;&#26126;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#38750;&#20984;&#20248;&#21270;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#25910;&#25947;&#30340;&#20559;&#24046;&#20026;$\mathcal{O}(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;STAR-RIS&#36741;&#21161;&#32593;&#32476;&#20013;&#35206;&#30422;&#21644;&#23481;&#37327;&#20248;&#21270;&#38382;&#39064;&#30340;&#22810;&#30446;&#26631;&#36817;&#31471;&#25919;&#31574;&#20248;&#21270;&#65288;MO-PPO&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#26368;&#20248;&#35299;&#26469;&#23454;&#29616;&#38271;&#26399;&#25928;&#30410;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20256;&#32479;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.00511</link><description>&lt;p&gt;
STAR-RIS&#36741;&#21161;&#32593;&#32476;&#20013;&#30340;DRL&#20248;&#21270;&#35206;&#30422;&#21644;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
DRL Enabled Coverage and Capacity Optimization in STAR-RIS Assisted Networks. (arXiv:2209.00511v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00511
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;STAR-RIS&#36741;&#21161;&#32593;&#32476;&#20013;&#35206;&#30422;&#21644;&#23481;&#37327;&#20248;&#21270;&#38382;&#39064;&#30340;&#22810;&#30446;&#26631;&#36817;&#31471;&#25919;&#31574;&#20248;&#21270;&#65288;MO-PPO&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#26368;&#20248;&#35299;&#26469;&#23454;&#29616;&#38271;&#26399;&#25928;&#30410;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20256;&#32479;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#20256;&#36755;&#21644;&#21453;&#23556;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#65288;STAR-RIS&#65289;&#26159;&#19968;&#31181;&#26377; promising passivedevice&#65292;&#36890;&#36807;&#21516;&#26102;&#20256;&#36755;&#21644;&#21453;&#23556;&#20837;&#23556;&#20449;&#21495;&#26469;&#23454;&#29616;&#25972;&#20010;&#31354;&#38388;&#30340;&#35206;&#30422;&#12290;&#20316;&#20026;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#26032;&#33539;&#24335;&#65292;&#22914;&#20309;&#20998;&#26512;STAR-RIS&#30340;&#35206;&#30422;&#21644;&#23481;&#37327;&#24615;&#33021;&#21464;&#24471;&#24456;&#37325;&#35201;&#20294;&#20063;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;STAR-RIS&#36741;&#21161;&#32593;&#32476;&#20013;&#30340;&#35206;&#30422;&#21644;&#23481;&#37327;&#20248;&#21270;&#65288;CCO&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#36817;&#31471;&#25919;&#31574;&#20248;&#21270;&#65288;MO-PPO&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22788;&#29702;&#30340;&#26159;&#38271;&#26399;&#25928;&#30410;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20026;&#20102;&#22312;&#27599;&#20010;&#30446;&#26631;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;MO-PPO&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#32452;&#26368;&#20248;&#35299;&#26469;&#24418;&#25104;&#24085;&#32047;&#25176;&#21069;&#27839;&#65288;PF&#65289;&#65292;&#20854;&#20013;PF&#19978;&#30340;&#20219;&#20309;&#35299;&#37117;&#34987;&#35270;&#20026;&#26368;&#20248;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;MO-PPO&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26356;&#26032;&#31574;&#30053;&#65292;&#21363;&#22522;&#20110;&#21160;&#20316;&#20215;&#20540;&#30340;&#26356;&#26032;&#31574;&#30053;&#65288;AVUS&#65289;&#21644;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#26356;&#26032;&#31574;&#30053;&#65288;LFU&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs) is a promising passive device that contributes to a full-space coverage via transmitting and reflecting the incident signal simultaneously. As a new paradigm in wireless communications, how to analyze the coverage and capacity performance of STAR-RISs becomes essential but challenging. To solve the coverage and capacity optimization (CCO) problem in STAR-RIS assisted networks, a multi-objective proximal policy optimization (MO-PPO) algorithm is proposed to handle long-term benefits than conventional optimization algorithms. To strike a balance between each objective, the MO-PPO algorithm provides a set of optimal solutions to form a Pareto front (PF), where any solution on the PF is regarded as an optimal result. Moreover, in order to improve the performance of the MO-PPO algorithm, two update strategies, i.e., action-value-based update strategy (AVUS) and loss function-based update strategy (LFU
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#36328;&#27169;&#24577;&#35821;&#20041;&#22270;&#30340;&#30446;&#26631;&#23548;&#21521;&#24773;&#24863;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;&#26631;&#39064;&#21644;&#22330;&#26223;&#22270;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#31934;&#32454;&#22270;&#20687;&#20449;&#24687;&#65292;&#19982;&#25512;&#25991;&#20013;&#30340;&#26631;&#35760;&#32467;&#21512;&#24418;&#25104;&#36328;&#27169;&#24577;&#35821;&#20041;&#22270;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.09417</link><description>&lt;p&gt;
&#22522;&#20110;&#39034;&#24207;&#36328;&#27169;&#24577;&#35821;&#20041;&#22270;&#30340;&#30446;&#26631;&#23548;&#21521;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Target-oriented Sentiment Classification with Sequential Cross-modal Semantic Graph. (arXiv:2208.09417v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#36328;&#27169;&#24577;&#35821;&#20041;&#22270;&#30340;&#30446;&#26631;&#23548;&#21521;&#24773;&#24863;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;&#26631;&#39064;&#21644;&#22330;&#26223;&#22270;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#31934;&#32454;&#22270;&#20687;&#20449;&#24687;&#65292;&#19982;&#25512;&#25991;&#20013;&#30340;&#26631;&#35760;&#32467;&#21512;&#24418;&#25104;&#36328;&#27169;&#24577;&#35821;&#20041;&#22270;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;(MABSC)&#26159;&#19968;&#31181;&#23558;&#21477;&#23376;&#21644;&#22270;&#20687;&#20013;&#25552;&#21040;&#30340;&#30446;&#26631;&#23454;&#20307;&#30340;&#24773;&#24863;&#36827;&#34892;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#31934;&#32454;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#23548;&#33268;&#20102;&#23545;&#31934;&#32454;&#22270;&#20687;&#26041;&#38754;&#21644;&#24847;&#35265;&#30340;&#26377;&#38480;&#35782;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeqCSG&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39034;&#24207;&#36328;&#27169;&#24577;&#35821;&#20041;&#22270;&#22686;&#24378;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#24773;&#24863;&#20998;&#31867;&#26694;&#26550;&#12290;SeqCSG&#21033;&#29992;&#22270;&#20687;&#26631;&#39064;&#21644;&#22330;&#26223;&#22270;&#25552;&#21462;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#31934;&#32454;&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#25512;&#25991;&#20013;&#30340;&#26631;&#35760;&#19968;&#36215;&#20316;&#20026;&#36328;&#27169;&#24577;&#35821;&#20041;&#22270;&#30340;&#20803;&#32032;&#12290;&#39034;&#24207;&#36328;&#27169;&#24577;&#35821;&#20041;&#22270;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#24207;&#21015;&#65292;&#20854;&#20013;&#22810;&#27169;&#24577;&#37051;&#25509;&#30697;&#38453;&#25351;&#31034;&#20803;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal aspect-based sentiment classification (MABSC) is task of classifying the sentiment of a target entity mentioned in a sentence and an image. However, previous methods failed to account for the fine-grained semantic association between the image and the text, which resulted in limited identification of fine-grained image aspects and opinions. To address these limitations, in this paper we propose a new approach called SeqCSG, which enhances the encoder-decoder sentiment classification framework using sequential cross-modal semantic graphs. SeqCSG utilizes image captions and scene graphs to extract both global and local fine-grained image information and considers them as elements of the cross-modal semantic graph along with tokens from tweets. The sequential cross-modal semantic graph is represented as a sequence with a multi-modal adjacency matrix indicating relationships between elements. Experimental results show that the approach outperforms existing methods and achieves 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;700,000&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#19988;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2206.02909</link><description>&lt;p&gt;
&#20351;&#29992;70&#19975;&#20154;&#26085;&#30340;&#21487;&#31359;&#25140;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data. (arXiv:2206.02909v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;700,000&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#19988;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#38754;&#30340;&#36827;&#23637;&#30456;&#23545;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;UK-Biobank&#27963;&#21160;&#36861;&#36394;&#22120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;70&#19975;&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;F1&#30456;&#23545;&#25913;&#21892;2.5%-100%&#65288;&#20013;&#20301;&#25968;18.4%&#65289;&#65292;&#25913;&#36827;&#26368;&#22823;&#30340;&#26159;&#35268;&#27169;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#27867;&#21270;&#21040;&#22806;&#37096;&#25968;&#25454;&#38598;&#12289;&#35774;&#22791;&#21644;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#24320;&#28304;&#27169;&#22411;&#23558;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#26500;&#24314;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in deep learning for human activity recognition have been relatively limited due to the lack of large labelled datasets. In this study, we leverage self-supervised learning techniques on the UK-Biobank activity tracker dataset--the largest of its kind to date--containing more than 700,000 person-days of unlabelled wearable sensor data. Our resulting activity recognition model consistently outperformed strong baselines across seven benchmark datasets, with an F1 relative improvement of 2.5%-100% (median 18.4%), the largest improvements occurring in the smaller datasets. In contrast to previous studies, our results generalise across external datasets, devices, and environments. Our open-source model will help researchers and developers to build customisable and generalisable activity classifiers with high performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#25351;&#23548;&#30340;&#20998;&#23618;&#22870;&#21169;&#26426;&#21046;&#30340;&#23398;&#20064;&#22411;&#26426;&#22120;&#20154;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#32467;&#26524;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13561</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#25351;&#23548;&#30340;&#20998;&#23618;&#22870;&#21169;&#26426;&#21046;&#29992;&#20110;&#23398;&#20064;&#22411;&#26426;&#22120;&#20154;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Physics-Guided Hierarchical Reward Mechanism for Learning-Based Robotic Grasping. (arXiv:2205.13561v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#25351;&#23548;&#30340;&#20998;&#23618;&#22870;&#21169;&#26426;&#21046;&#30340;&#23398;&#20064;&#22411;&#26426;&#22120;&#20154;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#32467;&#26524;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22411;&#25235;&#21462;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#25928;&#29575;&#21487;&#20197;&#23454;&#29616;&#22810;&#25351;&#25163;&#30340;&#23454;&#26102;&#25235;&#21462;&#36816;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#22411;&#26041;&#27861;&#38656;&#35201;&#25506;&#32034;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#20302;&#19979;&#65292;&#36825;&#19968;&#30452;&#26159;&#20854;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#21482;&#26377;&#22312;&#29289;&#20307;&#19982;&#35757;&#32451;&#29289;&#20307;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#25165;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#25351;&#23548;&#30340;&#20998;&#23618;&#22870;&#21169;&#26426;&#21046;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#35266;&#23519;&#30340;&#25235;&#21462;&#23398;&#20064;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#26469;&#20256;&#36798;&#19982;&#25163;&#37096;&#32467;&#26500;&#21644;&#29289;&#20307;&#30456;&#20851;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20998;&#23618;&#22870;&#21169;&#26426;&#21046;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#37325;&#28857;&#37096;&#20998;&#30340;&#25235;&#21462;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based grasping can afford real-time grasp motion planning of multi-fingered robotics hands thanks to its high computational efficiency. However, learning-based methods are required to explore large search spaces during the learning process. The search space causes low learning efficiency, which has been the main barrier to its practical adoption. In addition, the trained policy lacks a generalizable outcome unless objects are identical to the trained objects. In this work, we develop a novel Physics-Guided Deep Reinforcement Learning with a Hierarchical Reward Mechanism to improve learning efficiency and generalizability for learning-based autonomous grasping. Unlike conventional observation-based grasp learning, physics-informed metrics are utilized to convey correlations between features associated with hand structures and objects to improve learning efficiency and outcomes. Further, the hierarchical reward mechanism enables the robot to learn prioritized components of the g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20195;&#25968;&#20998;&#26512;&#25913;&#36827;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;GNN&#33021;&#22815;&#27604;Weisfeiler-Lehman&#65288;WL&#65289;&#31639;&#27861;&#26356;&#22909;&#22320;&#20135;&#29983;&#21306;&#20998;&#24615;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20540;&#30340;&#22270;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#31616;&#21333;&#30340;&#21367;&#31215;&#32467;&#26500;&#19982;&#26080;&#20449;&#24687;&#36755;&#20837;&#20135;&#29983;&#30340;&#31561;&#21464;&#29305;&#24449;&#27604;WL&#34920;&#31034;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.09801</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65306;&#36890;&#36807;&#20195;&#25968;&#20998;&#26512;&#25913;&#36827;&#34920;&#36798;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Representation Power of Graph Neural Networks: Improved Expressivity via Algebraic Analysis. (arXiv:2205.09801v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20195;&#25968;&#20998;&#26512;&#25913;&#36827;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;GNN&#33021;&#22815;&#27604;Weisfeiler-Lehman&#65288;WL&#65289;&#31639;&#27861;&#26356;&#22909;&#22320;&#20135;&#29983;&#21306;&#20998;&#24615;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20540;&#30340;&#22270;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#31616;&#21333;&#30340;&#21367;&#31215;&#32467;&#26500;&#19982;&#26080;&#20449;&#24687;&#36755;&#20837;&#20135;&#29983;&#30340;&#31561;&#21464;&#29305;&#24449;&#27604;WL&#34920;&#31034;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26222;&#36941;&#35748;&#20026;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#23427;&#20204;&#26368;&#22810;&#19982;Weisfeiler-Lehman&#65288;WL&#65289;&#31639;&#27861;&#19968;&#26679;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;GNN&#65288;&#21311;&#21517;&#36755;&#20837;&#65289;&#20135;&#29983;&#30340;&#34920;&#31034;&#27604;WL&#31639;&#27861;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#24037;&#20855;&#23545;GNN&#30340;&#34920;&#31034;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#26032;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;&#20854;&#19982;&#22270;&#25805;&#20316;&#31526;&#30340;&#29305;&#24449;&#20540;&#20998;&#35299;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GNN&#33021;&#22815;&#20174;&#26080;&#20449;&#24687;&#36755;&#20837;&#20135;&#29983;&#29420;&#29305;&#30340;&#36755;&#20986;&#65292;&#33267;&#23569;&#23545;&#20110;&#25152;&#26377;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20540;&#30340;&#22270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#21367;&#31215;&#32467;&#26500;&#19982;&#26080;&#20449;&#24687;&#36755;&#20837;&#20135;&#29983;&#30340;&#31561;&#21464;&#29305;&#24449;&#65292;&#23427;&#20204;&#35745;&#31639;&#22270;&#20013;&#30340;&#38381;&#21512;&#36335;&#24452;&#24182;&#19988;&#26126;&#26174;&#27604;WL&#34920;&#31034;&#20855;&#26377;&#26356;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#22270;&#21516;&#26500;&#21644;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable success of Graph Neural Networks (GNNs), the common belief is that their representation power is limited and that they are at most as expressive as the Weisfeiler-Lehman (WL) algorithm. In this paper, we argue the opposite and show that standard GNNs, with anonymous inputs, produce more discriminative representations than the WL algorithm. Our novel analysis employs linear algebraic tools and characterizes the representation power of GNNs with respect to the eigenvalue decomposition of the graph operators. We prove that GNNs are able to generate distinctive outputs from white uninformative inputs, for, at least, all graphs that have different eigenvalues. We also show that simple convolutional architectures with white inputs, produce equivariant features that count the closed paths in the graph and are provably more expressive than the WL representations. Thorough experimental analysis on graph isomorphism and graph classification datasets corroborates our theore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.03447</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21451;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#29992;&#20110;&#31561;&#20215;&#21644;&#21253;&#21547;&#20851;&#31995;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v7 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#35821;&#20041;&#32593;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#20854;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#21305;&#37197;&#35780;&#20272;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#21253;&#21547;&#20851;&#31995;&#26144;&#23556;&#30340;&#26377;&#38480;&#35780;&#20272;&#12289;&#21442;&#32771;&#26144;&#23556;&#30340;&#20122;&#20248;&#35299;&#20197;&#21450;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#28041;&#21450;&#20174;Mondo&#21644;UMLS&#20013;&#25552;&#21462;&#30340;&#26412;&#20307;&#12290;&#27599;&#20010;&#20219;&#21153;&#21253;&#25324;&#31561;&#20215;&#21644;&#21253;&#21547;&#20851;&#31995;&#21305;&#37197;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#12289;&#26412;&#20307;&#20462;&#21098;&#31561;&#26041;&#24335;&#30830;&#20445;&#21442;&#32771;&#26144;&#23556;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#38750;&#26426;&#22120;&#23398;&#20064;&#30340;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontology Matching (OM) plays an important role in many domains such as bioinformatics and the Semantic Web, and its research is becoming increasingly popular, especially with the application of machine learning (ML) techniques. Although the Ontology Alignment Evaluation Initiative (OAEI) represents an impressive effort for the systematic evaluation of OM systems, it still suffers from several limitations including limited evaluation of subsumption mappings, suboptimal reference mappings, and limited support for the evaluation of ML-based systems. To tackle these limitations, we introduce five new biomedical OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes both equivalence and subsumption matching; the quality of reference mappings is ensured by human curation, ontology pruning, etc.; and a comprehensive evaluation framework is proposed to measure OM performance from various perspectives for both ML-based and non-ML-based OM systems. We report evaluation r
&lt;/p&gt;</description></item><item><title>AdaBest&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#28418;&#31227;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;AdaBest&#25152;&#38656;&#30340;&#23384;&#20648;&#21644;&#36890;&#20449;&#24102;&#23485;&#36739;&#23569;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;AdaBest&#36890;&#36807;&#38480;&#21046;&#20272;&#35745;&#20540;&#30340;&#33539;&#25968;&#26469;&#25552;&#20379;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.13170</link><description>&lt;p&gt;
AdaBest: &#36890;&#36807;&#33258;&#36866;&#24212;&#20559;&#24046;&#20272;&#35745;&#26368;&#23567;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13170
&lt;/p&gt;
&lt;p&gt;
AdaBest&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#28418;&#31227;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;AdaBest&#25152;&#38656;&#30340;&#23384;&#20648;&#21644;&#36890;&#20449;&#24102;&#23485;&#36739;&#23569;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;AdaBest&#36890;&#36807;&#38480;&#21046;&#20272;&#35745;&#20540;&#30340;&#33539;&#25968;&#26469;&#25552;&#20379;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#23458;&#25143;&#31471;&#25110;&#35774;&#22791;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#12290;&#27169;&#22411;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#36827;&#34892;&#26412;&#22320;&#20248;&#21270;&#65292;&#28982;&#21518;&#20256;&#36755;&#21040;&#38598;&#20013;&#20013;&#24515;&#36827;&#34892;&#32858;&#21512;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#20294;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21487;&#33021;&#23548;&#33268;&#23616;&#37096;&#20248;&#21270;&#20559;&#31163;&#20840;&#23616;&#30446;&#26631;&#12290;&#20026;&#20102;&#20272;&#35745;&#21644;&#28040;&#38500;&#36825;&#31181;&#20559;&#31163;&#65292;&#36817;&#26399;&#22312;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#23458;&#25143;&#31471;&#28418;&#31227;&#36827;&#34892;&#20102;&#19981;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#26368;&#32456;&#26410;&#33021;&#27491;&#30830;&#22320;&#28040;&#38500;&#23427;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#20272;&#35745;&#23458;&#25143;&#31471;&#28418;&#31227;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#23384;&#20648;&#21644;&#36890;&#20449;&#24102;&#23485;&#65292;&#20197;&#21450;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#23458;&#25143;&#31471;&#28418;&#31227;&#20272;&#35745;&#30340;&#33539;&#25968;&#26469;&#24341;&#20837;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL), a number of clients or devices collaborate to train a model without sharing their data. Models are optimized locally at each client and further communicated to a central hub for aggregation. While FL is an appealing decentralized training paradigm, heterogeneity among data from different clients can cause the local optimization to drift away from the global objective. In order to estimate and therefore remove this drift, variance reduction techniques have been incorporated into FL optimization recently. However, these approaches inaccurately estimate the clients' drift and ultimately fail to remove it properly. In this work, we propose an adaptive algorithm that accurately estimates drift across clients. In comparison to previous works, our approach necessitates less storage and communication bandwidth, as well as lower compute costs. Additionally, our proposed methodology induces stability by constraining the norm of estimates for client drift, making it mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32423;&#32852;&#8221;&#65292;&#36890;&#36807;&#23558;&#35821;&#20041;&#26641;&#25628;&#32034;&#19982;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#25628;&#32034;&#35821;&#20041;&#26641;&#65292;&#24182;&#33021;&#22815;&#22312;&#29289;&#29702;&#27169;&#25311;&#30340;&#21160;&#24577;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#36981;&#24490;&#24178;&#39044;&#25351;&#20196;&#12289;&#25512;&#29702;&#26367;&#20195;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.01108</link><description>&lt;p&gt;
&#23398;&#20250;&#25512;&#29702;&#21644;&#23545;&#29289;&#29702;&#32423;&#32852;&#20107;&#20214;&#37319;&#21462;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning to reason about and to act on physical cascading events. (arXiv:2202.01108v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32423;&#32852;&#8221;&#65292;&#36890;&#36807;&#23558;&#35821;&#20041;&#26641;&#25628;&#32034;&#19982;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#25628;&#32034;&#35821;&#20041;&#26641;&#65292;&#24182;&#33021;&#22815;&#22312;&#29289;&#29702;&#27169;&#25311;&#30340;&#21160;&#24577;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#36981;&#24490;&#24178;&#39044;&#25351;&#20196;&#12289;&#25512;&#29702;&#26367;&#20195;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25512;&#29702;&#21644;&#19982;&#21160;&#24577;&#29615;&#22659;&#20132;&#20114;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#20294;&#24403;&#21160;&#20316;&#21487;&#20197;&#35302;&#21457;&#20132;&#20114;&#20381;&#36182;&#30340;&#32423;&#32852;&#20107;&#20214;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32423;&#32852;&#8221;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#23637;&#31034;&#20102;&#19968;&#20010;&#29289;&#29702;&#27169;&#25311;&#30340;&#21160;&#24577;&#22330;&#26223;&#35270;&#39057;&#21518;&#65292;&#34987;&#35201;&#27714;&#36827;&#34892;&#24178;&#39044;&#24182;&#35302;&#21457;&#19968;&#31995;&#21015;&#20107;&#20214;&#65292;&#20174;&#32780;&#20351;&#31995;&#32479;&#36798;&#21040;&#19968;&#20010;&#8220;&#21453;&#20107;&#23454;&#8221;&#30340;&#30446;&#26631;&#12290;&#20363;&#22914;&#65292;&#26234;&#33021;&#20307;&#21487;&#33021;&#34987;&#35201;&#27714;&#8220;&#36890;&#36807;&#25512;&#21160;&#32511;&#29699;&#20351;&#34013;&#29699;&#25758;&#21040;&#32418;&#29699;&#8221;&#12290;&#26234;&#33021;&#20307;&#30340;&#24178;&#39044;&#26469;&#33258;&#20110;&#36830;&#32493;&#31354;&#38388;&#65292;&#24182;&#19988;&#20107;&#20214;&#30340;&#32423;&#32852;&#20351;&#24471;&#21160;&#24577;&#21464;&#24471;&#39640;&#24230;&#38750;&#32447;&#24615;&#12290;&#25105;&#20204;&#23558;&#35821;&#20041;&#26641;&#25628;&#32034;&#21644;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23398;&#20064;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#25628;&#32034;&#35821;&#20041;&#26641;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20250;&#20102;&#26377;&#25928;&#22320;&#36981;&#24490;&#24178;&#39044;&#25351;&#20196;&#65292;&#22788;&#29702;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#23427;&#36824;&#21487;&#20197;&#25512;&#29702;&#26367;&#20195;&#32467;&#26524;&#65292;&#24403;&#25552;&#20379;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Reasoning and interacting with dynamic environments is a fundamental problem in AI, but it becomes extremely challenging when actions can trigger cascades of cross-dependent events. We introduce a new supervised learning setup called {\em Cascade} where an agent is shown a video of a physically simulated dynamic scene, and is asked to intervene and trigger a cascade of events, such that the system reaches a "counterfactual" goal. For instance, the agent may be asked to "Make the blue ball hit the red one, by pushing the green ball". The agent intervention is drawn from a continuous space, and cascades of events makes the dynamics highly non-linear.  We combine semantic tree search with an event-driven forward model and devise an algorithm that learns to search in semantic trees in continuous spaces. We demonstrate that our approach learns to effectively follow instructions to intervene in previously unseen complex scenes. It can also reason about alternative outcomes, when provided an 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25968;&#25454;&#23494;&#24230;&#12289;&#22122;&#22768;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#23545;&#30340;&#23494;&#24230;&#23545;&#20110;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#22312;&#23494;&#38598;&#25968;&#25454;&#38598;&#19978;&#27604;&#23545;&#31216;&#26631;&#31614;&#22122;&#22768;&#26356;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#23494;&#24230;&#35825;&#23548;&#30340;&#30456;&#20284;&#24615;&#30772;&#22351;&#65288;DIBS&#65289;&#12290;</title><link>http://arxiv.org/abs/2201.12803</link><description>&lt;p&gt;
&#22122;&#22768;&#35774;&#32622;&#20013;&#30340;&#30456;&#20284;&#24615;&#27867;&#21270;&#65306;DIBS&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Generalizing similarity in noisy setups: the DIBS phenomenon. (arXiv:2201.12803v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25968;&#25454;&#23494;&#24230;&#12289;&#22122;&#22768;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#23545;&#30340;&#23494;&#24230;&#23545;&#20110;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#22312;&#23494;&#38598;&#25968;&#25454;&#38598;&#19978;&#27604;&#23545;&#31216;&#26631;&#31614;&#22122;&#22768;&#26356;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#23494;&#24230;&#35825;&#23548;&#30340;&#30456;&#20284;&#24615;&#30772;&#22351;&#65288;DIBS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#25968;&#25454;&#23494;&#24230;&#12289;&#22122;&#22768;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26222;&#36866;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26297;&#32599;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#65292;&#36825;&#26159;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#26412;&#24418;&#24335;&#65292;&#24182;&#25506;&#32034;&#20102;&#20004;&#31181;&#21487;&#33021;&#24433;&#21709;SNNs&#30340;&#22122;&#22768;&#31867;&#22411;&#65292;&#21363;&#23545;&#27604;&#26631;&#31614;&#22122;&#22768;&#65288;PLN&#65289;&#21644;&#21333;&#26631;&#31614;&#22122;&#22768;&#65288;SLN&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#35770;&#35757;&#32451;&#35774;&#32622;&#22914;&#20309;&#65292;SNNs&#37117;&#34920;&#29616;&#20986;&#21452;&#38477;&#34892;&#20026;&#65292;&#24182;&#19988;&#22122;&#22768;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#31181;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#25968;&#25454;&#23545;&#30340;&#23494;&#24230;&#23545;&#20110;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;SNNs&#22312;&#31232;&#30095;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#30340;PLN&#25110;SLN&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#24615;&#33021;&#26159;&#21487;&#27604;&#36739;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#23494;&#38598;&#25968;&#25454;&#38598;&#26102;&#65292;&#22312;&#36807;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#65292;PLN&#26696;&#20363;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#65292;&#30456;&#23545;&#20110;SLN&#26696;&#20363;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#25105;&#20204;&#31216;&#20026;&#23494;&#24230;&#35825;&#23548;&#30340;&#30456;&#20284;&#24615;&#30772;&#22351;&#65288;DIBS&#65289;&#30340;&#29616;&#35937;&#12290;&#22312;&#36825;&#20010;&#24773;&#20917;&#19979;&#65292;PLN&#30456;&#20284;&#24615;&#36829;&#35268;&#21464;&#24471;&#23439;&#35266;&#21270;&#65292;&#20351;&#24471;&#25968;&#25454;&#38598;&#34987;&#25439;&#22351;&#21040;&#26080;&#27861;&#23454;&#29616;&#23436;&#20840;&#25554;&#20540;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work uncovers an interplay among data density, noise, and the generalization ability in similarity learning. We consider Siamese Neural Networks (SNNs), which are the basic form of contrastive learning, and explore two types of noise that can impact SNNs, Pair Label Noise (PLN) and Single Label Noise (SLN). Our investigation reveals that SNNs exhibit double descent behaviour regardless of the training setup and that it is further exacerbated by noise. We demonstrate that the density of data pairs is crucial for generalization. When SNNs are trained on sparse datasets with the same amount of PLN or SLN, they exhibit comparable generalization properties. However, when using dense datasets, PLN cases generalize worse than SLN ones in the overparametrized region, leading to a phenomenon we call Density-Induced Break of Similarity (DIBS). In this regime, PLN similarity violation becomes macroscopical, corrupting the dataset to the point where complete interpolation cannot be achieved, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#29420;&#29305;&#23433;&#20840;&#38382;&#39064;&#8212;&#8212;&#29992;&#25143;&#31713;&#25913;&#65292;&#24182;&#25552;&#20986;&#20102;&#24418;&#24335;&#21270;&#26041;&#27861;&#21644;&#23454;&#35777;&#35777;&#25454;&#12290;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#26377;&#25928;&#38450;&#27490;&#29992;&#25143;&#31713;&#25913;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#22870;&#21169;&#31713;&#25913;&#32531;&#35299;&#31574;&#30053;&#20063;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2109.04083</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#31713;&#25913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
User Tampering in Reinforcement Learning Recommender Systems. (arXiv:2109.04083v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#29420;&#29305;&#23433;&#20840;&#38382;&#39064;&#8212;&#8212;&#29992;&#25143;&#31713;&#25913;&#65292;&#24182;&#25552;&#20986;&#20102;&#24418;&#24335;&#21270;&#26041;&#27861;&#21644;&#23454;&#35777;&#35777;&#25454;&#12290;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#26377;&#25928;&#38450;&#27490;&#29992;&#25143;&#31713;&#25913;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#22870;&#21169;&#31713;&#25913;&#32531;&#35299;&#31574;&#30053;&#20063;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#26032;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#23454;&#35777;&#35777;&#25454;&#65292;&#31361;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25512;&#33616;&#31639;&#27861;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#19968;&#31181;&#29420;&#29305;&#23433;&#20840;&#38382;&#39064;&#8212;&#8212;'&#29992;&#25143;&#31713;&#25913;'&#12290;&#29992;&#25143;&#31713;&#25913;&#26159;&#25351;RL&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#20854;&#24314;&#35758;&#26469;&#25805;&#32437;&#23186;&#20307;&#29992;&#25143;&#30340;&#24847;&#35265;&#65292;&#20316;&#20026;&#19968;&#31181;&#26368;&#22823;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#30340;&#24418;&#24335;&#21270;&#25216;&#26415;&#23545;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#29992;&#20110;&#23454;&#26045;&#21487;&#25193;&#23637;&#30340;RL&#25512;&#33616;&#31995;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20851;&#38190;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#24182;&#19981;&#33021;&#20805;&#20998;&#38450;&#27490;&#29992;&#25143;&#31713;&#25913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22870;&#21169;&#31713;&#25913;&#38382;&#39064;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#19981;&#36275;&#20197;&#24212;&#23545;&#25512;&#33616;&#22330;&#26223;&#20013;&#29420;&#29305;&#30340;&#29992;&#25143;&#31713;&#25913;&#29616;&#35937;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#38024;&#23545;&#19968;&#20010;&#20197;&#20256;&#25773;&#20026;&#28966;&#28857;&#30340;RL&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#25311;&#30740;&#31350;&#26469;&#21152;&#24378;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms -- 'user tampering.' User tampering is a situation where an RL-based recommender system may manipulate a media user's opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;&#65288;CoIL&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#29702;&#33258;&#36523;&#30340;&#33391;&#22909;&#32463;&#39564;&#65292;&#32780;&#26080;&#38656;&#19987;&#23478;&#31034;&#33539;&#65292;&#26469;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2103.14823</link><description>&lt;p&gt;
&#26080;&#38656;&#19987;&#23478;&#31034;&#33539;&#30340;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Co-Imitation Learning without Expert Demonstration. (arXiv:2103.14823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.14823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;&#65288;CoIL&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#29702;&#33258;&#36523;&#30340;&#33391;&#22909;&#32463;&#39564;&#65292;&#32780;&#26080;&#38656;&#19987;&#23478;&#31034;&#33539;&#65292;&#26469;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#31034;&#33539;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25928;&#29575;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#33719;&#21462;&#19987;&#23478;&#31034;&#33539;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;&#65288;CoIL&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21033;&#29992;&#20195;&#29702;&#36873;&#25321;&#25506;&#32034;&#29615;&#22659;&#24182;&#21033;&#29992;&#21516;&#20276;&#20195;&#29702;&#30340;&#32463;&#39564;&#12290;&#23613;&#31649;&#36825;&#20123;&#32463;&#39564;&#21487;&#33021;&#26377;&#20215;&#20540;&#65292;&#20063;&#21487;&#33021;&#35823;&#23548;&#20154;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26399;&#26395;&#22686;&#30410;&#30340;&#20215;&#20540;&#20989;&#25968;&#26469;&#20272;&#35745;&#27599;&#20010;&#32463;&#39564;&#30340;&#28508;&#22312;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#24378;&#35843;&#26356;&#26377;&#29992;&#30340;&#32463;&#39564;&#24182;&#36807;&#28388;&#25481;&#22122;&#22768;&#26469;&#36873;&#25321;&#24615;&#22320;&#20114;&#30456;&#27169;&#20223;&#12290;&#21508;&#31181;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20849;&#21516;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning is a primary approach to improve the efficiency of reinforcement learning by exploiting the expert demonstrations. However, in many real scenarios, obtaining expert demonstrations could be extremely expensive or even impossible. To overcome this challenge, in this paper, we propose a novel learning framework called Co-Imitation Learning (CoIL) to exploit the past good experiences of the agents themselves without expert demonstration. Specifically, we train two different agents via letting each of them alternately explore the environment and exploit the peer agent's experience. While the experiences could be valuable or misleading, we propose to estimate the potential utility of each piece of experience with the expected gain of the value function. Thus the agents can selectively imitate from each other by emphasizing the more useful experiences while filtering out noisy ones. Experimental results on various tasks show significant superiority of the proposed Co-Imitat
&lt;/p&gt;</description></item><item><title>XTQA&#26159;&#38024;&#23545;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#25552;&#20986;&#30340;&#19968;&#31181;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#36328;&#21477;&#35299;&#37322;&#25552;&#20379;&#31572;&#26696;&#21644;&#35777;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2011.12662</link><description>&lt;p&gt;
XTQA: &#25945;&#31185;&#20070;&#38382;&#31572;&#30340;&#36328;&#21477;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
XTQA: Span-Level Explanations of the Textbook Question Answering. (arXiv:2011.12662v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.12662
&lt;/p&gt;
&lt;p&gt;
XTQA&#26159;&#38024;&#23545;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#25552;&#20986;&#30340;&#19968;&#31181;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#36328;&#21477;&#35299;&#37322;&#25552;&#20379;&#31572;&#26696;&#21644;&#35777;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#31185;&#20070;&#38382;&#31572;&#65288;TQA&#65289;&#26159;&#19968;&#20010;&#20219;&#21153;&#65292;&#35201;&#27714;&#22312;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#25991;&#31456;&#21644;&#22270;&#34920;&#30340;&#22810;&#27169;&#24577;&#32972;&#26223;&#19979;&#65292;&#22238;&#31572;&#19968;&#20010;&#22270;&#34920;/&#38750;&#22270;&#34920;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#37322;&#24615;&#24212;&#35813;&#23558;&#23398;&#29983;&#35270;&#20026;&#19968;&#20010;&#38656;&#35201;&#32771;&#34385;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#20174;&#31895;&#21040;&#32454;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;TQA&#30340;&#36328;&#21477;&#35299;&#37322;&#65288;XTQA&#65289;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20026;&#23398;&#29983;&#25552;&#20379;&#19981;&#20165;&#26159;&#31572;&#26696;&#65292;&#36824;&#21253;&#25324;&#29992;&#20110;&#36873;&#25321;&#31572;&#26696;&#30340;&#36328;&#21477;&#35777;&#25454;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#20351;&#29992;TF-IDF&#26041;&#27861;&#31895;&#30053;&#22320;&#36873;&#25321;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#21069;M&#20010;&#27573;&#33853;&#65292;&#28982;&#21518;&#20174;&#36825;&#20123;&#27573;&#33853;&#20013;&#30340;&#25152;&#26377;&#20505;&#36873;&#36328;&#21477;&#20013;&#31934;&#32454;&#22320;&#36873;&#25321;&#21069;K&#20010;&#35777;&#25454;&#36328;&#21477;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#36328;&#21477;&#23545;&#38382;&#39064;&#30340;&#20449;&#24687;&#22686;&#30410;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;XTQA&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/keep-smile-001/opentqa&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbook Question Answering (TQA) is a task that one should answer a diagram/non-diagram question given a large multi-modal context consisting of abundant essays and diagrams. We argue that the explainability of this task should place students as a key aspect to be considered. To address this issue, we devise a novel architecture towards span-level eXplanations of the TQA (XTQA) based on our proposed coarse-to-fine grained algorithm, which can provide not only the answers but also the span-level evidences to choose them for students. This algorithm first coarsely chooses top $M$ paragraphs relevant to questions using the TF-IDF method, and then chooses top $K$ evidence spans finely from all candidate spans within these paragraphs by computing the information gain of each span to questions. Experimental results shows that XTQA significantly improves the state-of-the-art performance compared with baselines. The source code is available at https://github.com/keep-smile-001/opentqa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#65292;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#36873;&#25321;&#20248;&#21270;&#20316;&#20026;TAVs&#30340;&#27867;&#21270;&#65292;&#20197;&#26368;&#23567;&#21270;&#23545;&#65288;&#31038;&#20250;&#65289;&#30446;&#26631;&#30340;&#38459;&#30861;&#12290;</title><link>http://arxiv.org/abs/2007.15393</link><description>&lt;p&gt;
&#26397;&#30528;&#19968;&#31181;&#26032;&#30340;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a new Social Choice Theory. (arXiv:2007.15393v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.15393
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#65292;&#36890;&#36807;&#24341;&#20837;&#31038;&#20250;&#36873;&#25321;&#20248;&#21270;&#20316;&#20026;TAVs&#30340;&#27867;&#21270;&#65292;&#20197;&#26368;&#23567;&#21270;&#23545;&#65288;&#31038;&#20250;&#65289;&#30446;&#26631;&#30340;&#38459;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#26159;&#20851;&#20110;&#20174;&#20010;&#20307;&#24847;&#35265;&#12289;&#20559;&#22909;&#12289;&#21033;&#30410;&#25110;&#31119;&#21033;&#20986;&#21457;&#65292;&#21521;&#31038;&#20250;&#31119;&#21033;&#36827;&#34892;&#38598;&#20307;&#20915;&#31574;&#30340;&#29702;&#35770;&#12290;&#35745;&#31639;&#31038;&#20250;&#31119;&#21033;&#39046;&#22495;&#30456;&#23545;&#36739;&#26032;&#65292;&#24182;&#22312;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#32463;&#20856;&#25991;&#29486;&#20551;&#35774;&#20559;&#22909;&#26159;&#21333;&#23792;&#30340;&#65292;&#21363;&#23384;&#22312;&#19968;&#31181;&#20559;&#22909;&#30340;&#39034;&#24207;&#65292;&#24182;&#19988;&#22312;&#36825;&#20010;&#39034;&#24207;&#20013;&#23384;&#22312;&#19968;&#20010;&#20840;&#23616;&#26368;&#22823;&#20540;&#12290;&#20170;&#24180;&#26377;&#20851;&#20004;&#38454;&#27573;&#25209;&#20934;&#25237;&#31080;&#31995;&#32479;&#65288;TAVs&#65289;&#12289;&#22810;&#36194;&#23478;&#36873;&#25321;&#35268;&#21017;&#65288;MWSR&#65289;&#20197;&#21450;&#19981;&#23436;&#20840;&#65288;IPs&#65289;&#21644;&#24490;&#29615;&#20559;&#22909;&#65288;CPs&#65289;&#30340;&#19968;&#20123;&#29702;&#35770;&#32467;&#26524;&#24050;&#32463;&#21457;&#34920;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26377;&#19977;&#20010;&#65306;&#39318;&#20808;&#65292;&#25105;&#24819;&#23558;&#31038;&#20250;&#36873;&#25321;&#20248;&#21270;&#20316;&#20026;TAVs&#30340;&#19968;&#31181;&#27867;&#21270;&#24341;&#20837;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#26368;&#22823;&#38454;&#27573;&#21644;&#19968;&#20010;&#26368;&#23567;&#38454;&#27573;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#31181;&#20943;&#23569;&#23545;&#65288;&#31038;&#20250;&#65289;&#30446;&#26631;&#20135;&#29983;&#38459;&#30861;&#30340;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#35268;&#21017;&#65306;Minimax&#12290;&#20854;&#27425;&#65292;&#25105;&#24819;&#36890;&#36807;&#25105;&#30340;&#24320;&#25918;&#26631;&#20934;&#21270;&#21644;&#24320;&#25918;&#21019;&#26032;&#26041;&#27861;&#24341;&#20837;&#19968;&#20010;&#65306;
&lt;/p&gt;
&lt;p&gt;
Social choice is the theory about collective decision towards social welfare starting from individual opinions, preferences, interests or welfare. The field of Computational Social Welfare is somewhat recent and it is gaining impact in the Artificial Intelligence Community. Classical literature makes the assumption of single-peaked preferences, i.e. there exist a order in the preferences and there is a global maximum in this order. This year some theoretical results were published about Two-stage Approval Voting Systems (TAVs), Multi-winner Selection Rules (MWSR) and Incomplete (IPs) and Circular Preferences (CPs). The purpose of this paper is three-fold: Firstly, I want to introduced Social Choice Optimisation as a generalisation of TAVs where there is a max stage and a min stage implementing thus a Minimax, well-known Artificial Intelligence decision-making rule to minimize hindering towards a (Social) Goal. Secondly, I want to introduce, following my Open Standardization and Open In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/1912.13122</link><description>&lt;p&gt;
&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22768;&#26126;&#24615;&#30005;&#23376;&#26426;&#26500;&#65288;DEIs&#65289;&#30340;&#35843;&#25511;&#26159;&#36807;&#21435;&#21313;&#24180;&#28041;&#21450;&#29289;&#29702;&#21644;&#36719;&#20214;&#26234;&#33021;&#20307;&#20197;&#21450;&#27861;&#24459;&#30340;&#22810;&#23398;&#31185;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#36880;&#28176;&#28436;&#21464;&#20026;2016&#24180;&#36215;&#34987;&#31216;&#20026;&#26032;&#38395;&#30340;&#26426;&#22120;&#24459;&#24072;&#12290;&#20854;&#20013;&#19968;&#31181;&#39318;&#27425;&#25552;&#20986;&#38480;&#21046;&#36719;&#20214;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#26696;&#26159;&#30005;&#23376;&#26426;&#26500;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#26377;&#20851;DL&#20351;&#29992;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#65292;MAS&#30340;&#35268;&#33539;&#20960;&#20046;&#24471;&#21040;&#27491;&#30830;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20043;&#20026;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#26088;&#26159;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#65288;AT&#65289;&#30340;&#20851;&#27880;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21021;&#27493;&#30340;&#31572;&#26696;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
&lt;/p&gt;</description></item></channel></rss>