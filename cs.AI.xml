<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#21494;&#29255;&#32972;&#21518;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;SRPNet&#65292;&#21487;&#20197;&#39044;&#27979;&#26377;&#25928;&#30340;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#31354;&#38388;&#30340;&#21160;&#20316;&#65292;&#36827;&#19968;&#27493;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#31354;&#38388;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26893;&#29289;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03175</link><description>&lt;p&gt;
&#25512;&#24320;&#32511;&#33394;&#65306;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#26893;&#29289;&#21494;&#29255;&#32972;&#21518;&#30340;&#20869;&#23481;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Push Past Green: Learning to Look Behind Plant Foliage by Moving It. (arXiv:2307.03175v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31227;&#21160;&#26893;&#29289;&#26469;&#26597;&#30475;&#21494;&#29255;&#32972;&#21518;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;SRPNet&#65292;&#21487;&#20197;&#39044;&#27979;&#26377;&#25928;&#30340;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#31354;&#38388;&#30340;&#21160;&#20316;&#65292;&#36827;&#19968;&#27493;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#31354;&#38388;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26893;&#29289;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20892;&#19994;&#24212;&#29992;&#65288;&#20363;&#22914;&#26816;&#26597;&#12289;&#34920;&#22411;&#20998;&#26512;&#12289;&#37319;&#25688;&#27700;&#26524;&#65289;&#38656;&#35201;&#25805;&#20316;&#26893;&#29289;&#21494;&#29255;&#20197;&#26597;&#30475;&#21494;&#23376;&#21644;&#26525;&#24178;&#30340;&#32972;&#21518;&#12290;&#37096;&#20998;&#21487;&#35265;&#24615;&#12289;&#26497;&#31471;&#26434;&#20081;&#12289;&#34180;&#32467;&#26500;&#20197;&#21450;&#26893;&#29289;&#30340;&#26410;&#30693;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#37117;&#20351;&#24471;&#36825;&#31181;&#25805;&#20316;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#26469;&#35757;&#32451;SRPNet&#65292;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#39044;&#27979;&#22312;&#32473;&#23450;&#26893;&#29289;&#19978;&#25191;&#34892;&#20505;&#36873;&#21160;&#20316;&#26102;&#20250;&#26174;&#38706;&#20986;&#22810;&#23569;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#20132;&#21449;&#29109;&#26041;&#27861;&#30340;SRPNet&#26469;&#39044;&#27979;&#26377;&#25928;&#22320;&#26174;&#38706;&#20986;&#26893;&#29289;&#21494;&#29255;&#19979;&#30340;&#31354;&#38388;&#30340;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;SRPNet&#19981;&#20165;&#39044;&#27979;&#26174;&#38706;&#20986;&#22810;&#23569;&#31354;&#38388;&#65292;&#36824;&#39044;&#27979;&#26174;&#38706;&#20986;&#31354;&#38388;&#30340;&#20301;&#32622;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#36880;&#27493;&#26174;&#38706;&#20986;&#26356;&#22810;&#30340;&#26893;&#29289;&#21494;&#29255;&#19979;&#30340;&#31354;&#38388;&#12290;&#22312;&#29289;&#29702;&#27979;&#35797;&#24179;&#21488;&#19978;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#30340;&#34276;&#34067;&#21644;&#30495;&#23454;&#26893;&#29289;&#65288;&#40857;&#34880;&#26641;&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;5&#20010;&#35774;&#32622;&#65292;&#21253;&#25324;2&#20010;&#27979;&#35797;&#27867;&#21270;&#24615;&#33021;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agriculture applications (e.g., inspection, phenotyping, plucking fruits) require manipulating the plant foliage to look behind the leaves and the branches. Partial visibility, extreme clutter, thin structures, and unknown geometry and dynamics for plants make such manipulation challenging. We tackle these challenges through data-driven methods. We use self-supervision to train SRPNet, a neural network that predicts what space is revealed on execution of a candidate action on a given plant. We use SRPNet with the cross-entropy method to predict actions that are effective at revealing space beneath plant foliage. Furthermore, as SRPNet does not just predict how much space is revealed but also where it is revealed, we can execute a sequence of actions that incrementally reveal more and more space beneath the plant foliage. We experiment with a synthetic (vines) and a real plant (Dracaena) on a physical test-bed across 5 settings including 2 settings that test generalization to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;BDD&#26041;&#27861;&#20013;&#21464;&#37327;&#25490;&#24207;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#22522;&#20110;&#21464;&#37327;&#35780;&#20998;&#20989;&#25968;&#30340;&#21442;&#25968;&#37197;&#32622;&#31354;&#38388;&#65292;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;BDD&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#20351;&#29992;&#40657;&#31665;&#20248;&#21270;&#25216;&#26415;&#39640;&#25928;&#22320;&#23547;&#25214;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03171</link><description>&lt;p&gt;
LEO&#65306;&#23398;&#20064;&#39640;&#25928;&#26377;&#24207;&#30340;&#22810;&#30446;&#26631;&#20108;&#36827;&#21046;&#20915;&#31574;&#22270;
&lt;/p&gt;
&lt;p&gt;
LEO: Learning Efficient Orderings for Multiobjective Binary Decision Diagrams. (arXiv:2307.03171v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;BDD&#26041;&#27861;&#20013;&#21464;&#37327;&#25490;&#24207;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#22522;&#20110;&#21464;&#37327;&#35780;&#20998;&#20989;&#25968;&#30340;&#21442;&#25968;&#37197;&#32622;&#31354;&#38388;&#65292;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;BDD&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#20351;&#29992;&#40657;&#31665;&#20248;&#21270;&#25216;&#26415;&#39640;&#25928;&#22320;&#23547;&#25214;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20108;&#36827;&#21046;&#20915;&#31574;&#22270;&#65288;BDD&#65289;&#30340;&#26041;&#27861;&#36817;&#24180;&#26469;&#22312;&#22810;&#30446;&#26631;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26500;&#24314;BDD&#26102;&#20351;&#29992;&#30340;&#21464;&#37327;&#25490;&#24207;&#23545;&#20854;&#22823;&#23567;&#20197;&#21450;&#30001;&#25918;&#26494;&#25110;&#38480;&#21046;&#30340;BDD&#23548;&#20986;&#30340;&#30028;&#38480;&#36136;&#37327;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#21464;&#37327;&#25490;&#24207;&#23545;&#22810;&#30446;&#26631;&#32972;&#21253;&#38382;&#39064;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65288;PF&#65289;&#26522;&#20030;&#26102;&#38388;&#30340;&#31867;&#20284;&#24433;&#21709;&#65292;&#34920;&#26126;&#26377;&#24517;&#35201;&#25512;&#23548;&#20986;&#21487;&#25552;&#39640;&#22810;&#30446;&#26631;BDD&#26041;&#27861;&#21487;&#25193;&#23637;&#24615;&#30340;&#21464;&#37327;&#25490;&#24207;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#22522;&#20110;&#21464;&#37327;&#35780;&#20998;&#20989;&#25968;&#30340;&#26032;&#30340;&#21442;&#25968;&#37197;&#32622;&#31354;&#38388;&#65292;&#35813;&#35780;&#20998;&#20989;&#25968;&#22312;&#19968;&#23567;&#32452;&#21487;&#35299;&#37322;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#21464;&#37327;&#29305;&#24449;&#19978;&#26159;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#40657;&#31665;&#20248;&#21270;&#39640;&#25928;&#22320;&#25506;&#32034;&#37197;&#32622;&#31354;&#38388;&#65292;&#36991;&#20813;&#20102;&#32500;&#24230;&#30340;&#35781;&#21650;&#65288;&#21464;&#37327;&#21644;&#30446;&#26631;&#30340;&#25968;&#37327;&#65289;&#65292;&#24182;&#25214;&#21040;&#20102;&#33391;&#22909;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approaches based on Binary decision diagrams (BDDs) have recently achieved state-of-the-art results for multiobjective integer programming problems. The variable ordering used in constructing BDDs can have a significant impact on their size and on the quality of bounds derived from relaxed or restricted BDDs for single-objective optimization problems. We first showcase a similar impact of variable ordering on the Pareto frontier (PF) enumeration time for the multiobjective knapsack problem, suggesting the need for deriving variable ordering methods that improve the scalability of the multiobjective BDD approach. To that end, we derive a novel parameter configuration space based on variable scoring functions which are linear in a small set of interpretable and easy-to-compute variable features. We show how the configuration space can be efficiently explored using black-box optimization, circumventing the curse of dimensionality (in the number of variables and objectives), and finding go
&lt;/p&gt;</description></item><item><title>Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.03170</link><description>&lt;p&gt;
Focused Transformer: &#21453;&#24046;&#35757;&#32451;&#23545;&#19978;&#19979;&#25991;&#32553;&#25918;&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03170
&lt;/p&gt;
&lt;p&gt;
Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#19978;&#19979;&#25991;&#21270;&#30340;&#26041;&#24335;&#21560;&#32435;&#26032;&#30340;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20026;&#27880;&#24847;&#21147;&#23618;&#25552;&#20379;&#35775;&#38382;&#22806;&#37096;&#23384;&#20648;&#22120;&#30340;&#33021;&#21147;&#65292;&#35813;&#23384;&#20648;&#22120;&#30001;&#65288;&#38190;&#65292;&#20540;&#65289;&#23545;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25991;&#26723;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30456;&#20851;&#38190;&#19982;&#26080;&#20851;&#38190;&#30340;&#27604;&#20363;&#20943;&#23569;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#26080;&#20851;&#38190;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;&#20998;&#24515;&#38382;&#39064;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#19982;&#19981;&#21516;&#35821;&#20041;&#20540;&#30456;&#20851;&#32852;&#30340;&#38190;&#21487;&#33021;&#37325;&#21472;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Focused Transformer&#65288;FoT&#65289;&#65292;&#19968;&#31181;&#21463;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#65288;&#38190;&#65292;&#20540;&#65289;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20351;&#19978;&#19979;&#25991;&#38271;&#24230;&#24471;&#20197;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-s
&lt;/p&gt;</description></item><item><title>BrickPal&#26159;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#29616;&#23454;&#30340;&#31215;&#26408;&#27169;&#22411;&#35013;&#37197;&#25351;&#23548;&#31995;&#32479;&#65292;&#21487;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#29983;&#25104;&#35013;&#37197;&#39034;&#24207;&#65292;&#24182;&#22312;&#22686;&#24378;&#29616;&#23454;&#22836;&#30420;&#20013;&#25552;&#20379;&#23454;&#26102;&#25351;&#23548;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;BrickPal&#33021;&#26377;&#25928;&#24110;&#21161;&#29992;&#25143;&#36827;&#34892;&#31215;&#26408;&#35013;&#37197;&#65292;&#32780;&#19988;&#29983;&#25104;&#30340;&#35013;&#37197;&#39034;&#24207;&#19982;&#25163;&#21160;&#35843;&#25972;&#30340;&#39034;&#24207;&#20855;&#26377;&#30456;&#21516;&#30340;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03162</link><description>&lt;p&gt;
BrickPal:&#22522;&#20110;&#22686;&#24378;&#29616;&#23454;&#30340;&#31215;&#26408;&#27169;&#22411;&#35013;&#37197;&#25351;&#23548;&#20070;
&lt;/p&gt;
&lt;p&gt;
BrickPal: Augmented Reality-based Assembly Instructions for Brick Models. (arXiv:2307.03162v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03162
&lt;/p&gt;
&lt;p&gt;
BrickPal&#26159;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#29616;&#23454;&#30340;&#31215;&#26408;&#27169;&#22411;&#35013;&#37197;&#25351;&#23548;&#31995;&#32479;&#65292;&#21487;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#29983;&#25104;&#35013;&#37197;&#39034;&#24207;&#65292;&#24182;&#22312;&#22686;&#24378;&#29616;&#23454;&#22836;&#30420;&#20013;&#25552;&#20379;&#23454;&#26102;&#25351;&#23548;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;BrickPal&#33021;&#26377;&#25928;&#24110;&#21161;&#29992;&#25143;&#36827;&#34892;&#31215;&#26408;&#35013;&#37197;&#65292;&#32780;&#19988;&#29983;&#25104;&#30340;&#35013;&#37197;&#39034;&#24207;&#19982;&#25163;&#21160;&#35843;&#25972;&#30340;&#39034;&#24207;&#20855;&#26377;&#30456;&#21516;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35013;&#37197;&#25351;&#23548;&#20070;&#26159;&#31867;&#20284;&#20048;&#39640;&#31215;&#26408;&#22871;&#35013;&#30340;&#24378;&#21046;&#24615;&#32452;&#25104;&#37096;&#20998;&#12290;&#20256;&#32479;&#30340;&#35013;&#37197;&#25351;&#23548;&#20070;&#21046;&#20316;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#21160;&#35843;&#25972;&#65292;&#36825;&#23545;&#20110;&#20241;&#38386;&#29992;&#25143;&#21644;&#23450;&#21046;&#31215;&#26408;&#22871;&#35013;&#26469;&#35828;&#26159;&#26840;&#25163;&#30340;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#32440;&#36136;&#25351;&#23548;&#20070;&#32570;&#20047;&#34920;&#29616;&#21147;&#21644;&#20114;&#21160;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BrickPal&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#29616;&#23454;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#22686;&#24378;&#29616;&#23454;&#22836;&#25140;&#26174;&#31034;&#22120;&#21487;&#35270;&#21270;&#35013;&#37197;&#25351;&#23548;&#20070;&#12290;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#29983;&#25104;&#21512;&#29702;&#30340;&#35013;&#37197;&#39034;&#24207;&#65292;&#24182;&#22312;&#22686;&#24378;&#29616;&#23454;&#22836;&#30420;&#20013;&#25552;&#20379;&#23454;&#26102;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#35013;&#37197;&#26041;&#27861;&#30456;&#27604;&#65292;BrickPal&#22312;&#24110;&#21161;&#29992;&#25143;&#36827;&#34892;&#31215;&#26408;&#35013;&#37197;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;NLP&#31639;&#27861;&#29983;&#25104;&#30340;&#35013;&#37197;&#39034;&#24207;&#19982;&#25163;&#21160;&#35843;&#25972;&#30340;&#39034;&#24207;&#20855;&#26377;&#30456;&#21516;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assembly instruction is a mandatory component of Lego-like brick sets.The conventional production of assembly instructions requires a considerable amount of manual fine-tuning, which is intractable for casual users and customized brick sets.Moreover, the traditional paper-based instructions lack expressiveness and interactivity.To tackle the two problems above, we present BrickPal, an augmented reality-based system, which visualizes assembly instructions in an augmented reality head-mounted display. It utilizes Natural Language Processing (NLP) techniques to generate plausible assembly sequences, and provide real-time guidance in the AR headset.Our user study demonstrates BrickPal's effectiveness at assisting users in brick assembly compared to traditional assembly methods. Additionally, the NLP algorithm-generated assembly sequences achieve the same usability with manually adapted sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03135</link><description>&lt;p&gt;
&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#24615;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#21644;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#21387;&#32553;&#26159;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#24555;&#30340;&#27169;&#22411;&#20197;&#20445;&#25345;&#36739;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#23567;&#22411;&#25110;&#20013;&#22411;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21487;&#27867;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#38382;&#39064;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;OOD&#21487;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#27169;&#20223;&#25945;&#24072;&#30340;&#35270;&#35273;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#26041;&#38754;&#35880;&#24910;&#22320;&#20419;&#36827;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20016;&#23500;&#23398;&#29983;&#27169;&#22411;&#30340;&#33258;&#20030;&#23398;&#20064;&#21644;&#25968;&#25454;&#25193;&#20805;&#26469;&#25552;&#39640;OOD&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#19979;&#30340;&#22810;&#35746;&#21333;&#25191;&#34892;&#38382;&#39064;&#12290;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#19982;&#21327;&#20316;&#65292;&#26368;&#22823;&#21270;&#25972;&#20307;&#21033;&#28070;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#35746;&#21333;&#30340;&#24773;&#20917;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#21644;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.03119</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#24847;&#22270;&#24863;&#30693;&#36890;&#20449;&#20197;&#23454;&#29616;&#37329;&#34701;&#20013;&#30340;&#26368;&#20248;&#22810;&#35746;&#21333;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance. (arXiv:2307.03119v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#19979;&#30340;&#22810;&#35746;&#21333;&#25191;&#34892;&#38382;&#39064;&#12290;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#19982;&#21327;&#20316;&#65292;&#26368;&#22823;&#21270;&#25972;&#20307;&#21033;&#28070;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#35746;&#21333;&#30340;&#24773;&#20917;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#21644;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35746;&#21333;&#25191;&#34892;&#26159;&#37327;&#21270;&#37329;&#34701;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#23436;&#25104;&#29305;&#23450;&#36164;&#20135;&#30340;&#19968;&#31995;&#21015;&#20132;&#26131;&#35746;&#21333;&#30340;&#25910;&#36141;&#25110;&#28165;&#31639;&#12290;&#26368;&#36817;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36827;&#23637;&#20026;&#35746;&#21333;&#25191;&#34892;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#24635;&#26159;&#38024;&#23545;&#21333;&#20010;&#35746;&#21333;&#36827;&#34892;&#20248;&#21270;&#65292;&#24573;&#35270;&#20102;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#35746;&#21333;&#30340;&#23454;&#36341;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#21644;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#26469;&#25191;&#34892;&#22810;&#35746;&#21333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#26234;&#33021;&#20307;&#35270;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#21592;&#26469;&#20132;&#26131;&#19968;&#20010;&#29305;&#23450;&#30340;&#35746;&#21333;&#65292;&#21516;&#26102;&#20445;&#25345;&#24444;&#27492;&#36890;&#20449;&#24182;&#21327;&#20316;&#20197;&#26368;&#22823;&#21270;&#24635;&#20307;&#21033;&#28070;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#20165;&#20132;&#25442;&#37096;&#20998;&#35266;&#27979;&#20449;&#24687;&#26469;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#65292;&#36825;&#22312;&#22797;&#26434;&#30340;&#37329;&#34701;&#29615;&#22659;&#20013;&#26159;&#20302;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order execution is a fundamental task in quantitative finance, aiming at finishing acquisition or liquidation for a number of trading orders of the specific assets. Recent advance in model-free reinforcement learning (RL) provides a data-driven solution to the order execution problem. However, the existing works always optimize execution for an individual order, overlooking the practice that multiple orders are specified to execute simultaneously, resulting in suboptimality and bias. In this paper, we first present a multi-agent RL (MARL) method for multi-order execution considering practical constraints. Specifically, we treat every agent as an individual operator to trade one specific order, while keeping communicating with each other and collaborating for maximizing the overall profits. Nevertheless, the existing MARL algorithms often incorporate communication among agents by exchanging only the information of their partial observations, which is inefficient in complicated financial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>OpenDelta&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;delta&#35843;&#25972;&#26041;&#27861;&#30340;&#21363;&#25554;&#21363;&#29992;&#23454;&#29616;&#12290;&#23427;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03084</link><description>&lt;p&gt;
OpenDelta: &#19968;&#31181;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#24211;
&lt;/p&gt;
&lt;p&gt;
OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models. (arXiv:2307.03084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03084
&lt;/p&gt;
&lt;p&gt;
OpenDelta&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;delta&#35843;&#25972;&#26041;&#27861;&#30340;&#21363;&#25554;&#21363;&#29992;&#23454;&#29616;&#12290;&#23427;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411; (PTMs) &#30340;&#35268;&#27169;&#32473;&#35843;&#25972;&#19979;&#28216;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#20840;&#21442;&#25968;&#24494;&#35843;&#28041;&#21450;&#39640;&#26114;&#30340;&#20248;&#21270;&#24320;&#38144;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#20063;&#31216;&#20026; "delta &#35843;&#25972;"&#65292;&#21363;&#20165;&#26356;&#26032;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#31216;&#20026; "delta &#27169;&#22359;"&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#24178;&#27169;&#22411;&#30340;&#21442;&#25968;&#22266;&#23450;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#23454;&#29616;&#30452;&#25509;&#20462;&#25913;&#20027;&#24178; PTMs &#30340;&#20195;&#30721;&#65292;&#24182;&#20026;&#27599;&#20010; PTM &#30828;&#32534;&#30721;&#29305;&#23450;&#30340; delta &#35843;&#25972;&#26041;&#27861;&#65292;delta &#35843;&#25972;&#30340;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; OpenDelta&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#21508;&#31181; delta &#35843;&#25972;&#26041;&#27861;&#30340;&#21363;&#25554;&#21363;&#29992;&#23454;&#29616;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26032;&#25216;&#26415;&#28040;&#38500;&#20102;&#20462;&#25913;&#20027;&#24178; PTMs &#20195;&#30721;&#30340;&#38656;&#27714;&#65292;&#20351; OpenDelta &#21487;&#20197;&#19982;&#19981;&#21516;&#30340;&#12289;&#29978;&#33267;&#26159;&#26032;&#30340; PTMs &#20860;&#23481;&#12290;OpenDelta &#30340;&#35774;&#35745;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as "delta tuning", which updates only a small subset of parameters, known as "delta modules", while keeping the backbone model's parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone PTMs and hard-code specific delta tuning methods for each PTM. In this paper, we present OpenDelta, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone PTMs' code, making OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to be simple, mo
&lt;/p&gt;</description></item><item><title>DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.03067</link><description>&lt;p&gt;
DeepOnto: &#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03067
&lt;/p&gt;
&lt;p&gt;
DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#22312;&#26412;&#20307;&#24037;&#31243;&#20013;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;PyTorch&#21644;Tensorflow&#20027;&#35201;&#26159;&#20026;Python&#24320;&#21457;&#30340;&#65292;&#32780;&#24191;&#27867;&#20351;&#29992;&#30340;&#26412;&#20307;API&#65288;&#22914;OWL API&#21644;Jena&#65289;&#20027;&#35201;&#26159;&#22522;&#20110;Java&#30340;&#12290;&#20026;&#20102;&#26041;&#20415;&#26080;&#32541;&#38598;&#25104;&#36825;&#20123;&#26694;&#26550;&#21644;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deeponto&#65292;&#19968;&#20010;&#19987;&#20026;&#26412;&#20307;&#24037;&#31243;&#35774;&#35745;&#30340;Python&#21253;&#12290;&#35813;&#21253;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#35748;&#21487;&#21644;&#21487;&#38752;&#30340;OWL API&#30340;&#26680;&#24515;&#26412;&#20307;&#22788;&#29702;&#27169;&#22359;&#65292;&#20197;&#26356;&#8220;Pythonic&#8221;&#30340;&#26041;&#24335;&#23553;&#35013;&#20854;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#25193;&#23637;&#20854;&#21151;&#33021;&#20197;&#21253;&#25324;&#20854;&#20182;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#35821;&#35328;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#25237;&#24433;&#31561;&#12290;&#22522;&#20110;&#36825;&#20010;&#27169;&#22359;&#65292;Deeponto&#25552;&#20379;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#21508;&#31181;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#20363;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03056</link><description>&lt;p&gt;
&#27867;&#21270;&#21453;&#21521;&#20256;&#25773;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#35299;&#37322;&#20013;&#65292;&#27867;&#21270;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#26799;&#24230;&#22270;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#20316;&#32773;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#21644;&#24212;&#29992;&#20110;BERT&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#27169;&#22411;&#36755;&#20986;&#23545;&#36755;&#20837;&#30340;&#26799;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25351;&#31034;&#21738;&#20123;&#36755;&#20837;&#29305;&#24449;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#23545;&#27169;&#22411;&#26412;&#36523;&#30340;&#20869;&#37096;&#24037;&#20316;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#26799;&#24230;&#35745;&#31639;&#26159;&#20351;&#29992;&#21322;&#29615;&#30340;&#26356;&#19968;&#33324;&#24418;&#24335;&#30340;&#29305;&#20363;&#12290;&#36825;&#31181;&#35266;&#23519;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#27867;&#21270;&#65292;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#22270;&#30340;&#20854;&#20182;&#21487;&#35299;&#37322;&#32479;&#35745;&#25968;&#25454;&#65292;&#20363;&#22914;&#26368;&#39640;&#21152;&#26435;&#36335;&#24452;&#21644;&#29109;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#27867;&#21270;&#31639;&#27861;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30740;&#31350;BERT&#22312;&#20027;&#35859;&#25968;&#19968;&#33268;&#24615;&#20219;&#21153;&#65288;SVA&#65289;&#19978;&#30340;&#34892;&#20026;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#27169;&#22411;&#32452;&#20214;&#19978;&#36890;&#36807;&#30340;&#26799;&#24230;&#27969;&#37327;&#21453;&#26144;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its impor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;Transformer&#36827;&#34892;&#33402;&#26415;&#35748;&#35777;&#65292;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;EfficientNet&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#33402;&#26415;&#21697;&#35748;&#35777;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03039</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;Transformer&#36827;&#34892;&#33402;&#26415;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Art Authentication with Vision Transformers. (arXiv:2307.03039v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;Transformer&#36827;&#34892;&#33402;&#26415;&#35748;&#35777;&#65292;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;EfficientNet&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#33402;&#26415;&#21697;&#35748;&#35777;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#21518;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#12290;&#35270;&#35273;Transformer&#24050;&#22312;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#20013;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#12290;&#23613;&#31649;&#22823;&#37327;&#30740;&#31350;&#24050;&#35777;&#26126;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33402;&#26415;&#24402;&#23646;&#21644;&#33402;&#26415;&#35748;&#35777;&#20219;&#21153;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#35270;&#35273;Transformer&#22312;&#33402;&#26415;&#35748;&#35777;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#33402;&#26415;&#21697;&#35748;&#35777;&#30340;&#21487;&#38752;&#24615;&#12290;&#20351;&#29992;&#30001;Vincent van Gogh&#30495;&#36857;&#21644;&#20004;&#20010;&#23545;&#27604;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#31934;&#24515;&#32534;&#21046;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23558;Swin Transformer&#30340;&#33402;&#26415;&#35748;&#35777;&#24615;&#33021;&#19982;EfficientNet&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#27169;&#20223;&#21697;&#21644;&#31867;&#20284;van Gogh&#39118;&#26684;&#30011;&#23478;&#20316;&#21697;&#30340;&#26631;&#20934;&#23545;&#27604;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;EfficientNet&#22312;&#25972;&#20307;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformers, initially developed for language, have been successfully applied to visual tasks. Vision Transformers have been shown to push the state-of-the-art in a wide range of tasks, including image classification, object detection, and semantic segmentation. While ample research has shown promising results in art attribution and art authentication tasks using Convolutional Neural Networks, this paper examines if the superiority of Vision Transformers extends to art authentication, improving, thus, the reliability of computer-based authentication of artworks. Using a carefully compiled dataset of authentic paintings by Vincent van Gogh and two contrast datasets, we compare the art authentication performances of Swin Transformers with those of EfficientNet. Using a standard contrast set containing imitations and proxies (works by painters with styles closely related to van Gogh), we find that EfficientNet achieves the best performance overall. With a contrast set th
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#27169;&#22411;&#65288;SNCBFs&#65289;&#30340;&#32452;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#39044;&#27979;&#38556;&#30861;&#29289;&#30340;&#31354;&#38388;&#20132;&#20114;&#27169;&#24335;&#65292;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#38556;&#30861;&#29289;&#36991;&#38556;&#12290;</title><link>http://arxiv.org/abs/2307.03015</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21160;&#24577;&#38556;&#30861;&#29289;&#36991;&#38556;&#30340;&#39034;&#24207;&#31070;&#32463;&#23631;&#38556;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sequential Neural Barriers for Scalable Dynamic Obstacle Avoidance. (arXiv:2307.03015v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03015
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#27169;&#22411;&#65288;SNCBFs&#65289;&#30340;&#32452;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#39044;&#27979;&#38556;&#30861;&#29289;&#30340;&#31354;&#38388;&#20132;&#20114;&#27169;&#24335;&#65292;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#38556;&#30861;&#29289;&#36991;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25193;&#22823;&#26426;&#22120;&#20154;&#22260;&#32469;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#23548;&#33322;&#26102;&#65292;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#38556;&#30861;&#29289;&#30340;&#22797;&#26434;&#20132;&#20114;&#21160;&#21147;&#23398;&#24456;&#38590;&#36890;&#36807;&#20998;&#26512;&#24314;&#27169;&#65292;&#32780;&#35268;&#21010;&#21644;&#25511;&#21046;&#30340;&#22797;&#26434;&#24615;&#38543;&#38556;&#30861;&#29289;&#25968;&#37327;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#23588;&#20026;&#26377;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23545;&#20998;&#24067;&#28418;&#31227;&#25935;&#24863;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#38556;&#30861;&#29289;&#23494;&#24230;&#19979;&#35757;&#32451;&#21644;&#25512;&#24191;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39034;&#24207;&#31070;&#32463;&#25511;&#21046;&#23631;&#38556;&#27169;&#22411;&#65288;SNCBFs&#65289;&#30340;&#32452;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#37325;&#35201;&#35266;&#23519;&#65306;&#22810;&#20010;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#31354;&#38388;&#20132;&#20114;&#27169;&#24335;&#21487;&#20197;&#36890;&#36807;&#27599;&#20010;&#38556;&#30861;&#29289;&#30340;&#29366;&#24577;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#35299;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#20998;&#35299;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20165;&#38024;&#23545;&#23569;&#37327;&#38556;&#30861;&#29289;&#35757;&#32451;&#30340;&#25511;&#21046;&#31574;&#30053;&#25512;&#24191;&#21040;&#38556;&#30861;&#29289;&#23494;&#24230;&#21487;&#20197;&#26159;100&#20493;&#30340;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are two major challenges for scaling up robot navigation around dynamic obstacles: the complex interaction dynamics of the obstacles can be hard to model analytically, and the complexity of planning and control grows exponentially in the number of obstacles. Data-driven and learning-based methods are thus particularly valuable in this context. However, data-driven methods are sensitive to distribution drift, making it hard to train and generalize learned models across different obstacle densities. We propose a novel method for compositional learning of Sequential Neural Control Barrier models (SNCBFs) to achieve scalability. Our approach exploits an important observation: the spatial interaction patterns of multiple dynamic obstacles can be decomposed and predicted through temporal sequences of states for each obstacle. Through decomposition, we can generalize control policies trained only with a small number of obstacles, to environments where the obstacle density can be 100x hi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#27969;&#31243;&#29992;&#20110;&#20248;&#21270;&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#20351;&#29992;&#35299;&#21078;&#29305;&#24449;&#21644;&#36845;&#20195;&#23398;&#20064;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22522;&#20110;&#23039;&#21183;&#30340;&#27963;&#21160;&#35782;&#21035;&#30340;&#20415;&#23452;&#19988;&#31283;&#20581;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03007</link><description>&lt;p&gt;
&#20351;&#29992;&#35299;&#21078;&#29305;&#24449;&#21644;&#36845;&#20195;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Optimization of Hand Pose Estimation using Anatomical Features and Iterative Learning. (arXiv:2307.03007v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#27969;&#31243;&#29992;&#20110;&#20248;&#21270;&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#65292;&#36890;&#36807;&#20351;&#29992;&#35299;&#21078;&#29305;&#24449;&#21644;&#36845;&#20195;&#23398;&#20064;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22522;&#20110;&#23039;&#21183;&#30340;&#27963;&#21160;&#35782;&#21035;&#30340;&#20415;&#23452;&#19988;&#31283;&#20581;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#35013;&#37197;&#24037;&#20154;&#22312;&#24037;&#20316;&#20013;&#38754;&#20020;&#30528;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;&#20154;&#24615;&#21270;&#36741;&#21161;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#20316;&#20026;&#23454;&#29616;&#25216;&#26415;&#30340;&#29289;&#20307;&#35782;&#21035;&#21364;&#38459;&#30861;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#22797;&#26434;&#30340;&#20154;&#24615;&#21270;&#35774;&#35745;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#25163;&#21183;&#23039;&#21183;&#30340;&#27963;&#21160;&#35782;&#21035;&#22312;&#22797;&#26434;&#20351;&#29992;&#24773;&#26223;&#19979;&#65288;&#22914;&#20329;&#25140;&#25163;&#22871;&#65289;&#20013;&#23384;&#22312;&#23039;&#21183;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#23558;&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#36866;&#24212;&#29305;&#23450;&#30340;&#20351;&#29992;&#22330;&#26223;&#65292;&#24182;&#23613;&#37327;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#12290;&#36825;&#20351;&#24471;&#22522;&#20110;&#25163;&#21183;&#23039;&#21183;&#30340;&#27963;&#21160;&#35782;&#21035;&#26356;&#21152;&#24265;&#20215;&#21644;&#31283;&#20581;&#12290;&#35813;&#27969;&#31243;&#21253;&#25324;&#19968;&#20010;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#24191;&#20041;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25163;&#21183;&#23039;&#21183;&#20272;&#35745;&#65292;&#31354;&#38388;&#21644;&#26102;&#38388;&#28388;&#27874;&#20197;&#32771;&#34385;&#25163;&#37096;&#35299;&#21078;&#32422;&#26463;&#65292;&#20197;&#21450;&#19968;&#20010;&#37325;&#26032;&#35757;&#32451;&#30340;&#27493;&#39588;&#26469;&#25913;&#36827;&#27169;&#22411;&#12290;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#21644;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#21442;&#25968;&#32452;&#21512;&#12290;&#28982;&#21518;&#23558;&#26368;&#20339;&#21442;&#25968;&#21644;&#27169;&#22411;&#32452;&#21512;&#24212;&#29992;&#21040;&#26410;&#26631;&#35760;&#35270;&#39057;&#26679;&#26412;&#19978;&#65292;&#26469;&#39564;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manual assembly workers face increasing complexity in their work. Human-centered assistance systems could help, but object recognition as an enabling technology hinders sophisticated human-centered design of these systems. At the same time, activity recognition based on hand poses suffers from poor pose estimation in complex usage scenarios, such as wearing gloves. This paper presents a self-supervised pipeline for adapting hand pose estimation to specific use cases with minimal human interaction. This enables cheap and robust hand posebased activity recognition. The pipeline consists of a general machine learning model for hand pose estimation trained on a generalized dataset, spatial and temporal filtering to account for anatomical constraints of the hand, and a retraining step to improve the model. Different parameter combinations are evaluated on a publicly available and annotated dataset. The best parameter and model combination is then applied to unlabelled videos from a manual a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#23548;&#33322;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#36523;&#20221;&#20998;&#31867;&#22120;&#20316;&#20026;&#23548;&#21521;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#28145;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32780;&#23548;&#33268;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02984</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#34892;&#36208;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;
&lt;/p&gt;
&lt;p&gt;
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications. (arXiv:2307.02984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#23548;&#33322;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#36523;&#20221;&#20998;&#31867;&#22120;&#20316;&#20026;&#23548;&#21521;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#28145;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32780;&#23548;&#33268;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#23637;&#31034;&#20102;&#23427;&#20204;&#29983;&#25104;&#19982;&#30446;&#26631;&#20998;&#24067;&#21305;&#37197;&#30340;&#21512;&#25104;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20174;&#38544;&#31169;&#35282;&#24230;&#26469;&#30475;&#65292;&#20351;&#29992;GAN&#20316;&#20026;&#25968;&#25454;&#20849;&#20139;&#30340;&#20195;&#29702;&#19981;&#26159;&#19968;&#20010;&#23433;&#20840;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23884;&#20837;&#25509;&#36817;&#30495;&#23454;&#26679;&#26412;&#30340;&#21103;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21463;k-&#21311;&#21517;&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23545;&#26679;&#26412;&#36827;&#34892;&#32858;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20250;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#23548;&#33322;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#28145;&#24230;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36741;&#21161;&#36523;&#20221;&#20998;&#31867;&#22120;&#20316;&#20026;&#23548;&#21521;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#38750;&#32447;&#24615;&#22320;&#22312;&#28857;&#20043;&#38388;&#31227;&#21160;&#65292;&#26368;&#23567;&#21270;&#19982;&#25509;&#36817;&#30495;&#23454;&#26679;&#26412;&#30340;&#21103;&#26412;&#21457;&#29983;&#20914;&#31361;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#38543;&#26426;&#28857;&#23545;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#36798;&#21040;&#20102;&#21516;&#26102;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#21644;&#26377;&#25928;&#35757;&#32451;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have demonstrated their ability to generate synthetic samples that match a target distribution. However, from a privacy perspective, using GANs as a proxy for data sharing is not a safe solution, as they tend to embed near-duplicates of real samples in the latent space. Recent works, inspired by k-anonymity principles, address this issue through sample aggregation in the latent space, with the drawback of reducing the dataset by a factor of k. Our work aims to mitigate this problem by proposing a latent space navigation strategy able to generate diverse synthetic samples that may support effective training of deep models, while addressing privacy concerns in a principled way. Our approach leverages an auxiliary identity classifier as a guide to non-linearly walk between points in the latent space, minimizing the risk of collision with near-duplicates of real samples. We empirically demonstrate that, given any random pair of points in the latent sp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#21270;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;&#24050;&#26377;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#19978;&#29983;&#25104;&#30340;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#35937;-&#25991;&#26412;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#24230;&#37327;&#26469;&#20248;&#21270;&#36328;&#25991;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.02971</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
On the Cultural Gap in Text-to-Image Generation. (arXiv:2307.02971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#21270;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;&#24050;&#26377;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#19978;&#29983;&#25104;&#30340;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#35937;-&#25991;&#26412;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#24230;&#37327;&#26469;&#20248;&#21270;&#36328;&#25991;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#24847;&#22806;&#21453;&#26144;&#20102;&#25991;&#21270;&#24046;&#36317;&#65292;&#24403;&#36755;&#20837;&#25991;&#26412;&#30340;&#25991;&#21270;&#20803;&#32032;&#24456;&#23569;&#20986;&#29616;&#22312;&#35757;&#32451;&#38598;&#20013;&#26102;&#65292;&#36825;&#34920;&#26126;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#23613;&#31649;&#21508;&#31181;T2I&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#20294;&#26159;&#38543;&#24847;&#30340;&#20363;&#23376;&#65292;&#20294;&#26159;&#30446;&#21069;&#27809;&#26377;&#19968;&#20010;&#22522;&#20934;&#26469;&#31995;&#32479;&#35780;&#20272;T2I&#27169;&#22411;&#29983;&#25104;&#36328;&#25991;&#21270;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#32508;&#21512;&#35780;&#20272;&#26631;&#20934;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#21270;&#65288;C3&#65289;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#30446;&#26631;&#25991;&#21270;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#22312;C3&#22522;&#20934;&#19978;&#30001;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#29305;&#23450;&#30340;&#25991;&#21270;&#23545;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#23545;&#35937;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#24230;&#37327;&#65292;&#29992;&#20110;&#36807;&#28388;&#30446;&#26631;&#25991;&#21270;&#20013;&#30340;&#24494;&#35843;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#36328;&#25991;&#21270;&#33021;&#21147;&#30340;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
One challenge in text-to-image (T2I) generation is the inadvertent reflection of culture gaps present in the training data, which signifies the disparity in generated image quality when the cultural elements of the input text are rarely collected in the training set. Although various T2I models have shown impressive but arbitrary examples, there is no benchmark to systematically evaluate a T2I model's ability to generate cross-cultural images. To bridge the gap, we propose a Challenging Cross-Cultural (C3) benchmark with comprehensive evaluation criteria, which can assess how well-suited a model is to a target culture. By analyzing the flawed images generated by the Stable Diffusion model on the C3 benchmark, we find that the model often fails to generate certain cultural objects. Accordingly, we propose a novel multi-modal metric that considers object-text alignment to filter the fine-tuning data in the target culture, which is used to fine-tune a T2I model to improve cross-cultural g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#23454;&#20540;&#35266;&#27979;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#22810;&#23618;&#20107;&#20214;&#39537;&#21160;&#32858;&#31867;&#12289;&#26102;&#38388;&#24046;&#20998;&#35823;&#24046;&#35843;&#21046;&#21644;&#36164;&#26684;&#30165;&#36857;&#31561;&#26041;&#27861;&#65292;&#24182;&#22312;&#32463;&#20856;RL&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#34920;&#26684;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.02947</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#23454;&#20540;&#35266;&#27979;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations. (arXiv:2307.02947v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#23454;&#20540;&#35266;&#27979;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#22810;&#23618;&#20107;&#20214;&#39537;&#21160;&#32858;&#31867;&#12289;&#26102;&#38388;&#24046;&#20998;&#35823;&#24046;&#35843;&#21046;&#21644;&#36164;&#26684;&#30165;&#36857;&#31561;&#26041;&#27861;&#65292;&#24182;&#22312;&#32463;&#20856;RL&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#34920;&#26684;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20197;&#39640;&#25928;&#19988;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#24335;&#23454;&#29616;RL&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#20540;&#35266;&#27979;&#30340;RL&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#22810;&#23618;&#20107;&#20214;&#39537;&#21160;&#32858;&#31867;&#65292;&#22686;&#21152;&#20102;&#26102;&#38388;&#24046;&#20998;&#65288;TD&#65289;&#35823;&#24046;&#35843;&#21046;&#21644;&#36164;&#26684;&#30165;&#36857;, &#24182;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#25913;&#36827;&#12290;&#28040;&#34701;&#30740;&#31350;&#35777;&#23454;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#32463;&#20856;RL&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#19981;&#26029;&#20248;&#20110;&#34920;&#26684;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#21457;&#29616;&#20102;&#31283;&#23450;&#30340;&#25511;&#21046;&#31574;&#30053;&#65306;&#23665;&#36710;&#12289;&#20498;&#31435;&#25670;&#21644;&#25670;&#33218;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#25240;&#20013;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) provides a powerful framework for decision-making in complex environments. However, implementing RL in hardware-efficient and bio-inspired ways remains a challenge. This paper presents a novel Spiking Neural Network (SNN) architecture for solving RL problems with real-valued observations. The proposed model incorporates multi-layered event-based clustering, with the addition of Temporal Difference (TD)-error modulation and eligibility traces, building upon prior work. An ablation study confirms the significant impact of these components on the proposed model's performance. A tabular actor-critic algorithm with eligibility traces and a state-of-the-art Proximal Policy Optimization (PPO) algorithm are used as benchmarks. Our network consistently outperforms the tabular approach and successfully discovers stable control policies on classic RL environments: mountain car, cart-pole, and acrobot. The proposed model offers an appealing trade-off in terms of computa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21069;&#39304;&#22810;&#27169;&#24577;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30340;&#24314;&#35758;&#26469;&#23454;&#26102;&#27604;&#36739;&#26144;&#23556;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#29992;&#25143;&#22312;&#25511;&#21046;&#26426;&#22120;&#20154;&#33218;&#26102;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;</title><link>http://arxiv.org/abs/2307.02933</link><description>&lt;p&gt;
&#26102;&#38388;&#21644;&#31354;&#38388;&#65306;&#21487;&#29992;&#30340;&#36741;&#21161;&#26426;&#22120;&#20154;&#33218;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
In Time and Space: Towards Usable Adaptive Control for Assistive Robotic Arms. (arXiv:2307.02933v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21069;&#39304;&#22810;&#27169;&#24577;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30340;&#24314;&#35758;&#26469;&#23454;&#26102;&#27604;&#36739;&#26144;&#23556;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#29992;&#25143;&#22312;&#25511;&#21046;&#26426;&#22120;&#20154;&#33218;&#26102;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#20154;&#33218;&#65292;&#22312;&#19982;&#20154;&#31867;&#36827;&#34892;&#23494;&#20999;&#21512;&#20316;&#30340;&#24773;&#20917;&#19979;&#36234;&#26469;&#36234;&#39057;&#32321;&#22320;&#37096;&#32626;&#65292;&#20363;&#22914;&#22312;&#21046;&#36896;&#25110;&#23478;&#24237;&#25252;&#29702;&#29615;&#22659;&#20013;&#12290;&#36825;&#20123;&#26426;&#22120;&#20154;&#33218;&#38656;&#35201;&#29992;&#25143;&#25511;&#21046;&#22810;&#20010;&#33258;&#30001;&#24230;&#65288;DoFs&#65289;&#26469;&#25191;&#34892;&#20219;&#21153;&#65292;&#20027;&#35201;&#28041;&#21450;&#25235;&#21462;&#21644;&#25805;&#20316;&#29289;&#20307;&#12290;&#26631;&#20934;&#36755;&#20837;&#35774;&#22791;&#20027;&#35201;&#20855;&#26377;&#20004;&#20010;DoFs&#65292;&#38656;&#35201;&#32791;&#26102;&#19988;&#35748;&#30693;&#36127;&#33655;&#22823;&#30340;&#27169;&#24335;&#20999;&#25442;&#26469;&#36873;&#25321;&#21333;&#20010;DoFs&#12290;&#29616;&#20195;&#33258;&#36866;&#24212;DoF&#26144;&#23556;&#25511;&#21046;&#65288;ADMCs&#65289;&#24050;&#32463;&#26174;&#31034;&#21487;&#20197;&#20943;&#23569;&#24517;&#35201;&#30340;&#27169;&#24335;&#20999;&#25442;&#27425;&#25968;&#65292;&#20294;&#23578;&#26410;&#33021;&#26174;&#33879;&#38477;&#20302;&#24863;&#30693;&#24037;&#20316;&#36127;&#33655;&#12290;&#29992;&#25143;&#20173;&#28982;&#25215;&#25285;&#23558;&#25277;&#35937;&#27169;&#24335;&#20999;&#25442;&#32435;&#20837;&#24037;&#20316;&#27969;&#31243;&#30340;&#24515;&#29702;&#36127;&#25285;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#21069;&#39304;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#20351;&#29992;ADMC&#30340;&#26356;&#26032;&#24314;&#35758;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#23454;&#26102;&#35270;&#35273;&#27604;&#36739;&#24403;&#21069;&#21644;&#24314;&#35758;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic solutions, in particular robotic arms, are becoming more frequently deployed for close collaboration with humans, for example in manufacturing or domestic care environments. These robotic arms require the user to control several Degrees-of-Freedom (DoFs) to perform tasks, primarily involving grasping and manipulating objects. Standard input devices predominantly have two DoFs, requiring time-consuming and cognitively demanding mode switches to select individual DoFs. Contemporary Adaptive DoF Mapping Controls (ADMCs) have shown to decrease the necessary number of mode switches but were up to now not able to significantly reduce the perceived workload. Users still bear the mental workload of incorporating abstract mode switching into their workflow. We address this by providing feed-forward multimodal feedback using updated recommendations of ADMC, allowing users to visually compare the current and the suggested mapping in real-time. We contrast the effectiveness of two new appr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LEA&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#39640;&#23545;&#25171;&#23383;&#38169;&#35823;&#30340;&#21477;&#23376;&#30456;&#20284;&#24615;&#40065;&#26834;&#24615;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#24341;&#20837;&#35789;&#27719;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#25991;&#26412;&#22122;&#38899;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#25171;&#23383;&#38169;&#35823;&#23548;&#33268;&#30340;&#26631;&#35760;&#20998;&#24067;&#20559;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.02912</link><description>&lt;p&gt;
LEA: &#20351;&#29992;&#35789;&#27719;&#27880;&#24847;&#20559;&#24046;&#25552;&#39640;&#23545;&#25171;&#23383;&#38169;&#35823;&#30340;&#21477;&#23376;&#30456;&#20284;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias. (arXiv:2307.02912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LEA&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#39640;&#23545;&#25171;&#23383;&#38169;&#35823;&#30340;&#21477;&#23376;&#30456;&#20284;&#24615;&#40065;&#26834;&#24615;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#24341;&#20837;&#35789;&#27719;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#25991;&#26412;&#22122;&#38899;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#25171;&#23383;&#38169;&#35823;&#23548;&#33268;&#30340;&#26631;&#35760;&#20998;&#24067;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22122;&#38899;&#65292;&#22914;&#25171;&#23383;&#38169;&#35823;&#25110;&#32553;&#20889;&#65292;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#20250;&#23545;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#32431;&#21464;&#21387;&#22120;&#27169;&#22411;&#36896;&#25104;&#24809;&#32602;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20063;&#36866;&#29992;&#20110;&#21477;&#23376;&#30456;&#20284;&#24615;&#65292;&#36825;&#26159;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#27604;&#22914;&#21305;&#37197;&#12289;&#26816;&#32034;&#25110;&#37322;&#20041;&#12290;&#21487;&#20197;&#20351;&#29992;&#20132;&#21449;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#21477;&#23376;&#30456;&#20284;&#24615;&#65292;&#20854;&#20013;&#20004;&#20010;&#21477;&#23376;&#22312;&#36755;&#20837;&#20013;&#36830;&#25509;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20043;&#21069;&#35299;&#20915;&#22122;&#38899;&#38382;&#39064;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#19982;&#35757;&#32451;&#26679;&#26412;&#30456;&#20284;&#30340;&#25439;&#22351;&#26679;&#26412;&#26102;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#25171;&#23383;&#38169;&#35823;&#24341;&#36215;&#30340;&#26631;&#35760;&#20998;&#24067;&#20559;&#31227;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#35789;&#27719;&#24863;&#30693;&#27880;&#24847;&#27169;&#22359;&#65288;LEA&#65289;&#26469;&#35299;&#20915;&#25991;&#26412;&#22122;&#38899;&#38382;&#39064;&#65292;&#35813;&#27169;&#22359;&#22312;&#20004;&#20010;&#21477;&#23376;&#20013;&#30340;&#35789;&#20043;&#38388;&#24341;&#20837;&#20102;&#35789;&#27719;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;token&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual noise, such as typos or abbreviations, is a well-known issue that penalizes vanilla Transformers for most downstream tasks. We show that this is also the case for sentence similarity, a fundamental task in multiple domains, e.g. matching, retrieval or paraphrasing. Sentence similarity can be approached using cross-encoders, where the two sentences are concatenated in the input allowing the model to exploit the inter-relations between them. Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when dealing with corrupted samples that are similar to the ones used for training. However, all these methods still suffer from the token distribution shift induced by typos. In this work, we propose to tackle textual noise by equipping cross-encoders with a novel LExical-aware Attention module (LEA) that incorporates lexical similarities between words in both sentences. By using raw text similarities, our approach avoids the to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#21548;&#31471;&#21040;&#31471;&#22810;&#36890;&#36947;&#35821;&#38899;&#20998;&#31163;&#12289;&#21435;&#28151;&#21709;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#65292;&#36890;&#36807;&#20943;&#23567;&#21069;&#21518;&#31471;&#32452;&#20214;&#20043;&#38388;&#30340;&#35823;&#24046;&#25104;&#26412;&#19981;&#21305;&#37197;&#26469;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02909</link><description>&lt;p&gt;
&#35270;&#21548;&#31471;&#21040;&#31471;&#22810;&#36890;&#36947;&#35821;&#38899;&#20998;&#31163;&#12289;&#21435;&#28151;&#21709;&#21644;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation and Recognition. (arXiv:2307.02909v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#21548;&#31471;&#21040;&#31471;&#22810;&#36890;&#36947;&#35821;&#38899;&#20998;&#31163;&#12289;&#21435;&#28151;&#21709;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#65292;&#36890;&#36807;&#20943;&#23567;&#21069;&#21518;&#31471;&#32452;&#20214;&#20043;&#38388;&#30340;&#35823;&#24046;&#25104;&#26412;&#19981;&#21305;&#37197;&#26469;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35782;&#21035;&#21253;&#21547;&#37325;&#21472;&#21457;&#35328;&#32773;&#12289;&#22122;&#22768;&#21644;&#28151;&#21709;&#30340;&#28151;&#38899;&#35821;&#38899;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#21548;&#22810;&#36890;&#36947;&#35821;&#38899;&#20998;&#31163;&#12289;&#21435;&#28151;&#21709;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#23558;&#35270;&#35273;&#20449;&#24687;&#34701;&#20837;&#21040;&#25152;&#26377;&#31995;&#32479;&#32452;&#20214;&#20013;&#65292;&#22522;&#20110;&#35270;&#35273;&#27169;&#24577;&#19982;&#22768;&#23398;&#20449;&#21495;&#22833;&#30495;&#20043;&#38388;&#30340;&#19981;&#21464;&#24615;&#12290;&#35270;&#39057;&#36755;&#20837;&#22312;&#22522;&#20110;&#25513;&#34109;&#30340;MVDR&#35821;&#38899;&#20998;&#31163;&#12289;&#22522;&#20110;DNN-WPE&#25110;&#20809;&#35889;&#26144;&#23556;&#65288;SpecM&#65289;&#30340;&#35821;&#38899;&#21435;&#28151;&#21709;&#21069;&#31471;&#21644;Conformer ASR&#21518;&#31471;&#20013;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#25345;&#32493;&#30340;&#35777;&#26126;&#12290;&#30740;&#31350;&#20102;&#25191;&#34892;&#35821;&#38899;&#20998;&#31163;&#21644;&#21435;&#28151;&#21709;&#30340;&#35270;&#21548;&#19968;&#20307;&#21270;&#21069;&#31471;&#26550;&#26500;&#65292;&#36890;&#36807;&#22522;&#20110;&#25513;&#34109;&#30340;WPD&#20197;&#27969;&#27700;&#32447;&#25110;&#32852;&#21512;&#26041;&#24335;&#36827;&#34892;&#12290;&#36890;&#36807;&#31471;&#21040;&#31471;&#32852;&#21512;&#24494;&#35843;&#65292;&#20351;&#29992;ASR&#25104;&#26412;&#20989;&#25968;&#21333;&#29420;&#25110;&#19982;&#20854;&#32447;&#24615;&#25554;&#20540;&#65292;&#26368;&#23567;&#21270;&#35821;&#38899;&#22686;&#24378;&#21069;&#31471;&#21644;ASR&#21518;&#31471;&#32452;&#20214;&#20043;&#38388;&#30340;&#35823;&#24046;&#25104;&#26412;&#19981;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate recognition of cocktail party speech containing overlapping speakers, noise and reverberation remains a highly challenging task to date. Motivated by the invariance of visual modality to acoustic signal corruption, an audio-visual multi-channel speech separation, dereverberation and recognition approach featuring a full incorporation of visual information into all system components is proposed in this paper. The efficacy of the video input is consistently demonstrated in mask-based MVDR speech separation, DNN-WPE or spectral mapping (SpecM) based speech dereverberation front-end and Conformer ASR back-end. Audio-visual integrated front-end architectures performing speech separation and dereverberation in a pipelined or joint fashion via mask-based WPD are investigated. The error cost mismatch between the speech enhancement front-end and ASR back-end components is minimized by end-to-end jointly fine-tuning using either the ASR cost function alone, or its interpolation with the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaBE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.02891</link><description>&lt;p&gt;
BaBE:&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#22686;&#24378;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables. (arXiv:2307.02891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaBE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#20010;&#32676;&#20307;&#20043;&#38388;&#19981;&#20844;&#24179;&#27495;&#35270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#30340;BaBE (Bayesian Bias Elimination)&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;&#30340;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data, i.e., it is a latent variable. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each grou
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#27861;&#24459;&#20998;&#31867;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20351;&#29992;SetFit&#24494;&#35843;&#30340;&#27169;&#22411;&#27604;&#26222;&#36890;&#24494;&#35843;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;LIME&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#21319;&#23545;&#27491;&#38754;&#21644;&#36127;&#38754;&#29305;&#24449;&#30340;&#35748;&#30693;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#27861;&#24459;&#19978;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#24182;&#23545;&#20998;&#31867;&#32467;&#26524;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02882</link><description>&lt;p&gt;
&#23545;&#27604;&#23601;&#26159;&#20320;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Contrast Is All You Need. (arXiv:2307.02882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02882
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#27861;&#24459;&#20998;&#31867;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20351;&#29992;SetFit&#24494;&#35843;&#30340;&#27169;&#22411;&#27604;&#26222;&#36890;&#24494;&#35843;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;LIME&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#21319;&#23545;&#27491;&#38754;&#21644;&#36127;&#38754;&#29305;&#24449;&#30340;&#35748;&#30693;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#27861;&#24459;&#19978;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#24182;&#23545;&#20998;&#31867;&#32467;&#26524;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#31232;&#32570;&#30340;&#20998;&#31867;&#22330;&#26223;&#65292;&#20854;&#20013;&#21487;&#29992;&#30340;&#26631;&#35760;&#27861;&#24459;&#25968;&#25454;&#24456;&#23569;&#19988;&#19981;&#24179;&#34913;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#24494;&#35843;&#30446;&#26631;&#65306;SetFit&#65288;&#21477;&#23376;&#36716;&#25442;&#22120;&#24494;&#35843;&#65289;&#65292;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#35774;&#32622;&#65292;&#20197;&#21450;&#22312;&#27861;&#24459;&#26465;&#27454;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#26222;&#36890;&#24494;&#35843;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;LIME&#65288;&#23616;&#37096;&#21487;&#35299;&#37322;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#65289;&#27604;&#36739;&#20102;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#20197;&#26597;&#30475;&#21738;&#20123;&#29305;&#23450;&#29305;&#24449;&#23545;&#27169;&#22411;&#30340;&#20998;&#31867;&#20915;&#31574;&#26377;&#36129;&#29486;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#26222;&#36890;&#24494;&#35843;&#30456;&#27604;&#65292;&#20351;&#29992;SetFit&#30340;&#23545;&#27604;&#35774;&#32622;&#34920;&#29616;&#26356;&#22909;&#12290;LIME&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#21319;&#23545;&#27491;&#38754;&#21644;&#36127;&#38754;&#29305;&#24449;&#30340;&#35748;&#30693;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#27861;&#24459;&#19978;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#24182;&#23545;&#20998;&#31867;&#32467;&#26524;&#26377;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#23545;&#27604;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20284;&#20046;&#26356;&#33258;&#20449;&#22320;&#22522;&#20110;&#27861;&#24459;&#20449;&#24687;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we analyze data-scarce classification scenarios, where available labeled legal data is small and imbalanced, potentially hurting the quality of the results. We focused on two finetuning objectives; SetFit (Sentence Transformer Finetuning), a contrastive learning setup, and a vanilla finetuning setup on a legal provision classification task. Additionally, we compare the features that are extracted with LIME (Local Interpretable Model-agnostic Explanations) to see which particular features contributed to the model's classification decisions. The results show that a contrastive setup with SetFit performed better than vanilla finetuning while using a fraction of the training samples. LIME results show that the contrastive learning approach helps boost both positive and negative features which are legally informative and contribute to the classification results. Thus a model finetuned with a contrastive objective seems to base its decisions more confidently on legally informa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;MLOps&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#38081;&#36335;&#39046;&#22495;&#20013;&#25345;&#32493;&#24320;&#21457;&#21644;&#23433;&#20840;&#20445;&#35777;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#31995;&#32479;&#12290;&#35813;&#27969;&#31243;&#25972;&#21512;&#20102;&#31995;&#32479;&#24037;&#31243;&#12289;&#23433;&#20840;&#20445;&#35777;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#65292;&#35299;&#20915;&#20102;&#20877;&#29616;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#12289;&#21327;&#20316;&#24615;&#21644;&#25345;&#32493;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02867</link><description>&lt;p&gt;
&#22312;&#38081;&#36335;&#39046;&#22495;&#20013;&#25345;&#32493;&#24320;&#21457;&#21644;&#23433;&#20840;&#20445;&#35777;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;MLOps&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards a safe MLOps Process for the Continuous Development and Safety Assurance of ML-based Systems in the Railway Domain. (arXiv:2307.02867v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;MLOps&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#38081;&#36335;&#39046;&#22495;&#20013;&#25345;&#32493;&#24320;&#21457;&#21644;&#23433;&#20840;&#20445;&#35777;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#31995;&#32479;&#12290;&#35813;&#27969;&#31243;&#25972;&#21512;&#20102;&#31995;&#32479;&#24037;&#31243;&#12289;&#23433;&#20840;&#20445;&#35777;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#65292;&#35299;&#20915;&#20102;&#20877;&#29616;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#12289;&#21327;&#20316;&#24615;&#21644;&#25345;&#32493;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#21333;&#29420;&#24182;&#19981;&#36275;&#20197;&#23454;&#29616;&#38750;&#21463;&#38480;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#26080;&#20154;&#39550;&#39542;&#21015;&#36710;&#36816;&#34892;&#65288;&#31216;&#20026;GoA 4&#65289;&#12290;&#29616;&#20170;&#65292;&#25152;&#38656;&#30340;&#24863;&#30693;&#20219;&#21153;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23454;&#29616;&#65292;&#22240;&#27492;&#38656;&#35201;&#21487;&#38752;&#39640;&#25928;&#22320;&#24320;&#21457;&#21644;&#37096;&#32626;&#12290;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#20351;&#29992;&#19968;&#20010;&#23433;&#20840;&#30340;MLOps&#27969;&#31243;&#65292;&#29992;&#20110;&#35299;&#20915;&#25913;&#36827;&#20877;&#29616;&#24615;&#12289;&#21487;&#36861;&#28335;&#24615;&#12289;&#21327;&#20316;&#24615;&#21644;&#26080;&#20154;&#39550;&#39542;&#36816;&#33829;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#26465;&#20214;&#30340;&#25345;&#32493;&#36866;&#24212;&#24615;&#12290;MLOps&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#24320;&#21457;&#21644;&#25805;&#20316;&#65288;Ops&#65289;&#65292;&#22522;&#20110;&#36816;&#33829;&#21453;&#39304;&#23454;&#29616;&#39640;&#39057;&#36719;&#20214;&#21457;&#24067;&#21644;&#25345;&#32493;&#21019;&#26032;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22312;&#38081;&#36335;&#39046;&#22495;&#20013;&#25345;&#32493;&#24320;&#21457;&#21644;&#23433;&#20840;&#20445;&#35777;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;MLOps&#27969;&#31243;&#12290;&#23427;&#23558;&#31995;&#32479;&#24037;&#31243;&#12289;&#23433;&#20840;&#20445;&#35777;&#21644;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#34701;&#20837;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#27969;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional automation technologies alone are not sufficient to enable driverless operation of trains (called Grade of Automation (GoA) 4) on non-restricted infrastructure. The required perception tasks are nowadays realized using Machine Learning (ML) and thus need to be developed and deployed reliably and efficiently. One important aspect to achieve this is to use an MLOps process for tackling improved reproducibility, traceability, collaboration, and continuous adaptation of a driverless operation to changing conditions. MLOps mixes ML application development and operation (Ops) and enables high frequency software releases and continuous innovation based on the feedback from operations. In this paper, we outline a safe MLOps process for the continuous development and safety assurance of ML-based systems in the railway domain. It integrates system engineering, safety assurance, and the ML life-cycle in a comprehensive workflow. We present the individual stages of the process and thei
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#36827;&#21270;&#35843;&#20248;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02839</link><description>&lt;p&gt;
&#20351;&#29992;&#36827;&#21270;&#35843;&#20248;&#22686;&#24378;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation. (arXiv:2307.02839v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#36827;&#21270;&#35843;&#20248;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#26159;&#24773;&#25253;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#20840;&#38754;&#30340;&#20449;&#24687;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#22797;&#26434;&#30340;&#29616;&#23454;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#27169;&#22411;&#26412;&#36523;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#38480;&#21046;&#65292;&#20197;&#21450;&#25991;&#26412;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20351;&#24471;&#20934;&#30830;&#29983;&#25104;&#21487;&#38752;&#20449;&#24687;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#20174;&#26032;&#38395;&#27573;&#33853;&#20013;&#25552;&#21462;&#22810;&#20010;&#32467;&#26500;&#21270;&#20107;&#20214;&#27169;&#24335;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#36827;&#21270;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#24182;&#36873;&#25321;&#26368;&#36866;&#24212;&#30340;&#20107;&#20214;&#27169;&#24335;&#36755;&#20837;LLM&#29983;&#25104;&#26032;&#38395;&#25688;&#35201;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#22120;(NSG)&#26469;&#36873;&#25321;&#21644;&#36827;&#21270;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#24182;&#29983;&#25104;&#26032;&#38395;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
News summary generation is an important task in the field of intelligence analysis, which can provide accurate and comprehensive information to help people better understand and respond to complex real-world events. However, traditional news summary generation methods face some challenges, which are limited by the model itself and the amount of training data, as well as the influence of text noise, making it difficult to generate reliable information accurately. In this paper, we propose a new paradigm for news summary generation using LLM with powerful natural language understanding and generative capabilities. We use LLM to extract multiple structured event patterns from the events contained in news paragraphs, evolve the event pattern population with genetic algorithm, and select the most adaptive event pattern to input into the LLM to generate news summaries. A News Summary Generator (NSG) is designed to select and evolve the event pattern populations and generate news summaries. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#23558;&#21407;&#22987;&#38899;&#39057;&#25991;&#20214;&#36755;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#36129;&#29486;&#65292;&#24182;&#23558;&#20256;&#32479;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12289;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#29992;&#20110;&#23545;&#27604;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.02820</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#35780;&#20272;&#21407;&#22987;&#27874;&#24418;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Evaluating raw waveforms with deep learning frameworks for speech emotion recognition. (arXiv:2307.02820v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#23558;&#21407;&#22987;&#38899;&#39057;&#25991;&#20214;&#36755;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#36129;&#29486;&#65292;&#24182;&#23558;&#20256;&#32479;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12289;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#29992;&#20110;&#23545;&#27604;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26159;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23637;&#31034;&#21644;&#22788;&#29702;&#35821;&#38899;&#20449;&#21495;&#65292;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;&#21407;&#22987;&#38899;&#39057;&#25991;&#20214;&#30452;&#25509;&#36755;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#65292;&#21033;&#29992;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65306;EMO-DB&#12289;RAVDESS&#12289;TESS&#12289;CREMA&#12289;SAVEE&#21644;TESS+RAVDESS&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65288;&#22914;&#26757;&#23572;&#39057;&#35889;&#22270;&#12289;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65289;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12289;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#20915;&#31574;&#26641;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#22810;&#25968;&#25237;&#31080;&#21644;&#22534;&#21472;&#26041;&#27861;&#20316;&#20026;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition is a challenging task in speech processing field. For this reason, feature extraction process has a crucial importance to demonstrate and process the speech signals. In this work, we represent a model, which feeds raw audio files directly into the deep neural networks without any feature extraction stage for the recognition of emotions utilizing six different data sets, EMO-DB, RAVDESS, TESS, CREMA, SAVEE, and TESS+RAVDESS. To demonstrate the contribution of proposed model, the performance of traditional feature extraction techniques namely, mel-scale spectogram, mel-frequency cepstral coefficients, are blended with machine learning algorithms, ensemble learning methods, deep and hybrid deep learning techniques. Support vector machine, decision tree, naive Bayes, random forests models are evaluated as machine learning algorithms while majority voting and stacking methods are assessed as ensemble learning techniques. Moreover, convolutional neural networks, lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#35268;&#21017;&#35299;&#32852;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20102;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#35299;&#30721;&#22120;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.02798</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#35268;&#21017;&#35299;&#32852;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Domain Adaptive Medical Image Segmentation through Consistency Regularized Disentangled Contrastive Learning. (arXiv:2307.02798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#35268;&#21017;&#35299;&#32852;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20102;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#35299;&#30721;&#22120;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26159;&#32531;&#35299;&#39046;&#22495;&#20559;&#31227;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#29616;&#19981;&#21450;&#20854;&#30417;&#30563;&#23545;&#24212;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;SSDA&#65289;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#20854;&#20013;&#23545;&#19968;&#20123;&#26631;&#35760;&#30340;&#30446;&#26631;&#26679;&#26412;&#30340;&#35775;&#38382;&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#36866;&#24212;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#20869;&#23481;&#35299;&#32852;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#21644;&#20687;&#32032;&#32423;&#29305;&#24449;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#22312;&#33258;&#23398;&#20064;&#33539;&#24335;&#20013;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;CL&#20174;&#28304;&#22270;&#20687;&#21644;&#30446;&#26631;&#22270;&#20687;&#20013;&#24378;&#21046;&#32534;&#30721;&#22120;&#23398;&#20064;&#21306;&#20998;&#24615;&#20869;&#23481;&#29305;&#23450;&#20294;&#39046;&#22495;&#19981;&#21464;&#30340;&#35821;&#20041;&#65292;&#32780;&#19968;&#33268;&#24615;&#35268;&#21017;&#36890;&#36807;&#20445;&#25345;&#31354;&#38388;&#25935;&#24863;&#24615;&#26469;&#24378;&#21046;&#25366;&#25496;&#23616;&#37096;&#20687;&#32032;&#32423;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#23545;&#36825;&#20010;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although unsupervised domain adaptation (UDA) is a promising direction to alleviate domain shift, they fall short of their supervised counterparts. In this work, we investigate relatively less explored semi-supervised domain adaptation (SSDA) for medical image segmentation, where access to a few labeled target samples can improve the adaptation performance substantially. Specifically, we propose a two-stage training process. First, an encoder is pre-trained in a self-learning paradigm using a novel domain-content disentangled contrastive learning (CL) along with a pixel-level feature consistency constraint. The proposed CL enforces the encoder to learn discriminative content-specific but domain-invariant semantics on a global scale from the source and target images, whereas consistency regularization enforces the mining of local pixel-level information by maintaining spatial sensitivity. This pre-trained encoder, along with a decoder, is further fine-tuned for the downstream task, (i.e
&lt;/p&gt;</description></item><item><title>BHEISR&#27169;&#22411;&#36890;&#36807;&#28040;&#38500;&#36807;&#28388;&#27873;&#27819;&#25928;&#24212;&#65292;&#20419;&#36827;&#20449;&#24565;&#21644;&#35856;&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#24615;&#21270;&#30340;&#31867;&#21035;&#20449;&#24687;&#28608;&#21457;&#29992;&#25143;&#30340;&#22909;&#22855;&#24515;&#21644;&#20852;&#36259;&#65292;&#40723;&#21169;&#29992;&#25143;&#25299;&#23485;&#20449;&#24565;&#35270;&#37326;&#21644;&#25506;&#32034;&#26032;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.02797</link><description>&lt;p&gt;
BHEISR: &#20174;&#20559;&#35265;&#21040;&#24179;&#34913; - &#28040;&#38500;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#33616;&#20013;&#30340;&#24847;&#35782;&#24418;&#24577;&#38548;&#31163;&#65292;&#20419;&#36827;&#20449;&#24565;&#21644;&#35856;
&lt;/p&gt;
&lt;p&gt;
BHEISR: Nudging from Bias to Balance -- Promoting Belief Harmony by Eliminating Ideological Segregation in Knowledge-based Recommendations. (arXiv:2307.02797v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02797
&lt;/p&gt;
&lt;p&gt;
BHEISR&#27169;&#22411;&#36890;&#36807;&#28040;&#38500;&#36807;&#28388;&#27873;&#27819;&#25928;&#24212;&#65292;&#20419;&#36827;&#20449;&#24565;&#21644;&#35856;&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#24615;&#21270;&#30340;&#31867;&#21035;&#20449;&#24687;&#28608;&#21457;&#29992;&#25143;&#30340;&#22909;&#22855;&#24515;&#21644;&#20852;&#36259;&#65292;&#40723;&#21169;&#29992;&#25143;&#25299;&#23485;&#20449;&#24565;&#35270;&#37326;&#21644;&#25506;&#32034;&#26032;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#30340;&#26159;&#20449;&#24565;&#22833;&#34913;&#21644;&#29992;&#25143;&#20559;&#35265;&#30340;&#21152;&#21095;&#29616;&#35937;&#65292;&#36825;&#19968;&#29616;&#35937;&#20027;&#35201;&#24402;&#22240;&#20110;&#36807;&#28388;&#27873;&#27819;&#12290;&#38024;&#23545;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20013;&#20171;&#26426;&#26500;&#65288;BHEISR&#65289;&#65292;&#23558;&#20854;&#32622;&#20110;&#29992;&#25143;&#21644;&#29616;&#26377;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#65292;&#20197;&#20943;&#36731;&#36807;&#28388;&#27873;&#27819;&#25928;&#24212;&#22312;&#29616;&#26377;&#25512;&#33616;&#31995;&#32479;&#20013;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#29992;&#25143;&#21019;&#36896;&#20449;&#24565;&#24179;&#34913;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#36807;&#28388;&#27873;&#27819;&#24102;&#26469;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;BHEISR&#27169;&#22411;&#34701;&#21512;&#20102;&#8220;&#25512;&#21160;&#29702;&#35770;&#8221;&#30340;&#21407;&#21017;&#65292;&#21516;&#26102;&#31177;&#25345;&#27665;&#20027;&#21644;&#36879;&#26126;&#30340;&#21407;&#21017;&#12290;&#23427;&#21033;&#29992;&#29992;&#25143;&#29305;&#23450;&#30340;&#31867;&#21035;&#20449;&#24687;&#26469;&#28608;&#21457;&#22909;&#22855;&#24515;&#65292;&#21363;&#20351;&#22312;&#29992;&#25143;&#21487;&#33021;&#26368;&#21021;&#35748;&#20026;&#19981;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#12290;&#36890;&#36807;&#36880;&#27493;&#28608;&#21457;&#23545;&#26032;&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#35813;&#27169;&#22411;&#40723;&#21169;&#29992;&#25143;&#25299;&#23485;&#20449;&#24565;&#35270;&#37326;&#24182;&#25506;&#32034;&#20182;&#20204;&#36890;&#24120;&#24573;&#35270;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26102;&#38388;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of personalized recommendation systems, the increasing concern is the amplification of belief imbalance and user biases, a phenomenon primarily attributed to the filter bubble. Addressing this critical issue, we introduce an innovative intermediate agency (BHEISR) between users and existing recommendation systems to attenuate the negative repercussions of the filter bubble effect in extant recommendation systems. The main objective is to strike a belief balance for users while minimizing the detrimental influence caused by filter bubbles. The BHEISR model amalgamates principles from nudge theory while upholding democratic and transparent principles. It harnesses user-specific category information to stimulate curiosity, even in areas users might initially deem uninteresting. By progressively stimulating interest in novel categories, the model encourages users to broaden their belief horizons and explore the information they typically overlook. Our model is time-sensitive a
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#21644;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27169;&#24335;&#65292;&#20174;&#21160;&#25163;&#32534;&#30721;&#21644;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.02792</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#24212;&#35813;&#20570;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Should Data Science Education Do with Large Language Models?. (arXiv:2307.02792v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02792
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#21644;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27169;&#24335;&#65292;&#20174;&#21160;&#25163;&#32534;&#30721;&#21644;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#31561;&#30340;&#24555;&#36895;&#21457;&#23637;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#21644;&#32479;&#35745;&#23398;&#12290;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#21487;&#20197;&#31616;&#21270;&#22797;&#26434;&#30340;&#27969;&#31243;&#65292;&#20174;&#32780;&#37325;&#22609;&#20102;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#35748;&#20026;LLM&#27491;&#22312;&#36716;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#65292;&#23558;&#20182;&#20204;&#30340;&#37325;&#28857;&#20174;&#21160;&#25163;&#32534;&#30721;&#12289;&#25968;&#25454;&#25972;&#29702;&#21644;&#36827;&#34892;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#36825;&#20123;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#35282;&#33394;&#30340;&#28436;&#21464;&#31867;&#20284;&#20110;&#20174;&#36719;&#20214;&#24037;&#31243;&#24072;&#36716;&#21464;&#20026;&#20135;&#21697;&#32463;&#29702;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;LLM&#22312;&#25968;&#25454;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#20013;&#35828;&#26126;&#20102;&#36825;&#31181;&#36716;&#21464;&#12290;&#36825;&#20123;&#21457;&#23637;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#26377;&#24847;&#20041;&#22320;&#21457;&#23637;&#12290;&#25945;&#32946;&#26041;&#27861;&#29616;&#22312;&#24517;&#39035;&#26356;&#21152;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;LLM&#21551;&#21457;&#30340;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;LLM&#36824;&#21487;&#20197;&#22312;&#35838;&#22530;&#19978;&#36215;&#21040;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20316;&#20026;&#20114;&#21160;&#24335;&#25945;&#23398;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics. These state-of-the-art tools can streamline complex processes. As a result, it reshapes the role of data scientists. We argue that LLMs are transforming the responsibilities of data scientists, shifting their focus from hands-on coding, data-wrangling and conducting standard analyses to assessing and managing analyses performed by these automated AIs. This evolution of roles is reminiscent of the transition from a software engineer to a product manager. We illustrate this transition with concrete data science case studies using LLMs in this paper. These developments necessitate a meaningful evolution in data science education. Pedagogy must now place greater emphasis on cultivating diverse skillsets among students, such as LLM-informed creativity, critical thinking, AI-guided programming. LLMs can also play a significant role in the classroom as interactive teaching and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#21457;&#29616;&#20998;&#31867;&#22120;&#23558;&#20010;&#20307;&#20998;&#20026;&#23376;&#32676;&#30340;&#33021;&#21147;&#22312;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#21644;&#21463;&#20445;&#25252;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#23646;&#24615;&#23545;&#31639;&#27861;&#20559;&#35265;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23376;&#32676;&#21487;&#20998;&#24615;&#12289;&#23376;&#32676;&#24046;&#24322;&#21644;&#27169;&#22411;&#22312;&#23384;&#22312;&#31995;&#32479;&#20559;&#35265;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38477;&#32423;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#65292;&#36825;&#20026;&#20844;&#24179;&#21307;&#23398;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.02791</link><description>&lt;p&gt;
&#23376;&#32676;&#21487;&#20998;&#24615;&#22312;&#32452;&#20844;&#24179;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Subgroup Separability in Group-Fair Medical Image Classification. (arXiv:2307.02791v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#21457;&#29616;&#20998;&#31867;&#22120;&#23558;&#20010;&#20307;&#20998;&#20026;&#23376;&#32676;&#30340;&#33021;&#21147;&#22312;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#21644;&#21463;&#20445;&#25252;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#23646;&#24615;&#23545;&#31639;&#27861;&#20559;&#35265;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23376;&#32676;&#21487;&#20998;&#24615;&#12289;&#23376;&#32676;&#24046;&#24322;&#21644;&#27169;&#22411;&#22312;&#23384;&#22312;&#31995;&#32479;&#20559;&#35265;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38477;&#32423;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#65292;&#36825;&#20026;&#20844;&#24179;&#21307;&#23398;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20998;&#31867;&#22120;&#23558;&#20010;&#20307;&#20998;&#20026;&#23376;&#32676;&#30340;&#33021;&#21147;&#22312;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#21644;&#21463;&#20445;&#25252;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65307;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#23646;&#24615;&#23545;&#31639;&#27861;&#20559;&#35265;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23376;&#32676;&#21487;&#20998;&#24615;&#12289;&#23376;&#32676;&#24046;&#24322;&#21644;&#27169;&#22411;&#22312;&#23384;&#22312;&#31995;&#32479;&#20559;&#35265;&#25968;&#25454;&#65288;&#22914;&#27424;&#35786;&#26029;&#65289;&#26102;&#30340;&#24615;&#33021;&#38477;&#32423;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#20559;&#35265;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#20026;&#20844;&#24179;&#21307;&#23398;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate performance disparities in deep classifiers. We find that the ability of classifiers to separate individuals into subgroups varies substantially across medical imaging modalities and protected characteristics; crucially, we show that this property is predictive of algorithmic bias. Through theoretical analysis and extensive empirical evaluation, we find a relationship between subgroup separability, subgroup disparities, and performance degradation when models are trained on data with systematic bias such as underdiagnosis. Our findings shed new light on the question of how models become biased, providing important insights for the development of fair medical imaging AI.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;3&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#26469;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#38480;&#37319;&#26679;&#65292;&#24182;&#34920;&#26126;&#20165;&#20960;&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#29983;&#25104;&#30340;&#26631;&#31614;&#23601;&#36275;&#22815;&#23454;&#29616;&#22270;&#20687;&#30340;&#23457;&#26597;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.02770</link><description>&lt;p&gt;
&#36890;&#36807;3&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#38480;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback. (arXiv:2307.02770v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;3&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#26469;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#38480;&#37319;&#26679;&#65292;&#24182;&#34920;&#26126;&#20165;&#20960;&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#29983;&#25104;&#30340;&#26631;&#31614;&#23601;&#36275;&#22815;&#23454;&#29616;&#22270;&#20687;&#30340;&#23457;&#26597;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23384;&#22312;&#38169;&#35823;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#22909;&#30340;&#22270;&#20687;&#65292;&#20294;&#26377;&#26102;&#20250;&#29983;&#25104;&#19981;&#29702;&#24819;&#30340;&#22270;&#20687;&#12290;&#22914;&#26524;&#26159;&#36825;&#26679;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#38459;&#27490;&#29983;&#25104;&#31967;&#31957;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#23457;&#26597;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#32463;&#36807;&#26368;&#23567;&#20154;&#31867;&#21453;&#39304;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23457;&#26597;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26497;&#39640;&#30340;&#20154;&#31867;&#21453;&#39304;&#25928;&#29575;&#21487;&#20197;&#23454;&#29616;&#23457;&#26597;&#65292;&#24182;&#19988;&#20165;&#20960;&#20998;&#38047;&#30340;&#20154;&#31867;&#21453;&#39304;&#29983;&#25104;&#30340;&#26631;&#31614;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have recently shown remarkable success in high-quality image generation. Sometimes, however, a pre-trained diffusion model exhibits partial misalignment in the sense that the model can generate good images, but it sometimes outputs undesirable images. If so, we simply need to prevent the generation of the bad images, and we call this task censoring. In this work, we present censored generation with a pre-trained diffusion model using a reward model trained on minimal human feedback. We show that censoring can be accomplished with extreme human feedback efficiency and that labels generated with a mere few minutes of human feedback are sufficient. Code available at: https://github.com/tetrzim/diffusion-human-feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02762</link><description>&lt;p&gt;
PRD: &#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. (arXiv:2307.02762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22238;&#31572;&#36136;&#37327;&#22312;&#33258;&#21160;&#21270;&#26041;&#38754;&#24456;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#24182;&#20027;&#35201;&#20351;&#29992;LLMs&#20316;&#20026;&#26080;&#21442;&#32771;&#24230;&#37327;&#34913;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21442;&#32771;&#25351;&#26631;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20182;&#20204;&#20197;&#34987;&#35748;&#20026;&#26159;&#8220;&#26368;&#24378;&#8221;&#30340;LLM&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#23545;&#20505;&#36873;&#27169;&#22411;&#30340;&#31572;&#26696;&#36827;&#34892;&#20004;&#20004;&#27604;&#36739;&#24182;&#25552;&#20379;&#25490;&#21517;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#24102;&#26469;&#33258;&#25105;&#25552;&#21319;&#65288;&#38738;&#30544;&#33258;&#24049;&#30340;&#31572;&#26696;&#65289;&#21644;&#20301;&#32622;&#20559;&#35265;&#12290;&#25105;&#20204;&#20174;&#25945;&#32946;&#39046;&#22495;&#65288;Cho and MacArthur, 2011&#65307;Walsh, 2014&#65289;&#20013;&#27762;&#21462;&#35265;&#35299;&#21644;&#25945;&#35757;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#21516;&#34892;&#35780;&#32423;&#65288;PR&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#27599;&#20010;&#21516;&#34892;LLM&#23545;&#25152;&#26377;&#31572;&#26696;&#23545;&#30340;&#20004;&#20004;&#20559;&#22909;&#65292;&#24182;&#36755;&#20986;&#27169;&#22411;&#30340;&#26368;&#32456;&#25490;&#21517;&#65307;&#20197;&#21450;&#65288;2&#65289;&#21516;&#34892;&#35752;&#35770;&#65288;PD&#65289;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20419;&#20351;&#20004;&#20010;LLMs&#36827;&#34892;&#35752;&#35770;&#24182;&#23581;&#35797;&#23601;&#20004;&#20010;&#20559;&#22909;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs as a reference-free metric for open-ended question answering. More specifically, they use the recognized "strongest" LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on preferences of two an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#21512;&#29702;&#21270;&#26041;&#27861;KGRec&#65292;&#29992;&#20110;&#30693;&#35782;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#36890;&#36807;&#20851;&#27880;&#30693;&#35782;&#21512;&#29702;&#21270;&#26426;&#21046;&#21644;&#29983;&#25104;&#23545;&#27604;&#24230;&#33258;&#30417;&#30563;&#20219;&#21153;&#65292;KGRec&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#26377;&#20449;&#24687;&#37327;&#30340;&#30693;&#35782;&#36830;&#25509;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#36830;&#25509;&#36827;&#34892;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2307.02759</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#33258;&#30417;&#30563;&#21512;&#29702;&#21270;&#26041;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Self-Supervised Rationalization for Recommendation. (arXiv:2307.02759v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#21512;&#29702;&#21270;&#26041;&#27861;KGRec&#65292;&#29992;&#20110;&#30693;&#35782;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#36890;&#36807;&#20851;&#27880;&#30693;&#35782;&#21512;&#29702;&#21270;&#26426;&#21046;&#21644;&#29983;&#25104;&#23545;&#27604;&#24230;&#33258;&#30417;&#30563;&#20219;&#21153;&#65292;KGRec&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#26377;&#20449;&#24687;&#37327;&#30340;&#30693;&#35782;&#36830;&#25509;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#36830;&#25509;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#21512;&#29702;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;KGRec&#65292;&#29992;&#20110;&#30693;&#35782;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35782;&#21035;&#26377;&#20449;&#24687;&#37327;&#30340;&#30693;&#35782;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#30693;&#35782;&#21512;&#29702;&#21270;&#26426;&#21046;&#65292;&#20026;&#30693;&#35782;&#19977;&#20803;&#32452;&#29983;&#25104;&#21512;&#29702;&#21270;&#24471;&#20998;&#12290;&#36890;&#36807;&#36825;&#20123;&#24471;&#20998;&#65292;KGRec&#36890;&#36807;&#21512;&#29702;&#21270;&#25513;&#30721;&#38598;&#25104;&#29983;&#25104;&#21644;&#23545;&#27604;&#24230;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#25512;&#33616;&#12290;&#20026;&#20102;&#31361;&#20986;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#21512;&#29702;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20197;&#25513;&#30721;&#37325;&#24314;&#24418;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#21512;&#29702;&#21270;&#24471;&#20998;&#23545;&#37325;&#35201;&#30693;&#35782;&#36827;&#34892;&#25513;&#30721;&#65292;KGRec&#34987;&#35757;&#32451;&#26469;&#37325;&#24314;&#24182;&#31361;&#20986;&#26377;&#29992;&#30340;&#30693;&#35782;&#36830;&#25509;&#65292;&#20316;&#20026;&#21512;&#29702;&#30340;&#20381;&#25454;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21512;&#29702;&#21270;&#21327;&#21516;&#20132;&#20114;&#23545;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27604;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#23545;&#40784;&#26469;&#33258;&#30693;&#35782;&#21644;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#35270;&#22270;&#30340;&#20449;&#21495;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#27604;&#24230;&#30340;&#25239;&#22122;&#22768;&#24615;&#65292;&#36890;&#36807;&#21028;&#26029;&#20004;&#20010;&#22270;&#20013;&#30340;&#28508;&#22312;&#22122;&#22768;&#36793;&#32536;&#65292;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new self-supervised rationalization method, called KGRec, for knowledge-aware recommender systems. To effectively identify informative knowledge connections, we propose an attentive knowledge rationalization mechanism that generates rational scores for knowledge triplets. With these scores, KGRec integrates generative and contrastive self-supervised tasks for recommendation through rational masking. To highlight rationales in the knowledge graph, we design a novel generative task in the form of masking-reconstructing. By masking important knowledge with high rational scores, KGRec is trained to rebuild and highlight useful knowledge connections that serve as rationales. To further rationalize the effect of collaborative interactions on knowledge graph learning, we introduce a contrastive learning task that aligns signals from knowledge and user-item interaction views. To ensure noise-resistant contrasting, potential noisy edges in both graphs judged by the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02752</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2307.02752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#20013;&#23545;&#22522;&#20934;&#30340;&#26222;&#36941;&#20351;&#29992;&#23548;&#33268;&#20102;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#24573;&#35270;&#12290;&#30001;&#20110;&#25506;&#32034;&#25110;&#23433;&#20840;&#32771;&#34385;&#30340;&#25361;&#25112;&#65292;&#23454;&#38469;&#31163;&#32447;RL&#25968;&#25454;&#38598;&#22312;&#29366;&#24577;&#31354;&#38388;&#19978;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20855;&#20307;&#35828;&#26126;&#20102;&#31163;&#32447;RL&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#29366;&#24577;&#35206;&#30422;&#29575;&#36981;&#24490;&#19968;&#20010;&#30001;&#20559;&#24577;&#31574;&#30053;&#25152;&#29305;&#24449;&#21270;&#30340;&#24130;&#24459;&#20998;&#24067;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#20998;&#24067;&#32422;&#26463;&#30340;&#20856;&#22411;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#22914;&#20445;&#23432;Q&#23398;&#20064;&#65288;CQL&#65289;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#25552;&#21462;&#31574;&#30053;&#26159;&#26080;&#25928;&#30340;&#12290;&#21463;&#33258;&#28982;&#26234;&#33021;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;CQL&#30340;&#22686;&#24378;&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#22238;&#24518;&#20197;&#24448;&#30456;&#20851;&#32463;&#39564;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02738</link><description>&lt;p&gt;
RecallM:&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#23558;&#20026;&#36830;&#32493;&#23398;&#20064;&#12289;&#22797;&#26434;&#25512;&#29702;&#21644;&#23398;&#20064;&#24207;&#21015;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#25171;&#19979;&#22522;&#30784;&#12290;&#21019;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#19987;&#27880;&#20110;&#20026;AGI&#31995;&#32479;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#23637;&#31034;&#20102;RecallM&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#23427;&#25552;&#20379;&#30340;&#25913;&#36827;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ideal long-term memory mechanism for Large Language Model (LLM) based chatbots, would lay the foundation for continual learning, complex reasoning and allow sequential and temporal dependencies to be learnt. Creating this type of memory mechanism is an extremely challenging problem. In this paper we explore different methods of achieving the effect of long-term memory. We propose a new architecture focused on creating adaptable and updatable long-term memory for AGI systems. We demonstrate through various experiments the benefits of the RecallM architecture, particularly the improved temporal understanding it provides.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#33457;&#26679;&#28369;&#20912;&#25968;&#25454;&#38598;&#65288;MMFS&#65289;&#65292;&#21253;&#21547;&#20102;256&#20010;&#31867;&#21035;&#30340;&#21160;&#20316;&#24471;&#20998;&#21644;&#31354;&#38388;&#21644;&#26102;&#38388;&#26631;&#31614;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#39318;&#27425;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#31867;&#65292;&#24182;&#39318;&#27425;&#20351;&#29992;&#39592;&#39612;&#27169;&#24577;&#36827;&#34892;&#31934;&#32454;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.02730</link><description>&lt;p&gt;
&#31934;&#32454;&#21160;&#20316;&#20998;&#26512;&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#33457;&#26679;&#28369;&#20912;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating. (arXiv:2307.02730v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#33457;&#26679;&#28369;&#20912;&#25968;&#25454;&#38598;&#65288;MMFS&#65289;&#65292;&#21253;&#21547;&#20102;256&#20010;&#31867;&#21035;&#30340;&#21160;&#20316;&#24471;&#20998;&#21644;&#31354;&#38388;&#21644;&#26102;&#38388;&#26631;&#31614;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#39318;&#27425;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#31867;&#65292;&#24182;&#39318;&#27425;&#20351;&#29992;&#39592;&#39612;&#27169;&#24577;&#36827;&#34892;&#31934;&#32454;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#20316;&#25968;&#25454;&#38598;&#30340;&#31934;&#32454;&#21160;&#20316;&#20998;&#26512;&#38754;&#20020;&#30528;&#21160;&#20316;&#31867;&#21035;&#19981;&#36275;&#12289;&#32454;&#31890;&#24230;&#20302;&#12289;&#27169;&#24577;&#21644;&#20219;&#21153;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#30340;&#33457;&#26679;&#28369;&#20912;&#25968;&#25454;&#38598;&#65288;MMFS&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#19990;&#30028;&#33457;&#26679;&#28369;&#20912;&#38182;&#26631;&#36187;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;MMFS&#21253;&#25324;&#21160;&#20316;&#35782;&#21035;&#21644;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#65292;&#20351;&#29992;RGB&#12289;&#39592;&#39612;&#21644;11671&#20010;&#35270;&#39057;&#29255;&#27573;&#37319;&#38598;&#20102;256&#20010;&#31867;&#21035;&#30340;&#21160;&#20316;&#24471;&#20998;&#65292;&#24182;&#21253;&#21547;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#39318;&#27425;&#25552;&#20986;&#20102;&#29420;&#31435;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#31867;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#32034;&#31934;&#32454;&#21160;&#20316;&#35782;&#21035;&#21644;&#36136;&#37327;&#35780;&#20272;&#12290;(2)MMFS&#39318;&#27425;&#24341;&#20837;&#20102;&#39592;&#39612;&#27169;&#24577;&#29992;&#20110;&#22797;&#26434;&#30340;&#31934;&#32454;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;(3)&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#40723;&#21169;&#26356;&#22810;&#30340;&#21160;&#20316;&#20998;&#26512;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;RGB&#21644;&#22522;&#20110;&#39592;&#39612;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fine-grained action analysis of the existing action datasets is challenged by insufficient action categories, low fine granularities, limited modalities, and tasks. In this paper, we propose a Multi-modality and Multi-task dataset of Figure Skating (MMFS) which was collected from the World Figure Skating Championships. MMFS, which possesses action recognition and action quality assessment, captures RGB, skeleton, and is collected the score of actions from 11671 clips with 256 categories including spatial and temporal labels. The key contributions of our dataset fall into three aspects as follows. (1) Independently spatial and temporal categories are first proposed to further explore fine-grained action recognition and quality assessment. (2) MMFS first introduces the skeleton modality for complex fine-grained action quality assessment. (3) Our multi-modality and multi-task dataset encourage more action analysis models. To benchmark our dataset, we adopt RGB-based and skeleton-based
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#25480;&#26435;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35745;&#31639;&#25480;&#26435;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#19979;&#30028;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#25480;&#26435;&#35745;&#31639;&#65292;&#24182;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.02728</link><description>&lt;p&gt;
&#20998;&#23618;&#25480;&#26435;&#65306;&#26397;&#30528;&#21487;&#34892;&#30340;&#22522;&#20110;&#25480;&#26435;&#30340;&#25216;&#33021;&#23398;&#20064;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning. (arXiv:2307.02728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02728
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25480;&#26435;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35745;&#31639;&#25480;&#26435;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#19979;&#30028;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#25480;&#26435;&#35745;&#31639;&#65292;&#24182;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26234;&#33021;&#20307;&#38656;&#35201;&#22823;&#37327;&#30340;&#25216;&#33021;&#12290; &#25480;&#26435; - &#25216;&#33021;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#26368;&#22823;&#20114;&#20449;&#24687; - &#20026;&#23398;&#20064;&#22823;&#37327;&#19981;&#21516;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#26465;&#36335;&#24452;&#65292;&#20294;&#20114;&#20449;&#24687;&#24456;&#38590;&#20248;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20998;&#23618;&#25480;&#26435;&#65292;&#36890;&#36807;&#38598;&#25104;&#30446;&#26631;&#26465;&#20214;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;&#35745;&#31639;&#25480;&#26435;&#26356;&#21152;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#21487;&#29992;&#20110;&#35745;&#31639;&#30701;&#26399;&#35270;&#35282;&#19979;&#30340;&#25480;&#26435;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#25351;&#25968;&#26102;&#38388;&#23610;&#24230;&#19979;&#30340;&#25480;&#26435;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#27169;&#25311;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#36129;&#29486;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#34434;&#34433;&#23548;&#33322;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#22235;&#32423;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#35206;&#30422;&#38754;&#31215;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
General purpose agents will require large repertoires of skills. Empowerment -- the maximum mutual information between skills and the states -- provides a pathway for learning large collections of distinct skills, but mutual information is difficult to optimize. We introduce a new framework, Hierarchical Empowerment, that makes computing empowerment more tractable by integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning. Our framework makes two specific contributions. First, we introduce a new variational lower bound on mutual information that can be used to compute empowerment over short horizons. Second, we introduce a hierarchical architecture for computing empowerment over exponentially longer time scales. We verify the contributions of the framework in a series of simulated robotics tasks. In a popular ant navigation domain, our four level agents are able to learn skills that cover a surface area over two orders of magnitude larger than prior work.
&lt;/p&gt;</description></item><item><title>TL-nvSRAM-CIM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23384;&#20648;&#22120;&#20869;&#35745;&#31639;&#26041;&#26696;&#65292;&#21033;&#29992;&#36229;&#39640;&#23494;&#24230;&#30340;&#19977;&#32423;ReRAM&#36741;&#21161;&#35745;&#31639;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#26435;&#37325;&#23481;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20102;&#38646;&#30452;&#27969;&#21151;&#32791;&#24674;&#22797;&#21644;&#19977;&#24577;MAC&#25805;&#20316;&#26469;&#25552;&#39640;&#33021;&#25928;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02717</link><description>&lt;p&gt;
TL-nvSRAM-CIM&#65306;&#20855;&#26377;&#38646;&#30452;&#27969;&#21151;&#32791;&#24674;&#22797;&#21644;&#19977;&#24577;MAC&#25805;&#20316;&#30340;&#36229;&#39640;&#23494;&#24230;&#19977;&#32423;ReRAM&#36741;&#21161;&#35745;&#31639;&#23384;&#20648;&#22120;&#20869;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations. (arXiv:2307.02717v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02717
&lt;/p&gt;
&lt;p&gt;
TL-nvSRAM-CIM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23384;&#20648;&#22120;&#20869;&#35745;&#31639;&#26041;&#26696;&#65292;&#21033;&#29992;&#36229;&#39640;&#23494;&#24230;&#30340;&#19977;&#32423;ReRAM&#36741;&#21161;&#35745;&#31639;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#26435;&#37325;&#23481;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20102;&#38646;&#30452;&#27969;&#21151;&#32791;&#24674;&#22797;&#21644;&#19977;&#24577;MAC&#25805;&#20316;&#26469;&#25552;&#39640;&#33021;&#25928;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#24222;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#25152;&#26377;&#26435;&#37325;&#25918;&#22312;&#33455;&#29255;&#19978;&#20173;&#28982;&#26159;SRAM-CIM&#30340;&#19968;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#23384;&#20648;&#23481;&#37327;&#26377;&#38480;&#12290;&#20043;&#21069;&#30340;&#38750;&#26131;&#22833;&#24615;SRAM-CIM&#65288;nvSRAM-CIM&#65289;&#36890;&#36807;&#22312;&#39640;&#25928;SRAM-CIM&#19978;&#38598;&#25104;&#39640;&#23494;&#24230;&#21333;&#32423;ReRAM&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#33455;&#29255;&#19978;&#20648;&#23384;&#26435;&#37325;&#20197;&#28040;&#38500;&#33455;&#29255;&#22806;&#30340;&#20869;&#23384;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;SL-nvSRAM-CIM&#22312;&#22686;&#21152;SL-ReRAM&#25968;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#21463;&#38480;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#39640;&#23494;&#24230;&#19977;&#32423;ReRAM&#36741;&#21161;&#30340;&#38750;&#26131;&#22833;&#24615;SRAM&#35745;&#31639;&#23384;&#20648;&#22120;&#20869;&#35745;&#31639;&#26041;&#26696;&#65288;TL-nvSRAM-CIM&#65289;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#37319;&#29992;&#20102;&#38598;&#32676;&#24335;&#36873;&#25321;&#22120;-n-ReRAM&#65288;cluster-nSnRs&#65289;&#26469;&#21487;&#38752;&#22320;&#24674;&#22797;&#26435;&#37325;&#65292;&#24182;&#28040;&#38500;&#20102;&#30452;&#27969;&#21151;&#32791;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24046;&#20998;&#35745;&#31639;&#26041;&#26696;&#30340;&#19977;&#24577;SRAM-CIM&#26426;&#21046;&#65292;&#29992;&#20110;&#33021;&#37327;&#39640;&#25928;&#30340;&#19977;&#24577;MAC&#25805;&#20316;&#65292;&#24182;&#20445;&#25345;&#39640;NN&#20934;&#30830;&#24615;&#12290;&#25552;&#20986;&#30340;TL-nvSRAM-CIM&#23454;&#29616;&#20102;7.
&lt;/p&gt;
&lt;p&gt;
Accommodating all the weights on-chip for large-scale NNs remains a great challenge for SRAM based computing-in-memory (SRAM-CIM) with limited on-chip capacity. Previous non-volatile SRAM-CIM (nvSRAM-CIM) addresses this issue by integrating high-density single-level ReRAMs on the top of high-efficiency SRAM-CIM for weight storage to eliminate the off-chip memory access. However, previous SL-nvSRAM-CIM suffers from poor scalability for an increased number of SL-ReRAMs and limited computing efficiency. To overcome these challenges, this work proposes an ultra-high-density three-level ReRAMs-assisted computing-in-nonvolatile-SRAM (TL-nvSRAM-CIM) scheme for large NN models. The clustered n-selector-n-ReRAM (cluster-nSnRs) is employed for reliable weight-restore with eliminated DC power. Furthermore, a ternary SRAM-CIM mechanism with differential computing scheme is proposed for energy-efficient ternary MAC operations while preserving high NN accuracy. The proposed TL-nvSRAM-CIM achieves 7.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39564;&#35777;&#20102;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#22312;&#20855;&#26377;&#19981;&#20934;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24773;&#20917;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LAF&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;LAF&#22312;MHWSIA&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02709</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#39564;&#35777;&#23545;&#19981;&#20934;&#30830;&#22320;&#30495;&#23454;&#26631;&#31614;&#30340;&#35780;&#20272;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Validation of the Practicability of Logical Assessment Formula for Evaluations with Inaccurate Ground-Truth Labels. (arXiv:2307.02709v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39564;&#35777;&#20102;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#22312;&#20855;&#26377;&#19981;&#20934;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24773;&#20917;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LAF&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;LAF&#22312;MHWSIA&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#65288;LAF&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#19981;&#20934;&#30830;&#22320;&#30495;&#23454;&#26631;&#31614;&#65288;IAGTLs&#65289;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#29702;&#35770;&#65292;&#29992;&#20110;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;LAF&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23545;&#20110;&#20855;&#26377;IAGTLs&#30340;&#35780;&#20272;&#30340;&#23454;&#29992;&#24615;&#23578;&#26410;&#24471;&#21040;&#39564;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;LAF&#24212;&#29992;&#20110;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#65288;TSfBC&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#26174;&#31034;LAF&#22312;TSfBC&#20013;&#23545;&#20110;&#20855;&#26377;IAGTLs&#30340;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21453;&#26144;&#20102;&#23558;LAF&#24212;&#29992;&#20110;MHWSIA&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical assessment formula (LAF) is a new theory proposed for evaluations with inaccurate ground-truth labels (IAGTLs) to assess the predictive models for various artificial intelligence applications. However, the practicability of LAF for evaluations with IAGTLs has not yet been validated in real-world practice. In this paper, to address this issue, we applied LAF to tumour segmentation for breast cancer (TSfBC) in medical histopathology whole slide image analysis (MHWSIA). Experimental results and analysis show the validity of LAF for evaluations with IAGTLs in the case of TSfBC and reflect the potentials of LAF applied to MHWSIA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SACHA&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#27880;&#24847;&#21147;&#30340;&#36719;&#28436;&#21592;&#25209;&#21028;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;SACHA&#20419;&#36827;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.02691</link><description>&lt;p&gt;
SACHA&#65306;&#22522;&#20110;&#21551;&#21457;&#24335;&#27880;&#24847;&#21147;&#30340;&#36719;&#28436;&#21592;&#25209;&#21028;&#22120;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SACHA: Soft Actor-Critic with Heuristic-Based Attention for Partially Observable Multi-Agent Path Finding. (arXiv:2307.02691v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02691
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SACHA&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#27880;&#24847;&#21147;&#30340;&#36719;&#28436;&#21592;&#25209;&#21028;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;SACHA&#20419;&#36827;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#26159;&#35768;&#22810;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#26234;&#33021;&#20307;&#24517;&#39035;&#35268;&#21010;&#20854;&#21040;&#36798;&#32473;&#23450;&#30446;&#26631;&#20301;&#32622;&#30340;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#24341;&#20837;&#20197;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;MAPF&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#22312;&#38598;&#20013;&#26041;&#24335;&#19979;&#22522;&#20110;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#37096;&#20998;&#35266;&#23519;&#26469;&#23398;&#20064;&#20998;&#25955;&#30340;&#21333;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#35774;&#23450;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#29305;&#21035;&#26159;&#22312;&#25317;&#22581;&#29615;&#22659;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#21551;&#21457;&#24335;&#27880;&#24847;&#21147;&#30340;&#36719;&#28436;&#21592;&#25209;&#21028;&#22120;&#65288;SACHA&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#25209;&#21028;&#22120;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#20419;&#36827;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;SACHA&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26368;&#30701;&#36335;&#24452;&#21551;&#21457;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) is a crucial component for many large-scale robotic systems, where agents must plan their collision-free paths to their given goal positions. Recently, multi-agent reinforcement learning has been introduced to solve the partially observable variant of MAPF by learning a decentralized single-agent policy in a centralized fashion based on each agent's partial observation. However, existing learning-based methods are ineffective in achieving complex multi-agent cooperation, especially in congested environments, due to the non-stationarity of this setting. To tackle this challenge, we propose a multi-agent actor-critic method called Soft Actor-Critic with Heuristic-Based Attention (SACHA), which employs novel heuristic-based attention mechanisms for both the actors and critics to encourage cooperation among agents. SACHA learns a neural network for each agent to selectively pay attention to the shortest path heuristic guidance from multiple agents within its
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#28436;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#38480;&#21046;&#19982;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02690</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#25193;&#23637;&#19978;&#19979;&#25991;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Scaling In-Context Demonstrations with Structured Attention. (arXiv:2307.02690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#28436;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#38480;&#21046;&#19982;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#20013;&#20174;&#23569;&#25968;&#28436;&#31034;&#20013;&#8220;&#23398;&#20064;&#8221;&#25191;&#34892;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#21463;&#21040;&#27169;&#22411;&#26550;&#26500;&#30340;&#38480;&#21046;&#65306;1&#65289;&#30001;&#20110;&#20301;&#32622;&#23884;&#20837;&#65292;&#28436;&#31034;&#30340;&#20351;&#29992;&#21463;&#21040;&#26368;&#22823;&#21477;&#23376;&#38271;&#24230;&#30340;&#38480;&#21046;&#65307;2&#65289;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38459;&#30861;&#29992;&#25143;&#26377;&#25928;&#20351;&#29992;&#26356;&#22810;&#30340;&#28436;&#31034;&#65307;3&#65289;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23545;&#28436;&#31034;&#30340;&#39034;&#24207;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#26356;&#22909;&#30340;&#26550;&#26500;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SAICL&#65288;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#65289;&#65292;&#23427;&#36890;&#36807;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#26367;&#25442;&#20840;&#27880;&#24847;&#21147;&#65292;&#24182;&#28040;&#38500;&#20102;&#20010;&#21035;&#28436;&#31034;&#20043;&#38388;&#19981;&#24517;&#35201;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#23545;&#28436;&#31034;&#30340;&#25490;&#21015;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of large language models (LLMs) highlights their ability to perform in-context learning, i.e., "learning" to perform a task from a few demonstrations in the context without any parameter updates. However, their capabilities of in-context learning are limited by the model architecture: 1) the use of demonstrations is constrained by a maximum sentence length due to positional embeddings; 2) the quadratic complexity of attention hinders users from using more demonstrations efficiently; 3) LLMs are shown to be sensitive to the order of the demonstrations. In this work, we tackle these challenges by proposing a better architectural design for in-context learning. We propose SAICL (Structured Attention for In-Context Learning), which replaces the full-attention by a structured attention mechanism designed for in-context learning, and removes unnecessary dependencies between individual demonstrations, while making the model invariant to the permutation of demonstrations. We e
&lt;/p&gt;</description></item><item><title>AI4OPT&#26159;&#19968;&#20010;&#34701;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20248;&#21270;&#30340;&#30740;&#31350;&#25152;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#20379;&#24212;&#38142;&#12289;&#33021;&#28304;&#31995;&#32479;&#12289;&#33455;&#29255;&#35774;&#35745;&#19982;&#21046;&#36896;&#20197;&#21450;&#21487;&#25345;&#32493;&#39135;&#21697;&#31995;&#32479;&#31561;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#24037;&#31243;&#25945;&#32946;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2307.02671</link><description>&lt;p&gt;
AI4OPT: &#20248;&#21270;&#36827;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;
&lt;/p&gt;
&lt;p&gt;
AI4OPT: AI Institute for Advances in Optimization. (arXiv:2307.02671v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02671
&lt;/p&gt;
&lt;p&gt;
AI4OPT&#26159;&#19968;&#20010;&#34701;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20248;&#21270;&#30340;&#30740;&#31350;&#25152;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#20379;&#24212;&#38142;&#12289;&#33021;&#28304;&#31995;&#32479;&#12289;&#33455;&#29255;&#35774;&#35745;&#19982;&#21046;&#36896;&#20197;&#21450;&#21487;&#25345;&#32493;&#39135;&#21697;&#31995;&#32479;&#31561;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#24037;&#31243;&#25945;&#32946;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;AI4OPT&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;AI4OPT&#26159;&#32654;&#22269;&#22269;&#23478;&#31185;&#23398;&#22522;&#37329;&#20250;&#65288;NSF&#65289;&#20851;&#20110;&#20248;&#21270;&#36827;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;&#12290;AI4OPT&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#21463;&#21040;&#20379;&#24212;&#38142;&#12289;&#33021;&#28304;&#31995;&#32479;&#12289;&#33455;&#29255;&#35774;&#35745;&#19982;&#21046;&#36896;&#20197;&#21450;&#21487;&#25345;&#32493;&#39135;&#21697;&#31995;&#32479;&#31561;&#32456;&#31471;&#24212;&#29992;&#26696;&#20363;&#30340;&#21551;&#21457;&#12290;AI4OPT&#36824;&#23558;&#20854;&#8220;&#25945;&#24072;&#25945;&#23398;&#8221;&#29702;&#24565;&#36816;&#29992;&#20110;&#20026;&#24037;&#31243;&#39046;&#22495;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#30340;&#38271;&#26399;&#25945;&#32946;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article is a short introduction to AI4OPT, the NSF AI Institute for Advances in Optimization. AI4OPT fuses AI and Optimization, inspired by end-use cases in supply chains, energy systems, chip design and manufacturing, and sustainable food systems. AI4OPT also applies its "teaching the teachers" philosophy to provide longitudinal educational pathways in AI for engineering.
&lt;/p&gt;</description></item><item><title>&#36830;&#25509;&#21644;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#21487;&#20197;&#36890;&#36807;&#34701;&#21512;&#36890;&#20449;&#12289;&#25511;&#21046;&#21644;&#23398;&#20064;&#31995;&#32479;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#31283;&#23450;&#36335;&#24452;&#36319;&#36394;&#12289;&#25239;&#20987;&#32593;&#32476;&#29289;&#29702;&#25915;&#20987;&#30340;&#24378;&#22823;&#25511;&#21046;&#21644;&#33258;&#36866;&#24212;&#23548;&#33322;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.02663</link><description>&lt;p&gt;
&#36890;&#20449;&#12289;&#25511;&#21046;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#23433;&#20840;&#21644;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#23548;&#33322;&#20013;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Convergence of Communications, Control, and Machine Learning for Secure and Autonomous Vehicle Navigation. (arXiv:2307.02663v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02663
&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#21644;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#21487;&#20197;&#36890;&#36807;&#34701;&#21512;&#36890;&#20449;&#12289;&#25511;&#21046;&#21644;&#23398;&#20064;&#31995;&#32479;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#31283;&#23450;&#36335;&#24452;&#36319;&#36394;&#12289;&#25239;&#20987;&#32593;&#32476;&#29289;&#29702;&#25915;&#20987;&#30340;&#24378;&#22823;&#25511;&#21046;&#21644;&#33258;&#36866;&#24212;&#23548;&#33322;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#21644;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#21487;&#20197;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#20013;&#30340;&#20154;&#20026;&#38169;&#35823;&#65292;&#25552;&#39640;&#36947;&#36335;&#25928;&#29575;&#65292;&#24182;&#25191;&#34892;&#20174;&#20132;&#36135;&#21040;&#26234;&#33021;&#22478;&#24066;&#30417;&#35270;&#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#23454;&#29616;&#36825;&#20123;&#22909;&#22788;&#38656;&#35201;CAVs&#33021;&#22815;&#33258;&#20027;&#23548;&#33322;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#27599;&#20010;CAV&#30340;&#23548;&#33322;&#25511;&#21046;&#22120;&#24517;&#39035;&#21033;&#29992;&#20256;&#24863;&#22120;&#21644;&#26080;&#32447;&#31995;&#32479;&#25910;&#38598;&#30340;&#20449;&#24687;&#65292;&#36827;&#34892;&#32437;&#21521;&#21644;&#27178;&#21521;&#31227;&#21160;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;CAVs&#30340;&#33258;&#20027;&#23548;&#33322;&#38656;&#35201;&#23558;&#36890;&#20449;&#12289;&#25511;&#21046;&#21644;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#34701;&#21512;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#26126;&#30830;&#25581;&#31034;&#19982;&#36825;&#31181;&#34701;&#21512;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#20004;&#20010;&#20027;&#35201;&#29992;&#20363;&#65306;&#38750;&#21327;&#35843;&#21644;&#21327;&#35843;CAVs&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#38750;&#21327;&#35843;CAVs&#30340;&#23548;&#33322;&#30456;&#20851;&#30340;&#25361;&#25112;&#21253;&#25324;&#31283;&#23450;&#36335;&#24452;&#36319;&#36394;&#12289;&#25239;&#20987;&#32593;&#32476;&#29289;&#29702;&#25915;&#20987;&#30340;&#24378;&#22823;&#25511;&#21046;&#21644;&#33258;&#36866;&#24212;&#23548;&#33322;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;&#21516;&#26102;&#65292;&#24403;&#22810;&#20010;CAVs&#21516;&#26102;&#23384;&#22312;&#26102;...
&lt;/p&gt;
&lt;p&gt;
Connected and autonomous vehicles (CAVs) can reduce human errors in traffic accidents, increase road efficiency, and execute various tasks ranging from delivery to smart city surveillance. Reaping these benefits requires CAVs to autonomously navigate to target destinations. To this end, each CAV's navigation controller must leverage the information collected by sensors and wireless systems for decision-making on longitudinal and lateral movements. However, enabling autonomous navigation for CAVs requires a convergent integration of communication, control, and learning systems. The goal of this article is to explicitly expose the challenges related to this convergence and propose solutions to address them in two major use cases: Uncoordinated and coordinated CAVs. In particular, challenges related to the navigation of uncoordinated CAVs include stable path tracking, robust control against cyber-physical attacks, and adaptive navigation controller design. Meanwhile, when multiple CAVs co
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#36890;&#36807;&#23545;&#31934;&#33521;&#36827;&#34892;&#25237;&#31080;&#65288;MOVE&#65289;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#21644;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#22312;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.02661</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#36890;&#36807;&#23545;&#31934;&#33521;&#36827;&#34892;&#25237;&#31080;
&lt;/p&gt;
&lt;p&gt;
Many-objective Optimization via Voting for Elites. (arXiv:2307.02661v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02661
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#36890;&#36807;&#23545;&#31934;&#33521;&#36827;&#34892;&#25237;&#31080;&#65288;MOVE&#65289;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#21644;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#22312;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#38382;&#39064;&#36890;&#24120;&#30001;&#22810;&#20010;&#30446;&#26631;&#32452;&#25104;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#36825;&#20123;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20855;&#22791;&#25361;&#25112;&#24615;&#30340;&#20551;&#35774;&#65292;&#22914;&#23545;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;/&#38590;&#24230;&#22312;&#21152;&#26435;&#27714;&#21644;&#30340;&#21333;&#30446;&#26631;&#33539;&#24335;&#20013;&#36827;&#34892;&#20102;&#35299;&#65292;&#25110;&#36890;&#36807;&#24222;&#22823;&#30340;&#31181;&#32676;&#26469;&#20811;&#26381;&#22810;&#30446;&#26631;&#24085;&#32047;&#25176;&#20248;&#21270;&#20013;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#32467;&#21512;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#21644;MAP-Elites&#31561;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#36890;&#36807;&#23545;&#31934;&#33521;&#36827;&#34892;&#25237;&#31080;&#65288;MOVE&#65289;&#30340;&#26041;&#27861;&#12290;MOVE&#36890;&#36807;&#32500;&#25252;&#22312;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#23376;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31934;&#33521;&#26144;&#23556;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;14&#20010;&#30446;&#26631;&#30340;&#22270;&#20687;&#31070;&#32463;&#28436;&#21270;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;MOVE&#22312;&#21482;&#20855;&#26377;50&#20010;&#31934;&#33521;&#30340;&#31181;&#32676;&#24773;&#20917;&#19979;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#20248;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#21333;&#30446;&#26631;&#22522;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#31639;&#27861;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21516;&#26742;&#20043;&#38388;&#36339;&#36716;&#65288;&#20197;&#20351;&#19968;&#20010;&#29238;&#20195;&#20135;&#29983;&#19968;&#20010;&#23646;&#20110;&#31934;&#33521;&#38598;&#21512;&#30340;&#23376;&#20195;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world problems are often comprised of many objectives and require solutions that carefully trade-off between them. Current approaches to many-objective optimization often require challenging assumptions, like knowledge of the importance/difficulty of objectives in a weighted-sum single-objective paradigm, or enormous populations to overcome the curse of dimensionality in multi-objective Pareto optimization. Combining elements from Many-Objective Evolutionary Algorithms and Quality Diversity algorithms like MAP-Elites, we propose Many-objective Optimization via Voting for Elites (MOVE). MOVE maintains a map of elites that perform well on different subsets of the objective functions. On a 14-objective image-neuroevolution problem, we demonstrate that MOVE is viable with a population of as few as 50 elites and outperforms a naive single-objective baseline. We find that the algorithm's performance relies on solutions jumping across bins (for a parent to produce a child that is elite f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#20449;&#24687;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#25340;&#36710;&#65292;&#36890;&#36807;&#39044;&#27979;&#21644;&#36866;&#24212;&#38656;&#27714;&#28608;&#22686;&#65292;&#24182;&#29983;&#25104;&#21327;&#21516;&#30340;&#36335;&#30001;&#21644;&#25509;&#20056;&#31574;&#30053;&#26469;&#26381;&#21153;&#26356;&#22810;&#35831;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.02637</link><description>&lt;p&gt;
Surge Routing&#65306;&#22522;&#20110;&#20107;&#20214;&#20449;&#24687;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#25340;&#36710;
&lt;/p&gt;
&lt;p&gt;
Surge Routing: Event-informed Multiagent Reinforcement Learning for Autonomous Rideshare. (arXiv:2307.02637v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02637
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#20449;&#24687;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#25340;&#36710;&#65292;&#36890;&#36807;&#39044;&#27979;&#21644;&#36866;&#24212;&#38656;&#27714;&#28608;&#22686;&#65292;&#24182;&#29983;&#25104;&#21327;&#21516;&#30340;&#36335;&#30001;&#21644;&#25509;&#20056;&#31574;&#30053;&#26469;&#26381;&#21153;&#26356;&#22810;&#35831;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20107;&#20214;&#36890;&#24120;&#20250;&#24341;&#21457;&#25340;&#36710;&#26381;&#21153;&#38656;&#27714;&#30340;&#28608;&#22686;&#65292;&#32780;&#36825;&#20123;&#38656;&#27714;&#19981;&#20250;&#34987;&#24179;&#22343;&#38656;&#27714;&#27169;&#24335;&#25152;&#25429;&#25417;&#21040;&#65292;&#32473;&#36335;&#30001;&#31639;&#27861;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20986;&#31199;&#36710;&#36710;&#38431;&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#33719;&#21462;&#20107;&#20214;&#25968;&#25454;&#20197;&#39044;&#27979;&#21644;&#36866;&#24212;&#38656;&#27714;&#28608;&#22686;&#65292;&#24182;&#29983;&#25104;&#21327;&#21516;&#30340;&#36335;&#30001;&#21644;&#25509;&#20056;&#31574;&#30053;&#65292;&#20197;&#26381;&#21153;&#26356;&#22810;&#30340;&#35831;&#27714;&#27604;&#20854;&#20182;&#36335;&#30001;&#21327;&#35758;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;(i)&#19968;&#20010;&#20107;&#20214;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20114;&#32852;&#32593;&#19978;&#25235;&#21462;&#20107;&#20214;&#20449;&#24687;&#24182;&#29983;&#25104;&#23494;&#38598;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#29305;&#24449;&#26469;&#39044;&#27979;&#38656;&#27714;&#65307;(ii)&#19968;&#20010;&#21452;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#65292;&#20351;&#29992;&#36825;&#20123;&#23494;&#38598;&#30340;&#21521;&#37327;&#34920;&#31034;&#26469;&#39044;&#27979;&#25972;&#20010;&#22320;&#22270;&#19978;&#30340;&#27599;&#23567;&#26102;&#38656;&#27714;&#65307;(iii)&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#21344;&#29992;&#26102;&#38388;&#34920;&#23558;&#20844;&#24320;&#30340;&#38656;&#27714;&#25968;&#25454;&#26144;&#23556;&#21040;&#31163;&#25955;&#21270;&#30340;&#34903;&#36947;&#20132;&#21449;&#21475;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large events such as conferences, concerts and sports games, often cause surges in demand for ride services that are not captured in average demand patterns, posing unique challenges for routing algorithms. We propose a learning framework for an autonomous fleet of taxis that scrapes event data from the internet to predict and adapt to surges in demand and generates cooperative routing and pickup policies that service a higher number of requests than other routing protocols. We achieve this through a combination of (i) an event processing framework that scrapes the internet for event information and generates dense vector representations that can be used as input features for a neural network that predicts demand; (ii) a two neural network system that predicts hourly demand over the entire map, using these dense vector representations; (iii) a probabilistic approach that leverages locale occupancy schedules to map publicly available demand data over sectors to discretized street inters
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25903;&#25345;AML&#24739;&#32773;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#19987;&#23478;&#38656;&#27714;&#39069;&#22806;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2307.02631</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#20197;&#25903;&#25345;AML&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
An explainable model to support the decision about the therapy protocol for AML. (arXiv:2307.02631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25903;&#25345;AML&#24739;&#32773;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#19987;&#23478;&#38656;&#27714;&#39069;&#22806;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#65288;AML&#65289;&#26159;&#19968;&#31181;&#26368;&#20855;&#20405;&#30053;&#24615;&#30340;&#34880;&#28082;&#32959;&#30244;&#12290;&#20026;&#20102;&#25903;&#25345;&#19987;&#23478;&#20851;&#20110;&#21512;&#36866;&#27835;&#30103;&#30340;&#20915;&#31574;&#65292;AML&#24739;&#32773;&#26681;&#25454;&#20854;&#32454;&#32990;&#36951;&#20256;&#21644;&#20998;&#23376;&#29305;&#24449;&#33719;&#24471;&#39044;&#21518;&#20449;&#24687;&#65292;&#36890;&#24120;&#20998;&#20026;&#26377;&#21033;&#12289;&#20013;&#31561;&#21644;&#19981;&#21033;&#19977;&#20010;&#39118;&#38505;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#24050;&#30693;&#38382;&#39064;&#65292;&#22914;&#21516;&#19968;&#39118;&#38505;&#32452;&#20013;&#24739;&#32773;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#21644;&#20013;&#39118;&#38505;&#31867;&#21035;&#30340;&#28165;&#26224;&#23450;&#20041;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;AML&#24739;&#32773;&#34987;&#24402;&#20026;&#20013;&#39118;&#38505;&#20998;&#31867;&#65292;&#19987;&#23478;&#24120;&#38656;&#36827;&#34892;&#20854;&#20182;&#27979;&#35797;&#21644;&#20998;&#26512;&#65292;&#23548;&#33268;&#27835;&#30103;&#24310;&#36831;&#21644;&#24739;&#32773;&#20020;&#24202;&#29366;&#20917;&#24694;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#25454;&#20998;&#26512;&#21644;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#26681;&#25454;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#30830;&#23450;&#26368;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acute Myeloid Leukemia (AML) is one of the most aggressive types of hematological neoplasm. To support the specialists' decision about the appropriate therapy, patients with AML receive a prognostic of outcomes according to their cytogenetic and molecular characteristics, often divided into three risk categories: favorable, intermediate, and adverse. However, the current risk classification has known problems, such as the heterogeneity between patients of the same risk group and no clear definition of the intermediate risk category. Moreover, as most patients with AML receive an intermediate-risk classification, specialists often demand other tests and analyses, leading to delayed treatment and worsening of the patient's clinical condition. This paper presents the data analysis and an explainable machine-learning model to support the decision about the most appropriate therapy protocol according to the patient's survival prediction. In addition to the prediction model being explainable
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38463;&#37324;&#24052;&#24052;&#24037;&#20316;&#36127;&#36733;&#30719;&#24037;(AWM)&#65292;&#19968;&#20010;&#23454;&#26102;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22823;&#35268;&#27169;&#24037;&#20316;&#36127;&#36733;&#20013;&#21457;&#29616;&#24037;&#20316;&#36127;&#36733;&#27169;&#24335;&#12290;&#36890;&#36807;&#23545;&#29992;&#25143;&#35831;&#27714;&#30340;SQL&#26597;&#35810;&#27169;&#24335;&#36827;&#34892;&#32534;&#30721;&#21644;&#21457;&#29616;&#65292;&#24182;&#22522;&#20110;&#21457;&#29616;&#30340;&#27169;&#24335;&#20248;&#21270;&#26597;&#35810;&#22788;&#29702;&#12290;&#35813;&#31995;&#32479;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#25968;&#25454;&#24211;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#24211;&#31995;&#32479;&#30340;&#36235;&#21183;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.02626</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20113;&#25968;&#25454;&#24211;&#30340;&#23454;&#26102;&#24037;&#20316;&#36127;&#36733;&#27169;&#24335;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Real-time Workload Pattern Analysis for Large-scale Cloud Databases. (arXiv:2307.02626v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38463;&#37324;&#24052;&#24052;&#24037;&#20316;&#36127;&#36733;&#30719;&#24037;(AWM)&#65292;&#19968;&#20010;&#23454;&#26102;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22823;&#35268;&#27169;&#24037;&#20316;&#36127;&#36733;&#20013;&#21457;&#29616;&#24037;&#20316;&#36127;&#36733;&#27169;&#24335;&#12290;&#36890;&#36807;&#23545;&#29992;&#25143;&#35831;&#27714;&#30340;SQL&#26597;&#35810;&#27169;&#24335;&#36827;&#34892;&#32534;&#30721;&#21644;&#21457;&#29616;&#65292;&#24182;&#22522;&#20110;&#21457;&#29616;&#30340;&#27169;&#24335;&#20248;&#21270;&#26597;&#35810;&#22788;&#29702;&#12290;&#35813;&#31995;&#32479;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#25968;&#25454;&#24211;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#24211;&#31995;&#32479;&#30340;&#36235;&#21183;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#31995;&#32479;&#19978;&#25176;&#31649;&#25968;&#25454;&#24211;&#26381;&#21153;&#24050;&#25104;&#20026;&#24120;&#35265;&#20570;&#27861;&#65292;&#36825;&#23548;&#33268;&#25968;&#25454;&#24211;&#24037;&#20316;&#36127;&#36733;&#30340;&#22686;&#21152;&#65292;&#20026;&#27169;&#24335;&#20998;&#26512;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#20174;&#21830;&#19994;&#36923;&#36753;&#30340;&#35282;&#24230;&#21457;&#29616;&#24037;&#20316;&#36127;&#36733;&#27169;&#24335;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#20102;&#35299;&#25968;&#25454;&#24211;&#31995;&#32479;&#30340;&#36235;&#21183;&#21644;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36127;&#36733;&#27169;&#24335;&#21457;&#29616;&#31995;&#32479;&#19981;&#36866;&#29992;&#20110;&#36890;&#24120;&#30001;&#34892;&#19994;&#20351;&#29992;&#30340;&#22823;&#35268;&#27169;&#20113;&#25968;&#25454;&#24211;&#12290;&#36825;&#26159;&#22240;&#20026;&#22823;&#35268;&#27169;&#20113;&#25968;&#25454;&#24211;&#30340;&#24037;&#20316;&#36127;&#36733;&#27169;&#24335;&#36890;&#24120;&#27604;&#26222;&#36890;&#25968;&#25454;&#24211;&#22797;&#26434;&#24471;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38463;&#37324;&#24052;&#24052;&#24037;&#20316;&#36127;&#36733;&#30719;&#24037;(AWM)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#22797;&#26434;&#22823;&#35268;&#27169;&#24037;&#20316;&#36127;&#36733;&#20013;&#21457;&#29616;&#24037;&#20316;&#36127;&#36733;&#27169;&#24335;&#30340;&#23454;&#26102;&#31995;&#32479;&#12290;AWM&#23545;&#29992;&#25143;&#35831;&#27714;&#30340;&#26085;&#24535;&#35760;&#24405;&#30340;SQL&#26597;&#35810;&#27169;&#24335;&#36827;&#34892;&#32534;&#30721;&#21644;&#21457;&#29616;&#65292;&#24182;&#22522;&#20110;&#21457;&#29616;&#30340;&#27169;&#24335;&#20248;&#21270;&#26597;&#35810;&#22788;&#29702;&#12290;&#39318;&#20808;&#65292;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#27169;&#22359;&#25910;&#38598;&#27969;&#24335;&#26597;&#35810;&#26085;&#24535;&#65292;&#24182;&#23545;&#26085;&#24535;&#25968;&#25454;&#36827;
&lt;/p&gt;
&lt;p&gt;
Hosting database services on cloud systems has become a common practice. This has led to the increasing volume of database workloads, which provides the opportunity for pattern analysis. Discovering workload patterns from a business logic perspective is conducive to better understanding the trends and characteristics of the database system. However, existing workload pattern discovery systems are not suitable for large-scale cloud databases which are commonly employed by the industry. This is because the workload patterns of large-scale cloud databases are generally far more complicated than those of ordinary databases. In this paper, we propose Alibaba Workload Miner (AWM), a real-time system for discovering workload patterns in complicated large-scale workloads. AWM encodes and discovers the SQL query patterns logged from user requests and optimizes the querying processing based on the discovered patterns. First, Data Collection &amp; Preprocessing Module collects streaming query logs an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02620</link><description>&lt;p&gt;
&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#35266;&#27979;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#21253;&#25324;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#12289;&#20379;&#26262;&#19982;&#21046;&#20919;&#31995;&#32479;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#20316;-&#24863;&#30693;&#24490;&#29615;&#36890;&#24120;&#20551;&#35774;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#37117;&#21487;&#20197;&#33719;&#24471;&#23545;&#29615;&#22659;&#29366;&#24577;&#30340;&#27979;&#37327;&#65292;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#28023;&#21644;&#34892;&#26143;&#26426;&#22120;&#20154;&#25506;&#32034;&#12289;&#26448;&#26009;&#35774;&#35745;&#21644;&#21307;&#23398;&#31561;&#24212;&#29992;&#20013;&#65292;&#27979;&#37327;&#25110;&#32773;&#36817;&#20284;&#29615;&#22659;&#29366;&#24577;&#21487;&#33021;&#20250;&#20135;&#29983;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36817;&#26469;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#37319;&#21462;&#20102;RL&#20195;&#29702;&#21487;&#33021;&#19981;&#38656;&#35201;&#25110;&#32773;&#19981;&#24819;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#36827;&#34892;&#26114;&#36149;&#27979;&#37327;&#30340;&#35266;&#28857;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Dynamic Multi-Step Observationless Agent (DMSOA)&#65292;&#24182;&#23558;&#20854;&#19982;&#25991;&#29486;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#22312;OpenAI gym&#21644;Atari Pong&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as deep-sea and planetary robot exploration, materials design and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and meas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#32852;&#37030;&#26041;&#27861;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#36865;&#21040;&#20445;&#31649;&#20154;&#30340;&#38450;&#28779;&#22681;&#24182;&#36827;&#34892;&#20803;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#21644;&#20849;&#20139;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#21512;&#30340;$p$-&#20540;&#21512;&#24182;&#26041;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#32852;&#37030;&#27969;&#34892;&#30149;&#30417;&#27979;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.02616</link><description>&lt;p&gt;
&#32852;&#37030;&#27969;&#34892;&#30149;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Federated Epidemic Surveillance. (arXiv:2307.02616v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#32852;&#37030;&#26041;&#27861;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#36865;&#21040;&#20445;&#31649;&#20154;&#30340;&#38450;&#28779;&#22681;&#24182;&#36827;&#34892;&#20803;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#21644;&#20849;&#20139;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#21512;&#30340;$p$-&#20540;&#21512;&#24182;&#26041;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#32852;&#37030;&#27969;&#34892;&#30149;&#30417;&#27979;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30149;&#30340;&#30417;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24403;&#20851;&#38190;&#25968;&#25454;&#20998;&#25955;&#19988;&#21033;&#30410;&#30456;&#20851;&#26041;&#26080;&#27861;&#25110;&#19981;&#24895;&#20849;&#20139;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#24212;&#24320;&#21457;&#32852;&#37030;&#26041;&#27861;&#26469;&#25972;&#21512;&#23454;&#20307;&#24895;&#24847;&#25552;&#20379;&#30340;&#36739;&#19981;&#25935;&#24863;&#30340;&#35777;&#25454;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#23558;&#20551;&#35774;&#26816;&#39564;&#25512;&#36865;&#21040;&#27599;&#20010;&#20445;&#31649;&#20154;&#30340;&#38450;&#28779;&#22681;&#21518;&#65292;&#20877;&#36890;&#36807;&#20803;&#20998;&#26512;&#26469;&#21512;&#24182;&#32467;&#26524;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30830;&#23450;&#37325;&#24314;&#20551;&#35774;&#26816;&#39564;&#21644;&#20248;&#21270;&#25512;&#29702;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#26469;&#35782;&#21035;&#25351;&#26631;&#30340;&#28608;&#22686;&#65292;&#24182;&#23545;&#30495;&#23454;&#25968;&#25454;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21151;&#25928;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#20551;&#35774;&#26816;&#39564;&#30340;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#21512;&#36866;&#30340;$p$-&#20540;&#21512;&#24182;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#20351;&#29992;$p$-&#20540;&#21512;&#24182;&#20316;&#20026;&#27969;&#34892;&#30149;&#30417;&#27979;&#30340;&#32852;&#37030;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#25972;&#21512;&#21487;&#29992;&#20449;&#24687;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The surveillance of a pandemic is a challenging task, especially when crucial data is distributed and stakeholders cannot or are unwilling to share. To overcome this obstacle, federated methodologies should be developed to incorporate less sensitive evidence that entities are willing to provide. This study aims to explore the feasibility of pushing hypothesis tests behind each custodian's firewall and then meta-analysis to combine the results, and to determine the optimal approach for reconstructing the hypothesis test and optimizing the inference. We propose a hypothesis testing framework to identify a surge in the indicators and conduct power analyses and experiments on real and semi-synthetic data to showcase the properties of our proposed hypothesis test and suggest suitable methods for combining $p$-values. Our findings highlight the potential of using $p$-value combination as a federated methodology for pandemic surveillance and provide valuable insights into integrating availabl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#23398;&#20064;&#21644;&#28176;&#36827;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#36807;&#31243;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#33719;&#21462;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#19981;&#28041;&#21450;&#22266;&#23450;&#30340;&#35789;&#27719;&#37327;&#22823;&#23567;&#65292;&#20063;&#19981;&#28041;&#21450;&#26377;&#21306;&#20998;&#24615;&#30340;&#30446;&#26631;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25345;&#32493;&#23398;&#20064;&#26356;&#22810;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.02615</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#21551;&#21457;&#30340;&#28176;&#36827;&#23545;&#40784;&#21644;&#27604;&#36739;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition. (arXiv:2307.02615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#23398;&#20064;&#21644;&#28176;&#36827;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#36807;&#31243;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#33719;&#21462;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#19981;&#28041;&#21450;&#22266;&#23450;&#30340;&#35789;&#27719;&#37327;&#22823;&#23567;&#65292;&#20063;&#19981;&#28041;&#21450;&#26377;&#21306;&#20998;&#24615;&#30340;&#30446;&#26631;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25345;&#32493;&#23398;&#20064;&#26356;&#22810;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#21463;&#30417;&#30563;&#21644;&#25345;&#32493;&#30340;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#20511;&#37492;&#20102;&#20154;&#31867;&#23156;&#20799;&#20064;&#24471;&#31532;&#19968;&#38376;&#35821;&#35328;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#27604;&#36739;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#33719;&#21462;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#21463;&#35748;&#30693;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#20351;&#35745;&#31639;&#27169;&#22411;&#33021;&#22815;&#27604;&#36739;&#21508;&#31181;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65292;&#23398;&#20064;&#36807;&#28388;&#20986;&#24182;&#25552;&#21462;&#20849;&#21516;&#30340;&#20449;&#24687;&#29992;&#20110;&#27599;&#20010;&#20849;&#20139;&#30340;&#35821;&#35328;&#26631;&#31614;&#12290;&#25105;&#20204;&#23558;&#35789;&#27719;&#33719;&#21462;&#26694;&#26550;&#23450;&#20041;&#20026;&#26082;&#21253;&#25324;&#20449;&#24687;&#36807;&#28388;&#36807;&#31243;&#65292;&#20063;&#21253;&#25324;&#34920;&#24449;-&#31526;&#21495;&#26144;&#23556;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#19981;&#28041;&#21450;&#22266;&#23450;&#30340;&#35789;&#27719;&#37327;&#22823;&#23567;&#65292;&#20063;&#19981;&#28041;&#21450;&#26377;&#21306;&#20998;&#24615;&#30340;&#30446;&#26631;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25345;&#32493;&#39640;&#25928;&#22320;&#23398;&#20064;&#26356;&#22810;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#25511;&#21046;&#23454;&#39564;&#20013;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#25928;&#22320;&#25345;&#32493;&#23398;&#20064;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language acquisition is an efficient, supervised, and continual process. In this work, we took inspiration from how human babies acquire their first language, and developed a computational process for word acquisition through comparative learning. Motivated by cognitive findings, we generated a small dataset that enables the computation models to compare the similarities and differences of various attributes, learn to filter out and extract the common information for each shared linguistic label. We frame the acquisition of words as not only the information filtration process, but also as representation-symbol mapping. This procedure does not involve a fixed vocabulary size, nor a discriminative objective, and allows the models to continually learn more concepts efficiently. Our results in controlled experiments have shown the potential of this approach for efficient continual learning of grounded words.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;ChatGPT&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#19968;&#20010;&#39069;&#22806;&#30340;&#31354;&#26684;&#25104;&#20026;&#20102;&#35268;&#36991;&#26816;&#27979;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2307.02599</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#31354;&#26684;&#32469;&#36807;ChatGPT&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evade ChatGPT Detectors via A Single Space. (arXiv:2307.02599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;ChatGPT&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#19968;&#20010;&#39069;&#22806;&#30340;&#31354;&#26684;&#25104;&#20026;&#20102;&#35268;&#36991;&#26816;&#27979;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#31038;&#20250;&#20215;&#20540;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;AI&#29983;&#25104;&#20869;&#23481;&#28389;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#26816;&#27979;&#20986;&#20869;&#23481;&#26159;&#30001;ChatGPT&#29983;&#25104;&#36824;&#26159;&#20154;&#31867;&#29983;&#25104;&#30340;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#26159;&#24314;&#31435;&#22312;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#30340;&#20551;&#35774;&#19978;&#30340;&#12290;&#36825;&#20123;&#24046;&#36317;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#20449;&#24687;&#25110;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36136;&#30097;&#20102;&#26816;&#27979;&#22120;&#20013;&#30340;&#20998;&#24067;&#24046;&#36317;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#22320;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#21644;&#39118;&#26684;&#24046;&#36317;&#12290;&#30456;&#21453;&#65292;"&#24494;&#23567;&#30340;&#24046;&#24322;"&#65292;&#22914;&#39069;&#22806;&#30340;&#19968;&#20010;&#31354;&#26684;&#65292;&#22312;&#26816;&#27979;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpaceInfi&#31574;&#30053;&#26469;&#35268;&#36991;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#26816;&#27979;&#22120;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#20026;&#20160;&#20040;SpaceInfi&#33021;&#25104;&#21151;&#35268;&#36991;&#26816;&#27979;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT brings revolutionary social value but also raises concerns about the misuse of AI-generated content. Consequently, an important question is how to detect whether content is generated by ChatGPT or by human. Existing detectors are built upon the assumption that there are distributional gaps between human-generated and AI-generated content. These gaps are typically identified using statistical information or classifiers. Our research challenges the distributional gap assumption in detectors. We find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and AI-generated content. Instead, the "subtle differences", such as an extra space, become crucial for detection. Based on this discovery, we propose the SpaceInfi strategy to evade detection. Experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. We also provide a theoretical explanation for why SpaceInfi is successful in evading perple
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02591</link><description>&lt;p&gt;
ODD: &#19968;&#20221;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#65288;ORAB&#65289;&#26159;&#38450;&#27490;&#33647;&#29289;&#36807;&#37327;&#30340;&#26032;&#39118;&#38505;&#22240;&#32032;&#12290;&#20197;&#24448;&#65292;ORAB&#20027;&#35201;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#21644;&#33647;&#29289;&#32473;&#20104;&#30417;&#27979;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19981;&#33021;&#28085;&#30422;&#25152;&#26377;&#24322;&#24120;&#34892;&#20026;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;ORAB&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#24191;&#27867;&#26377;&#35760;&#24405;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;ORAB&#26816;&#27979;&#12290;ODD&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;750&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#12290;ODD&#26088;&#22312;&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#35782;&#21035;ORAB&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20061;&#20010;&#31867;&#21035;&#65306;1&#65289;&#24050;&#30830;&#35748;&#24322;&#24120;&#34892;&#20026;&#65292;2&#65289;&#26263;&#31034;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;3&#65289;&#38463;&#29255;&#31867;&#33647;&#29289;&#65292;4&#65289;&#36866;&#24212;&#30151;&#65292;5&#65289;&#24050;&#35786;&#26029;&#30340;&#38463;&#29255;&#21046;&#21058;&#20381;&#36182;&#65292;6&#65289;&#33519;&#20108;&#27694;&#24179;&#31867;&#33647;&#29289;&#65292;7&#65289;&#33647;&#29289;&#21464;&#21270;&#65292;8&#65289;&#19982;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30456;&#20851;&#65292;9&#65289;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioid related aberrant behaviors (ORAB) present novel risk factors for opioid overdose. Previously, ORAB have been mainly assessed by survey results and by monitoring drug administrations. Such methods however, cannot scale up and do not cover the entire spectrum of aberrant behaviors. On the other hand, ORAB are widely documented in electronic health record notes. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset comprising of more than 750 publicly available EHR notes. ODD has been designed to identify ORAB from patients' EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing (NLP) mode
&lt;/p&gt;</description></item><item><title>TransformerG2G&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#26102;&#24577;&#22270;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.02588</link><description>&lt;p&gt;
TransformerG2G&#65306;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#23398;&#20064;&#26102;&#24577;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02588
&lt;/p&gt;
&lt;p&gt;
TransformerG2G&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#26102;&#24577;&#22270;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#23884;&#20837;&#24050;&#25104;&#20026;&#22788;&#29702;&#19981;&#21516;&#26102;&#38388;&#22270;&#20998;&#26512;&#20219;&#21153;&#65288;&#22914;&#38142;&#36335;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#22270;&#29983;&#25104;&#65289;&#30340;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#20123;&#26102;&#24577;&#22270;&#23637;&#29616;&#20102;&#24322;&#36136;&#30340;&#30636;&#26102;&#21160;&#24577;&#12289;&#19981;&#21516;&#30340;&#26102;&#38388;&#38388;&#38548;&#20197;&#21450;&#22312;&#28436;&#21270;&#36807;&#31243;&#20013;&#39640;&#24230;&#21464;&#21270;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#23558;&#21382;&#21490;&#22270;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#34701;&#20837;&#21040;&#23398;&#20064;&#26102;&#24577;&#21160;&#24577;&#30340;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;TransformerG2G&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#20174;&#24403;&#21069;&#29366;&#24577;&#65288;$t$&#65289;&#21644;&#20043;&#21069;&#30340;&#19978;&#19979;&#25991;&#65288;&#26102;&#38388;&#25139;[$t-1, t-l$]&#65292;$l$&#26159;&#19978;&#19979;&#25991;&#30340;&#38271;&#24230;&#65289;&#20013;&#39318;&#20808;&#23398;&#20064;&#20013;&#38388;&#33410;&#28857;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#20010;&#25237;&#24433;&#23618;&#26469;&#29983;&#25104;&#27599;&#20010;&#33410;&#28857;&#30340;&#20302;&#32500;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#65292;&#20316;&#20026;&#20854;&#28508;&#22312;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node's latent embedding at ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#34920;&#24449;&#24046;&#24322;&#24615;&#26469;&#38477;&#20302;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#24120;&#35265;&#22833;&#36133;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#26550;&#26500;&#20043;&#38388;&#19981;&#21516;&#28145;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#24046;&#24322;&#24615;&#65292;&#20197;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#22833;&#36133;&#27169;&#24335;&#30340;&#24378;&#22823;&#38598;&#21512;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38598;&#21512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02516</link><description>&lt;p&gt;
&#25506;&#32034;&#26032;&#30340;&#26041;&#27861;&#65306;&#24378;&#21270;&#34920;&#24449;&#24046;&#24322;&#20197;&#23398;&#20064;&#26032;&#29305;&#24449;&#24182;&#38477;&#20302;&#38169;&#35823;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency. (arXiv:2307.02516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#34920;&#24449;&#24046;&#24322;&#24615;&#26469;&#38477;&#20302;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#24120;&#35265;&#22833;&#36133;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#26550;&#26500;&#20043;&#38388;&#19981;&#21516;&#28145;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#24046;&#24322;&#24615;&#65292;&#20197;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#22833;&#36133;&#27169;&#24335;&#30340;&#24378;&#22823;&#38598;&#21512;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38598;&#21512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#23398;&#20064;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#22312;&#19968;&#32452;&#29420;&#31435;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36825;&#23548;&#33268;&#39044;&#27979;&#30456;&#20851;&#24615;&#21644;&#24120;&#35265;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#20197;&#24448;&#30340;&#23581;&#35797;&#30528;&#37325;&#20110;&#20943;&#23567;&#36755;&#20986;&#39044;&#27979;&#25110;logits&#30340;&#30456;&#20851;&#24615;&#65292;&#32467;&#26524;&#20135;&#29983;&#20102;&#19981;&#19968;&#33268;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#65292;&#21363;&#21033;&#29992;&#34920;&#24449;&#30456;&#20284;&#24615;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#24046;&#24322;&#24615;&#65292;&#32780;&#19981;&#26159;&#34913;&#37327;&#35757;&#32451;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20419;&#36827;&#20102;&#26550;&#26500;&#20043;&#38388;&#19981;&#21516;&#28145;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#30340;&#24046;&#24322;&#24615;&#65292;&#24182;&#26088;&#22312;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#22833;&#36133;&#27169;&#24335;&#30340;&#24378;&#22823;&#38598;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#39640;&#24230;&#24046;&#24322;&#30340;&#20013;&#38388;&#34920;&#31034;&#23548;&#33268;&#26356;&#23569;&#30456;&#20851;&#30340;&#36755;&#20986;&#39044;&#27979;&#21644;&#31245;&#24494;&#36739;&#20302;&#30340;&#38169;&#35823;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#39318;&#27425;&#25581;&#31034;&#20102;&#36830;&#25509;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Independently trained machine learning models tend to learn similar features. Given an ensemble of independently trained models, this results in correlated predictions and common failure modes. Previous attempts focusing on decorrelation of output predictions or logits yielded mixed results, particularly due to their reduction in model accuracy caused by conflicting optimization objectives. In this paper, we propose the novel idea of utilizing methods of the representational similarity field to promote dissimilarity during training instead of measuring similarity of trained models. To this end, we promote intermediate representations to be dissimilar at different depths between architectures, with the goal of learning robust ensembles with disjoint failure modes. We show that highly dissimilar intermediate representations result in less correlated output predictions and slightly lower error consistency, resulting in higher ensemble accuracy. With this, we shine first light on the conne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24739;&#32773;&#35821;&#38899;&#21644;&#36716;&#24405;&#25968;&#25454;&#65292;&#25506;&#32034;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#26816;&#27979;&#65292;&#21253;&#25324;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#38899;&#39057;&#25968;&#25454;&#30340;&#34701;&#21512;&#12290;&#26368;&#21518;&#23581;&#35797;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02514</link><description>&lt;p&gt;
&#20351;&#29992;&#24739;&#32773;&#35821;&#38899;&#36716;&#24405;&#21644;&#38899;&#39057;&#25968;&#25454;&#25506;&#32034;&#22810;&#27169;&#24577;&#26041;&#27861;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploring Multimodal Approaches for Alzheimer's Disease Detection Using Patient Speech Transcript and Audio Data. (arXiv:2307.02514v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24739;&#32773;&#35821;&#38899;&#21644;&#36716;&#24405;&#25968;&#25454;&#65292;&#25506;&#32034;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#26816;&#27979;&#65292;&#21253;&#25324;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#38899;&#39057;&#25968;&#25454;&#30340;&#34701;&#21512;&#12290;&#26368;&#21518;&#23581;&#35797;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20005;&#37325;&#24433;&#21709;&#24739;&#32773;&#20581;&#24247;&#30340;&#30196;&#21574;&#30151;&#12290;&#30001;&#20110;AD&#20250;&#24433;&#21709;&#24739;&#32773;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#24739;&#32773;&#30340;&#35821;&#38899;&#21487;&#20197;&#20316;&#20026;&#25351;&#31034;&#35813;&#30142;&#30149;&#30340;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;DementiaBank Pitt&#25968;&#25454;&#24211;&#30340;&#24739;&#32773;&#35821;&#38899;&#21644;&#36716;&#24405;&#25968;&#25454;&#65292;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;AD&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20174;&#35821;&#38899;&#36716;&#24405;&#26500;&#24314;&#22270;&#65292;&#24182;&#20351;&#29992;GNN&#25552;&#21462;&#29305;&#24449;&#29992;&#20110;AD&#26816;&#27979;&#12290;&#37319;&#29992;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21253;&#25324;&#21516;&#20041;&#35789;&#26367;&#25442;&#12289;&#22522;&#20110;GPT&#30340;&#22686;&#24378;&#22120;&#31561;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38598;&#22823;&#23567;&#36739;&#23567;&#30340;&#38382;&#39064;&#12290;&#36824;&#24341;&#20837;&#20102;&#38899;&#39057;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;WavLM&#27169;&#22411;&#25552;&#21462;&#38899;&#39057;&#29305;&#24449;&#12290;&#28982;&#21518;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#25991;&#26412;&#29305;&#24449;&#34701;&#21512;&#12290;&#26368;&#21518;&#65292;&#23581;&#35797;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#24405;&#22238;&#38899;&#39057;&#24182;&#23558;&#20854;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is a common form of dementia that severely impacts patient health. As AD impairs the patient's language understanding and expression ability, the speech of AD patients can serve as an indicator of this disease. This study investigates various methods for detecting AD using patients' speech and transcripts data from the DementiaBank Pitt database. The proposed approach involves pre-trained language models and Graph Neural Network (GNN) that constructs a graph from the speech transcript, and extracts features using GNN for AD detection. Data augmentation techniques, including synonym replacement, GPT-based augmenter, and so on, were used to address the small dataset size. Audio data was also introduced, and WavLM model was used to extract audio features. These features were then fused with text features using various methods. Finally, a contrastive learning approach was attempted by converting speech transcripts back to audio and using it for contrastive learning
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25913;&#36827;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02511</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27004;&#23618;&#24179;&#38754;&#31034;&#20363;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Computational Design at the Example of Floor Plans. (arXiv:2307.02511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25913;&#36827;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#29983;&#25104;&#22120;&#22240;&#20854;&#33021;&#22815;&#26681;&#25454;&#31616;&#21333;&#30340;&#25991;&#26412;&#25552;&#31034;&#21019;&#24314;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#35752;&#35770;&#12290;&#20294;&#26159;&#65292;&#22312;&#22303;&#26408;&#24037;&#31243;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#38656;&#35201;&#33021;&#22815;&#26681;&#25454;&#32473;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#21019;&#24314;&#29305;&#23450;&#30340;&#24314;&#31569;&#35774;&#35745;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#27004;&#23618;&#24179;&#38754;&#20316;&#20026;&#31034;&#20363;&#65292;&#25506;&#32034;&#22522;&#20110;&#25193;&#25955;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#30446;&#21069;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#22810;&#27425;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#23558;&#29983;&#25104;&#30340;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#20174;6%&#25552;&#39640;&#21040;90%&#65292;&#24182;&#25913;&#36827;&#20102;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#38656;&#35201;&#12290;&#36890;&#36807;&#36825;&#20123;&#65292;&#25105;&#20204;&#20026;&#22303;&#26408;&#24037;&#31243;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Image generators based on diffusion models are widely discussed recently for their capability to create images from simple text prompts. But, for practical use in civil engineering they need to be able to create specific construction plans for given constraints. Within this paper we explore the capabilities of those diffusion-based AI generators for computational design at the example of floor plans and identify their current limitation. We explain how the diffusion-models work and propose new diffusion models with improved semantic encoding. In several experiments we show that we can improve validity of generated floor plans from 6% to 90% and query performance for different examples. We identify short comings and derive future research challenges of those models and discuss the need to combine diffusion models with building information modelling. With this we provide key insights into the current state and future directions for diffusion models in civil engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.02507</link><description>&lt;p&gt;
STS-CCL&#65306;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#39044;&#27979;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting. (arXiv:2307.02507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#25429;&#25417;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20132;&#36890;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#36825;&#20010;&#22256;&#22659;&#65292;&#26412;&#25991;&#37319;&#29992;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21516;&#27493;&#19978;&#19979;&#25991;&#23545;&#27604;&#23398;&#20064;&#65288;STS-CCL&#65289;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#29992;&#20110;&#26102;&#31354;&#22270;&#25968;&#25454;&#30340;&#22522;&#26412;&#21644;&#24378;&#22823;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25200;&#21160;&#20102;&#22270;&#32467;&#26500;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21033;&#29992;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#36827;&#34892;&#33258;&#36866;&#24212;&#22686;&#24378;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#31354;&#21516;&#27493;&#23545;&#27604;&#27169;&#22359;&#65288;STS-CM&#65289;&#65292;&#20197;&#21516;&#26102;&#25429;&#25417;&#33391;&#22909;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#24182;&#23454;&#29616;&#22270;&#32423;&#23545;&#27604;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21306;&#20998;&#36127;&#31579;&#36873;&#20013;&#30340;&#33410;&#28857;&#20010;&#20307;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#29305;&#24449;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#23545;&#27604;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33410;&#28857;&#32423;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains to be a challenging task. In considering of the dilemma, this work employs the advanced contrastive learning and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, a Semantic Contextual Contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#22238;&#39038;&#20102;&#22823;&#22411;&#20195;&#30721;&#35757;&#32451;&#30340;transformer-based&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;AI&#36741;&#21161;&#32534;&#31243;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#25688;&#35201;&#12289;&#32570;&#38519;&#26816;&#27979;&#31561;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#23558;NLP&#25216;&#26415;&#19982;&#36719;&#20214;&#33258;&#28982;&#21270;&#30456;&#32467;&#21512;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2307.02503</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#21644;&#29702;&#35299;&#22823;&#22411;&#20195;&#30721;&#29992;&#20110;AI&#36741;&#21161;&#32534;&#31243;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review. (arXiv:2307.02503v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02503
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#22238;&#39038;&#20102;&#22823;&#22411;&#20195;&#30721;&#35757;&#32451;&#30340;transformer-based&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;AI&#36741;&#21161;&#32534;&#31243;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#25688;&#35201;&#12289;&#32570;&#38519;&#26816;&#27979;&#31561;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#23558;NLP&#25216;&#26415;&#19982;&#36719;&#20214;&#33258;&#28982;&#21270;&#30456;&#32467;&#21512;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#25991;&#29486;&#65292;&#29305;&#21035;&#20851;&#27880;&#20351;&#29992;&#22823;&#22411;&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#30340;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;AI&#36741;&#21161;&#32534;&#31243;&#20219;&#21153;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#32463;&#36807;&#36719;&#20214;&#33258;&#28982;&#21270;&#22686;&#24378;&#30340;LLMs&#22312;&#20419;&#36827;AI&#36741;&#21161;&#32534;&#31243;&#24212;&#29992;&#26041;&#38754;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#34917;&#20840;&#12289;&#20195;&#30721;&#32763;&#35793;&#12289;&#20195;&#30721;&#20248;&#21270;&#12289;&#20195;&#30721;&#25688;&#35201;&#12289;&#32570;&#38519;&#26816;&#27979;&#21644;&#20811;&#38534;&#26816;&#27979;&#12290;&#20854;&#20013;&#33879;&#21517;&#30340;&#24212;&#29992;&#21253;&#25324;&#30001;OpenAI&#30340;Codex&#21644;DeepMind AlphaCode&#39537;&#21160;&#30340;GitHub Copilot&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20027;&#35201;&#30340;LLMs&#21450;&#20854;&#22312;&#19982;AI&#36741;&#21161;&#32534;&#31243;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#23558;NLP&#25216;&#26415;&#19982;&#36719;&#20214;&#33258;&#28982;&#21270;&#30456;&#32467;&#21512;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#35752;&#35770;&#20102;&#25193;&#23637;AI&#36741;&#21161;&#32534;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive review of the literature concerning the utilization of Natural Language Processing (NLP) techniques, with a particular focus on transformer-based large language models (LLMs) trained using Big Code, within the domain of AI-assisted programming tasks. LLMs, augmented with software naturalness, have played a crucial role in facilitating AI-assisted programming applications, including code generation, code completion, code translation, code refinement, code summarization, defect detection, and clone detection. Notable examples of such applications include the GitHub Copilot powered by OpenAI's Codex and DeepMind AlphaCode. This paper presents an overview of the major LLMs and their applications in downstream tasks related to AI-assisted programming. Furthermore, it explores the challenges and opportunities associated with incorporating NLP techniques with software naturalness in these applications, with a discussion on extending AI-assisted programming 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23398;&#26234;&#33021;&#20307;&#21644;&#25968;&#23398;&#23884;&#20837;&#20316;&#20026;&#35299;&#20915;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22522;&#22240;&#32452;&#23398;&#24212;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;GPT&#30340;&#24037;&#20316;&#27969;&#23558;&#25991;&#29486;&#20013;&#30340;&#26041;&#31243;&#36716;&#25442;&#20026;LaTeX&#21644;Python&#26684;&#24335;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#21644;&#20132;&#20114;&#24335;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.02502</link><description>&lt;p&gt;
&#25968;&#23398;&#26234;&#33021;&#20307;&#65306;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#12289;&#25968;&#23398;&#23884;&#20837;&#21644;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics. (arXiv:2307.02502v1 [q-bio.OT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23398;&#26234;&#33021;&#20307;&#21644;&#25968;&#23398;&#23884;&#20837;&#20316;&#20026;&#35299;&#20915;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22522;&#22240;&#32452;&#23398;&#24212;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;GPT&#30340;&#24037;&#20316;&#27969;&#23558;&#25991;&#29486;&#20013;&#30340;&#26041;&#31243;&#36716;&#25442;&#20026;LaTeX&#21644;Python&#26684;&#24335;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#21644;&#20132;&#20114;&#24335;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21487;&#20197;&#36890;&#36807;&#26356;&#26131;&#20110;&#29702;&#35299;&#30340;&#25968;&#23398;&#30693;&#35782;&#26469;&#25552;&#21319;&#12290;&#38500;&#20102;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#20197;&#22806;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20063;&#22312;&#32534;&#31243;&#12289;&#31639;&#27861;&#21457;&#29616;&#21644;&#23450;&#29702;&#35777;&#26126;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#22522;&#22240;&#32452;&#23398;&#24212;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#36739;&#22823;&#12290;&#26412;&#39033;&#30446;&#24341;&#20837;&#20102;&#25968;&#23398;&#26234;&#33021;&#20307;&#21644;&#25968;&#23398;&#23884;&#20837;&#20316;&#20026;&#8220;&#25968;&#23398;&#25705;&#23572;&#23450;&#24459;&#8221;&#30340;&#26032;&#36827;&#23637;&#65292;&#20351;&#29992;&#22522;&#20110;GPT&#30340;&#24037;&#20316;&#27969;&#23558;&#25991;&#29486;&#20013;&#30340;&#26041;&#31243;&#36716;&#25442;&#20026;LaTeX&#21644;Python&#26684;&#24335;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#25968;&#23383;&#26041;&#31243;&#34920;&#31034;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#33258;&#21160;&#21270;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#24037;&#20855;&#12290;LLM&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#25552;&#20379;&#20102;&#35821;&#35328;&#29992;&#25143;&#30028;&#38754;&#65292;&#24182;&#20026;&#22823;&#35268;&#27169;&#30340;AI&#36741;&#21161;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#35821;&#35328;&#12290;&#37492;&#20110;&#26080;&#38480;&#30340;&#24418;&#24335;&#21487;&#33021;&#31354;&#38388;&#65292;&#19982;&#25968;&#23398;&#20114;&#21160;&#30340;&#25968;&#23398;&#26234;&#33021;&#20307;&#26377;&#21487;&#33021;&#20351;&#25105;&#20204;&#20174;&#8220;&#22823;&#25968;&#25454;&#8221;&#36716;&#21521;&#8220;&#22823;&#25968;&#23398;&#8221;&#12290;&#25968;&#23398;&#19982;&#33258;&#28982;&#35821;&#35328;&#19981;&#21516;&#65292;&#20855;&#26377;&#21487;&#20197;&#36890;&#36807;&#35777;&#26126;&#26469;&#39564;&#35777;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#22312;&#24212;&#29992;&#33539;&#22260;&#19978;&#26356;&#21152;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement in generative AI could be boosted with more accessible mathematics. Beyond human-AI chat, large language models (LLMs) are emerging in programming, algorithm discovery, and theorem proving, yet their genomics application is limited. This project introduces Math Agents and mathematical embedding as fresh entries to the "Moore's Law of Mathematics", using a GPT-based workflow to convert equations from literature into LaTeX and Python formats. While many digital equation representations exist, there's a lack of automated large-scale evaluation tools. LLMs are pivotal as linguistic user interfaces, providing natural language access for human-AI chat and formal languages for large-scale AI-assisted computational infrastructure. Given the infinite formal possibility spaces, Math Agents, which interact with math, could potentially shift us from "big data" to "big math". Math, unlike the more flexible natural language, has properties subject to proof, enabling its use beyond tr
&lt;/p&gt;</description></item><item><title>mPLUG-DocOwl&#26159;&#19968;&#31181;&#27169;&#22359;&#21270;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#12290;&#23427;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;&#12289;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#21644;&#25991;&#26723;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25552;&#21319;&#20102;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02499</link><description>&lt;p&gt;
mPLUG-DocOwl: &#27169;&#22359;&#21270;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding. (arXiv:2307.02499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02499
&lt;/p&gt;
&lt;p&gt;
mPLUG-DocOwl&#26159;&#19968;&#31181;&#27169;&#22359;&#21270;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#12290;&#23427;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;&#12289;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#21644;&#25991;&#26723;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25552;&#21319;&#20102;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#29702;&#35299;&#26159;&#25351;&#20174;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#23383;&#25991;&#26723;&#20013;&#33258;&#21160;&#25552;&#21462;&#12289;&#20998;&#26512;&#21644;&#29702;&#35299;&#20449;&#24687;&#65292;&#20363;&#22914;&#32593;&#39029;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#21253;&#25324;mPLUG-Owl&#65292;&#24050;&#32463;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;OCR&#30340;&#25991;&#26412;&#35782;&#21035;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#39046;&#22495;&#20869;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#24573;&#35270;OCR&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#22914;&#22797;&#26434;&#30340;&#34920;&#26684;&#25110;&#22823;&#22359;&#25991;&#26412;&#65292;&#36825;&#20123;&#29305;&#24449;&#23545;&#20110;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;mPLUG-Owl&#25552;&#20986;&#20102;mPLUG-DocOwl&#65292;&#29992;&#20110;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#35270;&#35273;-&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#38024;&#23545;&#35821;&#35328;&#12289;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#21644;&#25991;&#26723;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#26469;&#22686;&#24378;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified ins
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#34917;&#19969;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#30340;&#22270;&#20687;&#25110;&#28508;&#22312;&#31354;&#38388;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#24037;&#19994;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#65292;&#24182;&#19982;&#20854;&#20182;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.02495</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#20110;&#34917;&#19969;&#33258;&#32534;&#30721;&#22120;&#30340;&#22270;&#20687;&#25110;&#28508;&#22312;&#31354;&#38388;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#24037;&#19994;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis. (arXiv:2307.02495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#34917;&#19969;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#30340;&#22270;&#20687;&#25110;&#28508;&#22312;&#31354;&#38388;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#24037;&#19994;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#65292;&#24182;&#19982;&#20854;&#20182;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#34917;&#19969;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#30340;&#39068;&#33394;&#22270;&#20687;&#20013;&#26816;&#27979;&#24322;&#24120;&#30340;&#20960;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#21407;&#22987;&#22270;&#20687;&#21644;&#20854;&#37325;&#26500;&#20043;&#38388;&#30340;&#35823;&#24046;&#12289;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#27491;&#24120;&#22270;&#20687;&#20998;&#24067;&#30340;&#25903;&#25345;&#20272;&#35745;&#20197;&#21450;&#22522;&#20110;&#21407;&#22987;&#22270;&#20687;&#19982;&#37325;&#24314;&#22270;&#20687;&#30340;&#24674;&#22797;&#29256;&#26412;&#20043;&#38388;&#30340;&#35823;&#24046;&#36825;&#19977;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#24037;&#19994;&#22270;&#20687;&#25968;&#25454;&#24211;MVTecAD&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19982;&#20004;&#31181;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study several methods for detecting anomalies in color images, constructed on patch-based auto-encoders. Wecompare the performance of three types of methods based, first, on the error between the original image and its reconstruction,second, on the support estimation of the normal image distribution in the latent space, and third, on the error between the originalimage and a restored version of the reconstructed image. These methods are evaluated on the industrial image database MVTecADand compared to two competitive state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#22330;&#26223;TFDA&#65292;&#21363;&#19977;&#26080;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30446;&#26631;&#26631;&#31614;&#12289;&#28304;&#25968;&#25454;&#21644;&#39046;&#22495;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#23454;&#29992;&#65292;&#36991;&#20813;&#20102;&#23545;&#20808;&#21069;&#39046;&#22495;&#20449;&#24687;&#30340;&#20381;&#36182;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02493</link><description>&lt;p&gt;
FREEDOM: &#26080;&#30446;&#26631;&#26631;&#31614;&#12289;&#26080;&#28304;&#25968;&#25454;&#21644;&#26080;&#39046;&#22495;&#20449;&#24687;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization. (arXiv:2307.02493v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#22330;&#26223;TFDA&#65292;&#21363;&#19977;&#26080;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30446;&#26631;&#26631;&#31614;&#12289;&#28304;&#25968;&#25454;&#21644;&#39046;&#22495;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#23454;&#29992;&#65292;&#36991;&#20813;&#20102;&#23545;&#20808;&#21069;&#39046;&#22495;&#20449;&#24687;&#30340;&#20381;&#36182;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26381;&#21153;&#35282;&#24230;&#26469;&#30475;&#65292;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#26159;&#19968;&#31181;&#36866;&#24212;&#37096;&#32626;&#27169;&#22411;&#21040;&#23458;&#25143;&#25968;&#25454;&#38598;&#30340;&#26377;&#24076;&#26395;&#30340;&#22330;&#26223;&#12290;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#30446;&#26631;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#36866;&#24212;&#65292;&#24182;&#25903;&#25345;&#22810;&#20010;&#39046;&#22495;&#26500;&#24314;&#30340;&#28304;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#39046;&#22495;&#20449;&#24687;&#65292;&#21363;&#26377;&#22810;&#23569;&#20010;&#39046;&#22495;&#23384;&#22312;&#20197;&#21450;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#39046;&#22495;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;MSDA&#38656;&#35201;&#21516;&#26102;&#25317;&#26377;&#28304;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#65288;&#29289;&#29702;&#19978;&#65289;&#65292;&#36825;&#20250;&#23548;&#33268;&#23458;&#25143;&#35774;&#22791;&#23384;&#20648;&#38480;&#21046;&#25110;&#36890;&#36807;&#23558;&#23458;&#25143;&#25968;&#25454;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#24341;&#36215;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#26356;&#23454;&#38469;&#30340;&#27169;&#22411;&#33258;&#36866;&#24212;&#22330;&#26223;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#22330;&#26223;&#65292;&#21363;&#19977;&#26080;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;TFDA&#65289;&#65292;&#20854;&#20013;1&#65289;&#30446;&#26631;&#26631;&#31614;&#65292;2&#65289;&#28304;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#22823;&#37096;&#20998;3&#65289;&#28304;&#39046;&#22495;&#20449;&#24687;&#65288;&#39046;&#22495;&#26631;&#31614;+&#39046;&#22495;&#25968;&#37327;&#65289;&#37117;&#19981;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
From a service perspective, Multi-Source Domain Adaptation (MSDA) is a promising scenario to adapt a deployed model to a client's dataset. It can provide adaptation without a target label and support the case where a source dataset is constructed from multiple domains. However, it is impractical, wherein its training heavily relies on prior domain information of the multi-source dataset -- how many domains exist and the domain label of each data sample. Moreover, MSDA requires both source and target datasets simultaneously (physically), causing storage limitations on the client device or data privacy issues by transferring client data to a server. For a more practical scenario of model adaptation from a service provider's point of view, we relax these constraints and present a novel problem scenario of Three-Free Domain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, and mostly 3) source domain information (domain labels + the number of domains) are unavailable. Und
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;TablEye&#65292;&#36890;&#36807;&#29983;&#25104;&#34920;&#26684;&#22270;&#20687;&#26469;&#23454;&#29616;&#39046;&#22495;&#36716;&#25442;&#65292;&#20811;&#26381;&#20102;&#24418;&#25104;&#34920;&#26684;&#25968;&#25454;&#20808;&#39564;&#30693;&#35782;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.02491</link><description>&lt;p&gt;
TablEye: &#36890;&#36807;&#22270;&#20687;&#35270;&#35282;&#30475;&#23567;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
TablEye: Seeing small Tables through the Lens of Images. (arXiv:2307.02491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;TablEye&#65292;&#36890;&#36807;&#29983;&#25104;&#34920;&#26684;&#22270;&#20687;&#26469;&#23454;&#29616;&#39046;&#22495;&#36716;&#25442;&#65292;&#20811;&#26381;&#20102;&#24418;&#25104;&#34920;&#26684;&#25968;&#25454;&#20808;&#39564;&#30693;&#35782;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#30340;&#25506;&#32034;&#21464;&#24471;&#36843;&#20999;&#12290;&#34920;&#26684;&#25968;&#25454;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#26679;&#30340;&#20449;&#24687;&#65292;&#20294;&#20063;&#23384;&#22312;&#25968;&#25454;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#38480;&#21046;&#12290;&#26631;&#35760;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#21487;&#34892;&#25429;&#25417;&#27599;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#29420;&#31435;&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#31232;&#32570;&#24615;&#20197;&#21450;&#23450;&#20041;&#34920;&#26684;&#25968;&#25454;&#36793;&#30028;&#30340;&#20869;&#22312;&#27169;&#31946;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26410;&#24320;&#21457;&#20986;&#26080;&#38480;&#21046;&#26465;&#20214;&#30340;&#26377;&#24847;&#20041;&#19988;&#21487;&#34892;&#30340;&#23569;&#26679;&#26412;&#34920;&#26684;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;TablEye&#65292;&#36890;&#36807;&#37319;&#29992;&#39046;&#22495;&#36716;&#25442;&#26469;&#20811;&#26381;&#24418;&#25104;&#34920;&#26684;&#25968;&#25454;&#20808;&#39564;&#30693;&#35782;&#30340;&#38480;&#21046;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#34920;&#26684;&#22270;&#20687;&#26469;&#23454;&#29616;&#39046;&#22495;&#36716;&#25442;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20869;&#22312;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration of few-shot tabular learning becomes imperative. Tabular data is a versatile representation that captures diverse information, yet it is not exempt from limitations, property of data and model size. Labeling extensive tabular data can be challenging, and it may not be feasible to capture every important feature. Few-shot tabular learning, however, remains relatively unexplored, primarily due to scarcity of shared information among independent datasets and the inherent ambiguity in defining boundaries within tabular data. To the best of our knowledge, no meaningful and unrestricted few-shot tabular learning techniques have been developed without imposing constraints on the dataset. In this paper, we propose an innovative framework called TablEye, which aims to overcome the limit of forming prior knowledge for tabular data by adopting domain transformation. It facilitates domain transformation by generating tabular images, which effectively conserve the intrinsic semantic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#21472;&#21152;&#25991;&#26412;&#23545;VQA&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#20570;&#27861;&#19981;&#20250;&#20005;&#37325;&#38477;&#20302;VQA&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#39564;&#35777;&#22312;&#22270;&#20687;&#19978;&#21472;&#21152;&#25991;&#26412;&#30340;&#23454;&#36341;&#26041;&#24335;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;VQA&#20219;&#21153;&#20013;&#20351;&#29992;AI&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.02489</link><description>&lt;p&gt;
&#22270;&#20687;&#20013;&#24102;&#26377;&#21472;&#21152;&#25991;&#26412;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) on Images with Superimposed Text. (arXiv:2307.02489v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#21472;&#21152;&#25991;&#26412;&#23545;VQA&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#20570;&#27861;&#19981;&#20250;&#20005;&#37325;&#38477;&#20302;VQA&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#39564;&#35777;&#22312;&#22270;&#20687;&#19978;&#21472;&#21152;&#25991;&#26412;&#30340;&#23454;&#36341;&#26041;&#24335;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;VQA&#20219;&#21153;&#20013;&#20351;&#29992;AI&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21472;&#21152;&#30340;&#25991;&#26412;&#27880;&#37322;&#19968;&#30452;&#26159;&#30740;&#31350;&#19981;&#36275;&#30340;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#26222;&#36941;&#23384;&#22312;&#19988;&#38750;&#24120;&#37325;&#35201;&#12290;&#21307;&#23398;&#22270;&#20687;&#36824;&#31361;&#20986;&#20102;&#20302;&#20998;&#36776;&#29575;&#12289;&#22122;&#22768;&#21644;&#21472;&#21152;&#30340;&#25991;&#26412;&#20803;&#20449;&#24687;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#21472;&#21152;&#25991;&#26412;&#23545;VQA&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;AI&#25216;&#26415;&#36827;&#34892;VQA&#20219;&#21153;&#30340;&#21307;&#23398;&#22270;&#20687;&#20013;&#65292;&#28155;&#21152;&#36825;&#20123;&#25991;&#26412;&#20803;&#20449;&#24687;&#20063;&#19981;&#20250;&#20005;&#37325;&#38477;&#20302;VQA&#24615;&#33021;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#20204;&#39564;&#35777;&#20102;&#22312;&#22270;&#20687;&#19978;&#21472;&#21152;&#25991;&#26412;&#30340;&#20570;&#27861;&#65292;&#29978;&#33267;&#36866;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;VQA&#20219;&#21153;&#12290;&#35813;&#30740;&#31350;&#26377;&#21161;&#20110;&#25512;&#36827;&#23545;VQA&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Superimposed text annotations have been under-investigated, yet are ubiquitous, useful and important, especially in medical images. Medical images also highlight the challenges posed by low resolution, noise and superimposed textual meta-information. Therefor we probed the impact of superimposing text onto medical images on VQA. Our results revealed that this textual meta-information can be added without severely degrading key measures of VQA performance. Our findings are significant because they validate the practice of superimposing text on images, even for medical images subjected to the VQA task using AI techniques. The work helps advance understanding of VQA in general and, in particular, in the domain of healthcare and medicine.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#35777;&#26126;&#35268;&#21010;&#20013;&#30340;&#28436;&#32462;&#21487;&#21152;&#24615;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#39640;&#25928;&#30340;&#35268;&#21010;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#30340;&#21069;&#25552;&#38472;&#36848;&#24635;&#21644;&#25509;&#36817;&#20110;&#22522;&#20110;&#36825;&#20123;&#21069;&#25552;&#30340;&#32467;&#35770;&#23884;&#20837;&#12290;&#20174;&#32780;&#35777;&#26126;&#20102;&#28436;&#32462;&#21487;&#21152;&#24615;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2307.02472</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35777;&#26126;&#35268;&#21010;&#30340;&#28436;&#32462;&#21487;&#21152;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deductive Additivity for Planning of Natural Language Proofs. (arXiv:2307.02472v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#35777;&#26126;&#35268;&#21010;&#20013;&#30340;&#28436;&#32462;&#21487;&#21152;&#24615;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#39640;&#25928;&#30340;&#35268;&#21010;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#30340;&#21069;&#25552;&#38472;&#36848;&#24635;&#21644;&#25509;&#36817;&#20110;&#22522;&#20110;&#36825;&#20123;&#21069;&#25552;&#30340;&#32467;&#35770;&#23884;&#20837;&#12290;&#20174;&#32780;&#35777;&#26126;&#20102;&#28436;&#32462;&#21487;&#21152;&#24615;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35774;&#35745;&#29992;&#20110;&#22810;&#27493;&#39588;&#21629;&#39064;&#39564;&#35777;&#30340;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26816;&#32034;&#19968;&#32452;&#30456;&#20851;&#30340;&#21069;&#25552;&#38472;&#36848;&#65288;&#35268;&#21010;&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#36825;&#20123;&#38472;&#36848;&#20013;&#29983;&#25104;&#26032;&#30340;&#32467;&#35770;&#65288;&#28436;&#32462;&#65289;&#12290;&#35268;&#21010;&#38454;&#27573;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;Transformer&#25805;&#20316;&#65292;&#24182;&#19988;&#26080;&#27861;&#25193;&#23637;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#21069;&#25552;&#38472;&#36848;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#19982;&#28436;&#32462;&#25512;&#29702;&#20860;&#23481;&#30340;&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#39640;&#25928;&#30340;&#35268;&#21010;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23884;&#20837;&#31354;&#38388;&#26159;&#21542;&#20855;&#26377;&#25105;&#20204;&#31216;&#20043;&#20026;&#28436;&#32462;&#21487;&#21152;&#24615;&#30340;&#29305;&#24615;&#65306;&#21069;&#25552;&#38472;&#36848;&#23884;&#20837;&#30340;&#24635;&#21644;&#24212;&#35813;&#25509;&#36817;&#22522;&#20110;&#36825;&#20123;&#21069;&#25552;&#30340;&#32467;&#35770;&#30340;&#23884;&#20837;&#12290;&#38500;&#20102;&#26469;&#33258;GPT3&#30340;&#24494;&#35843;&#23884;&#20837;&#21644;&#26469;&#33258;BM25&#30340;&#31232;&#30095;&#23884;&#20837;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22810;&#31181;&#29616;&#25104;&#30340;&#23494;&#38598;&#23884;&#20837;&#28304;&#12290;&#25105;&#20204;&#22312;&#20869;&#22312;&#19978;&#30740;&#31350;&#20102;&#23884;&#20837;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#28436;&#32462;&#21487;&#21152;&#24615;&#30340;&#23646;&#24615;&#26159;&#21542;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current natural language systems designed for multi-step claim validation typically operate in two phases: retrieve a set of relevant premise statements using heuristics (planning), then generate novel conclusions from those statements using a large language model (deduction). The planning step often requires expensive Transformer operations and does not scale to arbitrary numbers of premise statements. In this paper, we investigate whether an efficient planning heuristic is possible via embedding spaces compatible with deductive reasoning. Specifically, we evaluate whether embedding spaces exhibit a property we call deductive additivity: the sum of premise statement embeddings should be close to embeddings of conclusions based on those premises. We explore multiple sources of off-the-shelf dense embeddings in addition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We study embedding models both intrinsically, evaluating whether the property of deductive additivity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20855;&#36523;&#20219;&#21153;&#35268;&#21010;&#20195;&#29702;&#65288;TaPA&#65289;&#65292;&#36890;&#36807;&#23558;LLM&#19982;&#35270;&#35273;&#24863;&#30693;&#27169;&#22411;&#23545;&#40784;&#65292;&#26681;&#25454;&#22330;&#26223;&#20013;&#24050;&#23384;&#22312;&#30340;&#23545;&#35937;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#21033;&#29992;GPT-3.5&#29983;&#25104;&#30340;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20855;&#36523;&#35745;&#21010;&#30340;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.01848</link><description>&lt;p&gt;
&#24102;&#26377;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20855;&#36523;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Embodied Task Planning with Large Language Models. (arXiv:2307.01848v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20855;&#36523;&#20219;&#21153;&#35268;&#21010;&#20195;&#29702;&#65288;TaPA&#65289;&#65292;&#36890;&#36807;&#23558;LLM&#19982;&#35270;&#35273;&#24863;&#30693;&#27169;&#22411;&#23545;&#40784;&#65292;&#26681;&#25454;&#22330;&#26223;&#20013;&#24050;&#23384;&#22312;&#30340;&#23545;&#35937;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#21033;&#29992;GPT-3.5&#29983;&#25104;&#30340;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20855;&#36523;&#35745;&#21010;&#30340;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#36890;&#29992;&#29615;&#22659;&#20013;&#25104;&#21151;&#23436;&#25104;&#22797;&#26434;&#30340;&#20154;&#31867;&#25351;&#20196;&#65292;&#20026;&#20855;&#36523;&#26234;&#33021;&#20307;&#25552;&#20379;&#24120;&#35782;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#30340;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#22312;&#22797;&#26434;&#20219;&#21153;&#30340;&#35745;&#21010;&#29983;&#25104;&#20013;&#23884;&#20837;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;&#28982;&#32780;&#23427;&#20204;&#32570;&#20047;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#20250;&#20135;&#29983;&#19981;&#21487;&#34892;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#36523;&#20219;&#21153;&#35268;&#21010;&#20195;&#29702;&#65288;TaPA&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#29289;&#29702;&#22330;&#26223;&#32422;&#26463;&#30340;&#20855;&#36523;&#20219;&#21153;&#35268;&#21010;&#65292;&#20854;&#20013;&#20195;&#29702;&#26681;&#25454;&#22330;&#26223;&#20013;&#24050;&#23384;&#22312;&#30340;&#23545;&#35937;&#36890;&#36807;&#23558;LLM&#19982;&#35270;&#35273;&#24863;&#30693;&#27169;&#22411;&#23545;&#40784;&#26469;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#23460;&#20869;&#22330;&#26223;&#12289;&#25351;&#20196;&#21644;&#34892;&#21160;&#35745;&#21010;&#19977;&#20803;&#32452;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25105;&#20204;&#20026;GPT-3.5&#25552;&#20379;&#20102;&#35774;&#35745;&#30340;&#25552;&#31034;&#20449;&#24687;&#21644;&#22330;&#26223;&#20013;&#24050;&#23384;&#22312;&#30340;&#23545;&#35937;&#21015;&#34920;&#65292;&#20197;&#29983;&#25104;&#22823;&#37327;&#30340;&#25351;&#20196;&#21644;&#30456;&#24212;&#30340;&#35745;&#21010;&#34892;&#21160;&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20855;&#36523;&#35745;&#21010;&#30340;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-traine
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#38656;&#35201;&#37327;&#23376;&#35745;&#31639;&#65292;&#21457;&#29616;&#25968;&#23383;&#30828;&#20214;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01301</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19979;&#19968;&#20195;&#26159;&#21542;&#38656;&#35201;&#37327;&#23376;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reliable AI: Does the Next Generation Require Quantum Computing?. (arXiv:2307.01301v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01301
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#38656;&#35201;&#37327;&#23376;&#35745;&#31639;&#65292;&#21457;&#29616;&#25968;&#23383;&#30828;&#20214;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#38656;&#35201;&#37327;&#23376;&#35745;&#31639;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#26159;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#30340;&#26680;&#24515;&#12290;&#22240;&#27492;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#38752;&#24615;&#21644;&#20540;&#24471;&#20449;&#36182;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#20363;&#22914;&#38544;&#31169;&#12289;&#36131;&#20219;&#12289;&#23433;&#20840;&#31561;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#37117;&#23384;&#22312;&#30528;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#26377;&#21508;&#31181;&#21407;&#22240;&#65292;&#21253;&#25324;&#25968;&#25454;&#19981;&#36275;&#12289;&#20559;&#35265;&#12289;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#22522;&#26412;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#22914;&#25968;&#23383;&#30828;&#20214;&#19978;&#30340;&#21487;&#35745;&#31639;&#24615;&#38382;&#39064;&#12290;&#36825;&#20123;&#21487;&#35745;&#31639;&#24615;&#38382;&#39064;&#30340;&#26681;&#28304;&#22312;&#20110;&#25968;&#23383;&#30828;&#20214;&#22522;&#20110;&#22270;&#28789;&#26426;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#31163;&#25955;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#23383;&#30828;&#20214;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this survey, we aim to explore the fundamental question of whether the next generation of artificial intelligence requires quantum computing. Artificial intelligence is increasingly playing a crucial role in many aspects of our daily lives and is central to the fourth industrial revolution. It is therefore imperative that artificial intelligence is reliable and trustworthy. However, there are still many issues with reliability of artificial intelligence, such as privacy, responsibility, safety, and security, in areas such as autonomous driving, healthcare, robotics, and others. These problems can have various causes, including insufficient data, biases, and robustness problems, as well as fundamental issues such as computability problems on digital hardware. The cause of these computability problems is rooted in the fact that digital hardware is based on the computing model of the Turing machine, which is inherently discrete. Notably, our findings demonstrate that digital hardware i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAL&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#20195;&#34920;&#24615;&#38169;&#35823;&#30340;&#25968;&#25454;&#23454;&#20363;&#26469;&#25913;&#36827;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#32771;&#34385;&#38169;&#35823;&#23454;&#20363;&#21450;&#20854;&#37051;&#22495;&#20013;&#30340;&#38169;&#35823;&#23494;&#24230;&#65292;REAL&#22312;&#20934;&#30830;&#29575;&#21644;F1-macro&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.00968</link><description>&lt;p&gt;
REAL:&#19968;&#31181;&#22522;&#20110;&#20195;&#34920;&#24615;&#38169;&#35823;&#39537;&#21160;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
REAL: A Representative Error-Driven Approach for Active Learning. (arXiv:2307.00968v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAL&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#20195;&#34920;&#24615;&#38169;&#35823;&#30340;&#25968;&#25454;&#23454;&#20363;&#26469;&#25913;&#36827;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#32771;&#34385;&#38169;&#35823;&#23454;&#20363;&#21450;&#20854;&#37051;&#22495;&#20013;&#30340;&#38169;&#35823;&#23494;&#24230;&#65292;REAL&#22312;&#20934;&#30830;&#29575;&#21644;F1-macro&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#26377;&#26377;&#38480;&#26631;&#35760;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#20197;&#33719;&#21462;&#26631;&#31614;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20027;&#21160;&#23398;&#20064;&#36890;&#24120;&#26681;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#26469;&#34913;&#37327;&#26410;&#26631;&#35760;&#23454;&#20363;&#30340;&#20449;&#24687;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#24182;&#19981;&#32771;&#34385;&#20855;&#26377;&#37051;&#22495;&#38169;&#35823;&#23494;&#24230;&#30340;&#38169;&#35823;&#23454;&#20363;&#65292;&#36825;&#20123;&#23454;&#20363;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAL&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#20195;&#34920;&#24615;&#38169;&#35823;&#30340;&#25968;&#25454;&#23454;&#20363;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#12290;&#23427;&#23558;&#23569;&#25968;&#39044;&#27979;&#20316;&#20026;&#32858;&#31867;&#20013;&#30340;&#8220;&#20266;&#38169;&#35823;&#8221;&#36827;&#34892;&#35782;&#21035;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#38169;&#35823;&#23494;&#24230;&#20026;&#35813;&#32858;&#31867;&#20998;&#37197;&#33258;&#36866;&#24212;&#30340;&#37319;&#26679;&#39044;&#31639;&#12290;&#22312;&#20116;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;REAL&#22312;&#20934;&#30830;&#29575;&#21644;F1-macro&#24471;&#20998;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#34920;&#29616;&#26368;&#20339;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose $REAL$, a novel approach to select data instances with $\underline{R}$epresentative $\underline{E}$rrors for $\underline{A}$ctive $\underline{L}$earning. It identifies minority predictions as \emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00209</link><description>&lt;p&gt;
&#22270;&#20687;&#30340;&#37325;&#35201;&#24615;&#65306;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22840;&#24352;&#65292;&#21363;&#22840;&#22823;&#20854;&#35789;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#22840;&#24352;&#26816;&#27979;&#26159;&#29702;&#35299;&#20154;&#31867;&#34920;&#36798;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#24050;&#32463;&#26377;&#20960;&#39033;&#20851;&#20110;&#22840;&#24352;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#25991;&#26412;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#27169;&#24577;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#31561;&#65289;&#26469;&#34920;&#36798;&#22840;&#24352;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#24494;&#21338;&#65288;&#20013;&#22269;&#30340;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#19968;&#20123;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#24494;&#21338;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#35270;&#20026;&#20004;&#31181;&#27169;&#24577;&#65292;&#25506;&#32034;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#22312;&#22840;&#24352;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#36825;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#20116;&#20010;&#19981;&#21516;&#30340;&#20027;&#39064;&#26500;&#24314;&#30340;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection of hyperbole is an important part of understanding human expression. There have been several studies on hyperbole detection, but most of which focus on text modality only. However, with the development of social media, people can create hyperbolic expressions with various modalities, including text, images, videos, etc. In this paper, we focus on multimodal hyperbole detection. We create a multimodal detection dataset\footnote{The dataset will be released to the community.} from Weibo (a Chinese social media) and carry out some studies on it. We treat the text and image from a piece of weibo as two modalities and explore the role of text and image for hyperbole detection. Different pre-trained multimodal encoders are also evaluated on this downstream task to show their performance. Besides, since this dataset is constructed from five different topics, we also evaluate the cross-domain performance of different 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#30340;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#65292;&#22312;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#27604;&#36739;&#20013;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15880</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards Open Vocabulary Learning: A Survey. (arXiv:2306.15880v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#30340;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#65292;&#22312;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#27604;&#36739;&#20013;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#26816;&#27979;&#31561;&#21508;&#31181;&#26680;&#24515;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22522;&#20110;&#23553;&#38381;&#38598;&#30340;&#20551;&#35774;&#65292;&#21363;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#24050;&#23450;&#20041;&#30340;&#31867;&#21035;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#12290;&#36825;&#20123;&#26032;&#26041;&#27861;&#26088;&#22312;&#23450;&#20301;&#21644;&#35782;&#21035;&#36229;&#20986;&#27880;&#37322;&#26631;&#31614;&#31354;&#38388;&#30340;&#31867;&#21035;&#12290;&#19982;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#30456;&#27604;&#65292;&#24320;&#25918;&#35789;&#27719;&#26041;&#27861;&#26356;&#21152;&#36890;&#29992;&#12289;&#23454;&#29992;&#21644;&#26377;&#25928;&#12290;&#26412;&#25991;&#23545;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#36817;&#26399;&#22312;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#24320;&#25918;&#38598;&#35782;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#31561;&#30456;&#20851;&#27010;&#24565;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#22312;&#20998;&#21106;&#20219;&#21153;&#30340;&#20960;&#20010;&#32039;&#23494;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of visual scene understanding, deep neural networks have made impressive advancements in various core tasks like segmentation, tracking, and detection. However, most approaches operate on the close-set assumption, meaning that the model can only identify pre-defined categories that are present in the training set. Recently, open vocabulary settings were proposed due to the rapid progress of vision language pre-training. These new approaches seek to locate and recognize categories beyond the annotated label space. The open vocabulary approach is more general, practical, and effective compared to weakly supervised and zero-shot settings. This paper provides a thorough review of open vocabulary learning, summarizing and analyzing recent developments in the field. In particular, we begin by comparing it to related concepts such as zero-shot learning, open-set recognition, and out-of-distribution detection. Then, we review several closely related tasks in the case of segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.15782</link><description>&lt;p&gt;
UTRNet: &#21360;&#21047;&#25991;&#26723;&#20013;&#39640;&#20998;&#36776;&#29575;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#23610;&#24230;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;UTRNet&#26550;&#26500;&#65292;&#19968;&#20010;&#28151;&#21512;CNN-RNN&#27169;&#22411;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#24037;&#20316;&#24456;&#38590;&#25512;&#24191;&#21040;&#20044;&#23572;&#37117;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#36275;&#22815;&#30340;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UTRSet-Real&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;11,000&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;UTRSet-Synth&#65292;&#19968;&#20010;&#19982;&#23454;&#38469;&#19990;&#30028;&#38750;&#24120;&#30456;&#20284;&#30340;&#21547;&#26377;20,000&#34892;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;IIITH&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#30495;&#23454;&#24615;&#36827;&#34892;&#20102;&#20462;&#27491;&#65292;&#20351;&#20854;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#26356;&#21487;&#38752;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;UrduDoc&#65292;&#19968;&#31181;&#29992;&#20110;&#25195;&#25551;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#24037;&#20855;&#65292;&#36890;&#36807;&#23558;UTRNet&#19982;&#25991;&#26412;&#30340;&#31471;&#21040;&#31471;&#20044;&#23572;&#37117;OCR&#38598;&#25104;&#22312;&#21360;&#21047;&#25991;&#26723;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multi-scale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAPS&#21644;MAPS-SE&#20004;&#20010;&#31639;&#27861;&#65292;&#21487;&#22312;&#22810;&#40657;&#30418;&#39044;&#35328;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#24182;&#20027;&#21160;&#36873;&#25321;&#21644;&#25913;&#36827;&#26368;&#20248;&#39044;&#35328;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10259</link><description>&lt;p&gt;
&#22810;&#40657;&#30418;&#39044;&#35328;&#19979;&#30340;&#20027;&#21160;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Active Policy Improvement from Multiple Black-box Oracles. (arXiv:2306.10259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAPS&#21644;MAPS-SE&#20004;&#20010;&#31639;&#27861;&#65292;&#21487;&#22312;&#22810;&#40657;&#30418;&#39044;&#35328;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#24182;&#20027;&#21160;&#36873;&#25321;&#21644;&#25913;&#36827;&#26368;&#20248;&#39044;&#35328;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#22797;&#26434;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30830;&#23450;&#26377;&#25928;&#31574;&#30053;&#24448;&#24448;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#25351;&#23548;&#25506;&#32034;&#65292;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#19979;&#65292;&#20154;&#20204;&#36890;&#24120;&#21482;&#33021;&#25509;&#35302;&#21040;&#22810;&#20010;&#27425;&#20248;&#30340;&#40657;&#30418;&#39044;&#35328;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#26368;&#20248;&#30340;&#39044;&#35328;&#65292;&#36825;&#20123;&#39044;&#35328;&#19981;&#33021;&#22312;&#25152;&#26377;&#29366;&#24577;&#19979;&#26222;&#36941;&#20248;&#20110;&#24444;&#27492;&#65292;&#36825;&#32473;&#20027;&#21160;&#20915;&#23450;&#22312;&#21738;&#31181;&#29366;&#24577;&#19979;&#20351;&#29992;&#21738;&#31181;&#39044;&#35328;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#21508;&#33258;&#20272;&#35745;&#20540;&#20989;&#25968;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;MAPS&#21644;MAPS-SE&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has made significant strides in various complex domains. However, identifying an effective policy via RL often necessitates extensive exploration. Imitation learning aims to mitigate this issue by using expert demonstrations to guide exploration. In real-world scenarios, one often has access to multiple suboptimal black-box experts, rather than a single optimal oracle. These experts do not universally outperform each other across all states, presenting a challenge in actively deciding which oracle to use and in which state. We introduce MAPS and MAPS-SE, a class of policy improvement algorithms that perform imitation learning from multiple suboptimal oracles. In particular, MAPS actively selects which of the oracles to imitate and improve their value function estimates, and MAPS-SE additionally leverages an active state exploration criterion to determine which states one should explore. We provide a comprehensive theoretical analysis and demonstrate that MAP
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22686;&#24378;&#27169;&#22411;&#22534;&#21472;&#26041;&#27861;&#65292;&#22312;Codeforces&#21644;Leetcode&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;77.8%&#30340;&#20934;&#30830;&#29575;&#21644;0.815&#30340;PR-AUC&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.10077</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#21442;&#25968;&#35843;&#20248;&#27169;&#22411;&#30340;&#22534;&#21472;&#26041;&#27861;&#35299;&#20915;&#32534;&#30721;&#38382;&#39064;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Stacking of Hyperparameter Tuned Models for Tagging Coding Problems. (arXiv:2306.10077v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22686;&#24378;&#27169;&#22411;&#22534;&#21472;&#26041;&#27861;&#65292;&#22312;Codeforces&#21644;Leetcode&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;77.8%&#30340;&#20934;&#30830;&#29575;&#21644;0.815&#30340;PR-AUC&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#30721;&#38382;&#39064;&#26159;&#38656;&#35201;&#20197;&#35745;&#31639;&#26426;&#31243;&#24207;&#24418;&#24335;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#12290;&#32534;&#30721;&#38382;&#39064;&#22312;&#23398;&#29983;&#21644;&#19987;&#19994;&#20154;&#22763;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#21319;&#20182;&#20204;&#30340;&#25216;&#33021;&#21644;&#32844;&#19994;&#26426;&#20250;&#12290;&#19968;&#20010;&#33021;&#22815;&#24110;&#21161;&#32451;&#20064;&#32534;&#30721;&#38382;&#39064;&#30340;AI&#31995;&#32479;&#23558;&#38750;&#24120;&#26377;&#29992;&#65292;&#24182;&#19988;&#23384;&#22312;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22686;&#24378;&#27169;&#22411;&#22534;&#21472;&#26041;&#27861;&#65292;&#20197;&#22312;&#20174;Codeforces&#21644;Leetcode&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;77.8&#65285;&#30340;&#20934;&#30830;&#29575;&#21644;0.815&#30340;PR-AUC&#20998;&#25968;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#20026;&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coding problems are problems that require a solution in the form of a computer program. Coding problems are popular among students and professionals as it enhances their skills and career opportunities. An AI system that would help those who practice coding problems would be highly useful and there is a huge potential for such a system. In this work, we propose a model which uses stacking of hyperparameter tuned boosting models to achieve impressive metric scores of 77.8% accuracy and 0.815 PR-AUC on the dataset that was scraped from Codeforces and Leetcode. We open source the dataset and the models developed for this work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#28040;&#36153;&#32773;&#32423;&#35745;&#31639;&#26426;&#19978;&#25968;&#31186;&#20869;&#20248;&#21270;&#26426;&#22120;&#20154;&#32467;&#26500;&#20197;&#36798;&#21040;&#25152;&#38656;&#34892;&#20026;&#65292;&#20026;&#33258;&#21160;&#21270;&#35774;&#35745;&#22797;&#26434;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03263</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#39640;&#25928;&#33258;&#21160;&#21270;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient automatic design of robots. (arXiv:2306.03263v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#28040;&#36153;&#32773;&#32423;&#35745;&#31639;&#26426;&#19978;&#25968;&#31186;&#20869;&#20248;&#21270;&#26426;&#22120;&#20154;&#32467;&#26500;&#20197;&#36798;&#21040;&#25152;&#38656;&#34892;&#20026;&#65292;&#20026;&#33258;&#21160;&#21270;&#35774;&#35745;&#22797;&#26434;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#20154;&#30340;&#29289;&#29702;&#32467;&#26500;&#12289;&#24863;&#23448;&#12289;&#39532;&#36798;&#24067;&#23616;&#21644;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#20204;&#30340;&#35774;&#35745;&#36890;&#24120;&#38750;&#24120;&#22256;&#38590;&#12290;20&#24180;&#26469;&#65292;&#21551;&#21457;&#20110;&#33258;&#28982;&#30028;&#30340;&#36827;&#21270;&#35774;&#35745;&#65292;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#33258;&#21160;&#35774;&#35745;&#26426;&#22120;&#20154;&#24050;&#32463;&#23581;&#35797;&#36807;&#26080;&#25968;&#27425;&#65292;&#20294;&#36825;&#20063;&#38750;&#24120;&#20302;&#25928;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#39318;&#27425;&#22312;&#21333;&#20010;&#28040;&#36153;&#32773;&#32423;&#35745;&#31639;&#26426;&#19978;&#30340;&#25968;&#31186;&#20869;&#20248;&#21270;&#26426;&#22120;&#20154;&#32467;&#26500;&#20197;&#23637;&#29616;&#25152;&#38656;&#34892;&#20026;&#65292;&#24182;&#21046;&#36896;&#20986;&#20855;&#26377;&#35813;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#26799;&#24230;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26412;&#31639;&#27861;&#19981;&#39044;&#35774;&#20219;&#20309;&#29305;&#23450;&#30340;&#35299;&#21078;&#24418;&#24335;&#65292;&#32780;&#26159;&#20174;&#38543;&#26426;&#29983;&#25104;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#31181;&#32676;&#24320;&#22987;&#65292;&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#26469;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#33258;&#21160;&#21270;&#35774;&#35745;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#38590;&#20197;&#25163;&#21160;&#35774;&#35745;&#30340;&#22797;&#26434;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots are notoriously difficult to design because of complex interdependencies between their physical structure, sensory and motor layouts, and behavior. Despite this, almost every detail of every robot built to date has been manually determined by a human designer after several months or years of iterative ideation, prototyping, and testing. Inspired by evolutionary design in nature, the automated design of robots using evolutionary algorithms has been attempted for two decades, but it too remains inefficient: days of supercomputing are required to design robots in simulation that, when manufactured, exhibit desired behavior. Here we show for the first time de-novo optimization of a robot's structure to exhibit a desired behavior, within seconds on a single consumer-grade computer, and the manufactured robot's retention of that behavior. Unlike other gradient-based robot design methods, this algorithm does not presuppose any particular anatomical form; starting instead from a randoml
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.09378</link><description>&lt;p&gt;
&#22312;Lenia&#20013;&#25429;&#33719;&#26032;&#20852;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09378
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39033;&#30446;&#25506;&#35752;&#20102;Lenia&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25968;&#23383;&#29983;&#29289;&#31995;&#32479;&#30340;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;&#12290;Lenia&#30340;&#29983;&#24577;&#31995;&#32479;&#30001;&#31616;&#21333;&#30340;&#20154;&#24037;&#29983;&#29289;&#32452;&#25104;&#65292;&#23427;&#20204;&#21487;&#20197;&#31227;&#21160;&#12289;&#28040;&#32791;&#12289;&#29983;&#38271;&#21644;&#32321;&#27542;&#12290;&#35813;&#24179;&#21488;&#26159;&#19968;&#20010;&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#21644;&#36827;&#21270;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#21019;&#24314;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#34892;&#20026;&#30340;&#22810;&#26679;&#21270;&#29983;&#29289;&#12290;&#35813;&#30740;&#31350;&#30340;&#20851;&#38190;&#26159;&#22312;Lenia&#20013;&#27979;&#37327;&#22797;&#26434;&#24615;&#65292;&#35782;&#21035;&#27979;&#37327;&#35268;&#21017;&#30340;&#38271;&#26399;&#22797;&#26434;&#24615;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#36827;&#21270;&#20986;&#23578;&#26410;&#21457;&#29616;&#30340;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;&#36951;&#20256;&#31639;&#27861;&#20351;&#29992;&#30456;&#37051;&#21306;&#22495;&#25110;&#26680;&#20316;&#20026;&#22522;&#22240;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;Lenia&#30340;&#20854;&#20182;&#21442;&#25968;&#65288;&#20363;&#22914;&#29983;&#38271;&#20989;&#25968;&#65289;&#19981;&#21464;&#65292;&#20197;&#20135;&#29983;&#19981;&#21516;&#20154;&#21475;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#28982;&#21518;&#27979;&#37327;&#36866;&#24212;&#24230;&#20540;&#20197;&#20915;&#23450;&#25152;&#24471;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#21464;&#21270;&#20316;&#20026;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
This research project investigates Lenia, an artificial life platform that simulates ecosystems of digital creatures. Lenia's ecosystem consists of simple, artificial organisms that can move, consume, grow, and reproduce. The platform is important as a tool for studying artificial life and evolution, as it provides a scalable and flexible environment for creating a diverse range of organisms with varying abilities and behaviors. Measuring complexity in Lenia is a key aspect of the study, which identifies the metrics for measuring long-term complex emerging behavior of rules, with the aim of evolving better Lenia behaviors which are yet not discovered. The Genetic Algorithm uses neighborhoods or kernels as genotype while keeping the rest of the parameters of Lenia as fixed, for example growth function, to produce different behaviors respective to the population and then measures fitness value to decide the complexity of the resulting behavior. First, we use Variation over Time as a fitn
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CubeScope&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#22810;&#26041;&#38754;&#22320;&#25366;&#25496;&#22797;&#26434;&#30340;&#26102;&#38388;&#25139;&#20107;&#20214;&#27969;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#31361;&#28982;&#30340;&#19981;&#36830;&#32493;&#24615;&#21644;&#19981;&#21516;&#30340;&#21160;&#24577;&#27169;&#24335;&#65292;&#24182;&#23545;&#25152;&#26377;&#23646;&#24615;&#36827;&#34892;&#22810;&#26041;&#38754;&#25688;&#35201;&#65292;&#24182;&#21457;&#29616;&#38544;&#34255;&#30340;&#32676;&#20307;&#21644;&#20854;&#20851;&#31995;&#12290;CubeScope&#36824;&#33021;&#26816;&#27979;&#21040;&#24322;&#24120;&#30340;&#31361;&#28982;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03789</link><description>&lt;p&gt;
&#24555;&#36895;&#21644;&#22810;&#26041;&#38754;&#25366;&#25496;&#22797;&#26434;&#30340;&#26102;&#38388;&#25139;&#20107;&#20214;&#27969;
&lt;/p&gt;
&lt;p&gt;
Fast and Multi-aspect Mining of Complex Time-stamped Event Streams. (arXiv:2303.03789v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CubeScope&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#22810;&#26041;&#38754;&#22320;&#25366;&#25496;&#22797;&#26434;&#30340;&#26102;&#38388;&#25139;&#20107;&#20214;&#27969;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#31361;&#28982;&#30340;&#19981;&#36830;&#32493;&#24615;&#21644;&#19981;&#21516;&#30340;&#21160;&#24577;&#27169;&#24335;&#65292;&#24182;&#23545;&#25152;&#26377;&#23646;&#24615;&#36827;&#34892;&#22810;&#26041;&#38754;&#25688;&#35201;&#65292;&#24182;&#21457;&#29616;&#38544;&#34255;&#30340;&#32676;&#20307;&#21644;&#20854;&#20851;&#31995;&#12290;CubeScope&#36824;&#33021;&#26816;&#27979;&#21040;&#24322;&#24120;&#30340;&#31361;&#28982;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#22312;&#32447;&#26102;&#38388;&#28436;&#21464;&#20107;&#20214;&#27969;&#65288;&#22914;&#22312;&#32447;&#36141;&#29289;&#26085;&#24535;&#65306;&#39033;&#30446;&#12289;&#20215;&#26684;&#12289;&#21697;&#29260;&#12289;&#26102;&#38388;&#21644;&#22320;&#29702;&#20301;&#32622;&#27963;&#21160;&#65306;&#19978;&#36710;&#21644;&#19979;&#36710;&#22320;&#28857;&#12289;&#26102;&#38388;&#65289;&#65292;&#25105;&#20204;&#22914;&#20309;&#23545;&#22823;&#35268;&#27169;&#12289;&#21160;&#24577;&#12289;&#39640;&#38454;&#24352;&#37327;&#27969;&#36827;&#34892;&#25688;&#35201;&#65311;&#25105;&#20204;&#30340;&#22238;&#31572;&#26159;&#19987;&#27880;&#20110;&#20004;&#31181;&#31867;&#22411;&#30340;&#27169;&#24335;&#65292;&#21363;&#8220;&#21046;&#24230;&#8221;&#21644;&#8220;&#32452;&#20214;&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#26041;&#27861;CubeScope&#26469;&#22788;&#29702;&#39640;&#38454;&#24352;&#37327;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#35782;&#21035;&#20219;&#20309;&#31361;&#28982;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#24182;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#21160;&#24577;&#27169;&#24335;&#8220;&#21046;&#24230;&#8221;&#65288;&#20363;&#22914;&#24037;&#20316;&#26085;/&#21608;&#26411;/&#20551;&#26399;&#27169;&#24335;&#65289;&#12290;&#22312;&#27599;&#20010;&#21046;&#24230;&#20013;&#65292;&#23427;&#36824;&#23545;&#25152;&#26377;&#23646;&#24615;&#65288;&#20363;&#22914;&#39033;&#30446;&#12289;&#20215;&#26684;&#12289;&#21697;&#29260;&#21644;&#26102;&#38388;&#65289;&#36827;&#34892;&#22810;&#26041;&#38754;&#25688;&#35201;&#65292;&#24182;&#21457;&#29616;&#34920;&#31034;&#28508;&#22312;&#32676;&#20307;&#65288;&#20363;&#22914;&#39033;&#30446;/&#21697;&#29260;&#32676;&#65289;&#21450;&#20854;&#20851;&#31995;&#30340;&#38544;&#34255;&#30340;&#8220;&#32452;&#20214;&#8221;&#12290;&#30001;&#20110;&#20854;&#31616;&#27905;&#32780;&#26377;&#25928;&#30340;&#25688;&#35201;&#65292;CubeScope&#36824;&#33021;&#26816;&#27979;&#21040;&#24322;&#24120;&#30340;&#31361;&#28982;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a huge, online stream of time-evolving events with multiple attributes, such as online shopping logs: (item, price, brand, time), and local mobility activities: (pick-up and drop-off locations, time), how can we summarize large, dynamic high-order tensor streams? How can we see any hidden patterns, rules, and anomalies? Our answer is to focus on two types of patterns, i.e., ''regimes'' and ''components'', for which we present CubeScope, an efficient and effective method over high-order tensor streams. Specifically, it identifies any sudden discontinuity and recognizes distinct dynamical patterns, ''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it also performs multi-way summarization for all attributes (e.g., item, price, brand, and time) and discovers hidden ''components'' representing latent groups (e.g., item/brand groups) and their relationship. Thanks to its concise but effective summarization, CubeScope can also detect the sudden appearance of anomalie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#31070;&#32463;&#21183;&#30340;&#38750;&#24179;&#34913;&#20998;&#23376;&#21435;&#22122;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#39640;&#21183;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02216</link><description>&lt;p&gt;
&#29992;&#20110;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#31070;&#32463;&#21183;&#30340;&#38750;&#24179;&#34913;&#20998;&#23376;&#21435;&#22122;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials. (arXiv:2303.02216v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#31070;&#32463;&#21183;&#30340;&#38750;&#24179;&#34913;&#20998;&#23376;&#21435;&#22122;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#39640;&#21183;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#36827;&#23637;&#20351;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#24320;&#21457;&#24555;&#36895;&#30340;&#27169;&#22411;&#26367;&#20195;&#26114;&#36149;&#30340;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21147;&#23398;&#65288;QM&#65289;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#20998;&#23376;&#21183;&#33021;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;GNN&#24314;&#31435;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#30340;&#21183;&#33021;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25968;&#25454;&#21463;&#21040;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;QM&#26041;&#27861;&#30340;&#29702;&#35770;&#23618;&#27425;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#20998;&#23376;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#38750;&#24179;&#34913;&#20998;&#23376;&#26500;&#22411;&#36827;&#34892;&#21435;&#22122;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#21487;&#36801;&#31227;&#30340;GNN&#21183;&#33021;&#39044;&#27979;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#37319;&#26679;&#30340;&#38750;&#24179;&#34913;&#26500;&#22411;&#30340;&#21407;&#23376;&#22352;&#26631;&#36827;&#34892;&#38543;&#26426;&#22122;&#22768;&#25200;&#21160;&#65292;&#24182;&#39044;&#35757;&#32451;GNN&#23545;&#25200;&#21160;&#30340;&#20998;&#23376;&#26500;&#22411;&#36827;&#34892;&#21435;&#22122;&#65292;&#20174;&#32780;&#24674;&#22797;&#21407;&#22987;&#22352;&#26631;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20005;&#26684;&#23454;&#39564;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#31070;&#32463;&#21183;&#33021;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#21487;&#36801;&#31227;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#19981;&#21516;&#31995;&#32479;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in equivariant graph neural networks (GNNs) have made deep learning amenable to developing fast surrogate models to expensive ab initio quantum mechanics (QM) approaches for molecular potential predictions. However, building accurate and transferable potential models using GNNs remains challenging, as the data is greatly limited by the expensive computational costs and level of theory of QM methods, especially for large and complex molecular systems. In this work, we propose denoise pretraining on nonequilibrium molecular conformations to achieve more accurate and transferable GNN potential predictions. Specifically, atomic coordinates of sampled nonequilibrium conformations are perturbed by random noises and GNNs are pretrained to denoise the perturbed molecular conformations which recovers the original coordinates. Rigorous experiments on multiple benchmarks reveal that pretraining significantly improves the accuracy of neural potentials. Furthermore, we show that the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#35299;&#20915;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FederatedTrust&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#20219;&#24615;&#38382;&#39064;&#65292;&#21253;&#25324;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#38382;&#36131;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.09844</link><description>&lt;p&gt;
FederatedTrust: &#35299;&#20915;&#21487;&#20449;&#20219;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FederatedTrust: A Solution for Trustworthy Federated Learning. (arXiv:2302.09844v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#35299;&#20915;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FederatedTrust&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#20219;&#24615;&#38382;&#39064;&#65292;&#21253;&#25324;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#38382;&#36131;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30528;&#20998;&#24067;&#24335;&#25968;&#25454;&#23396;&#31435;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25968;&#25454;&#23396;&#31435;&#20013;&#20445;&#23384;&#30528;&#25935;&#24863;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#21327;&#20316;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#21644;&#24615;&#33021;&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;&#36234;&#26469;&#36234;&#38656;&#35201;&#24314;&#31435;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#20449;&#20219;&#12290;&#29616;&#26377;&#25991;&#29486;&#25552;&#20986;&#20102;&#21508;&#31181;&#20851;&#20110;&#21487;&#20449;&#20219;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#38754;&#30340;&#26041;&#27861;&#65288;&#19981;&#21253;&#25324;&#25968;&#25454;&#38544;&#31169;&#65289;&#65292;&#23558;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#38382;&#36131;&#21046;&#35270;&#20026;&#37325;&#35201;&#25903;&#26609;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#38656;&#35201;&#30830;&#23450;&#19982;FL&#27169;&#22411;&#29305;&#21035;&#30456;&#20851;&#30340;&#21487;&#20449;&#20219;&#25903;&#26609;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#33021;&#22815;&#35745;&#31639;FL&#27169;&#22411;&#21487;&#20449;&#20219;&#27700;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#26816;&#26597;&#20102;&#35780;&#20272;&#21487;&#20449;&#20219;&#24615;&#30340;&#29616;&#26377;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
The rapid expansion of the Internet of Things (IoT) and Edge Computing has presented challenges for centralized Machine and Deep Learning (ML/DL) methods due to the presence of distributed data silos that hold sensitive information. To address concerns regarding data privacy, collaborative and privacy-preserving ML/DL techniques like Federated Learning (FL) have emerged. However, ensuring data privacy and performance alone is insufficient since there is a growing need to establish trust in model predictions. Existing literature has proposed various approaches on trustworthy ML/DL (excluding data privacy), identifying robustness, fairness, explainability, and accountability as important pillars. Nevertheless, further research is required to identify trustworthiness pillars and evaluation metrics specifically relevant to FL models, as well as to develop solutions that can compute the trustworthiness level of FL models. This work examines the existing requirements for evaluating trustwort
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07729</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#25991;&#31456;&#37117;&#20197;&#30740;&#31350;&#20142;&#28857;&#20316;&#20026;&#21069;&#35328;&#65292;&#20197;&#24635;&#32467;&#35770;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#12290;&#20142;&#28857;&#19981;&#20165;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#35770;&#25991;&#30340;&#36129;&#29486;&#65292;&#36824;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#22686;&#21152;&#20102;&#25991;&#31456;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#30740;&#31350;&#35770;&#25991;&#30340;&#29305;&#23450;&#27573;&#33853;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#26500;&#24314;&#30740;&#31350;&#20142;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#35206;&#30422;&#26426;&#21046;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#23618;&#30340;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#65292;&#23558;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20026;SciBERT&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;CSPubSum&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#36824;&#25552;&#20986;&#20102;MixSub&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#30340;&#26032;&#30340;&#36328;&#23398;&#31185;&#35770;&#25991;&#35821;&#26009;&#24211;&#12290;&#23545;&#20110;CSPubSum&#21644;MixSub&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#30456;&#20851;&#21464;&#20307;&#21644;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20854;&#20182;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;CSPubSum&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#35770;&#25991;&#30340;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays many research articles are prefaced with research highlights to summarize the main findings of the paper. Highlights not only help researchers precisely and quickly identify the contributions of a paper, they also enhance the discoverability of the article via search engines. We aim to automatically construct research highlights given certain segments of a research paper. We use a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings. We test our model on a benchmark dataset, CSPubSum, and also present MixSub, a new multi-disciplinary corpus of papers for automatic research highlight generation. For both CSPubSum and MixSub, we have observed that the proposed model achieves the best performance compared to related variants and other models proposed in the literature. On the CSPubSum dataset, our model achieves the best performance when the input is only the abstract of a paper as op
&lt;/p&gt;</description></item><item><title>&#36825;&#26412;&#35770;&#25991;&#26159;&#20316;&#32773;&#22312;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#24494;&#36719;&#30740;&#31350;&#38498;&#24037;&#20316;&#32463;&#39564;&#30340;&#25216;&#26415;&#22238;&#24518;&#24405;&#65292;&#28041;&#21450;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#24212;&#29992;&#20197;&#21450;&#21019;&#36896;&#36807;&#31243;&#20013;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2302.05449</link><description>&lt;p&gt;
Heckerthoughts.&#65288;arXiv:2302.05449v4 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Heckerthoughts. (arXiv:2302.05449v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05449
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26412;&#35770;&#25991;&#26159;&#20316;&#32773;&#22312;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#24494;&#36719;&#30740;&#31350;&#38498;&#24037;&#20316;&#32463;&#39564;&#30340;&#25216;&#26415;&#22238;&#24518;&#24405;&#65292;&#28041;&#21450;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#24212;&#29992;&#20197;&#21450;&#21019;&#36896;&#36807;&#31243;&#20013;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;&#25105;&#22312;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#24494;&#36719;&#30740;&#31350;&#38498;&#24037;&#20316;&#30340;&#25216;&#26415;&#22238;&#24518;&#24405;&#12290;&#21253;&#25324;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#30340;&#24212;&#29992;&#20197;&#21450;&#20854;&#21019;&#36896;&#32972;&#21518;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript is technical memoir about my work at Stanford and Microsoft Research. Included are fundamental concepts central to machine learning and artificial intelligence, applications of these concepts, and stories behind their creation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861; ESC&#65292;&#23427;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#36716;&#31227;&#24120;&#35782;&#30693;&#35782;&#65292;&#21487;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2301.13166</link><description>&lt;p&gt;
ESC&#65306;&#20855;&#22791;&#36719;&#20214;&#24120;&#35782;&#32422;&#26463;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation. (arXiv:2301.13166v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861; ESC&#65292;&#23427;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#36716;&#31227;&#24120;&#35782;&#30693;&#35782;&#65292;&#21487;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#23450;&#20301;&#21644;&#23548;&#33322;&#21040;&#29305;&#23450;&#29289;&#20307;&#30340;&#33021;&#21147;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25805;&#20316;&#24182;&#19982;&#29289;&#20307;&#20132;&#20114;&#20197;&#23436;&#25104;&#20219;&#21153;&#30340;&#23454;&#20307;&#20195;&#29702;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22312;&#20855;&#26377;&#26631;&#35760;&#29289;&#20307;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#36825;&#31181;&#35757;&#32451;&#25928;&#26524;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26032;&#39062;&#29289;&#20307;&#19978;&#27867;&#21270;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#23548;&#33322;&#26041;&#27861;&#8212;&#8212;&#20855;&#22791;&#36719;&#20214;&#24120;&#35782;&#32422;&#26463;&#30340;&#25506;&#32034;&#65288;ESC&#65289;&#65292;&#23427;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#36716;&#31227;&#21040;&#22312;&#35270;&#35273;&#29615;&#22659;&#19978;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#29289;&#20307;&#23548;&#33322;&#26102;&#19981;&#38656;&#35201;&#36827;&#34892;&#23548;&#33322;&#25110;&#20854;&#20182;&#35270;&#35273;&#29615;&#22659;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;ESC&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#22522;&#20110;&#25552;&#31034;&#30340;&#25509;&#22320;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24120;&#35782;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25151;&#38388;&#21644;&#29289;&#20307;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;ESC&#36890;&#36807;&#23558;&#24120;&#35782;&#30693;&#35782;&#24314;&#27169;&#20026;&#36719;&#36923;&#36753;&#35859;&#35789;&#26469;&#20351;&#20854;&#36716;&#21270;&#20026;&#23548;&#33322;&#21160;&#20316;&#65292;&#20174;&#32780;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#22312;MP3D&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;......
&lt;/p&gt;
&lt;p&gt;
The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22312;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.12313</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#30340;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Adapting Neural Link Predictors for Complex Query Answering. (arXiv:2301.12313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22312;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#27169;&#22411;&#38656;&#35201;&#22312;&#32570;&#22833;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#12290;&#26368;&#36817;&#65292;Arakelyan&#31561;&#20154;&#65288;2021&#65289;&#65307;Minervini&#31561;&#20154;&#65288;2022&#65289;&#34920;&#26126;&#65292;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#20063;&#21487;&#20197;&#29992;&#20110;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65306;&#20182;&#20204;&#30340;&#36830;&#32493;&#26597;&#35810;&#20998;&#35299;&#65288;CQD&#65289;&#26041;&#27861;&#36890;&#36807;&#23558;&#22797;&#26434;&#26597;&#35810;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#26597;&#35810;&#65292;&#20351;&#29992;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#22238;&#31572;&#24182;&#36890;&#36807;t-&#33539;&#25968;&#26469;&#32858;&#21512;&#20854;&#20998;&#25968;&#65292;&#20197;&#23545;&#27599;&#20010;&#22797;&#26434;&#26597;&#35810;&#30340;&#31572;&#26696;&#36827;&#34892;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;CQD&#19981;&#22788;&#29702;&#21542;&#23450;&#24182;&#19988;&#20165;&#20351;&#29992;&#21407;&#23376;&#35757;&#32451;&#26597;&#35810;&#30340;&#35757;&#32451;&#20449;&#21495;&#65306;&#22312;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#26399;&#38388;&#65292;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#27809;&#26377;&#36890;&#36807;&#27169;&#31946;&#36923;&#36753;t-&#33539;&#25968;&#36827;&#34892;&#26657;&#20934;&#20197;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#20998;&#25968;&#36866;&#24212;&#27169;&#22411;&#26469;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#20998;&#25968;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#36825;&#20010;&#26032;&#32452;&#20214;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#27861;&#22312;&#22797;&#26434;&#26597;&#35810;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022) showed that neural link predictors could also be used for answering complex queries: their Continuous Query Decomposition (CQD) method works by decomposing complex queries into atomic sub-queries, answers them using neural link predictors and aggregates their scores via t-norms for ranking the answers to each complex query. However, CQD does not handle negations and only uses the training signal from atomic training queries: neural link prediction scores are not calibrated to interact together via fuzzy logic t-norms during complex query answering. In this work, we propose to address this problem by training a parameter-efficient score adaptation model to re-calibrate neural link prediction scores: this new component is trained on complex queries by back-propa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2212.09811</link><description>&lt;p&gt;
&#39640;&#25928;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#21024;&#20943;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#21452;&#35821;&#32763;&#35793;&#31995;&#32479;&#30456;&#27604;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#20174;&#30693;&#35782;&#36716;&#31227;&#20013;&#33719;&#30410;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#22810;&#35821;&#35328;&#24615;&#30340;&#38480;&#21046;&#65292;&#38500;&#38750;&#36827;&#34892;&#22823;&#35268;&#27169;&#25193;&#23637;&#65292;&#21542;&#21017;&#20250;&#22686;&#21152;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#12290;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26159;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#22823;&#24133;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;NLLB-200&#26159;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#20363;&#23376;&#12290;&#23427;&#28085;&#30422;&#20102;202&#31181;&#35821;&#35328;&#65292;&#20294;&#20165;&#25512;&#29702;&#23601;&#38656;&#35201;&#33267;&#23569;&#22235;&#20010;32GB&#30340;GPU&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#65292;&#20801;&#35768;&#21024;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20294;&#32763;&#35793;&#36136;&#37327;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#65292;&#36825;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#35813;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#21098;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#35782;&#21035;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Compared to conventional bilingual translation systems, massively multilingual machine translation is appealing because a single model can translate into multiple languages and benefit from knowledge transfer for low resource languages. On the other hand, massively multilingual models suffer from the curse of multilinguality, unless scaling their size massively, which increases their training and inference costs. Sparse Mixture-of-Experts models are a way to drastically increase model capacity without the need for a proportional amount of computing. The recently released NLLB-200 is an example of such a model. It covers 202 languages but requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that allows the removal of up to 80\% of experts with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics allow to identify language-specific experts and p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#35265;&#30340;&#24322;&#26500;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#25429;&#33719;&#23545;&#35937;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#27719;&#24635;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2212.00443</link><description>&lt;p&gt;
&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#26080;&#20559;&#35265;&#30340;&#24322;&#26500;&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unbiased Heterogeneous Scene Graph Generation with Relation-aware Message Passing Neural Network. (arXiv:2212.00443v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#35265;&#30340;&#24322;&#26500;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#25429;&#33719;&#23545;&#35937;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#27719;&#24635;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#26694;&#26550;&#32858;&#28966;&#20110;&#23398;&#20064;&#22270;&#20687;&#20013;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#30001;&#20110;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#33021;&#22815;&#27169;&#25311;&#23545;&#35937;&#19982;&#37051;&#36817;&#23545;&#35937;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#23427;&#20204;&#26159;SGG&#30340;&#20027;&#23548;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;MPNN&#30340;&#26694;&#26550;&#23558;&#22330;&#26223;&#22270;&#35270;&#20026;&#21516;&#36136;&#22270;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20851;&#31995;&#24448;&#24448;&#39640;&#24230;&#20381;&#36182;&#20110;&#19982;&#20851;&#31995;&#30456;&#20851;&#32852;&#30340;&#23545;&#35937;&#36825;&#19968;&#20107;&#23454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#35265;&#30340;&#24322;&#26500;&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;HetSGG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#25429;&#33719;&#20851;&#31995;&#24863;&#30693;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#31216;&#20026;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;RMP&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#35859;&#35789;&#31867;&#22411;&#65292;&#27719;&#24635;&#20102;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;HetSGG&#26694;&#26550;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#23545;&#20110;SGG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent scene graph generation (SGG) frameworks have focused on learning complex relationships among multiple objects in an image. Thanks to the nature of the message passing neural network (MPNN) that models high-order interactions between objects and their neighboring objects, they are dominant representation learning modules for SGG. However, existing MPNN-based frameworks assume the scene graph as a homogeneous graph, which restricts the context-awareness of visual relations between objects. That is, they overlook the fact that the relations tend to be highly dependent on the objects with which the relations are associated. In this paper, we propose an unbiased heterogeneous scene graph generation (HetSGG) framework that captures relation-aware context using message passing neural networks. We devise a novel message passing layer, called relation-aware message passing neural network (RMP), that aggregates the contextual information of an image considering the predicate type between 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#27969;&#24335;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#23558;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#36716;&#21270;&#20026;&#24369;&#30417;&#30563;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#30495;&#27491;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#25193;&#23637;&#21040;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#26102;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.02499</link><description>&lt;p&gt;
&#20855;&#26377;&#30495;&#27491;&#38646;-shot&#33021;&#21147;&#30340;&#24369;&#30417;&#30563;&#27969;&#24335;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability. (arXiv:2211.02499v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#27969;&#24335;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#23558;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#36716;&#21270;&#20026;&#24369;&#30417;&#30563;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#30495;&#27491;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#25193;&#23637;&#21040;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#26102;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#26500;&#24314;&#30340;&#27969;&#24335;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65288;SM2&#65289;&#30340;&#24037;&#20316;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#22810;&#31181;&#21475;&#35821;&#35821;&#35328;&#36716;&#24405;&#25110;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#30340;&#25991;&#26412;&#12290;SM2&#30340;&#26680;&#24515;&#26159;Transformer Transducer&#65292;&#20855;&#26377;&#39640;&#24230;&#27969;&#24335;&#22788;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#23558;&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#20013;&#30340;&#36716;&#24405;&#36716;&#21270;&#32780;&#25104;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#26469;&#35757;&#32451;SM2&#27169;&#22411;&#65292;&#32780;&#38750;&#20154;&#24037;&#26631;&#35760;&#30340;&#35821;&#38899;&#32763;&#35793;&#25968;&#25454;&#12290;&#21033;&#29992;&#26469;&#33258;25&#31181;&#35821;&#35328;&#30340;35.1&#19975;&#23567;&#26102;&#30340;&#21311;&#21517;&#35821;&#38899;&#35757;&#32451;&#25968;&#25454;&#65292;SM2&#27169;&#22411;&#30340;&#35821;&#38899;&#32763;&#35793;&#36136;&#37327;&#19982;&#26576;&#20123;&#26368;&#26032;&#30340;&#22823;&#35268;&#27169;&#38750;&#27969;&#24335;&#35821;&#38899;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25193;&#23637;&#21040;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#26102;&#65292;SM2&#20855;&#26377;&#30495;&#27491;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;{&#28304;&#35821;&#38899;&#65292;&#30446;&#26631;&#25991;&#26412;}&#23545;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce our work of building a Streaming Multilingual Speech Model (SM2), which can transcribe or translate multiple spoken languages into texts of the target language. The backbone of SM2 is Transformer Transducer, which has high streaming capability. Instead of human labeled speech translation (ST) data, SM2 models are trained using weakly supervised data generated by converting the transcriptions in speech recognition corpora with a machine translation service. With 351 thousand hours of anonymized speech training data from 25 languages, SM2 models achieve comparable or even better ST quality than some recent popular large-scale non-streaming speech models. More importantly, we show that SM2 has the truly zero-shot capability when expanding to new target languages, yielding high quality ST results for {source-speech, target-text} pairs that are not seen during training.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DiffusionDB&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#21253;&#21547;1400&#19975;&#24352;&#22270;&#20687;&#21644;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26102;&#25152;&#38656;&#30340;&#36866;&#24403;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#25552;&#31034;&#26679;&#24335;&#21644;&#36229;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#65292;&#29978;&#33267;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2210.14896</link><description>&lt;p&gt;
DiffusionDB: &#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#30011;&#24266;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models. (arXiv:2210.14896v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14896
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DiffusionDB&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#21253;&#21547;1400&#19975;&#24352;&#22270;&#20687;&#21644;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26102;&#25152;&#38656;&#30340;&#36866;&#24403;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#25552;&#31034;&#26679;&#24335;&#21644;&#36229;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#65292;&#29978;&#33267;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#32454;&#33410;&#30340;&#22270;&#20687;&#38656;&#35201;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#32780;&#19988;&#24448;&#24448;&#19981;&#28165;&#26970;&#27169;&#22411;&#23545;&#19981;&#21516;&#25552;&#31034;&#30340;&#21453;&#24212;&#25110;&#26368;&#20339;&#25552;&#31034;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffusionDB&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;6.5TB&#65292;&#21253;&#21547;&#20351;&#29992;Stable Diffusion&#29983;&#25104;&#30340;1400&#19975;&#24352;&#22270;&#20687;&#65292;180&#19975;&#20010;&#21807;&#19968;&#25552;&#31034;&#21644;&#30001;&#30495;&#23454;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#31034;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#25351;&#20986;&#20102;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#30340;&#29305;&#23450;&#36229;&#21442;&#25968;&#20540;&#21644;&#25552;&#31034;&#26679;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#28508;&#22312;&#26377;&#23475;&#27169;&#22411;&#20351;&#29992;&#30340;&#35777;&#25454;&#65292;&#22914;&#29983;&#25104;&#35823;&#23548;&#20449;&#24687;&#12290;&#36825;&#20010;&#20154;&#20026;&#39537;&#21160;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#21069;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20026;&#20102;&#35299;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#28608;&#21160;&#20154;&#24515;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21355;&#26143;&#22270;&#20687;&#26469;&#39044;&#27979;&#35199;&#23612;&#32599;&#27827;&#30149;&#27602;&#30340;&#24490;&#29615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26469;&#32858;&#21512;&#37051;&#23621;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2209.05251</link><description>&lt;p&gt;
&#20174;&#21355;&#26143;&#20013;&#21457;&#29616;&#30149;&#27602;&#65306;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#35199;&#23612;&#32599;&#27827;&#30149;&#27602;&#30340;&#24490;&#29615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks. (arXiv:2209.05251v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21355;&#26143;&#22270;&#20687;&#26469;&#39044;&#27979;&#35199;&#23612;&#32599;&#27827;&#30149;&#27602;&#30340;&#24490;&#29615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26469;&#32858;&#21512;&#37051;&#23621;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35199;&#23612;&#32599;&#27827;&#30149;&#27602;(WNV)&#30340;&#21457;&#29983;&#20195;&#34920;&#20102;&#26368;&#24120;&#35265;&#30340;&#34442;&#23186;&#24615;&#21160;&#29289;&#20256;&#25773;&#30149;&#27602;&#24863;&#26579;&#20043;&#19968;&#12290;&#23427;&#30340;&#24490;&#29615;&#36890;&#24120;&#19982;&#36866;&#21512;&#20110;&#23186;&#20171;&#34442;&#23376;&#32321;&#27542;&#21644;&#30149;&#27602;&#22797;&#21046;&#30340;&#27668;&#20505;&#21644;&#29615;&#22659;&#26465;&#20214;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24050;&#24320;&#21457;&#20102;&#20960;&#31181;&#32479;&#35745;&#27169;&#22411;&#26469;&#22609;&#36896;&#21644;&#39044;&#27979;WNV&#30340;&#24490;&#29615;&#65306;&#29305;&#21035;&#26159;&#65292;&#22320;&#29699;&#35266;&#27979;(EO)&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21487;&#29992;&#24615;&#65292;&#21152;&#19978;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#23558;&#21355;&#26143;&#22270;&#20687;&#36755;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26469;&#39044;&#27979;WNV&#30340;&#24490;&#29615;&#65292;&#36825;&#20123;&#21355;&#26143;&#22270;&#20687;&#24050;&#32463;&#24191;&#27867;&#26174;&#31034;&#20855;&#26377;&#29615;&#22659;&#21644;&#27668;&#20505;&#29305;&#24449;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34429;&#28982;&#20043;&#21069;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#20998;&#26512;&#27599;&#20010;&#22320;&#29702;&#28857;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#38468;&#36817;&#28857;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20381;&#38752;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26469;&#32858;&#21512;&#37051;&#23621;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The occurrence of West Nile Virus (WNV) represents one of the most common mosquito-borne zoonosis viral infections. Its circulation is usually associated with climatic and environmental conditions suitable for vector proliferation and virus replication. On top of that, several statistical models have been developed to shape and forecast WNV circulation: in particular, the recent massive availability of Earth Observation (EO) data, coupled with the continuous advances in the field of Artificial Intelligence, offer valuable opportunities.  In this paper, we seek to predict WNV circulation by feeding Deep Neural Networks (DNNs) with satellite images, which have been extensively shown to hold environmental and climatic features. Notably, while previous approaches analyze each geographical site independently, we propose a spatial-aware approach that considers also the characteristics of close sites. Specifically, we build upon Graph Neural Networks (GNN) to aggregate features from neighbour
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#38024;&#23545;&#36825;&#19968;&#22797;&#26434;&#39046;&#22495;&#24314;&#31435;&#20102;&#35270;&#35273;&#39046;&#22495;&#20013;&#36830;&#32493;&#25511;&#21046;&#30340;&#31616;&#21333;&#22522;&#20934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#31163;&#32447;RL&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#20462;&#25913;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2206.04779</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#38024;&#23545;&#36825;&#19968;&#22797;&#26434;&#39046;&#22495;&#24314;&#31435;&#20102;&#35270;&#35273;&#39046;&#22495;&#20013;&#36830;&#32493;&#25511;&#21046;&#30340;&#31616;&#21333;&#22522;&#20934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#31163;&#32447;RL&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#20462;&#25913;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20351;&#24471;Agent&#21487;&#20197;&#36991;&#20813;&#36890;&#24120;&#36153;&#26102;&#26114;&#36149;&#30340;&#22312;&#32447;&#25968;&#25454;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#65292;&#22522;&#20110;&#35270;&#35273;&#35266;&#23519;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#22312;&#36825;&#20010;&#22797;&#26434;&#30340;&#39046;&#22495;&#20013;&#23545;&#20851;&#38190;&#25361;&#25112;&#30340;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24314;&#31435;&#31616;&#21333;&#30340;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#26088;&#22312;&#26356;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#31163;&#32447;RL&#38382;&#39064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#21463;&#31163;&#32447;RL&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#30340;&#19968;&#32452;&#26399;&#26395;&#25152;&#25351;&#23548;&#65292;&#21253;&#25324;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#31283;&#20581;&#24615;&#21644;&#21160;&#21147;&#23398;&#20013;&#21487;&#35270;&#21270;&#21464;&#21270;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#22871;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DreamerV2&#21644;DrQ-v2&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning has shown great promise in leveraging large pre-collected datasets for policy learning, allowing agents to forgo often-expensive online data collection. However, offline reinforcement learning from visual observations with continuous action spaces remains under-explored, with a limited understanding of the key challenges in this complex domain. In this paper, we establish simple baselines for continuous control in the visual domain and introduce a suite of benchmarking tasks for offline reinforcement learning from visual observations designed to better represent the data distributions present in real-world offline RL problems and guided by a set of desiderata for offline RL from visual observations, including robustness to visual distractions and visually identifiable changes in dynamics. Using this suite of benchmarking tasks, we show that simple modifications to two popular vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2, suf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.03447</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21451;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#29992;&#20110;&#31561;&#20215;&#21644;&#21253;&#21547;&#20851;&#31995;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v7 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#35821;&#20041;&#32593;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#20854;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#21305;&#37197;&#35780;&#20272;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#21253;&#21547;&#20851;&#31995;&#26144;&#23556;&#30340;&#26377;&#38480;&#35780;&#20272;&#12289;&#21442;&#32771;&#26144;&#23556;&#30340;&#20122;&#20248;&#35299;&#20197;&#21450;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#26412;&#20307;&#21305;&#37197;&#20219;&#21153;&#65292;&#28041;&#21450;&#20174;Mondo&#21644;UMLS&#20013;&#25552;&#21462;&#30340;&#26412;&#20307;&#12290;&#27599;&#20010;&#20219;&#21153;&#21253;&#25324;&#31561;&#20215;&#21644;&#21253;&#21547;&#20851;&#31995;&#21305;&#37197;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#12289;&#26412;&#20307;&#20462;&#21098;&#31561;&#26041;&#24335;&#30830;&#20445;&#21442;&#32771;&#26144;&#23556;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#38750;&#26426;&#22120;&#23398;&#20064;&#30340;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontology Matching (OM) plays an important role in many domains such as bioinformatics and the Semantic Web, and its research is becoming increasingly popular, especially with the application of machine learning (ML) techniques. Although the Ontology Alignment Evaluation Initiative (OAEI) represents an impressive effort for the systematic evaluation of OM systems, it still suffers from several limitations including limited evaluation of subsumption mappings, suboptimal reference mappings, and limited support for the evaluation of ML-based systems. To tackle these limitations, we introduce five new biomedical OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes both equivalence and subsumption matching; the quality of reference mappings is ensured by human curation, ontology pruning, etc.; and a comprehensive evaluation framework is proposed to measure OM performance from various perspectives for both ML-based and non-ML-based OM systems. We report evaluation r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#20250;&#22312;&#26410;&#24471;&#21040;&#36275;&#22815;&#26381;&#21153;&#30340;&#20154;&#32676;&#20013;&#20135;&#29983;&#39640;&#34394;&#25253;&#29575;&#65292;&#21487;&#33021;&#25918;&#22823;&#20102;&#31995;&#32479;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#30340;&#23454;&#39564;&#35774;&#32622;&#19981;&#36275;&#20197;&#20840;&#38754;&#30740;&#31350;&#31639;&#27861;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#65292;&#32780;&#19988;&#20351;&#29992;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20559;&#20506;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#32467;&#26524;&#30340;&#35299;&#37322;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2201.07856</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20559;&#20506;&#30340;&#28508;&#22312;&#26469;&#28304;&#20351;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26410;&#35786;&#26029;&#38382;&#39064;&#30340;&#30740;&#31350;&#22797;&#26434;&#21270;
&lt;/p&gt;
&lt;p&gt;
Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms. (arXiv:2201.07856v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#20250;&#22312;&#26410;&#24471;&#21040;&#36275;&#22815;&#26381;&#21153;&#30340;&#20154;&#32676;&#20013;&#20135;&#29983;&#39640;&#34394;&#25253;&#29575;&#65292;&#21487;&#33021;&#25918;&#22823;&#20102;&#31995;&#32479;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#30340;&#23454;&#39564;&#35774;&#32622;&#19981;&#36275;&#20197;&#20840;&#38754;&#30740;&#31350;&#31639;&#27861;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#65292;&#32780;&#19988;&#20351;&#29992;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20559;&#20506;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#32467;&#26524;&#30340;&#35299;&#37322;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#25253;&#21578;&#24341;&#36215;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#25918;&#22823;&#30001;&#35757;&#32451;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#20559;&#35265;&#23548;&#33268;&#30340;&#20581;&#24247;&#24046;&#24322;&#30340;&#25285;&#24551;&#12290; Seyyed-Kalantari&#31561;&#20154;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#8220;&#26410;&#21457;&#29616;&#8221;&#26631;&#31614;&#65288;&#34920;&#31034;&#27809;&#26377;&#30142;&#30149;&#65289;&#30340;&#20122;&#32452;&#20043;&#38388;&#20135;&#29983;&#20102;&#34394;&#25253;&#29575;&#65288;FPR&#65289;&#30340;&#24046;&#24322;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#24050;&#30693;&#21382;&#21490;&#19978;&#26410;&#24471;&#21040;&#36275;&#22815;&#26381;&#21153;&#30340;&#20122;&#32452;&#20013;&#19968;&#30452;&#20135;&#29983;&#26356;&#39640;&#30340;FPR&#65292;&#24182;&#19988;&#35813;&#30740;&#31350;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#20123;&#27169;&#22411;&#26174;&#31034;&#24182;&#19988;&#21487;&#33021;&#20250;&#25918;&#22823;&#31995;&#32479;&#30340;&#26410;&#35786;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#35813;&#30740;&#31350;&#20013;&#30340;&#23454;&#39564;&#35774;&#32622;&#19981;&#36275;&#20197;&#30740;&#31350;&#31639;&#27861;&#26410;&#35786;&#26029;&#38382;&#39064;&#12290;&#22312;&#32570;&#20047;&#20851;&#20110;&#25968;&#25454;&#38598;&#20559;&#20506;&#31243;&#24230;&#21644;&#24615;&#36136;&#30340;&#20855;&#20307;&#30693;&#35782;&#65288;&#25110;&#20551;&#35774;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#35843;&#26597;&#27169;&#22411;&#20559;&#20506;&#38382;&#39064;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#20182;&#20204;&#20351;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#23637;&#31034;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#30340;&#20559;&#20506;&#65288;&#30001;&#20110;&#38543;&#26426;&#20998;&#21106;&#65289;&#65292;&#20005;&#37325;&#22797;&#26434;&#21270;&#20102;&#25152;&#25253;&#36947;&#30340;&#24046;&#24322;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing number of reports raise concerns about the risk that machine learning algorithms could amplify health disparities due to biases embedded in the training data. Seyyed-Kalantari et al. find that models trained on three chest X-ray datasets yield disparities in false-positive rates (FPR) across subgroups on the 'no-finding' label (indicating the absence of disease). The models consistently yield higher FPR on subgroups known to be historically underserved, and the study concludes that the models exhibit and potentially even amplify systematic underdiagnosis. We argue that the experimental setup in the study is insufficient to study algorithmic underdiagnosis. In the absence of specific knowledge (or assumptions) about the extent and nature of the dataset bias, it is difficult to investigate model bias. Importantly, their use of test data exhibiting the same bias as the training data (due to random splitting) severely complicates the interpretation of the reported disparities
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#20851;&#20110;&#23454;&#29616;&#26368;&#23567;&#21464;&#21270;&#33539;&#24335;&#30340;&#20449;&#24565;&#20462;&#27491;&#36816;&#31639;&#31526;&#30340;&#36890;&#29992;&#27169;&#22411;&#35770;&#21051;&#30011;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#20855;&#26377;&#32463;&#20856;&#27169;&#22411;&#35770;&#35821;&#20041;&#30340;&#36923;&#36753;&#65292;&#21253;&#25324;&#20197;&#21069;&#32570;&#20047;&#21051;&#30011;&#30340;&#24418;&#24335;&#20027;&#20041;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#25512;&#24191;Katsuno&#21644;Mendelzon&#30340;&#26041;&#27861;&#65292;&#20026;&#20219;&#24847;&#22612;&#26031;&#22522;&#36923;&#36753;&#30340;AGM&#39118;&#26684;&#20462;&#27491;&#25552;&#20379;&#20102;&#21051;&#30011;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.13557</link><description>&lt;p&gt;
AGM&#20449;&#24565;&#20462;&#27491;&#30340;&#35821;&#20041;&#23398;&#36890;&#29992;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
AGM Belief Revision, Semantically. (arXiv:2112.13557v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#20851;&#20110;&#23454;&#29616;&#26368;&#23567;&#21464;&#21270;&#33539;&#24335;&#30340;&#20449;&#24565;&#20462;&#27491;&#36816;&#31639;&#31526;&#30340;&#36890;&#29992;&#27169;&#22411;&#35770;&#21051;&#30011;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#20855;&#26377;&#32463;&#20856;&#27169;&#22411;&#35770;&#35821;&#20041;&#30340;&#36923;&#36753;&#65292;&#21253;&#25324;&#20197;&#21069;&#32570;&#20047;&#21051;&#30011;&#30340;&#24418;&#24335;&#20027;&#20041;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#25512;&#24191;Katsuno&#21644;Mendelzon&#30340;&#26041;&#27861;&#65292;&#20026;&#20219;&#24847;&#22612;&#26031;&#22522;&#36923;&#36753;&#30340;AGM&#39118;&#26684;&#20462;&#27491;&#25552;&#20379;&#20102;&#21051;&#30011;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#23454;&#29616;&#26368;&#23567;&#21464;&#21270;&#33539;&#24335;&#30340;&#20449;&#24565;&#20462;&#27491;&#36816;&#31639;&#31526;&#30340;&#36890;&#29992;&#27169;&#22411;&#35770;&#21051;&#30011;&#65292;&#21442;&#32771;&#20102;Alchourr\'{o}n, G\"{a}rdenfors, &#21644; Makinson (AGM)&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#21051;&#30011;&#36866;&#29992;&#20110;&#25152;&#26377;&#22612;&#26031;&#22522;&#36923;&#36753;&#65292;&#21363;&#25152;&#26377;&#20855;&#26377;&#32463;&#20856;&#27169;&#22411;&#35770;&#35821;&#20041;&#30340;&#36923;&#36753;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#30693;&#35782;&#34920;&#31034;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#24418;&#24335;&#20027;&#20041;&#65292;&#21253;&#25324;&#20197;&#21069;&#32570;&#20047;&#27169;&#22411;&#35770;&#21051;&#30011;&#30340;&#35768;&#22810;&#24418;&#24335;&#20027;&#20041;&#12290;&#25105;&#20204;&#30340;&#20986;&#21457;&#28857;&#26159;Katsuno&#21644;Mendelzon(K&amp;M)&#30340;&#26041;&#27861;&#65292;&#20182;&#20204;&#20026;&#26377;&#38480;&#31526;&#21495;&#19978;&#30340;&#21629;&#39064;&#36923;&#36753;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#21051;&#30011;&#12290;&#25105;&#20204;&#23558;K&amp;M&#30340;&#26041;&#27861;&#25512;&#24191;&#21040;&#20219;&#24847;&#22612;&#26031;&#22522;&#36923;&#36753;&#30340;AGM&#39118;&#26684;&#20462;&#27491;&#30340;&#22522;&#30784;&#19978;&#65292;&#20854;&#20013;&#22522;&#30784;&#21487;&#20197;&#25351;&#20195;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#21508;&#31181;&#34920;&#31034;&#26041;&#24335;&#65288;&#22914;&#20449;&#24565;&#38598;&#21512;&#12289;&#20219;&#24847;&#25110;&#26377;&#38480;&#21477;&#23376;&#38598;&#21512;&#25110;&#21333;&#20010;&#21477;&#23376;&#65289;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#26680;&#24515;&#32467;&#26524;&#26159;&#19968;&#20010;&#34920;&#31034;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21452;&#21521;&#30340;co&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
We establish a generic, model-theoretic characterization of belief revision operators implementing the paradigm of minimal change according to the seminal work by Alchourr\'{o}n, G\"{a}rdenfors, and Makinson (AGM). Our characterization applies to all Tarskian logics, that is, all logics with a classical model-theoretic semantics, and hence a wide variety of formalisms used in knowledge representation and beyond, including many for which a model-theoretic characterization has hitherto been lacking. Our starting point is the approach by Katsuno and Mendelzon (K&amp;M), who provided such a characterization for propositional logic over finite signatures. We generalize K&amp;M's approach to the setting of AGM-style revision over bases in arbitrary Tarskian logics, where base may refer to one of the various ways of representing an agent's beliefs (such as belief sets, arbitrary or finite sets of sentences, or single sentences). Our first core result is a representation theorem providing a two-way co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#24179;&#34913;&#37326;&#22806;&#38754;&#23380;&#25968;&#25454;&#38598;&#65288;BFW&#65289;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#25105;&#20204;&#21457;&#29616;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#23384;&#22312;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#24179;&#22343;&#24615;&#33021;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2103.09118</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#24179;&#34913;&#38754;&#23380;&#20013;&#24179;&#34913;&#20559;&#35265;&#24182;&#20445;&#25252;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Balancing Biases and Preserving Privacy on Balanced Faces in the Wild. (arXiv:2103.09118v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.09118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#24179;&#34913;&#37326;&#22806;&#38754;&#23380;&#25968;&#25454;&#38598;&#65288;BFW&#65289;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#25105;&#20204;&#21457;&#29616;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#23384;&#22312;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#24179;&#22343;&#24615;&#33021;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#23384;&#22312;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#34913;&#37327;&#19981;&#21516;&#26063;&#32676;&#21644;&#24615;&#21035;&#23376;&#32676;&#20043;&#38388;&#30340;&#36825;&#20123;&#20559;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#24179;&#34913;&#37326;&#22806;&#38754;&#23380;&#25968;&#25454;&#38598;&#65288;BFW&#65289;&#12290;&#35813;&#25968;&#25454;&#38598;&#20801;&#35768;&#23545;&#27599;&#20010;&#23376;&#32676;&#30340;&#20154;&#33080;&#35782;&#21035;&#24615;&#33021;&#36827;&#34892;&#25551;&#36848;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20381;&#36182;&#21333;&#19968;&#20998;&#25968;&#38408;&#20540;&#26469;&#21306;&#20998;&#30495;&#23454;&#21644;&#20882;&#21517;&#39030;&#26367;&#32773;&#26679;&#26412;&#23545;&#20250;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23376;&#32676;&#20869;&#30340;&#24615;&#33021;&#24448;&#24448;&#19982;&#20840;&#23616;&#24179;&#22343;&#20540;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#29305;&#23450;&#30340;&#38169;&#35823;&#29575;&#20165;&#36866;&#29992;&#20110;&#19982;&#39564;&#35777;&#25968;&#25454;&#21305;&#37197;&#30340;&#20154;&#32676;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#24179;&#34913;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21033;&#29992;&#20174;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#38754;&#37096;&#29305;&#24449;&#12290;&#35813;&#26041;&#26696;&#25552;&#39640;&#20102;&#24179;&#22343;&#24615;&#33021;&#65292;&#24182;&#22312;&#21435;&#38500;&#20154;&#21475;&#32479;&#35745;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#30041;&#20102;&#36523;&#20221;&#20449;&#24687;&#12290;&#21435;&#38500;&#20154;&#21475;&#32479;&#35745;&#30693;&#35782;&#21487;&#20197;&#38450;&#27490;&#28508;&#22312;&#30340;&#20559;&#35265;&#24433;&#21709;&#20915;&#31574;&#65292;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are demographic biases present in current facial recognition (FR) models. To measure these biases across different ethnic and gender subgroups, we introduce our Balanced Faces in the Wild (BFW) dataset. This dataset allows for the characterization of FR performance per subgroup. We found that relying on a single score threshold to differentiate between genuine and imposters sample pairs leads to suboptimal results. Additionally, performance within subgroups often varies significantly from the global average. Therefore, specific error rates only hold for populations that match the validation data. To mitigate imbalanced performances, we propose a novel domain adaptation learning scheme that uses facial features extracted from state-of-the-art neural networks. This scheme boosts the average performance and preserves identity information while removing demographic knowledge. Removing demographic knowledge prevents potential biases from affecting decision-making and protects privacy 
&lt;/p&gt;</description></item><item><title>&#21487;&#24494;&#20998;&#36923;&#36753;&#26426;&#22120; (DLM) &#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#36923;&#36753;&#26550;&#26500;&#65292;&#21487;&#20197;&#35299;&#20915;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243; (ILP) &#21644;&#24378;&#21270;&#23398;&#20064; (RL) &#38382;&#39064;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23450;&#20041;&#20102;&#19968;&#31181;&#21463;&#38480;&#20294;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#19968;&#38454;&#36923;&#36753;&#31243;&#24207;&#30340;&#36830;&#32493;&#26494;&#24347;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#35770;&#23478;&#26550;&#26500;&#29992;&#20110;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2102.11529</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#36923;&#36753;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Differentiable Logic Machines. (arXiv:2102.11529v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.11529
&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#36923;&#36753;&#26426;&#22120; (DLM) &#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#36923;&#36753;&#26550;&#26500;&#65292;&#21487;&#20197;&#35299;&#20915;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243; (ILP) &#21644;&#24378;&#21270;&#23398;&#20064; (RL) &#38382;&#39064;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23450;&#20041;&#20102;&#19968;&#31181;&#21463;&#38480;&#20294;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#19968;&#38454;&#36923;&#36753;&#31243;&#24207;&#30340;&#36830;&#32493;&#26494;&#24347;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#35770;&#23478;&#26550;&#26500;&#29992;&#20110;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#12289;&#23398;&#20064;&#21644;&#20915;&#31574;&#30340;&#38598;&#25104;&#26159;&#26500;&#24314;&#26356;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#36923;&#36753;&#26550;&#26500;&#65292;&#31216;&#20026;&#21487;&#24494;&#20998;&#36923;&#36753;&#26426;&#22120;&#65288;DLM&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#19968;&#38454;&#36923;&#36753;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#21253;&#25324;&#20960;&#20010;&#21019;&#26032;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#36890;&#36807;&#20026;&#35859;&#35789;&#20998;&#37197;&#26435;&#37325;&#32780;&#19981;&#26159;&#35268;&#21017;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#21463;&#38480;&#20294;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#19968;&#38454;&#36923;&#36753;&#31243;&#24207;&#30340;&#36830;&#32493;&#26494;&#24347;&#65292;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#31070;&#32463;&#36923;&#36753;&#26041;&#27861;&#19981;&#21516;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#36825;&#31181;&#21487;&#24494;&#20998;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#65288;&#30417;&#30563;&#21644;RL&#65289;&#35757;&#32451;&#36807;&#31243;&#65292;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#24674;&#22797;&#19968;&#20010;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#21363;&#36923;&#36753;&#20844;&#24335;&#65289;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#21152;&#36895;RL&#35757;&#32451;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#28436;&#21592;&#35780;&#35770;&#23478;
&lt;/p&gt;
&lt;p&gt;
The integration of reasoning, learning, and decision-making is key to build more general artificial intelligence systems. As a step in this direction, we propose a novel neural-logic architecture, called differentiable logic machine (DLM), that can solve both inductive logic programming (ILP) and reinforcement learning (RL) problems, where the solution can be interpreted as a first-order logic program. Our proposition includes several innovations. Firstly, our architecture defines a restricted but expressive continuous relaxation of the space of first-order logic programs by assigning weights to predicates instead of rules, in contrast to most previous neural-logic approaches. Secondly, with this differentiable architecture, we propose several (supervised and RL) training procedures, based on gradient descent, which can recover a fully-interpretable solution (i.e., logic formula). Thirdly, to accelerate RL training, we also design a novel critic architecture that enables actor-critic a
&lt;/p&gt;</description></item><item><title>SAT&#27714;&#35299;&#20013;&#65292;&#31639;&#27861;&#36827;&#27493;&#23545;&#30828;&#20214;&#36827;&#27493;&#30340;&#24433;&#21709;&#33267;&#23569;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2008.02215</link><description>&lt;p&gt;
&#12298;SAT&#27714;&#35299;&#20013;&#30340;&#26102;&#38388;&#36291;&#36801;&#25361;&#25112;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Time Leap Challenge for SAT Solving. (arXiv:2008.02215v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.02215
&lt;/p&gt;
&lt;p&gt;
SAT&#27714;&#35299;&#20013;&#65292;&#31639;&#27861;&#36827;&#27493;&#23545;&#30828;&#20214;&#36827;&#27493;&#30340;&#24433;&#21709;&#33267;&#23569;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27604;&#36739;&#20102;&#36807;&#21435;&#20004;&#20010;&#21313;&#24180;&#20013;&#30828;&#20214;&#36827;&#27493;&#21644;&#31639;&#27861;&#36827;&#27493;&#23545;SAT&#27714;&#35299;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#26032;&#30340;&#35745;&#31639;&#26426;&#30828;&#20214;&#36827;&#34892;&#30340;20&#24180;&#21069;&#30340;SAT&#27714;&#35299;&#22120;&#19982;&#20351;&#29992;20&#24180;&#21069;&#30340;&#26087;&#30828;&#20214;&#36827;&#34892;&#30340;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#31639;&#27861;&#26041;&#38754;&#30340;&#36827;&#23637;&#23545;&#30828;&#20214;&#26041;&#38754;&#30340;&#36827;&#27493;&#33267;&#23569;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compare the impact of hardware advancement and algorithm advancement for SAT solving over the last two decades. In particular, we compare 20-year-old SAT-solvers on new computer hardware with modern SAT-solvers on 20-year-old hardware. Our findings show that the progress on the algorithmic side has at least as much impact as the progress on the hardware side.
&lt;/p&gt;</description></item></channel></rss>