<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#29983;&#25104;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#25688;&#35201;&#20559;&#35265;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.02321</link><description>&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#29305;&#24449;&#20998;&#26512;&#65306;&#20197;&#29305;&#26391;&#26222;&#21644;&#25308;&#30331;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterizing Political Bias in Automatic Summaries: A Case Study of Trump and Biden. (arXiv:2305.02321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#29983;&#25104;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#25688;&#35201;&#20559;&#35265;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;NLP&#31995;&#32479;&#21487;&#33021;&#23545;&#31038;&#20250;&#20559;&#35265;&#36827;&#34892;&#32534;&#30721;&#65307;&#28982;&#32780;&#65292;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#30340;&#25919;&#27835;&#20559;&#35265;&#20173;&#30456;&#23545;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23454;&#20307;&#26367;&#25442;&#26041;&#27861;&#30740;&#31350;&#20102;&#26032;&#38395;&#25991;&#31456;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#23478;&#25551;&#32472;&#12290;&#25105;&#20204;&#22522;&#20110;&#25919;&#27835;&#23454;&#20307;&#21644;&#35789;&#27719;&#36164;&#28304;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35780;&#20272;&#25277;&#21462;&#24335;&#21644;&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#20013;&#26377;&#20851;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#21644;&#20052;&#183;&#25308;&#30331;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#19968;&#33268;&#30340;&#24046;&#24322;&#65292;&#20363;&#22914;&#22312;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#24403;&#23454;&#20307;&#22312;&#28304;&#25991;&#31456;&#20013;&#37325;&#28857;&#20986;&#29616;&#26102;&#65292;&#36825;&#20123;&#25688;&#35201;&#24046;&#24322;&#26368;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21270;&#29305;&#24449;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#26410;&#26469;&#30740;&#31350;&#25688;&#35201;&#20559;&#35265;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing literature has shown that powerful NLP systems may encode social biases; however, the political bias of summarization models remains relatively unknown. In this work, we use an entity replacement method to investigate the portrayal of politicians in automatically generated summaries of news articles. We develop a computational framework based on political entities and lexical resources, and use it to assess biases about Donald Trump and Joe Biden in both extractive and abstractive summarization models. We find consistent differences, such as stronger associations of a collective US government (i.e., administration) with Biden than with Trump. These summary dissimilarities are most prominent when the entity is heavily featured in the source article. Our systematic characterization provides a framework for future studies of bias in summarization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#23454;&#26102;&#36752;&#23556;&#22330;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#21333;&#24352;&#26410;&#32463;&#36807;&#23039;&#21183;&#35843;&#25972;&#30340;&#22270;&#20687;&#20013;&#25512;&#26029;&#21644;&#28210;&#26579;&#20986;&#36924;&#30495;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#24863;&#30693;&#20154;&#20687;&#21512;&#25104;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02310</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#21333;&#24352;&#22270;&#20687;&#20154;&#20687;&#30340;&#23454;&#26102;&#36752;&#23556;&#22330;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Real-Time Radiance Fields for Single-Image Portrait View Synthesis. (arXiv:2305.02310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21333;&#24352;&#22270;&#29255;&#30340;&#23454;&#26102;&#36752;&#23556;&#22330;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#21333;&#24352;&#26410;&#32463;&#36807;&#23039;&#21183;&#35843;&#25972;&#30340;&#22270;&#20687;&#20013;&#25512;&#26029;&#21644;&#28210;&#26579;&#20986;&#36924;&#30495;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#24863;&#30693;&#20154;&#20687;&#21512;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#25293;&#25668;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#24352;&#26410;&#32463;&#36807;&#23039;&#21183;&#35843;&#25972;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#38754;&#37096;&#32918;&#20687;&#65289;&#20013;&#25512;&#26029;&#21644;&#28210;&#26579;&#20986;&#36924;&#30495;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#23454;&#26102;&#21512;&#25104;&#12290; &#32473;&#23450;&#21333;&#20010;RGB&#36755;&#20837;&#65292;&#25105;&#20204;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#30452;&#25509;&#39044;&#27979;&#30001;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#35268;&#33539;&#19977;&#38754;&#22270;&#34920;&#31034;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#36827;&#34892;&#19977;&#32500;&#24863;&#30693;&#30340;&#26032;&#35270;&#22270;&#21512;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#24555;&#36895;&#65288;24fps&#65289;&#65292;&#19988;&#20135;&#29983;&#30340;&#36136;&#37327;&#39640;&#20110;&#38656;&#35201;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#30340;&#24378;GAN&#21453;&#28436;&#22522;&#32447;&#12290;&#20026;&#20102;&#35757;&#32451;&#19977;&#38754;&#22270;&#32534;&#30721;&#22120;&#31649;&#36947;&#65292;&#25105;&#20204;&#21482;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#39044;&#35757;&#32451;&#30340;3D GAN&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#33976;&#39311;&#25104;&#21069;&#39304;&#32534;&#30721;&#22120;&#12290;&#25216;&#26415;&#36129;&#29486;&#21253;&#25324;&#22522;&#20110;Vision Transformer&#30340;&#19977;&#38754;&#22270;&#32534;&#30721;&#22120;&#12289;&#30456;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20197;&#21450;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#33391;&#22909;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#26174;&#30528;&#30340;&#40065;&#26834;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;3D&#24863;&#30693;&#20154;&#20687;&#21512;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our res
&lt;/p&gt;</description></item><item><title>Fashionpedia-Taste&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#23616;&#37096;&#23646;&#24615;&#65292;&#20154;&#31867;&#27880;&#24847;&#21147;&#21644;&#35828;&#26126;&#31561;&#22810;&#31181;&#22240;&#32032;&#65292;&#33021;&#22815;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20174;&#19981;&#21516;&#30340;&#20154;&#25991;&#35270;&#35282;&#21644;&#27169;&#24577;&#20840;&#38754;&#29702;&#35299;&#21644;&#35299;&#37322;&#20154;&#31867;&#30340;&#26102;&#23578;&#21697;&#21619;&#12290;</title><link>http://arxiv.org/abs/2305.02307</link><description>&lt;p&gt;
Fashionpedia-Taste&#65306;&#19968;&#20010;&#29992;&#20110;&#35299;&#37322;&#20154;&#31867;&#26102;&#23578;&#21697;&#21619;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fashionpedia-Taste: A Dataset towards Explaining Human Fashion Taste. (arXiv:2305.02307v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02307
&lt;/p&gt;
&lt;p&gt;
Fashionpedia-Taste&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#23616;&#37096;&#23646;&#24615;&#65292;&#20154;&#31867;&#27880;&#24847;&#21147;&#21644;&#35828;&#26126;&#31561;&#22810;&#31181;&#22240;&#32032;&#65292;&#33021;&#22815;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20174;&#19981;&#21516;&#30340;&#20154;&#25991;&#35270;&#35282;&#21644;&#27169;&#24577;&#20840;&#38754;&#29702;&#35299;&#21644;&#35299;&#37322;&#20154;&#31867;&#30340;&#26102;&#23578;&#21697;&#21619;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#24182;&#27809;&#26377;&#32771;&#34385;&#23548;&#33268;&#28040;&#36153;&#32773;&#21916;&#27426;&#25110;&#19981;&#21916;&#27426;&#26102;&#23578;&#22270;&#20687;&#30340;&#22810;&#31181;&#22240;&#32032;&#12290;&#21363;&#20351;&#26377;&#20004;&#20010;&#28040;&#36153;&#32773;&#21916;&#27426;&#30456;&#21516;&#30340;&#26102;&#23578;&#22270;&#20687;&#65292;&#20182;&#20204;&#20063;&#21487;&#33021;&#22240;&#20026;&#23436;&#20840;&#19981;&#21516;&#30340;&#21407;&#22240;&#21916;&#27426;&#36825;&#20010;&#22270;&#20687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#36153;&#32773;&#20026;&#20309;&#21916;&#27426;&#26576;&#20010;&#26102;&#23578;&#22270;&#20687;&#30340;&#21407;&#22240;&#12290;&#20026;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#25968;&#25454;&#38598; Fashionpedia-taste&#65292;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#27880;&#37322;&#65292;&#20197;&#35299;&#37322;&#21463;&#35797;&#32773;&#20174;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#20026;&#20160;&#20040;&#20250;&#21916;&#27426;&#25110;&#19981;&#21916;&#27426;&#19968;&#24352;&#26102;&#23578;&#22270;&#20687;&#65306;1&#65289;&#23616;&#37096;&#23646;&#24615;&#65307;2&#65289;&#20154;&#31867;&#27880;&#24847;&#21147;&#65307;3&#65289;&#35828;&#26126;&#12290;&#27492;&#22806;&#65292;&#21463;&#35797;&#32773;&#34987;&#35201;&#27714;&#25552;&#20379;&#20182;&#20204;&#30340;&#20010;&#20154;&#23646;&#24615;&#21644;&#26102;&#23578;&#20559;&#22909;&#65292;&#20363;&#22914;&#20010;&#24615;&#21644;&#21916;&#27426;&#30340;&#26102;&#23578;&#21697;&#29260;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24314;&#31435;&#35745;&#31639;&#27169;&#22411;&#65292;&#20174;&#19981;&#21516;&#30340;&#20154;&#25991;&#35270;&#35282;&#21644;&#27169;&#24577;&#20840;&#38754;&#29702;&#35299;&#21644;&#35299;&#37322;&#20154;&#31867;&#30340;&#26102;&#23578;&#21697;&#21619;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing fashion datasets do not consider the multi-facts that cause a consumer to like or dislike a fashion image. Even two consumers like a same fashion image, they could like this image for total different reasons. In this paper, we study the reason why a consumer like a certain fashion image. Towards this goal, we introduce an interpretability dataset, Fashionpedia-taste, consist of rich annotation to explain why a subject like or dislike a fashion image from the following 3 perspectives: 1) localized attributes; 2) human attention; 3) caption. Furthermore, subjects are asked to provide their personal attributes and preference on fashion, such as personality and preferred fashion brands. Our dataset makes it possible for researchers to build computational models to fully understand and interpret human fashion taste from different humanistic perspectives and modalities.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02305</link><description>&lt;p&gt;
&#26657;&#20934;&#21270;&#35299;&#37322;&#65306;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21644;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#39046;&#22495;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21487;&#33021;&#23548;&#33268;&#28389;&#29992;&#25110;&#19981;&#20351;&#29992;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#21019;&#24314;&#21487;&#20197;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20010;&#21035;&#39044;&#27979;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#20294;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;&#26657;&#20934;&#21270;&#35299;&#37322;(Calibrated Explanations&#65292;CE)&#65292;&#23427;&#22522;&#20110; Venn-Abers&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#30340;&#21516;&#26102;&#26657;&#20934;&#24213;&#23618;&#27169;&#22411;&#12290;CE&#19981;&#20165;&#25552;&#20379;&#24555;&#36895;&#12289;&#21487;&#38752;&#12289;&#31283;&#23450;&#21644;&#24378;&#20581;&#30340;&#35299;&#37322;&#65292;&#36824;&#25552;&#20379;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#26465;&#20214;&#35268;&#21017;&#65292;&#20063;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has become an integral part of decision support systems (DSSs) in various domains, but the lack of transparency in the predictive models used in AI-based DSSs can lead to misuse or disuse. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance, but they suffer from drawbacks such as instability. To address these issues, we propose a new feature importance explanation method, Calibrated Explanations (CE), which is based on Venn-Abers and calibrates the underlying model while generating feature importance explanations. CE provides fast, reliable, stable, and robust explanations, along with uncertainty quantification of the probability estimates and feature importance weights. Furthermore, the method is model agnostic with easily understood conditional rules and can also genera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.02301</link><description>&lt;p&gt;
Distilling Step-by-Step&#65281;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#32988;&#36807;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#20869;&#23384;&#25928;&#29575;&#20302;&#21644;&#35745;&#31639;&#23494;&#38598;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24494;&#35843;&#25110;&#31934;&#28860;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#31614;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#35201;&#24819;&#36798;&#21040;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Distilling Step-by-Step&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292; (a)&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#27604;LLM&#34920;&#29616;&#26356;&#22909;&#65292;(b)&#24182;&#36890;&#36807;&#21033;&#29992;&#24494;&#35843;&#25110;&#31934;&#28860;&#25152;&#38656;&#30340;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;&#20013;&#25552;&#21462;LLM&#22522;&#30784;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;&#19982;&#24494;&#35843;&#21644;&#31934;&#28860;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#20351;&#29992;&#36739;&#23569;&#30340;&#26631;&#35760;/&#26410;&#26631;&#35760;&#35757;&#32451;&#31034;&#20363;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#31532;&#20108;&#65292;&#19982;LLM&#30456;&#27604;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20063;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;BLEURT&#21644;COMET&#26159;&#26368;&#36866;&#21512;&#20316;&#20026;&#20854;&#35780;&#20272;&#25351;&#26631;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.02300</link><description>&lt;p&gt;
&#35780;&#20272;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Length-Controllable Machine Translation. (arXiv:2305.02300v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;BLEURT&#21644;COMET&#26159;&#26368;&#36866;&#21512;&#20316;&#20026;&#20854;&#35780;&#20272;&#25351;&#26631;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#31181;&#32422;&#26463;&#32763;&#35793;&#65292;&#26088;&#22312;&#22312;&#25511;&#21046;&#32763;&#35793;&#38271;&#24230;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#20445;&#30041;&#21407;&#22987;&#21547;&#20041;&#12290;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#25688;&#35201;&#25110;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#65292;&#20294;&#36825;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#21644;&#20934;&#30830;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#27425;&#31995;&#32479;&#35780;&#20272;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#35780;&#20272;&#20102;18&#20010;&#25688;&#35201;&#25110;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;BLEURT&#21644;COMET&#19982;&#20154;&#24037;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26368;&#39640;&#65292;&#26368;&#36866;&#21512;&#20316;&#20026;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length-controllable machine translation is a type of constrained translation. It aims to contain the original meaning as much as possible while controlling the length of the translation. We can use automatic summarization or machine translation evaluation metrics for length-controllable machine translation, but this is not necessarily suitable and accurate. This work is the first attempt to evaluate the automatic metrics for length-controllable machine translation tasks systematically. We conduct a rigorous human evaluation on two translation directions and evaluate 18 summarization or translation evaluation metrics. We find that BLEURT and COMET have the highest correlation with human evaluation and are most suitable as evaluation metrics for length-controllable machine translation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; DynamicStereo &#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#31435;&#20307;&#35270;&#39057;&#20013;&#20272;&#35745;&#35270;&#24046;&#65292;&#24182;&#20174;&#30456;&#37051;&#24103;&#20013;&#27719;&#38598;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#20854;&#39044;&#27979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; Dynamic Replica &#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26356;&#25509;&#36817;&#30495;&#23454;&#24212;&#29992;&#22330;&#26223;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#21160;&#24577;&#31435;&#20307;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02296</link><description>&lt;p&gt;
DynamicStereo&#65306;&#26469;&#33258;&#31435;&#20307;&#35270;&#39057;&#30340;&#19968;&#33268;&#21160;&#24577;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
DynamicStereo: Consistent Dynamic Depth from Stereo Videos. (arXiv:2305.02296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; DynamicStereo &#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#31435;&#20307;&#35270;&#39057;&#20013;&#20272;&#35745;&#35270;&#24046;&#65292;&#24182;&#20174;&#30456;&#37051;&#24103;&#20013;&#27719;&#38598;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#20854;&#39044;&#27979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; Dynamic Replica &#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26356;&#25509;&#36817;&#30495;&#23454;&#24212;&#29992;&#22330;&#26223;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#21160;&#24577;&#31435;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20174;&#31435;&#20307;&#25668;&#20687;&#22836;&#35266;&#23519;&#30340;&#21160;&#24577;&#22330;&#26223;&#20013;&#37325;&#24314;&#28145;&#24230;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31435;&#20307;&#28145;&#24230;&#26041;&#27861;&#29420;&#31435;&#22320;&#23545;&#24453;&#19981;&#21516;&#30340;&#31435;&#20307;&#24103;&#65292;&#23548;&#33268;&#26102;&#38388;&#19978;&#19981;&#19968;&#33268;&#30340;&#28145;&#24230;&#39044;&#27979;&#12290;&#23545;&#20110;&#27785;&#28024;&#24335; AR &#25110; VR &#22330;&#26223;&#65292;&#26102;&#38388;&#19968;&#33268;&#24615;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#38378;&#28865;&#20250;&#22823;&#22823;&#38477;&#20302;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; DynamicStereo&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110; transformer &#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#20272;&#35745;&#31435;&#20307;&#35270;&#39057;&#30340;&#35270;&#24046;&#12290;&#32593;&#32476;&#23398;&#20064;&#20174;&#30456;&#37051;&#24103;&#20013;&#27719;&#38598;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#20854;&#39044;&#27979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#36890;&#36807;&#21010;&#20998;&#27880;&#24847;&#21147;&#23618;&#26469;&#39640;&#25928;&#22788;&#29702;&#31435;&#20307;&#35270;&#39057;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102; Dynamic Replica&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25195;&#25551;&#29615;&#22659;&#20013;&#30340;&#20154;&#21644;&#21160;&#29289;&#30340;&#21512;&#25104;&#35270;&#39057;&#65292;&#20026;&#21160;&#24577;&#31435;&#20307;&#26356;&#25509;&#36817;&#30495;&#23454;&#24212;&#29992;&#25552;&#20379;&#20102;&#34917;&#20805;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#12290;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of reconstructing a dynamic scene observed from a stereo camera. Most existing methods for depth from stereo treat different stereo frames independently, leading to temporally inconsistent depth predictions. Temporal consistency is especially important for immersive AR or VR scenarios, where flickering greatly diminishes the user experience. We propose DynamicStereo, a novel transformer-based architecture to estimate disparity for stereo videos. The network learns to pool information from neighboring frames to improve the temporal consistency of its predictions. Our architecture is designed to process stereo videos efficiently through divided attention layers. We also introduce Dynamic Replica, a new benchmark dataset containing synthetic videos of people and animals in scanned environments, which provides complementary training and evaluation data for dynamic stereo closer to real applications than existing datasets. Training with this dataset further improves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#24335;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#32534;&#38431;&#25511;&#21046;&#26041;&#27861;&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#28369;&#27169;&#21019;&#26032;&#28388;&#27874;&#22120;&#65292;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#35774;&#35745;&#20013;&#23384;&#22312;&#30340;&#36895;&#24230;&#36339;&#36291;&#38382;&#39064;&#21644;&#25238;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#21644;&#24179;&#28369;&#30340;&#25511;&#21046;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2305.02288</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#24335;&#31070;&#32463;&#21160;&#21147;&#23398;&#21644;&#33258;&#36866;&#24212;&#28369;&#27169;&#21019;&#26032;&#28388;&#27874;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#20998;&#24067;&#24335;&#39046;&#33322;&#32773;&#36319;&#38543;&#32773;&#32534;&#38431;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Distributed Leader Follower Formation Control of Mobile Robots based on Bioinspired Neural Dynamics and Adaptive Sliding Innovation Filter. (arXiv:2305.02288v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#24335;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#32534;&#38431;&#25511;&#21046;&#26041;&#27861;&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#28369;&#27169;&#21019;&#26032;&#28388;&#27874;&#22120;&#65292;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;&#35774;&#35745;&#20013;&#23384;&#22312;&#30340;&#36895;&#24230;&#36339;&#36291;&#38382;&#39064;&#21644;&#25238;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#21644;&#24179;&#28369;&#30340;&#25511;&#21046;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#24046;&#21160;&#39537;&#21160;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#20998;&#24067;&#24335;&#39046;&#33322;&#32773;&#36319;&#38543;&#32773;&#32534;&#38431;&#25511;&#21046;&#38382;&#39064;&#12290;&#39318;&#20808;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#20272;&#35745;&#22120;&#65292;&#23427;&#20165;&#38656;&#35201;&#27599;&#20010;&#36319;&#38543;&#32773;&#33258;&#36523;&#21644;&#20854;&#30456;&#37051;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#24335;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#21453;&#27493;&#21644;&#28369;&#27169;&#25511;&#21046;&#28151;&#21512;&#32534;&#38431;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#31574;&#30053;&#35299;&#20915;&#20102;&#20256;&#32479;&#21453;&#27493;&#35774;&#35745;&#20013;&#23384;&#22312;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#36895;&#24230;&#36339;&#36291;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#31995;&#32479;&#21644;&#27979;&#37327;&#22122;&#22768;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#31574;&#30053;&#19981;&#20165;&#28040;&#38500;&#20102;&#20256;&#32479;&#28369;&#27169;&#25511;&#21046;&#20013;&#23384;&#22312;&#30340;&#25238;&#21160;&#38382;&#39064;&#65292;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#21644;&#24179;&#28369;&#30340;&#25511;&#21046;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;&#23558;&#33258;&#36866;&#24212;&#28369;&#27169;&#21019;&#26032;&#28388;&#27874;&#22120;&#19982;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#20379;&#40065;&#26834;&#30340;&#29366;&#24577;&#20272;&#35745;&#65292;&#33021;&#22815;&#20811;&#26381;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#27425;&#20223;&#30495;&#26469;&#28436;&#31034;&#21019;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigated the distributed leader follower formation control problem for multiple differentially driven mobile robots. A distributed estimator is first introduced and it only requires the state information from each follower itself and its neighbors. Then, we propose a bioinspired neural dynamic based backstepping and sliding mode control hybrid formation control method with proof of its stability. The proposed control strategy resolves the impractical speed jump issue that exists in the conventional backstepping design. Additionally, considering the system and measurement noises, the proposed control strategy not only removes the chattering issue existing in the conventional sliding mode control but also provides smooth control input with extra robustness. After that, an adaptive sliding innovation filter is integrated with the proposed control to provide accurate state estimates that are robust to modeling uncertainties. Finally, we performed multiple simulations to demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;</title><link>http://arxiv.org/abs/2305.02279</link><description>&lt;p&gt;
Learngene: &#20174;&#31062;&#20808;&#27169;&#22411;&#20013;&#32487;&#25215;&#21387;&#32553;&#30693;&#35782;&#21040;&#21518;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#29983;&#29289;&#30340;&#36830;&#32493;&#36827;&#21270;&#36807;&#31243;&#20013;&#65292;&#23427;&#30340;&#22522;&#22240;&#31215;&#32047;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#21644;&#30693;&#35782;&#65292;&#20351;&#26032;&#29983;&#21518;&#20195;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#20854;&#29305;&#23450;&#29615;&#22659;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#21363; Learngene&#65292;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#34701;&#21512;&#22522;&#22240;&#30340;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290; (i) &#31215;&#32047;&#65306;&#30693;&#35782;&#22312;&#31062;&#20808;&#27169;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#31215;&#32047;&#12290; (ii) &#21387;&#32553;&#65306;&#23558;&#31215;&#32047;&#30340;&#35814;&#23613;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#65292;&#21363; Learngene&#12290; (iii) &#32487;&#25215;&#65306;&#23558;&#21387;&#32553;&#30340; Learngene &#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#12290;&#30001;&#20110;&#31215;&#32047;&#24050;&#22312;&#19968;&#20123;&#25104;&#29087;&#30340;&#33539;&#24335;&#20013;&#24471;&#21040;&#30740;&#31350;&#65292;&#22914;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#32456;&#36523;&#23398;&#20064;&#65292;&#22240;&#27492;&#25105;&#20204;&#19987;&#27880;&#20110;&#21387;&#32553;&#21644;&#32487;&#25215;&#65292;&#36825;&#24341;&#21457;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#37197;&#32622;&#20102;&#25913;&#36827;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#21363;&#22522;&#20110;&#20013;&#36716;&#30340;&#32423;&#32852;&#32763;&#35793;&#27169;&#22411;&#65292;&#20351;&#29992;&#21152;&#26435;&#20013;&#36716;&#35821;&#35328;&#23884;&#20837;&#36755;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#32531;&#35299;&#26631;&#35760;&#21644;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02261</link><description>&lt;p&gt;
&#22522;&#20110;&#20013;&#36716;&#30340;&#32423;&#32852;&#32763;&#35793;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#19982;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Training and Decoding for Pivot-based Cascaded Translation Model. (arXiv:2305.02261v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#37197;&#32622;&#20102;&#25913;&#36827;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#21363;&#22522;&#20110;&#20013;&#36716;&#30340;&#32423;&#32852;&#32763;&#35793;&#27169;&#22411;&#65292;&#20351;&#29992;&#21152;&#26435;&#20013;&#36716;&#35821;&#35328;&#23884;&#20837;&#36755;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#32531;&#35299;&#26631;&#35760;&#21644;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20013;&#36716;&#35821;&#35328;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#25928;&#26524;&#12290;&#36890;&#24120;&#65292;&#28304;&#35821;&#35328;&#21040;&#20013;&#36716;&#35821;&#35328;&#27169;&#22411;&#21644;&#20013;&#36716;&#35821;&#35328;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20998;&#21035;&#35757;&#32451;&#65292;&#27809;&#26377;&#21033;&#29992;&#26377;&#38480;&#30340;&#65288;&#28304;&#35821;&#35328;&#65292;&#30446;&#26631;&#35821;&#35328;&#65289;&#24182;&#34892;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32423;&#32852;&#32763;&#35793;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#37197;&#32622;&#20102;&#25913;&#36827;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#20013;&#36716;&#35821;&#35328;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#26681;&#25454;&#28304;&#35821;&#35328;&#21040;&#20013;&#36716;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#20998;&#24067;&#20462;&#25913;&#20026;&#21152;&#26435;&#20013;&#36716;&#35821;&#35328;&#23884;&#20837;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#20013;&#36716;&#35821;&#35328;&#35299;&#30721;&#20013;&#30340;&#27874;&#26463;&#25628;&#32034;&#26102;&#65292;&#25105;&#20204;&#32531;&#35299;&#20102;&#26631;&#35760;&#21644;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing pivot language effectively can significantly improve low-resource machine translation. Usually, the two translation models, source-pivot and pivot-target, are trained individually and do not utilize the limited (source, target) parallel data. This work proposes an end-to-end training method for the cascaded translation model and configures an improved decoding algorithm. The input of the pivot-target model is modified to weighted pivot embedding based on the probability distribution output by the source-pivot model. This allows the model to be trained end-to-end. In addition, we mitigate the inconsistency between tokens and probability distributions while using beam search in pivot decoding. Experiments demonstrate that our method enhances the quality of translation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#21644;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#22823;&#32423;&#21035;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#21457;&#23637;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478;&#12290;</title><link>http://arxiv.org/abs/2305.02251</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65306;&#20174;&#26041;&#31243;&#24335;&#25506;&#32034;&#21040;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems. (arXiv:2305.02251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#21644;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#22823;&#32423;&#21035;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#21457;&#23637;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20174;&#26041;&#31243;&#24335;&#25506;&#32034;&#21644;&#31526;&#21495;&#22238;&#24402;&#21040;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#21644;&#20195;&#29702;&#12290;&#20174;&#8220;&#23439;&#35266;&#8221;&#21644;&#19978;&#19979;&#25991;&#35282;&#24230;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#20063;&#35752;&#35770;&#20102;&#24320;&#25918;&#38382;&#39064;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#35282;&#33394;&#65292;&#24110;&#21161;&#21457;&#29616;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#65292;&#20174;Adam&#31995;&#32479;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#21040;&#24403;&#21069;&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#22825;&#25991;&#23398;&#31561;&#39046;&#22495;&#30340;&#21162;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#33258;&#20027;&#24615;&#65292;&#24182;&#20197;&#33258;&#21160;&#39550;&#39542;&#30340;&#33258;&#20027;&#32423;&#21035;&#20026;&#31867;&#27604;&#12290;&#26368;&#22823;&#32423;&#21035;&#65292;&#31532;&#20116;&#32423;&#65292;&#23450;&#20041;&#20026;&#22312;&#29983;&#20135;&#31185;&#23398;&#30693;&#35782;&#26102;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#23454;&#29616;&#36825;&#19968;&#28857;&#26159;&#36808;&#21521;&#35299;&#20915;Nobel Turing Grand Challenge&#30340;&#19968;&#27493;&#65306;&#24320;&#21457;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#31185;&#23398;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478; - &#33021;&#21147;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents. It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge. Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy. Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving. The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge. Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#26356;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.02239</link><description>&lt;p&gt;
&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#22312;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Label-Description Training for Zero-Shot Text Classification. (arXiv:2305.02239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#26356;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20801;&#35768;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36716;&#31227;&#35821;&#20041;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#23567;&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25551;&#36848;&#20219;&#21153;&#26631;&#31614;&#12290;&#19982;&#36890;&#24120;&#26377;&#25991;&#26412;&#26631;&#27880;&#26631;&#31614;&#30340;&#24494;&#35843;&#25968;&#25454;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#21482;&#26159;&#29992;&#35821;&#35328;&#25551;&#36848;&#26631;&#31614;&#65292;&#20363;&#22914;&#20351;&#29992;&#19968;&#20123;&#30456;&#20851;&#26415;&#35821;&#12289;&#35789;&#20856;/&#30334;&#31185;&#20840;&#20070;&#26465;&#30446;&#21644;&#30701;&#27169;&#26495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20027;&#39064;&#21644;&#24773;&#24863;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#38646;&#26679;&#26412;&#39640;15-17&#65285;&#32477;&#23545;&#20540;&#12290;&#23427;&#36824;&#26356;&#20855;&#26377;&#38646;&#26679;&#26412;&#20998;&#31867;&#25152;&#38656;&#36873;&#25321;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;&#25552;&#31034;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;&#27169;&#24335;&#20197;&#21450;&#20174;&#26631;&#31614;&#26144;&#23556;&#21040;&#27169;&#22411;&#35789;&#27719;&#34920;&#20013;&#30340;&#20196;&#29260;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#20165;&#25551;&#36848;&#26631;&#31614;&#20294;&#19981;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#65292;&#22240;&#27492;&#22312;&#20854;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#31867;&#30340;&#37325;&#28857;&#26356;&#19987;&#27880;&#20110;&#26631;&#31614;&#32780;&#19981;&#26159;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language, e.g., using a few related terms, dictionary/encyclopedia entries, and short templates. Across a range of topic and sentiment datasets, our method is more accurate than zero-shot by 15-17% absolute. It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model's vocabulary. Furthermore, since our data merely describes the labels but does not use input texts, finetuning on it yields a model that p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#65292;&#21253;&#25324;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#21450;&#23545;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#30340;&#25903;&#25345;&#24230;&#21644;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02231</link><description>&lt;p&gt;
&#26500;&#24314;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#65306;&#20174;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#12289;&#20262;&#29702;&#21644;&#20027;&#35201;&#38656;&#27714;&#21040;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#30417;&#31649;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation. (arXiv:2305.02231v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02231
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#65292;&#21253;&#25324;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#21450;&#23545;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#30340;&#25903;&#25345;&#24230;&#21644;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#22522;&#20110;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#65292;&#20998;&#21035;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#21253;&#25324;&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#26377;&#21442;&#19982;&#27969;&#31243;&#21644;&#21442;&#19982;&#32773;&#21487;&#20449;&#24615;&#30340;&#32771;&#37327;&#12290;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#24895;&#26223;&#23558;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#19979;&#35201;&#20214;&#30340;&#25903;&#25345;&#24230;&#20197;&#21450;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;&#35780;&#20272;&#19971;&#20010;&#38656;&#27714;&#20043;&#25216;&#26415;&#26041;&#38754;&#12289;&#20262;&#29702;&#26041;&#38754;&#21644;&#30417;&#31649;&#25361;&#25112;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;ChatGPT&#22312;&#20837;&#38376;&#32423;&#20989;&#25968;&#24335;&#35821;&#35328;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#33719;&#24471;B-&#30340;&#25104;&#32489;&#24182;&#19988;&#33021;&#22815;&#32473;&#23398;&#29983;&#21644;&#35762;&#24072;&#24102;&#26469;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2305.02230</link><description>&lt;p&gt;
ChatGPT&#33021;&#36890;&#36807;&#20837;&#38376;&#32423;&#20989;&#25968;&#24335;&#35821;&#35328;&#32534;&#31243;&#35838;&#31243;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Pass An Introductory Level Functional Language Programming Course?. (arXiv:2305.02230v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;ChatGPT&#22312;&#20837;&#38376;&#32423;&#20989;&#25968;&#24335;&#35821;&#35328;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#33719;&#24471;B-&#30340;&#25104;&#32489;&#24182;&#19988;&#33021;&#22815;&#32473;&#23398;&#29983;&#21644;&#35762;&#24072;&#24102;&#26469;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24341;&#20837;&#24341;&#36215;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#35745;&#31639;&#26426;&#32534;&#31243;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23427;&#32534;&#20889;&#12289;&#20462;&#25913;&#29978;&#33267;&#32416;&#27491;&#20195;&#30721;&#30340;&#33021;&#21147;&#21152;&#19978;&#20854;&#26131;&#20110;&#20351;&#29992;&#21644;&#35775;&#38382;&#24050;&#32463;&#23545;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;ChatGPT&#22312;&#20837;&#38376;&#32423;&#20989;&#25968;&#24335;&#35821;&#35328;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#31995;&#32479;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#25226;ChatGPT&#35270;&#20026;&#25105;&#20204;&#30340;&#19968;&#21517;&#23398;&#29983;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#33719;&#24471;B-&#30340;&#25104;&#32489;&#65292;&#25490;&#21517;&#22312;314&#21517;&#23398;&#29983;&#20013;&#30340;&#31532;155&#20301;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#35780;&#20272;&#20174;&#23398;&#29983;&#21644;&#35762;&#24072;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;ChatGPT&#21487;&#20197;&#20026;&#36825;&#20004;&#20010;&#32676;&#20307;&#25552;&#20379;&#30340;&#20960;&#20010;&#28508;&#22312;&#22909;&#22788;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#26412;&#30740;&#31350;&#26126;&#30830;&#20102;ChatGPT&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent introduction of ChatGPT has drawn significant attention from both industry and academia due to its impressive capabilities in solving a diverse range of tasks, including language translation, text summarization, and computer programming. Its capability for writing, modifying, and even correcting code together with its ease of use and access is already dramatically impacting computer science education. This paper aims to explore how well ChatGPT can perform in an introductory-level functional language programming course. In our systematic evaluation, we treated ChatGPT as one of our students and demonstrated that it can achieve a grade B- and its rank in the class is 155 out of 314 students overall. Our comprehensive evaluation provides valuable insights into ChatGPT's impact from both student and instructor perspectives. Additionally, we identify several potential benefits that ChatGPT can offer to both groups. Overall, we believe that this study significantly clarifies and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.02220</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#65306;&#26469;&#33258;MEDIQA-Chat&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#25552;&#20132;&#30340;&#33258;&#21160;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#26041;&#26696;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#65306;&#31532;&#19968;&#31181;&#26159;&#22312;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#22914;&#36890;&#36807;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#20363;&#22914;ROUGE&#65292;BERTScore&#65289;&#27979;&#37327;&#65292;&#24182;&#20998;&#21035;&#22312;&#25152;&#26377;&#25552;&#20132;&#30340;&#26041;&#26696;&#20013;&#25490;&#21517;&#31532;&#20108;&#21644;&#31532;&#19968;&#12290;&#19987;&#23478;&#23457;&#26680;&#34920;&#26126;&#65292;&#36890;&#36807;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#20351;&#29992;GPT-4&#29983;&#25104;&#30340;&#31508;&#35760;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#19968;&#26679;&#21463;&#27426;&#36814;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#31508;&#35760;&#30340;&#26377;&#21069;&#36884;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#20998;&#31867;&#26041;&#27861;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#26680;&#24515;&#21457;&#29616;&#20026;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.02215</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#20998;&#31867;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages. (arXiv:2305.02215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#20998;&#31867;&#26041;&#27861;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#26680;&#24515;&#21457;&#29616;&#20026;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#20351;&#20154;&#20204;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#26426;&#22120;&#26159;&#22312;&#22797;&#21046;&#26576;&#20123;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36824;&#26159;&#21457;&#29616;&#20102;&#26681;&#26412;&#24615;&#30340;&#26032;&#29702;&#35770;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35266;&#28857;&#65292;&#20351;&#29992;&#35821;&#35328;&#20043;&#38388;&#30340;&#31867;&#22411;&#30456;&#20284;&#24615;&#26469;&#23545;&#27604;&#19981;&#21516;&#35821;&#35328;&#30340;transformer&#27169;&#22411;&#65292;&#35266;&#23519;&#36825;&#20123;&#30456;&#20284;&#24615;&#26159;&#21542;&#20986;&#29616;&#22312;&#29305;&#23450;&#30340;&#23618;&#27425;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20013;&#24515;&#26680;&#23545;&#40784;&#30340;&#26435;&#37325;&#30697;&#38453;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21477;&#27861;&#31867;&#22411;&#23398;&#30456;&#20284;&#24615;&#19982;&#20013;&#38388;&#23618;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#19968;&#21457;&#29616;&#30830;&#35748;&#20102;&#36890;&#36807;&#21477;&#27861;&#25506;&#38024;&#26041;&#27861;&#33719;&#24471;&#30340;BERT&#30340;&#32467;&#26524;&#65292;&#24182;&#22240;&#27492;&#37325;&#35201;&#22320;&#35777;&#26126;&#20102;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The overwhelming success of transformers is a real conundrum stimulating a compelling question: are these machines replicating some traditional linguistic models or discovering radically new theories? In this paper, we propose a novel standpoint to investigate this important question. Using typological similarities among languages, we aim to layer-wise compare transformers for different languages to observe whether these similarities emerge for particular layers. For this investigation, we propose to use Centered kernel alignment to measure similarity among weight matrices. We discovered that syntactic typological similarity is consistent with the similarity among weights in the middle layers. This finding confirms results obtained by syntactically probing BERT and, thus, gives an important confirmation that BERT is replicating traditional linguistic models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VGA&#26041;&#27861;&#30340;&#25340;&#36710;&#31995;&#32479;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#22823;&#35268;&#27169;MoD&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#21644;&#30456;&#24212;&#30340;&#36710;&#36742;&#36335;&#24452;&#12290;&#30740;&#31350;&#32773;&#27604;&#36739;&#20102;&#20351;&#29992;&#26368;&#20248;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#19982;&#20351;&#29992;&#26222;&#36890;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02209</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#32593;&#19978;&#25340;&#36710;&#65306;&#20219;&#21153;&#20998;&#37197;&#20248;&#21270;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Large-scale Online Ridesharing: The Effect of Assignment Optimality on System Performance. (arXiv:2305.02209v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VGA&#26041;&#27861;&#30340;&#25340;&#36710;&#31995;&#32479;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#22823;&#35268;&#27169;MoD&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#21644;&#30456;&#24212;&#30340;&#36710;&#36742;&#36335;&#24452;&#12290;&#30740;&#31350;&#32773;&#27604;&#36739;&#20102;&#20351;&#29992;&#26368;&#20248;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#19982;&#20351;&#29992;&#26222;&#36890;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#21160;&#20986;&#34892;&#65288;MoD&#65289;&#31995;&#32479;&#30001;&#19968;&#32676;&#20849;&#20139;&#27773;&#36710;&#32452;&#25104;&#65292;&#21487;&#29992;&#20110;&#21333;&#31243;&#28857;&#23545;&#28857;&#30340;&#20056;&#22352;&#12290;&#36890;&#36807;&#20351;&#29992;&#25340;&#36710;&#65292;&#21363;&#23558;&#22810;&#21517;&#20056;&#23458;&#20998;&#37197;&#21040;&#19968;&#36742;&#36710;&#19978;&#65292;&#21487;&#20197;&#20943;&#23569;&#36710;&#36742;&#21644;&#36710;&#38431;&#30340;&#34892;&#39542;&#24635;&#37324;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;MoD&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#30340;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;VGA&#26041;&#27861;&#30340;&#26368;&#36817;&#25552;&#20986;&#30340;&#25340;&#36710;&#31995;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#22823;&#35268;&#27169;MoD&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#21644;&#30456;&#24212;&#30340;&#36710;&#36742;&#36335;&#24452;&#12290;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#25152;&#26377;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#38382;&#39064;&#65292;&#23450;&#26399;&#22788;&#29702;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#36742;&#36710;&#21644;&#20056;&#23458;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26816;&#26597;&#20351;&#29992;&#26368;&#20248;&#25340;&#36710;&#20998;&#37197;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#26368;&#20248;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#19982;&#20351;&#29992;&#26222;&#36890;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobility-on-demand (MoD) systems consist of a fleet of shared vehicles that can be hailed for one-way point-to-point trips. The total distance driven by the vehicles and the fleet size can be reduced by employing ridesharing, i.e., by assigning multiple passengers to one vehicle. However, finding the optimal passenger-vehicle assignment in an MoD system is a hard combinatorial problem. In this work, we demonstrate how the VGA method, a recently proposed systematic method for ridesharing, can be used to compute the optimal passenger-vehicle assignments and corresponding vehicle routes in a massive-scale MoD system. In contrast to existing works, we solve all passenger-vehicle assignment problems to optimality, regularly dealing with instances containing thousands of vehicles and passengers. Moreover, to examine the impact of using optimal ridesharing assignments, we compare the performance of an MoD system that uses optimal assignments against an MoD system that uses assignments compute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CALM&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#34394;&#25311;&#35282;&#33394;&#21160;&#20316;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#26679;&#24335;&#26465;&#20214;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#20016;&#23500;&#22797;&#26434;&#30340;&#35821;&#20041;&#36816;&#21160;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.02195</link><description>&lt;p&gt;
&#26465;&#20214;&#23545;&#25239;&#28508;&#22312;&#27169;&#22411;&#29992;&#20110;&#21487;&#30452;&#25509;&#25805;&#25511;&#34394;&#25311;&#35282;&#33394;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CALM: Conditional Adversarial Latent Models for Directable Virtual Characters. (arXiv:2305.02195v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CALM&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#34394;&#25311;&#35282;&#33394;&#21160;&#20316;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#26679;&#24335;&#26465;&#20214;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#20016;&#23500;&#22797;&#26434;&#30340;&#35821;&#20041;&#36816;&#21160;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26465;&#20214;&#23545;&#25239;&#28508;&#22312;&#27169;&#22411;&#65288;CALM&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#21487;&#30452;&#25509;&#25805;&#25511;&#34394;&#25311;&#35282;&#33394;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#65292;CALM&#23398;&#20064;&#20102;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#36816;&#21160;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#36816;&#21160;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#23545;&#35282;&#33394;&#21160;&#20316;&#30340;&#30452;&#25509;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#32852;&#21512;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#21644;&#36816;&#21160;&#32534;&#30721;&#22120;&#65292;&#37325;&#24314;&#32473;&#23450;&#36816;&#21160;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22797;&#21046;&#23427;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CALM&#23398;&#20064;&#20102;&#19968;&#31181;&#35821;&#20041;&#36816;&#21160;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#35266;&#30340;&#30028;&#38754;&#25511;&#21046;&#29983;&#25104;&#30340;&#36816;&#21160;&#65292;&#36866;&#21512;&#26356;&#39640;&#23618;&#27425;&#30340;&#20219;&#21153;&#35757;&#32451;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#35282;&#33394;&#21487;&#20197;&#20351;&#29992;&#31867;&#20284;&#20110;&#35270;&#39057;&#28216;&#25103;&#30340;&#30452;&#35266;&#30028;&#38754;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present Conditional Adversarial Latent Models (CALM), an approach for generating diverse and directable behaviors for user-controlled interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human motion, and enables direct control over character movements. The approach jointly learns a control policy and a motion encoder that reconstructs key characteristics of a given motion without merely replicating it. The results show that CALM learns a semantic motion representation, enabling control over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces, akin to those found in video games.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#24425;&#31080;&#38382;&#39064;&#20013;&#22270;&#30340;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#33410;&#28857;&#36755;&#20837;&#36755;&#20986;&#29305;&#24449;&#30340;&#23545;&#31216;&#20462;&#21098;&#25216;&#26415;&#21644;&#20445;&#30041;&#29305;&#24449;&#22270;&#31354;&#38388;&#20301;&#32622;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02190</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22270;&#24425;&#31080;: &#22270;&#30340;&#31232;&#30095;&#24615;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Rethinking Graph Lottery Tickets: Graph Sparsity Matters. (arXiv:2305.02190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#24425;&#31080;&#38382;&#39064;&#20013;&#22270;&#30340;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#33410;&#28857;&#36755;&#20837;&#36755;&#20986;&#29305;&#24449;&#30340;&#23545;&#31216;&#20462;&#21098;&#25216;&#26415;&#21644;&#20445;&#30041;&#29305;&#24449;&#22270;&#31354;&#38388;&#20301;&#32622;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#29468;&#24819; (LTH) &#22768;&#31216;&#23384;&#22312;&#19968;&#31181;&#33719;&#32988;&#30340;&#24425;&#31080; (&#21363;&#65292;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#20462;&#21098;&#30340;&#23376;&#32593;&#32476;&#20197;&#21450;&#21407;&#22987;&#26435;&#37325;&#21021;&#22987;&#21270;)&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#19982;&#21407;&#22987;&#23494;&#38598;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#31216;&#20026; UGS&#65292;&#25193;&#23637;&#20102; LTH &#20197;&#20462;&#21098;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN)&#65292;&#20197;&#26377;&#25928;&#21152;&#36895; GNN &#25512;&#29702;&#12290;UGS &#21516;&#26102;&#20351;&#29992;&#30456;&#21516;&#30340;&#23631;&#34109;&#26426;&#21046;&#20462;&#21098;&#22270;&#37051;&#25509;&#30697;&#38453;&#21644;&#27169;&#22411;&#26435;&#37325;&#65292;&#20294;&#30001;&#20110;&#22270;&#37051;&#25509;&#30697;&#38453;&#21644;&#26435;&#37325;&#30697;&#38453;&#30340;&#35282;&#33394;&#38750;&#24120;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#31232;&#30095;&#21270;&#23548;&#33268;&#19981;&#21516;&#30340;&#24615;&#33021;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#22270;&#30340;&#31232;&#30095;&#31243;&#24230;&#36229;&#36807;&#19968;&#23450;&#31243;&#24230;&#26102;&#65292;&#31232;&#30095; GNN &#30340;&#24615;&#33021;&#20250;&#26174;&#30528;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#24403;&#22270;&#30340;&#31232;&#30095;&#31243;&#24230;&#36739;&#39640;&#26102;&#30340; GNN &#24615;&#33021;&#12290;&#39318;&#20808;&#65292;UGS &#20351;&#29992;&#20002;&#22833;&#20844;&#24335;&#20462;&#21098;&#37051;&#25509;&#30697;&#38453;&#65292;&#28982;&#32780;&#65292;&#35813;&#25216;&#26415;&#24182;&#26410;&#36866;&#24403;&#28041;&#21450;&#37051;&#25509;&#30697;&#38453;&#30340;&#25152;&#26377;&#20803;&#32032;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#31216;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20462;&#21098;&#37051;&#25509;&#30697;&#38453;&#30340;&#21516;&#26102;&#20445;&#35777;&#27599;&#20010;&#33410;&#28857;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#29305;&#24449;&#34987;&#20445;&#30041;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#23427;&#36991;&#20813;&#20102;&#30452;&#25509;&#20462;&#21098;&#22270;&#32467;&#26500;&#65292;&#32780;&#26159;&#22312;&#20445;&#30041;&#29305;&#24449;&#22270;&#30340;&#31354;&#38388;&#20301;&#32622;&#30340;&#21516;&#26102;&#20462;&#21098;&#20102; GNN &#23618;&#30340;&#29305;&#24449;&#36890;&#36947;&#65292;&#31867;&#20284;&#20110;&#22270;&#20687;&#21367;&#31215;&#32593;&#32476;&#37319;&#29992;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#22312;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102; 7.0% &#21644; 5.0x&#12290;
&lt;/p&gt;
&lt;p&gt;
Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance to the original dense network. A recent work, called UGS, extended LTH to prune graph neural networks (GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes the graph adjacency matrix and the model weights using the same masking mechanism, but since the roles of the graph adjacency matrix and the weight matrices are very different, we find that their sparsifications lead to different performance characteristics. Specifically, we find that the performance of a sparsified GNN degrades significantly when the graph sparsity goes beyond a certain extent. Therefore, we propose two techniques to improve GNN performance when the graph sparsity is high. First, UGS prunes the adjacency matrix using a loss formulation which, however, does not properly involve all elements of the ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#19982;&#25345;&#32493;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#38750;&#21333;&#35843;&#25512;&#29702;&#20219;&#21153;&#26102;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02171</link><description>&lt;p&gt;
&#25345;&#32493;&#25512;&#29702;&#65306;&#22312;&#31070;&#32463;&#31526;&#21495; AI &#20013;&#20351;&#29992;&#25345;&#32493;&#23398;&#20064;&#36827;&#34892;&#38750;&#21333;&#35843;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Continual Reasoning: Non-Monotonic Reasoning in Neurosymbolic AI using Continual Learning. (arXiv:2305.02171v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#19982;&#25345;&#32493;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#38750;&#21333;&#35843;&#25512;&#29702;&#20219;&#21153;&#26102;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#22312;&#30456;&#20284;&#24615;&#25512;&#29702;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#25237;&#36164;&#21644;&#20196;&#20154;&#30633;&#30446;&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#22312;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#24418;&#24335;&#65292;&#22914;&#38750;&#21333;&#35843;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#38750;&#21333;&#35843;&#26159;&#38750;&#32463;&#20856;&#25512;&#29702;&#30340;&#19968;&#20010;&#29305;&#24615;&#65292;&#36890;&#24120;&#22312;&#24120;&#35782;&#25512;&#29702;&#20013;&#30475;&#21040;&#65292;&#25512;&#29702;&#31995;&#32479;&#20801;&#35768;&#65288;&#19982;&#21476;&#20856;&#36923;&#36753;&#19981;&#21516;&#65289;&#20316;&#20986;&#21487;&#33021;&#31245;&#21518;&#34987;&#25764;&#22238;&#30340;&#32467;&#35770;&#65292;&#24403;&#26377;&#26032;&#20449;&#24687;&#21487;&#29992;&#26102;&#12290;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65288;&#22914;&#36923;&#36753;&#24352;&#37327;&#32593;&#32476;&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#19982;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;LTN&#22312;&#22788;&#29702;&#38750;&#21333;&#35843;&#25512;&#29702;&#20219;&#21153;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#20174;&#30693;&#35782;&#21644;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#22238;&#24518;&#30340;&#23398;&#20064;&#35838;&#31243;&#23558;&#25345;&#32493;&#23398;&#20064;&#21152;&#20837;&#21040;LTN&#20013;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#36807;&#31243;&#20026;&#8220;&#25345;&#32493;&#25512;&#29702;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the extensive investment and impressive recent progress at reasoning by similarity, deep learning continues to struggle with more complex forms of reasoning such as non-monotonic and commonsense reasoning. Non-monotonicity is a property of non-classical reasoning typically seen in commonsense reasoning, whereby a reasoning system is allowed (differently from classical logic) to jump to conclusions which may be retracted later, when new information becomes available. Neural-symbolic systems such as Logic Tensor Networks (LTN) have been shown to be effective at enabling deep neural networks to achieve reasoning capabilities. In this paper, we show that by combining a neural-symbolic system with methods from continual learning, LTN can obtain a higher level of accuracy when addressing non-monotonic reasoning tasks. Continual learning is added to LTNs by adopting a curriculum of learning from knowledge and data with recall. We call this process Continual Reasoning, a new methodolog
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02151</link><description>&lt;p&gt;
&#35821;&#35328;&#36317;&#31163;&#19982;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02151
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#23545;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#31181;&#25928;&#24212;&#22914;&#20309;&#26144;&#23556;&#21040;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24494;&#35843;&#26399;&#38388;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#19978;&#30340;&#24433;&#21709;&#65292;&#32780;&#26412;&#30740;&#31350;&#30740;&#31350;&#30340;&#26159;&#30001;MLLMs&#29983;&#25104;&#30340;&#30456;&#24212;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#30340;&#32477;&#23545;&#28436;&#21464;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#35821;&#35328;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#35843;&#26597;&#20854;&#19982;&#34920;&#31034;&#31354;&#38388;&#21644;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#35828;&#26126;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#29141;&#40614;&#20026;&#20160;&#20040;&#20415;&#23452;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20449;&#24687;&#35770;&#30340;&#29702;&#35770;&#24037;&#20316;&#19982;&#28216;&#25103;&#20869;&#23481;&#29983;&#25104;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#33021;&#22815;&#20135;&#29983;&#30340;&#26368;&#22797;&#26434;&#21046;&#21697;&#30340; Kolomogorov &#22797;&#26434;&#24230;&#19982;&#35813;&#29983;&#25104;&#22120;&#21487;&#33021;&#31354;&#38388;&#30340;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02131</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#29141;&#40614;&#26159;&#20415;&#23452;&#30340;&#65311;&#31185;&#23572;&#33707;&#25096;&#27931;&#22827;&#22797;&#26434;&#24615;&#19982;&#31243;&#24207;&#21270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Oatmeal is Cheap: Kolmogorov Complexity and Procedural Generation. (arXiv:2305.02131v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#29141;&#40614;&#20026;&#20160;&#20040;&#20415;&#23452;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20449;&#24687;&#35770;&#30340;&#29702;&#35770;&#24037;&#20316;&#19982;&#28216;&#25103;&#20869;&#23481;&#29983;&#25104;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#33021;&#22815;&#20135;&#29983;&#30340;&#26368;&#22797;&#26434;&#21046;&#21697;&#30340; Kolomogorov &#22797;&#26434;&#24230;&#19982;&#35813;&#29983;&#25104;&#22120;&#21487;&#33021;&#31354;&#38388;&#30340;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31243;&#24207;&#21270;&#29983;&#25104;&#22312;&#28216;&#25103;&#24320;&#21457;&#32773;&#20013;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23398;&#26415;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26032;&#24212;&#29992;&#19978;&#65292;&#19968;&#20123;&#30740;&#31350;&#21017;&#30528;&#37325;&#20110;&#23454;&#35777;&#20998;&#26512;&#12290;&#26412;&#25991;&#23558;&#20449;&#24687;&#35770;&#30340;&#29702;&#35770;&#24037;&#20316;&#19982;&#28216;&#25103;&#20869;&#23481;&#29983;&#25104;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#33021;&#22815;&#20135;&#29983;&#30340;&#26368;&#22797;&#26434;&#21046;&#21697;&#30340; Kolomogorov &#22797;&#26434;&#24230;&#19982;&#35813;&#29983;&#25104;&#22120;&#21487;&#33021;&#31354;&#38388;&#30340;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#30693;&#35782;&#32534;&#30721;&#22312;&#29983;&#25104;&#22120;&#20013;&#12289;&#36755;&#20986;&#31354;&#38388;&#30340;&#23494;&#24230;&#20197;&#21450;&#25152;&#20135;&#29983;&#30340;&#21046;&#21697;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#38480;&#21046;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#19987;&#23478;&#31243;&#24207;&#29983;&#25104;&#22120;&#35774;&#35745;&#32773;&#30340;&#32463;&#39564;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#29992;&#19968;&#20123;&#20363;&#23376;&#21152;&#20197;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although procedural generation is popular among game developers, academic research on the topic has primarily focused on new applications, with some research into empirical analysis. In this paper we relate theoretical work in information theory to the generation of content for games. We prove that there is a relationship between the Kolomogorov complexity of the most complex artifact a generator can produce, and the size of that generator's possibility space. In doing so, we identify the limiting relationship between the knowledge encoded in a generator, the density of its output space, and the intricacy of the artifacts it produces. We relate our result to the experience of expert procedural generator designers, and illustrate it with some examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02128</link><description>&lt;p&gt;
&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65306;&#22312;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#24230;&#37327;&#34892;&#20026;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning. (arXiv:2305.02128v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31185;&#23398;&#25552;&#20379;&#20102;&#22810;&#26679;&#24615;&#20855;&#26377;&#38887;&#24615;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#24378;&#21046;&#35201;&#27714;&#21516;&#36136;&#24615;&#20197;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#30340;&#25928;&#29575;&#12290;&#24403;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#19981;&#21463;&#21516;&#36136;&#31574;&#30053;&#30340;&#38480;&#21046;&#26102;&#65292;&#20010;&#20307;&#20195;&#29702;&#21487;&#33021;&#20250;&#21457;&#23637;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#20135;&#29983;&#26377;&#21033;&#20110;&#31995;&#32479;&#30340;&#26032;&#20852;&#20114;&#34917;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#32570;&#20047;&#34913;&#37327;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#20013;&#34892;&#20026;&#22810;&#26679;&#24615;&#30340;&#24037;&#20855;&#24847;&#21619;&#30528;&#25105;&#20204;&#26080;&#27861;&#28145;&#20837;&#20102;&#35299;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65288;SND&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#25506;&#35752;&#24182;&#35777;&#26126;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#23558;&#20854;&#19982;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#26368;&#26032;&#34892;&#20026;&#22810;&#26679;&#24615;&#25351;&#26631;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary science provides evidence that diversity confers resilience. Yet, traditional multi-agent reinforcement learning techniques commonly enforce homogeneity to increase training sample efficiency. When a system of learning agents is not constrained to homogeneous policies, individual agents may develop diverse behaviors, resulting in emergent complementarity that benefits the system. Despite this feat, there is a surprising lack of tools that measure behavioral diversity in systems of learning agents. Such techniques would pave the way towards understanding the impact of diversity in collective resilience and performance. In this paper, we introduce System Neural Diversity (SND): a measure of behavioral heterogeneity for multi-agent systems where agents have stochastic policies. %over a continuous state space. We discuss and prove its theoretical properties, and compare it with alternate, state-of-the-art behavioral diversity metrics used in cross-disciplinary domains. Through
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#29702;&#35770;&#21644;&#23454;&#39564;&#30340;&#26041;&#27861;&#25506;&#35752;&#20102;&#21033;&#29992;&#25658;&#24102;&#36712;&#36947;&#35282;&#21160;&#37327;&#30340;&#20809;&#23376;&#25110;&#32416;&#32544;&#20809;&#23376;&#30340;&#37327;&#23376;&#24178;&#28041;&#23454;&#29616;&#38750;&#23545;&#31216;&#38598;&#20307;&#20915;&#31574;&#65292;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02117</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#37327;&#23376;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Asymmetric quantum decision-making. (arXiv:2305.02117v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#29702;&#35770;&#21644;&#23454;&#39564;&#30340;&#26041;&#27861;&#25506;&#35752;&#20102;&#21033;&#29992;&#25658;&#24102;&#36712;&#36947;&#35282;&#21160;&#37327;&#30340;&#20809;&#23376;&#25110;&#32416;&#32544;&#20809;&#23376;&#30340;&#37327;&#23376;&#24178;&#28041;&#23454;&#29616;&#38750;&#23545;&#31216;&#38598;&#20307;&#20915;&#31574;&#65292;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20307;&#20915;&#31574;&#23545;&#20110;&#20449;&#24687;&#19982;&#36890;&#20449;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20915;&#31574;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#20914;&#31361;&#38459;&#30861;&#20102;&#25972;&#20010;&#31995;&#32479;&#28508;&#22312;&#25928;&#29992;&#30340;&#26368;&#22823;&#21270;&#12290;&#37327;&#23376;&#36807;&#31243;&#21487;&#20197;&#21033;&#29992;&#20809;&#23376;&#30340;&#32416;&#32544;&#25110;&#36712;&#36947;&#35282;&#21160;&#37327;&#30340;&#37327;&#23376;&#24178;&#28041;&#65292;&#23454;&#29616;&#20004;&#20010;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#26080;&#20914;&#31361;&#32852;&#21512;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069; &#30340;&#30740;&#31350;&#19968;&#30452;&#21576;&#29616;&#23545;&#31216;&#30340;&#32852;&#21512;&#20915;&#31574;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#20010;&#23646;&#24615;&#26377;&#21161;&#20110;&#32500;&#25252;&#21644;&#20445;&#25252;&#24179;&#31561;&#65292;&#20294;&#23427;&#19981;&#33021;&#35299;&#20915;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;&#20262;&#29702;&#21644;&#20844;&#24179;&#31561;&#20840;&#29699;&#24615;&#25361;&#25112;&#22312;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#34987;&#35748;&#20026;&#26159;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#33539;&#20363;&#12290;&#22240;&#27492;&#65292;&#20915;&#31574;&#31995;&#32479;&#19981;&#20165;&#24517;&#39035;&#20445;&#25345;&#29616;&#26377;&#30340;&#24179;&#31561;&#65292;&#36824;&#24517;&#39035;&#35299;&#20915;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20174;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#25506;&#35752;&#20102;&#21033;&#29992;&#25658;&#24102;&#36712;&#36947;&#35282;&#21160;&#37327;&#30340;&#20809;&#23376;&#25110;&#32416;&#32544;&#20809;&#23376;&#30340;&#37327;&#23376;&#24178;&#28041;&#23454;&#29616;&#38750;&#23545;&#31216;&#38598;&#20307;&#20915;&#31574;&#12290;&#34429;&#28982;&#25104;&#21151;&#23454;&#29616;&#20102;&#19981;&#23545;&#31216;&#24615;&#65292;&#20294;&#25152;&#24471;&#30340;&#32852;&#21512;&#20915;&#31574;&#21487;&#20197;&#22312;&#20445;&#25345;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#35813;&#30740;&#31350;&#20026;&#35774;&#35745;&#37327;&#23376;&#20915;&#31574;&#21327;&#35758;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#28508;&#22312;&#30340;&#37327;&#23376;&#36890;&#35759;&#21644;&#21152;&#23494;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collective decision-making is crucial to information and communication systems. Decision conflicts among agents hinder the maximization of potential utilities of the entire system. Quantum processes can realize conflict-free joint decisions among two agents using the entanglement of photons or quantum interference of orbital angular momentum (OAM). However, previous studies have always presented symmetric resultant joint decisions. Although this property helps maintain and preserve equality, it cannot resolve disparities. Global challenges, such as ethics and equity, are recognized in the field of responsible artificial intelligence as responsible research and innovation paradigm. Thus, decision-making systems must not only preserve existing equality but also tackle disparities. This study theoretically and numerically investigates asymmetric collective decision-making using quantum interference of photons carrying OAM or entangled photons. Although asymmetry is successfully realized, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.02109</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;O-RAN&#30340;&#21327;&#21516;&#65306;&#38754;&#21521;&#22810;&#20010;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#24377;&#24615;&#34394;&#25311;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26368;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#21253;&#25324;&#32593;&#32476;&#26465;&#20214;&#30340;&#21160;&#24577;&#24615;&#12289;&#31995;&#32479;&#20013;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;/&#20219;&#21153;&#30340;&#24182;&#23384;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#19982;&#20854;&#20182;&#32593;&#32476;&#26381;&#21153;&#30340;&#24182;&#34892;&#25191;&#34892;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#27867;&#22411;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#26469;&#35299;&#20915;DMS-FL&#20013;&#30340;&#19977;&#20010;&#26410;&#25506;&#32034;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#37319;&#29992;&#25668;&#20687;&#38519;&#38449;&#21644;&#28145;&#24230;&#23398;&#20064;&#26469;&#28040;&#38500;&#40479;&#31867;&#30417;&#27979;&#20013;&#30340;&#29942;&#39048;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#22270;&#20687;&#22788;&#29702;&#30340;&#24037;&#20316;&#37327;&#21644;&#34394;&#35686;&#27604;&#20363;&#65292;&#25552;&#39640;&#20102;&#30417;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02097</link><description>&lt;p&gt;
&#37319;&#29992;&#25668;&#20687;&#38519;&#38449;&#22270;&#20687;&#21644;&#28145;&#24230;&#23398;&#20064;&#28040;&#38500;&#40479;&#31867;&#20998;&#31867;&#20013;&#30340;&#20154;&#20026;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Removing Human Bottlenecks in Bird Classification Using Camera Trap Images and Deep Learning. (arXiv:2305.02097v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#37319;&#29992;&#25668;&#20687;&#38519;&#38449;&#21644;&#28145;&#24230;&#23398;&#20064;&#26469;&#28040;&#38500;&#40479;&#31867;&#30417;&#27979;&#20013;&#30340;&#29942;&#39048;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#22270;&#20687;&#22788;&#29702;&#30340;&#24037;&#20316;&#37327;&#21644;&#34394;&#35686;&#27604;&#20363;&#65292;&#25552;&#39640;&#20102;&#30417;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40479;&#31867;&#26159;&#30417;&#27979;&#29983;&#29289;&#22810;&#26679;&#24615;&#21644;&#26646;&#24687;&#22320;&#20581;&#24247;&#30340;&#37325;&#35201;&#25351;&#26631;&#65307;&#23427;&#20204;&#22312;&#29983;&#24577;&#31995;&#32479;&#31649;&#29702;&#20013;&#20063;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#40479;&#31867;&#25968;&#37327;&#30340;&#19979;&#38477;&#21487;&#33021;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#26381;&#21153;&#30340;&#38477;&#20302;&#65292;&#21253;&#25324;&#31181;&#23376;&#20256;&#25773;&#12289;&#25480;&#31881;&#21644;&#23475;&#34411;&#25511;&#21046;&#12290;&#23545;&#20110;&#29983;&#24577;&#23398;&#23478;&#26469;&#35828;&#65292;&#20934;&#30830;&#21644;&#38271;&#26399;&#22320;&#30417;&#27979;&#40479;&#31867;&#24182;&#35782;&#21035;&#20851;&#27880;&#29289;&#31181;&#65292;&#21516;&#26102;&#34913;&#37327;&#20445;&#25252;&#24178;&#39044;&#30340;&#25104;&#21151;&#65292;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30417;&#27979;&#24037;&#20316;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#24182;&#19988;&#22312;&#38271;&#26102;&#38388;&#25345;&#32493;&#21644;&#22823;&#35268;&#27169;&#31354;&#38388;&#23610;&#24230;&#19978;&#31649;&#29702;&#36215;&#26469;&#24448;&#24448;&#24456;&#22256;&#38590;&#12290;&#25668;&#20687;&#38519;&#38449;&#12289;&#22768;&#23398;&#30417;&#27979;&#22120;&#21644;&#26080;&#20154;&#26426;&#31561;&#25216;&#26415;&#25552;&#20379;&#20102;&#38750;&#20405;&#20837;&#24615;&#30417;&#27979;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#25668;&#20687;&#38519;&#38449;&#36827;&#34892;&#30417;&#27979;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;a&#65289;&#25668;&#20687;&#26426;&#29983;&#25104;&#22823;&#37327;&#22270;&#20687;&#65292;&#20351;&#24471;&#22312;&#36866;&#26102;&#22788;&#29702;&#21644;&#20998;&#26512;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#65307;b&#65289;&#34394;&#35686;&#27604;&#20363;&#39640;&#65292;&#20351;&#22788;&#29702;&#21644;&#20998;&#26512;&#38590;&#20197;&#27719;&#25253;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Birds are important indicators for monitoring both biodiversity and habitat health; they also play a crucial role in ecosystem management. Decline in bird populations can result in reduced eco-system services, including seed dispersal, pollination and pest control. Accurate and long-term monitoring of birds to identify species of concern while measuring the success of conservation interventions is essential for ecologists. However, monitoring is time consuming, costly and often difficult to manage over long durations and at meaningfully large spatial scales. Technology such as camera traps, acoustic monitors and drones provide methods for non-invasive monitoring. There are two main problems with using camera traps for monitoring: a) cameras generate many images, making it difficult to process and analyse the data in a timely manner; and b) the high proportion of false positives hinders the processing and analysis for reporting. In this paper, we outline an approach for overcoming these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#36817;&#22330;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02092</link><description>&lt;p&gt;
&#27627;&#31859;&#27874;&#31227;&#21160;&#38647;&#36798;&#25104;&#20687;&#30340;&#39640;&#25928;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient CNN-based Super Resolution Algorithms for mmWave Mobile Radar Imaging. (arXiv:2305.02092v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#36817;&#22330;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#26469;&#24212;&#23545;&#26032;&#22411;&#36817;&#22330;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#30340;&#38656;&#27714;&#65292;&#21363;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#21040;&#30005;&#30913;&#22495;&#65292;&#20197;&#23454;&#29616;&#23545;&#30001;&#38647;&#36798;&#20449;&#21495;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36817;&#22330;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#26159;&#36890;&#36807;&#22312;&#31354;&#38388;&#20013;&#25195;&#25551;&#38647;&#36798;&#26469;&#21019;&#24314;&#21512;&#25104;&#23380;&#24452;&#26469;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#39640;&#20445;&#30495;&#24230;&#30340;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#12289;&#20302;&#25104;&#26412;&#35774;&#22791;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#31354;&#38388;&#20351;&#20854;&#22791;&#21463;&#20851;&#27880;&#12290;&#30001;&#20110;SAR&#25104;&#20687;&#38656;&#35201;&#22823;&#30340;&#23380;&#24452;&#23610;&#23544;&#25165;&#33021;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#65292;&#22240;&#27492;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#33258;&#30001;&#25163;&#25345;&#26234;&#33021;&#25163;&#26426;SAR&#25910;&#38598;&#30340;&#19981;&#35268;&#21017;SAR&#23380;&#24452;&#25968;&#25454;&#23454;&#29616;&#39640;&#25928;&#12289;&#39640;&#31934;&#24230;&#30340;SAR&#25104;&#20687;&#26159;&#26377;&#25361;&#25112;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#36817;&#22330;SAR&#25104;&#20687;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#21331;&#36234;&#34920;&#29616;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#20351;&#29992;&#31471;&#21040;&#31471;&#32593;&#32476;&#30452;&#25509;&#23558;&#20302;&#20998;&#36776;&#29575;SAR&#22270;&#20687;&#25918;&#22823;&#65292;&#32780;&#31532;&#20108;&#31181;&#31639;&#27861;&#21017;&#37319;&#29992;&#20122;&#20687;&#32032;&#21367;&#31215;&#23618;&#65292;&#22312;&#36229;&#20998;&#36776;&#29575;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#20445;&#30041;&#22270;&#20687;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce an innovative super resolution approach to emerging modes of near-field synthetic aperture radar (SAR) imaging. Recent research extends convolutional neural network (CNN) architectures from the optical to the electromagnetic domain to achieve super resolution on images generated from radar signaling. Specifically, near-field synthetic aperture radar (SAR) imaging, a method for generating high-resolution images by scanning a radar across space to create a synthetic aperture, is of interest due to its high-fidelity spatial sensing capability, low cost devices, and large application space. Since SAR imaging requires large aperture sizes to achieve high resolution, super-resolution algorithms are valuable for many applications. Freehand smartphone SAR, an emerging sensing modality, requires irregular SAR apertures in the near-field and computation on mobile devices. Achieving efficient high-resolution SAR images from irregularly sampled data collected by freehan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21830;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#27627;&#31859;&#27874;&#38647;&#36798;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29289;&#20307;&#35782;&#21035;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#20197;&#39640;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#23454;&#26102;&#24615;&#35782;&#21035;&#29289;&#20307;&#65292;&#19981;&#21463;&#22806;&#35266;&#30340;&#24433;&#21709;&#65292;&#24182;&#35299;&#20915;&#20102;&#36974;&#25377;&#25928;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02085</link><description>&lt;p&gt;
&#27627;&#31859;&#27874;&#38647;&#36798;&#29289;&#20307;&#35782;&#21035;&#30340;&#31995;&#32479;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study on Object Recognition Using Millimeter-wave Radar. (arXiv:2305.02085v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21830;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#27627;&#31859;&#27874;&#38647;&#36798;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29289;&#20307;&#35782;&#21035;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#20197;&#39640;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#23454;&#26102;&#24615;&#35782;&#21035;&#29289;&#20307;&#65292;&#19981;&#21463;&#22806;&#35266;&#30340;&#24433;&#21709;&#65292;&#24182;&#35299;&#20915;&#20102;&#36974;&#25377;&#25928;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27627;&#31859;&#27874;&#38647;&#36798;&#21487;&#20197;&#22312;&#20809;&#32447;&#21644;&#22825;&#27668;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#19979;&#36827;&#34892;&#24863;&#30693;&#65292;&#22240;&#27492;&#22312;&#26234;&#33021;&#29615;&#22659;&#20013;&#27627;&#31859;&#27874;&#38647;&#36798;&#33267;&#20851;&#37325;&#35201;&#12290;&#26234;&#33021;&#36710;&#36742;&#31995;&#32479;&#21644;&#24037;&#19994;&#32423;&#27627;&#31859;&#27874;&#38647;&#36798;&#24050;&#32463;&#38598;&#25104;&#20102;&#36825;&#26679;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#38754;&#21521;&#31038;&#21306;&#30446;&#30340;&#30340;&#26234;&#33021;&#29615;&#22659;&#24212;&#29992;&#31243;&#24207;&#24456;&#38590;&#33719;&#24471;&#24037;&#19994;&#32423;&#27627;&#31859;&#27874;&#38647;&#36798;&#65292;&#32780;&#21830;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#20855;&#26377;&#38656;&#35201;&#30740;&#31350;&#30340;&#28508;&#22312;&#25361;&#25112;&#65292;&#20363;&#22914;&#35782;&#21035;&#29289;&#20307;&#21644;&#27963;&#21160;&#12289;&#23454;&#26102;&#20154;&#21592;&#36319;&#36394;&#12289;&#23545;&#35937;&#23450;&#20301;&#31561;&#12290;&#22270;&#20687;&#21644;&#35270;&#39057;&#25968;&#25454;&#23545;&#20110;&#36825;&#26679;&#30340;&#24037;&#20316;&#26469;&#35828;&#24456;&#23481;&#26131;&#25910;&#38598;&#12289;&#29702;&#35299;&#21644;&#27880;&#37322;&#12290;&#22270;&#20687;&#21644;&#35270;&#39057;&#25968;&#25454;&#21463;&#20809;&#32447;&#21644;&#22825;&#27668;&#30340;&#24433;&#21709;&#65292;&#23481;&#26131;&#21463;&#36974;&#25377;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#28040;&#38500;&#20381;&#36182;&#24615;&#24182;&#30830;&#20445;&#38544;&#31169;&#65292;&#21830;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#24212;&#35813;&#24471;&#21040;&#27979;&#35797;&#12290;&#22312;&#25512;&#36827;&#20043;&#21069;&#65292;&#24517;&#39035;&#35299;&#20915;&#27627;&#31859;&#27874;&#38647;&#36798;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#32622;&#19979;&#30340;&#23454;&#29992;&#24615;&#21644;&#24615;&#33021;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;Texas Instruments' IWR6843&#27627;&#31859;&#27874;&#38647;&#36798;&#25910;&#38598;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#26500;&#24314;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#20307;&#35782;&#21035;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#26174;&#31034;&#27627;&#31859;&#27874;&#38647;&#36798;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29289;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#21487;&#20197;&#20197;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#23454;&#26102;&#24615;&#35782;&#21035;&#29289;&#20307;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#29289;&#20307;&#22806;&#35266;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#24182;&#19988;&#20855;&#26377;&#25239;&#36974;&#25377;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;&#22522;&#20110;&#27627;&#31859;&#27874;&#38647;&#36798;&#30340;&#29289;&#20307;&#35782;&#21035;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its light and weather-independent sensing, millimeter-wave (MMW) radar is essential in smart environments. Intelligent vehicle systems and industry-grade MMW radars have integrated such capabilities. Industry-grade MMW radars are expensive and hard to get for community-purpose smart environment applications. However, commercially available MMW radars have hidden underpinning challenges that need to be investigated for tasks like recognizing objects and activities, real-time person tracking, object localization, etc. Image and video data are straightforward to gather, understand, and annotate for such jobs. Image and video data are light and weather-dependent, susceptible to the occlusion effect, and present privacy problems. To eliminate dependence and ensure privacy, commercial MMW radars should be tested. MMW radar's practicality and performance in varied operating settings must be addressed before promoting it. To address the problems, we collected a dataset using Texas Instr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24314;&#27169;&#21517;&#31216;&#19982;&#35282;&#33394;&#30456;&#20851;&#32852;&#30340;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#25429;&#25417;&#20195;&#29702;&#20154;&#22312;&#19981;&#21516;&#35282;&#33394;&#20013;&#20351;&#29992;&#19981;&#21516;&#21517;&#31216;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.02077</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#35282;&#33394;&#20381;&#36182;&#30340;&#21517;&#31216;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
An Ontology Design Pattern for Role-Dependent Names. (arXiv:2305.02077v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24314;&#27169;&#21517;&#31216;&#19982;&#35282;&#33394;&#30456;&#20851;&#32852;&#30340;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#25429;&#25417;&#20195;&#29702;&#20154;&#22312;&#19981;&#21516;&#35282;&#33394;&#20013;&#20351;&#29992;&#19981;&#21516;&#21517;&#31216;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#29992;&#20110;&#23545;&#21517;&#31216;&#20316;&#20026;&#35282;&#33394;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25429;&#25417;&#20195;&#29702;&#20154;&#20351;&#29992;&#19982;&#19981;&#21516;&#35282;&#33394;&#30456;&#20851;&#32852;&#30340;&#19981;&#21516;&#21517;&#31216;&#25191;&#34892;&#19981;&#21516;&#35282;&#33394;&#30340;&#24773;&#20917;&#12290;&#20195;&#29702;&#20154;&#20351;&#29992;&#19981;&#21516;&#21517;&#31216;&#25191;&#34892;&#35282;&#33394;&#30340;&#31034;&#20363;&#26159;&#30456;&#24403;&#24191;&#27867;&#30340;&#65292;&#20363;&#22914;&#20197;&#19981;&#21516;&#31508;&#21517;&#20889;&#20316;&#30340;&#20316;&#32773;&#65292;&#25110;&#32773;&#20855;&#26377;&#19968;&#20010;&#20197;&#19978;&#22269;&#23478;&#20844;&#27665;&#36523;&#20221;&#30340;&#19981;&#21516;&#27861;&#23450;&#21517;&#31216;&#12290;&#25552;&#20986;&#30340;&#27169;&#24335;&#26159;&#26631;&#20934;&#20195;&#29702;&#20154;&#35282;&#33394;&#21644;&#26631;&#20934;&#21517;&#31216;&#27169;&#24335;&#23384;&#26681;&#30340;&#20462;&#25913;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an ontology design pattern for modeling Names as part of Roles, to capture scenarios where an Agent performs different Roles using different Names associated with the different Roles. Examples of an Agent performing a Role using different Names are rather ubiquitous, e.g., authors who write under different pseudonyms, or different legal names for citizens of more than one country. The proposed pattern is a modified merger of a standard Agent Role and a standard Name pattern stub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#29992;&#20110;&#36817;&#22330;&#19981;&#35268;&#21017;SAR&#36229;&#20998;&#36776;&#29575;&#30340;&#31639;&#27861;&#65292;&#20197;&#24212;&#23545;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#20013;&#36935;&#21040;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20026;&#23454;&#29616;&#36793;&#32536;&#21644;&#29289;&#32852;&#32593;(IoT)&#25216;&#26415;&#22880;&#23450;&#25216;&#26415;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.02074</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36817;&#22330;&#19981;&#35268;&#21017;SAR&#36229;&#20998;&#36776;&#29575;&#30340;&#35270;&#35273;Transformer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Vision Transformer Approach for Efficient Near-Field Irregular SAR Super-Resolution. (arXiv:2305.02074v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#29992;&#20110;&#36817;&#22330;&#19981;&#35268;&#21017;SAR&#36229;&#20998;&#36776;&#29575;&#30340;&#31639;&#27861;&#65292;&#20197;&#24212;&#23545;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#20013;&#36935;&#21040;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20026;&#23454;&#29616;&#36793;&#32536;&#21644;&#29289;&#32852;&#32593;(IoT)&#25216;&#26415;&#22880;&#23450;&#25216;&#26415;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#38024;&#23545;&#38750;&#35268;&#21017;&#25195;&#25551;&#20960;&#20309;&#30340;&#36817;&#22330;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#12290;&#38543;&#30528;&#31532;&#20116;&#20195;(5G)&#27627;&#31859;&#27874;(mmWave)&#35774;&#22791;&#21464;&#24471;&#36234;&#26469;&#36234;&#23454;&#24800;&#21644;&#21487;&#29992;&#65292;&#39640;&#20998;&#36776;&#29575;SAR&#25104;&#20687;&#23545;&#29992;&#25143;&#24212;&#29992;&#21644;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#21464;&#24471;&#21487;&#34892;&#12290;&#26032;&#20852;&#24212;&#29992;&#22914;&#25163;&#25345;&#25104;&#20687;&#12289;&#26080;&#20154;&#26426;&#25104;&#20687;&#21644;&#27773;&#36710;SAR&#38754;&#20020;&#30528;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#30340;&#20960;&#20010;&#29420;&#29305;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#24674;&#22797;SAR&#22270;&#20687;&#38656;&#35201;&#22312;&#25972;&#20010;&#25195;&#25551;&#26399;&#38388;&#20102;&#35299;&#38453;&#21015;&#20301;&#32622;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#22522;&#20110;&#30456;&#26426;&#30340;&#23450;&#20301;&#31995;&#32479;&#65292;&#33021;&#22815;&#36275;&#22815;&#22320;&#20272;&#35745;&#20301;&#32622;&#65292;&#20294;&#23454;&#29616;&#39640;&#25928;&#30340;&#24674;&#22797;&#31639;&#27861;&#26159;&#23454;&#29616;&#36793;&#32536;&#21644;&#29289;&#32852;&#32593;(IoT)&#25216;&#26415;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#38750;&#21512;&#20316;&#36817;&#22330;SAR&#37319;&#26679;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20294;&#26159;&#36825;&#20123;&#31639;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#35797;&#39564;&#24615;&#30340;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a novel super-resolution algorithm for near-field synthetic-aperture radar (SAR) under irregular scanning geometries. As fifth-generation (5G) millimeter-wave (mmWave) devices are becoming increasingly affordable and available, high-resolution SAR imaging is feasible for end-user applications and non-laboratory environments. Emerging applications such freehand imaging, wherein a handheld radar is scanned throughout space by a user, unmanned aerial vehicle (UAV) imaging, and automotive SAR face several unique challenges for high-resolution imaging. First, recovering a SAR image requires knowledge of the array positions throughout the scan. While recent work has introduced camera-based positioning systems capable of adequately estimating the position, recovering the algorithm efficiently is a requirement to enable edge and Internet of Things (IoT) technologies. Efficient algorithms for non-cooperative near-field SAR sampling have been explored in recent work, bu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#24863;&#30693;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#30340;&#25972;&#20307;&#31934;&#24230;&#24182;&#32531;&#35299;&#32593;&#32476;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.02061</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#24863;&#30693;&#30340;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Attention Based Feature Fusion For Multi-Agent Collaborative Perception. (arXiv:2305.02061v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02061
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#24863;&#30693;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#30340;&#25972;&#20307;&#31934;&#24230;&#24182;&#32531;&#35299;&#32593;&#32476;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#65292;&#21327;&#20316;&#24863;&#30693;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#12290;&#36890;&#36807;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20197;&#25552;&#39640;&#25972;&#20010;&#31995;&#32479;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#20811;&#26381;&#20010;&#20307;&#24863;&#30693;&#30340;&#23616;&#38480;&#24615;&#12290;&#21327;&#20316;&#24863;&#30693;&#20811;&#26381;&#20102;&#20010;&#20307;&#24863;&#30693;&#22120;&#20214;&#30340;&#23616;&#38480;&#24615;&#65292;&#20351;&#36830;&#25509;&#20195;&#29702;&#20154;&#33021;&#22815;&#24863;&#30693;&#36229;&#20986;&#20182;&#20204;&#35270;&#32447;&#21644;&#35270;&#37326;&#20043;&#22806;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#21327;&#20316;&#24863;&#30693;&#30340;&#21487;&#38752;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25968;&#25454;&#32858;&#21512;&#31574;&#30053;&#21644;&#36890;&#20449;&#24102;&#23485;&#65292;&#24517;&#39035;&#20811;&#26381;&#26377;&#38480;&#32593;&#32476;&#36164;&#28304;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#30340;&#31934;&#24230;&#21644;&#32531;&#35299;&#26377;&#38480;&#30340;&#32593;&#32476;&#36164;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#30340;&#20013;&#38388;&#21327;&#20316;&#24863;&#30693;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#34701;&#21512;&#22810;&#20010;&#36830;&#25509;&#20195;&#29702;&#20154;&#20043;&#38388;&#20132;&#25442;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#30340;&#25972;&#20307;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#26174;&#33879;&#22686;&#21152;&#32593;&#32476;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of intelligent transportation systems (ITS), collaborative perception has emerged as a promising approach to overcome the limitations of individual perception by enabling multiple agents to exchange information, thus enhancing their situational awareness. Collaborative perception overcomes the limitations of individual sensors, allowing connected agents to perceive environments beyond their line-of-sight and field of view. However, the reliability of collaborative perception heavily depends on the data aggregation strategy and communication bandwidth, which must overcome the challenges posed by limited network resources. To improve the precision of object detection and alleviate limited network resources, we propose an intermediate collaborative perception solution in the form of a graph attention network (GAT). The proposed approach develops an attention-based aggregation strategy to fuse intermediate representations exchanged among multiple connected agents. This approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#26426;&#30028;&#38754;&#65292;&#23558;&#26426;&#22120;&#20154;&#36741;&#21161;&#24247;&#22797;&#30340;&#25972;&#20010;&#36807;&#31243;&#35270;&#20026;&#21327;&#21516;&#36866;&#24212;&#25110;&#30456;&#20114;&#23398;&#20064;&#36807;&#31243;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#39640;&#25277;&#35937;&#23618;&#31995;&#32479;&#23398;&#20064;&#36895;&#29575;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#35813;&#27169;&#22411;&#35774;&#35745;&#20102;&#21327;&#20316;&#30340;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#38750;&#24179;&#31283;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02058</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#31995;&#32479;&#23454;&#29616;&#20154;&#26426;&#21327;&#21516;&#36866;&#24212;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Human Machine Co-adaption Interface via Cooperation Markov Decision Process System. (arXiv:2305.02058v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#26426;&#30028;&#38754;&#65292;&#23558;&#26426;&#22120;&#20154;&#36741;&#21161;&#24247;&#22797;&#30340;&#25972;&#20010;&#36807;&#31243;&#35270;&#20026;&#21327;&#21516;&#36866;&#24212;&#25110;&#30456;&#20114;&#23398;&#20064;&#36807;&#31243;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#39640;&#25277;&#35937;&#23618;&#31995;&#32479;&#23398;&#20064;&#36895;&#29575;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#35813;&#27169;&#22411;&#35774;&#35745;&#20102;&#21327;&#20316;&#30340;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#38750;&#24179;&#31283;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21327;&#21516;&#36866;&#24212;&#25216;&#26415;&#65292;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#20154;&#26426;&#30028;&#38754;&#65292;&#20174;&#29992;&#25143;&#65288;&#24739;&#32773;&#65289;&#21644;&#26426;&#22120;&#65288;&#26426;&#22120;&#20154;&#65289;&#21452;&#26041;&#30340;&#35282;&#24230;&#26469;&#25913;&#21892;&#24247;&#22797;&#34920;&#29616;&#12290;&#20256;&#32479;&#30740;&#31350;&#26356;&#20391;&#37325;&#20110;&#26426;&#22120;&#20154;&#21327;&#21161;&#65292;&#21363;&#36890;&#36807;&#25913;&#21892;&#25511;&#21046;&#31574;&#30053;&#26469;&#23454;&#29616;&#25353;&#38656;&#21327;&#21161;&#30340;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#23558;&#26426;&#22120;&#20154;&#36741;&#21161;&#24247;&#22797;&#30340;&#25972;&#20010;&#36807;&#31243;&#35270;&#20026;&#21327;&#21516;&#36866;&#24212;&#25110;&#30456;&#20114;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#24378;&#35843;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#30340;&#36866;&#24212;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21327;&#21516;MDPs&#65288;CaMDPs&#65289;&#30340;&#27169;&#22411;&#65292;&#20197;&#22522;&#20110;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#23398;&#20064;&#36895;&#29575;&#26469;&#37327;&#21270;&#31995;&#32479;&#30340;&#39640;&#25277;&#35937;&#23618;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#21327;&#20316;&#22320;&#35843;&#25972;&#31574;&#30053;&#36845;&#20195;&#26694;&#26550;&#20013;&#30340;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#26681;&#25454;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#21516;MDPs&#65292;&#20223;&#30495;&#30740;&#31350;&#34920;&#26126;&#38750;&#24179;&#31283;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#27492;&#31995;&#32479;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to develop a new human-machine interface to improve rehabilitation performance from the perspective of both the user (patient) and the machine (robot) by introducing the co-adaption techniques via model-based reinforcement learning. Previous studies focus more on robot assistance, i.e., to improve the control strategy so as to fulfill the objective of Assist-As-Needed. In this study, we treat the full process of robot-assisted rehabilitation as a co-adaptive or mutual learning process and emphasize the adaptation of the user to the machine. To this end, we proposed a Co-adaptive MDPs (CaMDPs) model to quantify the learning rates based on cooperative multi-agent reinforcement learning (MARL) in the high abstraction layer of the systems. We proposed several approaches to cooperatively adjust the Policy Improvement among the two agents in the framework of Policy Iteration. Based on the proposed co-adaptive MDPs, the simulation study indicates the non-stationary problem can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#31181;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#20197;&#22312;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#30340;&#21516;&#26102;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02054</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20869;&#23384;&#33410;&#32422;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning. (arXiv:2305.02054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#31181;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#20197;&#22312;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#30340;&#21516;&#26102;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#26102;&#24120;&#24120;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#65292;&#36951;&#24536;&#20808;&#21069;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22238;&#25918;&#35760;&#24518;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#23427;&#20250;&#23545;&#26087;&#21644;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#21435;&#20851;&#32852;&#21644;&#28151;&#27927;&#12290;&#20182;&#20204;&#22825;&#30495;&#22320;&#25353;&#29031;&#29366;&#24577;&#36807;&#28193;&#30340;&#39034;&#24207;&#23384;&#20648;&#29366;&#24577;&#36716;&#21464;&#65292;&#32780;&#19981;&#32771;&#34385;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Grow-When-Required&#65288;GWR&#65289;&#33258;&#32452;&#32455;&#32593;&#32476;&#30340;&#26032;&#22411;&#35748;&#30693;&#21551;&#21457;&#24335;&#22238;&#25918;&#20869;&#23384;&#26041;&#27861;&#65292;&#23427;&#31867;&#20284;&#20110;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#19990;&#30028;&#35748;&#30693;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#20010;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#23558;&#30456;&#20284;&#30340;&#26679;&#26412;&#21512;&#24182;&#20197;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#24182;&#22686;&#21152;&#26679;&#26412;&#20043;&#38388;&#30340;&#20004;&#20004;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#34920;&#26126;&#65292;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#20801;&#35768;&#26174;&#30528;&#20943;&#23569;&#20869;&#23384;&#65292;&#21482;&#20250;&#20135;&#29983;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning agents often suffer from catastrophic forgetting, forgetting previously found solutions in parts of the input space when training on new data. Replay Memories are a common solution to the problem, decorrelating and shuffling old and new training samples. They naively store state transitions as they come in, without regard for redundancy. We introduce a novel cognitive-inspired replay memory approach based on the Grow-When-Required (GWR) self-organizing network, which resembles a map-based mental model of the world. Our approach organizes stored transitions into a concise environment-model-like network of state-nodes and transition-edges, merging similar samples to reduce the memory size and increase pair-wise distance among samples, which increases the relevancy of each sample. Overall, our paper shows that map-based experience replay allows for significant memory reduction with only small performance decreases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;mmWave&#38647;&#36798;&#21644;CNN&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#26032;&#25216;&#26415;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#33740;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#38750;&#29702;&#24819;&#25104;&#20687;&#26465;&#20214;&#19979;&#25552;&#39640;&#38745;&#24577;&#25163;&#21183;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02039</link><description>&lt;p&gt;
&#22522;&#20110;&#26032;&#22411;&#26080;&#33740;&#35757;&#32451;&#25216;&#26415;&#65292;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#38745;&#24577;&#25163;&#21183;&#20998;&#31867;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improved Static Hand Gesture Classification on Deep Convolutional Neural Networks using Novel Sterile Training Technique. (arXiv:2305.02039v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;mmWave&#38647;&#36798;&#21644;CNN&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#26032;&#25216;&#26415;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#33740;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#38750;&#29702;&#24819;&#25104;&#20687;&#26465;&#20214;&#19979;&#25552;&#39640;&#38745;&#24577;&#25163;&#21183;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#21644;&#39057;&#29575;&#35843;&#21046;&#36830;&#32493;&#27874;&#65288;FMCW&#65289;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#38647;&#36798;&#30340;&#38745;&#24577;&#25163;&#21183;&#20998;&#31867;&#30340;&#26032;&#22411;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;mmWave&#38647;&#36798;&#65292;&#21487;&#20197;&#22312;&#38750;&#29702;&#24819;&#25104;&#20687;&#26465;&#20214;&#19979;&#33719;&#21462;&#31934;&#30830;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#22240;&#27492;&#22312;&#20154;&#26426;&#20132;&#20114;(HCI)&#12289;&#22686;&#24378;/&#34394;&#25311;&#29616;&#23454;(AR/VR)&#31561;&#22810;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#33740;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21435;&#38500;&#31163;&#32676;&#20540;&#21644;&#24322;&#24120;&#36755;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate novel data collection and training techniques towards improving classification accuracy of non-moving (static) hand gestures using a convolutional neural network (CNN) and frequency-modulated-continuous-wave (FMCW) millimeter-wave (mmWave) radars. Recently, non-contact hand pose and static gesture recognition have received considerable attention in many applications ranging from human-computer interaction (HCI), augmented/virtual reality (AR/VR), and even therapeutic range of motion for medical applications. While most current solutions rely on optical or depth cameras, these methods require ideal lighting and temperature conditions. mmWave radar devices have recently emerged as a promising alternative offering low-cost system-on-chip sensors whose output signals contain precise spatial information even in non-ideal imaging conditions. Additionally, deep convolutional neural networks have been employed extensively in image recognition by learning both feat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21387;&#32553;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#20197;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#30446;&#26631;&#35757;&#32451;&#25216;&#26415;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02031</link><description>&lt;p&gt;
&#31995;&#32479;&#30740;&#31350;&#22522;&#20110;&#20266;&#30446;&#26631;&#35757;&#32451;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training. (arXiv:2305.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21387;&#32553;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#20197;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#30446;&#26631;&#35757;&#32451;&#25216;&#26415;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#21387;&#32553;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#36825;&#23545;&#20110;&#26381;&#21153;&#25968;&#30334;&#19975;&#29992;&#25143;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#25216;&#26415;&#65292;&#20854;&#20013;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#27169;&#20223;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#20351;&#24471;&#21487;&#20197;&#20174;&#25945;&#24072;&#21521;&#23398;&#29983;&#20256;&#36882;&#30693;&#35782;&#12290;&#19982;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20248;&#21270;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#38500;&#20102;&#26377;&#26631;&#35760;&#25968;&#25454;&#22806;&#65292;&#36824;&#26377;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#39640;&#21387;&#32553;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29616;&#23454;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33976;&#39311;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33976;&#39311;&#30340;&#29305;&#27530;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#20266;&#30446;&#26631;&#35757;&#32451;&#65288;PTT&#65289;&#25216;&#26415;&#65292;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically, in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#22810;&#27874;&#27573;&#20449;&#21495;&#34701;&#21512;&#65292;&#20197;&#24212;&#23545;&#23454;&#38469;&#22330;&#26223;&#20013;&#30446;&#26631;&#27169;&#22411;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02017</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27874;&#27573;&#20449;&#21495;&#34701;&#21512;&#29992;&#20110; 3-D SAR &#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Multiband Signal Fusion for 3-D SAR Super-Resolution. (arXiv:2305.02017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#22810;&#27874;&#27573;&#20449;&#21495;&#34701;&#21512;&#65292;&#20197;&#24212;&#23545;&#23454;&#38469;&#22330;&#26223;&#20013;&#30446;&#26631;&#27169;&#22411;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;3-D SAR&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#23433;&#20840;&#21644;&#24037;&#19994;&#24212;&#29992;&#65292;&#38656;&#35201;&#23545;&#38544;&#34109;&#25110;&#36974;&#25377;&#23545;&#35937;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#12290;&#33021;&#22815;&#20998;&#36776;&#38169;&#32508;&#22797;&#26434;&#30340; 3-D &#30446;&#26631;&#23545;&#20110;&#36825;&#20123;&#24212;&#29992;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#30452;&#25509;&#21462;&#20915;&#20110;&#31995;&#32479;&#24102;&#23485;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24102;&#23485;&#31995;&#32479;&#38754;&#20020;&#20960;&#20010;&#31105;&#27490;&#22240;&#32032;&#65292;&#22240;&#27492;&#21478;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#22312;&#19981;&#21516;&#39057;&#27573;&#19978;&#25805;&#20316;&#22810;&#20010;&#38647;&#36798;&#24182;&#34701;&#21512;&#22810;&#27874;&#27573;&#20449;&#21495;&#12290;&#24403;&#21069;&#30340;&#22810;&#27874;&#27573;&#20449;&#21495;&#34701;&#21512;&#26041;&#27861;&#20551;&#35774;&#19968;&#20010;&#31616;&#21333;&#30340;&#30446;&#26631;&#27169;&#22411;&#21644;&#23569;&#25968;&#28857;&#21453;&#23556;&#22120;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23454;&#38469;&#30340;&#23433;&#20840;&#31579;&#26597;&#21644;&#24037;&#19994;&#25104;&#20687;&#22330;&#26223;&#20013;&#30340;&#22823;&#37327;&#21453;&#23556;&#22120;&#26500;&#25104;&#30340;&#30446;&#26631;&#27169;&#22411;&#26080;&#25928;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#22810;&#27874;&#27573;&#20449;&#21495;&#34701;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#21517;&#20026; kR-Net &#30340;&#32593;&#32476;&#37319;&#29992;&#28151;&#21512;&#30340;&#21452;&#37325;&#22495;&#22797;&#20540;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three-dimensional (3-D) synthetic aperture radar (SAR) is widely used in many security and industrial applications requiring high-resolution imaging of concealed or occluded objects. The ability to resolve intricate 3-D targets is essential to the performance of such applications and depends directly on system bandwidth. However, because high-bandwidth systems face several prohibitive hurdles, an alternative solution is to operate multiple radars at distinct frequency bands and fuse the multiband signals. Current multiband signal fusion methods assume a simple target model and a small number of point reflectors, which is invalid for realistic security screening and industrial imaging scenarios wherein the target model effectively consists of a large number of reflectors. To the best of our knowledge, this study presents the first use of deep learning for multiband signal fusion. The proposed network, called kR-Net, employs a hybrid, dual-domain complex-valued convolutional neural netwo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02012</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#35780;&#36848;&#65306;SHAP &#21644; LIME
&lt;/p&gt;
&lt;p&gt;
Commentary on explainable artificial intelligence methods: SHAP and LIME. (arXiv:2305.02012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#20986;&#26469;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#36716;&#21270;&#20026;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#20256;&#36798;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#26088;&#22312;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36879;&#26126;&#65292;&#24182;&#22686;&#21152;&#26368;&#32456;&#29992;&#25143;&#23545;&#20854;&#36755;&#20986;&#30340;&#20449;&#20219;&#12290; SHapley Additive exPlanations&#65288;SHAP&#65289;&#21644;Local Interpretable Model Agnostic Explanation&#65288;LIME&#65289;&#26159;&#20004;&#31181;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;XAI&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35780;&#35770;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#26159;&#22914;&#20309;&#29983;&#25104;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#23427;&#20204;&#36755;&#20986;&#30340;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24403;&#21069;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;3D nnU-Net&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;CARDINAL&#65292;&#26469;&#35777;&#26126;&#20854;&#22312;&#24212;&#29992;&#20110;&#20020;&#24202;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01997</link><description>&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#20307;&#31215;&#25351;&#25968;&#30340;&#25552;&#21462;&#65306;&#21738;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#21487;&#20197;&#24212;&#29992;&#20110;&#20020;&#24202;&#65311;
&lt;/p&gt;
&lt;p&gt;
Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?. (arXiv:2305.01997v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24403;&#21069;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;3D nnU-Net&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;CARDINAL&#65292;&#26469;&#35777;&#26126;&#20854;&#22312;&#24212;&#29992;&#20110;&#20020;&#24202;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#33258;&#21160;&#20998;&#26512;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#21033;&#29992;&#22810;&#20010;&#30001;&#19987;&#23478;&#27880;&#37322;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65288;&#20854;&#20013;CAMUS&#26159;&#26368;&#22823;&#30340;&#20844;&#20849;&#25968;&#25454;&#24211;&#20043;&#19968;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#39044;&#27979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#25512;&#24191;&#33021;&#21147;&#31561;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#34987;&#20020;&#24202;&#21307;&#29983;&#35748;&#20026;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#24403;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CARDINAL&#30340;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;&#20854;&#21253;&#25324;&#24515;&#23574;&#20004;&#33108;&#21644;&#24515;&#23574;&#22235;&#33108;&#24207;&#21015;&#65292;&#24182;&#20855;&#26377;&#23436;&#25972;&#24515;&#33039;&#21608;&#26399;&#30340;&#21442;&#32771;&#20998;&#21106;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;3D nnU-Net&#20248;&#20110;&#26367;&#20195;&#30340;2D&#21644;&#24490;&#29615;&#20998;&#21106;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#25253;&#21578;&#20102;&#22312;CARDINAL&#19978;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when test
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#23545;&#20110;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#65292;&#22914;&#26524;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#23558;&#20986;&#29616;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#33021;&#22815;&#25551;&#36848;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01939</link><description>&lt;p&gt;
&#35777;&#26126;AI&#27169;&#22411;&#20013;&#31232;&#30095;&#31526;&#21495;&#27010;&#24565;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models. (arXiv:2305.01939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01939
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#23545;&#20110;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#65292;&#22914;&#26524;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#23558;&#20986;&#29616;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#33021;&#22815;&#25551;&#36848;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#20013;&#20986;&#29616;&#31526;&#21495;&#27010;&#24565;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#65288;1&#65289;&#27169;&#22411;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#21464;&#37327;&#30340;&#39640;&#38454;&#23548;&#25968;&#22343;&#20026;&#38646;&#65292;&#65288;2&#65289;AI&#27169;&#22411;&#21487;&#29992;&#20110;&#36974;&#25377;&#26679;&#26412;&#19988;&#36755;&#20837;&#26679;&#26412;&#36739;&#23569;&#36974;&#25377;&#26102;&#20250;&#20135;&#29983;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#65292;&#65288;3&#65289;AI&#27169;&#22411;&#22312;&#36974;&#25377;&#26679;&#26412;&#19978;&#30340;&#32622;&#20449;&#24230;&#24182;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#65292;&#21017;AI&#27169;&#22411;&#23558;&#32534;&#30721;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#12290;&#27599;&#20010;&#20132;&#20114;&#27010;&#24565;&#34920;&#31034;&#29305;&#23450;&#19968;&#32452;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#19968;&#23450;&#30340;&#25968;&#20540;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#20998;&#25968;&#24635;&#26159;&#21487;&#20197;&#34920;&#31034;&#20026;&#25152;&#26377;&#20132;&#20114;&#27010;&#24565;&#30340;&#20132;&#20114;&#25928;&#24212;&#20043;&#21644;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#24076;&#26395;&#35777;&#26126;&#20986;&#29616;&#31526;&#21495;&#27010;&#24565;&#30340;&#26465;&#20214;&#38750;&#24120;&#26222;&#36941;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#20110;&#22823;&#22810;&#25968;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#20132;&#20114;&#27010;&#24565;&#26469;&#27169;&#25311;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to prove the emergence of symbolic concepts in well-trained AI models. We prove that if (1) the high-order derivatives of the model output w.r.t. the input variables are all zero, (2) the AI model can be used on occluded samples and will yield higher confidence when the input sample is less occluded, and (3) the confidence of the AI model does not significantly degrade on occluded samples, then the AI model will encode sparse interactive concepts. Each interactive concept represents an interaction between a specific set of input variables, and has a certain numerical effect on the inference score of the model. Specifically, it is proved that the inference score of the model can always be represented as the sum of the interaction effects of all interactive concepts. In fact, we hope to prove that conditions for the emergence of symbolic concepts are quite common. It means that for most AI models, we can usually use a small number of interactive concepts to mimic the mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01938</link><description>&lt;p&gt;
Doc2SoarGraph&#65306;&#22522;&#20110;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#30340;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26723;&#30340;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20004;&#24180;&#26469;&#65292;&#23545;&#20110;&#34920;&#26684;&#25991;&#26412;&#25991;&#26723;&#65288;&#20363;&#22914;&#36130;&#21153;&#25253;&#21578;&#65289;&#30340;&#31163;&#25955;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22823;&#22810;&#36890;&#36807;&#25163;&#21160;&#36873;&#25321;&#21644;&#36716;&#25442;&#25991;&#26723;&#39029;&#38754;&#21040;&#32467;&#26500;&#21270;&#30340;&#34920;&#26684;&#21644;&#27573;&#33853;&#26469;&#31616;&#21270;&#36825;&#19968;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#19968;&#31181;&#26356;&#20026;&#29616;&#23454;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#20197; TAT-DQA &#30340;&#24418;&#24335;&#22238;&#31572;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Doc2SoarGraph &#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#20854;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545; TAT-DQA &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#31934;&#30830;&#21305;&#37197;&#65288;EM&#65289;&#21644; F1 &#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#27604;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;&#20102; 17.73% &#21644; 16.91%&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete reasoning over table-text documents (e.g., financial reports) gains increasing attention in recent two years. Existing works mostly simplify this challenge by manually selecting and transforming document pages to structured tables and paragraphs, hindering their practical application. In this work, we explore a more realistic problem setting in the form of TAT-DQA, i.e. to answer the question over a visually-rich table-text document. Specifically, we propose a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements (e.g., quantities, dates) of the given question and document with Semantic-oriented hierarchical Graph structures. We conduct extensive experiments on TAT-DQA dataset, and the results show that our proposed framework outperforms the best baseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score respectively on the test set, achieving the new state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2305.01918</link><description>&lt;p&gt;
&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#30340;&#31163;&#25955;&#24615;&#20351;&#24471;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#38590;&#20197;&#20445;&#35777;&#12290;&#34429;&#28982;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26631;&#31614;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26679;&#26412;&#23545;&#65292;&#20294;&#20173;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#21453;&#39304;&#26469;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;CLAIF&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;AI&#21453;&#39304;&#26500;&#24314;&#24102;&#26377;&#32454;&#31890;&#24230;&#26679;&#26412;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#26679;&#26412;&#23545;&#65292;&#20197;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#20154;&#24037;&#21453;&#39304;&#21644;AI&#21453;&#39304;&#20026;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings. However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings. Experimental results show that our method achieves stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MolKD&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21270;&#23398;&#21453;&#24212;&#19982;&#20998;&#23376;&#38388;&#30340;&#36328;&#27169;&#24577;&#30693;&#35782;&#25552;&#21462;&#21644;&#36716;&#31227;&#65292;&#20026;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#36741;&#21161;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01912</link><description>&lt;p&gt;
MolKD: &#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#25552;&#21462;&#21270;&#23398;&#21453;&#24212;&#20013;&#30340;&#36328;&#27169;&#24577;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
MolKD: Distilling Cross-Modal Knowledge in Chemical Reactions for Molecular Property Prediction. (arXiv:2305.01912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MolKD&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21270;&#23398;&#21453;&#24212;&#19982;&#20998;&#23376;&#38388;&#30340;&#36328;&#27169;&#24577;&#30693;&#35782;&#25552;&#21462;&#21644;&#36716;&#31227;&#65292;&#20026;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#36741;&#21161;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#26377;&#25928;&#22320;&#34920;&#31034;&#20998;&#23376;&#26159;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#21270;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#19982;&#21270;&#23398;&#21453;&#24212;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#32435;&#20837;&#21040;&#23398;&#20064;&#26377;&#25928;&#20998;&#23376;&#34920;&#31034;&#20013;&#12290; &#28982;&#32780;&#65292;&#21270;&#23398;&#21453;&#24212;&#21644;&#20998;&#23376;&#20043;&#38388;&#22266;&#26377;&#30340;&#36328;&#27169;&#24577;&#29305;&#24615;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;MolKD&#65292;&#23427;&#22312;&#21270;&#23398;&#21453;&#24212;&#20013;&#25552;&#21462;&#36328;&#27169;&#24577;&#30693;&#35782;&#65292;&#20197;&#36741;&#21161;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to effectively represent molecules is a long-standing challenge for molecular property prediction and drug discovery. This paper studies this problem and proposes to incorporate chemical domain knowledge, specifically related to chemical reactions, for learning effective molecular representations. However, the inherent cross-modality property between chemical reactions and molecules presents a significant challenge to address. To this end, we introduce a novel method, namely MolKD, which Distills cross-modal Knowledge in chemical reactions to assist Molecular property prediction. Specifically, the reaction-to-molecule distillation model within MolKD transfers cross-modal knowledge from a pre-trained teacher network learning with one modality (i.e., reactions) into a student network learning with another modality (i.e., molecules). Moreover, MolKD learns effective molecular representations by incorporating reaction yields to measure transformation efficiency of the reactant-product 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#20351;&#29992;&#38544;&#21464;&#37327;&#23545;&#23545;&#35937;&#25513;&#33180;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#21516;&#26102;&#36890;&#36807;&#32622;&#20449;&#24230;&#25513;&#33180;&#26041;&#27861;&#25552;&#39640;&#39640;&#31934;&#24230;&#25342;&#21462;&#26426;&#22120;&#20154;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.01910</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23454;&#20363;&#20998;&#21106;&#65306;&#29992;&#38544;&#21464;&#37327;&#25511;&#21046;&#19981;&#30830;&#23450;&#24615;&#21644;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#30340;Latent-MaskRCNN&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Distributional Instance Segmentation: Modeling Uncertainty and High Confidence Predictions with Latent-MaskRCNN. (arXiv:2305.01910v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#20351;&#29992;&#38544;&#21464;&#37327;&#23545;&#23545;&#35937;&#25513;&#33180;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#21516;&#26102;&#36890;&#36807;&#32622;&#20449;&#24230;&#25513;&#33180;&#26041;&#27861;&#25552;&#39640;&#39640;&#31934;&#24230;&#25342;&#21462;&#26426;&#22120;&#20154;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#35782;&#21035;&#21644;&#23454;&#20363;&#20998;&#21106;&#26159;&#20219;&#20309;&#26426;&#22120;&#20154;&#25110;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#25216;&#33021;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#27169;&#31946;&#30340;&#22330;&#26223;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#22312;&#39640;&#24615;&#33021;&#24212;&#29992;&#31243;&#24207;&#20013;&#23548;&#33268;&#20851;&#38190;&#38169;&#35823;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31867;&#20351;&#29992;&#38544;&#21464;&#37327;&#30340;&#20998;&#24067;&#24335;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#21487;&#20197;&#23545;&#23545;&#35937;&#25513;&#27169;&#30340;&#21487;&#34892;&#20551;&#35774;&#24314;&#27169;&#65292;&#20174;&#32780;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#25342;&#21462;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#32622;&#20449;&#24230;&#25513;&#27169;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24037;&#19994;&#24212;&#29992;&#26696;&#20363;&#25152;&#38656;&#30340;&#39640;&#31934;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#38169;&#35823;&#65292;&#21253;&#25324;&#25105;&#20204;&#26032;&#21457;&#24067;&#30340;&#27169;&#31946;&#22330;&#26223;&#25968;&#25454;&#38598;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26381;&#35013;&#25342;&#21462;&#26426;&#22120;&#20154;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#21452;&#37325;&#25342;&#21462;&#38169;&#35823;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object recognition and instance segmentation are fundamental skills in any robotic or autonomous system. Existing state-of-the-art methods are often unable to capture meaningful uncertainty in challenging or ambiguous scenes, and as such can cause critical errors in high-performance applications. In this paper, we explore a class of distributional instance segmentation models using latent codes that can model uncertainty over plausible hypotheses of object masks. For robotic picking applications, we propose a confidence mask method to achieve the high precision necessary in industrial use cases. We show that our method can significantly reduce critical errors in robotic systems, including our newly released dataset of ambiguous scenes in a robotic application. On a real-world apparel-picking robot, our method significantly reduces double pick errors while maintaining high performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#20986;&#21457;&#65292;&#20998;&#26512;&#27604;&#36739;&#20102;&#21313;&#31181;&#20195;&#34920;&#24615;&#30340;&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24402;&#32435;&#24635;&#32467;&#20986;&#20102;&#21407;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01901</link><description>&lt;p&gt;
&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#65306;&#32463;&#39564;&#30740;&#31350;&#21644;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Few-shot Event Detection: An Empirical Study and a Unified View. (arXiv:2305.01901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#20986;&#21457;&#65292;&#20998;&#26512;&#27604;&#36739;&#20102;&#21313;&#31181;&#20195;&#34920;&#24615;&#30340;&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24402;&#32435;&#24635;&#32467;&#20986;&#20102;&#21407;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979; (ED) &#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20063;&#24102;&#26469;&#20102;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#20363;&#22914;&#21508;&#31181;&#21160;&#26426;&#12289;&#20219;&#21153;&#21644;&#23454;&#39564;&#35774;&#32622;&#65292;&#36825;&#20123;&#24046;&#24322;&#22952;&#30861;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#26410;&#26469;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#24443;&#24213;&#30340;&#32463;&#39564;&#30740;&#31350;&#12289;&#19968;&#20010;ED&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#21644;&#19968;&#20010;&#26356;&#22909;&#30340;&#32479;&#19968;&#22522;&#20934;&#32447;&#12290;&#20026;&#20102;&#20844;&#24179;&#35780;&#20272;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#65306;&#20302;&#36164;&#28304;&#35774;&#32622;&#26469;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#36716;&#31227;&#35774;&#32622;&#26469;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21313;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#33268;&#34987;&#20998;&#20026;&#22522;&#20110;&#25552;&#31034;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#20026;&#20102;&#35843;&#26597;&#22522;&#20110;&#21407;&#22411;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#20998;&#35299;&#20102;&#35774;&#35745;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#19981;&#20165;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#33719;&#24471;2.7&#65285;F1&#25910;&#30410;&#65289;&#65292;&#32780;&#19988;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we choose two practical settings: low-resource setting to assess generalization ability and class-transfer setting for transferability. We compare ten representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis. To investigate the superior performance of prototype-based methods, we break down the design and build a unified framework. Based on that, we not only propose a simple yet effective method (e.g., 2.7% F1 gains under low-resource setting) but also offer many valuable research insights for future research.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20892;&#19994;&#12289;&#30044;&#29287;&#19994;&#21644;&#28180;&#19994;&#31561;&#39046;&#22495;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#39135;&#21697;&#20998;&#31867;&#12289;&#29983;&#38271;&#30417;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#21644;&#21697;&#36136;&#35780;&#20272;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#20197;&#21450;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.01899</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#65306;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Agrifood Systems with Artificial Intelligence: A Survey. (arXiv:2305.01899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20892;&#19994;&#12289;&#30044;&#29287;&#19994;&#21644;&#28180;&#19994;&#31561;&#39046;&#22495;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#39135;&#21697;&#20998;&#31867;&#12289;&#29983;&#38271;&#30417;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#21644;&#21697;&#36136;&#35780;&#20272;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#20197;&#21450;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#20154;&#21475;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#36716;&#21464;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#65292;&#20351;&#20854;&#26356;&#20855;&#29983;&#20135;&#21147;&#12289;&#25928;&#29575;&#12289;&#23433;&#20840;&#21644;&#21487;&#25345;&#32493;&#24615;&#65292;&#26159;&#32531;&#35299;&#28508;&#22312;&#31918;&#39135;&#30701;&#32570;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35821;&#35328;&#12289;&#35270;&#35273;&#12289;&#36965;&#24863;&#21644;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#24212;&#29992;&#31561;&#21508;&#20010;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#30340;&#25972;&#20307;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22914;&#20309;&#25913;&#21464;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#65292;&#24182;&#20026;&#29616;&#20195;&#20892;&#19994;&#39135;&#21697;&#34892;&#19994;&#20570;&#20986;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#33719;&#21462;&#26041;&#27861;&#65292;&#21253;&#25324;&#33719;&#21462;&#12289;&#23384;&#20648;&#21644;&#22788;&#29702;&#25216;&#26415;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#12289;&#30044;&#29287;&#19994;&#21644;&#28180;&#19994;&#31561;&#39046;&#22495;&#20013;&#30340;&#36827;&#23637;&#24773;&#20917;&#65292;&#28085;&#30422;&#20102;&#20892;&#19994;&#39135;&#21697;&#20998;&#31867;&#12289;&#29983;&#38271;&#30417;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#21644;&#21697;&#36136;&#35780;&#20272;&#31561;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20013;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the world population rapidly increasing, transforming our agrifood systems to be more productive, efficient, safe, and sustainable is crucial to mitigate potential food shortages. Recently, artificial intelligence (AI) techniques such as deep learning (DL) have demonstrated their strong abilities in various areas, including language, vision, remote sensing (RS), and agrifood systems applications. However, the overall impact of AI on agrifood systems remains unclear. In this paper, we thoroughly review how AI techniques can transform agrifood systems and contribute to the modern agrifood industry. Firstly, we summarize the data acquisition methods in agrifood systems, including acquisition, storage, and processing techniques. Secondly, we present a progress review of AI methods in agrifood systems, specifically in agriculture, animal husbandry, and fishery, covering topics such as agrifood classification, growth monitoring, yield prediction, and quality assessment. Furthermore, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36710;&#36742;&#23433;&#20840;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#65306;VSRQ&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;I-FAHP&#21644;FCA&#32858;&#31867;&#65292;&#25366;&#25496;&#36710;&#36742;&#26234;&#33021;&#32852;&#25509;&#31995;&#32479;&#30340;&#26131;&#21463;&#25915;&#20987;&#32452;&#20214;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20248;&#20808;&#27979;&#35797;&#65292;&#20197;&#38477;&#20302;&#39118;&#38505;&#24182;&#30830;&#20445;&#36710;&#36742;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2305.01898</link><description>&lt;p&gt;
VSRQ: &#36710;&#32852;&#32593;&#31995;&#32479;&#23433;&#20840;&#39118;&#38505;&#30340;&#37327;&#21270;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VSRQ: Quantitative Assessment Method for Safety Risk of Vehicle Intelligent Connected System. (arXiv:2305.01898v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36710;&#36742;&#23433;&#20840;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#65306;VSRQ&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;I-FAHP&#21644;FCA&#32858;&#31867;&#65292;&#25366;&#25496;&#36710;&#36742;&#26234;&#33021;&#32852;&#25509;&#31995;&#32479;&#30340;&#26131;&#21463;&#25915;&#20987;&#32452;&#20214;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20248;&#20808;&#27979;&#35797;&#65292;&#20197;&#38477;&#20302;&#39118;&#38505;&#24182;&#30830;&#20445;&#36710;&#36742;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36710;&#32852;&#32593;&#22312;&#29616;&#20195;&#27773;&#36710;&#20013;&#30340;&#24212;&#29992;&#19981;&#26029;&#25193;&#22823;&#65292;&#36710;&#36742;&#21151;&#33021;&#20063;&#38543;&#30528;&#26102;&#20195;&#30340;&#21457;&#23637;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#36825;&#20063;&#23548;&#33268;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#36710;&#36742;&#28431;&#27934;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#29305;&#21035;&#37325;&#35201;&#30340;&#26159;&#35201;&#35782;&#21035;&#39640;&#39118;&#38505;&#30340;&#36710;&#32852;&#32593;&#31995;&#32479;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21578;&#35785;&#23433;&#20840;&#20154;&#21592;&#21738;&#20010;&#31995;&#32479;&#26368;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#35753;&#20182;&#20204;&#36827;&#34892;&#26356;&#24443;&#24213;&#30340;&#26816;&#26597;&#21644;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;I-FAHP&#21644;FCA&#32858;&#31867;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#36710;&#36742;&#23433;&#20840;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#65306;VSRQ&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#21462;&#19982;&#36710;&#36742;&#23433;&#20840;&#30456;&#20851;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#20351;&#29992;&#27169;&#31946;&#32858;&#31867;&#20998;&#26512;&#65288;FCA&#65289;&#21644;&#27169;&#31946;&#23618;&#27425;&#20998;&#26512;&#36807;&#31243;&#65288;FAHP&#65289;&#25366;&#25496;&#36710;&#36742;&#26234;&#33021;&#32852;&#25509;&#31995;&#32479;&#30340;&#26131;&#21463;&#25915;&#20987;&#32452;&#20214;&#65292;&#24182;&#23545;&#26131;&#21463;&#25915;&#20987;&#30340;&#32452;&#20214;&#36827;&#34892;&#20248;&#20808;&#27979;&#35797;&#65292;&#20197;&#38477;&#20302;&#39118;&#38505;&#24182;&#30830;&#20445;&#36710;&#36742;&#23433;&#20840;&#12290;&#25105;&#20204;&#22312;OpenPilot&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of intelligent connected in modern vehicles continues to expand, and the functions of vehicles become more and more complex with the development of the times. This has also led to an increasing number of vehicle vulnerabilities and many safety issues. Therefore, it is particularly important to identify high-risk vehicle intelligent connected systems, because it can inform security personnel which systems are most vulnerable to attacks, allowing them to conduct more thorough inspections and tests. In this paper, we develop a new model for vehicle risk assessment by combining I-FAHP with FCA clustering: VSRQ model. We extract important indicators related to vehicle safety, use fuzzy cluster analys (FCA) combined with fuzzy analytic hierarchy process (FAHP) to mine the vulnerable components of the vehicle intelligent connected system, and conduct priority testing on vulnerable components to reduce risks and ensure vehicle safety. We evaluate the model on OpenPilot and experiment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35789;&#65292;&#21487;&#20197;&#23545;&#25152;&#36873;&#20195;&#30721;&#36827;&#34892;&#31934;&#31616;&#12289;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.01863</link><description>&lt;p&gt;
GPTutor: &#19968;&#31181;&#30001;ChatGPT&#39537;&#21160;&#30340;&#32534;&#31243;&#24037;&#20855;&#65292;&#29992;&#20110;&#31243;&#24207;&#20195;&#30721;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
GPTutor: a ChatGPT-powered programming tool for code explanation. (arXiv:2305.01863v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35789;&#65292;&#21487;&#20197;&#23545;&#25152;&#36873;&#20195;&#30721;&#36827;&#34892;&#31934;&#31616;&#12289;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26032;&#30340;&#32534;&#31243;&#25216;&#33021;&#38656;&#35201;&#20010;&#24615;&#21270;&#25351;&#23548;&#12290;&#38543;&#30528;ChatGPT API&#31561;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29616;&#22312;&#26377;&#21487;&#33021;&#21019;&#24314;&#19968;&#20010;&#26041;&#20415;&#30340;&#12289;&#20010;&#24615;&#21270;&#30340;AI&#32534;&#31243;&#25945;&#32946;&#36741;&#23548;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#29992;&#20110;&#25552;&#20379;&#32534;&#31243;&#20195;&#30721;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning new programming skills requires tailored guidance. With the emergence of advanced Natural Language Generation models like the ChatGPT API, there is now a possibility of creating a convenient and personalized tutoring system with AI for computer science education. This paper presents GPTutor, a ChatGPT-powered programming tool, which is a Visual Studio Code extension using the ChatGPT API to provide programming code explanations. By integrating Visual Studio Code API, GPTutor can comprehensively analyze the provided code by referencing the relevant source codes. As a result, GPTutor can use designed prompts to explain the selected code with a pop-up message. GPTutor is now published at the Visual Studio Code Extension Marketplace, and its source code is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor delivers the most concise and accurate explanations compared to vanilla ChatGPT and GitHub Copilot. Moreover, the feedback from students and teachers ind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23454;&#20363;&#20998;&#21106;&#30340;&#31283;&#20581;&#32447;&#25968;&#25454;&#25552;&#21462;&#26041;&#27861;LineFormer&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#32447;&#22270;&#20013;&#35270;&#35273;&#21644;&#32467;&#26500;&#21464;&#21270;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20854;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01837</link><description>&lt;p&gt;
LineFormer&#65306;&#23558;&#32447;&#22270;&#25968;&#25454;&#25552;&#21462;&#37325;&#26032;&#24605;&#32771;&#20026;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LineFormer: Rethinking Line Chart Data Extraction as Instance Segmentation. (arXiv:2305.01837v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23454;&#20363;&#20998;&#21106;&#30340;&#31283;&#20581;&#32447;&#25968;&#25454;&#25552;&#21462;&#26041;&#27861;LineFormer&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#32447;&#22270;&#20013;&#35270;&#35273;&#21644;&#32467;&#26500;&#21464;&#21270;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20854;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32447;&#22270;&#22270;&#20687;&#20013;&#25552;&#21462;&#25968;&#25454;&#26159;&#33258;&#21160;&#25991;&#26723;&#29702;&#35299;&#36807;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#20026;&#32447;&#22270;&#26159;&#19968;&#31181;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#26684;&#24335;&#12290;&#28982;&#32780;&#65292;&#22810;&#32447;&#22270;&#20013;&#30340;&#35270;&#35273;&#21644;&#32467;&#26500;&#21464;&#21270;&#37327;&#20351;&#23427;&#20204;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23545;&#20110;&#25152;&#26377;&#36825;&#20123;&#21464;&#21270;&#37117;&#19981;&#26159;&#38750;&#24120;&#20581;&#22766;&#65292;&#25110;&#32773;&#37319;&#21462;&#20102;&#19968;&#20010;&#20840;&#37096;&#22270;&#34920;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#35832;&#22914;&#22270;&#20363;&#20043;&#31867;&#30340;&#36741;&#21161;&#20449;&#24687;&#26469;&#25552;&#21462;&#32447;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23454;&#20363;&#20998;&#21106;&#30340;&#31283;&#20581;&#32447;&#25968;&#25454;&#25552;&#21462;&#26041;&#27861;LineFormer&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#34920;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21487;&#22312;https://github.com/TheJaeLal/LineFormer&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data extraction from line-chart images is an essential component of the automated document understanding process, as line charts are a ubiquitous data visualization format. However, the amount of visual and structural variations in multi-line graphs makes them particularly challenging for automated parsing. Existing works, however, are not robust to all these variations, either taking an all-chart unified approach or relying on auxiliary information such as legends for line data extraction. In this work, we propose LineFormer, a robust approach to line data extraction using instance segmentation. We achieve state-of-the-art performance on several benchmark synthetic and real chart datasets. Our implementation is available at https://github.com/TheJaeLal/LineFormer .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21160;&#24577;&#31995;&#32479;&#36335;&#24452;&#35268;&#21010;&#22120;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31639;&#27861;&#65292;&#20197;&#20811;&#26381;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#30340;&#31435;&#21363;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23637;&#31034;&#20854;&#22312;&#26377;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#33258;&#20027;&#25628;&#32034;&#21644;&#35206;&#30422;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.01834</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#31995;&#32479;&#36335;&#24452;&#35268;&#21010;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23454;&#26102;&#29615;&#22659;&#33258;&#20027;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Autonomous search of real-life environments combining dynamical system-based path planning and unsupervised learning. (arXiv:2305.01834v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21160;&#24577;&#31995;&#32479;&#36335;&#24452;&#35268;&#21010;&#22120;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31639;&#27861;&#65292;&#20197;&#20811;&#26381;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#30340;&#31435;&#21363;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23637;&#31034;&#20854;&#22312;&#26377;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#33258;&#20027;&#25628;&#32034;&#21644;&#35206;&#30422;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20351;&#29992;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#36827;&#34892;&#26377;&#38480;&#29615;&#22659;&#25628;&#32034;&#21644;&#36941;&#21382;&#30340;&#36827;&#23637;&#65292;&#20294;&#35813;&#39046;&#22495;&#30340;&#29616;&#29366;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#30446;&#21069;&#30340;&#23454;&#39564;&#24037;&#20316;&#23578;&#26410;&#24320;&#21457;&#20986;&#21487;&#28385;&#36275;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#38656;&#35201;&#20811;&#26381;&#30340;&#31435;&#21363;&#38382;&#39064;&#30340;&#24378;&#22823;&#26041;&#27861; &#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21160;&#24577;&#31995;&#32479;&#36335;&#24452;&#35268;&#21010;&#22120;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31639;&#27861;&#65292;&#20197;&#20811;&#26381;&#28151;&#27788;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#22120;&#30340;&#31435;&#21363;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#23637;&#31034;&#20854;&#22312;&#26377;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#33258;&#20027;&#25628;&#32034;&#21644;&#35206;&#30422;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advancements have been made towards the goal of using chaotic coverage path planners for autonomous search and traversal of spaces with limited environmental cues. However, the state of this field is still in its infancy as there has been little experimental work done. Current experimental work has not developed robust methods to satisfactorily address the immediate set of problems a chaotic coverage path planner needs to overcome in order to scan realistic environments within reasonable coverage times. These immediate problems are as follows: (1) an obstacle avoidance technique which generally maintains the kinematic efficiency of the robot's motion, (2) a means to spread chaotic trajectories across the environment (especially crucial for large and/or complex-shaped environments) that need to be covered, and (3) a real-time coverage calculation technique that is accurate and independent of cell size. This paper aims to progress the field by proposing algorithms that a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;KEPLET&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#20013;&#21152;&#20837;&#20027;&#39064;&#23454;&#20307;&#24863;&#30693;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#23454;&#20307;&#20132;&#20114;&#21644;&#35789;&#35821;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.01810</link><description>&lt;p&gt;
KEPLET: &#19968;&#31181;&#24102;&#26377;&#20027;&#39064;&#23454;&#20307;&#24863;&#30693;&#30340;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness. (arXiv:2305.01810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;KEPLET&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#20013;&#21152;&#20837;&#20027;&#39064;&#23454;&#20307;&#24863;&#30693;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#23454;&#20307;&#20132;&#20114;&#21644;&#35789;&#35821;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36890;&#36807;&#22312;&#26410;&#32467;&#26500;&#21270;&#25991;&#26412;&#35821;&#26009;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#26174;&#31034;&#20854;&#20248;&#36234;&#24615;&#12290;&#22312;&#23454;&#20307;&#20016;&#23500;&#30340;&#25991;&#26412;&#36164;&#28304;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#65292;&#30693;&#35782;&#22686;&#24378;&#30340;PLM&#65288;KEPLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#26631;&#35760;&#19982;&#25152;&#25552;&#21450;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#32467;&#21512;&#36215;&#26469;&#65292;&#22240;&#27492;&#22312;&#23454;&#20307;&#20013;&#24515;&#20219;&#21153;&#65288;&#22914;&#23454;&#20307;&#38142;&#25509;&#21644;&#20851;&#31995;&#20998;&#31867;&#65289;&#19978;&#26356;&#26377;&#25928;&#12290;&#34429;&#28982;&#20256;&#32479;KEPLM&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21033;&#29992;&#20102;&#32500;&#22522;&#30334;&#31185;&#30340;&#20016;&#23500;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#24573;&#30053;&#20102;&#27599;&#20010;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#22260;&#32469;&#19968;&#20010;&#20027;&#39064;&#23454;&#20307;&#30340;&#29420;&#29305;&#24067;&#23616;&#65288;&#30001;&#39029;&#38754;URL&#26631;&#35782;&#24182;&#22312;&#39029;&#38754;&#26631;&#39064;&#20013;&#26174;&#31034;&#65289;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#22914;&#26524;&#19981;&#21152;&#20837;&#20027;&#39064;&#23454;&#20307;&#65292;KEPLM&#23558;&#23548;&#33268;&#23454;&#20307;&#20132;&#20114;&#19981;&#36275;&#21644;&#20559;&#24046;&#65288;&#20851;&#31995;&#65289;&#35789;&#35821;&#35821;&#20041;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KEPLET&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#20027;&#39064;&#23454;&#20307;&#24863;&#30693;&#30340;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#65292;KEPLET&#30830;&#23450;&#39044;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#20027;&#39064;&#23454;&#20307;&#65292;&#24182;&#25913;&#36827;&#20102;KEPLM&#30340;&#23454;&#20307;&#20132;&#20114;&#21644;&#35789;&#35821;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Pre-trained Language Models (PLMs) have shown their superiority by pre-training on unstructured text corpus and then fine-tuning on downstream tasks. On entity-rich textual resources like Wikipedia, Knowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens and mentioned entities in pre-training, and are thus more effective on entity-centric tasks such as entity linking and relation classification. Although exploiting Wikipedia's rich structures to some extent, conventional KEPLMs still neglect a unique layout of the corpus where each Wikipedia page is around a topic entity (identified by the page URL and shown in the page title). In this paper, we demonstrate that KEPLMs without incorporating the topic entities will lead to insufficient entity interaction and biased (relation) word semantics. We thus propose KEPLET, a novel Knowledge-Enhanced Pre-trained LanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET identifies where to a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22810;&#20010;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#35780;&#20272;&#31574;&#30053;&#26469;&#34913;&#37327;&#20854;&#35760;&#24518;&#24615;&#33021;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#23376;&#32676;&#29305;&#23450;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#22312;IMDB&#21644;Yelp&#25968;&#25454;&#38598;&#19978;&#65292;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#30340;&#24046;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01801</link><description>&lt;p&gt;
&#24403;&#26032;&#30340;&#19981;&#19968;&#23450;&#26159;&#26356;&#22909;&#30340;&#65306;&#28145;&#24230;&#23398;&#20064;&#26159;&#21542;&#30495;&#27491;&#21463;&#30410;&#20110;&#22522;&#20110;&#38544;&#24335;&#21453;&#39304;&#30340;&#25512;&#33616;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Newer is Not Better: Does Deep Learning Really Benefit Recommendation From Implicit Feedback?. (arXiv:2305.01801v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22810;&#20010;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#35780;&#20272;&#31574;&#30053;&#26469;&#34913;&#37327;&#20854;&#35760;&#24518;&#24615;&#33021;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#23376;&#32676;&#29305;&#23450;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#22312;IMDB&#21644;Yelp&#25968;&#25454;&#38598;&#19978;&#65292;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#31070;&#32463;&#27169;&#22411;&#34987;&#22810;&#27425;&#23459;&#20256;&#20026;&#25512;&#33616;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#26159;&#22810;&#20010;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#24182;&#19981;&#33021;&#21487;&#38752;&#22320;&#22797;&#29616;&#12290;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#29616;&#26377;&#30340;&#35780;&#20272;&#26159;&#22312;&#19981;&#19968;&#33268;&#30340;&#21327;&#35758;&#19979;&#36827;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21487;&#37325;&#22797;&#24615;&#38382;&#39064;&#20351;&#20154;&#20204;&#38590;&#20197;&#20102;&#35299;&#23454;&#38469;&#19978;&#21487;&#20197;&#20174;&#36825;&#20123;&#31070;&#32463;&#27169;&#22411;&#20013;&#33719;&#24471;&#22810;&#23569;&#30410;&#22788;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#20844;&#24179;&#32780;&#20840;&#38754;&#30340;&#32489;&#25928;&#27604;&#36739;&#26469;&#27604;&#36739;&#20256;&#32479;&#27169;&#22411;&#21644;&#31070;&#32463;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#12289;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#38544;&#24335;&#25968;&#25454;&#30340;&#39030;&#37096;&#25512;&#33616;&#30340;&#26368;&#26032;&#31070;&#32463;&#25512;&#33616;&#27169;&#22411;&#21644;&#20256;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#35780;&#20272;&#31574;&#30053;&#65292;&#29992;&#20110;&#34913;&#37327;&#25512;&#33616;&#27169;&#22411;&#30340;&#35760;&#24518;&#24615;&#33021;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#23376;&#32676;&#29305;&#23450;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, neural models have been repeatedly touted to exhibit state-of-the-art performance in recommendation. Nevertheless, multiple recent studies have revealed that the reported state-of-the-art results of many neural recommendation models cannot be reliably replicated. A primary reason is that existing evaluations are performed under various inconsistent protocols. Correspondingly, these replicability issues make it difficult to understand how much benefit we can actually gain from these neural models. It then becomes clear that a fair and comprehensive performance comparison between traditional and neural models is needed.  Motivated by these issues, we perform a large-scale, systematic study to compare recent neural recommendation models against traditional ones in top-n recommendation from implicit data. We propose a set of evaluation strategies for measuring memorization performance, generalization performance, and subgroup-specific performance of recommendation models. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01788</link><description>&lt;p&gt;
&#35270;&#35273;&#19982;&#23450;&#20041;&#30456;&#36935;&#65306;&#34701;&#21512;&#35789;&#20041;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information. (arXiv:2305.01788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20934;&#30830;&#22320;&#25551;&#36848;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30446;&#26631;&#35789;&#27491;&#30830;&#24847;&#20041;&#30340;&#22270;&#20687;&#12290;&#20197;&#24448;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#35789;&#20041;&#22810;&#20041;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#27719;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#35789;&#20041;&#23450;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#30340;&#35789;&#20041;&#20449;&#24687;&#26102;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25913;&#36827;&#35789;&#20856;&#22806;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;GPT-3&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26126;&#26174;&#25552;&#39640;&#20102;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#22312;&#35789;&#20856;&#22806;&#20363;&#23376;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method. We will publish source 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#20445;&#25345;&#22810;&#23792;&#39044;&#27979;&#20998;&#24067;&#30340;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#30697;&#21305;&#37197;&#35268;&#21017;&#23454;&#29616;&#20102;&#26080;&#26679;&#26412;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01773</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24265;&#20215;&#21644;&#30830;&#23450;&#24615;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Cheap and Deterministic Inference for Deep State-Space Models of Interacting Dynamical Systems. (arXiv:2305.01773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#20445;&#25345;&#22810;&#23792;&#39044;&#27979;&#20998;&#24067;&#30340;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#30697;&#21305;&#37197;&#35268;&#21017;&#23454;&#29616;&#20102;&#26080;&#26679;&#26412;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34987;&#29992;&#20110;&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#22240;&#20026;&#23427;&#20204;&#20248;&#38597;&#22320;&#36866;&#24212;&#20110;&#20855;&#26377;&#21464;&#21270;&#21644;&#22823;&#37327;&#20195;&#29702;&#30340;&#31995;&#32479;&#12290;&#34429;&#28982;&#22312;&#30830;&#23450;&#24615;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#26377;&#20852;&#36259;&#33719;&#24471;&#26410;&#26469;&#36712;&#36857;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#38543;&#26426;&#31995;&#32479;&#65292;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#36895;&#24230;&#24930;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#65292;&#35201;&#20040;&#20570;&#20986;&#31616;&#21270;&#20551;&#35774;&#65292;&#20351;&#24471;&#39044;&#27979;&#20998;&#24067;&#26159;&#21333;&#23792;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#24213;&#23618;&#30340;&#30456;&#20114;&#20316;&#29992;&#21160;&#24577;&#31995;&#32479;&#12290;&#39044;&#27979;&#20998;&#24067;&#26159;&#22810;&#23792;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#24418;&#24335;&#65292;&#20854;&#20013;&#39640;&#26031;&#20998;&#37327;&#30340;&#30697;&#21487;&#20197;&#36890;&#36807;&#30830;&#23450;&#24615;&#30697;&#21305;&#37197;&#35268;&#21017;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#30697;&#21305;&#37197;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#26080;&#26679;&#26412;&#25512;&#26029;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#21644;&#31283;&#23450;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21644;&#39044;&#27979;&#38543;&#26426;&#31995;&#32479;&#30340;&#36712;&#36857;&#65292;&#21363;&#20351;&#23384;&#22312;&#24040;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are often used to model interacting dynamical systems since they gracefully scale to systems with a varying and high number of agents. While there has been much progress made for deterministic interacting systems, modeling is much more challenging for stochastic systems in which one is interested in obtaining a predictive distribution over future trajectories. Existing methods are either computationally slow since they rely on Monte Carlo sampling or make simplifying assumptions such that the predictive distribution is unimodal. In this work, we present a deep state-space model which employs graph neural networks in order to model the underlying interacting dynamical system. The predictive distribution is multimodal and has the form of a Gaussian mixture model, where the moments of the Gaussian components can be computed via deterministic moment matching rules. Our moment matching scheme can be exploited for sample-free inference, leading to more efficient and sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#26524;&#25552;&#31034;&#35821;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20154;&#31867;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#36825;&#20123;&#25552;&#31034;&#35821;&#21487;&#20197;&#29992;&#26469;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.01764</link><description>&lt;p&gt;
&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#22240;&#26524;&#25552;&#31034;&#35821;
&lt;/p&gt;
&lt;p&gt;
Psychologically-Inspired Causal Prompts. (arXiv:2305.01764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#26524;&#25552;&#31034;&#35821;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20154;&#31867;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#36825;&#20123;&#25552;&#31034;&#35821;&#21487;&#20197;&#29992;&#26469;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#25968;&#25454;&#38598;&#19981;&#20165;&#20165;&#21547;&#26377;&#36755;&#20837;&#36755;&#20986;&#23545;&#65292;&#36824;&#21253;&#21547;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#25991;&#20197;&#24773;&#24863;&#20998;&#31867;&#20026;&#20363;&#65292;&#25506;&#35752;&#35780;&#35770;&#65288;X&#65289;&#21644;&#24773;&#24863;&#65288;Y&#65289;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#24515;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#21487;&#20197;&#24433;&#21709;&#24773;&#32490;&#65292;&#24403;&#19968;&#20010;&#20154;&#39318;&#27425;&#36827;&#34892;&#35780;&#20998;&#24182;&#22312;&#35780;&#35770;&#20013;&#36827;&#34892;&#33258;&#25105;&#21512;&#29702;&#21270;&#26102;&#65288;&#24773;&#24863;&#24341;&#36215;&#35780;&#35770;&#65292;&#21363;Y-&gt;X&#65289;&#65292;&#19982;&#39318;&#20808;&#25551;&#36848;&#33258;&#24049;&#30340;&#32463;&#21382;&#24182;&#26435;&#34913;&#21033;&#24330;&#20197;&#20570;&#20986;&#26368;&#21518;&#35780;&#20998;&#26102;&#65288;&#35780;&#35770;&#24341;&#36215;&#24773;&#24863;&#65292;&#21363;X-&gt;Y&#65289;&#65292;&#20250;&#24341;&#21457;&#19981;&#21516;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#35780;&#27880;&#32773;&#36890;&#36807;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#25512;&#26029;&#29992;&#25143;&#30340;&#21407;&#22987;&#35780;&#20998;&#65292;&#21017;&#36825;&#20063;&#26159;&#23436;&#20840;&#19981;&#21516;&#30340;&#24515;&#29702;&#36807;&#31243;&#65288;&#35780;&#35770;&#24341;&#36215;&#35780;&#20998;&#65292;&#21363;X-ToM-&gt; Y&#65289;&#12290;&#26412;&#25991;&#23558;&#36825;&#19977;&#31181;&#24773;&#24863;&#20998;&#31867;&#30340;&#20154;&#31867;&#24515;&#29702;&#36807;&#31243;&#30340;&#22240;&#26524;&#26426;&#21046;&#36716;&#21270;&#20026;&#19977;&#20010;&#25552;&#31034;&#35821;&#65292;&#24182;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#25552;&#31034;&#35821;&#65292;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP datasets are richer than just input-output pairs; rather, they carry causal relations between the input and output variables. In this work, we take sentiment classification as an example and look into the causal relations between the review (X) and sentiment (Y). As psychology studies show that language can affect emotion, different psychological processes are evoked when a person first makes a rating and then self-rationalizes their feeling in a review (where the sentiment causes the review, i.e., Y -&gt; X), versus first describes their experience, and weighs the pros and cons to give a final rating (where the review causes the sentiment, i.e., X -&gt; Y ). Furthermore, it is also a completely different psychological process if an annotator infers the original rating of the user by theory of mind (ToM) (where the review causes the rating, i.e., X -ToM-&gt; Y ). In this paper, we verbalize these three causal mechanisms of human psychological processes of sentiment classification into three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#39044;&#27979;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#21738;&#20123;&#22270;&#26696;&#23558;&#20986;&#29616;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.01761</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#32593;&#32476;&#29992;&#20110;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Networks for Antibiogram Pattern Prediction. (arXiv:2305.01761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#39044;&#27979;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#21738;&#20123;&#22270;&#26696;&#23558;&#20986;&#29616;&#65292;&#24182;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#26159;&#23545;&#24863;&#26579;&#24739;&#32773;&#30340;&#25239;&#29983;&#32032;&#32784;&#33647;&#24615;&#26816;&#27979;&#32467;&#26524;&#36827;&#34892;&#21608;&#26399;&#24615;&#24635;&#32467;&#12290;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#26377;&#21161;&#20110;&#21307;&#29983;&#20102;&#35299;&#22320;&#21306;&#32784;&#33647;&#24615;&#29575;&#24182;&#36873;&#25321;&#36866;&#24403;&#30340;&#22788;&#26041;&#25239;&#29983;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#39044;&#27979;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#21738;&#20123;&#22270;&#26696;&#23558;&#20986;&#29616;&#12290;&#23613;&#31649;&#35813;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#22788;&#29702;&#35813;&#38382;&#39064;&#20250;&#36935;&#21040;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#39318;&#20808;&#65292;&#25239;&#29983;&#32032;&#25935;&#24863;&#24615;&#22270;&#26696;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#30001;&#20110;&#22522;&#22240;&#30456;&#20284;&#24615;&#32780;&#24444;&#27492;&#32039;&#23494;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
An antibiogram is a periodic summary of antibiotic resistance results of organisms from infected patients to selected antimicrobial drugs. Antibiograms help clinicians to understand regional resistance rates and select appropriate antibiotics in prescriptions. In practice, significant combinations of antibiotic resistance may appear in different antibiograms, forming antibiogram patterns. Such patterns may imply the prevalence of some infectious diseases in certain regions. Thus it is of crucial importance to monitor antibiotic resistance trends and track the spread of multi-drug resistant organisms. In this paper, we propose a novel problem of antibiogram pattern prediction that aims to predict which patterns will appear in the future. Despite its importance, tackling this problem encounters a series of challenges and has not yet been explored in the literature. First of all, antibiogram patterns are not i.i.d as they may have strong relations with each other due to genomic similariti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;VoicePrivacy 2020 Challenge&#20013;&#28436;&#35762;&#32773;&#21311;&#21517;&#21270;&#23545;&#24773;&#24863;&#35821;&#38899;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20854;&#26410;&#33021;&#26377;&#25928;&#22320;&#20445;&#25252;&#28436;&#35762;&#32773;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2305.01759</link><description>&lt;p&gt;
&#28436;&#35762;&#32773;&#21311;&#21517;&#21270;&#23545;&#24773;&#24863;&#35821;&#38899;&#30340;&#24433;&#21709;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Speaker Anonymization on Emotional Speech. (arXiv:2305.01759v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;VoicePrivacy 2020 Challenge&#20013;&#28436;&#35762;&#32773;&#21311;&#21517;&#21270;&#23545;&#24773;&#24863;&#35821;&#38899;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20854;&#26410;&#33021;&#26377;&#25928;&#22320;&#20445;&#25252;&#28436;&#35762;&#32773;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#25968;&#25454;&#25658;&#24102;&#30528;&#20010;&#20154;&#30340;&#20449;&#24687;&#65292;&#22914;&#28436;&#35762;&#32773;&#30340;&#36523;&#20221;&#21644;&#24773;&#24863;&#29366;&#24577;&#12290;&#36825;&#20123;&#23646;&#24615;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#12290;&#38543;&#30528;&#34394;&#25311;&#21161;&#25163;&#30340;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#26032;&#19968;&#20195;&#30340;&#38544;&#31169;&#23041;&#32961;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#20445;&#25252;&#35821;&#38899;&#38544;&#31169;&#30340;&#35805;&#39064;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;VoicePrivacy&#20513;&#35758;&#65292;&#26088;&#22312;&#20419;&#36827;&#20026;&#35821;&#38899;&#25216;&#26415;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#24037;&#20855;&#12290;VoicePrivacy 2020&#25361;&#25112;&#36187;&#65288;VPC&#65289;&#36873;&#23450;&#30340;&#20219;&#21153;&#26159;&#28436;&#35762;&#32773;&#21311;&#21517;&#21270;&#12290;&#30446;&#26631;&#26159;&#38544;&#34255;&#28304;&#28436;&#35762;&#32773;&#30340;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#35328;&#20449;&#24687;&#12290;VPC&#30340;&#22522;&#20934;&#32447;&#20351;&#29992;&#20102;&#35821;&#38899;&#36716;&#25442;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;VPC&#22522;&#20934;&#32447;&#31995;&#32479;&#23545;&#35821;&#38899;&#35805;&#35821;&#20013;&#24773;&#24863;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#25353;&#29031;&#25915;&#20987;&#32773;&#23545;&#21311;&#21517;&#21270;&#31995;&#32479;&#30340;&#20102;&#35299;&#65292;&#25191;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;VPC&#22522;&#20934;&#32447;&#31995;&#32479;&#26410;&#33021;&#26377;&#25928;&#22320;&#21311;&#21517;&#21270;&#35821;&#38899;&#20013;&#30340;&#24773;&#24863;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#28436;&#35762;&#32773;&#30340;&#38544;&#31169;&#36896;&#25104;&#28508;&#22312;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech data carries a range of personal information, such as the speaker's identity and emotional state. These attributes can be used for malicious purposes. With the development of virtual assistants, a new generation of privacy threats has emerged. Current studies have addressed the topic of preserving speech privacy. One of them, the VoicePrivacy initiative aims to promote the development of privacy preservation tools for speech technology. The task selected for the VoicePrivacy 2020 Challenge (VPC) is about speaker anonymization. The goal is to hide the source speaker's identity while preserving the linguistic information. The baseline of the VPC makes use of a voice conversion. This paper studies the impact of the speaker anonymization baseline system of the VPC on emotional information present in speech utterances. Evaluation is performed following the VPC rules regarding the attackers' knowledge about the anonymization system. Our results show that the VPC baseline system does n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;KB-BINDER&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#30693;&#35782;&#24211;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;KBQA&#38382;&#39064;&#30340;&#21487;&#35299;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01750</link><description>&lt;p&gt;
&#22522;&#20110;&#23569;&#26679;&#26412;&#32972;&#26223;&#23398;&#20064;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Few-shot In-context Learning for Knowledge Base Question Answering. (arXiv:2305.01750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;KB-BINDER&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#30693;&#35782;&#24211;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;KBQA&#38382;&#39064;&#30340;&#21487;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#24212;&#23545;&#21508;&#31181;&#21487;&#33021;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30693;&#35782;&#24211;&#26550;&#26500;&#39033;&#20043;&#38388;&#30340;&#24322;&#26500;&#24615;&#36890;&#24120;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#19987;&#38376;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31181;KBQA&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KB-BINDER&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;KBQA&#25968;&#25454;&#38598;&#32479;&#19968;&#12290;&#39318;&#20808;&#65292;KB-BINDER&#21033;&#29992;&#20687;Codex&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#27169;&#20223;&#23569;&#37327;&#28436;&#31034;&#26469;&#29983;&#25104;&#29305;&#23450;&#38382;&#39064;&#30340;&#36923;&#36753;&#24418;&#24335;&#20316;&#20026;&#33609;&#31295;&#12290;&#20854;&#27425;&#65292;KB-BINDER&#22522;&#20110;&#30693;&#35782;&#24211;&#26469;&#32465;&#23450;&#29983;&#25104;&#30340;&#33609;&#31295;&#33267;&#21487;&#25191;&#34892;&#24418;&#24335;&#65292;&#36890;&#36807;BM25&#20998;&#25968;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#20844;&#24320;&#30340;&#24322;&#26500;KBQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KB-BINDER&#21487;&#20197;&#22312;&#23569;&#37327;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#24212;&#29992;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01747</link><description>&lt;p&gt;
&#24102;&#26377;&#26377;&#38480;&#27880;&#37322;&#30340;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#20266;&#26631;&#31614;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations. (arXiv:2305.01747v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#24212;&#29992;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20266;&#26631;&#31614;&#21450;&#20854;&#25512;&#24191;&#65292;&#20266;&#26631;&#31614;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21407;&#22987;&#25512;&#26029;&#20316;&#20026;&#33258;&#35757;&#32451;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20266;&#26631;&#31614;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#37096;&#20998;&#35299;&#37322;&#20102;&#20854;&#23454;&#35777;&#25104;&#21151;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#21407;&#29702;&#19979;&#20266;&#26631;&#31614;&#30340;&#23436;&#20840;&#27867;&#21270;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#23398;&#20064;&#36924;&#36817;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#36890;&#36807;&#23398;&#20064;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38408;&#20540;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#20266;&#26631;&#31614;&#21644;&#20854;&#25512;&#24191;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study pseudo labelling and its generalisation for semi-supervised segmentation of medical images. Pseudo labelling has achieved great empirical successes in semi-supervised learning, by utilising raw inferences on unlabelled data as pseudo labels for self-training. In our paper, we build a connection between pseudo labelling and the Expectation Maximization algorithm which partially explains its empirical successes. We thereby realise that the original pseudo labelling is an empirical estimation of its underlying full formulation. Following this insight, we demonstrate the full generalisation of pseudo labels under Bayes' principle, called Bayesian Pseudo Labels. We then provide a variational approach to learn to approximate Bayesian Pseudo Labels, by learning a threshold to select good quality pseudo labels. In the rest of the paper, we demonstrate the applications of Pseudo Labelling and its generalisation Bayesian Psuedo Labelling in semi-supervised segmentation of medical images
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#23376;&#21270;&#21160;&#20316;&#31354;&#38388;&#30340;&#32447;&#24615;Q&#20989;&#25968;&#20998;&#35299;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#21160;&#20316;&#32452;&#21512;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#21516;&#26102;&#24182;&#19981;&#29306;&#29298;&#31574;&#30053;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#20960;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.01738</link><description>&lt;p&gt;
&#21033;&#29992;&#22240;&#23376;&#21270;&#21160;&#20316;&#31354;&#38388;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Factored Action Spaces for Efficient Offline Reinforcement Learning in Healthcare. (arXiv:2305.01738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#23376;&#21270;&#21160;&#20316;&#31354;&#38388;&#30340;&#32447;&#24615;Q&#20989;&#25968;&#20998;&#35299;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#21160;&#20316;&#32452;&#21512;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#21516;&#26102;&#24182;&#19981;&#29306;&#29298;&#31574;&#30053;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#20960;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20855;&#26377;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#65292;&#20854;&#20013;&#27599;&#20010;&#21160;&#20316;&#26159;&#23376;&#21160;&#20316;&#30340;&#32452;&#21512;&#12290;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#22266;&#26377;&#30340;&#20998;&#35299;&#32467;&#26500;&#65292;&#23548;&#33268;&#21487;&#33021;&#23545;&#23569;&#35265;&#30340;&#23376;&#21160;&#20316;&#32452;&#21512;&#20570;&#20986;&#30340;&#25512;&#29702;&#27809;&#26377;&#24847;&#20041;&#65307;&#36825;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#23588;&#20854;&#38382;&#39064;&#31361;&#20986;&#65292;&#22240;&#20026;&#25968;&#25454;&#21487;&#33021;&#21463;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#22240;&#23376;&#21270;&#21160;&#20316;&#31354;&#38388;&#24341;&#36215;&#30340;&#32447;&#24615;Q&#20989;&#25968;&#20998;&#35299;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#30830;&#23450;&#20102;&#24403;&#29992;&#20110;&#36817;&#20284;Q&#20989;&#25968;&#26102;&#20445;&#35777;&#20135;&#29983;&#38646;&#20559;&#24046;&#30340;&#24773;&#20917;&#12290;&#22312;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#33539;&#22260;&#20043;&#22806;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#26377;&#29992;&#30340;&#65292;&#22240;&#20026;&#23427;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#32780;&#19981;&#19968;&#23450;&#29306;&#29298;&#31574;&#30053;&#26368;&#20248;&#24615;&#65292;&#20801;&#35768;&#25105;&#20204;&#23454;&#29616;&#26356;&#22909;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#22312;&#20351;&#29992;&#30001;&#21307;&#30103;&#20445;&#20581;&#21551;&#31034;&#30340;&#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#20960;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26631;&#20934;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many reinforcement learning (RL) applications have combinatorial action spaces, where each action is a composition of sub-actions. A standard RL approach ignores this inherent factorization structure, resulting in a potential failure to make meaningful inferences about rarely observed sub-action combinations; this is particularly problematic for offline settings, where data may be limited. In this work, we propose a form of linear Q-function decomposition induced by factored action spaces. We study the theoretical properties of our approach, identifying scenarios where it is guaranteed to lead to zero bias when used to approximate the Q-function. Outside the regimes with theoretical guarantees, we show that our approach can still be useful because it leads to better sample efficiency without necessarily sacrificing policy optimality, allowing us to achieve a better bias-variance trade-off. Across several offline RL problems using simulators and real-world datasets motivated by healthca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#20915;&#31574;&#35268;&#21017;&#31995;&#32479;&#26500;&#24314;&#20915;&#31574;&#26641;&#21644;&#26080;&#29615;&#20915;&#31574;&#22270;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#24517;&#26500;&#24314;&#23436;&#25972;&#20915;&#31574;&#26641;&#32780;&#21482;&#38656;&#25551;&#36848;&#32473;&#23450;&#36755;&#20837;&#30340;&#35745;&#31639;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01721</link><description>&lt;p&gt;
&#20174;&#20915;&#31574;&#35268;&#21017;&#31995;&#32479;&#26500;&#24314;&#20915;&#31574;&#26641;&#21644;&#26080;&#29615;&#20915;&#31574;&#22270;
&lt;/p&gt;
&lt;p&gt;
Construction of Decision Trees and Acyclic Decision Graphs from Decision Rule Systems. (arXiv:2305.01721v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01721
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#20915;&#31574;&#35268;&#21017;&#31995;&#32479;&#26500;&#24314;&#20915;&#31574;&#26641;&#21644;&#26080;&#29615;&#20915;&#31574;&#22270;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#24517;&#26500;&#24314;&#23436;&#25972;&#20915;&#31574;&#26641;&#32780;&#21482;&#38656;&#25551;&#36848;&#32473;&#23450;&#36755;&#20837;&#30340;&#35745;&#31639;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#21644;&#20915;&#31574;&#35268;&#21017;&#31995;&#32479;&#34987;&#24191;&#27867;&#29992;&#20316;&#20998;&#31867;&#22120;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#36816;&#31639;&#27861;&#12290;&#23427;&#20204;&#26159;&#26368;&#26131;&#20110;&#35299;&#37322;&#30340;&#25968;&#25454;&#20998;&#26512;&#27169;&#22411;&#20043;&#19968;&#12290;&#30740;&#31350;&#36825;&#20004;&#31181;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#20197;&#30475;&#20316;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#12290;&#35813;&#25991;&#30740;&#31350;&#20102;&#20174;&#20915;&#31574;&#35268;&#21017;&#31995;&#32479;&#26500;&#24314;&#20915;&#31574;&#26641;&#21644;&#26080;&#29615;&#20915;&#31574;&#22270;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#32473;&#23450;&#36755;&#20837;&#26102;&#19981;&#26500;&#24314;&#25972;&#20010;&#20915;&#31574;&#26641;&#65292;&#32780;&#26159;&#25551;&#36848;&#22312;&#35813;&#26641;&#20013;&#30340;&#35745;&#31639;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees and systems of decision rules are widely used as classifiers, as a means for knowledge representation, and as algorithms. They are among the most interpretable models for data analysis. The study of the relationships between these two models can be seen as an important task of computer science. Methods for transforming decision trees into systems of decision rules are simple and well-known. In this paper, we consider the inverse transformation problem, which is not trivial. We study the complexity of constructing decision trees and acyclic decision graphs representing decision trees from decision rule systems, and we discuss the possibility of not building the entire decision tree, but describing the computation path in this tree for the given input.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01713</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35299;&#37322;&#30340;&#38750;&#20132;&#20114;&#35821;&#20041;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32454;&#21270;&#36830;&#32493;&#31354;&#38388;&#30340;&#21477;&#23376;&#34920;&#24449;&#19978;&#36827;&#34892;&#35299;&#32806;&#21487;&#20197;&#22312;&#23450;&#20301;&#26126;&#30830;&#21457;&#29983;&#30340;&#29983;&#25104;&#22240;&#32032;&#30340;&#21516;&#26102;&#65292;&#25913;&#36827;&#21487;&#35299;&#37322;&#24615;&#21644;&#35821;&#20041;&#25511;&#21046;&#65292;&#36825;&#20026;&#22522;&#20110;&#31070;&#32463;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#20102;&#19968;&#20123;&#31526;&#21495;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#28789;&#27963;&#24615;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#38500;&#32534;&#30721;&#30340;&#38544;&#34255;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;INN&#33021;&#22815;&#23558;&#20998;&#24067;&#24335;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#22909;&#30340;&#35821;&#20041;&#19978;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling sentence representations over continuous spaces can be a critical process in improving interpretability and semantic control by localising explicit generative factors. Such process confers to neural-based language models some of the advantages that are characteristic of symbolic models, while keeping their flexibility. This work presents a methodology for disentangling the hidden space of a BERT-GPT2 autoencoder by transforming it into a more separable semantic space with the support of a flow-based invertible neural network (INN). Experimental results indicate that the INN can transform the distributed hidden space into a better semantically disentangled latent space, resulting in better interpretability and controllability, when compared to recent state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#23545;AI&#35843;&#35299;&#27807;&#36890;&#25216;&#26415;&#30340;&#31192;&#23494;&#20351;&#29992;&#25345;&#28040;&#26497;&#24577;&#24230;&#65292;&#26399;&#26395;&#21035;&#20154;&#30340;&#20351;&#29992;&#29575;&#27604;&#23454;&#38469;&#24773;&#20917;&#39640;&#65292;&#24456;&#22810;&#20154;&#35748;&#20026;&#21035;&#20154;&#23545;AICTs&#20351;&#29992;&#19981;&#36127;&#36131;&#20219;&#65292;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#20851;&#20110;AI&#35843;&#35299;&#27807;&#36890;&#25216;&#26415;&#30340;&#38169;&#35823;&#30475;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01670</link><description>&lt;p&gt;
&#22522;&#20110;&#26399;&#26395;&#35282;&#24230;&#30475;&#24453;AI&#35843;&#35299;&#27807;&#36890;&#30340;&#25285;&#24551;&#21644;&#19981;&#21516;&#24577;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fears about AI-mediated communication are grounded in different expectations for one's own versus others' use. (arXiv:2305.01670v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01670
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;AI&#35843;&#35299;&#27807;&#36890;&#25216;&#26415;&#30340;&#31192;&#23494;&#20351;&#29992;&#25345;&#28040;&#26497;&#24577;&#24230;&#65292;&#26399;&#26395;&#21035;&#20154;&#30340;&#20351;&#29992;&#29575;&#27604;&#23454;&#38469;&#24773;&#20917;&#39640;&#65292;&#24456;&#22810;&#20154;&#35748;&#20026;&#21035;&#20154;&#23545;AICTs&#20351;&#29992;&#19981;&#36127;&#36131;&#20219;&#65292;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#20851;&#20110;AI&#35843;&#35299;&#27807;&#36890;&#25216;&#26415;&#30340;&#38169;&#35823;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23186;&#20171;&#20132;&#27969;&#25216;&#26415;(AICTs)&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#21363;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#22686;&#24378;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#28040;&#24687;&#27807;&#36890;&#24050;&#32463;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20154;&#38469;&#20851;&#31995;&#30340;&#26410;&#26469;&#20197;&#21450;&#25259;&#38706;&#21644;&#37319;&#29992;&#34892;&#20026;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#20154;&#20204;&#23545;&#20844;&#24320;&#21644;&#38544;&#31192;&#30340;AICTs&#30340;&#21487;&#25509;&#21463;&#24615;&#21644;&#20351;&#29992;&#24773;&#20917;&#30340;&#30475;&#27861;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#20004;&#39033;&#20195;&#34920;&#24615;&#26679;&#26412;&#30740;&#31350;&#65288;&#33521;&#22269;&#65306;N=477&#65292;&#32654;&#22269;&#65306;N=765&#65289;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20844;&#24320;AICTs&#30456;&#27604;&#65292;&#31192;&#23494;AICTs&#20351;&#29992;&#34987;&#35748;&#20026;&#19981;&#22826;&#21487;&#25509;&#21463;&#65292;&#20154;&#20204;&#24448;&#24448;&#39640;&#20272;&#21035;&#20154;&#30340;AICTs&#20351;&#29992;&#29575;&#65292;&#32780;&#19988;&#20154;&#20204;&#20063;&#26399;&#26395;&#20182;&#20154;&#19981;&#36127;&#36131;&#20219;&#22320;&#20351;&#29992;AICTs&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25285;&#24515;&#36825;&#31181;&#30072;&#24418;&#26399;&#26395;&#21644;&#19981;&#21516;&#30340;&#26399;&#26395;&#35282;&#24230;&#20250;&#24341;&#36215;&#19968;&#31181;&#33258;&#25105;&#23454;&#29616;&#30340;&#24754;&#35266;&#30475;&#27861;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;AI&#35843;&#35299;&#27807;&#36890;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of AI-mediated communication technologies (AICTs), which are digital tools that use AI to augment interpersonal messages, has raised concerns about the future of interpersonal trust and prompted discussions about disclosure and uptake. This paper contributes to this discussion by assessing perceptions about the acceptability and use of open and secret AICTs for oneself and others. In two studies with representative samples (UK: N=477, US: N=765), we found that secret AICT use is deemed less acceptable than open AICT use, people tend to overestimate others' AICT use, and people expect others to use AICTs irresponsibly. Thus, we raise concerns about the potential for misperceptions and different expectations for others to drive self-fulfilling pessimistic outlooks about AI-mediated communication.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#20197;&#21464;&#25442;&#20026;&#39537;&#21160;&#8221;&#30340;&#35270;&#35273;&#25512;&#29702;&#65288;TVR&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#24050;&#26377;&#20219;&#21153;&#20165;&#20851;&#27880;&#38745;&#24577;&#29615;&#22659;&#29366;&#24577;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;CLEVR&#26500;&#24314;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01668</link><description>&lt;p&gt;
&#35270;&#35273;&#25512;&#29702;&#65306;&#20174;&#29366;&#24577;&#21040;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Visual Reasoning: from State to Transformation. (arXiv:2305.01668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01668
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#20197;&#21464;&#25442;&#20026;&#39537;&#21160;&#8221;&#30340;&#35270;&#35273;&#25512;&#29702;&#65288;TVR&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#24050;&#26377;&#20219;&#21153;&#20165;&#20851;&#27880;&#38745;&#24577;&#29615;&#22659;&#29366;&#24577;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;CLEVR&#26500;&#24314;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#24050;&#26377;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;VQA&#20013;&#30340;CLEVR&#65292;&#24573;&#30053;&#20102;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65306;&#21464;&#25442;&#12290;&#23427;&#20204;&#20165;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#22312;&#38745;&#24577;&#29615;&#22659;&#65288;&#20363;&#22914;&#21333;&#20010;&#22270;&#20687;&#65289;&#20013;&#29702;&#35299;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#20165;&#20851;&#27880;&#29366;&#24577;&#30340;&#35270;&#35273;&#25512;&#29702;&#23384;&#22312;&#38480;&#21046;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#21453;&#26144;&#19981;&#21516;&#29366;&#24577;&#20043;&#38388;&#21160;&#24577;&#25512;&#26029;&#30340;&#33021;&#21147;&#65292;&#32780;&#22312;Piaget&#30340;&#29702;&#35770;&#20013;&#65292;&#36825;&#31181;&#25512;&#26029;&#33021;&#21147;&#21516;&#26679;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20197;&#21464;&#25442;&#20026;&#39537;&#21160;&#8221;&#30340;&#35270;&#35273;&#25512;&#29702;&#65288;TVR&#65289;&#20219;&#21153;&#12290;&#32473;&#23450;&#21021;&#22987;&#21644;&#26368;&#32456;&#29366;&#24577;&#65292;&#30446;&#26631;&#26159;&#25512;&#26029;&#30456;&#24212;&#30340;&#20013;&#38388;&#21464;&#25442;&#12290;&#22522;&#20110;CLEVR&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;TRANCE&#30340;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#30340;&#35774;&#32622;&#65306;&#22522;&#26412;&#65288;&#21333;&#27493;&#21464;&#25442;&#65289;&#12289;&#20107;&#20214;&#65288;&#22810;&#27493;&#21464;&#25442;&#65289;&#21644;&#35270;&#22270;&#65288;&#22810;&#27493;&#21464;&#25442;&#19982;&#21464;&#24418;&#35270;&#22270;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#21478;&#19968;&#20010;&#21517;&#20026;TRANCO&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing visual reasoning tasks, such as CLEVR in VQA, ignore an important factor, i.e.~transformation. They are solely defined to test how well machines understand concepts and relations within static settings, like one image. Such \textbf{state driven} visual reasoning has limitations in reflecting the ability to infer the dynamics between different states, which has shown to be equally important for human cognition in Piaget's theory. To tackle this problem, we propose a novel \textbf{transformation driven} visual reasoning (TVR) task. Given both the initial and final states, the target becomes to infer the corresponding intermediate transformation. Following this definition, a new synthetic dataset namely TRANCE is first constructed on the basis of CLEVR, including three levels of settings, i.e.~Basic (single-step transformation), Event (multi-step transformation), and View (multi-step transformation with variant views). Next, we build another real dataset called TRANCO based 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#30340;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.01661</link><description>&lt;p&gt;
SIA-FTP: &#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SIA-FTP: A Spoken Instruction Aware Flight Trajectory Prediction Framework. (arXiv:2305.01661v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#30340;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#36890;&#35759;&#36827;&#34892;&#22320;&#31354;&#21327;&#21830;&#26159;&#30830;&#20445;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#65288;ATC&#65289;&#25805;&#20316;&#23433;&#20840;&#21644;&#25928;&#29575;&#30340;&#37325;&#35201;&#21069;&#25552;&#12290;&#20294;&#26159;&#65292;&#38543;&#30528;&#20132;&#36890;&#27969;&#37327;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#20154;&#20026;&#22240;&#32032;&#23548;&#33268;&#30340;&#38169;&#35823;&#25351;&#20196;&#32473;ATC&#23433;&#20840;&#24102;&#26469;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#65288;FTP&#65289;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21382;&#21490;&#36712;&#36857;&#30340;&#39134;&#34892;&#29366;&#24577;&#65292;&#22312;&#23454;&#26102;&#26426;&#21160;&#25351;&#20196;&#30340;&#39044;&#27979;&#19978;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24310;&#36831;&#65292;&#36825;&#19981;&#21033;&#20110;&#20914;&#31361;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIA-FTP&#30340;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;FTP&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#21547;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#26469;&#25903;&#25345;&#39640;&#26426;&#21160;FTP&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#27169;&#24577;&#24046;&#36317;&#24182;&#26368;&#23567;&#21270;&#25968;&#25454;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#27880;&#24847;&#26426;&#21046;&#26469;&#34701;&#21512;&#35821;&#38899;&#25351;&#20196;&#23884;&#20837;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;SIA-FTP&#65292;&#19982;&#29616;&#26377;&#30340;FTP&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ground-air negotiation via speech communication is a vital prerequisite for ensuring safety and efficiency in air traffic control (ATC) operations. However, with the increase in traffic flow, incorrect instructions caused by human factors bring a great threat to ATC safety. Existing flight trajectory prediction (FTP) approaches primarily rely on the flight status of historical trajectory, leading to significant delays in the prediction of real-time maneuvering instruction, which is not conducive to conflict detection. A major reason is that spoken instructions and flight trajectories are presented in different modalities in the current air traffic control (ATC) system, bringing great challenges to considering the maneuvering instruction in the FTP tasks. In this paper, a spoken instruction-aware FTP framework, called SIA-FTP, is innovatively proposed to support high-maneuvering FTP tasks by incorporating instant spoken instruction. To address the modality gap and minimize the data requ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20559;&#24207; Shapley &#20540;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#19977;&#31181;&#31639;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#32467;&#26524;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#21512;&#20316;&#20013;&#39034;&#24207;&#20316;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01660</link><description>&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#65306;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#24207; Shapley &#20540;
&lt;/p&gt;
&lt;p&gt;
Data valuation: The partial ordinal Shapley value for machine learning. (arXiv:2305.01660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20559;&#24207; Shapley &#20540;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#19977;&#31181;&#31639;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#32467;&#26524;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#21512;&#20316;&#20013;&#39034;&#24207;&#20316;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20351;&#29992; Shapley &#20540;&#36827;&#34892;&#25968;&#25454;&#20272;&#20540;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#32570;&#20047;&#20851;&#20110;&#25968;&#25454;&#21512;&#20316;&#20013;&#39034;&#24207;&#20316;&#29992;&#30340;&#35752;&#35770;&#65292;&#22240;&#27492;&#35299;&#20915;&#25968;&#25454;&#39034;&#24207;&#30340;&#20316;&#29992;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#36890;&#36807;&#32676;&#35770;&#20013;&#30340;&#25277;&#35937;&#20195;&#25968;&#30740;&#31350;&#20102;&#20559;&#24207; Shapley &#20540;&#30340;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20559;&#24207; Shapley &#20540;&#30340;&#35745;&#31639;&#38656;&#35201;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19977;&#20010;&#31639;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#32467;&#26524;&#65292;&#20998;&#21035;&#20026;&#25130;&#26029;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#12289;&#20998;&#31867;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#21644;&#20998;&#31867;&#25130;&#26029;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#12290;&#36825;&#19977;&#20010;&#31639;&#27861;&#30340;&#23454;&#29616;&#19981;&#21516;&#65292;&#20294;&#37117;&#21487;&#20197;&#36890;&#36807;&#19968;&#23450;&#31243;&#24230;&#30340;&#36817;&#20284;&#26469;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation using Shapley value has emerged as a prevalent research domain in machine learning applications. However, it is a challenge to address the role of order in data cooperation as most research lacks such discussion. To tackle this problem, this paper studies the definition of the partial ordinal Shapley value by group theory in abstract algebra. Besides, since the calculation of the partial ordinal Shapley value requires exponential time, this paper also gives three algorithms for approximating the results. The Truncated Monte Carlo algorithm is derived from the classic Shapley value approximation algorithm. The Classification Monte Carlo algorithm and the Classification Truncated Monte Carlo algorithm are based on the fact that the data points in the same class provide similar information, then we can accelerate the calculation by leaving out some data points in each class.
&lt;/p&gt;</description></item><item><title>FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01658</link><description>&lt;p&gt;
FlightBERT++&#65306;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01658
&lt;/p&gt;
&lt;p&gt;
FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26159;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#31354;&#31649;&#21592;&#26356;&#23433;&#20840;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#33258;&#22238;&#24402;&#26041;&#24335;&#25191;&#34892;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#23481;&#26131;&#20986;&#29616;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FlightBERT++&#65292;&#20197;i&#65289;&#30452;&#25509;&#20197;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#39044;&#27979;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#65292;&#21644;ii&#65289;&#25913;&#21892;FlightBERT&#26694;&#26550;&#20013;&#20108;&#36827;&#21046;&#32534;&#30721;&#65288;BE&#65289;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23454;&#29616;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20174;&#21382;&#21490;&#35266;&#27979;&#20013;&#23398;&#20064;&#26102;&#31354;&#27169;&#24335;&#65292;&#32780;&#35299;&#30721;&#22120;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#39134;&#34892;&#29366;&#24577;&#12290;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#27604;&#65292;&#39069;&#22806;&#30340;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#65288;HACG&#65289;&#19987;&#38376;&#35774;&#35745;&#32771;&#34385;&#20808;&#21069;&#30340;&#26102;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#25968;&#25454;&#19978;&#19979;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#35821;&#20041;&#37325;&#25490;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.01598</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#65306;&#21033;&#29992;&#25968;&#25454;&#36827;&#34892;&#31243;&#24207;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
From Words to Code: Harnessing Data for Program Synthesis from Natural Language. (arXiv:2305.01598v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#25968;&#25454;&#19978;&#19979;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#35821;&#20041;&#37325;&#25490;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#27491;&#30830;&#25805;&#20316;&#25968;&#25454;&#30340;&#31243;&#24207;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#24213;&#23618;&#30340;&#32534;&#31243;&#35821;&#35328;&#21644; API &#23545;&#20110;&#35768;&#22810;&#19981;&#29087;&#32451;&#30340;&#31243;&#24207;&#21592;&#26469;&#35828;&#23398;&#20064;&#36215;&#26469;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20195;&#30721;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#25968;&#25454;&#25805;&#20316;&#39046;&#22495;&#65292;&#38500;&#20102;&#25152;&#38656;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#22806;&#65292;&#25105;&#20204;&#36824;&#26377;&#25968;&#25454;&#38598;&#20316;&#20026;&#35813;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#28155;&#21152;&#21040;&#21457;&#36865;&#32473; LLM &#30340;&#25552;&#31034;&#20013;&#30340;&#26041;&#24335;&#26377;&#38480;&#22320;&#21033;&#29992;&#25968;&#25454;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#29992;&#30340;&#36755;&#20837;&#25968;&#25454;&#26469;&#25191;&#34892; LLM &#29983;&#25104;&#30340;&#20505;&#36873;&#31243;&#24207;&#24182;&#25910;&#38598;&#23427;&#20204;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#20041;&#37325;&#25490;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22522;&#20110;&#31243;&#24207;&#36755;&#20986;&#30340;&#19977;&#20010;&#20449;&#21495;&#65288;a&#65289;&#35821;&#20041;&#36807;&#28388;&#21644;&#33391;&#22909;&#26684;&#24335;&#24471;&#20998;&#35843;&#25972;&#65306;&#31243;&#24207;&#26159;&#21542;&#31526;&#21512;&#35821;&#20041;&#21644;&#26684;&#24335;&#65292; (b) &#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#24471;&#20998;: &#31243;&#24207;&#26159;&#21542;&#20026;&#36755;&#20837;&#25968;&#25454;&#25552;&#20379;&#20102;&#36755;&#20986;&#12290;, (c) &#32467;&#26500;&#19982;&#35268;&#33539;&#24471;&#20998;&#65306;&#31243;&#24207;&#26159;&#21542;&#36981;&#24490;API&#30340;&#32467;&#26500;&#21644;&#35268;&#33539;&#65292;&#20197;&#37325;&#25490; LLM &#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating programs to correctly manipulate data is a difficult task, as the underlying programming languages and APIs can be challenging to learn for many users who are not skilled programmers. Large language models (LLMs) demonstrate remarkable potential for generating code from natural language, but in the data manipulation domain, apart from the natural language (NL) description of the intended task, we also have the dataset on which the task is to be performed, or the "data context". Existing approaches have utilized data context in a limited way by simply adding relevant information from the input data into the prompts sent to the LLM.  In this work, we utilize the available input data to execute the candidate programs generated by the LLMs and gather their outputs. We introduce semantic reranking, a technique to rerank the programs generated by LLMs based on three signals coming the program outputs: (a) semantic filtering and well-formedness based score tuning: do programs even ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.01381</link><description>&lt;p&gt;
&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26679;&#26412;&#26377;&#25928;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#20248;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#24191;&#27867;&#29992;&#20110;&#25351;&#23450;&#31995;&#32479;&#31574;&#30053;&#30340;&#39640;&#32423;&#30446;&#26631;&#65292;&#33258;&#20027;&#31995;&#32479;&#23398;&#20064;&#30456;&#23545;&#20110;&#36825;&#26679;&#30340;&#35268;&#33539;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290; &#20294;&#26159;&#65292;&#20174;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#24182;&#19981;&#36731;&#26494;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26356;&#36890;&#29992;&#30340;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#65292;&#24403;&#19982;&#29616;&#25104;&#30340;&#26080;&#27169;&#22411;RL&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26368;&#22823;&#21270;&#32473;&#23450;LTL&#35268;&#33539;&#28385;&#36275;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26377;&#20851;&#36873;&#25321;RL&#20013;&#20851;&#38190;&#21442;&#25968;&#20197;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#20026;&#20102;&#30452;&#25509;&#35780;&#20272;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#22120;PRISM&#26469;&#35745;&#31639;LTL&#35268;&#33539;&#30340;&#28385;&#36275;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#24322;&#24341;&#23548;&#30340;&#22270;&#20687;&#31713;&#25913;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#23545;&#31713;&#25913;&#25935;&#24863;&#19988;&#20855;&#26377;&#32039;&#20945;&#35270;&#35273;&#27169;&#24335;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20855;&#26377;&#36739;&#24191;&#27867;&#30340;&#25512;&#24191;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13349</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#24322;&#24341;&#23548;&#37325;&#24314;&#23398;&#20064;&#30340;&#22270;&#20687;&#31713;&#25913;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discrepancy-Guided Reconstruction Learning for Image Forgery Detection. (arXiv:2304.13349v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#24322;&#24341;&#23548;&#30340;&#22270;&#20687;&#31713;&#25913;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#23545;&#31713;&#25913;&#25935;&#24863;&#19988;&#20855;&#26377;&#32039;&#20945;&#35270;&#35273;&#27169;&#24335;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20855;&#26377;&#36739;&#24191;&#27867;&#30340;&#25512;&#24191;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#31713;&#25913;&#26816;&#27979;&#33539;&#24335;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26082;&#25935;&#24863;&#20110;&#31713;&#25913;&#21448;&#20855;&#26377;&#32039;&#20945;&#35270;&#35273;&#27169;&#24335;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#31713;&#25913;&#29305;&#23450;&#27169;&#24335;&#65288;&#20363;&#22914;&#22122;&#22768;&#12289;&#32441;&#29702;&#21644;&#39057;&#29575;&#65289;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20855;&#26377;&#24191;&#27867;&#30340;&#25512;&#24191;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#24046;&#24322;&#24341;&#23548;&#32534;&#30721;&#22120;&#65288;DisGE&#65289;&#26469;&#25552;&#21462;&#25935;&#24863;&#20110;&#31713;&#25913;&#30340;&#35270;&#35273;&#27169;&#24335;&#12290;DisGE&#30001;&#20004;&#20010;&#20998;&#25903;&#32452;&#25104;&#65292;&#20854;&#20013;&#20027;&#27969;&#30340;&#39592;&#24178;&#20998;&#25903;&#29992;&#20110;&#25552;&#21462;&#19968;&#33324;&#35821;&#20041;&#29305;&#24449;&#65292;&#32780;&#36741;&#21161;&#30340;&#24046;&#24322;&#24615;&#22806;&#37096;&#27880;&#24847;&#21147;&#20998;&#25903;&#29992;&#20110;&#25552;&#21462;&#26126;&#30830;&#30340;&#31713;&#25913;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#22836;&#37325;&#24314;&#65288;DouHR&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#19981;&#21516;&#39063;&#31890;&#31354;&#38388;&#20013;&#30340;&#30495;&#23454;&#32039;&#20945;&#35270;&#35273;&#27169;&#24335;&#12290;&#22312;DouHR&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#24046;&#24322;&#32858;&#21512;&#26816;&#27979;&#22120;&#65288;DisAD&#65289;&#26469;&#32858;&#21512;&#36825;&#20123;&#30495;&#23454;&#32039;&#20945;&#30340;&#35270;&#35273;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#26410;&#30693;&#27169;&#24335;&#30340;&#31713;&#25913;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel image forgery detection paradigm for boosting the model learning capacity on both forgery-sensitive and genuine compact visual patterns. Compared to the existing methods that only focus on the discrepant-specific patterns (\eg, noises, textures, and frequencies), our method has a greater generalization. Specifically, we first propose a Discrepancy-Guided Encoder (DisGE) to extract forgery-sensitive visual patterns. DisGE consists of two branches, where the mainstream backbone branch is used to extract general semantic features, and the accessorial discrepant external attention branch is used to extract explicit forgery cues. Besides, a Double-Head Reconstruction (DouHR) module is proposed to enhance genuine compact visual patterns in different granular spaces. Under DouHR, we further introduce a Discrepancy-Aggregation Detector (DisAD) to aggregate these genuine compact visual patterns, such that the forgery detection capability on unknown patterns can
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.10819</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#20449;&#20219;&#26435;&#34913;&#19979;&#30340;&#21512;&#25104;&#25968;&#25454;&#23457;&#35745;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#12289;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#27844;&#38706;&#25935;&#24863;&#21644;&#38544;&#31169;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#24819;&#27861;&#65292;&#20197;&#20943;&#36731;&#30495;&#23454;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39118;&#38505;&#12289;&#20559;&#35265;&#12289;&#20260;&#23475;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20010;&#27010;&#24565;&#20381;&#36182;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#19981;&#20559;&#25191;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#24544;&#23454;&#20110;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#26032;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20817;&#29616;&#20102;&#20854;&#25215;&#35834;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#23427;&#20204;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#22260;&#32469;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35745;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#38134;&#34892;&#12289;&#20154;&#21147;&#36164;&#28304;&#65292;&#20197;&#21450;&#20174;&#34920;&#26684;&#65292;&#26102;&#38388;&#24207;&#21015;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#29992;&#20363;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#20449;&#20219;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.09875</link><description>&lt;p&gt;
GREAT&#20998;&#25968;&#65306;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#20110;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32858;&#21512;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#32467;&#26524;&#19978;&#65292;&#20197;&#35780;&#20272;&#21644;&#25490;&#21517;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23616;&#37096;&#32479;&#35745;&#37327;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#20195;&#34920;&#22522;&#30784;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#30495;&#27491;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;GREAT&#20998;&#25968;&#27491;&#24335;&#20855;&#26377;&#19968;&#20010;&#20840;&#23616;&#32479;&#35745;&#37327;&#30340;&#29289;&#29702;&#24847;&#20041;&#65292;&#25429;&#25417;&#26469;&#33258;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#12290;&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26679;&#26412;&#22343;&#20540;&#19982;&#30495;&#23454;&#22343;&#20540;&#20043;&#38388;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;GREAT&#20998;&#25968;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#20351;&#29992;GREAT&#20998;&#25968;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#39640;&#25928;&#32780;&#19988;&#35268;&#27169;&#21487;&#25193;&#23637;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current studies on adversarial robustness mainly focus on aggregating local robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true global robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score , for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SCAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#65292;&#35299;&#20915;&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.08495</link><description>&lt;p&gt;
&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#32676;&#20307;&#25928;&#29992;&#20248;&#21270;&#65306;&#19968;&#31181;&#31574;&#30053;&#24615;&#21644;&#20247;&#21253;&#24847;&#35782;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach. (arXiv:2304.08495v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SCAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#65292;&#35299;&#20915;&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#31243;&#25512;&#33616;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#22797;&#26434;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#24403;&#32771;&#34385;&#21040;&#20248;&#21270;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#35832;&#22810;&#21442;&#25968;&#65292;&#22914;&#26223;&#28857;&#21463;&#27426;&#36814;&#31243;&#24230;&#12289;&#25490;&#38431;&#26102;&#38388;&#12289;&#27493;&#34892;&#26102;&#38388;&#21644;&#33829;&#19994;&#26102;&#38388;&#31561;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38598;&#20013;&#22312;&#21333;&#20154;&#35270;&#35282;&#19978;&#65292;&#26410;&#33021;&#35299;&#20915;&#33258;&#28982;&#20154;&#32676;&#34892;&#20026;&#24341;&#36215;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#22914;&#36138;&#23146;&#36335;&#30001;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25112;&#30053;&#21644;&#20247;&#21253;&#24847;&#35782;&#34892;&#31243;&#25512;&#33616;&#65288;SCAIR&#65289;&#8221;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#12290;&#25105;&#20204;&#23558;&#36335;&#32447;&#25512;&#33616;&#31574;&#30053;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29366;&#24577;&#32534;&#30721;&#26426;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#23454;&#29616;&#23454;&#26102;&#35268;&#21010;&#21644;&#20998;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#39064;&#20844;&#22253;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#21508;&#31181;&#31454;&#20105;&#24615;&#21644;&#29616;&#23454;&#30340;&#22522;&#32447;&#27979;&#35797;&#65292;&#35777;&#26126;SCAIR&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Itinerary recommendation is a complex sequence prediction problem with numerous real-world applications. This task becomes even more challenging when considering the optimization of multiple user queuing times and crowd levels, as well as numerous involved parameters, such as attraction popularity, queuing time, walking time, and operating hours. Existing solutions typically focus on single-person perspectives and fail to address real-world issues resulting from natural crowd behavior, like the Selfish Routing problem. In this paper, we introduce the Strategic and Crowd-Aware Itinerary Recommendation (SCAIR) algorithm, which optimizes group utility in real-world settings. We model the route recommendation strategy as a Markov Decision Process and propose a State Encoding mechanism that enables real-time planning and allocation in linear time. We evaluate our algorithm against various competitive and realistic baselines using a theme park dataset, demonstrating that SCAIR outperforms th
&lt;/p&gt;</description></item><item><title>ImpressionGPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;ImpressionGPT&#22312;&#20855;&#26377;&#36739;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08448</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65306;ImpressionGPT
&lt;/p&gt;
&lt;p&gt;
ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT. (arXiv:2304.08448v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08448
&lt;/p&gt;
&lt;p&gt;
ImpressionGPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;ImpressionGPT&#22312;&#20855;&#26377;&#36739;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;"Impression"&#37096;&#20998;&#26159;&#25918;&#23556;&#31185;&#21307;&#24072;&#21644;&#20854;&#20182;&#21307;&#29983;&#20132;&#27969;&#30340;&#37325;&#35201;&#22522;&#30784;&#65292;&#36890;&#24120;&#26159;&#22522;&#20110;"Findings"&#37096;&#20998;&#32534;&#20889;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#24072;&#26469;&#35828;&#65292;&#32534;&#20889;&#22823;&#37327;&#30340;&#21360;&#35937;&#25551;&#36848;&#21487;&#33021;&#26159;&#36153;&#26102;&#36153;&#21147;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21360;&#35937;&#29983;&#25104;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#34429;&#28982;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#25918;&#23556;&#23398;&#65289;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26410;&#32463;&#35843;&#26597;&#65292;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ImpressionGPT&#65292;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'Impression' section of a radiology report is a critical basis for communication between radiologists and other physicians, and it is typically written by radiologists based on the 'Findings' section. However, writing numerous impressions can be laborious and error-prone for radiologists. Although recent studies have achieved promising results in automatic impression generation using large-scale medical text data for pre-training and fine-tuning pre-trained language models, such models often require substantial amounts of medical text data and have poor generalization performance. While large language models (LLMs) like ChatGPT have shown strong generalization capabilities and performance, their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, which leverages the in-context learning capability of LLMs by constructing dynamic contexts using domain-specific, individualized dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359; X-REM&#65292;&#23427;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#65292;&#20854;&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#33258;&#21160;&#29983;&#25104;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17579</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20248;&#21270;&#22522;&#20110;&#26816;&#32034;&#30340;&#33016;&#37096; X &#23556;&#32447;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Image-Text Matching Improves Retrieval-based Chest X-Ray Report Generation. (arXiv:2303.17579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359; X-REM&#65292;&#23427;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#65292;&#20854;&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#33258;&#21160;&#29983;&#25104;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#29983;&#25104;&#20020;&#24202;&#20934;&#30830;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#12290;&#20197;&#21069;&#20381;&#36182;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#32780;&#32463;&#24120;&#29983;&#25104;&#19981;&#36830;&#36143;&#21644;&#19981;&#27491;&#30830;&#30340;&#25991;&#26412;&#65292;&#32780;&#22522;&#20110;&#26816;&#32034;&#30340;&#23581;&#35797;&#32463;&#24120;&#26816;&#32034;&#21040;&#19982;&#36755;&#20837;&#22270;&#20687;&#19981;&#30456;&#20851;&#30340;&#25253;&#21578;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Contrastive X-Ray REport Match&#65288;X-REM&#65289;&#30340;&#26032;&#22411;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#35821;&#35328;&#22270;&#20687;&#27169;&#22411;&#35745;&#31639;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#22312;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24615;&#26102;&#32463;&#24120;&#20002;&#22833;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#20132;&#20114;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#20020;&#24202;&#24230;&#37327;&#26041;&#38754;&#65292;X-REM&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#36890;&#36807;&#23545;&#29983;&#25104;&#30340;&#25253;&#21578;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#34920;&#26126; X-R...
&lt;/p&gt;
&lt;p&gt;
Automated generation of clinically accurate radiology reports can improve patient care. Previous report generation methods that rely on image captioning models often generate incoherent and incorrect text due to their lack of relevant domain knowledge, while retrieval-based attempts frequently retrieve reports that are irrelevant to the input image. In this work, we propose Contrastive X-Ray REport Match (X-REM), a novel retrieval-based radiology report generation module that uses an image-text matching score to measure the similarity of a chest X-ray image and radiology report for report retrieval. We observe that computing the image-text matching score with a language-image model can effectively capture the fine-grained interaction between image and text that is often lost when using cosine similarity. X-REM outperforms multiple prior radiology report generation modules in terms of both natural language and clinical metrics. Human evaluation of the generated reports suggests that X-R
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26234;&#33021;&#30340;&#33539;&#30068;&#26694;&#26550;&#65292;&#28041;&#21450;&#23545;&#35937;&#21644;&#22330;&#26223;&#30340;&#34920;&#31034;&#21450;&#27169;&#25311;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#21644;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;AI&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.04571</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#29992;&#26234;&#33021;&#30340;&#33539;&#30068;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Categorical Framework of General Intelligence. (arXiv:2303.04571v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26234;&#33021;&#30340;&#33539;&#30068;&#26694;&#26550;&#65292;&#28041;&#21450;&#23545;&#35937;&#21644;&#22330;&#26223;&#30340;&#34920;&#31034;&#21450;&#27169;&#25311;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#21644;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;AI&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Alan Turing&#22312;1950&#24180;&#25552;&#20986;&#8220;&#26426;&#22120;&#26159;&#21542;&#21487;&#20197;&#24605;&#32771;&#8221;&#30340;&#38382;&#39064;&#20197;&#26469;&#65292;&#30001;&#20110;&#32570;&#23569;&#25166;&#23454;&#30340;&#36890;&#29992;&#26234;&#33021;&#25968;&#23398;&#22522;&#30784;&#65292;&#33267;&#20170;&#38590;&#20197;&#32473;&#20986;&#30452;&#25509;&#31572;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#24448;&#36825;&#19968;&#30446;&#26631;&#30340;&#33539;&#30068;&#26694;&#26550;&#65292;&#24471;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#23618;&#27425;&#30740;&#31350;&#23545;&#35937;&#34920;&#31034;&#65292;&#24341;&#20837;&#20102;&#33258;&#25105;&#29366;&#24577;&#24863;&#30693;&#30340;&#27010;&#24565;&#20316;&#20026;&#31867;&#21035;&#23545;&#24212;&#33258;&#25105;&#24847;&#35782;&#30340;&#27169;&#25311;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#24378;&#21046;&#25191;&#34892;&#21644;&#35780;&#20272;&#31639;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#23545;&#35937;&#34920;&#31034;&#25193;&#23637;&#21040;&#22330;&#26223;&#34920;&#31034;&#65292;&#20351;&#29992;&#22270;&#34920;&#21644;&#30028;&#38480;&#65292;&#36825;&#20123;&#25104;&#20026;&#20102;&#25968;&#23398;&#24314;&#27169;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;AI&#23433;&#20840;&#30340;&#26500;&#24314;&#22359;&#12290;&#20316;&#20026;&#19968;&#20010;&#38468;&#24102;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#24341;&#20837;&#20102;&#21508;&#31181;&#33539;&#30068;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#21487;&#20197;&#29992;&#20316;&#27169;&#22411;&#35757;&#32451;&#30340;&#23545;&#20934;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can machines think? Since Alan Turing asked this question in 1950, nobody is able to give a direct answer, due to the lack of solid mathematical foundations for general intelligence. In this paper, we introduce a categorical framework towards this goal, with two main results. First, we investigate object representation through presheaves, introducing the notion of self-state awareness as a categorical analogue to self-consciousness, along with corresponding algorithms for its enforcement and evaluation. Secondly, we extend object representation to scenario representation using diagrams and limits, which then become building blocks for mathematical modeling, interpretability and AI safety. As an ancillary result, our framework introduces various categorical invariance properties that can serve as the alignment signals for model training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25903;&#25345;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#32858;&#31867;&#12289;&#25490;&#24207;&#32452;&#20214;&#65292;&#20174;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#30340;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#35760;&#24405;&#20013;&#29983;&#25104;&#20219;&#21153;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;ProceL&#21644;CrossTask&#25968;&#25454;&#38598;&#19978;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#20219;&#21153;&#22270;&#26356;&#21152;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2302.09173</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#36716;&#24405;&#20013;&#26080;&#30417;&#30563;&#29983;&#25104;&#20219;&#21153;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Task Graph Generation from Instructional Video Transcripts. (arXiv:2302.09173v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25903;&#25345;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#32858;&#31867;&#12289;&#25490;&#24207;&#32452;&#20214;&#65292;&#20174;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#30340;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#35760;&#24405;&#20013;&#29983;&#25104;&#20219;&#21153;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;ProceL&#21644;CrossTask&#25968;&#25454;&#38598;&#19978;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#20219;&#21153;&#22270;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#20219;&#21153;&#22270;&#30340;&#38382;&#39064;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#25552;&#20379;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#65288;&#22914;&#21046;&#20316;&#21654;&#21857;&#65289;&#30340;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#35760;&#24405;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20851;&#38190;&#27493;&#39588;&#21450;&#20854;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#38754;&#21521;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#32858;&#31867;&#21644;&#25490;&#24207;&#32452;&#20214;&#65292;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20934;&#30830;&#30340;&#20219;&#21153;&#22270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;ProceL&#21644;CrossTask&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#30340;&#20219;&#21153;&#22270;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the problem of generating task graphs of real-world activities. Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps. We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a completely unsupervised manner. We show that the proposed approach generates more accurate task graphs compared to a supervised learning approach on tasks from the ProceL and CrossTask datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#37327;&#21830;&#21153;&#25991;&#20214;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;&#20197;&#21450;&#34892;&#39033;&#30446;&#35782;&#21035;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;55&#20010;&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#36229;&#36807;&#20197;&#24448;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21253;&#25324;&#20247;&#22810;&#19981;&#21516;&#24067;&#23616;&#21644;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#30740;&#31350;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2302.05658</link><description>&lt;p&gt;
DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#29992;&#20110;&#25991;&#20214;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DocILE Benchmark for Document Information Localization and Extraction. (arXiv:2302.05658v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#37327;&#21830;&#21153;&#25991;&#20214;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;&#20197;&#21450;&#34892;&#39033;&#30446;&#35782;&#21035;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;55&#20010;&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#36229;&#36807;&#20197;&#24448;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21253;&#25324;&#20247;&#22810;&#19981;&#21516;&#24067;&#23616;&#21644;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#30740;&#31350;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20851;&#38190;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;&#20197;&#21450;&#34892;&#39033;&#30446;&#35782;&#21035;&#20219;&#21153;&#30340;&#21830;&#21153;&#25991;&#20214;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290; &#23427;&#21253;&#21547;6.7k&#20010;&#24102;&#27880;&#37322;&#30340;&#21830;&#21153;&#25991;&#20214;&#65292;100k&#20010;&#21512;&#25104;&#29983;&#25104;&#30340;&#25991;&#26723;&#20197;&#21450;&#36817;1M&#20010;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12290; &#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#29305;&#23450;&#20110;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#20855;&#26377;&#20197;&#19979;&#20851;&#38190;&#29305;&#24449;&#65306;&#65288;i&#65289;&#22312;55&#20010;&#31867;&#21035;&#20013;&#27880;&#37322;&#65292;&#20854;&#31890;&#24230;&#36828;&#36828;&#36229;&#36807;&#20197;&#21069;&#21457;&#24067;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;; &#65288;ii&#65289;&#34892;&#39033;&#30446;&#35782;&#21035;&#34920;&#31034;&#19968;&#39033;&#26497;&#20855;&#23454;&#29992;&#24615;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#22312;&#34920;&#26684;&#20013;&#24517;&#39035;&#23558;&#20851;&#38190;&#20449;&#24687;&#20998;&#37197;&#32473;&#39033;&#30446;; &#65288;iii&#65289;&#25991;&#26723;&#26469;&#33258;&#20247;&#22810;&#24067;&#23616;&#65292;&#27979;&#35797;&#38598;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#26696;&#20363;&#20197;&#21450;&#35757;&#32451;&#38598;&#20013;&#24120;&#35265;&#30340;&#24067;&#23616;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#37197;&#26377;&#22810;&#20010;&#22522;&#32447;&#65292;&#21253;&#25324;RoBERTa&#12289; LayoutLMv3&#21644;&#22522;&#20110;DETR&#30340;&#34920;&#26684;Transformer&#65307;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the DocILE benchmark with the largest dataset of business documents for the tasks of Key Information Localization and Extraction and Line Item Recognition. It contains 6.7k annotated business documents, 100k synthetically generated documents, and nearly~1M unlabeled documents for unsupervised pre-training. The dataset has been built with knowledge of domainand task-specific aspects, resulting in the following key features: (i) annotations in 55 classes, which surpasses the granularity of previously published key information extraction datasets by a large margin; (ii) Line Item Recognition represents a highly practical information extraction task, where key information has to be assigned to items in a table; (iii) documents come from numerous layouts and the test set includes zero- and few-shot cases as well as layouts commonly seen in the training set. The benchmark comes with several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table Transformer; app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;APAM&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#38271;&#23614;&#21644;&#22122;&#22768;&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.03488</link><description>&lt;p&gt;
APAM&#65306;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#21644;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning. (arXiv:2302.03488v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;APAM&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#38271;&#23614;&#21644;&#22122;&#22768;&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#36890;&#24120;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#29305;&#28857;&#65292;&#36825;&#20123;&#38382;&#39064;&#20250;&#25361;&#25112;&#22797;&#26434;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;&#24120;&#29992;&#30340;&#37325;&#25277;&#26679;&#25216;&#26415;&#65288;&#22914;&#36807;&#37319;&#26679;&#25110;&#27424;&#37319;&#26679;&#65289;&#23481;&#26131;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#20511;&#21161;&#23569;&#37327;&#20803;&#25968;&#25454;&#23398;&#20064;&#25968;&#25454;&#26435;&#37325;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23545;&#20110;&#24369;&#34920;&#31034;&#25968;&#25454;&#30340;&#20248;&#28857;&#24840;&#21457;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#38271;&#23614;&#21644;&#22122;&#22768;&#26631;&#31614;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#36866;&#24212;&#38382;&#39064;&#22495;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#26032;&#21152;&#26435;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#21487;&#20197;&#23398;&#20064;&#26174;&#24335;&#21152;&#26435;&#20989;&#25968;&#24182;&#26681;&#25454;&#20803;&#25968;&#25454;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#39033;&#26435;&#37325;&#19978;&#36827;&#19968;&#27493;&#37319;&#29992;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#22810;&#39033;&#24335;&#25193;&#23637;&#21644;&#37325;&#28857;&#27491;&#21017;&#21270;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#26041;&#27861;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20197;&#20174;&#24369;&#34920;&#31034;&#31867;&#20013;&#23398;&#20064;&#26356;&#20855;&#21487;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#38271;&#23614;&#21644;&#22122;&#38899;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical natural language processing (NLP) tasks are commonly long-tailed with noisy labels. Those problems challenge the generalization and robustness of complex models such as Deep Neural Networks (DNNs). Some commonly used resampling techniques, such as oversampling or undersampling, could easily lead to overfitting. It is growing popular to learn the data weights leveraging a small amount of metadata. Besides, recent studies have shown the advantages of self-supervised pre-training, particularly to the under-represented data. In this work, we propose a general framework to handle the problem of both long-tail and noisy labels. The model is adapted to the domain of problems in a contrastive learning manner. The re-weighting module is a feed-forward network that learns explicit weighting functions and adapts weights according to metadata. The framework further adapts weights of terms in the loss function through a combination of the polynomial expansion of cross-entropy loss and foc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;&#65292;&#36890;&#36807;&#25193;&#23637;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#22312;&#35745;&#31639;&#24050;&#30693;&#26597;&#35810;&#22270;&#20687;&#30340;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.02865</link><description>&lt;p&gt;
&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#24674;&#22797;&#20102;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs. (arXiv:2302.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;&#65292;&#36890;&#36807;&#25193;&#23637;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#22312;&#35745;&#31639;&#24050;&#30693;&#26597;&#35810;&#22270;&#20687;&#30340;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#32534;&#30721;&#22120;&#34987;&#35777;&#26126;&#21487;&#20197;&#32763;&#36716;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65306;&#23427;&#20204;&#21487;&#20197;&#23558;&#27599;&#20010;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#32534;&#30721;&#25104;&#29983;&#25104;&#35813;&#22270;&#20687;&#30340;&#30495;&#23454;&#28508;&#21464;&#37327;&#65288;Zimmermann&#31561;&#20154;&#65292;2021&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#35266;&#23519;&#32467;&#26524;&#36890;&#24120;&#23384;&#22312;&#20869;&#22312;&#30340;&#27169;&#31946;&#24615;&#12290;&#20363;&#22914;&#65292;&#22270;&#20687;&#21487;&#33021;&#27169;&#31946;&#25110;&#21482;&#26174;&#31034;3D&#29289;&#20307;&#30340;2D&#35270;&#22270;&#65292;&#22240;&#27492;&#21487;&#33021;&#26377;&#22810;&#20010;&#28508;&#21464;&#37327;&#29983;&#25104;&#23427;&#20204;&#12290;&#36825;&#20351;&#24471;&#28508;&#21464;&#37327;&#30340;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#20855;&#26377;&#24322;&#26041;&#24046;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24120;&#35265;&#30340;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#65292;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#32780;&#19981;&#26159;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#20998;&#24067;&#24674;&#22797;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#27491;&#30830;&#21518;&#39564;&#20998;&#24067;&#65292;&#21253;&#25324;&#20854;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#20272;&#35745;&#65292;&#35813;&#20272;&#35745;&#23384;&#22312;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#26059;&#36716;&#12290;&#38500;&#20102;&#25552;&#20379;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20043;&#22806;&#65292;&#36825;&#20123;&#21518;&#39564;&#20998;&#24067;&#36824;&#20801;&#35768;&#22312;&#22270;&#20687;&#26816;&#32034;&#20013;&#35745;&#31639;&#21487;&#20449;&#21306;&#38388;&#12290;&#23427;&#20204;&#21253;&#25324;&#20855;&#26377;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#21516;&#30340;&#28508;&#21464;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given quer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.01567</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Online Error Detection in Cyber-Physical Systems. (arXiv:2302.01567v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#24615;&#26159;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20027;&#35201;&#30340;&#35774;&#35745;&#26631;&#20934;&#20043;&#19968;&#12290;&#36825;&#26159;&#30001;&#20110;CPS&#20013;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#20204;&#30340;&#22833;&#25928;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;CPS&#20013;&#20351;&#29992;&#24378;&#22823;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#26426;&#21046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20256;&#32479;&#30340;&#23481;&#38169;&#26041;&#27861;&#21253;&#25324;&#20887;&#20313;&#26102;&#38388;&#12289;&#30828;&#20214;&#12289;&#20449;&#24687;&#21644;/&#25110;&#36719;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38500;&#20102;&#20302;&#38169;&#35823;&#35206;&#30422;&#29575;&#22806;&#65292;&#36824;&#20250;&#24102;&#26469;&#26497;&#22823;&#30340;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability is one of the major design criteria in Cyber-Physical Systems (CPSs). This is because of the existence of some critical applications in CPSs and their failure is catastrophic. Therefore, employing strong error detection and correction mechanisms in CPSs is inevitable. CPSs are composed of a variety of units, including sensors, networks, and microcontrollers. Each of these units is probable to be in a faulty state at any time and the occurred fault can result in erroneous output. The fault may cause the units of CPS to malfunction and eventually crash. Traditional fault-tolerant approaches include redundancy time, hardware, information, and/or software. However, these approaches impose significant overheads besides their low error coverage, which limits their applicability. In addition, the interval between error occurrence and detection is too long in these approaches. In this paper, based on Deep Reinforcement Learning (DRL), a new error detection approach is proposed that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.01222</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#30340;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for medium-term wind power prediction based on temporal attention mechanisms. (arXiv:2302.01222v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#26159;&#19968;&#31181;&#24191;&#27867;&#20998;&#24067;&#12289;&#21487;&#20877;&#29983;&#21644;&#29615;&#20445;&#30340;&#33021;&#28304;&#65292;&#23545;&#32531;&#35299;&#20840;&#29699;&#21464;&#26262;&#21644;&#33021;&#28304;&#30701;&#32570;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#30830;&#23450;&#24615;&#21644;&#27874;&#21160;&#24615;&#65292;&#22823;&#35268;&#27169;&#39118;&#30005;&#31995;&#32479;&#30340;&#32593;&#26684;&#38598;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#21487;&#20197;&#20026;&#33021;&#37327;&#35843;&#24230;&#25552;&#20379;&#22522;&#26412;&#20381;&#25454;&#65292;&#22240;&#27492;&#31934;&#30830;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#21464;&#20998;&#27169;&#24335;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#23450;&#20041;&#20102;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;TPE-VMD-TFT&#26041;&#27861;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind energy is a widely distributed, recyclable and environmentally friendly energy source that plays an important role in mitigating global warming and energy shortages. Wind energy's uncertainty and fluctuating nature makes grid integration of large-scale wind energy systems challenging. Medium-term wind power forecasts can provide an essential basis for energy dispatch, so accurate wind power forecasts are essential. Much research has yielded excellent results in recent years. However, many of them require additional experimentation and analysis when applied to other data. In this paper, we propose a novel short-term forecasting framework by tree-structured parzen estimator (TPE) and decomposition algorithms. This framework defines the TPE-VMD-TFT method for 24-h and 48-h ahead wind power forecasting based on variational mode decomposition (VMD) and time fusion transformer (TFT). In the Engie wind dataset from the electricity company in France, the results show that the proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.06683</link><description>&lt;p&gt;
&#25163;&#26415;&#32858;&#21512;&#65306;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#21644;&#22810;&#26679;&#20219;&#21153;&#21327;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#19987;&#27880;&#20110;&#26816;&#27979;&#24739;&#32773;&#21487;&#33021;&#21516;&#26102;&#20986;&#29616;&#30340;&#19968;&#37096;&#20998;&#21457;&#29616;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20020;&#24202;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#21327;&#35843;&#23545;&#20110;&#32858;&#21512;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#26415;&#32858;&#21512;&#65292;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#30340;iid&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25163;&#26415;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25163;&#26415;&#32858;&#21512;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#30340;&#31574;&#30053;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21407;&#21017;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#26426;&#21046;&#24110;&#21161;&#27599;&#20010;&#26679;&#26412;&#25214;&#21040;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#21015;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#34920;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#22312;8&#20010;&#19981;&#21516;&#30340;NLP&#25968;&#25454;&#38598;&#19978;&#65292;&#33258;&#36866;&#24212;ICL&#26041;&#27861;&#30456;&#23545;&#20110;&#24120;&#35268;&#35774;&#32622;&#25552;&#39640;&#20102;40%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.10375</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#22522;&#20110;&#20449;&#24687;&#21387;&#32553;&#35270;&#35282;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#21462;&#21644;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering. (arXiv:2212.10375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10375
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21407;&#21017;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#26426;&#21046;&#24110;&#21161;&#27599;&#20010;&#26679;&#26412;&#25214;&#21040;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#21015;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#34920;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#22312;8&#20010;&#19981;&#21516;&#30340;NLP&#25968;&#25454;&#38598;&#19978;&#65292;&#33258;&#36866;&#24212;ICL&#26041;&#27861;&#30456;&#23545;&#20110;&#24120;&#35268;&#35774;&#32622;&#25552;&#39640;&#20102;40%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064; (ICL) &#20013;&#20855;&#26377;&#24778;&#20154;&#30340;&#23569;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#20173;&#28982;&#26222;&#36941;&#37319;&#29992;&#38543;&#26426;&#36873;&#21462;&#26679;&#26412;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#20570;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ICL&#21407;&#21017;&#65306;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#24341;&#20837;&#33258;&#36866;&#24212;&#26426;&#21046;&#26469;&#24110;&#21161;&#27599;&#20010;&#26679;&#26412;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#24471;&#21040;&#27491;&#30830;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#21015;&#65288;&#21363;&#36873;&#21462;&#21644;&#25490;&#24207;&#65289;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#34920;&#29616;&#12290;&#20026;&#20102;&#39564;&#35777;&#33258;&#36866;&#24212;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36873;&#25321;-&#25490;&#24207;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;&#29992;&#26032;&#30340;&#36873;&#25321;&#21644;&#25490;&#24207;&#31639;&#27861;&#23454;&#20363;&#21270;&#12290;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;NLP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;ICL&#26041;&#27861;&#30456;&#23545;&#20110;&#24120;&#35268;&#35774;&#32622;&#25552;&#39640;&#20102;40%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;&#33258;&#36866;&#24212;ICL&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#21363;&#21487;&#33021;&#36890;&#36807;&#26356;&#20808;&#36827;&#30340;&#31639;&#27861;&#26469;&#32553;&#23567;ICL&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#24067;&#20195;&#30721;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65306;https://github.com/jxlr/SAICL_iclr22&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example permutation (i.e., selection and ordering) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area: https://githu
&lt;/p&gt;</description></item><item><title>Pangu&#26159;&#19968;&#20010;&#27867;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#19982;&#29616;&#23454;&#29615;&#22659;&#30340;&#25509;&#36712;&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36776;&#21035;&#33021;&#21147;&#32780;&#38750;&#29983;&#25104;&#33021;&#21147;&#65292;&#30001;&#19968;&#20010;&#31526;&#21495;&#20195;&#29702;&#21644;&#19968;&#20010;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#24037;&#20316;&#12290;&#36825;&#19968;&#26041;&#26696;&#24050;&#32463;&#22312;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09736</link><description>&lt;p&gt;
&#19981;&#29983;&#25104;&#65292;&#36776;&#21035;&#65306;&#19968;&#31181;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#25509;&#36712;&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments. (arXiv:2212.09736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09736
&lt;/p&gt;
&lt;p&gt;
Pangu&#26159;&#19968;&#20010;&#27867;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#19982;&#29616;&#23454;&#29615;&#22659;&#30340;&#25509;&#36712;&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36776;&#21035;&#33021;&#21147;&#32780;&#38750;&#29983;&#25104;&#33021;&#21147;&#65292;&#30001;&#19968;&#20010;&#31526;&#21495;&#20195;&#29702;&#21644;&#19968;&#20010;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#24037;&#20316;&#12290;&#36825;&#19968;&#26041;&#26696;&#24050;&#32463;&#22312;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35821;&#35328;&#27169;&#22411;&#26368;&#32570;&#22833;&#30340;&#23601;&#26159;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#25509;&#36712;&#24615;&#12290;&#24050;&#26377;&#30340;&#30456;&#20851;&#24037;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30452;&#25509;&#29983;&#25104;&#35745;&#21010;&#65292;&#20197;&#20415;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#20197;&#36798;&#21040;&#39044;&#26399;&#30340;&#25928;&#26524;&#65292;&#36825;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#36127;&#25285;&#20102;&#30830;&#20445;&#35821;&#27861;&#27491;&#30830;&#24615;&#12289;&#24544;&#23454;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#37325;&#25285;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27867;&#29992;&#30340;&#26694;&#26550;Pangu&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#19982;&#29616;&#23454;&#29615;&#22659;&#30340;&#25509;&#36712;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36776;&#21035;&#33021;&#21147;&#32780;&#38750;&#29983;&#25104;&#33021;&#21147;&#65292;&#30001;&#19968;&#20010;&#31526;&#21495;&#20195;&#29702;&#21644;&#19968;&#20010;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#24037;&#20316;&#65306;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#25506;&#32034;&#20197;&#36880;&#27493;&#26500;&#24314;&#26377;&#25928;&#30340;&#35745;&#21010;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22791;&#36873;&#35745;&#21010;&#30340;&#21512;&#29702;&#24615;&#20197;&#24341;&#23548;&#25628;&#32034;&#36807;&#31243;&#12290;&#38024;&#23545;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#20855;&#26377;&#24222;&#22823;&#30340;&#29615;&#22659;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;Pangu&#30340;&#26174;&#33879;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#65306;BERT&#22522;&#35821;&#35328;&#27169;&#22411;&#24050;&#36275;&#22815;&#24212;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HB-MCP&#31639;&#27861;&#65292;&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#35299;&#20915;POMDP&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#28151;&#21512;&#32622;&#20449;&#24230;&#12290;&#35813;&#31639;&#27861;&#20248;&#20110;&#19981;&#25903;&#25345;&#28151;&#21512;&#32622;&#20449;&#24230;&#30340;&#26368;&#20808;&#36827;&#22312;&#32447;&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#22312;&#26356;&#22823;&#30340;&#35745;&#21010;&#33539;&#22260;&#20869;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.07735</link><description>&lt;p&gt;
&#28151;&#21512;&#32622;&#20449;&#24230;POMDP&#20013;&#30340;&#33945;&#29305;&#21345;&#32599;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Planning in Hybrid Belief POMDPs. (arXiv:2211.07735v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HB-MCP&#31639;&#27861;&#65292;&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#35299;&#20915;POMDP&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#28151;&#21512;&#32622;&#20449;&#24230;&#12290;&#35813;&#31639;&#27861;&#20248;&#20110;&#19981;&#25903;&#25345;&#28151;&#21512;&#32622;&#20449;&#24230;&#30340;&#26368;&#20808;&#36827;&#22312;&#32447;&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#22312;&#26356;&#22823;&#30340;&#35745;&#21010;&#33539;&#22260;&#20869;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#38382;&#39064;&#38656;&#35201;&#21516;&#26102;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#38543;&#26426;&#21464;&#37327;&#36827;&#34892;&#28151;&#21512;&#20449;&#24565;&#30340;&#25512;&#29702;&#65292;&#20294;&#22312;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#24773;&#20917;&#20960;&#20046;&#27809;&#26377;&#34987;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#22312;&#32447;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#27714;&#35299;&#22120;&#19981;&#30452;&#25509;&#25903;&#25345;&#28151;&#21512;&#20449;&#24565;&#12290;&#20316;&#20026;&#26412;&#39033;&#24037;&#20316;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#28151;&#21512;&#32622;&#20449;&#24230;&#33945;&#29305;&#21345;&#32599;&#35268;&#21010;&#65288;HB-MCP&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;POMDP&#65292;&#24182;&#20445;&#25345;&#28151;&#21512;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#19978;&#32622;&#20449;&#24230;&#65288;UCB&#65289;&#25506;&#32034;&#22870;&#21169;&#26469;&#25351;&#23548;&#20551;&#35774;&#26641;&#21644;&#20449;&#24565;&#26641;&#30340;&#22686;&#38271;&#12290;&#28982;&#21518;&#65292;&#22312;&#26410;&#35299;&#20915;&#25968;&#25454;&#20851;&#32852;&#23548;&#33268;&#22810;&#27169;&#24577;&#20449;&#24565;&#20551;&#35774;&#30340;&#39640;&#24230;&#27169;&#31946;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;HB-MCP&#20248;&#20110;&#19981;&#25903;&#25345;&#28151;&#21512;&#20449;&#24565;&#30340;&#26368;&#20808;&#36827;&#30340;&#22312;&#32447;&#27714;&#35299;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#35745;&#21010;&#33539;&#22260;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#30340;&#21021;&#22987;&#32622;&#20449;&#24230;&#37197;&#32622;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world problems often require reasoning about hybrid beliefs, over both discrete and continuous random variables. Yet, such a setting has hardly been investigated in the context of planning. Moreover, existing online Partially Observable Markov Decision Processes (POMDPs) solvers do not support hybrid beliefs directly. In particular, these solvers do not address the added computational burden due to an increasing number of hypotheses with the planning horizon, which can grow exponentially. As part of this work, we present a novel algorithm, Hybrid Belief Monte Carlo Planning (HB-MCP) that utilizes the Monte Carlo Tree Search (MCTS) algorithm to solve a POMDP while maintaining a hybrid belief. We illustrate how the upper confidence bound (UCB) exploration bonus can be leveraged to guide the growth of hypotheses trees alongside the belief trees. We then evaluate our approach in highly aliased simulated environments where unresolved data association leads to multi-modal belief hypothe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#25552;&#20986;&#20102;&#36755;&#20837;&#20381;&#36182;NMR&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2211.01317</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#27169;&#24577;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#20302;&#36164;&#28304;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming. (arXiv:2211.01317v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#25552;&#20986;&#20102;&#36755;&#20837;&#20381;&#36182;NMR&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#20219;&#21153;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243; (NMR) &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20302;&#36164;&#28304;&#38899;&#20048;&#20998;&#31867;&#12290;NMR&#26088;&#22312;&#36890;&#36807;&#20462;&#25913;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#28304;&#22495;&#37325;&#26032;&#35843;&#25972;&#29992;&#20110;&#30446;&#26631;&#22495;&#12290;&#38500;&#20102;&#24050;&#30693;&#30340;&#19982;&#36755;&#20837;&#26080;&#20851;&#30340;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#37325;&#26032;&#32534;&#31243;&#33539;&#24335;&#65306;&#36755;&#20837;&#20381;&#36182;NMR&#65292;&#20197;&#22686;&#21152;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#38899;&#39057;&#65289;&#30340;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#25104;&#21151;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#20004;&#31181;&#36755;&#20837;&#30456;&#20851;&#30340;NMR&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning (TL) approaches have shown promising results when handling tasks with limited training data. However, considerable memory and computational resources are often required for fine-tuning pre-trained neural networks with target domain data. In this work, we introduce a novel method for leveraging pre-trained models for low-resource (music) classification based on the concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a pre-trained model from a source domain to a target domain by modifying the input of a frozen pre-trained model. In addition to the known, input-independent, reprogramming method, we propose an advanced reprogramming paradigm: Input-dependent NMR, to increase adaptability to complex input data such as musical audio. Experimental results suggest that a neural model pre-trained on large-scale datasets can successfully perform music genre classification by using this reprogramming method. The two proposed Input-dependent NMR TL methods outpe
&lt;/p&gt;</description></item><item><title>ROBOT&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#33021;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10953</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#21457;&#29616;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Discovering Many Diverse Solutions with Bayesian Optimization. (arXiv:2210.10953v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10953
&lt;/p&gt;
&lt;p&gt;
ROBOT&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#33021;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ROBOT is a new Bayesian optimization method that can find a portfolio of high-performing diverse solutions, addressing the limitation of traditional single-objective Bayesian optimization methods that only seek to find a single best solution.
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#30340;&#39640;&#25928;&#20248;&#21270;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#23547;&#27714;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#22312;&#35299;&#20915;&#26041;&#26696;&#21518;&#26399;&#21487;&#33021;&#21464;&#24471;&#26840;&#25163;&#30340;&#24773;&#20917;&#19979;&#20250;&#26377;&#24456;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROBOT&#30340;&#25490;&#24207;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;ROBOT&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#21457;&#29616;&#22823;&#37327;&#39640;&#24615;&#33021;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#19982;&#23547;&#25214;&#21333;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#38656;&#35201;&#24456;&#23569;&#30340;&#39069;&#22806;&#20989;&#25968;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular approach for sample-efficient optimization of black-box objective functions. While BO has been successfully applied to a wide range of scientific applications, traditional approaches to single-objective BO only seek to find a single best solution. This can be a significant limitation in situations where solutions may later turn out to be intractable. For example, a designed molecule may turn out to violate constraints that can only be reasonably evaluated after the optimization process has concluded. To address this issue, we propose Rank-Ordered Bayesian Optimization with Trust-regions (ROBOT) which aims to find a portfolio of high-performing solutions that are diverse according to a user-specified diversity metric. We evaluate ROBOT on several real-world applications and show that it can discover large sets of high-performing diverse solutions while requiring few additional function evaluations compared to finding a single best solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#35299;&#26512;&#31574;&#30053;&#26799;&#24230;&#65288;APG&#65289;&#8221;&#30340;&#31163;&#32447;&#23398;&#20064;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#22312;&#36319;&#36394;&#35823;&#24046;&#19978;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#20223;&#30495;&#22120;&#31163;&#32447;&#35757;&#32451;&#25511;&#21046;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#22312;&#31639;&#21147;&#26377;&#38480;&#30340;&#31995;&#32479;&#19978;&#23454;&#29616;&#39640;&#25928;&#65292;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.13052</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#26512;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#39640;&#25928;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Training Efficient Controllers via Analytic Policy Gradient. (arXiv:2209.13052v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#35299;&#26512;&#31574;&#30053;&#26799;&#24230;&#65288;APG&#65289;&#8221;&#30340;&#31163;&#32447;&#23398;&#20064;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#22312;&#36319;&#36394;&#35823;&#24046;&#19978;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#20223;&#30495;&#22120;&#31163;&#32447;&#35757;&#32451;&#25511;&#21046;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#22312;&#31639;&#21147;&#26377;&#38480;&#30340;&#31995;&#32479;&#19978;&#23454;&#29616;&#39640;&#25928;&#65292;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25511;&#21046;&#35774;&#35745;&#24456;&#22797;&#26434;&#65292;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#20197;&#20934;&#30830;&#22320;&#36319;&#36394;&#36712;&#36857;&#12290;&#34429;&#28982;&#22312;&#32447;&#20248;&#21270;&#26041;&#27861;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#23454;&#29616;&#24456;&#22909;&#30340;&#36319;&#36394;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#39640;&#31639;&#21147;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#31163;&#32447;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20801;&#35768;&#26426;&#22120;&#20154;&#24555;&#36895;&#65292;&#39640;&#25928;&#22320;&#25191;&#34892;&#65292;&#20294;&#22312;&#36712;&#36857;&#36319;&#36394;&#20219;&#21153;&#20013;&#24456;&#38590;&#19982;MPC&#30340;&#20934;&#30830;&#24615;&#30456;&#21305;&#37197;&#12290;&#23545;&#20110;&#31639;&#21147;&#26377;&#38480;&#30340;&#31995;&#32479;&#65288;&#22914;&#31354;&#20013;&#39134;&#34892;&#22120;&#65289;&#65292;&#39640;&#25928;&#30340;&#25511;&#21046;&#22120;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#26512;&#31574;&#30053;&#26799;&#24230;&#65288;APG&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;APG&#21033;&#29992;&#21487;&#24494;&#20998;&#20223;&#30495;&#22120;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#22312;&#36319;&#36394;&#35823;&#24046;&#19978;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#31163;&#32447;&#35757;&#32451;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#26469;&#35299;&#20915;APG&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;CartPole&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control design for robotic systems is complex and often requires solving an optimization to follow a trajectory accurately. Online optimization approaches like Model Predictive Control (MPC) have been shown to achieve great tracking performance, but require high computing power. Conversely, learning-based offline optimization approaches, such as Reinforcement Learning (RL), allow fast and efficient execution on the robot but hardly match the accuracy of MPC in trajectory tracking tasks. In systems with limited compute, such as aerial vehicles, an accurate controller that is efficient at execution time is imperative. We propose an Analytic Policy Gradient (APG) method to tackle this problem. APG exploits the availability of differentiable simulators by training a controller offline with gradient descent on the tracking error. We address training instabilities that frequently occur with APG through curriculum learning and experiment on a widely used controls benchmark, the CartPole, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#22823;&#23567;&#26080;&#20154;&#26426;&#30340;&#21333;&#19968;&#36817;&#24748;&#20572;&#20301;&#32622;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#36866;&#24212;&#19981;&#21516;&#26080;&#20154;&#26426;&#21442;&#25968;&#21644;&#24178;&#25200;&#65292;&#23454;&#29616;&#24555;&#36895;&#36816;&#34892;&#24182;&#25104;&#21151;&#36866;&#24212;&#22806;&#37096;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2209.09232</link><description>&lt;p&gt;
&#23398;&#20064;&#36866;&#29992;&#20110;&#19981;&#21516;&#22823;&#23567;&#26080;&#20154;&#26426;&#30340;&#21333;&#19968;&#36817;&#24748;&#20572;&#20301;&#32622;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning a Single Near-hover Position Controller for Vastly Different Quadcopters. (arXiv:2209.09232v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#22823;&#23567;&#26080;&#20154;&#26426;&#30340;&#21333;&#19968;&#36817;&#24748;&#20572;&#20301;&#32622;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#36866;&#24212;&#19981;&#21516;&#26080;&#20154;&#26426;&#21442;&#25968;&#21644;&#24178;&#25200;&#65292;&#23454;&#29616;&#24555;&#36895;&#36816;&#34892;&#24182;&#25104;&#21151;&#36866;&#24212;&#22806;&#37096;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#36817;&#24748;&#20572;&#20301;&#32622;&#25511;&#21046;&#22120;&#65292;&#21487;&#37096;&#32626;&#21040;&#36136;&#37327;&#12289;&#22823;&#23567;&#21644;&#30005;&#26426;&#24120;&#25968;&#38750;&#24120;&#19981;&#21516;&#30340;&#22235;&#36724;&#39134;&#34892;&#22120;&#19978;&#65292;&#32780;&#19988;&#22312;&#36816;&#34892;&#26102;&#20063;&#33021;&#24555;&#36895;&#36866;&#24212;&#26410;&#30693;&#24178;&#25200;&#12290;&#20854;&#26680;&#24515;&#31639;&#27861;&#24605;&#24819;&#26159;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#22312;&#32447;&#36866;&#24212;&#19981;&#20165;&#24212;&#29992;&#20110;&#26080;&#20154;&#26426;&#30340;&#24178;&#25200;&#65292;&#36824;&#33021;&#36866;&#24212;&#30456;&#21516;&#26694;&#26550;&#20013;&#30340;&#26426;&#22120;&#20154;&#21160;&#24577;&#21644;&#30828;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#21442;&#25968;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#36825;&#23558;&#29992;&#20110;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#35843;&#33410;&#65292;&#21516;&#26102;&#20063;&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20165;&#23545;&#36825;&#20004;&#20010;&#32593;&#32476;&#36827;&#34892;&#22521;&#35757;&#65292;&#20197;&#39134;&#34892;&#22235;&#36724;&#39134;&#34892;&#22120;&#21040;&#30446;&#26631;&#20301;&#32622;&#24182;&#36991;&#20813;&#25758;&#21040;&#22320;&#38754;&#20026;&#30446;&#26631;&#12290;&#25105;&#20204;&#30452;&#25509;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#30456;&#21516;&#30340;&#25511;&#21046;&#22120;&#65292;&#22312;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#12289;&#22823;&#23567;&#12289;&#30005;&#26426;&#21644;&#34746;&#26059;&#26728;&#25512;&#21147;&#27604;&#30340;&#22235;&#36724;&#39134;&#34892;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#25104;&#21151;&#30340;&#36817;&#24748;&#20572;&#24615;&#33021;&#24182;&#24555;&#36895;&#36866;&#24212;&#36816;&#21160;&#21464;&#21270;&#20197;&#21450;&#22806;&#37096;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an adaptive near-hover position controller for quadcopters, which can be deployed to quadcopters of very different mass, size and motor constants, and also shows rapid adaptation to unknown disturbances during runtime. The core algorithmic idea is to learn a single policy that can adapt online at test time not only to the disturbances applied to the drone, but also to the robot dynamics and hardware in the same framework. We achieve this by training a neural network to estimate a latent representation of the robot and environment parameters, which is used to condition the behaviour of the controller, also represented as a neural network. We train both networks exclusively in simulation with the goal of flying the quadcopters to goal positions and avoiding crashes to the ground. We directly deploy the same controller trained in the simulation without any modifications on two quadcopters in the real world with differences in mass, size, motors, and propellers with mas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"ImGCL"&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#33258;&#36866;&#24212;&#22320;&#24179;&#34913;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#20174;&#26080;&#26631;&#31614;&#33410;&#28857;&#65288;&#22270;&#65289;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#25972;&#21512;&#22312;&#32447;&#32858;&#31867;&#21644;&#36880;&#27493;&#24179;&#34913;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#21306;&#20998;&#34920;&#31034;&#65292;&#23454;&#29616;&#19982;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.11332</link><description>&lt;p&gt;
ImGCL&#65306;&#37325;&#35775;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#22312;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node Classification. (arXiv:2205.11332v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"ImGCL"&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#33258;&#36866;&#24212;&#22320;&#24179;&#34913;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#20174;&#26080;&#26631;&#31614;&#33410;&#28857;&#65288;&#22270;&#65289;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#25972;&#21512;&#22312;&#32447;&#32858;&#31867;&#21644;&#36880;&#27493;&#24179;&#34913;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#21306;&#20998;&#34920;&#31034;&#65292;&#23454;&#29616;&#19982;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#30001;&#20110;&#20854;&#22312;&#26080;&#26631;&#31614;&#33410;&#28857;/&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#36229;&#20961;&#24615;&#33021;&#32780;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290; &#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#32473;&#23450;&#22270;&#24418;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#28508;&#22312;&#31867;&#21035;&#20998;&#24067;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#36825;&#31181;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#20998;&#31867;&#20998;&#24067;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#20102;GCL&#20013;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290; &#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;GCL&#26041;&#27861;&#26080;&#27861;&#33719;&#24471;&#26377;&#21306;&#21035;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#24046;&#21170;&#30340;&#24615;&#33021;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#30340;&#27491;&#24335;GCL&#26694;&#26550;&#65288;ImGCL&#65289;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#22320;&#24179;&#34913;&#20174;GCL&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21306;&#20998;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#19982;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL) has attracted a surge of attention due to its superior performance for learning node/graph representations without labels. However, in practice, the underlying class distribution of unlabeled nodes for the given graph is usually imbalanced. This highly imbalanced class distribution inevitably deteriorates the quality of learned node representations in GCL. Indeed, we empirically find that most state-of-the-art GCL methods cannot obtain discriminative representations and exhibit poor performance on imbalanced node classification. Motivated by this observation, we propose a principled GCL framework on Imbalanced node classification (ImGCL), which automatically and adaptively balances the representations learned from GCL without labels. Specifically, we first introduce the online clustering based progressively balanced sampling (PBS) method with theoretical rationale, which balances the training sets based on pseudo-labels obtained from learned representat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26799;&#24230;&#23545;&#40784;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26799;&#24230;&#23545;&#40784;&#25439;&#22833;&#22312;&#20195;&#29702;&#27169;&#22411;&#19978;&#20272;&#35745;&#20934;&#30830;&#26799;&#24230;&#65292;&#25552;&#39640;&#22312;&#23569;&#37327;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.09518</link><description>&lt;p&gt;
&#36890;&#36807;&#23569;&#37327;&#26597;&#35810;&#23454;&#29616;&#26799;&#24230;&#23545;&#40784;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gradient Aligned Attacks via a Few Queries. (arXiv:2205.09518v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26799;&#24230;&#23545;&#40784;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26799;&#24230;&#23545;&#40784;&#25439;&#22833;&#22312;&#20195;&#29702;&#27169;&#22411;&#19978;&#20272;&#35745;&#20934;&#30830;&#26799;&#24230;&#65292;&#25552;&#39640;&#22312;&#23569;&#37327;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#26597;&#35810;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#22312;&#25915;&#20987;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#21482;&#20801;&#35768;&#23569;&#37327;&#26597;&#35810;&#30340;&#26032;&#39062;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#40657;&#30418;&#26597;&#35810;&#25915;&#20987;&#23637;&#29616;&#20986;&#24456;&#20302;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#23545;&#40784;&#25915;&#20987;&#65288;GAA&#65289;&#65292;&#20351;&#29992;&#26799;&#24230;&#23545;&#40784;&#25439;&#22833;&#65288;GAL&#65289;&#22312;&#20195;&#29702;&#27169;&#22411;&#19978;&#20272;&#35745;&#20934;&#30830;&#26799;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#21463;&#23475;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box query attacks, which rely only on the output of the victim model, have proven to be effective in attacking deep learning models. However, existing black-box query attacks show low performance in a novel scenario where only a few queries are allowed. To address this issue, we propose gradient aligned attacks (GAA), which use the gradient aligned losses (GAL) we designed on the surrogate model to estimate the accurate gradient to improve the attack performance on the victim model. Specifically, we propose a gradient aligned mechanism to ensure that the derivatives of the loss function with respect to the logit vector have the same weight coefficients between the surrogate and victim models. Using this mechanism, we transform the cross-entropy (CE) loss and margin loss into gradient aligned forms, i.e. the gradient aligned CE or margin losses. These losses not only improve the attack performance of our gradient aligned attacks in the novel scenario but also increase the query ef
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2203.07648</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#31561;&#30740;&#31350;&#36827;&#23637;&#23578;&#26410;&#24191;&#27867;&#32771;&#34385;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#36825;&#19968;&#31867;&#21035;&#65288;&#21363;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#20869;&#30340;&#20132;&#27969;&#24847;&#20041;&#65289;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21487;&#36801;&#31227;&#33267;&#21508;&#31181;&#31038;&#20250;&#35821;&#29992;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#12289;&#20167;&#24680;&#35328;&#35770;&#12289;&#24189;&#40664;&#12289;&#35773;&#21050;&#65289;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#20197;&#21450;&#19968;&#33324;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#20363;&#22914;&#65292;&#19982;&#20004;&#20010;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#29992;20&#20010;&#35757;&#32451;&#26679;&#26412;&#24494;&#35843;&#26102;&#65292;&#24179;&#22343;F1&#20540;&#22312;16&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;11.66&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in representation and contrastive learning in NLP has not widely considered the class of \textit{sociopragmatic meaning} (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data, across both the general and few-shot settings. For example, compared to two popular pre-trained language models, our method obtains an improvement of $11.66$ average $F_1$ on $16$ datasets when fine-tuned on only $20$ training samples per dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36882;&#24402;&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;(RF-GNN)&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#28041;&#21450;&#22810;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#24555;&#36895;&#36817;&#20284;&#25512;&#26029;&#12290;&#22312;&#22810;&#20010;&#22270;&#24418;&#27169;&#22411;&#23478;&#26063;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;RF-GNN&#22312;&#34920;&#36798;&#24615;&#22270;&#24418;&#27169;&#22411;&#20013;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#25191;&#34892;&#25512;&#26029;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2107.05729</link><description>&lt;p&gt;
&#39640;&#38454;&#22270;&#24418;&#27169;&#22411;&#20013;&#22270;&#32593;&#32476;&#25512;&#26029;&#30340;&#19968;&#33324;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization of graph network inferences in higher-order graphical models. (arXiv:2107.05729v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.05729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36882;&#24402;&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;(RF-GNN)&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#28041;&#21450;&#22810;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#24555;&#36895;&#36817;&#20284;&#25512;&#26029;&#12290;&#22312;&#22810;&#20010;&#22270;&#24418;&#27169;&#22411;&#23478;&#26063;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;RF-GNN&#22312;&#34920;&#36798;&#24615;&#22270;&#24418;&#27169;&#22411;&#20013;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#25191;&#34892;&#25512;&#26029;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#22270;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#25551;&#36848;&#22797;&#26434;&#32479;&#35745;&#32467;&#26500;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20855;&#26377;&#20174;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#33218;&#21040;&#29702;&#35299;&#31070;&#32463;&#35745;&#31639;&#31561;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#23454;&#38469;&#24212;&#29992;&#12290;&#36825;&#20123;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#22312;&#19968;&#33324;&#22270;&#24418;&#26465;&#20214;&#19979;&#65292;&#22914;&#36793;&#38469;&#21270;&#31561;&#25512;&#26029;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#36825;&#20123;&#25512;&#26029;&#36890;&#24120;&#30001;&#20998;&#24067;&#24335;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65288;&#20363;&#22914;&#20449;&#20219;&#20256;&#25773;&#65289;&#36817;&#20284;&#65292;&#20294;&#26159;&#23427;&#19981;&#24635;&#33021;&#22312;&#20855;&#26377;&#24490;&#29615;&#30340;&#22270;&#24418;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#19981;&#24635;&#33021;&#36731;&#26494;&#25351;&#23450;&#22797;&#26434;&#30340;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#20855;&#26377;&#19981;&#21487;&#35745;&#31639;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#30340;&#34920;&#36798;&#24615;&#22270;&#24418;&#27169;&#22411;&#20013;&#65292;&#36825;&#20123;&#22256;&#38590;&#32463;&#24120;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#36882;&#24402;&#22240;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;RF-GNN&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#28041;&#21450;&#22810;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#24555;&#36895;&#36817;&#20284;&#25512;&#26029;&#12290;&#22312;&#20960;&#20010;&#22270;&#24418;&#27169;&#22411;&#23478;&#26063;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;RF-GNN&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#20998;&#24067;&#19979;&#30340;&#19968;&#33324;&#21270;&#65292;&#23637;&#31034;&#20102;RF-GNN&#22312;&#34920;&#36798;&#24615;&#22270;&#24418;&#27169;&#22411;&#20013;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#25191;&#34892;&#25512;&#26029;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic graphical models provide a powerful tool to describe complex statistical structure, with many real-world applications in science and engineering from controlling robotic arms to understanding neuronal computations. A major challenge for these graphical models is that inferences such as marginalization are intractable for general graphs. These inferences are often approximated by a distributed message-passing algorithm such as Belief Propagation, which does not always perform well on graphs with cycles, nor can it always be easily specified for complex continuous probability distributions. Such difficulties arise frequently in expressive graphical models that include intractable higher-order interactions. In this paper we define the Recurrent Factor Graph Neural Network (RF-GNN) to achieve fast approximate inference on graphical models that involve many-variable interactions. Experimental results on several families of graphical models demonstrate the out-of-distribution g
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;Uni-Encoder&#65292;&#35299;&#20915;&#20102;Cross-Encoder&#22810;&#27425;&#32534;&#30721;&#30456;&#21516;&#19978;&#19979;&#25991;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;Poly-Encoder&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#35813;&#33539;&#20363;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2106.01263</link><description>&lt;p&gt;
Uni-Encoder: &#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24555;&#36895;&#20934;&#30830;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01263
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;Uni-Encoder&#65292;&#35299;&#20915;&#20102;Cross-Encoder&#22810;&#27425;&#32534;&#30721;&#30456;&#21516;&#19978;&#19979;&#25991;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;Poly-Encoder&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#35813;&#33539;&#20363;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26679;&#26412;&#19982;&#25490;&#24207;&#26159;&#29616;&#20195;&#29983;&#25104;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#20174;&#29983;&#25104;&#30340;&#23569;&#37327;&#20505;&#36873;&#31572;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#31572;&#26696;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25490;&#24207;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#31216;&#20026;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#33539;&#20363;&#65292;&#35813;&#32534;&#30721;&#22120;&#20998;&#21035;&#23545;&#27599;&#20010;&#19978;&#19979;&#25991;-&#20505;&#36873;&#23545;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#26681;&#25454;&#20854;&#36866;&#24212;&#24230;&#24471;&#20998;&#23545;&#20505;&#36873;&#36827;&#34892;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#20132;&#21449;&#32534;&#30721;&#22120;&#20026;&#27599;&#20010;&#20505;&#36873;&#37325;&#22797;&#32534;&#30721;&#30456;&#21516;&#30340;&#20887;&#38271;&#19978;&#19979;&#25991;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;Poly-Encoder&#36890;&#36807;&#20943;&#23569;&#19978;&#19979;&#25991;&#21644;&#20505;&#36873;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#20195;&#20215;&#26159;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#31216;&#20026;Uni-Encoder&#65292;&#23427;&#20687;&#20132;&#21449;&#32534;&#30721;&#22120;&#19968;&#26679;&#23436;&#20840;&#20851;&#27880;&#27599;&#20010;&#20505;&#36873;&#23545;&#65292;&#21516;&#26102;&#20687;Poly-&#32534;&#30721;&#22120;&#19968;&#26679;&#21482;&#32534;&#30721;&#19968;&#27425;&#19978;&#19979;&#25991;&#12290;Uni-Encoder&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#38024;&#23545;&#25152;&#26377;&#20505;&#36873;&#20351;&#29992;&#30456;&#21516;&#30340;&#20301;&#32622;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-Encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps the full attention over each pair as in Cross-Encoder while only encoding the context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates
&lt;/p&gt;</description></item></channel></rss>