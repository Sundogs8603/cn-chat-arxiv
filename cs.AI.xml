<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#24182;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2307.08044</link><description>&lt;p&gt;
&#26580;&#24615;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65306;&#36890;&#36807;&#25490;&#21517;&#22238;&#24402;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression. (arXiv:2307.08044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#24182;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20107;&#20214;&#20998;&#26512;&#65292;&#20063;&#34987;&#31216;&#20026;&#29983;&#23384;&#20998;&#26512;&#65292;&#26088;&#22312;&#26681;&#25454;&#19968;&#32452;&#29305;&#24449;&#39044;&#27979;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#34987;&#25130;&#23614;&#30340;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#20351;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#22797;&#26434;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#21644;&#21152;&#36895;&#22833;&#25928;&#26102;&#38388;&#65288;AFT&#65289;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#19968;&#20123;&#20551;&#35774;&#65292;&#22914;&#27604;&#20363;&#39118;&#38505;&#21644;&#32447;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;AFT&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#30340;&#21442;&#25968;&#20998;&#24067;&#20551;&#35774;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21361;&#38505;&#27169;&#22411;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#25991;&#29486;&#20013;&#23545;&#20110;AFT&#30340;&#34920;&#31034;&#23398;&#20064;&#23578;&#26410;&#24191;&#27867;&#25506;&#32034;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#20197;&#21361;&#38505;&#20026;&#37325;&#28857;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#26356;&#21152;&#31616;&#21333;&#21644;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#26469;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-to-event analysis, also known as survival analysis, aims to predict the time of occurrence of an event, given a set of features. One of the major challenges in this area is dealing with censored data, which can make learning algorithms more complex. Traditional methods such as Cox's proportional hazards model and the accelerated failure time (AFT) model have been popular in this field, but they often require assumptions such as proportional hazards and linearity. In particular, the AFT models often require pre-specified parametric distributional assumptions. To improve predictive performance and alleviate strict assumptions, there have been many deep learning approaches for hazard-based models in recent years. However, representation learning for AFT has not been widely explored in the neural network literature, despite its simplicity and interpretability in comparison to hazard-focused methods. In this work, we introduce the Deep AFT Rank-regression model for Time-to-event predic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26469;&#35299;&#20915;&#39564;&#35777;&#35821;&#27861;&#27491;&#30830;&#21477;&#23376;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.08036</link><description>&lt;p&gt;
&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#23454;&#29616;&#23545;&#35821;&#27861;&#27491;&#30830;&#21477;&#23376;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Neural-Symbolic Approach Towards Identifying Grammatically Correct Sentences. (arXiv:2307.08036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08036
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26469;&#35299;&#20915;&#39564;&#35777;&#35821;&#27861;&#27491;&#30830;&#21477;&#23376;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36523;&#36793;&#30340;&#25991;&#26412;&#20869;&#23481;&#27599;&#22825;&#37117;&#22312;&#22686;&#38271;&#12290;&#22312;&#32593;&#19978;&#25253;&#32440;&#12289;&#21338;&#23458;&#25110;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#25105;&#20204;&#27491;&#22312;&#25776;&#20889;&#22823;&#37327;&#30340;&#25991;&#31456;&#12290;&#31867;&#20284;&#22320;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22914;&#35821;&#35328;&#27169;&#22411;&#25110;&#20256;&#32479;&#30340;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#27491;&#22312;&#21033;&#29992;&#20197;&#19978;&#25152;&#26377;&#20869;&#23481;&#65292;&#25552;&#39640;&#20182;&#20204;&#23398;&#20064;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#30340;&#20934;&#30830;&#24615;&#26469;&#24212;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25361;&#25112;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#33719;&#21462;&#26469;&#33258;&#26377;&#25928;&#26469;&#28304;&#30340;&#20889;&#20316;&#25991;&#26412;&#23545;&#20110;&#24212;&#23545;&#25991;&#26412;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#29978;&#33267;&#20195;&#35789;&#28040;&#35299;&#31561;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#35201;&#36827;&#34892;&#33391;&#22909;&#30340;&#25688;&#35201;&#65292;&#38656;&#35201;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21477;&#23376;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#24418;&#25104;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#27809;&#26377;&#33719;&#21462;&#21040;&#20889;&#24471;&#22909;&#30340;&#33521;&#25991;&#21477;&#23376;&#29978;&#33267;&#38750;&#26377;&#25928;&#30340;&#21477;&#23376;&#20250;&#24590;&#20040;&#26679;&#21602;&#65311;&#23613;&#31649;&#33719;&#24471;&#20889;&#24471;&#22909;&#30340;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#34987;&#24191;&#27867;&#35748;&#21487;&#65292;&#20294;&#25214;&#20986;&#39564;&#35777;&#23427;&#20204;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual content around us is growing on a daily basis. Numerous articles are being written as we speak on online newspapers, blogs, or social media. Similarly, recent advances in the AI field, like language models or traditional classic AI approaches, are utilizing all the above to improve their learned representation to tackle NLP challenges with human-like accuracy. It is commonly accepted that it is crucial to have access to well-written text from valid sources to tackle challenges like text summarization, question-answering, machine translation, or even pronoun resolution. For instance, to summarize well, one needs to select the most important sentences in order to concatenate them to form the summary. However, what happens if we do not have access to well-formed English sentences or even non-valid sentences? Despite the importance of having access to well-written sentences, figuring out ways to validate them is still an open area of research. To address this problem, we present a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36125;&#21494;&#26031;&#25512;&#29702;&#22312;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#21253;&#25324;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#24615;&#20445;&#35777;&#12289;&#25968;&#25454;&#25928;&#29575;&#20197;&#21450;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#24046;&#36317;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32463;&#20856;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08024</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#22312;&#25968;&#25454;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#21644;&#23433;&#20840;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference for data-efficient, explainable, and safe robotic motion planning: A review. (arXiv:2307.08024v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08024
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36125;&#21494;&#26031;&#25512;&#29702;&#22312;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#21253;&#25324;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#24615;&#20445;&#35777;&#12289;&#25968;&#25454;&#25928;&#29575;&#20197;&#21450;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#24046;&#36317;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32463;&#20856;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#22312;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#65292;&#21253;&#25324;&#31574;&#30053;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#23433;&#20840;&#24615;&#21644;&#26368;&#20339;&#24615;&#20445;&#35777;&#12289;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#25968;&#25454;&#25928;&#29575;&#20197;&#21450;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26102;&#20943;&#23567;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#36125;&#21494;&#26031;&#25512;&#29702;&#22312;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#28382;&#21518;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#20840;&#38754;&#29702;&#35770;&#12290;&#27492;&#22806;&#65292;&#36824;&#27809;&#26377;&#32508;&#36848;&#26469;&#24635;&#32467;&#36125;&#21494;&#26031;&#25512;&#29702;&#22312;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#36827;&#23637;&#65292;&#20197;&#20415;&#32473;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#31995;&#32479;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#27010;&#29575;&#29702;&#35770;&#65292;&#36825;&#20123;&#26159;&#35299;&#20915;&#22797;&#26434;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#39318;&#35201;&#21069;&#25552;&#12290;&#20854;&#27425;&#65292;&#32473;&#20986;&#20102;&#36125;&#21494;&#26031;&#20272;&#35745;&#29992;&#20110;&#20272;&#35745;&#31574;&#30053;&#25110;&#26410;&#30693;&#20989;&#25968;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#36825;&#20123;&#21518;&#39564;&#27010;&#29575;&#29992;&#20110;&#35745;&#31639;&#31574;&#30053;&#12290;&#31532;&#19977;&#65292;&#20171;&#32461;&#20102;&#32463;&#20856;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference has many advantages in robotic motion planning over four perspectives: The uncertainty quantification of the policy, safety (risk-aware) and optimum guarantees of robot motions, data-efficiency in training of reinforcement learning, and reducing the sim2real gap when the robot is applied to real-world tasks. However, the application of Bayesian inference in robotic motion planning is lagging behind the comprehensive theory of Bayesian inference. Further, there are no comprehensive reviews to summarize the progress of Bayesian inference to give researchers a systematic understanding in robotic motion planning. This paper first provides the probabilistic theories of Bayesian inference which are the preliminary of Bayesian inference for complex cases. Second, the Bayesian estimation is given to estimate the posterior of policies or unknown functions which are used to compute the policy. Third, the classical model-based Bayesian RL and model-free Bayesian RL algorithms f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#20803;&#32423;&#28151;&#21512;&#35757;&#32451;&#26694;&#26550;&#26469;&#35299;&#20915;&#35270;&#35273;&#21644;&#35821;&#35328;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31934;&#32454;&#30340;&#21333;&#20803;&#65292;&#24182;&#21033;&#29992;&#21333;&#20301;&#20869;&#29615;&#22659;&#30340;&#19981;&#21464;&#24615;&#65292;&#20351;&#24471;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#65292;&#20943;&#23569;&#20102;&#26333;&#20809;&#20559;&#24046;&#65292;&#24182;&#22312; TEACH&#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.08016</link><description>&lt;p&gt;
&#23558;&#20219;&#21153;&#20998;&#35299;&#65306;&#35270;&#35273;&#21644;&#35821;&#35328;&#20915;&#31574;&#30340;&#21333;&#20803;&#32423;&#28151;&#21512;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Breaking Down the Task: A Unit-Grained Hybrid Training Framework for Vision and Language Decision Making. (arXiv:2307.08016v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#20803;&#32423;&#28151;&#21512;&#35757;&#32451;&#26694;&#26550;&#26469;&#35299;&#20915;&#35270;&#35273;&#21644;&#35821;&#35328;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31934;&#32454;&#30340;&#21333;&#20803;&#65292;&#24182;&#21033;&#29992;&#21333;&#20301;&#20869;&#29615;&#22659;&#30340;&#19981;&#21464;&#24615;&#65292;&#20351;&#24471;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#65292;&#20943;&#23569;&#20102;&#26333;&#20809;&#20559;&#24046;&#65292;&#24182;&#22312; TEACH&#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#20915;&#31574;&#65288;VLDM&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#22797;&#26434;&#30340;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#23436;&#25104;&#28041;&#21450;&#29615;&#22659;&#23548;&#33322;&#21644;&#29289;&#20307;&#25805;&#32437;&#30340;&#32452;&#21512;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;VLDM&#20013;&#28041;&#21450;&#30340;&#38271;&#26399;&#34892;&#21160;&#24207;&#21015;&#20351;&#20219;&#21153;&#38590;&#20197;&#23398;&#20064;&#12290;&#20174;&#29615;&#22659;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#21457;&#29616;&#20219;&#21153;&#21487;&#20197;&#34987;&#32454;&#20998;&#20026;&#31934;&#32454;&#30340;&#8220;&#21333;&#20803;&#8221;&#65292;&#27599;&#20010;&#21333;&#20803;&#21253;&#21547;&#19968;&#20010;&#23548;&#33322;&#38454;&#27573;&#21644;&#19968;&#20010;&#20132;&#20114;&#38454;&#27573;&#12290;&#30001;&#20110;&#21333;&#20301;&#20869;&#30340;&#29615;&#22659;&#20445;&#25345;&#19981;&#21464;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#24471;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#65292;&#24182;&#20943;&#23569;&#20102;&#26333;&#20809;&#20559;&#24046;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#21333;&#20803;&#32423;&#37197;&#32622;&#65292;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24102;&#26377;&#20869;&#22312;&#24490;&#29615;&#29366;&#24577;&#30340;&#21333;&#20803;&#36716;&#25442;&#22120;&#65288;UT&#65289;&#65292;&#35813;&#29366;&#24577;&#32500;&#25252;&#30528;&#19968;&#20010;&#21333;&#20803;&#32423;&#30340;&#36328;&#27169;&#24577;&#20869;&#23384;&#12290;&#36890;&#36807;&#22312;TEACH&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vision language decision making (VLDM) is a challenging multimodal task. The agent have to understand complex human instructions and complete compositional tasks involving environment navigation and object manipulation. However, the long action sequences involved in VLDM make the task difficult to learn. From an environment perspective, we find that task episodes can be divided into fine-grained \textit{units}, each containing a navigation phase and an interaction phase. Since the environment within a unit stays unchanged, we propose a novel hybrid-training framework that enables active exploration in the environment and reduces the exposure bias. Such framework leverages the unit-grained configurations and is model-agnostic. Specifically, we design a Unit-Transformer (UT) with an intrinsic recurrent state that maintains a unit-scale cross-modal memory. Through extensive experiments on the TEACH benchmark, we demonstrate that our proposed framework outperforms existing state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22235;&#31181;&#35299;&#37322;&#24615;&#26041;&#27861;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#28909;&#22270;&#20998;&#26512;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#38024;&#23545;&#29305;&#23450;&#30149;&#29702;&#31867;&#21035;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.08003</link><description>&lt;p&gt;
SHAMSUL: &#21033;&#29992;&#26412;&#22320;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#21516;&#26102;&#28909;&#22270;&#20998;&#26512;&#20197;&#30740;&#31350;&#21307;&#23398;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods. (arXiv:2307.08003v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22235;&#31181;&#35299;&#37322;&#24615;&#26041;&#27861;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#28909;&#22270;&#20998;&#26512;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#38024;&#23545;&#29305;&#23450;&#30149;&#29702;&#31867;&#21035;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#24050;&#25104;&#20026;&#21307;&#30103;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#19968;&#20010;&#28909;&#38376;&#35758;&#39064;&#12290;&#36825;&#31181;&#20851;&#27880;&#26469;&#28304;&#20110;&#23545;&#36879;&#26126;&#24230;&#12289;&#27861;&#24459;&#21644;&#20262;&#29702;&#32771;&#34385;&#20197;&#21450;&#36825;&#20123;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#29983;&#25104;&#30340;&#39044;&#27979;&#30340;&#21307;&#23398;&#24847;&#20041;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22235;&#31181;&#24050;&#24314;&#31435;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;: &#23616;&#37096;&#21487;&#35299;&#37322;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;(LIME)&#12289;Shapley&#21152;&#24615;&#35299;&#37322;(SHAP)&#12289;&#26799;&#24230;&#21152;&#26435;&#31867;&#21035;&#28608;&#27963;&#26144;&#23556;(Grad-CAM)&#21644;&#23618;&#20869;&#30456;&#20851;&#20256;&#25773;(LRP)&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#26631;&#31614;&#22810;&#31867;&#33016;&#37096;&#25918;&#23556;&#23398;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#37322;&#19982;&#29305;&#23450;&#30149;&#29702;&#31867;&#21035;&#30456;&#20851;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#39044;&#27979;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#25552;&#20379;&#20102;&#20840;&#38754;&#21644;&#20844;&#27491;&#30340;&#35780;&#20272;&#65292;
&lt;/p&gt;
&lt;p&gt;
The interpretability of deep neural networks has become a subject of great interest within the medical and healthcare domain. This attention stems from concerns regarding transparency, legal and ethical considerations, and the medical significance of predictions generated by these deep neural networks in clinical decision support systems. To address this matter, our study delves into the application of four well-established interpretability methods: Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations (SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise Relevance Propagation (LRP). Leveraging the approach of transfer learning with a multi-label-multi-class chest radiography dataset, we aim to interpret predictions pertaining to specific pathology classes. Our analysis encompasses both single-label and multi-label predictions, providing a comprehensive and unbiased assessment through quantitative and qualitative investigations, w
&lt;/p&gt;</description></item><item><title>MargCTGAN&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#22312;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36890;&#36807;&#28155;&#21152;&#29305;&#24449;&#21305;&#37197;&#30340;&#21435;&#30456;&#20851;&#36793;&#38469;&#65292;&#25913;&#21892;&#20102;CTGAN&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#32479;&#35745;&#23646;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.07997</link><description>&lt;p&gt;
MargCTGAN: "&#36793;&#38469;&#21270;" &#26356;&#22909;&#30340;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;CTGAN&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MargCTGAN: A "Marginally'' Better CTGAN for the Low Sample Regime. (arXiv:2307.07997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07997
&lt;/p&gt;
&lt;p&gt;
MargCTGAN&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#22312;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36890;&#36807;&#28155;&#21152;&#29305;&#24449;&#21305;&#37197;&#30340;&#21435;&#30456;&#20851;&#36793;&#38469;&#65292;&#25913;&#21892;&#20102;CTGAN&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#32479;&#35745;&#23646;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#32780;&#26377;&#29992;&#30340;&#21512;&#25104;&#25968;&#25454;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#29992;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#32479;&#35745;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#31181;&#30095;&#24573;&#22312;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#20276;&#38543;&#30528;&#36825;&#20123;&#32479;&#35745;&#25351;&#26631;&#30340;&#36805;&#36895;&#24694;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#22120;&#22312;&#36793;&#38469;&#20998;&#24067;&#12289;&#21015;&#23545;&#30456;&#20851;&#24615;&#12289;&#32852;&#21512;&#20998;&#24067;&#21644;&#19979;&#28216;&#20219;&#21153;&#23454;&#29992;&#24615;&#34920;&#29616;&#31561;&#26041;&#38754;&#22312;&#39640;&#21040;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#35780;&#20272;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27969;&#34892;&#30340;CTGAN&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#36739;&#24046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MargCTGAN&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#29305;&#24449;&#21305;&#37197;&#30340;&#21435;&#30456;&#20851;&#36793;&#38469;&#65292;&#20174;&#32780;&#22312;&#19979;&#28216;&#23454;&#29992;&#24615;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#32479;&#35745;&#23646;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of realistic and useful synthetic data is significant. However, current evaluation methods for synthetic tabular data generation predominantly focus on downstream task usefulness, often neglecting the importance of statistical properties. This oversight becomes particularly prominent in low sample scenarios, accompanied by a swift deterioration of these statistical measures. In this paper, we address this issue by conducting an evaluation of three state-of-the-art synthetic tabular data generators based on their marginal distribution, column-pair correlation, joint distribution and downstream task utility performance across high to low sample regimes. The popular CTGAN model shows strong utility, but underperforms in low sample settings in terms of utility. To overcome this limitation, we propose MargCTGAN that adds feature matching of de-correlated marginals, which results in a consistent improvement in downstream utility as well as statistical properties of the syntheti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;Auto-Polynomial&#65292;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.07956</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Polynomial Filter Learning for Graph Neural Networks. (arXiv:2307.07956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;Auto-Polynomial&#65292;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#20316;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#25351;&#23548;&#21407;&#21017;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#22312;&#24314;&#27169;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#20449;&#21495;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#34920;&#36798;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#21019;&#26032;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Auto-Polynomial&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#33258;&#21160;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#26356;&#22909;&#30340;&#28388;&#27874;&#22120;&#65292;&#36866;&#24212;&#21508;&#31181;&#22797;&#26434;&#30340;&#22270;&#20449;&#21495;&#12290;&#32508;&#21512;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#22312;&#32771;&#34385;&#22810;&#31181;&#26631;&#31614;&#27604;&#20363;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polynomial graph filters have been widely used as guiding principles in the design of Graph Neural Networks (GNNs). Recently, the adaptive learning of the polynomial graph filters has demonstrated promising performance for modeling graph signals on both homophilic and heterophilic graphs, owning to their flexibility and expressiveness. In this work, we conduct a novel preliminary study to explore the potential and limitations of polynomial graph filter learning approaches, revealing a severe overfitting issue. To improve the effectiveness of polynomial graph filters, we propose Auto-Polynomial, a novel and general automated polynomial graph filter learning framework that efficiently learns better filters capable of adapting to various complex graph signals. Comprehensive experiments and ablation studies demonstrate significant and consistent performance improvements on both homophilic and heterophilic graphs across multiple learning settings considering various labeling ratios, which u
&lt;/p&gt;</description></item><item><title>MinT&#36890;&#36807;&#22810;&#35270;&#35282;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#19981;&#21516;&#26631;&#27880;&#39118;&#26684;&#30340;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07951</link><description>&lt;p&gt;
MinT: &#36890;&#36807;&#22810;&#35270;&#35282;&#24494;&#35843;&#25552;&#21319;&#25968;&#23398;&#25512;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning. (arXiv:2307.07951v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07951
&lt;/p&gt;
&lt;p&gt;
MinT&#36890;&#36807;&#22810;&#35270;&#35282;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#19981;&#21516;&#26631;&#27880;&#39118;&#26684;&#30340;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#65292;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#36827;&#34892;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#35768;&#22810;&#24403;&#21069;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#19987;&#38376;&#21270;LM&#65292;&#24182;&#19988;&#36807;&#24230;&#20381;&#36182;&#20110;&#24378;&#22823;&#20294;&#20302;&#25928;&#30340;&#22823;&#22411;LM&#65288;LLM&#65289;&#25152;&#25552;&#20379;&#30340;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#36991;&#20813;&#36807;&#24230;&#20381;&#36182;LLM&#25945;&#24072;&#30340;&#26032;&#24605;&#36335;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#26631;&#27880;&#39118;&#26684;&#30340;&#29616;&#26377;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#30340;&#22810;&#35270;&#35282;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19981;&#21516;&#30340;&#26631;&#27880;&#26684;&#24335;&#35270;&#20026;&#19981;&#21516;&#30340;&#8220;&#35270;&#22270;&#8221;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#21033;&#29992;&#23427;&#20204;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#25351;&#20196;&#38468;&#21152;&#21040;&#36755;&#20837;&#38382;&#39064;&#19978;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#19981;&#21516;&#26684;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#20351;LLaMA-7B&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#20808;&#21069;&#26041;&#27861;&#20197;&#21450;&#31934;&#24515;&#24314;&#31435;&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#27169;&#22411;&#21462;&#24471;&#20102;&#27963;&#36291;
&lt;/p&gt;
&lt;p&gt;
Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on knowledge distillation from powerful but inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different "views" and leverages them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables a LLaMA-7B model to outperform prior approaches that utilize knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.07944</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#26469;&#37325;&#26032;&#23457;&#35270;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#19982;&#20266;&#26631;&#31614;&#25216;&#26415;&#30340;&#36741;&#21161;&#24050;&#32463;&#25104;&#20026;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#65292;&#21407;&#22240;&#26159;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#20302;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#20849;&#23384;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21516;&#26102;&#23398;&#20064;&#26816;&#27979;&#25152;&#26377;&#31867;&#21035;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#24341;&#23548;&#19981;&#21516;&#20998;&#24067;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#29615;&#22659;&#24046;&#24322;&#65288;&#20363;&#22914;&#65292;&#20809;&#26463;&#25968;&#37327;&#65289;&#24102;&#26469;&#30340;&#24178;&#25200;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#22495;&#26816;&#26597;&#65288;CDE&#65289;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#23454;&#20363;&#22797;&#21046;&#31896;&#36148;&#21040;&#28304;&#29615;&#22659;&#20013;&#24182;&#27979;&#37327;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#21644;&#32531;&#35299;&#29289;&#20307;&#30340;&#36716;&#31227;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#32534;&#30721;&#29575;&#26368;&#22823;&#21270;&#65288;KECOR&#65289;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#30830;&#23450;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#28857;&#20113;&#65292;&#20197;&#26368;&#23567;&#21270;&#26631;&#27880;&#25152;&#38656;&#30340;&#27604;&#29305;&#25968;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#20943;&#36731;LiDAR&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#27880;&#37322;&#36127;&#25285;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.07942</link><description>&lt;p&gt;
KECOR:&#29992;&#20110;&#20027;&#21160;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20869;&#26680;&#32534;&#30721;&#29575;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection. (arXiv:2307.07942v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#32534;&#30721;&#29575;&#26368;&#22823;&#21270;&#65288;KECOR&#65289;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#30830;&#23450;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#28857;&#20113;&#65292;&#20197;&#26368;&#23567;&#21270;&#26631;&#27880;&#25152;&#38656;&#30340;&#27604;&#29305;&#25968;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#20943;&#36731;LiDAR&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#27880;&#37322;&#36127;&#25285;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#21487;&#38752;&#30340;&#22522;&#20110;LiDAR&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20854;&#25104;&#21151;&#21462;&#20915;&#20110;&#33719;&#21462;&#22823;&#37327;&#31934;&#30830;&#30340;3D&#27880;&#37322;&#12290;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#31614;&#21644;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#30340;&#31639;&#27861;&#26469;&#20943;&#36731;&#27880;&#37322;&#36127;&#25285;&#12290;&#23613;&#31649;AL&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#26410;&#26631;&#35760;&#28857;&#20113;&#65292;&#23548;&#33268;&#36873;&#25321;&#26356;&#22810;&#23454;&#20363;&#36827;&#34892;&#26631;&#35760;&#24182;&#38477;&#20302;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#26680;&#32534;&#30721;&#29575;&#26368;&#22823;&#21270;&#65288;KECOR&#65289;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#26088;&#22312;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#30830;&#23450;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#28857;&#20113;&#20197;&#33719;&#21462;&#26631;&#31614;&#12290;&#36138;&#23146;&#25628;&#32034;&#34987;&#24212;&#29992;&#20110;&#23547;&#25214;&#33021;&#22815;&#26368;&#22823;&#21270;&#32534;&#30721;&#28508;&#22312;&#29305;&#24449;&#25152;&#38656;&#30340;&#26368;&#23567;&#27604;&#29305;&#25968;&#30340;&#26399;&#26395;&#28857;&#20113;&#12290;&#20026;&#20102;&#30830;&#23450;&#25152;&#36873;&#26679;&#26412;&#30340;&#29420;&#29305;&#24615;&#21644;&#20449;&#24687;&#37327;&#65292;&#20174;&#27169;&#22411;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving a reliable LiDAR-based object detector in autonomous driving is paramount, but its success hinges on obtaining large amounts of precise 3D annotations. Active learning (AL) seeks to mitigate the annotation burden through algorithms that use fewer labels and can attain performance comparable to fully supervised learning. Although AL has shown promise, current approaches prioritize the selection of unlabeled point clouds with high uncertainty and/or diversity, leading to the selection of more instances for labeling and reduced computational efficiency. In this paper, we resort to a novel kernel coding rate maximization (KECOR) strategy which aims to identify the most informative point clouds to acquire labels through the lens of information theory. Greedy search is applied to seek desired point clouds that can maximize the minimal number of bits required to encode the latent features. To determine the uniqueness and informativeness of the selected samples from the model perspec
&lt;/p&gt;</description></item><item><title>GeoGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#20027;GPT&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19982;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38477;&#20302;&#38750;&#19987;&#19994;&#29992;&#25143;&#35299;&#20915;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#38376;&#27099;&#12290;</title><link>http://arxiv.org/abs/2307.07930</link><description>&lt;p&gt;
GeoGPT:&#36890;&#36807;&#33258;&#20027;GPT&#29702;&#35299;&#21644;&#22788;&#29702;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT. (arXiv:2307.07930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07930
&lt;/p&gt;
&lt;p&gt;
GeoGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#20027;GPT&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19982;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38477;&#20302;&#38750;&#19987;&#19994;&#29992;&#25143;&#35299;&#20915;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GIS&#20915;&#31574;&#32773;&#38656;&#35201;&#32467;&#21512;&#19968;&#31995;&#21015;&#30340;&#31354;&#38388;&#31639;&#27861;&#21644;&#25805;&#20316;&#26469;&#35299;&#20915;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#35821;&#20041;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19982;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38477;&#20302;&#38750;&#19987;&#19994;&#29992;&#25143;&#35299;&#20915;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-makers in GIS need to combine a series of spatial algorithms and operations to solve geospatial tasks. For example, in the task of facility siting, the Buffer tool is usually first used to locate areas close or away from some specific entities; then, the Intersect or Erase tool is used to select candidate areas satisfied multiple requirements. Though professionals can easily understand and solve these geospatial tasks by sequentially utilizing relevant tools, it is difficult for non-professionals to handle these problems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents strong performance in semantic understanding and reasoning. Especially, AutoGPT can further extend the capabilities of large language models (LLMs) by automatically reasoning and calling externally defined tools. Inspired by these studies, we attempt to lower the threshold of non-professional users to solve geospatial tasks by integrating the semantic understanding ability inherent in LLMs 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#31070;&#32463;&#26550;&#26500;&#26816;&#32034;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#21457;&#29616;&#30456;&#20284;&#31070;&#32463;&#26550;&#26500;&#26102;&#25152;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07919</link><description>&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Retrieval. (arXiv:2307.07919v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#31070;&#32463;&#26550;&#26500;&#26816;&#32034;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#21457;&#29616;&#30456;&#20284;&#31070;&#32463;&#26550;&#26500;&#26102;&#25152;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#35774;&#35745;&#30340;&#22686;&#21152;&#21644;&#29616;&#26377;&#31070;&#32463;&#26550;&#26500;&#30340;&#22823;&#37327;&#23384;&#22312;&#65292;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#23558;&#33258;&#24049;&#30340;&#36129;&#29486;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#27604;&#36739;&#65292;&#25110;&#32773;&#24314;&#31435;&#33258;&#24049;&#30340;&#35774;&#35745;&#19982;&#20854;&#20182;&#30456;&#20851;&#35774;&#35745;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20026;&#20102;&#20197;&#39640;&#25928;&#19988;&#33258;&#21160;&#30340;&#26041;&#24335;&#21457;&#29616;&#30456;&#20284;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#31070;&#32463;&#26550;&#26500;&#26816;&#32034;&#65292;&#23427;&#26816;&#32034;&#19968;&#32452;&#19982;&#26597;&#35810;&#31070;&#32463;&#26550;&#26500;&#20855;&#26377;&#30456;&#20284;&#35774;&#35745;&#30340;&#29616;&#26377;&#31070;&#32463;&#26550;&#26500;&#12290;&#30001;&#20110;&#31070;&#32463;&#26550;&#26500;&#20013;&#30340;&#22270;&#30340;&#22823;&#23567;&#21644;&#27169;&#24335;&#65292;&#29616;&#26377;&#30340;&#22270;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#33021;&#35299;&#20915;&#35745;&#31639;&#22270;&#38382;&#39064;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22270;&#20998;&#25104;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#37325;&#24314;&#23439;&#22270;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#23545;&#20154;&#24037;&#35774;&#35745;&#21644;&#21512;&#25104;&#31070;&#32463;&#26550;&#26500;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing number of new neural architecture designs and substantial existing neural architectures, it becomes difficult for the researchers to situate their contributions compared with existing neural architectures or establish the connections between their designs and other relevant ones. To discover similar neural architectures in an efficient and automatic manner, we define a new problem Neural Architecture Retrieval which retrieves a set of existing neural architectures which have similar designs to the query neural architecture. Existing graph pre-training strategies cannot address the computational graph in neural architectures due to the graph size and motifs. To fulfill this potential, we propose to divide the graph into motifs which are used to rebuild the macro graph to tackle these issues, and introduce multi-level contrastive learning to achieve accurate graph representation learning. Extensive evaluations on both human-designed and synthesized neural architecture
&lt;/p&gt;</description></item><item><title>DualMind&#20351;&#29992;&#21452;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#25511;&#21046;&#20219;&#21153;&#20013;&#23398;&#20064;&#20849;&#21516;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#27169;&#20223;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DualMind&#22312;MetaWorld&#21644;Habitat&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#24615;&#20195;&#29702;&#65292;&#20855;&#26377;&#36229;&#36807;50%&#21644;70%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.07909</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#30340;&#21482;&#26159;&#27169;&#20223;&#21527;&#65311;&#20855;&#26377;&#21452;&#38454;&#27573;&#35757;&#32451;&#30340;&#27867;&#21270;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training. (arXiv:2307.07909v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07909
&lt;/p&gt;
&lt;p&gt;
DualMind&#20351;&#29992;&#21452;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#25511;&#21046;&#20219;&#21153;&#20013;&#23398;&#20064;&#20849;&#21516;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#27169;&#20223;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DualMind&#22312;MetaWorld&#21644;Habitat&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#24615;&#20195;&#29702;&#65292;&#20855;&#26377;&#36229;&#36807;50%&#21644;70%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DualMind&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#24615;&#20195;&#29702;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#36807;&#24230;&#25311;&#21512;&#34892;&#20026;&#21644;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;DualMind&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#21452;&#38454;&#27573;&#8221;&#35757;&#32451;&#31574;&#30053;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#22312;&#19990;&#30028;&#20013;&#34892;&#21160;&#30340;&#26041;&#24335;&#12290;&#27169;&#22411;&#39318;&#20808;&#36890;&#36807;&#38024;&#23545;&#25511;&#21046;&#20219;&#21153;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#26469;&#23398;&#20064;&#22522;&#26412;&#30340;&#20849;&#21516;&#30693;&#35782;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#22522;&#20110;&#32473;&#23450;&#25552;&#31034;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290; DualMind&#21487;&#20197;&#22788;&#29702;&#36328;&#22495;&#12289;&#22330;&#26223;&#21644;&#20855;&#20307;&#38382;&#39064;&#65292;&#24182;&#20165;&#20351;&#29992;&#21333;&#32452;&#27169;&#22411;&#26435;&#37325;&#26469;&#25191;&#34892;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;MetaWorld&#21644;Habitat&#19978;&#35780;&#20272;&#20102;DualMind&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;Habitat&#21644;MetaWorld&#19978;&#30340;&#34920;&#29616;&#20998;&#21035;&#36229;&#36807;&#20102;&#20854;&#20182;&#36890;&#29992;&#24615;&#20195;&#29702;&#30340;50%&#21644;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DualMind, a generalist agent designed to tackle various decision-making tasks that addresses challenges posed by current methods, such as overfitting behaviors and dependence on task-specific fine-tuning. DualMind uses a novel "Dual-phase" training strategy that emulates how humans learn to act in the world. The model first learns fundamental common knowledge through a self-supervised objective tailored for control tasks and then learns how to make decisions based on different contexts through imitating behaviors conditioned on given prompts. DualMind can handle tasks across domains, scenes, and embodiments using just a single set of model weights and can execute zero-shot prompting without requiring task-specific fine-tuning. We evaluate DualMind on MetaWorld and Habitat through extensive experiments and demonstrate its superior generalizability compared to previous techniques, outperforming other generalist agents by over 50$\%$ and 70$\%$ on Habitat and MetaWorld, respe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32420;&#32500;&#23618;&#29255;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.07893</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#32420;&#32500;&#25104;&#22411;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#25968;&#25454;&#26377;&#38480;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations. (arXiv:2307.07893v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32420;&#32500;&#23618;&#29255;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#21160;&#21270;&#32420;&#32500;&#25104;&#22411;(AFP)&#30340;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#26377;&#32570;&#38519;&#26679;&#26412;&#65292;&#32780;&#36825;&#20123;&#26679;&#26412;&#24456;&#38590;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20174;&#22522;&#30784;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#27491;&#24120;&#26679;&#26412;&#21644;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#30340;&#20108;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#32420;&#32500;&#23618;&#29255;&#65288;tow&#65289;&#23545;&#32420;&#32500;&#38138;&#35774;&#34920;&#38754;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20998;&#21106;&#25104;&#23567;&#31383;&#21475;&#12290;&#20854;&#20013;&#19981;&#21253;&#21547;&#24322;&#24120;&#30340;&#31383;&#21475;&#23376;&#38598;&#20256;&#36882;&#32473;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#37325;&#26500;&#36755;&#20837;&#12290;&#22240;&#20026;&#33258;&#21160;&#32534;&#30721;&#22120;&#26159;&#29992;&#27491;&#24120;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#36825;&#20123;&#26679;&#26412;&#65292;&#23427;&#20135;&#29983;&#30340;&#37325;&#26500;&#27604;&#23545;&#20110;&#24322;&#24120;&#26679;&#26412;&#26356;&#31934;&#30830;&#12290;&#22240;&#27492;&#65292;&#37325;&#26500;&#35823;&#24046;&#30340;&#20540;&#34987;&#29992;&#20316;&#19968;&#20010;&#37327;&#21270;&#25351;&#26631;&#65292;&#29992;&#20110;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#28508;&#22312;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current defect detection systems for Automated Fibre Placement (AFP) are mostly based on end-to-end supervised learning methods requiring abundant labelled defective samples, which are not easily generated in sufficient numbers. To address this data scarcity problem, we introduce an autoencoder-based approach compatible with small datasets. Fortunately, the problem from a foundational point of view can be simplified as a binary classification between normal and abnormal samples. The proposed approach uses a depth map of the fibre layup surface, split into small windows aligned to each composite strip (tow). A subset of these windows that do not contain anomalies is passed to an autoencoder to reconstruct the input. Because the autoencoder is trained with normal samples, it produces more accurate reconstructions for these samples than for abnormal ones. Therefore, the value of reconstruction error is used as a quantitative metric for whether there are potential anomalies. These values a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.07887</link><description>&lt;p&gt;
&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#65306;&#19968;&#20010;&#31614;&#21517;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;&#25195;&#25551;&#25991;&#26723;&#26102;&#65292;&#25163;&#20889;&#25991;&#26412;&#21487;&#33021;&#35206;&#30422;&#25171;&#21360;&#25991;&#26412;&#12290;&#36825;&#22312;&#25991;&#26723;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#36896;&#25104;&#22256;&#38590;&#65292;&#24182;&#19988;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20165;&#20851;&#27880;&#25163;&#20889;&#25991;&#26412;&#30340;&#20108;&#20998;&#31867;&#65292;&#35201;&#20040;&#36827;&#34892;&#19977;&#31867;&#25991;&#26723;&#30340;&#20998;&#21106;&#65292;&#21363;&#25163;&#20889;&#12289;&#25171;&#21360;&#21644;&#32972;&#26223;&#20687;&#32032;&#30340;&#35782;&#21035;&#12290;&#36825;&#23548;&#33268;&#25163;&#20889;&#21644;&#25171;&#21360;&#37325;&#21472;&#30340;&#20687;&#32032;&#21482;&#34987;&#20998;&#37197;&#21040;&#19968;&#20010;&#31867;&#21035;&#20013;&#65292;&#22240;&#27492;&#22312;&#21478;&#19968;&#20010;&#31867;&#21035;&#20013;&#19981;&#34987;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#30446;&#26631;&#26159;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#37325;&#21472;&#37096;&#20998;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;&#30495;&#23454;&#30340;&#27861;&#24459;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#20013;&#20351;&#29992;&#30690;&#37327;&#34920;&#36798;&#36827;&#34892;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#21644;&#25552;&#39640;&#35782;&#21035;&#36895;&#24230;&#65292;&#20351;&#20854;&#25104;&#20026;&#39318;&#20010;&#21487;&#29992;&#20110;&#24555;&#36895;&#21464;&#21160;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#22312;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07876</link><description>&lt;p&gt;
&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#20013;&#20351;&#29992;&#30690;&#37327;&#34920;&#36798;&#36827;&#34892;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Online Goal Recognition in Discrete and Continuous Domains Using a Vectorial Representation. (arXiv:2307.07876v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#20013;&#20351;&#29992;&#30690;&#37327;&#34920;&#36798;&#36827;&#34892;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#21644;&#25552;&#39640;&#35782;&#21035;&#36895;&#24230;&#65292;&#20351;&#20854;&#25104;&#20026;&#39318;&#20010;&#21487;&#29992;&#20110;&#24555;&#36895;&#21464;&#21160;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#22312;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20302;&#21487;&#35266;&#27979;&#24615;&#19979;&#26377;&#25928;&#25512;&#26029;&#30446;&#26631;&#65292;&#32780;&#23545;&#20110;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#20013;&#37117;&#33021;&#24037;&#20316;&#30340;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#27599;&#20010;&#26032;&#30340;&#35266;&#27979;&#20013;&#22810;&#27425;&#35843;&#29992;&#35268;&#21010;&#22120;&#65292;&#36896;&#25104;&#20102;&#24456;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#24555;&#36895;&#32780;&#21487;&#38752;&#22320;&#35782;&#21035;&#30446;&#26631;&#23545;&#20110;&#20219;&#20309;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#30495;&#23454;&#29289;&#29702;&#19990;&#30028;&#26159;&#24555;&#36895;&#21464;&#21160;&#30340;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#31163;&#25955;&#22495;&#20013;&#27599;&#20010;&#21487;&#33021;&#30340;&#30446;&#26631;&#21482;&#38656;&#35201;&#19968;&#27425;&#35843;&#29992;&#35268;&#21010;&#22120;&#65292;&#32780;&#22312;&#36830;&#32493;&#22495;&#20013;&#21033;&#29992;&#31616;&#21270;&#30340;&#36816;&#21160;&#27169;&#22411;&#26469;&#20943;&#23567;&#35745;&#31639;&#36127;&#25285;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26041;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22312;&#32447;&#35782;&#21035;&#26041;&#38754;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20351;&#20043;&#25104;&#20026;&#39318;&#20010;&#23454;&#38469;&#21487;&#29992;&#20110;&#38656;&#35201;&#20122;&#31186;&#32423;&#21709;&#24212;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#22312;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent work on online goal recognition efficiently infers goals under low observability, comparatively less work focuses on online goal recognition that works in both discrete and continuous domains. Online goal recognition approaches often rely on repeated calls to the planner at each new observation, incurring high computational costs. Recognizing goals online in continuous space quickly and reliably is critical for any trajectory planning problem since the real physical world is fast-moving, e.g. robot applications. We develop an efficient method for goal recognition that relies either on a single call to the planner for each possible goal in discrete domains or a simplified motion model that reduces the computational burden in continuous ones. The resulting approach performs the online component of recognition orders of magnitude faster than the current state of the art, making it the first online method effectively usable for robotics applications that require sub-second rec
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#32570;&#20047;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.07872</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#26159;&#21542;&#20250;&#21457;&#29983;&#21452;&#19979;&#38477;&#29616;&#35937;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Double Descent Occur in Self-Supervised Learning?. (arXiv:2307.07872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07872
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#32570;&#20047;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20851;&#20110;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#30417;&#30563;&#27169;&#22411;&#19978;&#65292;&#32780;&#23545;&#20110;&#33258;&#30417;&#30563;&#35774;&#32622;&#30340;&#30740;&#31350;&#21364;&#21457;&#29616;&#36825;&#31181;&#29616;&#35937;&#30340;&#32570;&#22833;&#20196;&#20154;&#24778;&#35766;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33258;&#30417;&#30563;&#27169;&#22411;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#21644;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#26469;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#27979;&#35797;&#25439;&#22833;&#35201;&#20040;&#21576;&#29616;&#32463;&#20856;&#30340;U&#22411;&#26354;&#32447;&#65292;&#35201;&#20040;&#21333;&#35843;&#36882;&#20943;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#21452;&#19979;&#38477;&#26354;&#32447;&#12290;&#25105;&#20204;&#24076;&#26395;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#33021;&#22815;&#25581;&#31034;&#36825;&#19968;&#29616;&#35937;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most investigations into double descent have focused on supervised models while the few works studying self-supervised settings find a surprising lack of the phenomenon. These results imply that double descent may not exist in self-supervised models. We show this empirically using a standard and linear autoencoder, two previously unstudied settings. The test loss is found to have either a classical U-shape or to monotonically decrease instead of exhibiting a double-descent curve. We hope that further work on this will help elucidate the theoretical underpinnings of this phenomenon.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#21457;&#23637;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#20351;&#20195;&#29702;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#24037;&#20855;&#20197;&#20415;&#20110;&#36827;&#34892;&#30456;&#20851;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.07871</link><description>&lt;p&gt;
&#31038;&#20250;AI&#23398;&#26657;&#65306;&#20174;&#21457;&#23637;&#24515;&#29702;&#23398;&#21040;&#20154;&#24037;&#31038;&#20250;&#25991;&#21270;&#20195;&#29702;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents. (arXiv:2307.07871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#21457;&#23637;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#20351;&#20195;&#29702;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#24037;&#20855;&#20197;&#20415;&#20110;&#36827;&#34892;&#30456;&#20851;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#38271;&#26399;&#20197;&#26469;&#24050;&#32463;&#30830;&#31435;&#20102;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#22312;&#20154;&#31867;&#26234;&#21147;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#20837;&#12289;&#21442;&#19982;&#21644;&#20174;&#20154;&#31867;&#25991;&#21270;&#20013;&#21463;&#30410;&#12290;&#31038;&#20250;&#20132;&#20114;&#20195;&#29702;&#30340;AI&#30740;&#31350;&#22823;&#22810;&#20851;&#27880;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25991;&#21270;&#30340;&#20986;&#29616;&#65288;&#36890;&#24120;&#27809;&#26377;&#24378;&#28872;&#30340;&#21457;&#23637;&#24515;&#29702;&#23398;&#22522;&#30784;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;Michael Tomasello&#21644;Jerome Bruner&#30340;&#29702;&#35770;&#65292;&#20171;&#32461;&#20102;&#20182;&#20204;&#30340;&#19968;&#20123;&#27010;&#24565;&#65292;&#24182;&#27010;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#21644;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#8212;&#8212;&#19968;&#20010;&#21253;&#25324;&#23450;&#21046;&#21442;&#25968;&#21270;&#29615;&#22659;&#30340;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#20851;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;RL&#20195;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27492;&#31867;&#23454;&#39564;&#30340;&#31034;&#20363;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#21560;&#24341;AI&#31038;&#21306;&#22260;&#32469;&#36825;&#20123;&#27010;&#24565;&#36827;&#34892;&#35752;&#35770;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developmental psychologists have long-established the importance of socio-cognitive abilities in human intelligence. These abilities enable us to enter, participate and benefit from human culture. AI research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). We argue that AI research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture too. We discuss the theories of Michael Tomasello and Jerome Bruner to introduce some of their concepts to AI and outline key concepts and socio-cognitive abilities. We present The SocialAI school - a tool including a customizable parameterized uite of procedurally generated environments, which simplifies conducting experiments regarding those concepts. We show examples of such experiments with RL agents and Large Language Models. The main motivation of this work is to engage the AI community around the 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07870</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#21270;&#35282;&#24230;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07870
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#21472;&#21152;&#12290;LLMs&#34920;&#29616;&#20986;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#20135;&#29983;&#30340;&#35282;&#24230;&#32780;&#25913;&#21464;&#65288;&#19982;&#20154;&#31867;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#36890;&#24120;&#20855;&#26377;&#26356;&#19968;&#33268;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35282;&#24230;&#21487;&#25511;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#12289;VSM&#12289;IPIP&#65289;&#26469;&#30740;&#31350;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#35282;&#24230;&#32780;&#25913;&#21464;&#12290;&#36890;&#36807;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#31034;&#20013;&#65288;&#38544;&#24335;&#25110;&#26174;&#24335;&#65289;&#26263;&#31034;&#20102;&#26576;&#20123;&#20215;&#20540;&#35266;&#26102;&#65292;LLMs&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#26263;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#20998;&#31867;&#31639;&#27861;&#21644;SVM&#26680;&#20989;&#25968;&#22312;&#35910;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;RBF SVM&#26680;&#24515;&#31639;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.07863</link><description>&lt;p&gt;
&#26816;&#27979;&#35910;&#31867;&#30340;&#20998;&#31867;&#31639;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#26680;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans. (arXiv:2307.07863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#20998;&#31867;&#31639;&#27861;&#21644;SVM&#26680;&#20989;&#25968;&#22312;&#35910;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;RBF SVM&#26680;&#24515;&#31639;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26893;&#29289;&#32946;&#31181;&#24072;&#21644;&#20892;&#19994;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#35910;&#31867;&#25968;&#25454;&#38598;&#26469;&#35782;&#21035;&#29702;&#24819;&#29305;&#24449;&#12289;&#25239;&#30149;&#24615;&#21644;&#33829;&#20859;&#21547;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20316;&#29289;&#20135;&#37327;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20998;&#31867;&#31639;&#27861;&#65292;&#21253;&#25324;&#32447;&#24615;&#12289;&#22810;&#39033;&#24335;&#21644;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#65292;&#20197;&#21450;&#20854;&#20182;&#27969;&#34892;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#20998;&#26512;&#26159;&#22312;&#35910;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#20027;&#35201;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#26159;&#20934;&#30830;&#29575;&#65292;&#32780;RBF SVM&#26680;&#24515;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;93.34%&#65292;&#31934;&#30830;&#29575;92.61%&#65292;&#21484;&#22238;&#29575;92.35%&#21644;F1&#24471;&#20998;91.40%&#12290;&#38500;&#20102;&#29087;&#32451;&#30340;&#21487;&#35270;&#21270;&#21644;&#32463;&#39564;&#20998;&#26512;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#35843;&#32771;&#34385;&#19981;&#21516;&#30340;SVM&#31639;&#27861;&#26469;&#22788;&#29702;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plant breeders and agricultural researchers can increase crop productivity by identifying desirable features, disease resistance, and nutritional content by analysing the Dry Bean dataset. This study analyses and compares different Support Vector Machine (SVM) classification algorithms, namely linear, polynomial, and radial basis function (RBF), along with other popular classification algorithms. The analysis is performed on the Dry Bean Dataset, with PCA (Principal Component Analysis) conducted as a preprocessing step for dimensionality reduction. The primary evaluation metric used is accuracy, and the RBF SVM kernel algorithm achieves the highest Accuracy of 93.34%, Precision of 92.61%, Recall of 92.35% and F1 Score as 91.40%. Along with adept visualization and empirical analysis, this study offers valuable guidance by emphasizing the importance of considering different SVM algorithms for complex and non-linear structured datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#33258;&#21160;&#20572;&#36710;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#21551;&#21457;&#24335;&#20989;&#25968;&#26469;&#25429;&#25417;&#25628;&#32034;&#31354;&#38388;&#30340;&#19981;&#21516;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#24615;&#33021;&#21644;&#23454;&#29616;&#23454;&#26102;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2307.07857</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#33258;&#21160;&#20572;&#36710;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Multi-Heuristic Search-based Motion Planning for Automated Parking. (arXiv:2307.07857v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#33258;&#21160;&#20572;&#36710;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#21551;&#21457;&#24335;&#20989;&#25968;&#26469;&#25429;&#25417;&#25628;&#32034;&#31354;&#38388;&#30340;&#19981;&#21516;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#24615;&#33021;&#21644;&#23454;&#29616;&#23454;&#26102;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20572;&#36710;&#22330;&#25110;&#24314;&#31569;&#24037;&#22320;&#31561;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#36710;&#36742;&#30340;&#22823;&#25628;&#32034;&#31354;&#38388;&#21644;&#21160;&#21147;&#23398;&#32422;&#26463;&#65292;&#23454;&#26102;&#35268;&#21010;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35268;&#21010;&#22120;&#21033;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36807;&#20110;&#20381;&#36182;&#21333;&#20010;&#21551;&#21457;&#24335;&#20989;&#25968;&#30340;&#36136;&#37327;&#65292;&#29992;&#20110;&#24341;&#23548;&#25628;&#32034;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#36798;&#21040;&#21512;&#29702;&#30340;&#35745;&#31639;&#24615;&#33021;&#65292;&#23548;&#33268;&#36710;&#36742;&#21453;&#24212;&#19981;&#21450;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#65292;&#20801;&#35768;&#20351;&#29992;&#22810;&#20010;&#21551;&#21457;&#24335;&#20989;&#25968;&#21450;&#20854;&#21508;&#33258;&#30340;&#20248;&#21183;&#26469;&#25429;&#25417;&#32473;&#23450;&#25628;&#32034;&#31354;&#38388;&#30340;&#19981;&#21516;&#22797;&#26434;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27492;&#26041;&#27861;&#22312;&#27492;&#38382;&#39064;&#19978;&#23578;&#26410;&#34987;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#23450;&#20041;&#20102;&#22810;&#20010;&#21487;&#25509;&#21463;&#21644;&#38750;&#21487;&#25509;&#21463;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#65292;&#25193;&#23637;&#20102;&#21407;&#22987;&#30340;&#22810;&#21551;&#21457;&#24335;A*&#25628;&#32034;&#20197;&#36827;&#34892;&#21452;&#21521;&#20351;&#29992;&#24182;&#22788;&#29702;&#28151;&#21512;&#36830;&#32493;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In unstructured environments like parking lots or construction sites, due to the large search-space and kinodynamic constraints of the vehicle, it is challenging to achieve real-time planning. Several state-of-the-art planners utilize heuristic search-based algorithms. However, they heavily rely on the quality of the single heuristic function, used to guide the search. Therefore, they are not capable to achieve reasonable computational performance, resulting in unnecessary delays in the response of the vehicle. In this work, we are adopting a Multi-Heuristic Search approach, that enables the use of multiple heuristic functions and their individual advantages to capture different complexities of a given search space. Based on our knowledge, this approach was not used previously for this problem. For this purpose, multiple admissible and non-admissible heuristic functions are defined, the original Multi-Heuristic A* Search was extended for bidirectional use and dealing with hybrid contin
&lt;/p&gt;</description></item><item><title>AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.07851</link><description>&lt;p&gt;
AspectCSE: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07851
&lt;/p&gt;
&lt;p&gt;
AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#31895;&#30053;&#36817;&#20284;&#65292;&#20294;&#24573;&#30053;&#20102;&#20351;&#25991;&#26412;&#30456;&#20284;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#26041;&#38754;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#39044;&#27979;&#26356;&#21152;&#38024;&#23545;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AspectCSE&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;AspectCSE&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#21892;3.97%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;Wikidata&#30693;&#35782;&#22270;&#23646;&#24615;&#26469;&#35757;&#32451;&#22810;&#26041;&#38754;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;&#30456;&#20284;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#26041;&#38754;&#23884;&#20837;&#22312;&#29305;&#23450;&#26041;&#38754;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#36827;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
&lt;/p&gt;</description></item><item><title>AIOptimizer&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#24615;&#33021;&#20248;&#21270;&#24037;&#20855;&#21407;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;&#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#21892;&#36719;&#20214;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21487;&#36127;&#25285;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20934;&#30830;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#31561;&#35774;&#35745;&#22240;&#32032;&#12290;AIOptimizer&#36824;&#25552;&#20379;&#25925;&#38556;&#35782;&#21035;&#12289;&#25104;&#26412;&#20248;&#21270;&#24314;&#35758;&#12289;&#25928;&#29575;&#39044;&#27979;&#21644;&#21327;&#20316;&#31561;&#21151;&#33021;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#24341;&#25806;&#36827;&#34892;&#25104;&#26412;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.07846</link><description>&lt;p&gt;
AIOptimizer &#8212;&#8212;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#24615;&#33021;&#20248;&#21270;&#21407;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#25104;&#26412;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
AIOptimizer -- A reinforcement learning-based software performance optimisation prototype for cost minimisation. (arXiv:2307.07846v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07846
&lt;/p&gt;
&lt;p&gt;
AIOptimizer&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#24615;&#33021;&#20248;&#21270;&#24037;&#20855;&#21407;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;&#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#21892;&#36719;&#20214;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21487;&#36127;&#25285;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20934;&#30830;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#31561;&#35774;&#35745;&#22240;&#32032;&#12290;AIOptimizer&#36824;&#25552;&#20379;&#25925;&#38556;&#35782;&#21035;&#12289;&#25104;&#26412;&#20248;&#21270;&#24314;&#35758;&#12289;&#25928;&#29575;&#39044;&#27979;&#21644;&#21327;&#20316;&#31561;&#21151;&#33021;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#24341;&#25806;&#36827;&#34892;&#25104;&#26412;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25991;&#31456;&#20171;&#32461;&#20102;AIOptimizer&#65292;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#38477;&#20302;&#30340;&#36719;&#20214;&#24615;&#33021;&#20248;&#21270;&#24037;&#20855;&#30340;&#21407;&#22411;&#12290;AIOptimizer&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#21892;&#36719;&#20214;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21487;&#36127;&#25285;&#24615;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;AIOptimizer&#30340;&#35774;&#35745;&#22240;&#32032;&#65292;&#22914;&#20934;&#30830;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;&#20026;&#20102;&#25552;&#20379;&#26377;&#25928;&#30340;&#29992;&#25143;&#20013;&#24515;&#30340;&#24615;&#33021;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#24378;&#35843;&#20102;&#27169;&#22359;&#21270;&#35774;&#35745;&#12289;&#25968;&#25454;&#25910;&#38598;&#25216;&#26415;&#12289;&#25345;&#32493;&#23398;&#20064;&#21644;&#24377;&#24615;&#38598;&#25104;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#36824;&#35843;&#26597;&#20102;AIOptimizer&#30340;&#29305;&#24615;&#65292;&#22914;&#25925;&#38556;&#35782;&#21035;&#12289;&#25104;&#26412;&#20248;&#21270;&#24314;&#35758;&#12289;&#25928;&#29575;&#39044;&#27979;&#21644;&#21327;&#20316;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#20960;&#20010;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;AIOptimizer&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#24341;&#25806;&#36827;&#34892;&#25104;&#26412;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31361;&#20986;AIOptimizer&#20316;&#20026;&#19968;&#31181;&#21033;&#29992;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;&#25104;&#26412;&#20248;&#21270;&#30340;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research article introduces AIOptimizer, a prototype for a software performance optimisation tool based on cost reduction. AIOptimizer uses a recommendation system driven by reinforcement learning to improve software system efficiency and affordability. The paper highlights AIOptimizer's design factors, such as accuracy, adaptability, scalability, and user-friendliness. To provide effective and user-centric performance optimisation solutions, it emphasises the use of a modular design, data gathering techniques, continuous learning, and resilient integration. The article also investigates AIOptimizer features such as fault identification, cost optimisation recommendations, efficiency prediction, and cooperation. Furthermore, it explores several software development life cycle models and introduces AIOptimizer uses a reinforcement learning-based recommendation engine for cost optimisation. The purpose of this research study is to highlight AIOptimizer as a prototype that uses advanc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2307.07840</link><description>&lt;p&gt;
RegExplainer: &#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22238;&#24402;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#37322;&#25216;&#26415;&#22823;&#22810;&#38480;&#20110;&#29702;&#35299;&#20998;&#31867;&#20219;&#21153;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23547;&#27714;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65288;XAIG-R&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#36830;&#32493;&#26377;&#24207;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#24212;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;&#20026;&#20102;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;MixupExplainer&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#21644;&#22270;mixup&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#35299;&#37322;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07832</link><description>&lt;p&gt;
MixupExplainer:&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#36890;&#29992;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation. (arXiv:2307.07832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;MixupExplainer&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#21644;&#22270;mixup&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#35299;&#37322;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22240;&#20854;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#24448;&#24448;&#19981;&#21487;&#35299;&#37322;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20107;&#21518;&#23454;&#20363;&#32423;&#35299;&#37322;&#26041;&#27861;&#26469;&#29702;&#35299;GNN&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#21457;&#29616;&#35299;&#37322;&#35757;&#32451;&#36807;&#30340;GNN&#39044;&#27979;&#34892;&#20026;&#30340;&#23376;&#32467;&#26500;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#20013;&#29305;&#21035;&#24433;&#21709;&#35299;&#37322;&#36136;&#37327;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#20005;&#26684;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#25324;&#29420;&#31435;&#20110;&#26631;&#31614;&#30340;&#22270;&#21464;&#37327;&#30340;&#24191;&#20041;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#24418;&#24335;&#65292;&#31561;&#20215;&#20110;&#20256;&#32479;&#30340;GIB&#12290;&#21463;&#24191;&#20041;GIB&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;mixup&#26041;&#27861;&#65292;MixupExplainer&#65292;&#20855;&#26377;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#36861;&#36394;&#38142;&#25509;&#25512;&#33616;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#31561;&#21521;&#36317;&#31163;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;&#38750;&#32447;&#24615;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#20174;&#20960;&#20309;&#35270;&#35282;&#25506;&#32034;&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#20110;&#36861;&#36394;&#24615;&#30740;&#31350;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#20316;&#32773;&#22312;&#22810;&#20010;&#39033;&#30446;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#23545;&#20854;&#20182;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#36215;&#21040;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.07781</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38750;&#31561;&#21521;&#36317;&#31163;&#21644;&#32452;&#21512;&#25913;&#36827;&#36861;&#36394;&#38142;&#25509;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Improving Trace Link Recommendation by Using Non-Isotropic Distances and Combinations. (arXiv:2307.07781v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#36861;&#36394;&#38142;&#25509;&#25512;&#33616;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#31561;&#21521;&#36317;&#31163;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;&#38750;&#32447;&#24615;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#20174;&#20960;&#20309;&#35270;&#35282;&#25506;&#32034;&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#20110;&#36861;&#36394;&#24615;&#30740;&#31350;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#20316;&#32773;&#22312;&#22810;&#20010;&#39033;&#30446;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#23545;&#20854;&#20182;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#36215;&#21040;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#26500;&#20214;&#20043;&#38388;&#23384;&#22312;&#36861;&#36394;&#38142;&#25509;&#21487;&#20197;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#12289;&#32500;&#25252;&#21644;&#36816;&#33829;&#36807;&#31243;&#20013;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36861;&#36394;&#38142;&#25509;&#30340;&#21019;&#24314;&#21644;&#32500;&#25252;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24378;&#22823;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#23545;&#33258;&#21160;&#35745;&#31639;&#36861;&#36394;&#38142;&#25509;&#36827;&#34892;&#30740;&#31350;&#30340;&#21162;&#21147;&#36880;&#28176;&#22686;&#21152;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#30740;&#31350;&#29992;&#20110;&#35745;&#31639;&#36861;&#36394;&#38142;&#25509;&#30340;&#38750;&#32447;&#24615;&#30456;&#20284;&#24230;&#24230;&#37327;&#26102;&#25152;&#20570;&#30340;&#19968;&#20123;&#35266;&#23519;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20174;&#20960;&#20309;&#35270;&#35282;&#26469;&#30475;&#24453;&#35821;&#20041;&#30456;&#20284;&#24615;&#21487;&#20197;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#36861;&#36394;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#24320;&#28304;&#39033;&#30446;&#21644;&#20004;&#20010;&#24037;&#19994;&#39033;&#30446;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26356;&#20855;&#26222;&#36941;&#24615;&#65292;&#20063;&#21487;&#20197;&#20026;&#20854;&#20182;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of trace links between artifacts of the software development life cycle can improve the efficiency of many activities during software development, maintenance and operations. Unfortunately, the creation and maintenance of trace links is time-consuming and error-prone. Research efforts have been spent to automatically compute trace links and lately gained momentum, e.g., due to the availability of powerful tools in the area of natural language processing. In this paper, we report on some observations that we made during studying non-linear similarity measures for computing trace links. We argue, that taking a geometric viewpoint on semantic similarity can be helpful for future traceability research. We evaluated our observations on a dataset of four open source projects and two industrial projects. We furthermore point out that our findings are more general and can build the basis for other information retrieval problems as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#36890;&#36807;&#30830;&#23450;&#26367;&#20195;&#36335;&#24452;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#35266;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#24182;&#24110;&#21161;&#35782;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.07764</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI with counterfactual paths. (arXiv:2307.07764v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#36890;&#36807;&#30830;&#23450;&#26367;&#20195;&#36335;&#24452;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#35266;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#24182;&#24110;&#21161;&#35782;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#21407;&#21017;&#19978;&#26088;&#22312;&#20351;&#40657;&#30418;&#27169;&#22411;&#36879;&#26126;&#21487;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26465;&#20214;&#32622;&#25442;&#29983;&#25104;&#20102;&#21453;&#20107;&#23454;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#30830;&#23450;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#32467;&#26524;&#30340;&#26367;&#20195;&#36335;&#24452;&#26469;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#35299;&#37322;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#26816;&#26597;&#30693;&#35782;&#22270;&#35889;&#20013;&#36755;&#20837;&#25968;&#25454;&#30340;&#20551;&#35774;&#24615;&#21464;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#31995;&#32479;&#22320;&#39564;&#35777;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#26816;&#26597;&#23545;&#27169;&#22411;&#39044;&#27979;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#25110;&#29305;&#24449;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#30340;&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#26356;&#30452;&#35266;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) is an increasingly important area of research in machine learning, which in principle aims to make black-box models transparent and interpretable. In this paper, we propose a novel approach to XAI that uses counterfactual paths generated by conditional permutations. Our method provides counterfactual explanations by identifying alternative paths that could have led to different outcomes. The proposed method is particularly suitable for generating explanations based on counterfactual paths in knowledge graphs. By examining hypothetical changes to the input data in the knowledge graph, we can systematically validate the behaviour of the model and examine the features or combination of features that are most important to the model's predictions. Our approach provides a more intuitive and interpretable explanation for the model's behaviour than traditional feature weighting methods and can help identify and mitigate biases in the model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#21487;&#21464;&#24418;&#36816;&#21160;&#35843;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#36716;&#31227;&#12290;&#36890;&#36807;&#20960;&#20309;&#26680;&#20559;&#31227;&#21644;&#33258;&#36866;&#24212;&#26435;&#37325;&#35843;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#29305;&#24449;&#23545;&#40784;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#26381;&#35013;&#19978;&#30340;&#32467;&#26500;&#22270;&#26696;&#21644;&#19981;&#36830;&#32493;&#30340;&#23039;&#21183;&#36716;&#31227;&#65292;&#24182;&#25552;&#20379;&#26356;&#21152;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07754</link><description>&lt;p&gt;
&#21452;&#21521;&#21487;&#21464;&#24418;&#36816;&#21160;&#35843;&#21046;&#29992;&#20110;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer. (arXiv:2307.07754v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07754
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#21487;&#21464;&#24418;&#36816;&#21160;&#35843;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#36716;&#31227;&#12290;&#36890;&#36807;&#20960;&#20309;&#26680;&#20559;&#31227;&#21644;&#33258;&#36866;&#24212;&#26435;&#37325;&#35843;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#29305;&#24449;&#23545;&#40784;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#26381;&#35013;&#19978;&#30340;&#32467;&#26500;&#22270;&#26696;&#21644;&#19981;&#36830;&#32493;&#30340;&#23039;&#21183;&#36716;&#31227;&#65292;&#24182;&#25552;&#20379;&#26356;&#21152;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#36716;&#31227;&#26159;&#19968;&#20010;&#23558;&#26222;&#36890;&#28304;&#20154;&#20307;&#22270;&#20687;&#26681;&#25454;&#19968;&#31995;&#21015;&#30446;&#26631;&#20154;&#29289;&#23039;&#24577;&#36827;&#34892;&#21160;&#30011;&#21270;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#12290;&#37492;&#20110;&#22312;&#26381;&#35013;&#30340;&#39640;&#24230;&#32467;&#26500;&#24615;&#22270;&#26696;&#21644;&#19981;&#36830;&#32493;&#30340;&#23039;&#21183;&#36716;&#31227;&#19978;&#23384;&#22312;&#30340;&#22256;&#38590;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#65292;&#22914;&#25197;&#26354;&#30340;&#32441;&#29702;&#21644;&#38378;&#28865;&#30340;&#20266;&#24433;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21464;&#24418;&#36816;&#21160;&#35843;&#21046;&#65288;DMM&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20960;&#20309;&#26680;&#20559;&#31227;&#21644;&#33258;&#36866;&#24212;&#26435;&#37325;&#35843;&#21046;&#26469;&#21516;&#26102;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;&#19982;&#22312;&#39118;&#26684;&#36716;&#31227;&#20013;&#20351;&#29992;&#30340;&#26222;&#36890;&#39118;&#26684;&#35843;&#21046;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#35843;&#21046;&#26426;&#21046;&#36890;&#36807;&#19968;&#31181;&#38750;&#35268;&#21017;&#24863;&#21463;&#37326;&#26681;&#25454;&#23545;&#35937;&#24418;&#29366;&#33258;&#36866;&#24212;&#37325;&#26500;&#24179;&#28369;&#24103;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#36716;&#31227;&#12290;&#20026;&#22686;&#24378;&#26102;&#31354;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#21452;&#21521;&#20256;&#25773;&#20174;&#30001;&#22122;&#22768;&#23039;&#21183;&#29983;&#25104;&#30340;&#30072;&#21464;&#22270;&#20687;&#24207;&#21015;&#20013;&#25552;&#21462;&#38544;&#34255;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-based human pose transfer is a video-to-video generation task that animates a plain source human image based on a series of target human poses. Considering the difficulties in transferring highly structural patterns on the garments and discontinuous poses, existing methods often generate unsatisfactory results such as distorted textures and flickering artifacts. To address these issues, we propose a novel Deformable Motion Modulation (DMM) that utilizes geometric kernel offset with adaptive weight modulation to simultaneously perform feature alignment and style transfer. Different from normal style modulation used in style transfer, the proposed modulation mechanism adaptively reconstructs smoothed frames from style codes according to the object shape through an irregular receptive field of view. To enhance the spatio-temporal consistency, we leverage bidirectional propagation to extract the hidden motion information from a warped image sequence generated by noisy poses. The prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#39564;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25193;&#23637;&#21644;&#32467;&#26500;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20316;&#20026;&#25512;&#24191;&#30340;&#20449;&#24687;&#20808;&#39564;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#24191;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#19978;&#25552;&#20379;&#20102;&#34920;&#36798;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#24182;&#20135;&#29983;&#20102;&#38750;&#31354;&#25512;&#24191;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26159;&#25512;&#23548;&#20986;&#21487;&#22788;&#29702;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#25512;&#24191;&#30028;&#38480;&#35745;&#31639;&#26041;&#27861;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25512;&#24191;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07753</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#24615;&#20808;&#39564;&#65292;&#25552;&#39640;&#25512;&#24191;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks. (arXiv:2307.07753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#39564;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25193;&#23637;&#21644;&#32467;&#26500;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20316;&#20026;&#25512;&#24191;&#30340;&#20449;&#24687;&#20808;&#39564;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#24191;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#19978;&#25552;&#20379;&#20102;&#34920;&#36798;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#24182;&#20135;&#29983;&#20102;&#38750;&#31354;&#25512;&#24191;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26159;&#25512;&#23548;&#20986;&#21487;&#22788;&#29702;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#25512;&#24191;&#30028;&#38480;&#35745;&#31639;&#26041;&#27861;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25512;&#24191;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20808;&#39564;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25512;&#24191;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#21487;&#25193;&#23637;&#21644;&#32467;&#26500;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20316;&#20026;&#20855;&#26377;&#25512;&#24191;&#20445;&#35777;&#30340;&#20449;&#24687;&#20808;&#39564;&#12290;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#22312;&#22823;&#35268;&#27169;&#19978;&#25552;&#20379;&#20102;&#34920;&#36798;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#31867;&#20284;&#20110;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#65292;&#24182;&#36827;&#19968;&#27493;&#20135;&#29983;&#20102;&#38750;&#31354;&#25512;&#24191;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20010;&#24819;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#30340;&#20808;&#39564;&#30340;&#26377;&#21033;&#29305;&#24615;&#26159;&#21487;&#21462;&#30340;&#12290;&#20027;&#35201;&#30340;&#25512;&#21160;&#22240;&#32032;&#26159;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#65306;(1) Kronecker&#31215;&#27714;&#21644;&#30340;&#35745;&#31639;&#65292;(2) &#25512;&#23548;&#21644;&#20248;&#21270;&#21487;&#22788;&#29702;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#32780;&#23548;&#33268;&#25913;&#36827;&#30340;&#25512;&#24191;&#30028;&#38480;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35814;&#23613;&#22320;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25512;&#24191;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel prior learning method for advancing generalization and uncertainty estimation in deep neural networks. The key idea is to exploit scalable and structured posteriors of neural networks as informative priors with generalization guarantees. Our learned priors provide expressive probabilistic representations at large scale, like Bayesian counterparts of pre-trained models on ImageNet, and further produce non-vacuous generalization bounds. We also extend this idea to a continual learning framework, where the favorable properties of our priors are desirable. Major enablers are our technical contributions: (1) the sums-of-Kronecker-product computations, and (2) the derivations and optimizations of tractable objectives that lead to improved generalization bounds. Empirically, we exhaustively show the effectiveness of this method for uncertainty estimation and generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22235;&#36275;&#26426;&#22120;&#20154;&#31283;&#23450;&#27493;&#24577;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07752</link><description>&lt;p&gt;
&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#31283;&#23450;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#34892;&#36208;
&lt;/p&gt;
&lt;p&gt;
Combining model-predictive control and predictive reinforcement learning for stable quadrupedal robot locomotion. (arXiv:2307.07752v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22235;&#36275;&#26426;&#22120;&#20154;&#31283;&#23450;&#27493;&#24577;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#30340;&#27493;&#24577;&#29983;&#25104;&#26159;&#22235;&#36275;&#26426;&#22120;&#20154;&#34892;&#36208;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20250;&#24433;&#21709;&#21040;&#20854;&#20182;&#20851;&#38190;&#24615;&#33021;&#22240;&#32032;&#65292;&#27604;&#22914;&#22312;&#19981;&#24179;&#22374;&#22320;&#24418;&#19978;&#30340;&#26426;&#21160;&#24615;&#21644;&#21151;&#32791;&#12290;&#27493;&#24577;&#29983;&#25104;&#30340;&#31283;&#23450;&#24615;&#26469;&#33258;&#20110;&#23545;&#22235;&#36275;&#26426;&#22120;&#20154;&#36523;&#20307;&#19982;&#36816;&#21160;&#29615;&#22659;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#39640;&#25928;&#25511;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26159;&#19968;&#31181;&#24050;&#32463;&#24456;&#25104;&#29087;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20351;&#29992;&#20219;&#20309;&#22312;&#32447;&#23398;&#20064;&#65288;&#38500;&#20102;&#19968;&#20123;&#33258;&#36866;&#24212;&#21464;&#21270;&#65289;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#26041;&#20415;&#30340;&#29366;&#24577;&#32422;&#26463;&#31649;&#29702;&#30028;&#38754;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20381;&#38752;&#22522;&#20110;&#32431;&#32463;&#39564;&#30340;&#36866;&#24212;&#24615;&#35843;&#25972;&#12290;&#22312;&#20854;&#22522;&#26412;&#24418;&#24335;&#20013;&#65292;&#30001;&#20110;&#26426;&#22120;&#20154;&#30340;&#22797;&#26434;&#24615;&#21644;&#26114;&#36149;&#30340;&#20223;&#30495;/&#23454;&#39564;&#38656;&#27714;&#65292;RL&#24182;&#19981;&#24635;&#26159;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#25511;&#21046;&#26041;&#27861;&#26469;&#35299;&#20915;&#22235;&#36275;&#26426;&#22120;&#20154;&#31283;&#23450;&#27493;&#24577;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable gait generation is a crucial problem for legged robot locomotion as this impacts other critical performance factors such as, e.g. mobility over an uneven terrain and power consumption. Gait generation stability results from the efficient control of the interaction between the legged robot's body and the environment where it moves. Here, we study how this can be achieved by a combination of model-predictive and predictive reinforcement learning controllers. Model-predictive control (MPC) is a well-established method that does not utilize any online learning (except for some adaptive variations) as it provides a convenient interface for state constraints management. Reinforcement learning (RL), in contrast, relies on adaptation based on pure experience. In its bare-bone variants, RL is not always suitable for robots due to their high complexity and expensive simulation/experimentation. In this work, we combine both control methods to address the quadrupedal robot stable gate gener
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SINC&#30340;&#33258;&#20027;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#36991;&#20813;&#20102;&#27169;&#26495;&#25935;&#24863;&#24615;&#21644;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07742</link><description>&lt;p&gt;
SINC: &#33258;&#20027;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SINC: Self-Supervised In-Context Learning for Vision-Language Tasks. (arXiv:2307.07742v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SINC&#30340;&#33258;&#20027;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#36991;&#20813;&#20102;&#27169;&#26495;&#25935;&#24863;&#24615;&#21644;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;Transformers&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24341;&#20154;&#20837;&#32988;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;&#30340;&#28436;&#31034;&#20013;&#65292;&#36805;&#36895;&#26500;&#24314;&#26032;&#30340;&#39044;&#27979;&#22120;&#65292;&#32780;&#26080;&#38656;&#26799;&#24230;&#26356;&#26032;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#22312;&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#20013;&#20419;&#36827;&#20102;&#36825;&#31181;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#20449;&#24687;&#34701;&#20837;&#21040;&#24050;&#32463;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32487;&#25215;&#20102;&#35821;&#35328;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#22914;&#27169;&#26495;&#25935;&#24863;&#24615;&#21644;&#20135;&#29983;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#25552;&#39640;&#20102;&#35745;&#31639;&#38656;&#27714;&#65292;&#20351;&#24471;&#23398;&#20064;&#21644;&#25805;&#20316;&#36825;&#20123;&#27169;&#22411;&#36164;&#28304;&#23494;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22914;&#20309;&#22312;&#19981;&#38480;&#21046;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#35753;&#36890;&#29992;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;&#8221;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#32780;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#33258;&#20027;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;SINC&#65289;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#25552;&#31034;&#20026;&#22522;&#30784;&#36827;&#34892;&#23398;&#20064;&#65292;&#36825;&#20123;&#25552;&#31034;&#21253;&#25324;&#37327;&#36523;&#23450;&#21046;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pre-trained Transformers exhibit an intriguing capacity for in-context learning. Without gradient updates, these models can rapidly construct new predictors from demonstrations presented in the inputs. Recent works promote this ability in the vision-language domain by incorporating visual information into large language models that can already make in-context predictions. However, these methods could inherit issues in the language domain, such as template sensitivity and hallucination. Also, the scale of these language models raises a significant demand for computations, making learning and operating these models resource-intensive. To this end, we raise a question: ``How can we enable in-context learning for general models without being constrained on large language models?". To answer it, we propose a succinct and general framework, Self-supervised IN-Context learning (SINC), that introduces a meta-model to learn on self-supervised prompts consisting of tailored demonstrations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#27010;&#24565;&#21464;&#21270;&#35268;&#21017;&#30340;&#28145;&#24230;&#27169;&#22411;CRAB&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#21644;&#35299;&#26512;&#28508;&#31354;&#38388;&#20013;&#30340;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#22312;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#20013;&#26080;&#38656;&#36741;&#21161;&#30417;&#30563;&#30340;&#20840;&#23616;&#35268;&#21017;&#21457;&#29616;&#21644;&#31572;&#26696;&#29983;&#25104;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#20248;&#20110;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.07734</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#30340;&#27010;&#24565;&#21464;&#21270;&#35268;&#21017;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Abstracting Concept-Changing Rules for Solving Raven's Progressive Matrix Problems. (arXiv:2307.07734v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#27010;&#24565;&#21464;&#21270;&#35268;&#21017;&#30340;&#28145;&#24230;&#27169;&#22411;CRAB&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#21644;&#35299;&#26512;&#28508;&#31354;&#38388;&#20013;&#30340;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#22312;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#20013;&#26080;&#38656;&#36741;&#21161;&#30417;&#30563;&#30340;&#20840;&#23616;&#35268;&#21017;&#21457;&#29616;&#21644;&#31572;&#26696;&#29983;&#25104;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#20248;&#20110;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#33021;&#20013;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#26377;&#21161;&#20110;&#22312;&#26032;&#29615;&#22659;&#20013;&#21457;&#29616;&#28508;&#22312;&#35268;&#21017;&#12290; Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20505;&#36873;&#39033;&#20013;&#36873;&#25321;&#26469;&#23454;&#29616;&#26426;&#22120;&#26234;&#33021;&#20013;&#30340;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20197;&#29983;&#25104;&#31572;&#26696;&#30340;&#26041;&#24335;&#35299;&#20915;RPM&#21487;&#20197;&#22686;&#36827;&#23545;&#35268;&#21017;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#27714;&#35299;&#22120;&#22312;&#27809;&#26377;&#36741;&#21161;&#30417;&#30563;&#65288;&#20363;&#22914;&#65292;&#35268;&#21017;&#27880;&#37322;&#21644;&#20505;&#36873;&#39033;&#20013;&#30340;&#24178;&#25200;&#39033;&#65289;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#21457;&#29616;&#20840;&#23616;&#30340;&#27010;&#24565;&#21464;&#21270;&#35268;&#21017;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27010;&#24565;&#21464;&#21270;&#35268;&#21017;&#25552;&#21462;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;Concept-changing Rule ABstraction&#65292;CRAB&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#24182;&#35299;&#26512;&#28508;&#31354;&#38388;&#20013;&#30340;&#27010;&#24565;&#21464;&#21270;&#35268;&#21017;&#12290;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#36807;&#31243;&#65292;CRAB&#21487;&#20197;&#33258;&#21160;&#25552;&#21462;&#27599;&#20010;&#27010;&#24565;&#19978;&#22312;&#25968;&#25454;&#38598;&#19978;&#20849;&#20139;&#30340;&#20840;&#23616;&#35268;&#21017;&#65292;&#24182;&#24418;&#25104;&#21487;&#23398;&#20064;&#30340;&#20840;&#23616;&#35268;&#21017;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#19982;&#26080;&#36741;&#21161;&#30417;&#30563;&#35757;&#32451;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;CRAB&#22312;&#20219;&#24847;&#20301;&#32622;&#30340;&#35268;&#21017;&#21457;&#29616;&#21644;&#31572;&#26696;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abstract visual reasoning ability in human intelligence benefits discovering underlying rules in the novel environment. Raven's Progressive Matrix (RPM) is a classic test to realize such ability in machine intelligence by selecting from candidates. Recent studies suggest that solving RPM in an answer-generation way boosts a more in-depth understanding of rules. However, existing generative solvers cannot discover the global concept-changing rules without auxiliary supervision (e.g., rule annotations and distractors in candidate sets). To this end, we propose a deep latent variable model for Concept-changing Rule ABstraction (CRAB) by learning interpretable concepts and parsing concept-changing rules in the latent space. With the iterative learning process, CRAB can automatically abstract global rules shared on the dataset on each concept and form the learnable prior knowledge of global rules. CRAB outperforms the baselines trained without auxiliary supervision in the arbitrary-posi
&lt;/p&gt;</description></item><item><title>NeurASP&#26159;&#23558;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;Answer Set Programming&#20013;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#27010;&#29575;&#20998;&#24067;&#30340;&#24418;&#24335;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#65292;NeurASP&#33021;&#22815;&#23558;&#23376;&#31526;&#21495;&#21644;&#31526;&#21495;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#31526;&#21495;&#25512;&#29702;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#24863;&#30693;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ASP&#35268;&#21017;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20174;&#26174;&#24335;&#22797;&#26434;&#35821;&#20041;&#32422;&#26463;&#20013;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07700</link><description>&lt;p&gt;
NeurASP&#65306;&#23558;&#31070;&#32463;&#32593;&#32476;&#34701;&#20837;&#21040;Answer Set Programming &#20013;
&lt;/p&gt;
&lt;p&gt;
NeurASP: Embracing Neural Networks into Answer Set Programming. (arXiv:2307.07700v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07700
&lt;/p&gt;
&lt;p&gt;
NeurASP&#26159;&#23558;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;Answer Set Programming&#20013;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#27010;&#29575;&#20998;&#24067;&#30340;&#24418;&#24335;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#65292;NeurASP&#33021;&#22815;&#23558;&#23376;&#31526;&#21495;&#21644;&#31526;&#21495;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#31526;&#21495;&#25512;&#29702;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#24863;&#30693;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ASP&#35268;&#21017;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20174;&#26174;&#24335;&#22797;&#26434;&#35821;&#20041;&#32422;&#26463;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NeurASP&#65292;&#23427;&#26159;&#23545;Answer Set Programs&#30340;&#31616;&#21333;&#25193;&#23637;&#65292;&#36890;&#36807;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#35270;&#20026;Answer Set Programs&#20013;&#21407;&#23376;&#20107;&#23454;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;NeurASP&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#23558;&#23376;&#31526;&#21495;&#21644;&#31526;&#21495;&#35745;&#31639;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;NeurASP&#22914;&#20309;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;Answer Set Programming&#20013;&#30340;&#31526;&#21495;&#25512;&#29702;&#26469;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#24863;&#30693;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;NeurASP&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ASP&#35268;&#21017;&#36827;&#34892;&#35757;&#32451;&#26469;&#26356;&#22909;&#22320;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#19981;&#20165;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#38544;&#24335;&#30456;&#20851;&#24615;&#65292;&#36824;&#20174;&#35268;&#21017;&#25152;&#34920;&#31034;&#30340;&#26174;&#24335;&#22797;&#26434;&#35821;&#20041;&#32422;&#26463;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NeurASP, a simple extension of answer set programs by embracing neural networks. By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation. We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network's perception result by applying symbolic reasoning in answer set programming. Also, NeurASP can be used to train a neural network better by training with ASP rules so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07699</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31572;&#26696;&#38598;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models to Generate Answer Set Programs. (arXiv:2307.07699v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#21644;GPT-4&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#20986;&#35299;&#20915;&#26576;&#20123;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#37319;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#26377;&#38480;&#19988;&#30456;&#23545;&#27973;&#26174;&#12290;&#30456;&#21453;&#65292;&#24418;&#24335;&#36923;&#36753;&#25797;&#38271;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#65292;&#20294;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#24418;&#24335;&#36923;&#36753;&#26159;&#19968;&#20010;&#38750;&#19987;&#23478;&#38590;&#20197;&#24212;&#23545;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;LLM&#23558;&#36923;&#36753;&#35868;&#39064;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;LLM&#30340;&#25552;&#31034;&#65292;&#20197;&#36880;&#27493;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20165;&#20165;&#36890;&#36807;&#20960;&#20010;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#65292;LLMs&#23601;&#33021;&#29983;&#25104;&#30456;&#24403;&#22797;&#26434;&#30340;&#31572;&#26696;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated exceptional performance in various natural language processing tasks and have shown the ability to solve certain reasoning problems. However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques. In contrast, formal logic is adept at handling complex reasoning, but translating natural language descriptions into formal logic is a challenging task that non-experts struggle with. This paper proposes a neuro-symbolic method that combines the strengths of large language models and answer set programming. Specifically, we employ an LLM to transform natural language descriptions of logic puzzles into answer set programs. We carefully design prompts for an LLM to convert natural language descriptions into answer set programs in a step by step manner. Surprisingly, with just a few in-context learning examples, LLMs can generate reasonably complex answer se
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#32534;&#31243;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#31034;&#20363;&#21644;&#21487;&#37325;&#29992;&#30340;&#30693;&#35782;&#27169;&#22359;&#65292;&#21363;&#21487;&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07696</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#32534;&#31243;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#25991;&#26412;&#30340;&#24378;&#22823;&#21644;&#36890;&#29992;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text. (arXiv:2307.07696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07696
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#32534;&#31243;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#31034;&#20363;&#21644;&#21487;&#37325;&#29992;&#30340;&#30693;&#35782;&#27169;&#22359;&#65292;&#21363;&#21487;&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#40065;&#26834;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#36824;&#19981;&#33021;&#19982;&#38024;&#23545;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;&#26412;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#23569;&#31034;&#20363;&#35821;&#20041;&#35299;&#26512;&#22120;&#12290;&#23427;&#21487;&#20197;&#23558;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#20316;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#30340;&#36755;&#20837;&#65292;&#35813;&#31243;&#24207;&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#22768;&#26126;&#24615;&#30693;&#35782;&#34920;&#31034;&#24418;&#24335;&#12290;&#36825;&#31181;&#32452;&#21512;&#32467;&#26524;&#24418;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#21482;&#38656;&#35201;&#23569;&#37327;&#31034;&#20363;&#26469;&#25351;&#23548;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#20197;&#21450;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#30340;&#21487;&#37325;&#29992;ASP&#30693;&#35782;&#27169;&#22359;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#20960;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;bAbI&#12289;StepGame&#12289;CLUTRR&#21644;gSCAN&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25104;&#21151;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot pla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#25216;&#26415;&#65292;&#20854;&#20013;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#24067;&#23616;&#30340;&#26041;&#27861;&#12290;&#20171;&#32461;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25253;&#21578;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.07691</link><description>&lt;p&gt;
&#22312;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#25216;&#26415;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Change Detection Techniques in Document Images. (arXiv:2307.07691v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#25216;&#26415;&#65292;&#20854;&#20013;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#24067;&#23616;&#30340;&#26041;&#27861;&#12290;&#20171;&#32461;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25253;&#21578;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#30142;&#30149;&#35786;&#26029;&#12289;&#36890;&#36807;&#36965;&#24863;&#21457;&#29616;&#22478;&#24066;&#30340;&#22686;&#38271;&#27169;&#24335;&#20197;&#21450;&#26816;&#27979;&#27861;&#24459;&#25991;&#20214;&#21644;&#21512;&#21516;&#20013;&#30340;&#21464;&#21270;&#31561;&#19981;&#21516;&#39046;&#22495;&#20013;&#37117;&#26377;&#24212;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#25991;&#26723;&#22270;&#20687;&#30340;&#19981;&#21516;&#29256;&#26412;&#20013;&#26816;&#27979;&#21464;&#21270;&#30340;&#26680;&#24515;&#25216;&#26415;&#21644;&#35268;&#21017;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#20027;&#35201;&#35752;&#35770;&#20102;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#24067;&#23616;&#30340;&#20004;&#31181;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#12290;&#22522;&#20110;&#20869;&#23481;&#30340;&#25216;&#26415;&#26234;&#33021;&#25552;&#21462;&#21644;&#20998;&#26512;&#22270;&#20687;&#20869;&#23481;&#65288;&#25991;&#26412;&#25110;&#38750;&#25991;&#26412;&#65289;&#65292;&#20197;&#26174;&#31034;&#21487;&#33021;&#30340;&#24046;&#24322;&#65292;&#32780;&#22522;&#20110;&#24067;&#23616;&#30340;&#25216;&#26415;&#20351;&#29992;&#32467;&#26500;&#20449;&#24687;&#26469;&#39044;&#27979;&#25991;&#26723;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#22312;&#21464;&#21270;&#26816;&#27979;&#23454;&#39564;&#20013;&#20351;&#29992;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25253;&#21578;&#20102;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30340;&#19981;&#36275;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#24037;&#20316;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of change detection in images finds application in different domains like diagnosis of diseases in the medical field, detecting growth patterns of cities through remote sensing, and finding changes in legal documents and contracts. However, this paper presents a survey on core techniques and rules to detect changes in different versions of a document image. Our discussions on change detection focus on two categories -content-based and layout-based. The content-based techniques intelligently extract and analyze the image contents (text or non-text) to show the possible differences, whereas the layout-based techniques use structural information to predict document changes. We also summarize the existing datasets and evaluation metrics used in change detection experiments. The shortcomings and challenges the existing methods face are reported, along with some pointers for future research work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07686</link><description>&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#25903;&#25345;OpenMP Fortran&#21644;C++&#20195;&#30721;&#30456;&#20114;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#65288;CodeBLEU&#65289;&#21644;&#23450;&#24615;&#65288;&#20154;&#24037;&#35780;&#20272;&#65289;&#26041;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#32534;&#30721;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;5.1&#20493;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#23450;&#32534;&#30721;&#29087;&#24713;&#24230;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;9.9&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#20195;&#30721;&#32763;&#35793;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39318;&#20808;&#23637;&#31034;&#20102;&#21333;&#29420;&#36827;&#34892;&#21160;&#20316;&#27745;&#26579;&#21644;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#25915;&#20987;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#39640;&#25928;&#22320;&#25915;&#20987;MARL&#26234;&#33021;&#20307;&#65292;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.07670</link><description>&lt;p&gt;
&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning. (arXiv:2307.07670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39318;&#20808;&#23637;&#31034;&#20102;&#21333;&#29420;&#36827;&#34892;&#21160;&#20316;&#27745;&#26579;&#21644;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#25915;&#20987;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#39640;&#25928;&#22320;&#25915;&#20987;MARL&#26234;&#33021;&#20307;&#65292;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#20102;&#35299;&#23545;MARL&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#23545;&#20110;&#23433;&#20840;&#24212;&#29992;&#35813;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;MARL&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#32771;&#34385;&#30340;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#19968;&#20010;&#22806;&#37096;&#25915;&#20987;&#32773;&#65292;&#20182;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#25509;&#25910;&#21040;&#22870;&#21169;&#20043;&#21069;&#20462;&#25913;&#22870;&#21169;&#65292;&#25110;&#22312;&#29615;&#22659;&#25509;&#25910;&#21040;&#21160;&#20316;&#20043;&#21069;&#25805;&#32437;&#21160;&#20316;&#12290;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#26159;&#23558;&#27599;&#20010;&#26234;&#33021;&#20307;&#24341;&#23548;&#21040;&#30446;&#26631;&#31574;&#30053;&#65292;&#25110;&#22312;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#26576;&#20010;&#29305;&#23450;&#22870;&#21169;&#20989;&#25968;&#19979;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#21453;&#39304;&#21644;&#21160;&#20316;&#30340;&#25805;&#32437;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#21482;&#36827;&#34892;&#21160;&#20316;&#27745;&#26579;&#25915;&#20987;&#21644;&#21482;&#36827;&#34892;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21516;&#26102;&#36827;&#34892;&#21160;&#20316;&#27745;&#26579;&#21644;&#22870;&#21169;&#27745;&#26579;&#30340;&#28151;&#21512;&#25915;&#20987;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#28151;&#21512;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#39640;&#25928;&#22320;&#25915;&#20987;MARL&#26234;&#33021;&#20307;&#65292;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and the reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ARRLC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#36798;&#21040;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38750;&#40065;&#26834;&#31639;&#27861;&#24182;&#19988;&#25910;&#25947;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.07666</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#39640;&#25928;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ARRLC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#36798;&#21040;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38750;&#40065;&#26834;&#31639;&#27861;&#24182;&#19988;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#26088;&#22312;&#22312;&#19981;&#30830;&#23450;&#24615;&#38754;&#21069;&#25214;&#21040;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#20851;&#27880;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#65292;&#20854;&#20013;&#20195;&#29702;&#26426;&#22120;&#19981;&#24635;&#26159;&#25353;&#29031;&#31574;&#30053;&#25351;&#23450;&#30340;&#21160;&#20316;&#36827;&#34892;&#65292;&#32780;&#26159;&#20197;&#27010;&#29575;$1-\rho$&#25191;&#34892;&#31574;&#30053;&#25351;&#23450;&#30340;&#21160;&#20316;&#65292;&#20197;&#27010;&#29575;$\rho$&#25191;&#34892;&#26367;&#20195;&#30340;&#23545;&#25239;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23384;&#22312;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#20854;&#30340;&#34892;&#21160;&#40065;&#26834;&#36125;&#23572;&#26364;&#26368;&#20248;&#26041;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#35777;&#20070;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;(ARRLC)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;ARRLC&#20248;&#20110;&#38750;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#27604;&#40065;&#26834;TD&#31639;&#27861;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#30340;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#25439;&#22833;&#20989;&#25968;MPDIoU&#65292;&#24182;&#36890;&#36807;&#21253;&#21547;&#22810;&#20010;&#30456;&#20851;&#22240;&#32032;&#30340;&#26368;&#23567;&#28857;&#36317;&#31163;&#26469;&#27604;&#36739;&#36793;&#30028;&#26694;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07662</link><description>&lt;p&gt;
MPDIoU:&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#30340;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
MPDIoU: A Loss for Efficient and Accurate Bounding Box Regression. (arXiv:2307.07662v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#30340;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#25439;&#22833;&#20989;&#25968;MPDIoU&#65292;&#24182;&#36890;&#36807;&#21253;&#21547;&#22810;&#20010;&#30456;&#20851;&#22240;&#32032;&#30340;&#26368;&#23567;&#28857;&#36317;&#31163;&#26469;&#27604;&#36739;&#36793;&#30028;&#26694;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#30028;&#26694;&#22238;&#24402;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#26159;&#30446;&#26631;&#23450;&#20301;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36793;&#30028;&#26694;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#26080;&#27861;&#22312;&#39044;&#27979;&#30340;&#36793;&#30028;&#26694;&#19982;&#30495;&#23454;&#36793;&#30028;&#26694;&#20855;&#26377;&#30456;&#21516;&#23485;&#39640;&#27604;&#20294;&#23485;&#24230;&#21644;&#39640;&#24230;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#20805;&#20998;&#25506;&#32034;&#20102;&#27700;&#24179;&#30697;&#24418;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#28857;&#36317;&#31163;&#30340;&#36793;&#30028;&#26694;&#30456;&#20284;&#24615;&#27604;&#36739;&#25351;&#26631;MPDIoU&#65292;&#35813;&#25351;&#26631;&#21253;&#21547;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#20013;&#32771;&#34385;&#30340;&#25152;&#26377;&#30456;&#20851;&#22240;&#32032;&#65292;&#21363;&#37325;&#21472;&#25110;&#38750;&#37325;&#21472;&#21306;&#22495;&#12289;&#20013;&#24515;&#28857;&#36317;&#31163;&#21644;&#23485;&#24230;&#39640;&#24230;&#30340;&#20559;&#24046;&#65292;&#21516;&#26102;&#31616;&#21270;&#20102;&#35745;&#31639;&#36807;&#31243;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MPDIoU&#30340;&#36793;&#30028;&#26694;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;LMPDIoU&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;MPDIoU&#25439;&#22833;&#20989;&#25968;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#20013;&#21487;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bounding box regression (BBR) has been widely used in object detection and instance segmentation, which is an important step in object localization. However, most of the existing loss functions for bounding box regression cannot be optimized when the predicted box has the same aspect ratio as the groundtruth box, but the width and height values are exactly different. In order to tackle the issues mentioned above, we fully explore the geometric features of horizontal rectangle and propose a novel bounding box similarity comparison metric MPDIoU based on minimum point distance, which contains all of the relevant factors considered in the existing loss functions, namely overlapping or non-overlapping area, central points distance, and deviation of width and height, while simplifying the calculation process. On this basis, we propose a bounding box regression loss function based on MPDIoU, called LMPDIoU . Experimental results show that the MPDIoU loss function is applied to state-of-the-a
&lt;/p&gt;</description></item><item><title>SALC&#26159;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#36741;&#21161;&#30340;&#23398;&#20064;&#32858;&#31867;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#20197;&#36866;&#24212;&#26102;&#21464;&#23460;&#20869;&#29615;&#22659;&#65292;&#25552;&#39640;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07650</link><description>&lt;p&gt;
SALC&#65306;&#22522;&#20110;&#39592;&#26550;&#36741;&#21161;&#30340;&#23398;&#20064;&#32858;&#31867;&#29992;&#20110;&#26102;&#21464;&#23460;&#20869;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization. (arXiv:2307.07650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07650
&lt;/p&gt;
&lt;p&gt;
SALC&#26159;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#36741;&#21161;&#30340;&#23398;&#20064;&#32858;&#31867;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#20197;&#36866;&#24212;&#26102;&#21464;&#23460;&#20869;&#29615;&#22659;&#65292;&#25552;&#39640;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26080;&#32447;&#23460;&#20869;&#23450;&#20301;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#20351;&#29992;&#20174;WiFi&#35775;&#38382;&#28857;&#65288;AP&#65289;&#33719;&#21462;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#24314;&#31435;&#25351;&#32441;&#25968;&#25454;&#24211;&#26159;&#23460;&#20869;&#23450;&#20301;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20013;&#23545;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#30340;&#26102;&#21464;&#38382;&#39064;&#30740;&#31350;&#19981;&#20805;&#20998;&#12290;&#19982;&#20256;&#32479;&#30340;&#38745;&#24577;&#25351;&#32441;&#30456;&#27604;&#65292;&#21160;&#24577;&#37325;&#24314;&#30340;&#25968;&#25454;&#24211;&#21487;&#20197;&#36866;&#24212;&#39640;&#24230;&#21464;&#21270;&#30340;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#20301;&#20934;&#30830;&#24615;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#26102;&#21464;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#36741;&#21161;&#23398;&#20064;&#32858;&#31867;&#23450;&#20301;&#65288;SALC&#65289;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;RSS&#23548;&#21521;&#30340;&#22320;&#22270;&#36741;&#21161;&#32858;&#31867;&#65288;ROMAC&#65289;&#12289;&#22522;&#20110;&#32858;&#31867;&#30340;&#22312;&#32447;&#25968;&#25454;&#24211;&#24314;&#31435;&#65288;CODE&#65289;&#21644;&#22522;&#20110;&#32858;&#31867;&#30340;&#23450;&#20301;&#20272;&#35745;&#65288;CsLE&#65289;&#12290;SALC&#26041;&#26696;&#21516;&#26102;&#32771;&#34385;&#20102;&#22522;&#20110;&#39592;&#26550;&#26368;&#30701;&#36335;&#24452;&#65288;SSP&#65289;&#21644;&#21442;&#32771;&#28857;&#65288;RPs&#65289;&#20043;&#38388;&#30340;&#26102;&#21464;RSS&#27979;&#37327;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless indoor localization has attracted significant amount of attention in recent years. Using received signal strength (RSS) obtained from WiFi access points (APs) for establishing fingerprinting database is a widely utilized method in indoor localization. However, the time-variant problem for indoor positioning systems is not well-investigated in existing literature. Compared to conventional static fingerprinting, the dynamicallyreconstructed database can adapt to a highly-changing environment, which achieves sustainability of localization accuracy. To deal with the time-varying issue, we propose a skeleton-assisted learning-based clustering localization (SALC) system, including RSS-oriented map-assisted clustering (ROMAC), cluster-based online database establishment (CODE), and cluster-scaled location estimation (CsLE). The SALC scheme jointly considers similarities from the skeleton-based shortest path (SSP) and the time-varying RSS measurements across the reference points (RPs)
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#23481;&#26131;&#34987;&#26500;&#26550;&#20026;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#65292;&#32780;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#21463;&#27426;&#36814;&#31243;&#24230;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.07645</link><description>&lt;p&gt;
&#32654;&#22269;&#39184;&#21381;&#35780;&#35770;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31227;&#27665;&#32654;&#39135;&#20182;&#32773;&#21270;&#21644;&#20302;&#22768;&#26395;&#26500;&#26550;
&lt;/p&gt;
&lt;p&gt;
Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models. (arXiv:2307.07645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07645
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#23481;&#26131;&#34987;&#26500;&#26550;&#20026;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#65292;&#32780;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#21463;&#27426;&#36814;&#31243;&#24230;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#29702;&#35299;&#23545;&#39135;&#29289;&#30340;&#38544;&#21547;&#24577;&#24230;&#26377;&#21161;&#20110;&#20943;&#36731;&#22240;&#39135;&#29289;&#20316;&#20026;&#25991;&#21270;&#21644;&#31181;&#26063;&#36523;&#20221;&#30340;&#26631;&#24535;&#32780;&#23548;&#33268;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#23545;&#39135;&#29289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26159;&#19968;&#31181;&#24494;&#20405;&#30053;&#65292;&#23427;&#23545;&#26377;&#23475;&#30340;&#20844;&#20849;&#35805;&#35821;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#21453;&#36807;&#26469;&#21152;&#28145;&#23545;&#27665;&#26063;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#24182;&#23545;&#39184;&#39302;&#30340;&#32463;&#27982;&#32467;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#20180;&#32454;&#30340;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#19968;&#39033;&#22823;&#35268;&#27169;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#23545;&#31227;&#27665;&#32654;&#39135;&#24577;&#24230;&#30340;&#31038;&#20250;&#29702;&#35770;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#22312;14&#20010;&#32654;&#22269;&#24030;&#30340;&#26694;&#26550;&#24046;&#24322;&#12290;&#22312;&#25511;&#21046;&#20102;&#39184;&#21381;&#20215;&#26684;&#21644;&#37051;&#37324;&#31181;&#26063;&#22810;&#26679;&#24615;&#31561;&#22240;&#32032;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#26377;&#21487;&#33021;&#20197;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#30340;&#24418;&#24335;&#36827;&#34892;&#26500;&#26550;&#65292;&#22914;&#30495;&#23454;&#24615;&#65288;&#20363;&#22914;&#65292;&#30495;&#23454;&#65292;&#20256;&#32479;&#65289;&#65292;&#24322;&#22269;&#24773;&#35843;&#65288;&#20363;&#22914;&#65292;&#24322;&#22269;&#65292;&#19981;&#21516;&#65289;&#21644;&#20856;&#22411;&#24615;&#65288;&#20363;&#22914;&#65292;&#20856;&#22411;&#65292;&#36890;&#24120;&#65289;&#12290;&#20294;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#65288;&#20363;&#22914;&#65292;&#21360;&#24230;&#65292;&#22696;&#35199;&#21733;&#65289;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying and understanding implicit attitudes toward food can help efforts to mitigate social prejudice due to food's pervasive role as a marker of cultural and ethnic identity. Stereotypes about food are a form of microaggression that contribute to harmful public discourse that may in turn perpetuate prejudice toward ethnic groups and negatively impact economic outcomes for restaurants. Through careful linguistic analyses, we evaluate social theories about attitudes toward immigrant cuisine in a large-scale study of framing differences in 2.1M English language Yelp reviews of restaurants in 14 US states. Controlling for factors such as restaurant price and neighborhood racial diversity, we find that immigrant cuisines are more likely to be framed in objectifying and othering terms of authenticity (e.g., authentic, traditional), exoticism (e.g., exotic, different), and prototypicality (e.g., typical, usual), but that non-Western immigrant cuisines (e.g., Indian, Mexican) receive mor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19981;&#21516;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20276;&#38543;&#20914;&#31361;&#39044;&#27979;&#30340;&#35299;&#37322;&#26469;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#12290;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#35774;&#32622;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#20174;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07636</link><description>&lt;p&gt;
&#19981;&#21516;&#35299;&#37322;: &#21033;&#29992;&#20998;&#27495;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance. (arXiv:2307.07636v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07636
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19981;&#21516;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20276;&#38543;&#20914;&#31361;&#39044;&#27979;&#30340;&#35299;&#37322;&#26469;&#20943;&#23569;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#12290;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#35774;&#32622;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#20174;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#35299;&#37322;&#24615;&#26159;&#26085;&#30410;&#22797;&#26434;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#19968;&#20010;&#21487;&#21462;&#29305;&#24449;&#65292;&#20294;&#29616;&#20195;&#35299;&#37322;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#30340;&#12290;&#35299;&#37322;&#30340;&#35821;&#20041;&#24182;&#19981;&#24635;&#26159;&#23436;&#20840;&#29702;&#35299;&#30340; - &#35299;&#37322;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;"&#35299;&#37322;"&#19968;&#20010;&#20915;&#31574;&#65292;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21482;&#26159;&#25903;&#25345;&#19968;&#20010;&#20915;&#31574;&#65311;&#25105;&#20204;&#33021;&#21542;&#24110;&#21161;&#20154;&#20204;&#20174;&#20276;&#38543;&#27491;&#30830;&#39044;&#27979;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#27934;&#23519;&#21147;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#20381;&#36182;&#35299;&#37322;&#25152;&#25552;&#20513;&#30340;&#38169;&#35823;&#39044;&#27979;&#65311;&#22312;&#36825;&#20010;&#35282;&#24230;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#35299;&#37322;&#27010;&#24565;: &#20276;&#38543;&#20914;&#31361;&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;&#22810;&#26679;&#24615;&#35774;&#32622;&#19979;&#19981;&#21516;&#35299;&#37322;&#30340;&#20248;&#21183;&#65292;&#20854;&#20013;&#20855;&#26377;&#30456;&#20284;&#24615;&#33021;&#30340;&#22810;&#20010;&#27169;&#22411;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#19981;&#21516;&#30340;&#35299;&#37322;&#21487;&#20197;&#36890;&#36807;&#35843;&#29992;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#23454;&#29616;&#12290;&#36890;&#36807;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#35299;&#37322;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
While explainability is a desirable characteristic of increasingly complex black-box models, modern explanation methods have been shown to be inconsistent and contradictory. The semantics of explanations is not always fully understood - to what extent do explanations "explain" a decision and to what extent do they merely advocate for a decision? Can we help humans gain insights from explanations accompanying correct predictions and not over-rely on incorrect predictions advocated for by explanations? With this perspective in mind, we introduce the notion of dissenting explanations: conflicting predictions with accompanying explanations. We first explore the advantage of dissenting explanations in the setting of model multiplicity, where multiple models with similar performance may have different predictions. In such cases, providing dissenting explanations could be done by invoking the explanations of disagreeing models. Through a pilot study, we demonstrate that dissenting explanation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;AI-human&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#20915;&#31574;&#24314;&#35758;&#24341;&#23548;&#20154;&#31867;&#30340;&#24605;&#32771;&#21644;&#34892;&#21160;&#12290;&#36890;&#36807;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#24335;&#65292;&#21050;&#28608;&#20154;&#31867;&#30340;&#24555;&#36895;&#24605;&#32771;&#12289;&#24930;&#24605;&#32771;&#25110;&#20803;&#35748;&#30693;&#12290;&#20855;&#20307;&#36873;&#25321;&#20351;&#29992;&#21738;&#31181;&#24341;&#23548;&#26041;&#24335;&#21462;&#20915;&#20110;&#29305;&#23450;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07628</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#30340;&#24555;&#24930;AI&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Value-based Fast and Slow AI Nudging. (arXiv:2307.07628v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;AI-human&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#20915;&#31574;&#24314;&#35758;&#24341;&#23548;&#20154;&#31867;&#30340;&#24605;&#32771;&#21644;&#34892;&#21160;&#12290;&#36890;&#36807;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#24335;&#65292;&#21050;&#28608;&#20154;&#31867;&#30340;&#24555;&#36895;&#24605;&#32771;&#12289;&#24930;&#24605;&#32771;&#25110;&#20803;&#35748;&#30693;&#12290;&#20855;&#20307;&#36873;&#25321;&#20351;&#29992;&#21738;&#31181;&#24341;&#23548;&#26041;&#24335;&#21462;&#20915;&#20110;&#29305;&#23450;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#23548;&#26159;&#19968;&#31181;&#26088;&#22312;&#24433;&#21709;&#20154;&#20204;&#24605;&#32771;&#21644;&#34892;&#21160;&#30340;&#34892;&#20026;&#31574;&#30053;&#12290;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#21487;&#20197;&#25214;&#21040;&#35768;&#22810;&#24341;&#23548;&#25216;&#26415;&#65292;&#36825;&#20123;&#24341;&#23548;&#25216;&#26415;&#21487;&#20197;&#38024;&#23545;&#20154;&#31867;&#24555;&#36895;&#21644;&#26080;&#24847;&#35782;&#30340;&#24605;&#32500;&#65292;&#20363;&#22914;&#20351;&#29992;&#22270;&#20687;&#20135;&#29983;&#24656;&#24807;&#65292;&#25110;&#26356;&#35880;&#24910;&#21644;&#36153;&#21147;&#30340;&#24930;&#24605;&#32500;&#65292;&#20363;&#22914;&#37322;&#25918;&#35753;&#25105;&#20204;&#21453;&#24605;&#36873;&#25321;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#19968;&#20010;&#22522;&#20110;&#20215;&#20540;&#30340;AI&#20154;&#21512;&#20316;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;AI&#31995;&#32479;&#36890;&#36807;&#25552;&#20379;&#20915;&#31574;&#24314;&#35758;&#26469;&#24341;&#23548;&#20154;&#31867;&#12290;&#22522;&#20110;&#24314;&#35758;&#20309;&#26102;&#21576;&#29616;&#32473;&#20154;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#24335;&#65292;&#20998;&#21035;&#29992;&#20110;&#21050;&#28608;&#20154;&#31867;&#30340;&#24555;&#36895;&#24605;&#32771;&#12289;&#24930;&#24605;&#32771;&#25110;&#20803;&#35748;&#30693;&#12290;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#24341;&#23548;&#26041;&#24335;&#30340;&#26159;&#19982;&#29305;&#23450;&#20915;&#31574;&#22330;&#26223;&#30456;&#20851;&#30340;&#20215;&#20540;&#35266;&#12290;&#20215;&#20540;&#35266;&#21253;&#25324;&#20915;&#31574;&#36136;&#37327;&#12289;&#36895;&#24230;&#12289;&#20154;&#31867;&#25216;&#33021;&#25552;&#21319;&#21644;&#23398;&#20064;&#12289;&#20154;&#31867;&#20027;&#21160;&#24615;&#20197;&#21450;&#38544;&#31169;&#31561;&#12290;&#22312;&#21516;&#19968;&#20010;&#20915;&#31574;&#22330;&#26223;&#20013;&#21487;&#20197;&#23384;&#22312;&#22810;&#20010;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nudging is a behavioral strategy aimed at influencing people's thoughts and actions. Nudging techniques can be found in many situations in our daily lives, and these nudging techniques can targeted at human fast and unconscious thinking, e.g., by using images to generate fear or the more careful and effortful slow thinking, e.g., by releasing information that makes us reflect on our choices. In this paper, we propose and discuss a value-based AI-human collaborative framework where AI systems nudge humans by proposing decision recommendations. Three different nudging modalities, based on when recommendations are presented to the human, are intended to stimulate human fast thinking, slow thinking, or meta-cognition. Values that are relevant to a specific decision scenario are used to decide when and how to use each of these nudging modalities. Examples of values are decision quality, speed, human upskilling and learning, human agency, and privacy. Several values can be present at the sam
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#35780;&#20272;&#21592;&#21644;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07544</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#65306;&#36890;&#36807;&#25166;&#26681;&#30693;&#35782;&#25552;&#39640;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge. (arXiv:2307.07544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07544
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#35780;&#20272;&#21592;&#21644;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#33258;&#25105;&#29031;&#39038;&#33021;&#21147;&#20307;&#29616;&#22312;&#8220;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#65288;ADL&#65289;&#8221;&#20013;&#65292;ADL&#20316;&#20026;&#21151;&#33021;&#33021;&#21147;&#65288;&#36816;&#20316;&#33021;&#21147;&#65289;&#30340;&#34913;&#37327;&#12290;&#21151;&#33021;&#33021;&#21147;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#38656;&#35201;&#20010;&#20154;&#25252;&#29702;&#21644;&#21327;&#21161;&#30340;&#24694;&#21155;&#29983;&#27963;&#26465;&#20214;&#12290;&#20026;&#20102;&#20934;&#30830;&#35782;&#21035;&#38656;&#35201;&#25903;&#25345;&#30340;&#20154;&#65292;&#21327;&#21161;&#35745;&#21010;&#20250;&#25345;&#32493;&#35780;&#20272;&#21442;&#19982;&#32773;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21151;&#33021;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#27700;&#24179;&#30340;&#19987;&#23478;&#35780;&#20272;&#21592;&#26102;&#65292;&#35780;&#20272;&#36807;&#31243;&#21487;&#33021;&#36935;&#21040;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#23588;&#20854;&#26159;&#21021;&#23398;&#32773;&#35780;&#20272;&#21592;&#21487;&#33021;&#32570;&#20047;&#19982;&#21442;&#19982;&#32773;&#36827;&#34892;&#23454;&#38469;&#20114;&#21160;&#25152;&#38656;&#30340;&#24517;&#35201;&#20934;&#22791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23545;&#35805;&#31995;&#32479;&#65292;&#20197;&#33258;&#28982;&#19988;&#21487;&#37325;&#29616;&#30340;&#26041;&#24335;&#27169;&#25311;&#35780;&#20272;&#21592;&#21644;&#20855;&#26377;&#19981;&#21516;&#21151;&#33021;&#33021;&#21147;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#23545;&#35805;&#31995;&#32479;&#30001;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare, the ability to care for oneself is reflected in the "Activities of Daily Living (ADL)," which serve as a measure of functional ability (functioning). A lack of functioning may lead to poor living conditions requiring personal care and assistance. To accurately identify those in need of support, assistance programs continuously evaluate participants' functioning across various domains. However, the assessment process may encounter consistency issues when multiple assessors with varying levels of expertise are involved. Novice assessors, in particular, may lack the necessary preparation for real-world interactions with participants. To address this issue, we developed a dialogue system that simulates interactions between assessors and individuals of varying functioning in a natural and reproducible way. The dialogue system consists of two major modules, one for natural language understanding (NLU) and one for natural language generation (NLG), respectively. In order to gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#24207;&#25968;&#25454;&#30340;&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;MAPU&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#34109;&#21644;&#26102;&#38388;&#25554;&#34917;&#30340;&#26041;&#24335;&#65292;&#25429;&#25417;&#28304;&#39046;&#22495;&#30340;&#26102;&#38388;&#20449;&#24687;&#24182;&#24341;&#23548;&#30446;&#26631;&#27169;&#22411;&#20135;&#29983;&#30446;&#26631;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07542</link><description>&lt;p&gt;
&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#19982;&#26102;&#38388;&#25554;&#34917;&#30340;&#26102;&#24207;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Temporal Imputation for Time Series Data. (arXiv:2307.07542v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#24207;&#25968;&#25454;&#30340;&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;MAPU&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#34109;&#21644;&#26102;&#38388;&#25554;&#34917;&#30340;&#26041;&#24335;&#65292;&#25429;&#25417;&#28304;&#39046;&#22495;&#30340;&#26102;&#38388;&#20449;&#24687;&#24182;&#24341;&#23548;&#30446;&#26631;&#27169;&#22411;&#20135;&#29983;&#30446;&#26631;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#65288;SFDA&#65289;&#26088;&#22312;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20174;&#24050;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#20445;&#25345;&#28304;&#39046;&#22495;&#30340;&#38544;&#31169;&#12290;&#23613;&#31649;&#22312;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#26159;&#22312;&#26102;&#24207;&#24212;&#29992;&#20013;&#65292;SFDA&#36824;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#35270;&#35273;&#24212;&#29992;&#30340;SFDA&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#20174;&#32780;&#23548;&#33268;&#33258;&#36866;&#24212;&#24615;&#33021;&#21463;&#25439;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#30340;&#26102;&#24207;&#25968;&#25454;&#26041;&#27861;&#65292;&#21363;MAsk and imPUte (MAPU)&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#25429;&#25417;&#28304;&#39046;&#22495;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#26102;&#24207;&#20449;&#21495;&#36827;&#34892;&#38543;&#26426;&#25513;&#34109;&#65292;&#21516;&#26102;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25554;&#34917;&#22120;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20174;&#25513;&#34109;&#29256;&#26412;&#20013;&#24674;&#22797;&#21407;&#22987;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#22312;&#36866;&#24212;&#27493;&#39588;&#20013;&#65292;&#25554;&#34917;&#22120;&#32593;&#32476;&#34987;&#21033;&#29992;&#26469;&#24341;&#23548;&#30446;&#26631;&#27169;&#22411;&#20135;&#29983;&#30446;&#26631;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a labeled source domain to an unlabeled target domain without access to the source domain data, preserving source domain privacy. Despite its prevalence in visual applications, SFDA is largely unexplored in time series applications. The existing SFDA methods that are mainly designed for visual applications may fail to handle the temporal dynamics in time series, leading to impaired adaptation performance. To address this challenge, this paper presents a simple yet effective approach for source-free domain adaptation on time series data, namely MAsk and imPUte (MAPU). First, to capture temporal information of the source domain, our method performs random masking on the time series signals while leveraging a novel temporal imputer to recover the original signal from a masked version in the embedding space. Second, in the adaptation step, the imputer network is leveraged to guide the target model to produce target 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32422;&#26463;&#19979;&#23398;&#20064;&#22810;&#20010;&#21327;&#35843;&#20195;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;DAG&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#38750;DAG&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07529</link><description>&lt;p&gt;
&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32422;&#26463;&#19979;&#23398;&#20064;&#22810;&#20010;&#21327;&#35843;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints. (arXiv:2307.07529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32422;&#26463;&#19979;&#23398;&#20064;&#22810;&#20010;&#21327;&#35843;&#20195;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;DAG&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#38750;DAG&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#32422;&#26463;&#19979;&#23398;&#20064;&#22810;&#20010;&#21327;&#35843;&#20195;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;MARL&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#21033;&#29992;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;DAG&#32467;&#26500;&#65292;&#20197;&#36798;&#21040;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#22870;&#21169;&#30340;MARL&#27169;&#22411;&#30340;&#26032;&#22411;&#20195;&#29702;&#20540;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#19979;&#30028;&#12290;&#22312;&#35745;&#31639;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21033;&#29992;&#39046;&#23548;&#32773;&#20195;&#29702;&#21644;&#22870;&#21169;&#29983;&#25104;&#21644;&#20998;&#21457;&#20195;&#29702;&#30340;&#26032;&#27010;&#24565;&#65292;&#24341;&#23548;&#20998;&#35299;&#30340;&#20174;&#23646;&#20195;&#29702;&#22312;&#20855;&#26377;DAG&#32422;&#26463;&#30340;&#29615;&#22659;&#20013;&#26356;&#22909;&#22320;&#25506;&#32034;&#21442;&#25968;&#31354;&#38388;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#22235;&#20010;DAG&#29615;&#22659;&#65292;&#21253;&#25324;Intel&#39640;&#20135;&#37327;&#25171;&#21253;&#21644;&#27979;&#35797;&#24037;&#21378;&#30340;&#23454;&#38469;&#35843;&#24230;&#65292;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#23427;&#20248;&#20110;&#20854;&#20182;&#38750;DAG&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel multi-agent reinforcement learning (MARL) method to learn multiple coordinated agents under directed acyclic graph (DAG) constraints. Unlike existing MARL approaches, our method explicitly exploits the DAG structure between agents to achieve more effective learning performance. Theoretically, we propose a novel surrogate value function based on a MARL model with synthetic rewards (MARLM-SR) and prove that it serves as a lower bound of the optimal value function. Computationally, we propose a practical training algorithm that exploits new notion of leader agent and reward generator and distributor agent to guide the decomposed follower agents to better explore the parameter space in environments with DAG constraints. Empirically, we exploit four DAG environments including a real-world scheduling for one of Intel's high volume packaging and test factory to benchmark our methods and show it outperforms the other non-DAG approaches.
&lt;/p&gt;</description></item><item><title>PatchSorter&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#26631;&#27880;&#24037;&#20855;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#30452;&#35266;&#30340;&#32593;&#32476;&#30028;&#38754;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#39640;&#21534;&#21520;&#37327;&#26631;&#27880;&#65292;&#30456;&#36739;&#20110;&#26080;&#36741;&#21161;&#26631;&#27880;&#65292;&#27599;&#31186;&#26631;&#31614;&#25968;&#25552;&#39640;&#36229;&#36807;7&#20493;&#65292;&#23545;&#26631;&#27880;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#38750;&#24120;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.07528</link><description>&lt;p&gt;
PatchSorter: &#19968;&#31181;&#29992;&#20110;&#23545;&#35937;&#26631;&#27880;&#30340;&#39640;&#21534;&#21520;&#37327;&#28145;&#24230;&#23398;&#20064;&#25968;&#23383;&#30149;&#29702;&#23398;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
PatchSorter: A High Throughput Deep Learning Digital Pathology Tool for Object Labeling. (arXiv:2307.07528v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07528
&lt;/p&gt;
&lt;p&gt;
PatchSorter&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#26631;&#27880;&#24037;&#20855;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#30452;&#35266;&#30340;&#32593;&#32476;&#30028;&#38754;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#39640;&#21534;&#21520;&#37327;&#26631;&#27880;&#65292;&#30456;&#36739;&#20110;&#26080;&#36741;&#21161;&#26631;&#27880;&#65292;&#27599;&#31186;&#26631;&#31614;&#25968;&#25552;&#39640;&#36229;&#36807;7&#20493;&#65292;&#23545;&#26631;&#27880;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#38750;&#24120;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#19982;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#21453;&#24212;&#30456;&#20851;&#30340;&#27169;&#24335;&#30340;&#21457;&#29616;&#36890;&#24120;&#38656;&#35201;&#26631;&#27880;&#22823;&#37327;&#32452;&#32455;&#23398;&#23545;&#35937;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#26631;&#27880;&#24037;&#20855; PatchSorter&#65292;&#23427;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#30452;&#35266;&#30340;&#32593;&#32476;&#30028;&#38754;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#36229;&#36807;100,000&#20010;&#23545;&#35937;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#26080;&#36741;&#21161;&#26631;&#27880;&#65292;&#27599;&#31186;&#26631;&#31614;&#25968;&#25552;&#39640;&#36229;&#36807;7&#20493;&#65292;&#23545;&#26631;&#27880;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#38750;&#24120;&#23567;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#39640;&#21534;&#21520;&#37327;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of patterns associated with diagnosis, prognosis, and therapy response in digital pathology images often requires intractable labeling of large quantities of histological objects. Here we release an open-source labeling tool, PatchSorter, which integrates deep learning with an intuitive web interface. Using &gt;100,000 objects, we demonstrate a &gt;7x improvement in labels per second over unaided labeling, with minimal impact on labeling accuracy, thus enabling high-throughput labeling of large datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#21512;&#30740;&#31350;&#35843;&#26597;&#20102;&#33258;&#20027;&#36710;&#36742;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#29616;&#26377;&#25991;&#29486;&#24182;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#30740;&#31350;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.07527</link><description>&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;: &#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Autonomous Vehicle's Trajectory Prediction: A comprehensive survey, Challenges, and Future Research Directions. (arXiv:2307.07527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#21512;&#30740;&#31350;&#35843;&#26597;&#20102;&#33258;&#20027;&#36710;&#36742;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#29616;&#26377;&#25991;&#29486;&#24182;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#30740;&#31350;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742; (AVs) &#36890;&#36807;&#20195;&#26367;&#20154;&#24037;&#39550;&#39542;&#21592;&#32780;&#37319;&#29992;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#20915;&#31574;&#31995;&#32479;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33021;&#22815;&#26377;&#25928;&#22320;&#39550;&#39542;&#36947;&#36335;&#65292;AVs &#24517;&#39035;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#39044;&#27979;&#39550;&#39542;&#33021;&#21147;&#65292;&#21363;&#39044;&#27979;&#21608;&#22260;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#65292;&#20511;&#37492;&#29616;&#26377;&#25991;&#29486;&#23545;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#26159;&#25512;&#36827;&#35813;&#39046;&#22495;&#21644;&#21457;&#23637;&#20840;&#38754;&#29702;&#35299;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;AVs&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#20102;&#19982;AVs&#36712;&#36857;&#39044;&#27979;&#30456;&#20851;&#30340;&#20004;&#30334;&#20313;&#39033;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#30340;&#19968;&#33324;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25509;&#30528;&#25105;&#20204;&#35752;&#35770;&#20102;&#30446;&#21069;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have emerged as a promising solution by replacing human drivers with advanced computer-aided decision-making systems. However, for AVs to effectively navigate the road, they must possess the capability to predict the future behavior of nearby traffic participants, similar to the predictive driving abilities of human drivers. Building upon existing literature is crucial to advance the field and develop a comprehensive understanding of trajectory prediction methods in the context of automated driving. To address this need, we have undertaken a comprehensive review that focuses on trajectory prediction methods for AVs, with a particular emphasis on machine learning techniques including deep learning and reinforcement learning-based approaches. We have extensively examined over two hundred studies related to trajectory prediction in the context of AVs. The paper begins with an introduction to the general problem of predicting vehicle trajectories and provides an o
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20351;&#24471;&#26426;&#22120;&#20855;&#22791;&#20102;&#20154;&#31867;&#21270;&#30340;&#21709;&#24212;&#33021;&#21147;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22270;&#28789;&#30340;&#24605;&#32771;&#26426;&#22120;&#27010;&#24565;&#65292;&#24182;&#23545;&#26426;&#22120;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22270;&#28789;&#27979;&#35797;&#26159;&#35780;&#20272;&#26426;&#22120;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#20294;&#26234;&#33021;&#36824;&#26377;&#20854;&#20182;&#26041;&#38754;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#23637;&#31034;&#20102;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2307.07526</link><description>&lt;p&gt;
&#29616;&#22312;&#21487;&#20197;&#35828;&#26426;&#22120;&#21487;&#20197;&#24605;&#32771;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can I say, now machines can think?. (arXiv:2307.07526v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07526
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20351;&#24471;&#26426;&#22120;&#20855;&#22791;&#20102;&#20154;&#31867;&#21270;&#30340;&#21709;&#24212;&#33021;&#21147;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22270;&#28789;&#30340;&#24605;&#32771;&#26426;&#22120;&#27010;&#24565;&#65292;&#24182;&#23545;&#26426;&#22120;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22270;&#28789;&#27979;&#35797;&#26159;&#35780;&#20272;&#26426;&#22120;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#20294;&#26234;&#33021;&#36824;&#26377;&#20854;&#20182;&#26041;&#38754;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#23637;&#31034;&#20102;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#19981;&#21516;&#39046;&#22495;&#20026;&#26032;&#19968;&#20195;&#26426;&#22120;&#24102;&#26469;&#20102;&#26426;&#20250;&#12290;&#36825;&#20123;&#26426;&#22120;&#20855;&#26377;&#21508;&#31181;&#33021;&#21147;&#65292;&#20363;&#22914;&#21487;&#20197;&#29983;&#25104;&#22270;&#20687;&#12289;&#29983;&#25104;&#31572;&#26696;&#25110;&#25925;&#20107;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;"&#25552;&#31034;"&#32534;&#20889;&#20195;&#30721;&#12290;&#36825;&#20123;&#26426;&#22120;&#34987;&#35748;&#20026;&#26159;'&#24605;&#32771;&#30340;&#24605;&#24819;'&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29983;&#25104;&#20154;&#31867;&#21270;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#21644;&#25506;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#30340;&#26426;&#22120;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22270;&#28789;&#30340;&#24605;&#32771;&#26426;&#22120;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#26032;&#30340;&#25216;&#26415;&#36827;&#23637;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24605;&#32771;&#26426;&#22120;&#30340;&#24322;&#35758;&#21644;&#21518;&#26524;&#65292;&#20197;&#21450;&#35780;&#20272;&#26426;&#22120;&#35748;&#30693;&#33021;&#21147;&#30340;&#21487;&#29992;&#25216;&#26415;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22270;&#28789;&#27979;&#35797;&#26159;&#35780;&#20272;&#26426;&#22120;&#33021;&#21147;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#36824;&#26377;&#20854;&#20182;&#26041;&#38754;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#23637;&#31034;&#20102;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI techniques have opened the path for new generations of machines in diverse domains. These machines have various capabilities for example, they can produce images, generate answers or stories, and write codes based on the "prompts" only provided by users. These machines are considered 'thinking minds' because they have the ability to generate human-like responses. In this study, we have analyzed and explored the capabilities of artificial intelligence-enabled machines. We have revisited on Turing's concept of thinking machines and compared it with recent technological advancements. The objections and consequences of the thinking machines are also discussed in this study, along with available techniques to evaluate machines' cognitive capabilities. We have concluded that Turing Test is a critical aspect of evaluating machines' ability. However, there are other aspects of intelligence too, and AI machines exhibit most of these aspects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21151;&#33021;&#27169;&#22411;&#30340;&#32422;&#31616;&#23450;&#20041;&#65292;&#23558;&#22240;&#26524;&#20851;&#31995;&#23450;&#20041;&#20026;&#23558;&#22240;&#26524;&#32852;&#31995;&#36215;&#26469;&#30340;&#20989;&#25968;&#12290;&#20316;&#32773;&#20351;&#29992;&#22686;&#37327;&#21387;&#32553;&#21644;&#23545;&#27604;&#21069;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20135;&#29983;&#31526;&#21512;&#30452;&#35273;&#30340;&#22240;&#26524;&#34920;&#36798;&#65292;&#24182;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#22240;&#26524;&#22330;&#26223;&#12290;&#36825;&#31181;&#27169;&#22411;&#19982;&#27010;&#29575;&#29702;&#35770;&#20860;&#23481;&#20294;&#19981;&#21487;&#32422;&#65292;&#24182;&#19988;&#32463;&#36807;&#19982;&#20854;&#20182;&#22240;&#26524;&#29702;&#35770;&#30340;&#27604;&#36739;&#65292;&#36824;&#24212;&#29992;&#20110;&#33258;&#30001;&#24847;&#24535;&#12289;&#22240;&#26524;&#35299;&#37322;&#21644;&#24515;&#29702;&#22240;&#26524;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07524</link><description>&lt;p&gt;
&#23558;&#22240;&#26524;&#20851;&#31995;&#32422;&#31616;&#20026;&#32467;&#26500;&#27169;&#22411;&#30340;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Reducing Causality to Functions with Structural Models. (arXiv:2307.07524v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21151;&#33021;&#27169;&#22411;&#30340;&#32422;&#31616;&#23450;&#20041;&#65292;&#23558;&#22240;&#26524;&#20851;&#31995;&#23450;&#20041;&#20026;&#23558;&#22240;&#26524;&#32852;&#31995;&#36215;&#26469;&#30340;&#20989;&#25968;&#12290;&#20316;&#32773;&#20351;&#29992;&#22686;&#37327;&#21387;&#32553;&#21644;&#23545;&#27604;&#21069;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20135;&#29983;&#31526;&#21512;&#30452;&#35273;&#30340;&#22240;&#26524;&#34920;&#36798;&#65292;&#24182;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#22240;&#26524;&#22330;&#26223;&#12290;&#36825;&#31181;&#27169;&#22411;&#19982;&#27010;&#29575;&#29702;&#35770;&#20860;&#23481;&#20294;&#19981;&#21487;&#32422;&#65292;&#24182;&#19988;&#32463;&#36807;&#19982;&#20854;&#20182;&#22240;&#26524;&#29702;&#35770;&#30340;&#27604;&#36739;&#65292;&#36824;&#24212;&#29992;&#20110;&#33258;&#30001;&#24847;&#24535;&#12289;&#22240;&#26524;&#35299;&#37322;&#21644;&#24515;&#29702;&#22240;&#26524;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22312;&#21746;&#23398;&#21644;&#32479;&#35745;&#23398;&#39046;&#22495;&#65292;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#31934;&#30830;&#23450;&#20041;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22240;&#26524;&#20851;&#31995;&#24212;&#35813;&#34987;&#23450;&#20041;&#20026;&#23558;&#22240;&#26524;&#32852;&#31995;&#36215;&#26469;&#30340;&#20989;&#25968;&#65288;&#22312;&#25968;&#23398;&#20013;&#34920;&#31034;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#21151;&#33021;&#27169;&#22411;&#65288;SFM&#65289;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#32422;&#31616;&#23450;&#20041;&#12290;&#36890;&#36807;&#20351;&#29992;&#22686;&#37327;&#21387;&#32553;&#21644;&#23545;&#27604;&#21069;&#21521;&#25512;&#29702;&#65292;SFM&#21487;&#20197;&#20135;&#29983;&#31526;&#21512;&#25105;&#20204;&#30340;&#30452;&#35273;&#30340;&#22240;&#26524;&#34920;&#36798;&#65292;&#22914;&#8220;X&#23548;&#33268;Y&#8221;&#21644;&#8220;X&#26159;Y&#30340;&#21407;&#22240;&#8221;&#12290;&#25105;&#20204;&#32534;&#35793;&#20102;&#19968;&#32452;&#22240;&#26524;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20854;&#20013;&#20351;&#29992;SFM&#12290;SFM&#19982;&#27010;&#29575;&#29702;&#35770;&#20860;&#23481;&#20294;&#19981;&#21487;&#32422;&#65292;&#25105;&#20204;&#36824;&#23558;SFM&#19982;&#20854;&#20182;&#22240;&#26524;&#29702;&#35770;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#30001;&#24847;&#24535;&#12289;&#22240;&#26524;&#35299;&#37322;&#21644;&#24515;&#29702;&#22240;&#26524;&#31561;&#19979;&#28216;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise definition of causality is currently an open problem in philosophy and statistics. We believe causality should be defined as functions (in mathematics) that map causes to effects. We propose a reductive definition of causality based on Structural Functional Model (SFM). Using delta compression and contrastive forward inference, SFM can produce causal utterances like "X causes Y" and "X is the cause of Y" that match our intuitions. We compile a dataset of causal scenarios and use SFM in all of them. SFM is compatible with but not reducible to probability theory. We also compare SFM with other theories of causation and apply SFM to downstream problems like free will, causal explanation, and mental causation.
&lt;/p&gt;</description></item><item><title>PapagAI&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25945;&#23398;&#29702;&#35770;&#24182;&#23454;&#29616;&#20026;&#28151;&#21512;AI&#31995;&#32479;&#30340;&#24320;&#28304;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#24182;&#34917;&#20805;&#35762;&#24072;&#30340;&#25945;&#23398;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.07523</link><description>&lt;p&gt;
PapagAI&#65306;&#21453;&#24605;&#24615;&#25991;&#31456;&#30340;&#33258;&#21160;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
PapagAI:Automated Feedback for Reflective Essays. (arXiv:2307.07523v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07523
&lt;/p&gt;
&lt;p&gt;
PapagAI&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25945;&#23398;&#29702;&#35770;&#24182;&#23454;&#29616;&#20026;&#28151;&#21512;AI&#31995;&#32479;&#30340;&#24320;&#28304;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#24182;&#34917;&#20805;&#35762;&#24072;&#30340;&#25945;&#23398;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#31561;&#25945;&#32946;&#26399;&#38388;&#65292;&#25776;&#20889;&#21453;&#24605;&#24615;&#23454;&#36341;&#26159;&#39044;&#22791;&#25945;&#24072;&#36827;&#34892;&#30340;&#24120;&#35268;&#32451;&#20064;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#30340;&#35762;&#24072;&#38656;&#35201;&#25552;&#20379;&#20010;&#21035;&#21453;&#39304;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#39033;&#24120;&#35268;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#25945;&#23398;&#29702;&#35770;&#24182;&#23454;&#29616;&#20026;&#28151;&#21512;AI&#31995;&#32479;&#30340;&#24320;&#28304;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#32452;&#20214;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23454;&#29616;&#23398;&#29983;&#30340;&#26356;&#22909;&#23398;&#20064;&#32467;&#26524;&#65292;&#24182;&#34917;&#20805;&#35762;&#24072;&#30340;&#25945;&#23398;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Written reflective practice is a regular exercise pre-service teachers perform during their higher education. Usually, their lecturers are expected to provide individual feedback, which can be a challenging task to perform on a regular basis. In this paper, we present the first open-source automated feedback tool based on didactic theory and implemented as a hybrid AI system. We describe the components and discuss the advantages and disadvantages of our system compared to the state-of-art generative large language models. The main objective of our work is to enable better learning outcomes for students and to complement the teaching activities of lecturers.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.07522</link><description>&lt;p&gt;
&#30001;&#29983;&#25104;&#38381;&#29615;&#20154;&#24037;&#26234;&#33021;&#24341;&#39046;&#30340;&#22522;&#30784;&#31185;&#23398;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07522
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#27491;&#22312;&#39072;&#35206;&#25216;&#26415;&#21019;&#26032;&#12289;&#20135;&#21697;&#24320;&#21457;&#21644;&#25972;&#20010;&#31038;&#20250;&#12290;&#20154;&#24037;&#26234;&#33021;&#23545;&#25216;&#26415;&#30340;&#36129;&#29486;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#36884;&#24452;&#23454;&#29616;&#65292;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26126;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#26631;&#20934;&#65292;&#33539;&#22260;&#20174;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#21040;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#23454;&#36341;&#21644;&#27169;&#22411;&#21457;&#29616;&#38656;&#35201;&#35775;&#38382;&#39640;&#36136;&#37327;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#22522;&#30784;&#31185;&#23398;&#30340;&#36129;&#29486;&#36739;&#23569;&#12290;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33021;&#20195;&#34920;&#20102;&#36890;&#36807;&#23450;&#37327;&#27169;&#22411;&#22686;&#24378;&#21644;&#21152;&#36895;&#22522;&#30784;&#28145;&#24230;&#31185;&#23398;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#12289;&#33258;&#21160;&#21270;&#30340;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#26041;&#27861;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#24320;&#25918;&#24335;&#33258;&#20027;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Int
&lt;/p&gt;</description></item><item><title>CephGPT-4&#26159;&#19968;&#20010;&#20855;&#26377;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#22810;&#27169;&#24577;&#39045;&#39052;&#27979;&#37327;&#19982;&#35786;&#26029;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#39045;&#39052;&#26631;&#24535;&#28857;&#21644;&#29983;&#25104;&#35786;&#26029;&#25253;&#21578;&#65292;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;&#27491;&#30072;&#27979;&#37327;&#21644;&#35786;&#26029;&#24212;&#29992;&#24102;&#26469;&#38761;&#21629;&#24615;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07518</link><description>&lt;p&gt;
CephGPT-4:&#19968;&#31181;&#20855;&#26377;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#22810;&#27169;&#24577;&#39045;&#39052;&#27979;&#37327;&#19982;&#35786;&#26029;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CephGPT-4: An Interactive Multimodal Cephalometric Measurement and Diagnostic System with Visual Large Language Model. (arXiv:2307.07518v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07518
&lt;/p&gt;
&lt;p&gt;
CephGPT-4&#26159;&#19968;&#20010;&#20855;&#26377;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#22810;&#27169;&#24577;&#39045;&#39052;&#27979;&#37327;&#19982;&#35786;&#26029;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#39045;&#39052;&#26631;&#24535;&#28857;&#21644;&#29983;&#25104;&#35786;&#26029;&#25253;&#21578;&#65292;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;&#27491;&#30072;&#27979;&#37327;&#21644;&#35786;&#26029;&#24212;&#29992;&#24102;&#26469;&#38761;&#21629;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#22810;&#27169;&#24577;&#39045;&#39052;&#21307;&#23398;&#25968;&#25454;&#30340;&#35786;&#26029;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#39045;&#39052;&#20998;&#26512;&#21644;&#35786;&#26029;&#23545;&#35805;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#26500;&#24314;&#20102;&#22810;&#27169;&#24577;&#27491;&#30072;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#39045;&#39052;&#24433;&#20687;&#21644;&#21307;&#24739;&#23545;&#35805;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;U-net&#33258;&#21160;&#20998;&#26512;&#39045;&#39052;&#26631;&#24535;&#28857;&#21644;&#29983;&#25104;&#35786;&#26029;&#25253;&#21578;&#12290;&#28982;&#21518;&#65292;&#23558;&#39045;&#39052;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#30340;&#35786;&#26029;&#25253;&#21578;&#20998;&#21035;&#22312;Minigpt-4&#21644;VisualGLM&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CephGPT-4&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#39072;&#35206;&#27491;&#30072;&#27979;&#37327;&#21644;&#35786;&#26029;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#21019;&#26032;&#22312;&#27491;&#30072;&#23398;&#39046;&#22495;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale multimodal language models (LMMs) have achieved remarkable success in general domains. However, the exploration of diagnostic language models based on multimodal cephalometric medical data remains limited. In this paper, we propose a novel multimodal cephalometric analysis and diagnostic dialogue model. Firstly, a multimodal orthodontic medical dataset is constructed, comprising cephalometric images and doctor-patient dialogue data, with automatic analysis of cephalometric landmarks using U-net and generation of diagnostic reports. Then, the cephalometric dataset and generated diagnostic reports are separately fine-tuned on Minigpt-4 and VisualGLM. Results demonstrate that the CephGPT-4 model exhibits excellent performance and has the potential to revolutionize orthodontic measurement and diagnostic applications. These innovations hold revolutionary application potential in the field of orthodontics.
&lt;/p&gt;</description></item><item><title>&#20174;&#24212;&#29992;&#26412;&#20307;&#35770;&#30340;&#35282;&#24230;&#35299;&#20915;&#22240;&#26524;&#38382;&#39064;&#65292;&#36890;&#36807;&#31995;&#32479;&#21151;&#33021;&#27010;&#24565;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#65292;&#23558;&#20219;&#20309;&#21407;&#22240;&#20998;&#35299;&#20026;&#23454;&#29616;&#12289;&#38450;&#27490;&#12289;&#20801;&#35768;&#21644;&#19981;&#20801;&#35768;&#22235;&#20010;&#23376;&#21151;&#33021;&#65292;&#26368;&#21518;&#19977;&#20010;&#23376;&#21151;&#33021;&#21487;&#20197;&#20165;&#29992;&#23454;&#29616;&#26469;&#23450;&#20041;&#12290;&#22240;&#26524;&#30340;&#26412;&#36136;&#22312;&#20110;&#23454;&#29616;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07517</link><description>&lt;p&gt;
&#24341;&#21457;&#21363;&#23454;&#29616;&#8212;&#8212;&#22240;&#26524;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Causing is Achieving -- A solution to the problem of causation. (arXiv:2307.07517v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07517
&lt;/p&gt;
&lt;p&gt;
&#20174;&#24212;&#29992;&#26412;&#20307;&#35770;&#30340;&#35282;&#24230;&#35299;&#20915;&#22240;&#26524;&#38382;&#39064;&#65292;&#36890;&#36807;&#31995;&#32479;&#21151;&#33021;&#27010;&#24565;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#65292;&#23558;&#20219;&#20309;&#21407;&#22240;&#20998;&#35299;&#20026;&#23454;&#29616;&#12289;&#38450;&#27490;&#12289;&#20801;&#35768;&#21644;&#19981;&#20801;&#35768;&#22235;&#20010;&#23376;&#21151;&#33021;&#65292;&#26368;&#21518;&#19977;&#20010;&#23376;&#21151;&#33021;&#21487;&#20197;&#20165;&#29992;&#23454;&#29616;&#26469;&#23450;&#20041;&#12290;&#22240;&#26524;&#30340;&#26412;&#36136;&#22312;&#20110;&#23454;&#29616;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24212;&#29992;&#26412;&#20307;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#26368;&#36817;&#19968;&#20010;&#25361;&#25112;&#22240;&#26524;&#29702;&#35299;&#21644;&#24314;&#27169;&#38382;&#39064;&#30340;&#21069;&#25552;&#26159;&#22240;&#26524;&#24456;&#30495;&#23454;&#12290;&#22240;&#32780;&#33719;&#24471;&#20102;&#20197;&#19979;&#19977;&#20010;&#32467;&#26524;&#65306;(1)&#36890;&#36807;&#31995;&#32479;&#21151;&#33021;&#30340;&#27010;&#24565;&#21487;&#20197;&#29702;&#35299;&#22240;&#26524;&#65307;(2)&#20219;&#20309;&#21407;&#22240;&#37117;&#21487;&#20197;&#20165;&#29992;&#22235;&#20010;&#23376;&#21151;&#33021;&#36827;&#34892;&#20998;&#35299;&#65292;&#21363;&#23454;&#29616;&#12289;&#38450;&#27490;&#12289;&#20801;&#35768;&#21644;&#19981;&#20801;&#35768;&#65307;(3)&#26368;&#21518;&#19977;&#20010;&#23376;&#21151;&#33021;&#21487;&#20197;&#20165;&#29992;&#23454;&#29616;&#26469;&#23450;&#20041;&#12290;&#25454;&#27492;&#21487;&#20197;&#24471;&#20986;&#65292;&#22240;&#26524;&#30340;&#26412;&#36136;&#22312;&#20110;&#19968;&#20010;&#21333;&#19968;&#30340;&#21151;&#33021;&#65292;&#21363;&#23454;&#29616;&#12290;&#29616;&#22312;&#38656;&#35201;&#38416;&#26126;&#23454;&#29616;&#21151;&#33021;&#30340;&#24615;&#36136;&#65292;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#21482;&#37096;&#20998;&#23637;&#24320;&#20102;&#35813;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#35752;&#35770;&#20102;&#19978;&#36848;&#22240;&#26524;&#35770;&#20013;&#30340;&#20960;&#20010;&#22522;&#26412;&#21407;&#21017;&#65292;&#22240;&#20026;&#36825;&#20123;&#21407;&#21017;&#22312;&#35752;&#35770;&#20013;&#24456;&#26377;&#24110;&#21161;&#65292;&#28982;&#21518;&#24635;&#32467;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#26368;&#32456;&#25581;&#31034;&#20102;&#23454;&#29616;&#30340;&#24615;&#36136;&#65292;&#20174;&#32780;&#20840;&#38754;&#35299;&#20915;&#20102;&#22240;&#26524;&#26159;&#20160;&#20040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the standpoint of applied ontology, the problem of understanding and modeling causation has been recently challenged on the premise that causation is real. As a consequence, the following three results were obtained: (1) causation can be understood via the notion of systemic function; (2) any cause can be decomposed using only four subfunctions, namely Achieves, Prevents, Allows, and Disallows; and (3) the last three subfunctions can be defined in terms of Achieves alone. It follows that the essence of causation lies in a single function, namely Achieves. It remains to elucidate the nature of the Achieves function, which has been elaborated only partially in the previous work. In this paper, we first discuss a couple of underlying policies in the above-mentioned causal theory since these are useful in the discussion, then summarize the results obtained in the former paper, and finally reveal the nature of Achieves giving a complete solution to the problem of what causation is.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#25351;&#20986;&#20102;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#33258;&#20027;&#33021;&#21147;&#12289;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#20197;&#21450;&#20307;&#39564;&#21040;&#27169;&#31946;&#38382;&#39064;&#30340;&#22823;&#19990;&#30028;&#31561;&#29305;&#28857;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#21017;&#19982;&#27492;&#30456;&#21453;&#12290;</title><link>http://arxiv.org/abs/2307.07515</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#31639;&#27861;&#27169;&#20223;&#65306;&#20026;&#20160;&#20040;&#20154;&#24037;&#8220;&#20195;&#29702;&#8221;&#19981;&#26159;&#65288;&#20063;&#19981;&#20250;&#25104;&#20026;&#65289;&#30495;&#27491;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is algorithmic mimicry: why artificial "agents" are not (and won't be) proper agents. (arXiv:2307.07515v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#25351;&#20986;&#20102;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#33258;&#20027;&#33021;&#21147;&#12289;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#20197;&#21450;&#20307;&#39564;&#21040;&#27169;&#31946;&#38382;&#39064;&#30340;&#22823;&#19990;&#30028;&#31561;&#29305;&#28857;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#21017;&#19982;&#27492;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#37325;&#28857;&#25506;&#35752;&#8220;&#20195;&#29702;&#8221;&#27010;&#24565;&#65292;&#26469;&#25506;&#35752;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;&#20316;&#32773;&#25351;&#20986;&#20102;&#19977;&#20010;&#22522;&#26412;&#30340;&#24046;&#24322;&#65306;&#65288;1&#65289;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#30340;&#33258;&#20027;&#33021;&#21147;&#65292;&#33021;&#22815;&#35774;&#23450;&#33258;&#36523;&#30340;&#20869;&#22312;&#30446;&#26631;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#23384;&#22312;&#20110;&#19968;&#20010;&#30001;&#22806;&#37096;&#20195;&#29702;&#25552;&#20379;&#30446;&#26631;&#20989;&#25968;&#30340;&#35745;&#31639;&#29615;&#22659;&#20013;&#12290;&#65288;2&#65289;&#29983;&#29289;&#31995;&#32479;&#26159;&#20855;&#20307;&#20307;&#29616;&#30340;&#65292;&#21363;&#20854;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#65292;&#32780;&#31639;&#27861;&#36816;&#34892;&#22312;&#35745;&#31639;&#32467;&#26500;&#19978;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#23558;&#36719;&#20214;&#19982;&#30828;&#20214;&#38548;&#31163;&#12290;&#65288;3&#65289;&#29983;&#29289;&#31995;&#32479;&#20307;&#39564;&#21040;&#19968;&#20010;&#24222;&#22823;&#30340;&#19990;&#30028;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38382;&#39064;&#26159;&#27169;&#31946;&#30340;&#65288;&#24182;&#38750;&#20840;&#37096;&#21487;&#23450;&#20041;&#65289;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#23384;&#22312;&#20110;&#19968;&#20010;&#23567;&#19990;&#30028;&#20013;&#65292;&#20854;&#20013;&#25152;&#26377;&#38382;&#39064;&#37117;&#26159;&#26126;&#30830;&#30340;&#12290;&#36825;&#19977;&#20010;&#24046;&#24322;&#35828;&#26126;&#20102;&#29983;&#29289;&#21644;&#31639;&#27861;&#31995;&#32479;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the prospect of developing artificial general intelligence (AGI)? I investigate this question by systematically comparing living and algorithmic systems, with a special focus on the notion of "agency." There are three fundamental differences to consider: (1) Living systems are autopoietic, that is, self-manufacturing, and therefore able to set their own intrinsic goals, while algorithms exist in a computational environment with target functions that are both provided by an external agent. (2) Living systems are embodied in the sense that there is no separation between their symbolic and physical aspects, while algorithms run on computational architectures that maximally isolate software from hardware. (3) Living systems experience a large world, in which most problems are ill-defined (and not all definable), while algorithms exist in a small world, in which all problems are well-defined. These three differences imply that living and algorithmic systems have very different capab
&lt;/p&gt;</description></item><item><title>Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.</title><link>http://arxiv.org/abs/2307.07514</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#19981;&#26159;&#28216;&#25103;&#12290;(arXiv:2307.07514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Explainability is NOT a Game. (arXiv:2307.07514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07514
&lt;/p&gt;
&lt;p&gt;
Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#24110;&#21161;&#20154;&#31867;&#20915;&#31574;&#32773;&#29702;&#35299;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;XAI&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#26469;&#29702;&#35770;&#19978;&#35777;&#26126;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#35770;&#35777;&#65292;&#35828;&#26126;Shapley&#20540;&#21487;&#33021;&#20250;&#32473;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#25552;&#20379;&#35823;&#23548;&#65292;&#20351;&#20854;&#20026;&#39044;&#27979;&#20013;&#26080;&#20851;&#30340;&#29305;&#24449;&#20998;&#37197;&#26356;&#39640;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#23545;&#19982;&#39044;&#27979;&#26377;&#20851;&#30340;&#29305;&#24449;&#20998;&#37197;&#36739;&#20302;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#24847;&#20041;&#22312;&#20110;&#23427;&#20204;&#26377;&#25928;&#22320;&#25361;&#25112;&#20102;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#22810;&#31181;&#25552;&#35758;&#29992;&#27861;&#65292;&#36825;&#20123;&#29992;&#27861;&#27491;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#24555;&#36895;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding complex machine learning (ML) models. One of the hallmarks of XAI are measures of relative feature importance, which are theoretically justified through the use of Shapley values. This paper builds on recent work and offers a simple argument for why Shapley values can provide misleading measures of relative feature importance, by assigning more importance to features that are irrelevant for a prediction, and assigning less importance to features that are relevant for a prediction. The significance of these results is that they effectively challenge the many proposed uses of measures of relative feature importance in a fast-growing range of high-stakes application domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#22270;&#20687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#27515;&#20129;&#29575;&#65292;&#24182;&#22312;MIMIC-IV&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;0.7829&#30340;&#24179;&#22343;C-index&#12290;</title><link>http://arxiv.org/abs/2307.07513</link><description>&lt;p&gt;
&#20351;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#22270;&#20687;&#25913;&#21892;ICU&#27515;&#20129;&#29575;&#39044;&#27979;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An empirical study of using radiology reports and images to improve ICU mortality prediction. (arXiv:2307.07513v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#22270;&#20687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#27515;&#20129;&#29575;&#65292;&#24182;&#22312;MIMIC-IV&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;0.7829&#30340;&#24179;&#22343;C-index&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#35780;&#20998;&#31995;&#32479;&#22312;ICU&#31649;&#29702;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#33021;&#39044;&#27979;&#37325;&#35201;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#27515;&#20129;&#29575;&#12290;&#35768;&#22810;&#35780;&#20998;&#31995;&#32479;&#24050;&#32463;&#22312;ICU&#20013;&#24320;&#21457;&#21644;&#20351;&#29992;&#12290;&#36825;&#20123;&#35780;&#20998;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#20250;&#20007;&#22833;&#21465;&#36848;&#21644;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#20020;&#24202;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;ICU&#27515;&#20129;&#29575;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22235;&#32452;&#29305;&#24449;&#65306;&#65288;1&#65289;&#31616;&#21270;&#24613;&#24615;&#29983;&#29702;&#23398;&#35780;&#20998;&#65288;SAPS&#65289;II&#30340;&#29983;&#29702;&#27979;&#37327;&#12289;&#65288;2&#65289;&#25918;&#23556;&#19987;&#23478;&#39044;&#23450;&#20041;&#30340;&#24120;&#35265;&#33016;&#37096;&#30142;&#30149;&#12289;&#65288;3&#65289;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#65288;4&#65289;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;Medical Information Mart for Intensive Care IV&#65288;MIMIC-IV&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;0.7829&#30340;&#24179;&#22343;C-index&#65288;95%&#30340;&#32622;&#20449;&#21306;&#38388;&#65289;
&lt;/p&gt;
&lt;p&gt;
Background: The predictive Intensive Care Unit (ICU) scoring system plays an important role in ICU management because it predicts important outcomes, especially mortality. Many scoring systems have been developed and used in the ICU. These scoring systems are primarily based on the structured clinical data in the electronic health record (EHR), which may suffer the loss of important clinical information in the narratives and images. Methods: In this work, we build a deep learning based survival prediction model with multi-modality data to predict ICU mortality. Four sets of features are investigated: (1) physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2) common thorax diseases pre-defined by radiologists, (3) BERT-based text representations, and (4) chest X-ray image features. We use the Medical Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the proposed model. Results: Our model achieves the average C-index of 0.7829 (95% confidence i
&lt;/p&gt;</description></item><item><title>RoPDA&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36830;&#32493;&#25552;&#31034;&#36827;&#34892;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#25216;&#26415;&#20197;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.07417</link><description>&lt;p&gt;
RoPDA&#65306;&#29992;&#20110;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#40065;&#26834;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07417
&lt;/p&gt;
&lt;p&gt;
RoPDA&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36830;&#32493;&#25552;&#31034;&#36827;&#34892;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#25216;&#26415;&#20197;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;NER&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#23384;&#22312;&#30772;&#22351;&#21477;&#27861;&#32467;&#26500;&#12289;&#26631;&#35760;-&#26631;&#31614;&#19981;&#21305;&#37197;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#25110;&#25163;&#21160;&#24037;&#20316;&#30340;&#38656;&#27714;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RoPDA: &#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#40065;&#26834;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#36830;&#32493;&#25552;&#31034;&#65292;RoPDA&#36890;&#36807;&#20116;&#20010;&#22522;&#26412;&#30340;&#22686;&#24378;&#25805;&#20316;&#36827;&#34892;&#23454;&#20307;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#29983;&#25104;&#26631;&#31614;&#32763;&#36716;&#21644;&#20445;&#30041;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#65306;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#12290;&#21069;&#32773;&#26377;&#25928;&#22320;&#28040;&#38500;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#21518;&#32773;&#38450;&#27490;&#30452;&#25509;&#21033;&#29992;&#26631;&#31614;&#32763;&#36716;&#26679;&#26412;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been widely used in low-resource NER tasks to tackle the problem of data sparsity. However, previous data augmentation methods have the disadvantages of disrupted syntactic structures, token-label mismatch, and requirement for external knowledge or manual effort. To address these issues, we propose \textbf{Ro}bust \textbf{P}rompt-based \textbf{D}ata \textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained language models (PLMs) with continuous prompt, RoPDA performs entity augmentation and context augmentation through five fundamental augmentation operations to generate label-flipping and label-preserving examples. To optimize the utilization of the augmented samples, we present two techniques: Self-Consistency Filtering and mixup. The former effectively eliminates low-quality samples, while the latter prevents performance degradation arising from the direct utilization of label-flipping samples. Extensive experiments on three benchmarks from diffe
&lt;/p&gt;</description></item><item><title>DataAssist&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#33410;&#30465;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07119</link><description>&lt;p&gt;
DataAssist:&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07119
&lt;/p&gt;
&lt;p&gt;
DataAssist&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#33410;&#30465;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20027;&#35201;&#20851;&#27880;&#20110;&#27169;&#22411;&#36873;&#25321;&#21644;&#21442;&#25968;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#25968;&#25454;&#28165;&#27927;&#21644;&#25972;&#29702;&#25152;&#21344;&#25454;&#30340;&#22823;&#37096;&#20998;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DataAssist&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#20934;&#22791;&#21644;&#28165;&#27927;&#24179;&#21488;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DataAssist&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#21644;&#25968;&#25454;&#28165;&#27927;&#30340;&#31649;&#36947;&#65292;&#21253;&#25324;&#20026;&#29992;&#25143;&#36873;&#25321;&#30340;&#21464;&#37327;&#29983;&#25104;&#21487;&#35270;&#21270;&#65292;&#32479;&#19968;&#25968;&#25454;&#27880;&#37322;&#65292;&#25552;&#20379;&#24322;&#24120;&#20540;&#21024;&#38500;&#24314;&#35758;&#20197;&#21450;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#23548;&#20986;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36731;&#26494;&#19982;&#20854;&#20182;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#25110;&#29992;&#25143;&#25351;&#23450;&#30340;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#20197;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;&#24037;&#20855;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32463;&#27982;&#23398;&#12289;&#21830;&#19994;&#21644;&#39044;&#27979;&#24212;&#29992;&#65292;&#21487;&#33410;&#30465;&#36229;&#36807;50\%&#30340;&#25968;&#25454;&#28165;&#29702;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current automated machine learning (ML) tools are model-centric, focusing on model selection and parameter optimization. However, the majority of the time in data analysis is devoted to data cleaning and wrangling, for which limited tools are available. Here we present DataAssist, an automated data preparation and cleaning platform that enhances dataset quality using ML-informed methods. We show that DataAssist provides a pipeline for exploratory data analysis and data cleaning, including generating visualization for user-selected variables, unifying data annotation, suggesting anomaly removal, and preprocessing data. The exported dataset can be readily integrated with other autoML tools or user-specified model for downstream analysis. Our data-centric tool is applicable to a variety of fields, including economics, business, and forecasting applications saving over 50\% time of the time spent on data cleansing and preparation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#36890;&#36807;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26469;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#35774;&#35745;&#30340;&#20248;&#28857;&#65292;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.06947</link><description>&lt;p&gt;
&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#65306;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;
&lt;/p&gt;
&lt;p&gt;
Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#36890;&#36807;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26469;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#35774;&#35745;&#30340;&#20248;&#28857;&#65292;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#39057;&#35782;&#21035;&#27169;&#22411;&#21033;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#38271;&#36317;&#31163;&#26102;&#31354;&#19978;&#19979;&#25991;&#24314;&#27169;&#12290;&#35270;&#39057;Transformer&#35774;&#35745;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#20197;&#39640;&#35745;&#31639;&#25104;&#26412;&#27169;&#25311;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29992;&#20110;&#35270;&#39057;&#30340;&#21367;&#31215;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#38271;&#36317;&#31163;&#20381;&#36182;&#24314;&#27169;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20004;&#31181;&#35774;&#35745;&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#65288;Video-FocalNet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#21487;&#20197;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#22522;&#20110;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26550;&#26500;&#65292;&#23545;&#33258;&#27880;&#24847;&#21147;&#30340;&#20132;&#20114;&#21644;&#32858;&#21512;&#27493;&#39588;&#36827;&#34892;&#20102;&#39072;&#20498;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#32858;&#21512;&#27493;&#39588;&#21644;&#20132;&#20114;&#27493;&#39588;&#37117;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#21367;&#31215;&#21644;&#36880;&#20803;&#32032;&#20056;&#27861;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#27604;&#35270;&#39057;&#34920;&#36798;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23545;&#24212;&#37096;&#20998;&#35201;&#20302;&#24471;&#22810;&#12290;&#25105;&#20204;&#24191;&#27867;&#25506;&#32034;&#20102;&#28966;&#28857;&#35843;&#21046;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost. In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency. Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively explore the design space of focal modu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Rad-ReStruct&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;hi-VQA&#65292;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#26469;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23454;&#39564;&#35777;&#26126;hi-VQA&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05766</link><description>&lt;p&gt;
Rad-ReStruct: &#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;VQA&#22522;&#20934;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Rad-ReStruct&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;hi-VQA&#65292;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#26469;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23454;&#39564;&#35777;&#26126;hi-VQA&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#26159;&#25918;&#23556;&#31185;&#21307;&#29983;&#19982;&#20854;&#20182;&#21307;&#21153;&#20154;&#21592;&#20043;&#38388;&#27807;&#36890;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#20294;&#20854;&#21487;&#33021;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#20854;&#20013;&#19968;&#31181;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#30340;&#26041;&#27861;&#26159;&#32467;&#26500;&#21270;&#25253;&#21578;&#65292;&#23427;&#27604;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#26356;&#33410;&#32422;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25253;&#21578;&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#24182;&#19988;&#30446;&#21069;&#27809;&#26377;&#20844;&#24320;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Rad-ReStruct&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#25552;&#20986;&#20102;hi-VQA&#65292;&#19968;&#31181;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#20197;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;hi-VQA&#22312;&#21307;&#23398;VQA&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiology reporting is a crucial part of the communication between radiologists and other medical professionals, but it can be time-consuming and error-prone. One approach to alleviate this is structured reporting, which saves time and enables a more accurate evaluation than free-text reports. However, there is limited research on automating structured reporting, and no public benchmark is available for evaluating and comparing different methods. To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that provides fine-grained, hierarchically ordered annotations in the form of structured reports for X-Ray images. We model the structured reporting task as hierarchical visual question answering (VQA) and propose hi-VQA, a novel method that considers prior context in the form of previously asked questions and answers for populating a structured radiology report. Our experiments show that hi-VQA achieves competitive performance to the state-of-the-art on the medical VQA benc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;xAI&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#22312;AI&#24490;&#29615;&#20013;&#36827;&#34892;&#35270;&#35273;&#26816;&#26597;&#30340;&#35770;&#25991;&#25506;&#35752;&#20102;&#24037;&#19994; 5.0 &#20013;&#20154;&#26426;&#21327;&#20316;&#30340;&#26032;&#26426;&#20250;&#65292;&#24182;&#20998;&#20139;&#20102;&#20851;&#20110;&#35270;&#35273;&#26816;&#26597;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#12289;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#21644;&#32593;&#32476;&#23433;&#20840;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05508</link><description>&lt;p&gt;
&#36890;&#36807;xAI&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#22312;AI&#24490;&#29615;&#20013;&#36827;&#34892;&#35270;&#35273;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Human in the AI loop via xAI and Active Learning for Visual Inspection. (arXiv:2307.05508v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;xAI&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#22312;AI&#24490;&#29615;&#20013;&#36827;&#34892;&#35270;&#35273;&#26816;&#26597;&#30340;&#35770;&#25991;&#25506;&#35752;&#20102;&#24037;&#19994; 5.0 &#20013;&#20154;&#26426;&#21327;&#20316;&#30340;&#26032;&#26426;&#20250;&#65292;&#24182;&#20998;&#20139;&#20102;&#20851;&#20110;&#35270;&#35273;&#26816;&#26597;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#12289;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#21644;&#32593;&#32476;&#23433;&#20840;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#38761;&#21629;&#36890;&#36807;&#24341;&#20837;&#33258;&#21160;&#21270;&#26469;&#25913;&#21464;&#21046;&#36896;&#19994;&#65292;&#22686;&#21152;&#30340;&#33258;&#21160;&#21270;&#25913;&#21464;&#20102;&#20154;&#24037;&#24037;&#20154;&#30340;&#35282;&#33394;&#12290;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#24320;&#36767;&#20102;&#20154;&#26426;&#21327;&#20316;&#30340;&#26032;&#39046;&#22495;&#12290;&#26412;&#31456;&#39318;&#20808;&#25551;&#36848;&#20102;&#24037;&#19994;5.0&#65292;&#20154;&#26426;&#21327;&#20316;&#20197;&#21450;&#20851;&#20110;&#36136;&#37327;&#26816;&#26597;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#37325;&#28857;&#26159;&#35270;&#35273;&#26816;&#26597;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22312;&#35270;&#35273;&#26816;&#26597;&#20013;&#23454;&#29616;&#21644;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#30340;&#35266;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#22312;&#27431;&#30431;H2020 STAR&#39033;&#30446;&#20013;&#20851;&#20110;&#35270;&#35273;&#26816;&#26597;&#30340;&#19968;&#20123;&#32467;&#26524;&#65292;&#32771;&#34385;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#21644;&#32593;&#32476;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial revolutions have historically disrupted manufacturing by introducing automation into production. Increasing automation reshapes the role of the human worker. Advances in robotics and artificial intelligence open new frontiers of human-machine collaboration. In this chapter, we first describe Industry 5.0, human-machine collaboration, and state-of-the-art regarding quality inspection, emphasizing visual inspection. We then provide our perspective on how human-machine collaboration could be realized and enhanced in visual inspection. Finally, we share some of the results obtained in the EU H2020 STAR project regarding visual inspection, considering artificial intelligence, human digital twins, and cybersecurity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.05360</link><description>&lt;p&gt;
&#25581;&#24320;&#24040;&#20154;&#30340;&#30495;&#38754;&#30446;&#65306;&#23545;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#28145;&#21051;&#22320;&#37325;&#22609;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#25216;&#26415;&#39046;&#22495;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#26377;&#30528;&#29420;&#29305;&#20043;&#22788;&#65292;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#36718;&#23545;&#35805;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#23637;&#31034;&#20986;&#23545;&#32534;&#30721;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#23545;ChatGPT&#30340;&#32534;&#30721;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;Python&#32534;&#31243;&#35821;&#35328;&#65292;&#20197;&#21450;&#38598;&#20013;&#22312;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#19978;&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#20027;&#39064;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35780;&#20272;ChatGPT&#35299;&#20915;&#25152;&#25552;&#20132;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#35780;&#20272;&#20854;&#20195;&#30721;&#36136;&#37327;&#20197;&#21450;&#20195;&#30721;&#24341;&#21457;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;&#24403;ChatGPT&#30340;&#20195;&#30721;&#25104;&#21151;&#25191;&#34892;&#20294;&#26410;&#33021;&#35299;&#20915;&#25163;&#22836;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#20250;&#30740;&#31350;&#36890;&#36807;&#30340;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#27169;&#24335;&#65292;&#20197;&#20102;&#35299;ChatGPT&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.05358</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#35843;&#33410;&#22120;&#35299;&#20915;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20174;&#20998;&#25955;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#30001;&#20110;&#20998;&#25955;&#23458;&#25143;&#31471;&#19978;&#26631;&#31614;&#31232;&#32570;&#65292;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#20986;&#29616;&#20197;&#20174;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20013;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FSSL&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26631;&#31614;&#25968;&#25454;&#29420;&#31435;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FSSL&#30340;&#26356;&#23454;&#38469;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#20165;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#21516;&#65292;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20063;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;FSSL&#26694;&#26550;&#65292;FedDure&#12290;FedDure&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#65288;C-reg&#65289;&#21644;&#32454;&#35843;&#33410;&#22120;&#65288;F-reg&#65289;&#35299;&#38500;&#20102;&#20197;&#21069;&#30340;&#20551;&#35774;&#65306;C-reg&#36890;&#36807;&#36319;&#36394;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#23398;&#20064;&#25928;&#26524;&#26469;&#35268;&#33539;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#65307;F-reg&#23398;&#20064;&#19968;&#20010;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#36866;&#24212;&#23458;&#25143;&#31471;&#20869;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24037;&#20855;ECS&#65292;&#29992;&#20110;&#20445;&#35777;&#25968;&#25454;&#36136;&#37327;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#26816;&#27979;&#20986;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.04368</link><description>&lt;p&gt;
ECS -- &#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
ECS -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.04368v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24037;&#20855;ECS&#65292;&#29992;&#20110;&#20445;&#35777;&#25968;&#25454;&#36136;&#37327;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#26816;&#27979;&#20986;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#21450;&#20854;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#26041;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#25968;&#23398;&#22522;&#30784;&#65292;&#28982;&#21518;&#36890;&#36807;&#22810;&#20010;&#31034;&#20363;&#20171;&#32461;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#20986;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important. In this paper we present a novel approach for the assurance of data quality. For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples. This results in the detection of data points with potentially harmful properties for the use in safety-critical systems.
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.04251</link><description>&lt;p&gt;
ChatGPT&#22312;&#29983;&#25104;&#24335;AI&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#24212;&#29992;&#65306;&#19968;&#20221;&#31616;&#27905;&#30340;&#35843;&#26597;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04251
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32463;&#36807;&#31934;&#24515;&#35757;&#32451;&#24182;&#20351;&#29992;&#20102;&#22823;&#37327;&#25968;&#25454;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#65292;&#24182;&#25512;&#21160;&#20102;LLM&#33021;&#21147;&#30340;&#36793;&#30028;&#12290;ChatGPT&#22312;&#22823;&#35268;&#27169;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26222;&#36941;&#20844;&#20247;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#20114;&#21160;&#65292;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#36824;&#24341;&#21457;&#20102;&#24320;&#21457;&#31867;&#20284;&#25216;&#26415;&#21644;&#30740;&#31350;&#20854;&#24212;&#29992;&#21644;&#24433;&#21709;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;ChatGPT&#21450;&#20854;&#28436;&#21270;&#30340;&#24403;&#21069;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#31616;&#26126;&#35843;&#26597;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;ChatGPT&#30340;&#29627;&#29827;&#30418;&#21644;&#40657;&#30418;&#35270;&#35282;&#65292;&#21253;&#25324;&#25216;&#26415;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;&#22522;&#26412;&#35201;&#32032;&#65292;&#20197;&#21450;&#20854;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#24433;&#21709;&#12290;&#29627;&#29827;&#30418;&#26041;&#27861;&#30528;&#37325;&#20110;&#29702;&#35299;&#25216;&#26415;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#32780;&#40657;&#30418;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#65292;&#22240;&#27492;&#30740;&#31350;&#20854;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.03913</link><description>&lt;p&gt;
&#22312;&#21457;&#23637;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20013;&#24212;&#29992;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#20197;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20849;&#21516;&#35748;&#30693;&#31995;&#32479;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems. (arXiv:2307.03913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21644;&#24212;&#29992;&#24050;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#23558;&#20316;&#20026;&#19968;&#21517;&#38431;&#21451;&#32780;&#19981;&#20165;&#20165;&#26159;&#24037;&#20855;&#19982;&#20154;&#31867;&#21327;&#20316;&#12290;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#27599;&#20010;&#25104;&#21592;&#30340;&#24050;&#30693;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#65292;&#24182;&#23558;&#32852;&#21512;&#24615;&#33021;&#25552;&#39640;&#21040;&#20219;&#20309;&#23454;&#20307;&#20043;&#19978;&#12290;2023&#24180;&#20840;&#22269;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#25112;&#30053;&#35745;&#21010;&#26356;&#26032;&#35748;&#35782;&#21040;&#65292;&#20027;&#35201;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29420;&#31435;&#24615;&#33021;&#30340;&#30740;&#31350;&#35745;&#21010;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#20154;&#24037;&#26234;&#33021;&#22312;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#24517;&#39035;&#25552;&#20379;&#30340;&#21151;&#33021;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#33021;&#20316;&#20026;&#20154;&#31867;&#30340;&#38431;&#21451;&#23384;&#22312;&#20105;&#35758;&#12290;&#20027;&#35201;&#30340;&#20851;&#27880;&#28857;&#22312;&#20110;&#37319;&#29992;"&#21327;&#20316;"&#33539;&#24335;&#26159;&#21542;&#19982;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#30456;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research and application have used human-AI teaming (HAT) as a new paradigm to develop AI systems. HAT recognizes that AI will function as a teammate instead of simply a tool in collaboration with humans. Effective human-AI teams need to be capable of taking advantage of the unique abilities of both humans and AI while overcoming the known challenges and limitations of each member, augmenting human capabilities, and raising joint performance beyond that of either entity. The National AI Research and Strategic Plan 2023 update has recognized that research programs focusing primarily on the independent performance of AI systems generally fail to consider the functionality that AI must provide within the context of dynamic, adaptive, and collaborative teams and calls for further research on human-AI teaming and collaboration. However, there has been debate about whether AI can work as a teammate with humans. The primary concern is that adopting the "teaming" paradigm contradicts the human
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25903;&#25345;AML&#24739;&#32773;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#19987;&#23478;&#38656;&#27714;&#39069;&#22806;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2307.02631</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#20197;&#25903;&#25345;AML&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
An explainable model to support the decision about the therapy protocol for AML. (arXiv:2307.02631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25903;&#25345;AML&#24739;&#32773;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#19987;&#23478;&#38656;&#27714;&#39069;&#22806;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#65288;AML&#65289;&#26159;&#19968;&#31181;&#26368;&#20855;&#20405;&#30053;&#24615;&#30340;&#34880;&#28082;&#32959;&#30244;&#12290;&#20026;&#20102;&#25903;&#25345;&#19987;&#23478;&#20851;&#20110;&#21512;&#36866;&#27835;&#30103;&#30340;&#20915;&#31574;&#65292;AML&#24739;&#32773;&#26681;&#25454;&#20854;&#32454;&#32990;&#36951;&#20256;&#21644;&#20998;&#23376;&#29305;&#24449;&#33719;&#24471;&#39044;&#21518;&#20449;&#24687;&#65292;&#36890;&#24120;&#20998;&#20026;&#26377;&#21033;&#12289;&#20013;&#31561;&#21644;&#19981;&#21033;&#19977;&#20010;&#39118;&#38505;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#24050;&#30693;&#38382;&#39064;&#65292;&#22914;&#21516;&#19968;&#39118;&#38505;&#32452;&#20013;&#24739;&#32773;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#21644;&#20013;&#39118;&#38505;&#31867;&#21035;&#30340;&#28165;&#26224;&#23450;&#20041;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;AML&#24739;&#32773;&#34987;&#24402;&#20026;&#20013;&#39118;&#38505;&#20998;&#31867;&#65292;&#19987;&#23478;&#24120;&#38656;&#36827;&#34892;&#20854;&#20182;&#27979;&#35797;&#21644;&#20998;&#26512;&#65292;&#23548;&#33268;&#27835;&#30103;&#24310;&#36831;&#21644;&#24739;&#32773;&#20020;&#24202;&#29366;&#20917;&#24694;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#25454;&#20998;&#26512;&#21644;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#26681;&#25454;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#30830;&#23450;&#26368;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acute Myeloid Leukemia (AML) is one of the most aggressive types of hematological neoplasm. To support the specialists' decision about the appropriate therapy, patients with AML receive a prognostic of outcomes according to their cytogenetic and molecular characteristics, often divided into three risk categories: favorable, intermediate, and adverse. However, the current risk classification has known problems, such as the heterogeneity between patients of the same risk group and no clear definition of the intermediate risk category. Moreover, as most patients with AML receive an intermediate-risk classification, specialists often demand other tests and analyses, leading to delayed treatment and worsening of the patient's clinical condition. This paper presents the data analysis and an explainable machine-learning model to support the decision about the most appropriate therapy protocol according to the patient's survival prediction. In addition to the prediction model being explainable
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.02054</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;
&lt;/p&gt;
&lt;p&gt;
Emoji Prediction using Transformer Models. (arXiv:2307.02054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02054
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#30340;&#39057;&#29575;&#22823;&#24133;&#22686;&#21152;&#65292;&#20351;&#24471;&#23427;&#20204;&#25104;&#20026;&#20102;&#29702;&#35299;&#22312;&#32447;&#27807;&#36890;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#21547;&#31946;&#30340;&#29305;&#24615;&#65292;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#20013;&#34920;&#24773;&#31526;&#21495;&#30340;&#21547;&#20041;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;BERT&#36827;&#34892;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#65292;BERT&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;&#25991;&#26412;&#21644;&#34920;&#24773;&#31526;&#21495;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#21512;&#36866;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;75&#65285;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the use of emojis in social media has increased dramatically, making them an important element in understanding online communication. However, predicting the meaning of emojis in a given text is a challenging task due to their ambiguous nature. In this study, we propose a transformer-based approach for emoji prediction using BERT, a widely-used pre-trained language model. We fine-tuned BERT on a large corpus of text containing both text and emojis to predict the most appropriate emoji for a given text. Our experimental results demonstrate that our approach outperforms several state-of-the-art models in predicting emojis with an accuracy of over 75 percent. This work has potential applications in natural language processing, sentiment analysis, and social media marketing.
&lt;/p&gt;</description></item><item><title>InstructEval&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#23545;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31574;&#21010;&#30340;&#25163;&#21160;&#32534;&#20889;&#30340;&#25351;&#20196;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00259</link><description>&lt;p&gt;
InstructEval: &#31995;&#32479;&#35780;&#20272;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InstructEval: Systematic Evaluation of Instruction Selection Methods. (arXiv:2307.00259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00259
&lt;/p&gt;
&lt;p&gt;
InstructEval&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#23545;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31574;&#21010;&#30340;&#25163;&#21160;&#32534;&#20889;&#30340;&#25351;&#20196;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (ICL) &#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#21644;&#19968;&#23567;&#32452;&#27880;&#37322;&#31034;&#20363;&#26469;&#25552;&#31034;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25191;&#34892;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25552;&#31034;&#20013;&#20351;&#29992;&#30340;&#36755;&#20837;&#30340;&#32454;&#33410;&#23545; ICL &#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#36825;&#28608;&#21169;&#20102;&#25351;&#20196;&#36873;&#25321;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#36873;&#25321;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#20165;&#38480;&#20110;&#27169;&#22411;&#21644;&#20219;&#21153;&#30340;&#27973;&#23618;&#23376;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#27934;&#23519;&#21147;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010; ICL &#35780;&#20272;&#22871;&#20214;&#65292;&#20197;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#35813;&#22871;&#20214;&#21253;&#25324;&#26469;&#33258;4&#20010;&#19981;&#21516;&#27169;&#22411;&#23478;&#26063;&#30340;13&#20010;&#24320;&#28304;LLM&#65292;&#28085;&#30422;9&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20195;&#34920;&#20102;3&#20010;&#20998;&#31867;&#20013;&#21508;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#20102;7&#31181;&#21463;&#27426;&#36814;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#30456;&#23545;&#20110;ICL&#30456;&#20851;&#30340;&#20116;&#39033;&#26399;&#26395;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#31574;&#21010;&#30340;&#25163;&#21160;&#32534;&#20889;&#30340;&#25351;&#20196;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that the precise details of the inputs used in the prompt significantly impacts ICL, which has incentivized instruction selection algorithms. The effect of instruction-choice however is severely underexplored, with existing analyses being restricted to shallow subsets of models and tasks, which limits the generalizability of their insights. We develop an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from 4 distinct model families and covers 9 different tasks, representing a range of task types across 3 categories. In this work, we evaluate the relative performance of 7 popular instruction selection methods using our benchmark over five desiderata relevant to ICL. We discover that using curated manually-written instru
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#65292;&#29992;&#20110;&#33719;&#21462;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20154;&#31867;&#30340;&#20171;&#20837;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#26469;&#35780;&#20272;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#26631;&#20934;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13935</link><description>&lt;p&gt;
&#33391;&#22909;&#30340;&#35299;&#37322;&#32773;&#26263;&#22320;&#37324;&#26159;&#20154;&#31867;-&#20027;&#21160;&#23398;&#20064;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Good Explainers Secretly Human-in-the-Loop Active Learners?. (arXiv:2306.13935v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#65292;&#29992;&#20110;&#33719;&#21462;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20154;&#31867;&#30340;&#20171;&#20837;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#26469;&#35780;&#20272;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#26631;&#20934;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#36817;&#24180;&#26469;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23427;&#22312;&#30740;&#31350;&#27169;&#22411;&#39044;&#27979;&#20197;&#25910;&#38598;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#30456;&#24403;&#20110;&#20027;&#21160;&#23398;&#20064;&#65292;&#20854;&#20013;&#26597;&#35810;&#31574;&#30053;&#28041;&#21450;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20154;&#31867;&#35282;&#33394;&#30340;&#25968;&#23398;&#36817;&#20284;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#30340;&#36890;&#29992;&#24418;&#24335;&#21270;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#27604;&#36739;&#27492;&#29992;&#27861;&#19982;&#26631;&#20934;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#25193;&#23637;&#24037;&#20316;&#27969;&#12290;&#19968;&#20010;&#39069;&#22806;&#30340;&#22909;&#22788;&#26159;&#65292;&#23427;&#20204;&#30340;&#25928;&#29992;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#26469;&#35780;&#20272;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#26114;&#36149;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) techniques have become popular for multiple use-cases in the past few years. Here we consider its use in studying model predictions to gather additional training data. We argue that this is equivalent to Active Learning, where the query strategy involves a human-in-the-loop. We provide a mathematical approximation for the role of the human, and present a general formalization of the end-to-end workflow. This enables us to rigorously compare this use with standard Active Learning algorithms, while allowing for extensions to the workflow. An added benefit is that their utility can be assessed via simulation instead of conducting expensive user-studies. We also present some initial promising results.
&lt;/p&gt;</description></item><item><title>DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.09862</link><description>&lt;p&gt;
DoubleAdapt&#65306;&#19968;&#31181;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09862
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#26159;&#37327;&#21270;&#25237;&#36164;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#20934;&#30830;&#39044;&#27979;&#20215;&#26684;&#36235;&#21183;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20316;&#20026;&#19968;&#39033;&#22312;&#32447;&#26381;&#21153;&#65292;&#32929;&#31080;&#25968;&#25454;&#38543;&#26102;&#38543;&#22320;&#25345;&#32493;&#21040;&#36798;&#12290;&#20351;&#29992;&#26368;&#26032;&#25968;&#25454;&#23545;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#26356;&#26032;&#26159;&#23454;&#29992;&#32780;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#26032;&#25968;&#25454;&#21487;&#33021;&#25581;&#31034;&#20102;&#26410;&#26469;&#32929;&#31080;&#24066;&#22330;&#20013;&#20250;&#37325;&#22797;&#20986;&#29616;&#30340;&#19968;&#20123;&#26032;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#24067;&#28418;&#31227;&#65288;&#21363;&#27010;&#24565;&#28418;&#31227;&#65289;&#30340;&#25361;&#25112;&#65292;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#38543;&#30528;&#32929;&#31080;&#24066;&#22330;&#21160;&#24577;&#28436;&#21464;&#65292;&#26410;&#26469;&#25968;&#25454;&#30340;&#20998;&#24067;&#21487;&#33021;&#20250;&#19982;&#22686;&#37327;&#25968;&#25454;&#31245;&#24494;&#25110;&#26174;&#30528;&#22320;&#19981;&#21516;&#65292;&#20174;&#32780;&#38459;&#30861;&#22686;&#37327;&#26356;&#26032;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20004;&#20010;&#36866;&#37197;&#22120;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#8212;&#8212;DoubleAdapt&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08044</link><description>&lt;p&gt;
&#21098;&#26525;&#26041;&#24335;&#25552;&#39640;&#21487;&#38752;&#31574;&#30053;&#65306;&#19968;&#31181;&#22810;&#30446;&#26631;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#37325;&#30151;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21307;&#30103;&#20915;&#31574;&#20855;&#26377;&#36830;&#32493;&#24615;&#65292;&#22240;&#27492;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#26377;&#26395;&#21046;&#23450;&#31934;&#30830;&#30340;&#25968;&#25454;&#39537;&#21160;&#27835;&#30103;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20027;&#35201;&#22522;&#20110;&#27515;&#20129;&#29575;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#31232;&#30095;&#24615;&#65292;&#23548;&#33268;&#31163;&#32447;&#20272;&#35745;&#30340;&#31283;&#23450;&#24615;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#21487;&#38752;&#30340;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#23558;&#30456;&#20851;&#20294;&#22024;&#26434;&#30340;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#24863;&#20852;&#36259;&#30340;&#20027;&#35201;&#32467;&#26524;&#65288;&#20363;&#22914;&#24739;&#32773;&#29983;&#23384;&#29575;&#65289;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#26681;&#25454;&#25152;&#26377;&#21487;&#29992;&#22870;&#21169;&#23545;&#21160;&#20316;&#38598;&#36827;&#34892;&#21098;&#26525;&#65292;&#28982;&#21518;&#22522;&#20110;&#31232;&#30095;&#20027;&#35201;&#22870;&#21169;&#65292;&#20351;&#29992;&#21463;&#38480;&#21160;&#20316;&#38598;&#36827;&#34892;&#26368;&#32456;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#35299;&#31163;&#20934;&#30830;&#21644;&#36817;&#20284;&#22870;&#21169;&#26469;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#30340;&#28508;&#22312;&#25197;&#26354;&#65292;&#23454;&#29616;&#20102;&#19978;&#36848;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all whi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.02797</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#23545;&#25277;&#35937;&#31526;&#21495;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#39640;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#12290;&#25105;&#20204;&#26681;&#25454;&#20154;&#31867;&#25968;&#25454;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.02786</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36335;&#24452;&#20960;&#20309;&#23548;&#33322;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;
&lt;/p&gt;
&lt;p&gt;
Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#35299;&#37322;&#65288;&#19981;&#36879;&#26126;&#30340;&#65289;&#39044;&#27979;&#27169;&#22411;&#20915;&#31574;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#20854;&#29983;&#25104;&#24448;&#24448;&#21463;&#21040;&#31639;&#27861;&#21644;&#29305;&#23450;&#39046;&#22495;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#22914;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#21644;&#23646;&#24615;&#30340;&#65288;&#19981;&#65289;&#21487;&#21464;&#24615;&#25110;&#21464;&#21270;&#30340;&#26041;&#21521;&#24615;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20854;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#38500;&#20102;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#26412;&#36523;&#30340;&#35201;&#27714;&#20043;&#22806;&#65292;&#24050;&#30693;&#31639;&#27861;&#21487;&#34892;&#24615;&#36335;&#24452;&#19982;&#20107;&#23454;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21363;&#31639;&#27861;&#21487;&#35785;&#27714;&#65292;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#25216;&#26415;&#32771;&#34385;&#22240;&#32032;&#12290;&#23613;&#31649;&#36825;&#20004;&#20010;&#35201;&#27714;&#30830;&#20445;&#20102;&#26053;&#31243;&#30340;&#27493;&#39588;&#21644;&#30446;&#30340;&#22320;&#30340;&#21512;&#29702;&#24615;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#24573;&#30053;&#20102;&#36825;&#31181;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#26053;&#31243;&#65307;&#28982;&#21518;&#23637;&#31034;&#20102;&#22914;&#20309;&#23548;&#33322;&#12289;&#25512;&#29702;&#21644;&#27604;&#36739;&#36825;&#20123;&#36712;&#36857;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to algorithmic and domain-specific constraints -- such as density-based feasibility and attribute (im)mutability or directionality of change -- that aim to maximise their real-life utility. In addition to desiderata with respect to the counterfactual instance itself, existence of a viable path connecting it with the factual data point, known as algorithmic recourse, has become an important technical consideration. While both of these requirements ensure that the steps of the journey as well as its destination are admissible, current literature neglects the multiplicity of such counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys; we then show how to navigate, reason about and compare the geometry of these trajectories -
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#36890;&#39046;&#22495;&#24773;&#22659;&#25512;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#22659;&#20915;&#31574;&#12289;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#21644;&#35299;&#20915;&#20154;&#31867;&#39550;&#39542;&#32771;&#35797;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#22235;&#31181;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#22312;&#19981;&#21516;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02520</link><description>&lt;p&gt;
&#20132;&#36890;&#29702;&#35299;&#30340;&#24773;&#22659;&#25512;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Situational Reasoning for Traffic Understanding. (arXiv:2306.02520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#36890;&#39046;&#22495;&#24773;&#22659;&#25512;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#22659;&#20915;&#31574;&#12289;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#21644;&#35299;&#20915;&#20154;&#31867;&#39550;&#39542;&#32771;&#35797;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#22235;&#31181;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#22312;&#19981;&#21516;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#30417;&#25511;(ITMo)&#25216;&#26415;&#26377;&#28508;&#21147;&#25913;&#21892;&#36947;&#36335;&#23433;&#20840;/&#23433;&#20840;&#24615;&#65292;&#23454;&#29616;&#26234;&#33021;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#12290;&#20102;&#35299;&#20132;&#36890;&#24773;&#20917;&#38656;&#35201;&#23558;&#24863;&#30693;&#20449;&#24687;&#19982;&#39046;&#22495;&#29305;&#23450;&#21644;&#22240;&#26524;&#24120;&#35782;&#30693;&#35782;&#22797;&#26434;&#34701;&#21512;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#20026;&#20132;&#36890;&#30417;&#25511;&#25552;&#20379;&#20102;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#20294;&#27169;&#22411;&#33021;&#21542;&#26377;&#25928;&#22320;&#23545;&#40784;&#36825;&#20123;&#20449;&#24687;&#26469;&#28304;&#24182;&#22312;&#26032;&#22330;&#26223;&#20013;&#25512;&#29702;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#35780;&#20272;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#29992;&#20110;&#20132;&#36890;&#39046;&#22495;&#24773;&#22659;&#25512;&#29702;&#30340;&#26032;&#22411;&#25991;&#26412;&#20219;&#21153;&#65306;i) BDD-QA&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;(LMs)&#25191;&#34892;&#24773;&#22659;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;ii) TV-QA&#65292;&#35780;&#20272;LMs&#25512;&#29702;&#22797;&#26434;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;iii) HDT-QA&#65292;&#35780;&#20272;&#27169;&#22411;&#35299;&#20915;&#20154;&#31867;&#39550;&#39542;&#32771;&#35797;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#36890;&#29992;&#24615;&#30340;&#22235;&#31181;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent Traffic Monitoring (ITMo) technologies hold the potential for improving road safety/security and for enabling smart city infrastructure. Understanding traffic situations requires a complex fusion of perceptual information with domain-specific and causal commonsense knowledge. Whereas prior work has provided benchmarks and methods for traffic monitoring, it remains unclear whether models can effectively align these information sources and reason in novel scenarios. To address this assessment gap, we devise three novel text-based tasks for situational reasoning in the traffic domain: i) BDD-QA, which evaluates the ability of Language Models (LMs) to perform situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason about complex event causality, and iii) HDT-QA, which evaluates the ability of models to solve human driving exams. We adopt four knowledge-enhanced methods that have shown generalization capability across language reasoning tasks in prior work
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#23547;&#25214;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#30340;&#38750;&#20132;&#25442;&#31639;&#27861;&#25110;&#25552;&#20379;&#19981;&#21487;&#34892;&#24615;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26377;&#25928;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#26469;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25214;&#21040;&#24050;&#30693;&#30340;&#26368;&#20339;&#31639;&#27861;&#25110;&#25913;&#36827;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01097</link><description>&lt;p&gt;
&#19981;&#23481;&#26131;&#20986;&#38169;&#30340;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;:&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#35299;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Matrix Multiplication Without Tears: A Constraint Programming Approach. (arXiv:2306.01097v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#23547;&#25214;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#30340;&#38750;&#20132;&#25442;&#31639;&#27861;&#25110;&#25552;&#20379;&#19981;&#21487;&#34892;&#24615;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26377;&#25928;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#26469;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25214;&#21040;&#24050;&#30693;&#30340;&#26368;&#20339;&#31639;&#27861;&#25110;&#25913;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#23558;&#19968;&#20010; $N \times M$ &#30697;&#38453;&#19982;&#19968;&#20010; $M \times P$ &#30340;&#30697;&#38453;&#30456;&#20056;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#20110;&#26420;&#32032; $NMP$ &#26041;&#27861;&#24314;&#35758;&#30340;&#20056;&#27861;&#27425;&#25968;&#12290;&#20854;&#20013;&#26368;&#33879;&#21517;&#30340;&#26159; Strassen &#31639;&#27861;&#65292;&#21487;&#23558;&#20004;&#20010; $2\times 2$ &#30340;&#30697;&#38453;&#30456;&#20056;&#65292;&#21482;&#38656; 7 &#27425;&#32780;&#38750; 8 &#27425;&#20056;&#27861;&#12290;&#36825;&#24341;&#20986;&#20102;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#20854;&#20013;&#24517;&#39035;&#36873;&#25321;&#19968;&#32452; $R &lt;NMP$ &#20056;&#27861;&#39033;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#20197;&#28385;&#36275;&#36755;&#20986;&#30697;&#38453;&#30340;&#27491;&#30830;&#24615;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#23547;&#25214;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#30340;&#38750;&#20132;&#25442;&#31639;&#27861;&#25110;&#25552;&#20379;&#19981;&#21487;&#34892;&#24615;&#35777;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26377;&#25928;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#26469;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#20943;&#23569;&#35201;&#26816;&#26597;&#30340;&#35299;&#20915;&#26041;&#26696;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25214;&#21040;&#24050;&#30693;&#30340;&#26368;&#20339;&#31639;&#27861;&#25110;&#25913;&#36827;&#30340;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#27979;&#35797;&#30340;&#25152;&#26377;&#30697;&#38453;&#22823;&#23567;&#65292;&#26368;&#22823;&#20026; $768 \times 768$&#12290;
&lt;/p&gt;
&lt;p&gt;
It is known that the multiplication of an $N \times M$ matrix with an $M \times P$ matrix can be performed using fewer multiplications than what the naive $NMP$ approach suggests. The most famous instance of this is Strassen's algorithm for multiplying two $2\times 2$ matrices in 7 instead of 8 multiplications. This gives rise to the constraint satisfaction problem of fast matrix multiplication, where a set of $R &lt; NMP$ multiplication terms must be chosen and combined such that they satisfy correctness constraints on the output matrix. Despite its highly combinatorial nature, this problem has not been exhaustively examined from that perspective, as evidenced for example by the recent deep reinforcement learning approach of AlphaTensor. In this work, we propose a simple yet novel Constraint Programming approach to find non-commutative algorithms for fast matrix multiplication or provide proof of infeasibility otherwise. We propose a set of symmetry-breaking constraints and valid inequal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21010;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01007</link><description>&lt;p&gt;
&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#20844;&#24179;&#35299;&#32544;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21010;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#25968;&#25454;&#25353;&#26102;&#38388;&#39034;&#24207;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#25509;&#25910;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20998;&#24067;&#20551;&#35774;&#21487;&#33021;&#32463;&#24120;&#21464;&#21270;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#23545;&#21160;&#24577;&#36951;&#25022;&#25110;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#20005;&#26684;&#30028;&#38480;&#26469;&#23637;&#31034;&#20854;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#23436;&#20840;&#24573;&#30053;&#20102;&#24102;&#26377;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#23398;&#20064;&#65292;&#20854;&#23450;&#20041;&#20026;&#36328;&#19981;&#21516;&#23376;&#26063;&#32676;&#65288;&#20363;&#22914;&#65292;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#30340;&#32479;&#35745;&#24179;&#31561;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#65292;&#22312;&#36866;&#24212;&#26032;&#29615;&#22659;&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#32773;&#38656;&#35201;&#20351;&#29992;&#20840;&#23616;&#26356;&#25913;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#26114;&#36149;&#21644;&#20302;&#25928;&#30340;&#12290;&#21463;&#21040;&#31232;&#30095;&#26426;&#21046;&#36716;&#31227;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22768;&#31216;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#21464;&#21270;&#29615;&#22659;&#21487;&#20197;&#24402;&#22240;&#20110;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#37096;&#20998;&#23398;&#20064;&#21442;&#25968;&#30340;&#37096;&#20998;&#21464;&#21270;&#65292;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#19981;&#21464;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#22312;&#20551;&#35774;&#20174;&#19981;&#21516;&#23376;&#20154;&#32676;&#25910;&#38598;&#30340;&#25968;&#25454;&#20855;&#26377;&#20844;&#24179;&#30340;&#27169;&#22411;&#34920;&#31034;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#21442;&#25968;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#27599;&#20010;&#23376;&#20154;&#32676;&#27169;&#22411;&#34920;&#31034;&#20844;&#27491;&#24615;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data coll
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16960</link><description>&lt;p&gt;
&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#31038;&#20250;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Socially Aligned Language Models in Simulated Human Society. (arXiv:2305.16960v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#23545;&#40784;&#26088;&#22312;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#25353;&#29031;&#26082;&#23450;&#30340;&#31038;&#20250;&#20215;&#20540;&#34892;&#20107;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#20154;&#20204;&#36890;&#36807;&#31038;&#20132;&#20114;&#21160;&#24471;&#20986;&#23545;&#20215;&#20540;&#21028;&#26029;&#30340;&#20849;&#35782;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21017;&#22312;&#23396;&#31435;&#22320;&#22797;&#21046;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#26102;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#23548;&#33268;&#22312;&#38476;&#29983;&#22330;&#26223;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20801;&#35768;LMs&#20174;&#27169;&#25311;&#30340;&#31038;&#20132;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;LMs&#35757;&#32451;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#20351;&#25105;&#20204;&#31163;&#24320;&#21457;&#33021;&#22815;&#24378;&#26377;&#21147;&#19988;&#20934;&#30830;&#21453;&#26144;&#31038;&#20250;&#35268;&#33539;&#21644;&#20215;&#20540;&#30340;AI&#31995;&#32479;&#26356;&#36817;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#33258;&#36866;&#24212;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#26426;&#21046;&#65292;&#36890;&#36807;&#33021;&#37327;&#24863;&#30693;&#30340;&#31574;&#30053;&#65292;&#22312;EH&#36793;&#32536;&#35774;&#22791;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#20934;&#30830;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.14094</link><description>&lt;p&gt;
&#36890;&#36807;&#33021;&#37327;&#24863;&#30693;&#30340;&#26089;&#26399;&#36864;&#20986;&#23454;&#29616;&#21487;&#25345;&#32493;&#30340;&#36793;&#32536;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Sustainable Edge Intelligence Through Energy-Aware Early Exiting. (arXiv:2305.14094v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#33258;&#36866;&#24212;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#26426;&#21046;&#65292;&#36890;&#36807;&#33021;&#37327;&#24863;&#30693;&#30340;&#31574;&#30053;&#65292;&#22312;EH&#36793;&#32536;&#35774;&#22791;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#20934;&#30830;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28040;&#32791;&#22823;&#37327;&#33021;&#37327;&#65292;&#36825;&#21487;&#33021;&#20250;&#24555;&#36895;&#32791;&#23613;&#30005;&#27744;&#24182;&#24433;&#21709;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#25345;&#32493;&#36816;&#34892;&#65292;&#26412;&#25991;&#32771;&#34385;&#19968;&#20010;&#24102;&#26377;&#21487;&#20805;&#30005;&#30005;&#27744;&#21644;&#33021;&#37327;&#25910;&#33719;&#33021;&#21147;&#30340;&#36793;&#32536;&#35774;&#22791;&#12290;&#38500;&#20102;&#29615;&#22659;&#33021;&#28304;&#30340;&#38543;&#26426;&#24615;&#22806;&#65292;&#25910;&#33719;&#36895;&#29575;&#36890;&#24120;&#19981;&#36275;&#20197;&#28385;&#36275;&#25512;&#29702;&#33021;&#28304;&#38656;&#27714;&#65292;&#22312;&#33021;&#28304;&#19981;&#21487;&#30693;&#30340;&#35774;&#22791;&#20013;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#37327;&#33258;&#36866;&#24212;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#22312;&#20805;&#28385;&#29615;&#22659;&#33021;&#28304;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#20934;&#30830;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models have emerged as a promising solution for Internet of Things (IoT) applications. However, due to their computational complexity, DL models consume significant amounts of energy, which can rapidly drain the battery and compromise the performance of IoT devices. For sustainable operation, we consider an edge device with a rechargeable battery and energy harvesting (EH) capabilities. In addition to the stochastic nature of the ambient energy source, the harvesting rate is often insufficient to meet the inference energy requirements, leading to drastic performance degradation in energy-agnostic devices. To mitigate this problem, we propose energy-adaptive dynamic early exiting (EE) to enable efficient and accurate inference in an EH edge intelligence system. Our approach derives an energy-aware EE policy that determines the optimal amount of computational processing on a per-sample basis. The proposed policy balances the energy consumption to match the limited inco
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;(SLAP)&#65292;&#20351;&#29992;&#19977;&#32500;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;&#21333;&#19968;&#22810;&#20219;&#21153;&#21644;&#35821;&#35328;&#26465;&#20214;&#21270;&#30340;&#21160;&#20316;&#39044;&#27979;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#24341;&#20837;&#26410;&#35265;&#36807;&#30340;&#24178;&#25200;&#29289;&#21644;&#29289;&#20307;&#37197;&#32622;&#26102;&#36798;&#21040;47.5%&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11235</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Spatial-Language Attention Policies for Efficient Robot Learning. (arXiv:2304.11235v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;(SLAP)&#65292;&#20351;&#29992;&#19977;&#32500;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;&#21333;&#19968;&#22810;&#20219;&#21153;&#21644;&#35821;&#35328;&#26465;&#20214;&#21270;&#30340;&#21160;&#20316;&#39044;&#27979;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#24341;&#20837;&#26410;&#35265;&#36807;&#30340;&#24178;&#25200;&#29289;&#21644;&#29289;&#20307;&#37197;&#32622;&#26102;&#36798;&#21040;47.5%&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;Transformer&#24314;&#31435;&#21644;&#35757;&#32451;&#26426;&#22120;&#20154;&#20915;&#31574;&#21046;&#23450;&#30340;&#31354;&#38388;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#24555;&#36895;&#35757;&#32451;&#25110;&#24494;&#35843;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#21160;&#20316;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#24182;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;&#65288;SLAP&#65289;&#12290;SLAP&#20351;&#29992;&#19977;&#32500;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;&#21333;&#19968;&#22810;&#20219;&#21153;&#12289;&#35821;&#35328;&#26465;&#20214;&#21270;&#30340;&#21160;&#20316;&#39044;&#27979;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;80%&#30340;&#25104;&#21151;&#29575;&#65292;&#36328;&#36234;&#20102;8&#39033;&#20219;&#21153;&#24182;&#20165;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#65292;&#22312;&#24341;&#20837;&#26410;&#35265;&#36807;&#30340;&#24178;&#25200;&#29289;&#21644;&#29289;&#20307;&#37197;&#32622;&#26102;&#20173;&#20445;&#25345;&#20102;47.5%&#30340;&#25104;&#21151;&#29575;&#65292;&#21363;&#20351;&#27599;&#20010;&#20219;&#21153;&#20165;&#20351;&#29992;&#23569;&#25968;&#31034;&#20363;&#12290;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65288;&#20165;&#20351;&#29992;&#26410;&#35265;&#24178;&#25200;&#29289;&#21644;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#29575;&#20026;20%&#65289;&#65292;&#36825;&#34920;&#31034;&#20102;30%&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate how to build and train spatial representations for robot decision making with Transformers. In particular, for robots to operate in a range of environments, we must be able to quickly train or fine-tune robot sensorimotor policies that are robust to clutter, data efficient, and generalize well to different circumstances. As a solution, we propose Spatial Language Attention Policies (SLAP). SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations).
&lt;/p&gt;</description></item><item><title>CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04391</link><description>&lt;p&gt;
CAFIN: &#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#30340;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04391
&lt;/p&gt;
&lt;p&gt;
CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25152;&#23398;&#23884;&#20837;&#30340;&#32039;&#20945;&#24615;&#21644;&#20016;&#23500;&#24615;&#20197;&#21450;&#26410;&#26631;&#35760;&#22270;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#22312;(&#22823;&#22411;)&#22270;&#19978;&#24050;&#32463;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#37325;&#35270;&#12290;&#24403;&#36825;&#20123;&#33410;&#28857;&#34920;&#31034;&#34987;&#37096;&#32626;&#26102;&#65292;&#24517;&#39035;&#20351;&#29992;&#36866;&#24403;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#26465;&#20214;&#29983;&#25104;&#20197;&#20943;&#23569;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#36896;&#25104;&#30340;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24050;&#32463;&#35843;&#26597;&#20102;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#36825;&#20123;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#27809;&#26377;&#32771;&#34385;&#36830;&#25509;&#27169;&#24335;&#22312;&#22270;&#20013;&#23548;&#33268;&#30340;&#19981;&#21516;&#33410;&#28857;&#24433;&#21709;(&#25110;&#20013;&#24515;&#24615;&#33021;&#37327;)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#24402;&#32435;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CAFIN&#65288;Centrality Aware Fairness inducing IN-processing&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#25913;&#36827;GraphSAGE&#34920;&#31034;&#30340;&#36827;&#31243;&#25216;&#26415;&#8212;&#8212;&#26080;&#30417;&#30563;&#22270;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#26694;&#26550;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning on (large) graphs has received significant attention in the research community due to the compactness and richness of the learned embeddings and the abundance of unlabelled graph data. When deployed, these node representations must be generated with appropriate fairness constraints to minimize bias induced by them on downstream tasks. Consequently, group and individual fairness notions for graph learning algorithms have been investigated for specific downstream tasks. One major limitation of these fairness notions is that they do not consider the connectivity patterns in the graph leading to varied node influence (or centrality power). In this paper, we design a centrality-aware fairness framework for inductive graph representation learning algorithms. We propose CAFIN (Centrality Aware Fairness inducing IN-processing), an in-processing technique that leverages graph structure to improve GraphSAGE's representations - a popular framework in the unsup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#23558;&#30072;&#21464;&#25918;&#32622;&#20110;&#20013;&#24515;&#20301;&#32622;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#26426;&#21046;FiT&#65292;&#35813;&#26426;&#21046;&#26088;&#22312;&#36890;&#36807;&#21482;&#36827;&#34892;&#20960;&#20010;&#21021;&#27493;&#26597;&#35810;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#30340;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.02312</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#30340;&#30431;&#21451;&#36827;&#34892;&#21487;&#36716;&#31227;&#25915;&#20987;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to choose your best allies for a transferable attack?. (arXiv:2304.02312v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#23558;&#30072;&#21464;&#25918;&#32622;&#20110;&#20013;&#24515;&#20301;&#32622;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#26426;&#21046;FiT&#65292;&#35813;&#26426;&#21046;&#26088;&#22312;&#36890;&#36807;&#21482;&#36827;&#34892;&#20960;&#20010;&#21021;&#27493;&#26597;&#35810;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#30340;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#19968;&#20010;&#20026;&#28304;&#27169;&#22411;&#32780;&#21046;&#36896;&#30340;&#23545;&#25239;&#26679;&#26412;&#21487;&#20197;&#27450;&#39575;&#21478;&#19968;&#20010;&#30446;&#26631;&#27169;&#22411;&#65292;&#20351;&#23545;&#25239;&#25915;&#20987;&#30340;&#23041;&#32961;&#26356;&#21152;&#30495;&#23454;&#12290;&#34913;&#37327;&#21487;&#36716;&#31227;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20294;&#25915;&#20987;&#25104;&#21151;&#29575;&#26412;&#36523;&#24182;&#19981;&#33021;&#25552;&#20379;&#22362;&#23454;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#30072;&#21464;&#25918;&#32622;&#20110;&#20013;&#24515;&#20301;&#32622;&#12290;&#36825;&#20010;&#26032;&#24037;&#20855;&#26174;&#31034;&#65292;&#22914;&#26524;&#25915;&#20987;&#32773;&#38543;&#26426;&#36873;&#25321;&#28304;&#27169;&#22411;&#65292;&#37027;&#20040;&#21487;&#36716;&#31227;&#25915;&#20987;&#30340;&#34920;&#29616;&#21487;&#33021;&#36828;&#36828;&#19981;&#21450;&#40657;&#30418;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;FiT&#65292;&#35813;&#26426;&#21046;&#26088;&#22312;&#36890;&#36807;&#21482;&#36827;&#34892;&#20960;&#20010;&#21021;&#27493;&#26597;&#35810;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#30340;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FiT&#22312;&#36873;&#25321;&#22810;&#20010;&#25915;&#20987;&#24773;&#22659;&#19979;&#30340;&#26368;&#20339;&#28304;&#27169;&#22411;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20363;&#22914;&#21333;&#19968;&#27169;&#22411;&#25915;&#20987;&#12289;&#38598;&#25104;&#27169;&#22411;&#25915;&#20987;&#21644;&#22810;&#25915;&#20987;&#65288;&#20195;&#30721;&#21487;&#22312;https://github.com/weny1choi/FiT&#20013;&#25214;&#21040;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transferability of adversarial examples is a key issue in the security of deep neural networks. The possibility of an adversarial example crafted for a source model fooling another targeted model makes the threat of adversarial attacks more realistic. Measuring transferability is a crucial problem, but the Attack Success Rate alone does not provide a sound evaluation. This paper proposes a new methodology for evaluating transferability by putting distortion in a central position. This new tool shows that transferable attacks may perform far worse than a black box attack if the attacker randomly picks the source model. To address this issue, we propose a new selection mechanism, called FiT, which aims at choosing the best source model with only a few preliminary queries to the target. Our experimental results show that FiT is highly effective at selecting the best source model for multiple scenarios such as single-model attacks, ensemble-model attacks and multiple attacks (Code avai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;iPSRL&#21644;iRLSVI&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#23450;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#25022;&#65292;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.11369</link><description>&lt;p&gt;
&#12298;&#26725;&#25509;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#20048;&#35266;&#30340;&#25925;&#20107;&#12299;
&lt;/p&gt;
&lt;p&gt;
Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale. (arXiv:2303.11369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;iPSRL&#21644;iRLSVI&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#23450;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#25022;&#65292;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20197;&#19979;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#19981;&#23436;&#32654;&#19987;&#23478;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#65292;&#26368;&#22909;&#30340;&#26041;&#24335;&#26159;&#20160;&#20040;&#26469;&#21033;&#29992;&#23427;&#26469;&#24341;&#23548; MDP &#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#34920;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#24773;&#21518;&#39564;&#37319;&#26679;&#30340; RL&#65288;iPSRL&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#20449;&#24687;&#26469;&#29983;&#25104;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#22914;&#26524;&#19987;&#23478;&#36275;&#22815;&#33021;&#24178;&#65292;&#21017;&#20854;&#32047;&#31215;&#36125;&#21494;&#26031;&#36951;&#25022;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#22823;&#23567; N &#19979;&#20250;&#25351;&#25968;&#24555;&#36895;&#19979;&#38477;&#21040;&#38646;&#12290;&#30001;&#20110;&#35813;&#31639;&#27861;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#65292;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102; iRLSVI &#31639;&#27861;&#65292;&#21487;&#30475;&#20316;&#26159;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#30340; RLSVI &#31639;&#27861;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20004;&#20010;&#22522;&#20934;&#65288;&#27809;&#26377;&#31163;&#32447;&#25968;&#25454;&#65292;&#25110;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20294;&#19981;&#21033;&#29992;&#29983;&#25104;&#31574;&#30053;&#20449;&#24687;&#65289;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340; iRLSVI &#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in N, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy. Our algorithm bridges online RL and imitation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20165;&#38656;&#20960;&#20998;&#38047;&#23601;&#33021;&#23558;&#21407;&#22987;FLOP&#35745;&#25968;&#30340;&#26368;&#39640;40%&#20943;&#23569;&#32780;&#20934;&#30830;&#24230;&#20165;&#19979;&#38477;&#19981;&#36229;&#36807;4%&#12290;</title><link>http://arxiv.org/abs/2303.04185</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#19982;&#26080;&#26631;&#31614;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Gradient-Free Structured Pruning with Unlabeled Data. (arXiv:2303.04185v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20165;&#38656;&#20960;&#20998;&#38047;&#23601;&#33021;&#23558;&#21407;&#22987;FLOP&#35745;&#25968;&#30340;&#26368;&#39640;40%&#20943;&#23569;&#32780;&#20934;&#30830;&#24230;&#20165;&#19979;&#38477;&#19981;&#36229;&#36807;4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#35299;&#20915;&#22256;&#38590;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#25104;&#21151;&#20276;&#38543;&#30528;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#24310;&#36831;&#12290;&#38543;&#30528;&#24320;&#21457;&#20154;&#21592;&#21644;&#31532;&#19977;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#23450;&#21046;&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#25512;&#29702;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22823;&#12290;&#35768;&#22810;&#21162;&#21147;&#23581;&#35797;&#36890;&#36807;&#21098;&#26525;&#21644;&#33976;&#39311;&#31561;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26469;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#35201;&#20040;&#38656;&#35201;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#35201;&#20040;&#22240;&#20026;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#20197;&#24674;&#22797;&#20934;&#30830;&#24615;&#32780;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26694;&#26550;&#12290;&#20351;&#29992;BERT$_{BASE}$&#21644;DistilBERT&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#20165;&#38656;&#20960;&#20998;&#38047;&#65292;&#21363;&#21487;&#23558;&#21407;&#22987;FLOP&#35745;&#25968;&#30340;&#26368;&#39640;40%&#20943;&#23569;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#19981;&#36229;&#36807;4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40% of the original FLOP count can be reduced with less than a 4% accur
&lt;/p&gt;</description></item><item><title>CleanCLIP&#26159;&#19968;&#20010;&#36890;&#36807;&#29420;&#31435;&#37325;&#26032;&#23545;&#40784;&#20010;&#21035;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#21066;&#24369;&#21518;&#38376;&#25915;&#20987;&#24341;&#20837;&#30340;&#34394;&#20551;&#20851;&#32852;&#30340;&#24494;&#35843;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.03323</link><description>&lt;p&gt;
&#28165;&#27905;CLIP: &#32531;&#35299;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. (arXiv:2303.03323v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03323
&lt;/p&gt;
&lt;p&gt;
CleanCLIP&#26159;&#19968;&#20010;&#36890;&#36807;&#29420;&#31435;&#37325;&#26032;&#23545;&#40784;&#20010;&#21035;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#21066;&#24369;&#21518;&#38376;&#25915;&#20987;&#24341;&#20837;&#30340;&#34394;&#20551;&#20851;&#32852;&#30340;&#24494;&#35843;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#27604;&#39044;&#35757;&#32451;&#24050;&#34987;&#29992;&#20110;&#22312;&#22823;&#37327;&#37197;&#23545;&#30340;&#22270;&#25991;&#25968;&#25454;&#19978;&#35757;&#32451;&#22810;&#27169;&#24577;&#34920;&#31034;&#27169;&#22411;&#65292;&#22914;CLIP&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#31867;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#22312;&#21547;&#26377;&#21518;&#38376;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;CLIP&#23398;&#20064;&#21040;&#20102;&#23884;&#20837;&#24335;&#21518;&#38376;&#35302;&#21457;&#22120;&#19982;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#22312;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#23545;&#40784;&#12290;&#21363;&#20351;&#27880;&#20837;&#20102;&#23569;&#37327;&#30340;&#27602;&#21270;&#31034;&#20363;&#65292;&#20363;&#22914;&#22312;3000000&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#20102;75&#20010;&#31034;&#20363;&#65292;&#20063;&#33021;&#26174;&#33879;&#25805;&#32437;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20351;&#20854;&#38590;&#20197;&#26816;&#27979;&#25110;&#24536;&#35760;&#36825;&#31181;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CleanCLIP&#65292;&#19968;&#31181;&#36890;&#36807;&#29420;&#31435;&#37325;&#26032;&#23545;&#40784;&#20010;&#21035;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#21066;&#24369;&#21518;&#38376;&#25915;&#20987;&#24341;&#20837;&#30340;&#23398;&#20064;&#21040;&#30340;&#34394;&#20551;&#20851;&#32852;&#30340;&#24494;&#35843;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#21644;&#21333;&#27169;&#24577;&#33258;&#30417;&#30563;&#30340;&#32452;&#21512;&#36827;&#34892;&#26080;&#30417;&#30563;&#24494;&#35843;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model's behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36328;&#26102;&#26399;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;SpanKL&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#22810;&#26631;&#31614;&#39044;&#27979;&#26469;&#23454;&#29616;&#35760;&#24518;&#20445;&#30041;&#21644;&#20914;&#31361;&#38450;&#27490;&#65292;&#35813;&#27169;&#22411;&#22312;&#36328;&#26102;&#26399;NER&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#31034;&#20986;&#39640;&#23454;&#38469;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.12200</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36328;&#26102;&#26399;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Neural Span-Based Continual Named Entity Recognition Model. (arXiv:2302.12200v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36328;&#26102;&#26399;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;SpanKL&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#22810;&#26631;&#31614;&#39044;&#27979;&#26469;&#23454;&#29616;&#35760;&#24518;&#20445;&#30041;&#21644;&#20914;&#31361;&#38450;&#27490;&#65292;&#35813;&#27169;&#22411;&#22312;&#36328;&#26102;&#26399;NER&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#31034;&#20986;&#39640;&#23454;&#38469;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#20307;&#31867;&#22411;&#19981;&#26029;&#22686;&#21152;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#20010;&#20154;&#21161;&#25163;&#65289;&#20013;&#65292;&#20855;&#22791;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65288;NER&#65289;&#20855;&#26377;&#29616;&#23454;&#20215;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;NER&#30340;&#23398;&#20064;&#33539;&#24335;&#36880;&#28176;&#21457;&#23637;&#20986;&#20102;&#26032;&#30340;&#27169;&#24335;&#65292;&#22914;&#22522;&#20110;&#36328;&#24230;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36328;&#26102;&#26399;&#23398;&#20064;&#22312;&#36825;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SpanKL&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26469;&#20445;&#30041;&#35760;&#24518;&#65292;&#24182;&#37319;&#29992;&#22810;&#26631;&#31614;&#39044;&#27979;&#26469;&#38450;&#27490;&#36328;&#26102;&#26399;NER&#20013;&#30340;&#20914;&#31361;&#12290;&#19982;&#20043;&#21069;&#30340;&#24207;&#21015;&#26631;&#35760;&#26041;&#27861;&#19981;&#21516;&#65292;&#22312;SpanKL&#20013;&#65292;&#36328;&#24230;&#21644;&#23454;&#20307;&#32423;&#21035;&#30340;&#29420;&#31435;&#24314;&#27169;&#20197;&#21450;&#35774;&#35745;&#30340;&#19968;&#33268;&#20248;&#21270;&#20419;&#36827;&#20102;&#27599;&#20010;&#22686;&#37327;&#27493;&#39588;&#30340;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#20102;&#36951;&#24536;&#12290;&#22312;&#20174;OntoNotes&#21644;Few-NERD&#34893;&#29983;&#30340;&#21512;&#25104;CL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SpanKL&#22312;&#35768;&#22810;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#20174;CL&#21040;&#19978;&#38480;&#30340;&#24046;&#36317;&#26368;&#23567;&#65292;&#26174;&#31034;&#20102;&#20854;&#39640;&#23454;&#38469;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) models capable of Continual Learning (CL) are realistically valuable in areas where entity types continuously increase (e.g., personal assistants). Meanwhile the learning paradigm of NER advances to new patterns such as the span-based methods. However, its potential to CL has not been fully explored. In this paper, we propose SpanKL, a simple yet effective Span-based model with Knowledge distillation (KD) to preserve memories and multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence labeling approaches, the inherently independent modeling in span and entity level with the designed coherent optimization on SpanKL promotes its learning at each incremental step and mitigates the forgetting. Experiments on synthetic CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly outperforms previous SoTA in many aspects, and obtains the smallest gap from CL to the upper bound revealing its high practiced value. The code i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#27934;&#23519;&#21147;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21160;&#24577;&#21644;&#34892;&#20026;&#29305;&#24449;&#19978;&#23494;&#20999;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#27934;&#23519;&#21147;&#65292;&#34920;&#29616;&#20986;&#27934;&#23519;&#21147;&#30340;&#24310;&#36831;&#12289;&#31361;&#28982;&#24615;&#21644;&#36873;&#25321;&#24615;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2302.11351</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#20154;&#31867;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Regularised neural networks mimic human insight. (arXiv:2302.11351v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#27934;&#23519;&#21147;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21160;&#24577;&#21644;&#34892;&#20026;&#29305;&#24449;&#19978;&#23494;&#20999;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#27934;&#23519;&#21147;&#65292;&#34920;&#29616;&#20986;&#27934;&#23519;&#21147;&#30340;&#24310;&#36831;&#12289;&#31361;&#28982;&#24615;&#21644;&#36873;&#25321;&#24615;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#26102;&#20250;&#23637;&#29616;&#20986;&#31361;&#28982;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#30340;&#24773;&#20917;&#65292;&#36825;&#19982;&#27934;&#23519;&#21147;&#26102;&#21051;&#30456;&#20851;&#12290;&#36825;&#31181;&#27934;&#23519;&#21147;&#30456;&#20851;&#30340;&#24615;&#33021;&#25552;&#21319;&#20284;&#20046;&#24456;&#29305;&#27530;&#65292;&#22240;&#20026;&#23427;&#20204;&#21069;&#38754;&#26377;&#19968;&#20010;&#36739;&#38271;&#26102;&#38388;&#30340;&#20725;&#23616;&#65292;&#21464;&#21270;&#24322;&#24120;&#31361;&#28982;&#65292;&#24182;&#19988;&#21482;&#21457;&#29983;&#22312;&#19968;&#37096;&#20998;&#23398;&#20064;&#32773;&#36523;&#19978;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#26159;&#21542;&#20063;&#23384;&#22312;&#31867;&#20284;&#27934;&#23519;&#21147;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#20915;&#31574;&#20219;&#21153;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#38544;&#34255;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#20542;&#21521;&#20110;&#36890;&#36807;&#27934;&#23519;&#21147;&#21457;&#29616;&#36825;&#31181;&#35268;&#24459;&#65292;&#32780;&#19981;&#26159;&#36880;&#28176;&#21457;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24102;&#26377;&#27491;&#21017;&#21270;&#38376;&#25511;&#35843;&#33410;&#30340;&#31070;&#32463;&#32593;&#32476;&#32039;&#23494;&#27169;&#20223;&#20102;&#20154;&#31867;&#27934;&#23519;&#21147;&#30340;&#34892;&#20026;&#29305;&#24449;&#65292;&#34920;&#29616;&#20986;&#27934;&#23519;&#21147;&#30340;&#24310;&#36831;&#12289;&#31361;&#28982;&#24615;&#21644;&#36873;&#25321;&#24615;&#21457;&#29983;&#12290;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27934;&#23519;&#21147;&#34892;&#20026;&#20851;&#38190;&#22320;&#21462;&#20915;&#20110;&#22122;&#22768;&#28155;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans sometimes show sudden improvements in task performance that have been linked to moments of insight. Such insight-related performance improvements appear special because they are preceded by an extended period of impasse, are unusually abrupt, and occur only in some, but not all, learners. Here, we ask whether insight-like behaviour also occurs in artificial neural networks trained with gradient descent algorithms. We compared learning dynamics in humans and regularised neural networks in a perceptual decision task that provided a hidden opportunity which allowed to solve the task more efficiently. We show that humans tend to discover this regularity through insight, rather than gradually. Notably, neural networks with regularised gate modulation closely mimicked behavioural characteristics of human insights, exhibiting delay of insight, suddenness and selective occurrence. Analyses of network learning dynamics revealed that insight-like behaviour crucially depended on noise adde
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20844;&#24179;&#25193;&#25955;&#8221;&#30340;&#26032;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#24182;&#20351;&#27169;&#22411;&#25509;&#21463;&#20844;&#24179;&#24615;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2302.10893</link><description>&lt;p&gt;
&#20844;&#24179;&#25193;&#25955;&#65306;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20844;&#24179;&#25193;&#25955;&#8221;&#30340;&#26032;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#24182;&#20351;&#27169;&#22411;&#25509;&#21463;&#20844;&#24179;&#24615;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#12290;&#20294;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#20114;&#32852;&#32593;&#19978;&#38543;&#26426;&#25277;&#21462;&#30340;&#21313;&#20159;&#32423;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#20063;&#20250;&#21463;&#21040;&#36864;&#21270;&#21644;&#20559;&#35265;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#20107;&#23454;&#19978;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#33021;&#21152;&#21095;&#36825;&#20123;&#20559;&#35265;&#12290;&#20026;&#20102;&#19981;&#20165;&#25581;&#31034;&#32780;&#19988;&#23545;&#25239;&#36825;&#20123;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#20844;&#24179;&#25193;&#25955;&#65292;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#20154;&#31867;&#25351;&#23548;&#30340;&#20559;&#24046;&#36716;&#31227;&#65292;&#21487;&#22312;&#20219;&#20309;&#26041;&#21521;&#19978;&#20135;&#29983;&#20219;&#24847;&#26032;&#30340;&#27604;&#20363;&#65292;&#20363;&#22914;&#65292;&#36523;&#20221;&#32452;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#25152;&#31034;&#65292;&#36825;&#31181;&#25511;&#21046;&#20351;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#33021;&#22815;&#25509;&#21463;&#25351;&#23548;&#65292;&#26080;&#38656;&#25968;&#25454;&#36807;&#28388;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.
&lt;/p&gt;</description></item><item><title>&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;</title><link>http://arxiv.org/abs/2302.08942</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08942
&lt;/p&gt;
&lt;p&gt;
&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#24320;&#21457;&#20102;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#31532;&#19968;&#20010;&#20851;&#20110;Wasserstein&#36317;&#31163;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#20363;&#31354;&#38388;&#26159;&#26377;&#30028;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#32467;&#26524;&#21033;&#29992;&#20102;&#38477;&#32500;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#33258;&#28982;&#36866;&#29992;&#20110;Wasserstein GAN&#21644;Energy-Based GAN&#65292;&#32780;&#25105;&#20204;&#30340;&#30028;&#38480;&#20026;&#36825;&#20004;&#31181;GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#26159;&#29702;&#35770;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;Wasserstein GAN&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
&lt;/p&gt;</description></item><item><title>HumanMAC&#26159;&#19968;&#20010;&#25513;&#30721;&#21160;&#20316;&#20462;&#22797;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#38454;&#27573;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#21644;&#25512;&#26029;&#38454;&#27573;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#25968;&#25454;&#30340;&#25511;&#21046;&#19979;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.03665</link><description>&lt;p&gt;
HumanMAC&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#25513;&#30721;&#21160;&#20316;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
HumanMAC: Masked Motion Completion for Human Motion Prediction. (arXiv:2302.03665v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03665
&lt;/p&gt;
&lt;p&gt;
HumanMAC&#26159;&#19968;&#20010;&#25513;&#30721;&#21160;&#20316;&#20462;&#22797;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#38454;&#27573;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#21644;&#25512;&#26029;&#38454;&#27573;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#25968;&#25454;&#30340;&#25511;&#21046;&#19979;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20197;&#32534;&#30721;-&#35299;&#30721;&#39118;&#26684;&#20026;&#22522;&#30784;&#30340;&#20808;&#21069;&#26041;&#27861;&#22312;&#32463;&#39564;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#25439;&#22833;&#32422;&#26463;&#12289;&#32321;&#29712;&#30340;&#22521;&#35757;&#36807;&#31243;&#20197;&#21450;&#39044;&#27979;&#20013;&#19981;&#21516;&#31867;&#21035;&#36816;&#21160;&#30340;&#31232;&#32570;&#20999;&#25442;&#12290;&#26412;&#25991;&#20174;&#26032;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#25513;&#34109;&#23436;&#25104;&#26041;&#24335;&#35299;&#20915;&#20102;&#20197;&#19978;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#26469;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#29983;&#25104;&#36816;&#21160;&#12290;&#22312;&#25512;&#26029;&#38454;&#27573;&#65292;&#36890;&#36807;&#21435;&#22122;&#36807;&#31243;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36816;&#21160;&#39044;&#27979;&#24182;&#22312;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#25968;&#25454;&#30340;&#25511;&#21046;&#19979;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HumanMAC&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction is a classical problem in computer vision and computer graphics, which has a wide range of practical applications. Previous effects achieve great empirical performance based on an encoding-decoding style. The methods of this style work by first encoding previous motions to latent representations and then decoding the latent representations into predicted motions. However, in practice, they are still unsatisfactory due to several issues, including complicated loss constraints, cumbersome training processes, and scarce switch of different categories of motions in prediction. In this paper, to address the above issues, we jump out of the foregoing style and propose a novel framework from a new perspective. Specifically, our framework works in a masked completion fashion. In the training stage, we learn a motion diffusion model that generates motions from random noise. In the inference stage, with a denoising procedure, we make motion prediction conditioning on obse
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23618;&#27425;&#27880;&#24847;&#21147;&#24341;&#23548;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#21160;&#24577;&#21644;&#20851;&#27880;&#22320;&#21457;&#29616;&#24182;&#21033;&#29992;WSI&#30340;&#22810;&#20010;&#20998;&#36776;&#29575;&#30340;&#21306;&#20998;&#24615;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.08125</link><description>&lt;p&gt;
&#36335;&#24452;&#23398;&#23478;&#24335;&#35786;&#26029;: &#22522;&#20110;Transformer&#30340;&#23618;&#27425;&#27880;&#24847;&#21147;&#24341;&#23548;&#22810;&#23454;&#20363;&#23398;&#20064;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2301.08125v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23618;&#27425;&#27880;&#24847;&#21147;&#24341;&#23548;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#21160;&#24577;&#21644;&#20851;&#27880;&#22320;&#21457;&#29616;&#24182;&#21033;&#29992;WSI&#30340;&#22810;&#20010;&#20998;&#36776;&#29575;&#30340;&#21306;&#20998;&#24615;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#21644;transformers&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#31867;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#22312;&#19981;&#21516;&#25918;&#22823;&#20493;&#29575;&#19979;&#36873;&#25321;&#24615;&#35266;&#23519;&#32452;&#32455;&#30149;&#29702;&#23398;&#32452;&#32455;&#30340;&#29305;&#23450;&#21306;&#22495;&#19981;&#21516;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#19981;&#20250;&#23618;&#27425;&#21270;&#21644;&#27880;&#37325;&#22320;&#32467;&#21512;WSI&#30340;&#22810;&#20998;&#36776;&#29575;&#65292;&#23548;&#33268;&#23545;WSI&#21644;&#20854;&#20182;&#20998;&#36776;&#29575;&#20449;&#24687;&#30340;&#20851;&#27880;&#20007;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;WSI&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#21160;&#24577;&#22320;&#21644;&#20851;&#27880;&#22320;&#21457;&#29616;&#36328;WSI&#30340;&#22810;&#20010;&#20998;&#36776;&#29575;&#30340;&#21306;&#20998;&#24615;&#21306;&#22495;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27880;&#24847;&#21147;&#36716;&#25442;&#22120;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;transformer&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26356;&#20840;&#38754;&#30340;WSI&#65288;bag&#65289;&#34920;&#31034;&#12290;&#35813;transformer&#30001;&#22810;&#20010;&#38598;&#25104;&#27880;&#24847;&#21147;&#27169;&#22359;&#32452;&#25104;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;transformer&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning (MIL) and transformers are increasingly popular in histopathology Whole Slide Image (WSI) classification. However, unlike human pathologists who selectively observe specific regions of histopathology tissues under different magnifications, most methods do not incorporate multiple resolutions of the WSIs, hierarchically and attentively, thereby leading to a loss of focus on the WSIs and information from other resolutions. To resolve this issue, we propose a Hierarchical Attention-Guided Multiple Instance Learning framework to fully exploit the WSIs. This framework can dynamically and attentively discover the discriminative regions across multiple resolutions of the WSIs. Within this framework, an Integrated Attention Transformer is proposed to further enhance the performance of the transformer and obtain a more holistic WSI (bag) representation. This transformer consists of multiple Integrated Attention Modules, which is the combination of a transformer layer 
&lt;/p&gt;</description></item><item><title>StitchNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#32593;&#32476;&#65292;&#26080;&#38656;&#20256;&#32479;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#36890;&#36807;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#29255;&#27573;&#30340;&#36873;&#25321;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;StitchNet&#36824;&#21487;&#20197;&#23454;&#29616;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2301.01947</link><description>&lt;p&gt;
StitchNet: &#20174;&#39044;&#35757;&#32451;&#29255;&#27573;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
StitchNet: Composing Neural Networks from Pre-Trained Fragments. (arXiv:2301.01947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01947
&lt;/p&gt;
&lt;p&gt;
StitchNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#32593;&#32476;&#65292;&#26080;&#38656;&#20256;&#32479;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#36890;&#36807;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#29255;&#27573;&#30340;&#36873;&#25321;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;StitchNet&#36824;&#21487;&#20197;&#23454;&#29616;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#33539;&#24335;StitchNet&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#65288;&#19968;&#20010;&#25110;&#22810;&#20010;&#36830;&#32493;&#30340;&#32593;&#32476;&#23618;&#65289;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;StitchNet&#20801;&#35768;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#20256;&#32479;&#30340;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#25105;&#20204;&#21033;&#29992;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20316;&#20026;&#19968;&#31181;&#20860;&#23481;&#24615;&#24230;&#37327;&#65292;&#20197;&#26377;&#25928;&#22320;&#25351;&#23548;&#36873;&#25321;&#36825;&#20123;&#29255;&#27573;&#65292;&#20197;&#32452;&#21512;&#36866;&#21512;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#30340;&#20219;&#21153;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#29255;&#27573;&#21487;&#20197;&#34987;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#21019;&#24314;&#19982;&#20256;&#32479;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#20934;&#30830;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#26032;&#33539;&#24335;&#25152;&#33021;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose StitchNet, a novel neural network creation paradigm that stitches together fragments (one or more consecutive network layers) from multiple pre-trained neural networks. StitchNet allows the creation of high-performing neural networks without the large compute and data requirements needed under traditional model creation processes via backpropagation training. We leverage Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide the selection of these fragments in composing a network for a given task tailored to specific accuracy needs and computing resource constraints. We then show that these fragments can be stitched together to create neural networks with comparable accuracy to traditionally trained networks at a fraction of computing resource and data requirements. Finally, we explore a novel on-the-fly personalized model creation and inference application enabled by this new paradigm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#33033;&#20914;&#32534;&#30721;&#32593;&#32476;&#29702;&#35770;&#65292;&#36890;&#36807;&#24341;&#20837;&#38381;&#24335;&#26368;&#20248;&#20272;&#35745;&#21644;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.12887</link><description>&lt;p&gt;
&#24102;&#26377;&#33033;&#20914;&#32534;&#30721;&#32593;&#32476;&#30340;&#38381;&#24335;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Closed-form control with spike coding networks. (arXiv:2212.12887v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#33033;&#20914;&#32534;&#30721;&#32593;&#32476;&#29702;&#35770;&#65292;&#36890;&#36807;&#24341;&#20837;&#38381;&#24335;&#26368;&#20248;&#20272;&#35745;&#21644;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#36827;&#34892;&#39640;&#25928;&#21644;&#40065;&#26834;&#30340;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#29983;&#29289;&#31995;&#32479;&#30340;&#34892;&#20026;&#26159;&#36890;&#36807;&#31232;&#30095;&#21644;&#19981;&#35268;&#21017;&#30340;&#33033;&#20914;&#27169;&#24335;&#20135;&#29983;&#30340;&#65292;&#36825;&#25552;&#20379;&#20102;&#40065;&#26834;&#21644;&#39640;&#25928;&#30340;&#25511;&#21046;&#65292;&#20294;&#29992;&#20110;&#25511;&#21046;&#30340;&#22823;&#37096;&#20998;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#27963;&#21160;&#27169;&#24335;&#26159;&#23494;&#38598;&#21644;&#35268;&#21017;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#20195;&#30721;&#25928;&#29575;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#23545;&#20110;&#23436;&#20840;&#35782;&#21035;&#30340;&#31995;&#32479;&#65292;&#20063;&#38656;&#35201;&#32593;&#32476;&#35757;&#32451;&#25110;&#20248;&#21270;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#33455;&#29255;&#20302;&#21151;&#32791;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#23454;&#26045;&#21464;&#24471;&#22797;&#26434;&#12290;&#33033;&#20914;&#32534;&#30721;&#32593;&#32476;(SCNs)&#30340;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#25552;&#20379;&#20102;&#22312;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#21160;&#24577;&#31995;&#32479;&#30340;&#23436;&#20840;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#35268;&#21017;&#12289;&#31232;&#30095;&#21644;&#40065;&#26834;&#30340;&#33033;&#20914;&#27963;&#21160;&#65292;&#20294;&#22914;&#20309;&#30452;&#25509;&#24212;&#29992;&#20110;&#25511;&#21046;&#38382;&#39064;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38381;&#24335;&#26368;&#20248;&#20272;&#35745;&#21644;&#25511;&#21046;&#26469;&#25193;&#23637;SCN&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient and robust control using spiking neural networks (SNNs) is still an open problem. Whilst behaviour of biological agents is produced through sparse and irregular spiking patterns, which provide both robust and efficient control, the activity patterns in most artificial spiking neural networks used for control are dense and regular -- resulting in potentially less efficient codes. Additionally, for most existing control solutions network training or optimization is necessary, even for fully identified systems, complicating their implementation in on-chip low-power solutions. The neuroscience theory of Spike Coding Networks (SCNs) offers a fully analytical solution for implementing dynamical systems in recurrent spiking neural networks -- while maintaining irregular, sparse, and robust spiking activity -- but it's not clear how to directly apply it to control problems. Here, we extend SCN theory by incorporating closed-form optimal estimation and control. The resulting networks 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22909;&#22855;&#24515;&#20316;&#20026;&#20869;&#22312;&#21160;&#26426;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36827;&#21270;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#20013;&#12290;&#19982;&#24120;&#29992;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#25351;&#26631;&#30456;&#27604;&#65292;&#22909;&#22855;&#24515;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#25214;&#21040;&#22810;&#31181;&#33021;&#22815;&#33719;&#24471;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.03530</link><description>&lt;p&gt;
&#22909;&#22855;&#24515;&#22312;&#31574;&#30053;&#25628;&#32034;&#20013;&#20419;&#36827;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Curiosity creates Diversity in Policy Search. (arXiv:2212.03530v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22909;&#22855;&#24515;&#20316;&#20026;&#20869;&#22312;&#21160;&#26426;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36827;&#21270;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#20013;&#12290;&#19982;&#24120;&#29992;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#25351;&#26631;&#30456;&#27604;&#65292;&#22909;&#22855;&#24515;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#25214;&#21040;&#22810;&#31181;&#33021;&#22815;&#33719;&#24471;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#31574;&#30053;&#26102;&#65292;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#36890;&#24120;&#32570;&#20047;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#30830;&#23450;&#24212;&#35813;&#25913;&#36827;&#25110;&#36991;&#20813;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#31574;&#30053;&#25628;&#32034;&#36807;&#31243;&#24448;&#24448;&#20250;&#30450;&#30446;&#22320;&#25628;&#32034;&#20135;&#29983;&#22870;&#21169;&#30340;&#36716;&#25442;&#65292;&#27809;&#26377;&#26089;&#26399;&#22870;&#21169;&#21487;&#20197;&#20559;&#21521;&#20110;&#26576;&#20010;&#26041;&#21521;&#12290;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#26469;&#25506;&#32034;&#26032;&#30340;&#36716;&#25442;&#65292;&#30452;&#21040;&#25214;&#21040;&#22870;&#21169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#20869;&#22312;&#21160;&#26426;&#23450;&#20041;&#8212;&#8212;&#22909;&#22855;&#24515;&#65292;&#32467;&#21512;&#36827;&#21270;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Curiosity-ES&#65292;&#19968;&#31181;&#36866;&#24212;&#20110;&#20351;&#29992;&#22909;&#22855;&#24515;&#20316;&#20026;&#36866;&#24212;&#24230;&#24230;&#37327;&#30340;&#36827;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#22909;&#22855;&#24515;&#19982;&#24120;&#29992;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#25351;&#26631;&#8212;&#8212;&#26032;&#39062;&#24615;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#22909;&#22855;&#24515;&#33021;&#22815;&#22312;&#23436;&#25972;&#30340;&#22238;&#21512;&#20013;&#29983;&#25104;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#26631;&#20934;&#65292;&#24182;&#19988;&#33021;&#22815;&#25214;&#21040;&#22810;&#31181;&#33021;&#22815;&#33719;&#24471;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
When searching for policies, reward-sparse environments often lack sufficient information about which behaviors to improve upon or avoid. In such environments, the policy search process is bound to blindly search for reward-yielding transitions and no early reward can bias this search in one direction or another. A way to overcome this is to use intrinsic motivation in order to explore new transitions until a reward is found. In this work, we use a recently proposed definition of intrinsic motivation, Curiosity, in an evolutionary policy search method. We propose Curiosity-ES, an evolutionary strategy adapted to use Curiosity as a fitness metric. We compare Curiosity with Novelty, a commonly used diversity metric, and find that Curiosity can generate higher diversity over full episodes without the need for an explicit diversity criterion and lead to multiple policies which find reward.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20869;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#65292;&#36890;&#36807;&#32467;&#21512;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01793</link><description>&lt;p&gt;
\{kappa}HGCN: &#36890;&#36807;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#23398;&#20064;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
\{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning. (arXiv:2212.01793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20869;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#65292;&#36890;&#36807;&#32467;&#21512;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#29366;&#32467;&#26500;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#21644;&#24130;&#24459;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#30456;&#27604;&#20110;&#24179;&#22374;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#26354;&#38754;&#21452;&#26354;&#31354;&#38388;&#25552;&#20379;&#20102;&#26356;&#26131;&#22788;&#29702;&#21644;&#23884;&#20837;&#30340;&#31354;&#38388;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23637;&#29616;&#38544;&#21547;&#26641;&#29366;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#26641;&#29366;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#32463;&#24120;&#23637;&#31034;&#20986;&#26641;&#29366;&#12289;&#24179;&#22374;&#21644;&#22278;&#24418;&#21306;&#22495;&#30340;&#24322;&#36136;&#32452;&#25104;&#12290;&#23558;&#36825;&#26679;&#24322;&#36136;&#30340;&#32467;&#26500;&#30452;&#25509;&#23884;&#20837;&#19968;&#20010;&#21516;&#36136;&#21270;&#30340;&#23884;&#20837;&#31354;&#38388;&#65288;&#21363;&#21452;&#26354;&#31354;&#38388;&#65289;&#24517;&#28982;&#23548;&#33268;&#37325;&#22823;&#22833;&#30495;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#32570;&#28857;&#65292;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#21452;&#26354;&#31354;&#38388;&#30340;&#26354;&#29575;&#65292;&#20197;&#23454;&#29616;&#28789;&#27963;&#20934;&#30830;&#22320;&#24314;&#27169;&#26641;&#29366;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in real-world applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvatur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CLIP&#65292;&#36890;&#36807;&#32467;&#21512;&#35838;&#31243;&#23398;&#20064;&#21644;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01452</link><description>&lt;p&gt;
CLIP: &#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#26356;&#24555;&#22320;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLIP: Train Faster with Less Data. (arXiv:2212.01452v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CLIP&#65292;&#36890;&#36807;&#32467;&#21512;&#35838;&#31243;&#23398;&#20064;&#21644;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#27491;&#20174;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#36716;&#21521;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#22312;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#20013;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25913;&#36827;&#21644;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#37325;&#26032;&#35774;&#35745;&#27169;&#22411;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#65292;&#21363;&#20351;&#29992;&#36845;&#20195;&#25968;&#25454;&#20462;&#21098;&#30340;&#35838;&#31243;&#23398;&#20064;&#12290;CLIP&#32467;&#21512;&#20102;&#35838;&#31243;&#23398;&#20064;&#21644;&#25968;&#25454;&#38598;&#20462;&#21098;&#36825;&#20004;&#31181;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#37319;&#29992;&#20102;&#26377;&#25439;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36845;&#20195;&#22320;&#21435;&#38500;&#26368;&#19981;&#37325;&#35201;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#28176;&#20943;&#23567;&#22312;&#35838;&#31243;&#23398;&#20064;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#22312;&#20247;&#31609;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#29702;&#24565;&#65292;&#36890;&#36807;&#20943;&#23567;&#25910;&#25947;&#26102;&#38388;&#21644;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23558;&#35838;&#31243;&#23398;&#20064;&#21644;&#25968;&#25454;&#38598;&#20462;&#21098;&#32467;&#21512;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models require an enormous amount of data for training. However, recently there is a shift in machine learning from model-centric to data-centric approaches. In data-centric approaches, the focus is to refine and improve the quality of the data to improve the learning performance of the models rather than redesigning model architectures. In this paper, we propose CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two data-centric approaches i.e., curriculum learning and dataset pruning to improve the model learning accuracy and convergence speed. The proposed scheme applies loss-aware dataset pruning to iteratively remove the least significant samples and progressively reduces the size of the effective dataset in the curriculum learning training. Extensive experiments performed on crowd density estimation models validate the notion behind combining the two approaches by reducing the convergence time and improving generalization. To our knowledge, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#23436;&#25972;&#26631;&#31614;&#23545;&#20154;&#32676;&#35745;&#25968;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19981;&#23436;&#25972;&#26631;&#31614;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35757;&#32451;&#26032;&#30340;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#23436;&#32654;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01450</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#23436;&#25972;&#26631;&#31614;&#36827;&#34892;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Crowd Density Estimation using Imperfect Labels. (arXiv:2212.01450v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#23436;&#25972;&#26631;&#31614;&#23545;&#20154;&#32676;&#35745;&#25968;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19981;&#23436;&#25972;&#26631;&#31614;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35757;&#32451;&#26032;&#30340;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#23436;&#32654;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#20272;&#35745;&#26159;&#20154;&#32676;&#35745;&#25968;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#22836;&#37096;&#26631;&#27880;&#30340;&#20154;&#32676;&#22270;&#20687;&#26469;&#20272;&#35745;&#26410;&#35265;&#22270;&#20687;&#20013;&#30340;&#20154;&#32676;&#23494;&#24230;&#12290;&#36890;&#24120;&#65292;&#27169;&#22411;&#30340;&#23398;&#20064;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#26631;&#27880;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#19981;&#20934;&#30830;&#30340;&#26631;&#27880;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#36807;&#31243;&#20013;&#30340;&#23450;&#20301;&#21644;&#35745;&#25968;&#38169;&#35823;&#12290;&#24050;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#20351;&#29992;&#23436;&#32654;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#20154;&#32676;&#35745;&#25968;&#65292;&#20294;&#27809;&#26377;&#20154;&#25506;&#32034;&#27880;&#37322;&#38169;&#35823;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#23436;&#25972;&#26631;&#31614;&#65288;&#21253;&#25324;&#22122;&#22768;&#21644;&#32570;&#22833;&#26631;&#31614;&#65289;&#23545;&#20154;&#32676;&#35745;&#25968;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#31216;&#20026;&#26631;&#27880;&#22120;&#65289;&#33258;&#21160;&#29983;&#25104;&#19981;&#23436;&#25972;&#26631;&#31614;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#26032;&#30340;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#65288;&#30446;&#26631;&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#23436;&#32654;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density estimation is one of the most widely used methods for crowd counting in which a deep learning model learns from head-annotated crowd images to estimate crowd density in unseen images. Typically, the learning performance of the model is highly impacted by the accuracy of the annotations and inaccurate annotations may lead to localization and counting errors during prediction. A significant amount of works exist on crowd counting using perfectly labelled datasets but none of these explore the impact of annotation errors on the model accuracy. In this paper, we investigate the impact of imperfect labels (both noisy and missing labels) on crowd counting accuracy. We propose a system that automatically generates imperfect labels using a deep learning model (called annotator) which are then used to train a new crowd counting model (target model). Our analysis on two crowd counting models and two benchmark datasets shows that the proposed scheme achieves accuracy closer to that of the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#21644;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35774;&#22791;&#24178;&#25200;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.16162</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#21644;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35774;&#22791;&#24178;&#25200;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21253;&#21547;&#26680;&#24515;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#21450;&#35774;&#22791;&#38598;&#32676;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#20551;&#35774;&#19981;&#21516;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20855;&#26377;&#30456;&#21516;&#20219;&#21153;&#30340;&#38598;&#32676;&#36827;&#34892;&#21327;&#20316;&#12290;&#20026;&#20102;&#22312;&#26080;&#32447;&#38142;&#36335;&#19978;&#23454;&#29616;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#31751;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#19978;&#34892;&#38142;&#36335;&#65292;&#21516;&#26102;&#37319;&#29992;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#29992;&#20110;&#19979;&#34892;&#38142;&#36335;&#65292;&#27599;&#20010;&#31639;&#27861;&#36845;&#20195;&#21482;&#38656;&#35201;&#19968;&#20010;&#36164;&#28304;&#22359;&#65292;&#19981;&#21463;&#36793;&#32536;&#26381;&#21153;&#22120;&#21644;&#35774;&#22791;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#35774;&#32622;&#38754;&#20020;&#30528;&#19978;&#34892;&#38142;&#36335;&#35774;&#22791;&#24178;&#25200;&#21644;&#19979;&#34892;&#38142;&#36335;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#20005;&#26684;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#35774;&#22791;&#24314;&#27169;&#20026;&#19968;&#20010;&#27850;&#26494;&#38598;&#32676;&#36807;&#31243;&#65292;&#22312;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#23545;&#30001;&#24178;&#25200;&#24341;&#36215;&#30340;&#19978;&#34892;&#38142;&#36335;&#21644;&#19979;&#34892;&#38142;&#36335;&#30340;&#35823;&#24046;&#36827;&#34892;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#25968;&#23398;&#26041;&#27861;&#26469;&#25512;&#23548;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a communication-efficient hierarchical federated learning algorithm for distributed setups including core servers and multiple edge servers with clusters of devices. Assuming different learning tasks, clusters with a same task collaborate. To implement the algorithm over wireless links, we propose a scalable clustered over-the-air aggregation scheme for the uplink with a bandwidth-limited broadcast scheme for the downlink that requires only a single resource block for each algorithm iteration, independent of the number of edge servers and devices. This setup is faced with interference of devices in the uplink and interference of edge servers in the downlink that are to be modeled rigorously. We first develop a spatial model for the setup by modeling devices as a Poisson cluster process over the edge servers and quantify uplink and downlink error terms due to the interference. Accordingly, we present a comprehensive mathematical approach to derive the convergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36974;&#32617;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#36974;&#32617;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#36974;&#32617;&#24341;&#36215;&#30340;&#32570;&#22833;&#20559;&#24046;&#65292;&#24182;&#28040;&#38500;&#25110;&#26368;&#23567;&#21270;&#20102;&#36974;&#32617;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.14646</link><description>&lt;p&gt;
&#25913;&#36827;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36974;&#32617;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Improved Input Masking for Convolutional Neural Networks. (arXiv:2211.14646v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36974;&#32617;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#36974;&#32617;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#36974;&#32617;&#24341;&#36215;&#30340;&#32570;&#22833;&#20559;&#24046;&#65292;&#24182;&#28040;&#38500;&#25110;&#26368;&#23567;&#21270;&#20102;&#36974;&#32617;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#26469;&#35828;&#65292;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#20013;&#31227;&#38500;&#29305;&#24449;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35270;&#35273;&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#36974;&#32617;&#22270;&#20687;&#30340;&#19968;&#37096;&#20998;&#36890;&#24120;&#20250;&#24341;&#36215;&#24456;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#36825;&#26159;&#22240;&#20026;&#29992;&#20110;&#36974;&#32617;&#30340;&#22522;&#20934;&#39068;&#33394;&#65288;&#36890;&#24120;&#26159;&#28784;&#33394;&#25110;&#40657;&#33394;&#65289;&#26159;&#22788;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#12290;&#27492;&#22806;&#65292;&#36974;&#32617;&#26412;&#36523;&#30340;&#24418;&#29366;&#21487;&#20197;&#21253;&#21547;&#19981;&#38656;&#35201;&#30340;&#20449;&#21495;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#21033;&#29992;&#36825;&#20123;&#20449;&#21495;&#36827;&#34892;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#23545;&#22270;&#20687;&#36974;&#32617;&#30340;&#32570;&#22833;&#20559;&#24046;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CNN&#36974;&#32617;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#23618;&#36974;&#32617;&#65292;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#23569;&#36974;&#32617;&#24341;&#36215;&#30340;&#32570;&#22833;&#20559;&#24046;&#12290;&#30452;&#35266;&#19978;&#65292;&#23618;&#36974;&#32617;&#23558;&#19968;&#20010;&#36974;&#32617;&#24212;&#29992;&#20110;&#20013;&#38388;&#28608;&#27963;&#22270;&#65292;&#20351;&#24471;&#27169;&#22411;&#21482;&#22788;&#29702;&#27809;&#26377;&#36974;&#32617;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;i&#65289;&#33021;&#22815;&#28040;&#38500;&#25110;&#26368;&#23567;&#21270;&#36974;&#32617;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to remove features from the input of machine learning models is very important to understand and interpret model predictions. However, this is non-trivial for vision models since masking out parts of the input image typically causes large distribution shifts. This is because the baseline color used for masking (typically grey or black) is out of distribution. Furthermore, the shape of the mask itself can contain unwanted signals which can be used by the model for its predictions. Recently, there has been some progress in mitigating this issue (called missingness bias) in image masking for vision transformers. In this work, we propose a new masking method for CNNs we call layer masking in which the missingness bias caused by masking is reduced to a large extent. Intuitively, layer masking applies a mask to intermediate activation maps so that the model only processes the unmasked input. We show that our method (i) is able to eliminate or minimize the influence of the mask sh
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.13709</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#19981;&#33391;&#20559;&#35265;&#65306;&#36991;&#20813;&#34913;&#37327;&#21361;&#26426;
&lt;/p&gt;
&lt;p&gt;
Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13709
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#26222;&#21450;&#65292;&#39044;&#27979;&#20854;&#20351;&#29992;&#21487;&#33021;&#23545;&#20154;&#20204;&#36896;&#25104;&#20260;&#23475;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20010;&#21463;&#21040;&#20851;&#27880;&#30340;&#38382;&#39064;&#26159;&#36825;&#19968;&#25216;&#26415;&#22312;&#34892;&#20026;&#20013;&#26174;&#31034;&#20986;&#26377;&#23475;&#20559;&#35265;&#12290;&#23613;&#31649;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#65292;&#20294;&#25105;&#20204;&#34913;&#37327;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#36890;&#24120;&#19981;&#28165;&#26970;&#23427;&#20204;&#21040;&#24213;&#34913;&#37327;&#20102;&#20160;&#20040;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#35752;&#35770;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24515;&#29702;&#27979;&#37327;&#23398;&#19987;&#27880;&#20110;&#34913;&#37327;&#19981;&#30452;&#25509;&#21487;&#35266;&#23519;&#21040;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20004;&#20010;&#26680;&#24515;&#27010;&#24565;&#65292;&#21363;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models and Natural Language Processing (NLP) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems (e.g., it is often unclear what they actually measure). In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#22823;&#35789;&#27719;&#37327;&#30340;&#38271;&#23614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20808;&#21069;&#30340;&#23454;&#20363;&#20998;&#21106;&#31639;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#20250;&#24433;&#21709;&#27169;&#22411;&#23398;&#20064;&#31232;&#26377;&#31867;&#21035;&#24182;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#23454;&#38469;&#25361;&#25112;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.13435</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#38271;&#23614;&#23454;&#20363;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Long-tailed Instance Segmentation with Noisy Labels. (arXiv:2211.13435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#22823;&#35789;&#27719;&#37327;&#30340;&#38271;&#23614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20808;&#21069;&#30340;&#23454;&#20363;&#20998;&#21106;&#31639;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#20250;&#24433;&#21709;&#27169;&#22411;&#23398;&#20064;&#31232;&#26377;&#31867;&#21035;&#24182;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#23454;&#38469;&#25361;&#25112;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22122;&#22768;&#26631;&#31614;&#65292;&#21363;&#19968;&#20123;&#27880;&#37322;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#24773;&#20917;&#20855;&#26377;&#29616;&#23454;&#24847;&#20041;&#30340;&#21407;&#22240;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#20174;&#30495;&#23454;&#19990;&#30028;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#36981;&#24490;&#38271;&#23614;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#23454;&#20363;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#30001;&#20110;&#19968;&#24133;&#22270;&#20687;&#20013;&#26377;&#35768;&#22810;&#23454;&#20363;&#65292;&#20854;&#20013;&#19968;&#20123;&#23454;&#20363;&#24456;&#23567;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#24341;&#20837;&#27880;&#37322;&#35823;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#22122;&#22768;&#26631;&#31614;&#30340;&#22823;&#35789;&#27719;&#37327;&#30340;&#38271;&#23614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23454;&#20363;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#23454;&#20363;&#20998;&#21106;&#31639;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#23558;&#38459;&#30861;&#27169;&#22411;&#23398;&#20064;&#31232;&#26377;&#31867;&#21035;&#24182;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#65292;&#36825;&#21551;&#21457;&#25105;&#20204;&#25506;&#32034;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#23454;&#38469;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/GuanlinLee/Noi&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the instance segmentation task on a long-tailed dataset, which contains label noise, i.e., some of the annotations are incorrect. There are two main reasons making this case realistic. First, datasets collected from real world usually obey a long-tailed distribution. Second, for instance segmentation datasets, as there are many instances in one image and some of them are tiny, it is easier to introduce noise into the annotations. Specifically, we propose a new dataset, which is a large vocabulary long-tailed dataset containing label noise for instance segmentation. Furthermore, we evaluate previous proposed instance segmentation algorithms on this dataset. The results indicate that the noise in the training dataset will hamper the model in learning rare categories and decrease the overall performance, and inspire us to explore more effective approaches to address this practical challenge. The code and dataset are available in https://github.com/GuanlinLee/Noi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#32452;&#32455;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#20154;&#26426;&#36827;&#34892;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;&#30340;&#27169;&#22411;&#65288;DroneNet&#65289;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;CNN&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#38477;&#20302;&#25512;&#26029;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2211.07137</link><description>&lt;p&gt;
DroneNet: &#20351;&#29992;&#33258;&#32452;&#32455;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#20154;&#26426;&#36827;&#34892;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DroneNet: Crowd Density Estimation using Self-ONNs for Drones. (arXiv:2211.07137v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07137
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#32452;&#32455;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#20154;&#26426;&#36827;&#34892;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;&#30340;&#27169;&#22411;&#65288;DroneNet&#65289;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;CNN&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#38477;&#20302;&#25512;&#26029;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#20154;&#26426;&#36827;&#34892;&#35270;&#39057;&#30417;&#25511;&#26082;&#26041;&#20415;&#21448;&#39640;&#25928;&#65292;&#30001;&#20110;&#26080;&#20154;&#26426;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#37096;&#32626;&#21644;&#31227;&#21160;&#27809;&#26377;&#38556;&#30861;&#12290;&#26080;&#20154;&#26426;&#35270;&#39057;&#30417;&#25511;&#30340;&#19968;&#20010;&#26377;&#36259;&#24212;&#29992;&#26159;&#20272;&#35745;&#20844;&#20849;&#22330;&#25152;&#30340;&#20154;&#32676;&#23494;&#24230;&#65288;&#21253;&#25324;&#34892;&#20154;&#21644;&#36710;&#36742;&#65289;&#12290;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#65292;&#21033;&#29992;&#22270;&#20687;&#21644;&#35270;&#39057;&#36827;&#34892;&#33258;&#21160;&#20154;&#32676;&#35745;&#25968;&#21644;&#23494;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#36890;&#24120;&#21462;&#20915;&#20110;&#27169;&#22411;&#26550;&#26500;&#65292;&#21363;&#26356;&#28145;&#30340;CNN&#27169;&#22411;&#22312;&#22686;&#21152;&#25512;&#26029;&#26102;&#38388;&#30340;&#20195;&#20215;&#19979;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#20154;&#26426;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#65288;DroneNet&#65289;&#65292;&#20351;&#29992;&#33258;&#32452;&#32455;&#25805;&#20316;&#31070;&#32463;&#32593;&#32476;&#65288;Self-ONN&#65289;&#12290;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#65292;Self-ONN&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#22312;&#20004;&#20010;&#26080;&#20154;&#26426;&#35270;&#35282;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;DroneNet&#22312;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;&#19978;&#34920;&#29616;&#20986;
&lt;/p&gt;
&lt;p&gt;
Video surveillance using drones is both convenient and efficient due to the ease of deployment and unobstructed movement of drones in many scenarios. An interesting application of drone-based video surveillance is to estimate crowd densities (both pedestrians and vehicles) in public places. Deep learning using convolution neural networks (CNNs) is employed for automatic crowd counting and density estimation using images and videos. However, the performance and accuracy of such models typically depend upon the model architecture i.e., deeper CNN models improve accuracy at the cost of increased inference time. In this paper, we propose a novel crowd density estimation model for drones (DroneNet) using Self-organized Operational Neural Networks (Self-ONN). Self-ONN provides efficient learning capabilities with lower computational complexity as compared to CNN-based models. We tested our algorithm on two drone-view public datasets. Our evaluation shows that the proposed DroneNet shows supe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02641</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65306;&#26469;&#33258;&#26102;&#39057;&#20998;&#26512;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;&#20998;&#31867;&#26159;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#22522;&#30784;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#22791;&#21463;&#36861;&#25447;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#65292;MI-EEG&#20998;&#31867;&#22120;&#30340;&#36235;&#21183;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#36716;&#21464;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#25552;&#39640;&#12290; Tensor-CSPNet&#30340;&#20986;&#29616;&#26159;BCI&#30740;&#31350;&#20013;&#31532;&#19968;&#20010;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#65292;&#20854;&#24402;&#22240;&#20110;&#20449;&#21495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#30340;&#29305;&#24449;&#21270;&#12290;&#20174;&#26681;&#26412;&#19978;&#35762;&#65292;Tensor-CSPNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#12290;&#19982;&#21033;&#29992;EEG&#20449;&#21495;&#30340;&#19968;&#38454;&#32479;&#35745;&#37327;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#21033;&#29992;&#36825;&#20123;&#20108;&#38454;&#32479;&#35745;&#37327;&#20195;&#34920;&#20102;&#32463;&#20856;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#20123;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#21306;&#20998;&#20449;&#24687;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;MI-EEG&#20998;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;GDL&#20998;&#31867;&#22120;&#65292;
&lt;/p&gt;
&lt;p&gt;
The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#34987;&#24573;&#35270;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25104;&#21151;&#25915;&#20987;&#26159;&#30001;&#20110;&#37096;&#20998;&#26412;&#22320;&#21028;&#21035;&#22120;&#23545;&#27602;&#32032;&#36807;&#24230;&#25311;&#21512;&#25152;&#33268;&#12290;</title><link>http://arxiv.org/abs/2210.10886</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#19982;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. (arXiv:2210.10886v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#34987;&#24573;&#35270;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25104;&#21151;&#25915;&#20987;&#26159;&#30001;&#20110;&#37096;&#20998;&#26412;&#22320;&#21028;&#21035;&#22120;&#23545;&#27602;&#32032;&#36807;&#24230;&#25311;&#21512;&#25152;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22522;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#25216;&#26415;&#24050;&#32463;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#20013;&#65292;&#29992;&#20110;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#20197;&#25903;&#25345;&#24320;&#25918;&#30740;&#31350;&#24182;&#22686;&#21152;&#21307;&#23398;&#25968;&#25454;&#38598;&#12290;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;GANs&#65289;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#35757;&#32451;&#20013;&#22830;&#27169;&#22411;&#24182;&#20445;&#25345;&#26412;&#22320;&#21407;&#22987;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;FL&#26381;&#21153;&#22120;&#26080;&#27861;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#22823;&#22810;&#25968;&#21518;&#38376;&#25915;&#20987;&#31574;&#30053;&#38598;&#20013;&#22312;&#20998;&#31867;&#27169;&#22411;&#21644;&#20013;&#24515;&#21270;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#33021;&#21542;&#24433;&#21709;GAN&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#22914;&#26524;&#21487;&#20197;&#24433;&#21709;&#65292;&#22914;&#20309;&#22312;FL&#29615;&#22659;&#20013;&#36827;&#34892;&#38450;&#24481;&#20063;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32852;&#37030;GANs&#65288;FedGANs&#65289;&#20013;&#21518;&#38376;&#25915;&#20987;&#36825;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#25915;&#20987;&#30340;&#25104;&#21151;&#38543;&#21518;&#34987;&#30830;&#23450;&#20026;&#37096;&#20998;&#26412;&#22320;&#21028;&#21035;&#22120;&#23545;&#27602;&#32032;&#36807;&#24230;&#25311;&#21512;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research and augment medical datasets. Training generative adversarial neural networks (GANs) usually require large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data while keeping raw data locally. However, given that the FL server cannot access the raw data, it is vulnerable to backdoor attacks, an adversarial by poisoning training data. Most backdoor attack strategies focus on classification models and centralized domains. It is still an open question if the existing backdoor attacks can affect GAN training and, if so, how to defend against the attack in the FL setting. In this work, we investigate the overlooked issue of backdoor attacks in federated GANs (FedGANs). The success of this attack is subsequently determined to be the result of some local discriminators overfitting the poison
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#26469;&#24110;&#21161;&#26816;&#27979;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00131</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65306;&#19968;&#20010;&#20197;&#22240;&#26524;&#20851;&#31995;&#20026;&#22522;&#30784;&#30340;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution. (arXiv:2210.00131v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#26469;&#24110;&#21161;&#26816;&#27979;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#24120;&#24120;&#23384;&#22312;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#38382;&#39064;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#26631;&#35760;&#39044;&#27979;&#65292;&#22312;&#25512;&#26029;&#26102;&#21487;&#33021;&#26377;&#22810;&#20010;&#21333;&#35789;&#31526;&#21512;&#29992;&#25143;&#20135;&#29983;&#33258;&#28982;&#35821;&#35328;&#30340;&#24847;&#22270;&#65292;&#28982;&#32780;&#22312;&#35757;&#32451;&#26102;&#21482;&#26377;&#19968;&#20010;&#21333;&#35789;&#33021;&#22815;&#26368;&#23567;&#21270;&#20219;&#21153;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#21512;&#29702;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#25551;&#36848;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#22312;&#29983;&#25104;&#34394;&#20551;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#20854;&#31616;&#27905;&#24615;&#65292;&#25105;&#20204;&#30340;&#22240;&#26524;&#27169;&#22411;&#30452;&#25509;&#25351;&#23548;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#20013;&#30340;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#19978;&#65292;&#20197;&#24110;&#21161; 1) &#26816;&#27979;&#25512;&#26029;&#26102;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#21033;&#29992;&#20102; 2&#65289;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#28085;&#30422;&#20102; A&#65289;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;BERT-base&#21040;GPT 3.5&#65292;B&#65289;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20174;&#36974;&#34109;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#21040;&#36825;&#20123;&#30446;&#26631;&#30340;&#28151;&#21512;&#65292;&#20197;&#21450;C&#65289;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20174;&#20165;&#39044;&#35757;&#32451;&#21040;&#22686;&#24378;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language modeling tasks are often underspecified: for a given token prediction, many words may satisfy the user's intent of producing natural language at inference time, however only one word would minimize the task's loss function at training time. We provide a simple yet plausible causal mechanism describing the role underspecification plays in the generation of spurious correlations. Despite its simplicity, our causal model directly informs the development of two lightweight black-box evaluation methods, that we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid in the detection of inference-time task underspecification by exploiting 2) previously unreported gender vs. time and gender vs. location spurious correlations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B) pre-training objectives: from masked &amp; autoregressive language modeling to a mixture of these objectives, and C) training stages: from pre-training only to reinforcement l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23558;Transformer&#24212;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21452;Transformer&#24341;&#23548;&#30340;&#26102;&#24207;&#36923;&#36753;&#26694;&#26550;(T2TL)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20004;&#27425;&#21033;&#29992;Transformer&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33021;&#22815;&#39640;&#25928;&#29702;&#35299;&#20219;&#21153;&#25351;&#20196;&#65292;&#24182;&#25913;&#36827;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.13220</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;Transformer&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#26102;&#24207;&#36923;&#36753;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Exploiting Transformer in Sparse Reward Reinforcement Learning for Interpretable Temporal Logic Motion Planning. (arXiv:2209.13220v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23558;Transformer&#24212;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21452;Transformer&#24341;&#23548;&#30340;&#26102;&#24207;&#36923;&#36753;&#26694;&#26550;(T2TL)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20004;&#27425;&#21033;&#29992;Transformer&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33021;&#22815;&#39640;&#25928;&#29702;&#35299;&#20219;&#21153;&#25351;&#20196;&#65292;&#24182;&#25913;&#36827;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#31639;&#27861;&#22312;&#32771;&#34385;&#30340;&#20219;&#21153;&#20013;&#39640;&#24230;&#20381;&#36182;&#20110;&#25163;&#21160;&#23450;&#21046;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;Transformer&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21452;Transformer&#24341;&#23548;&#30340;&#26102;&#24207;&#36923;&#36753;&#26694;&#26550;(T2TL)&#65292;&#23427;&#20004;&#27425;&#21033;&#29992;Transformer&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#21363;&#39318;&#20808;&#36890;&#36807;Transformer&#27169;&#22359;&#23545;LTL&#25351;&#20196;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#25928;&#29702;&#35299;&#20219;&#21153;&#25351;&#20196;&#65292;&#28982;&#21518;&#20877;&#27425;&#36890;&#36807;Transformer&#23545;&#19978;&#19979;&#25991;&#21464;&#37327;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#25913;&#36827;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;LTL&#25351;&#20196;&#30001;&#38480;&#23432;&#27861;LTL&#25351;&#23450;&#12290;&#20316;&#20026;&#19968;&#31181;&#35821;&#20041;&#20445;&#25345;&#30340;&#37325;&#20889;&#25805;&#20316;&#65292;&#21033;&#29992;LTL&#36827;&#23637;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#21487;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#65292;&#20174;&#32780;&#23558;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20915;&#31574;&#36807;&#31243;&#36716;&#25442;&#20026;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automaton based approaches have enabled robots to perform various complex tasks. However, most existing automaton based algorithms highly rely on the manually customized representation of states for the considered task, limiting its applicability in deep reinforcement learning algorithms. To address this issue, by incorporating Transformer into reinforcement learning, we develop a Double-Transformer-guided Temporal Logic framework (T2TL) that exploits the structural feature of Transformer twice, i.e., first encoding the LTL instruction via the Transformer module for efficient understanding of task instructions during the training and then encoding the context variable via the Transformer again for improved task performance. Particularly, the LTL instruction is specified by co-safe LTL. As a semantics-preserving rewriting operation, LTL progression is exploited to decompose the complex task into learnable sub-goals, which not only converts non-Markovian reward decision processes to Mark
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#24130;&#27861;&#21644;&#21453;&#24130;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#32447;&#24615;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#24494;&#20998;&#31639;&#23376;&#65292;&#36890;&#36807;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#23454;&#26045;&#36845;&#20195;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#27714;&#35299;&#26368;&#22823;&#27491;&#29305;&#24449;&#20540;&#12289;&#26368;&#23567;&#29305;&#24449;&#20540;&#21644;&#20869;&#37096;&#29305;&#24449;&#20540;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.11134</link><description>&lt;p&gt;
&#22522;&#20110;&#24130;&#27861;&#21644;&#21453;&#24130;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#32447;&#24615;&#29305;&#24449;&#20540;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Neural Networks Based on Power Method and Inverse Power Method for Solving Linear Eigenvalue Problems. (arXiv:2209.11134v5 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#24130;&#27861;&#21644;&#21453;&#24130;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#32447;&#24615;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#24494;&#20998;&#31639;&#23376;&#65292;&#36890;&#36807;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#23454;&#26045;&#36845;&#20195;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#27714;&#35299;&#26368;&#22823;&#27491;&#29305;&#24449;&#20540;&#12289;&#26368;&#23567;&#29305;&#24449;&#20540;&#21644;&#20869;&#37096;&#29305;&#24449;&#20540;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21463;&#24130;&#27861;&#21644;&#21453;&#24130;&#27861;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#27714;&#35299;&#32447;&#24615;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#26041;&#27861;&#31867;&#20284;&#65292;&#20854;&#20013;&#24494;&#20998;&#31639;&#23376;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#12290;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#29305;&#24449;&#20989;&#25968;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#29305;&#23450;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#26045;&#36845;&#20195;&#31639;&#27861;&#12290;&#22312;&#32473;&#23450;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#27714;&#35299;&#26368;&#22823;&#27491;&#29305;&#24449;&#20540;&#12289;&#26368;&#23567;&#29305;&#24449;&#20540;&#21644;&#20869;&#37096;&#29305;&#24449;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#32500;&#12289;&#20108;&#32500;&#21644;&#39640;&#32500;&#25968;&#20540;&#23454;&#39564;&#20013;&#32771;&#23519;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we propose two kinds of neural networks inspired by power method and inverse power method to solve linear eigenvalue problems. These neural networks share similar ideas with traditional methods, in which the differential operator is realized by automatic differentiation. The eigenfunction of the eigenvalue problem is learned by the neural network and the iterative algorithms are implemented by optimizing the specially defined loss function. The largest positive eigenvalue, smallest eigenvalue and interior eigenvalues with the given prior knowledge can be solved efficiently. We examine the applicability and accuracy of our methods in the numerical experiments in one dimension, two dimensions and higher dimensions. Numerical results show that accurate eigenvalue and eigenfunction approximations can be obtained by our methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35760;&#24518;&#22686;&#24378;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#21644;&#27604;&#36739;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2209.10818</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20010;&#21463;&#21551;&#21457;&#20110;&#22823;&#33041;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Memory-Augmented Graph Neural Networks: A Brain-Inspired Review. (arXiv:2209.10818v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35760;&#24518;&#22686;&#24378;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#21644;&#27604;&#36739;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#23545;&#29616;&#26377;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35760;&#24518;&#22686;&#24378;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27861;&#21644;&#19968;&#32452;&#29992;&#20110;&#27604;&#36739;&#23427;&#20204;&#35760;&#24518;&#26426;&#21046;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a comprehensive review of the existing literature on memory-augmented GNNs. We review these works through the lens of psychology and neuroscience, which has several established theories on how multiple memory systems and mechanisms operate in biological brains. We propose a taxonomy of memory-augmented GNNs and a set of criteria for comparing their memory mechanisms. We also provide critical discussions on the limitations of these works. Finally, we discuss the challenges and future directions for this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2208.09418</link><description>&lt;p&gt;
SAFARI&#65306;&#40065;&#26834;&#24615;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#22810;&#21151;&#33021;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#24314;&#31435;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#36947;&#38556;&#30861;&#12290;&#23613;&#31649;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31038;&#21306;&#20570;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#35299;&#37322;&#32570;&#20047;&#40065;&#26834;&#24615;&#8212;&#8212;&#26080;&#27861;&#21306;&#20998;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#32473;&#23450;&#30340;XAI&#26041;&#27861;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#20849;&#21516;&#24212;&#23545;&#30340;&#20960;&#20010;&#25361;&#25112;&#65306;i)&#29616;&#26377;&#25351;&#26631;&#19981;&#20840;&#38754;&#65307;ii)XAI&#25216;&#26415;&#39640;&#24230;&#24322;&#36136;&#65307;iii)&#35823;&#35299;&#36890;&#24120;&#26159;&#32597;&#35265;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#20998;&#21035;&#28041;&#21450;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#12290;&#20351;&#29992;&#20855;&#26377;&#23450;&#21046;&#36866;&#24212;&#24230;&#20989;&#25968;&#30340;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26469;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#20351;&#29992;&#19987;&#38376;&#29992;&#20110;&#20272;&#35745;&#32597;&#35265;&#20107;&#20214;&#27010;&#29575;&#30340;&#23376;&#38598;&#27169;&#25311;&#65288;SS&#65289;&#26469;&#36827;&#34892;&#25972;&#20307;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explainable AI (XAI) community, explanations lack robustness -- indistinguishable input perturbations may lead to different XAI results. Thus, it is vital to assess how robust DL interpretability is, given an XAI method. In this paper, we identify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not comprehensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation methods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respectively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to estimate rare event probabilities, is used for evaluating overa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21322;&#30417;&#30563;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#27880;&#30340;&#35821;&#21477;&#19978;&#24212;&#29992;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#20013;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#21644;&#39046;&#22495;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.06767</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#30340;&#36328;&#35821;&#35328;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised cross-lingual speech emotion recognition. (arXiv:2207.06767v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06767
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21322;&#30417;&#30563;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#27880;&#30340;&#35821;&#21477;&#19978;&#24212;&#29992;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#20013;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#21644;&#39046;&#22495;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20351;&#29992;&#65292;&#21333;&#35821;&#31181;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28304;&#35821;&#31181;&#21644;&#30446;&#26631;&#35821;&#31181;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#20197;&#21450;&#30446;&#26631;&#35821;&#31181;&#20013;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#65292;&#36328;&#35821;&#35328;SER&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#22240;&#32032;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24403;&#30446;&#26631;&#35821;&#31181;&#65288;&#21363;&#26032;&#35821;&#35328;&#65289;&#20013;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#26679;&#26412;&#21487;&#29992;&#26102;&#30340;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Transformer&#65292;&#24182;&#36890;&#36807;&#22312;&#26410;&#26631;&#27880;&#30340;&#35821;&#21477;&#19978;&#21033;&#29992;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#36866;&#24212;&#26032;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30828;&#20266;&#26631;&#31614;&#21644;&#36719;&#20266;&#26631;&#31614;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29420;&#31435;&#35828;&#35805;&#20154;&#30340;&#35774;&#23450;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#20004;&#20010;&#30446;&#26631;&#35821;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance in Speech Emotion Recognition (SER) on a single language has increased greatly in the last few years thanks to the use of deep learning techniques. However, cross-lingual SER remains a challenge in real-world applications due to two main factors: the first is the big gap among the source and the target domain distributions; the second factor is the major availability of unlabeled utterances in contrast to the labeled ones for the new language. Taking into account previous aspects, we propose a Semi-Supervised Learning (SSL) method for cross-lingual emotion recognition when only few labeled examples in the target domain (i.e. the new language) are available. Our method is based on a Transformer and it adapts to the new domain by exploiting a pseudo-labeling strategy on the unlabeled utterances. In particular, the use of a hard and soft pseudo-labels approach is investigated. We thoroughly evaluate the performance of the proposed method in a speaker-independent setup on both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#32593;&#32476;&#20108;&#20540;&#21270;&#35270;&#20026;&#19968;&#20010;&#20108;&#20540;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#20316;&#20026;&#20998;&#31867;&#22120;&#21644;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20197;&#35299;&#20915;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.07433</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#31867;&#30340;&#20108;&#20540;&#21270;&#65306;&#36719;&#20989;&#25968;&#30495;&#30340;&#24517;&#35201;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Binarizing by Classification: Is soft function really necessary?. (arXiv:2205.07433v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#32593;&#32476;&#20108;&#20540;&#21270;&#35270;&#20026;&#19968;&#20010;&#20108;&#20540;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#20316;&#20026;&#20998;&#31867;&#22120;&#21644;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20197;&#35299;&#20915;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;"Sign"&#20989;&#25968;&#23545;&#26435;&#37325;&#21644;&#28608;&#27963;&#36827;&#34892;&#20108;&#20540;&#21270;&#65292;&#36825;&#38656;&#35201;&#26799;&#24230;&#20272;&#35745;&#22120;&#26469;&#20811;&#26381;&#20854;&#19981;&#21487;&#24494;&#24615;&#65292;&#24182;&#19988;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#24102;&#26469;&#26799;&#24230;&#35823;&#24046;&#12290;&#34429;&#28982;&#35768;&#22810;&#25163;&#21160;&#35774;&#35745;&#30340;&#36719;&#20989;&#25968;&#34987;&#25552;&#20986;&#20316;&#20026;&#26799;&#24230;&#20272;&#35745;&#22120;&#20197;&#26356;&#22909;&#22320;&#36924;&#36817;&#26799;&#24230;&#65292;&#20294;&#23427;&#20204;&#30340;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#65292;&#24182;&#19988;&#20108;&#20540;&#27169;&#22411;&#19982;&#23436;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#20173;&#23384;&#22312;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20943;&#23569;&#26799;&#24230;&#35823;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#32593;&#32476;&#20108;&#20540;&#21270;&#35270;&#20026;&#19968;&#20010;&#20108;&#20540;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20316;&#20026;&#20998;&#31867;&#22120;&#20197;&#21450;&#22312;&#21453;&#21521;&#20256;&#36882;&#20013;&#20316;&#20026;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#30001;&#20110;MLP&#20855;&#26377;&#36866;&#24212;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#30340;&#29702;&#35770;&#33021;&#21147;&#65292;&#23427;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20108;&#20540;&#21270;&#32593;&#32476;&#24182;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#36719;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;MLP&#20063;&#20855;&#26377;&#36827;&#34892;&#32593;&#32476;&#20108;&#20540;&#21270;&#21644;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary neural networks leverage $\mathrm{Sign}$ function to binarize weights and activations, which require gradient estimators to overcome its non-differentiability and will inevitably bring gradient errors during backpropagation. Although many hand-designed soft functions have been proposed as gradient estimators to better approximate gradients, their mechanism is not clear and there are still huge performance gaps between binary models and their full-precision counterparts. To address these issues and reduce gradient error, we propose to tackle network binarization as a binary classification problem and use a multi-layer perceptron (MLP) as the classifier in the forward pass and gradient estimator in the backward pass. Benefiting from the MLP's theoretical capability to fit any continuous function, it can be adaptively learned to binarize networks and backpropagate gradients without any prior knowledge of soft functions. From this perspective, we further empirically justify that eve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21457;&#29616;&#21644;&#21512;&#25104;&#29289;&#20307;&#20809;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29289;&#20307;&#34920;&#31034;&#20026;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20809;&#22330;&#26469;&#25552;&#39640;&#28210;&#26579;&#36136;&#37327;&#21644;&#25805;&#20316;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.03923</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21457;&#29616;&#21644;&#21512;&#25104;&#29289;&#20307;&#20809;&#22330;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21457;&#29616;&#21644;&#21512;&#25104;&#29289;&#20307;&#20809;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29289;&#20307;&#34920;&#31034;&#20026;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20809;&#22330;&#26469;&#25552;&#39640;&#28210;&#26579;&#36136;&#37327;&#21644;&#25805;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#27861;&#65292;&#21253;&#25324;&#36830;&#32493;&#21644;&#31163;&#25955;&#34920;&#31034;&#65292;&#24050;&#32463;&#25104;&#20026;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#30340;&#24378;&#22823;&#26032;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#26080;&#30417;&#30563;&#21457;&#29616;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#27861;&#12290;&#28982;&#32780;&#65292;&#23556;&#32447;&#34892;&#36827;&#30340;&#39640;&#25104;&#26412;&#65292;&#21152;&#19978;&#27599;&#20010;&#29289;&#20307;&#34920;&#31034;&#27861;&#37117;&#24517;&#39035;&#21333;&#29420;&#36827;&#34892;&#23556;&#32447;&#34892;&#36827;&#30340;&#20107;&#23454;&#65292;&#23548;&#33268;&#37319;&#26679;&#19981;&#36275;&#30340;&#20142;&#24230;&#22330;&#65292;&#20174;&#32780;&#20135;&#29983;&#22122;&#28857;&#28210;&#26579;&#12289;&#20302;&#24103;&#29575;&#12289;&#39640;&#20869;&#23384;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#35757;&#32451;&#21644;&#28210;&#26579;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#29289;&#20307;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20809;&#22330;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20809;&#22330;&#22797;&#21512;&#27169;&#22359;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20809;&#22330;&#37325;&#24314;&#20840;&#23616;&#20809;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#32452;&#21512;&#24615;&#29289;&#20307;&#20809;&#22330;&#65288;COLF&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#23398;&#20064;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#27861;&#65292;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#21644;&#26032;&#35270;&#35282;&#21512;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#32852;&#37030;&#33976;&#39311;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#30693;&#35782;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.07028</link><description>&lt;p&gt;
&#25506;&#32034;&#26080;&#20195;&#29702;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#22312;&#32852;&#37030;&#33976;&#39311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#32852;&#37030;&#33976;&#39311;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#30693;&#35782;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm, &#22312;&#27492;&#26381;&#21153;&#22120;&#21608;&#26399;&#24615;&#22320;&#25910;&#38598;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;, &#32780;&#19981;&#32452;&#35013;&#20854;&#31169;&#26377;&#25968;&#25454;. &#26377;&#38480;&#30340;&#36890;&#35759;&#21644;&#20010;&#24615;&#21270;&#38656;&#27714;&#23545;FL&#25552;&#20986;&#20102;&#20005;&#23803;&#25361;&#25112;. &#32852;&#37030;&#33976;&#39311; (FD) &#34987;&#25552;&#20986;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;, &#19982;&#27492;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;, &#25903;&#25345;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36890;&#35759;&#24320;&#38144;. &#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FD&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#20013;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#30340;. &#19968;&#20123;&#26368;&#36817;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#30340;FD&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#39069;&#22806;&#30340;&#20844;&#20849;&#25968;&#25454;&#30340;&#38656;&#27714;, &#20294;&#30001;&#20110;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#32780;&#20135;&#29983;&#20102;&#26126;&#26174;&#30340;&#24046;&#24322;, &#23548;&#33268;&#26381;&#21153;&#22120;&#19978;&#30340;&#27169;&#22411;&#34920;&#31034;&#19981;&#26126;&#30830;&#65292;&#24182;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;.
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving machine learning paradigm in which the server periodically aggregates local model parameters from clients without assembling their private data.  Constrained communication and personalization requirements pose severe challenges to FL. Federated distillation (FD) is proposed to simultaneously address the above two problems, which exchanges knowledge between the server and clients, supporting heterogeneous local models while significantly reducing communication overhead. However, most existing FD methods require a proxy dataset, which is often unavailable in reality.  A few recent proxy-data-free FD approaches can eliminate the need for additional public data, but suffer from remarkable discrepancy among local knowledge due to client-side model heterogeneity, leading to ambiguous representation on the server and inevitable accuracy degradation.  To tackle this issue, we propose a proxy-data-free FD algorithm based on distributed knowledge c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#35270;&#35273;Transformer&#30340;&#21442;&#25968;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#23376;&#27169;&#22359;&#25237;&#24433;&#21040;&#23376;&#31354;&#38388;&#36827;&#34892;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#19982;&#21442;&#25968;&#25104;&#26412;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2203.16329</link><description>&lt;p&gt;
&#35270;&#35273;Transformer&#30340;&#21442;&#25968;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient Model Adaptation for Vision Transformers. (arXiv:2203.16329v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#35270;&#35273;Transformer&#30340;&#21442;&#25968;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#23376;&#27169;&#22359;&#25237;&#24433;&#21040;&#23376;&#31354;&#38388;&#36827;&#34892;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#19982;&#21442;&#25968;&#25104;&#26412;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;Transformer&#65289;&#36866;&#24212;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#23454;&#29616;&#20102;&#24456;&#22909;&#30340;&#36801;&#31227;&#23398;&#20064;&#24615;&#33021;&#12290;&#24120;&#35265;&#30340;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#35201;&#20040;&#26356;&#26032;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#35201;&#20040;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#22120;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#35270;&#35273;Transformer&#30340;&#21442;&#25968;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#23376;&#31354;&#38388;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#39640;&#25928;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23545;&#27599;&#31181;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#24615;&#33021;&#19982;&#21442;&#25968;&#25104;&#26412;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#26694;&#26550;&#65292;&#39318;&#20808;&#36890;&#36807;&#24230;&#37327;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#26469;&#36873;&#25321;&#23376;&#27169;&#22359;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Kronecker&#36866;&#24212;&#26041;&#27861;&#23558;&#23427;&#20204;&#25237;&#24433;&#21040;&#23376;&#31354;&#38388;&#36827;&#34892;&#36827;&#19968;&#27493;&#20998;&#35299;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19968;&#20010;&#30456;&#20284;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computer vision, it has achieved great transfer learning performance via adapting large-scale pretrained vision models (e.g., vision transformers) to downstream tasks. Common approaches for model adaptation either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient model adaptation strategies for vision transformers on the image classification task. We formulate efficient model adaptation as a subspace training problem and perform a comprehensive benchmarking over different efficient adaptation methods. We conduct an empirical study on each efficient model adaptation method focusing on its performance alongside parameter cost. Furthermore, we propose a parameter-efficient model adaptation framework, which first selects submodules by measuring local intrinsic dimensions and then projects them into subspace for further decomposition via a novel Kronecker Adaptation (KAdaptation) method. We analyze and compare our method with a dive
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#65292;&#21516;&#26102;&#25351;&#20986;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2111.07533</link><description>&lt;p&gt;
&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65306;&#27010;&#24565;&#12289;&#25216;&#26415;&#19982;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated scholarly paper review: Concepts, technologies, and challenges. (arXiv:2111.07533v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07533
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#65292;&#21516;&#26102;&#25351;&#20986;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#34892;&#35780;&#23457;&#26159;&#30740;&#31350;&#35780;&#20215;&#30340;&#24191;&#27867;&#25509;&#21463;&#26426;&#21046;&#65292;&#22312;&#23398;&#26415;&#20986;&#29256;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#25928;&#29575;&#20302;&#19979;&#21644;&#21487;&#37325;&#22797;&#24615;&#24046;&#65292;&#36825;&#19968;&#26426;&#21046;&#38271;&#26399;&#20197;&#26469;&#22791;&#21463;&#25209;&#35780;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#36741;&#21161;&#21516;&#34892;&#35780;&#23457;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#28041;&#21450;&#20154;&#21592;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38480;&#21046;&#20173;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#24182;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#12290;&#22312;&#23457;&#26597;&#21644;&#35752;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;ASPR &#30340;&#27599;&#20010;&#38454;&#27573;&#24050;&#32463;&#26377;&#30456;&#24212;&#30340;&#30740;&#31350;&#21644;&#21021;&#27493;&#23454;&#26045;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;ASPR&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer review is a widely accepted mechanism for research evaluation, playing a pivotal role in academic publishing. However, criticisms have long been leveled on this mechanism, mostly because of its poor efficiency and low reproducibility. Recent years have seen the application of artificial intelligence (AI) in assisting the peer review process. Nonetheless, with the involvement of humans, such limitations remain inevitable. In this paper, we propose the concept and pipeline of automated scholarly paper review (ASPR) and review the relevant literature and technologies of achieving a full-scale computerized review process. On the basis of the review and discussion, we conclude that there is already corresponding research and preliminary implementation at each stage of ASPR. We further look into the challenges in ASPR with the existing technologies. The major difficulties lie in imperfect document parsing and representation, inadequate data, defective human-computer interaction, and fla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#19977;&#20540;&#37327;&#21270;&#26041;&#27861;&#65288;PTQ&#65289;&#65292;&#36890;&#36807;&#38598;&#25104;L2&#24402;&#19968;&#21270;&#12289;&#21098;&#26525;&#21644;&#26435;&#37325;&#34928;&#20943;&#39033;&#65292;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27604;&#29305;&#23485;&#24230;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20934;&#30830;&#24615;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#22823;&#22823;&#20943;&#23567;&#19988;&#20445;&#25345;&#36739;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2107.10998</link><description>&lt;p&gt;
&#21098;&#26525;&#19977;&#20540;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pruning Ternary Quantization. (arXiv:2107.10998v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.10998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#19977;&#20540;&#37327;&#21270;&#26041;&#27861;&#65288;PTQ&#65289;&#65292;&#36890;&#36807;&#38598;&#25104;L2&#24402;&#19968;&#21270;&#12289;&#21098;&#26525;&#21644;&#26435;&#37325;&#34928;&#20943;&#39033;&#65292;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27604;&#29305;&#23485;&#24230;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20934;&#30830;&#24615;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#22823;&#22823;&#20943;&#23567;&#19988;&#20445;&#25345;&#36739;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20934;&#30830;&#24615;&#26159;&#28145;&#24230;&#27169;&#22411;&#21387;&#32553;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#23558;&#36825;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#20998;&#24320;&#22788;&#29702;&#65292;&#22240;&#20026;&#21516;&#26102;&#20248;&#21270;&#23427;&#20204;&#38750;&#24120;&#22256;&#38590;&#12290;&#20363;&#22914;&#65292;&#20302;&#27604;&#29305;&#37327;&#21270;&#26088;&#22312;&#33719;&#24471;&#26356;&#24555;&#30340;&#27169;&#22411;&#65307;&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#26088;&#22312;&#25552;&#39640;&#21387;&#32553;&#27604;&#21644;&#20934;&#30830;&#24615;&#65307;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26088;&#22312;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#20026;&#20102;&#21516;&#26102;&#20248;&#21270;&#27604;&#29305;&#23485;&#24230;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21098;&#26525;&#19977;&#20540;&#37327;&#21270;&#65288;PTQ&#65289;&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#26377;&#25928;&#12289;&#23545;&#31216;&#30340;&#19977;&#20540;&#37327;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;L2&#24402;&#19968;&#21270;&#12289;&#21098;&#26525;&#21644;&#26435;&#37325;&#34928;&#20943;&#39033;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#20943;&#23567;&#37327;&#21270;&#36807;&#31243;&#20013;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26435;&#37325;&#24046;&#24322;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#24230;&#21387;&#32553;&#30340;&#19977;&#20540;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24102;&#26469;&#20102;&#26368;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#21644;&#26368;&#39640;&#30340;&#21387;&#32553;&#27604;&#12290;&#20363;&#22914;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#20165;&#20026;939kb&#65288;49&#20493;&#65289;&#30340;2&#20301;&#19977;&#20540;ResNet-18&#27169;&#22411;&#65292;&#20165;&#20934;&#30830;&#24615;&#19979;&#38477;4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference time, model size, and accuracy are three key factors in deep model compression. Most of the existing work addresses these three key factors separately as it is difficult to optimize them all at the same time. For example, low-bit quantization aims at obtaining a faster model; weight sharing quantization aims at improving compression ratio and accuracy; and mixed-precision quantization aims at balancing accuracy and inference time. To simultaneously optimize bit-width, model size, and accuracy, we propose pruning ternary quantization (PTQ): a simple, effective, symmetric ternary quantization method. We integrate L2 normalization, pruning, and the weight decay term to reduce the weight discrepancy in the gradient estimator during quantization, thus producing highly compressed ternary weights. Our method brings the highest test accuracy and the highest compression ratio. For example, it produces a 939kb (49$\times$) 2bit ternary ResNet-18 model with only 4\% accuracy drop on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#22270;&#20013;&#20197;&#22312;&#32447;&#30340;&#26041;&#24335;&#20026;&#36793;&#21644;&#23376;&#22270;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#25193;&#23637;&#30340;&#33609;&#22270;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.04486</link><description>&lt;p&gt;
&#22312;&#27969;&#22270;&#20013;&#22522;&#20110;&#33609;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sketch-Based Anomaly Detection in Streaming Graphs. (arXiv:2106.04486v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#22270;&#20013;&#20197;&#22312;&#32447;&#30340;&#26041;&#24335;&#20026;&#36793;&#21644;&#23376;&#22270;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#25193;&#23637;&#30340;&#33609;&#22270;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#22270;&#30340;&#22270;&#36793;&#27969;&#20013;&#65292;&#22914;&#20309;&#20197;&#22312;&#32447;&#30340;&#26041;&#24335;&#20026;&#36793;&#21644;&#23376;&#22270;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#65292;&#20197;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#22312;&#24120;&#25968;&#26102;&#38388;&#21644;&#20869;&#23384;&#19979;&#36827;&#34892;&#65311;&#26412;&#25991;&#39318;&#20808;&#23558;&#35745;&#25968;&#26368;&#23567;&#21270;&#33609;&#22270;&#25968;&#25454;&#32467;&#26500;&#25193;&#23637;&#20026;&#39640;&#38454;&#33609;&#22270;&#65292;&#35813;&#39640;&#38454;&#33609;&#22270;&#20855;&#26377;&#20445;&#30041;&#31264;&#23494;&#23376;&#22270;&#32467;&#26500;&#30340;&#26377;&#29992;&#23646;&#24615;&#65288;&#36755;&#20837;&#20013;&#30340;&#31264;&#23494;&#23376;&#22270;&#36716;&#21270;&#20026;&#25968;&#25454;&#32467;&#26500;&#20013;&#30340;&#23494;&#38598;&#23376;&#30697;&#38453;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;4&#20010;&#21033;&#29992;&#36825;&#20010;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#65288;a&#65289;&#21516;&#26102;&#26816;&#27979;&#36793;&#21644;&#22270;&#30340;&#24322;&#24120;&#65307;&#65288;b&#65289;&#20197;&#24120;&#25968;&#20869;&#23384;&#21644;&#24120;&#25968;&#26356;&#26032;&#26102;&#38388;&#22788;&#29702;&#27599;&#20010;&#26032;&#21040;&#36798;&#30340;&#36793;&#30340;&#36793;&#21644;&#22270;&#65307;&#65288;c&#65289;&#22312;4&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#23558;&#31264;&#23494;&#23376;&#22270;&#25628;&#32034;&#32435;&#20837;&#27969;&#24335;&#26041;&#27861;&#20197;&#26816;&#27979;&#22270;&#24418;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges and subgraphs in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? For example, in intrusion detection, existing work seeks to detect either anomalous edges or anomalous subgraphs, but not both. In this paper, we first extend the count-min sketch data structure to a higher-order sketch. This higher-order sketch has the useful property of preserving the dense subgraph structure (dense subgraphs in the input turn into dense submatrices in the data structure). We then propose 4 online algorithms that utilize this enhanced data structure, which (a) detect both edge and graph anomalies; (b) process each edge and graph in constant memory and constant update time per newly arriving edge, and; (c) outperform state-of-the-art baselines on 4 real-world datasets. Our method is the first streaming approach that incorporates dense subgraph search to detect gra
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25506;&#35752;&#20102;&#20010;&#20307;&#26234;&#33021;&#26159;&#21542;&#23545;&#20110;&#38598;&#20307;&#26234;&#33021;&#30340;&#20135;&#29983;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#21450;&#24590;&#26679;&#30340;&#20010;&#20307;&#26234;&#33021;&#26377;&#21033;&#20110;&#26356;&#22823;&#30340;&#38598;&#20307;&#26234;&#33021;&#12290;&#22312;Lotka-Volterra&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20010;&#20307;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#25504;&#39135;&#32773;&#30340;&#34892;&#20026;&#65292;&#26377;&#21033;&#20110;&#19982;&#20854;&#20182;&#31181;&#32676;&#20849;&#23384;&#65292;&#20294;&#22914;&#26524;&#29454;&#29289;&#21644;&#25504;&#39135;&#32773;&#37117;&#36275;&#22815;&#26234;&#33021;&#20197;&#25512;&#26029;&#24444;&#27492;&#30340;&#34892;&#20026;&#65292;&#20849;&#23384;&#23558;&#20276;&#38543;&#30528;&#20004;&#20010;&#31181;&#32676;&#30340;&#26080;&#38480;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2012.12689</link><description>&lt;p&gt;
&#20803;&#32032;&#36234;&#31528;&#65292;&#25972;&#20307;&#36234;&#32874;&#26126;&#12290;&#25110;&#32773;&#65292;&#21487;&#33021;&#24182;&#38750;&#22914;&#27492;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Less Intelligent the Elements, the More Intelligent the Whole. Or, Possibly Not?. (arXiv:2012.12689v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.12689
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20010;&#20307;&#26234;&#33021;&#26159;&#21542;&#23545;&#20110;&#38598;&#20307;&#26234;&#33021;&#30340;&#20135;&#29983;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#21450;&#24590;&#26679;&#30340;&#20010;&#20307;&#26234;&#33021;&#26377;&#21033;&#20110;&#26356;&#22823;&#30340;&#38598;&#20307;&#26234;&#33021;&#12290;&#22312;Lotka-Volterra&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20010;&#20307;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#25504;&#39135;&#32773;&#30340;&#34892;&#20026;&#65292;&#26377;&#21033;&#20110;&#19982;&#20854;&#20182;&#31181;&#32676;&#20849;&#23384;&#65292;&#20294;&#22914;&#26524;&#29454;&#29289;&#21644;&#25504;&#39135;&#32773;&#37117;&#36275;&#22815;&#26234;&#33021;&#20197;&#25512;&#26029;&#24444;&#27492;&#30340;&#34892;&#20026;&#65292;&#20849;&#23384;&#23558;&#20276;&#38543;&#30528;&#20004;&#20010;&#31181;&#32676;&#30340;&#26080;&#38480;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#33041;&#20013;&#30340;&#31070;&#32463;&#20803;&#19982;&#31038;&#20250;&#20013;&#30340;&#20154;&#31867;&#20043;&#38388;&#30340;&#21033;&#32500;&#22374;&#31867;&#27604;&#65292;&#38382;&#33258;&#24049;&#26159;&#21542;&#20010;&#20307;&#26234;&#33021;&#23545;&#20110;&#38598;&#20307;&#26234;&#33021;&#30340;&#20135;&#29983;&#26159;&#24517;&#35201;&#30340;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24590;&#26679;&#30340;&#20010;&#20307;&#26234;&#33021;&#26377;&#21033;&#20110;&#26356;&#22823;&#30340;&#38598;&#20307;&#26234;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36830;&#25509;&#20027;&#20041;&#35748;&#30693;&#31185;&#23398;&#12289;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#12289;&#32676;&#20307;&#24515;&#29702;&#23398;&#12289;&#32463;&#27982;&#23398;&#21644;&#29289;&#29702;&#23398;&#30340;&#19981;&#21516;&#27934;&#35265;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#27934;&#35265;&#24212;&#29992;&#20110;Lotka-Volterra&#27169;&#22411;&#20013;&#23548;&#33268;&#25504;&#39135;&#32773;&#21644;&#29454;&#29289;&#35201;&#20040;&#20849;&#23384;&#35201;&#20040;&#20840;&#29699;&#28781;&#32477;&#30340;&#26234;&#33021;&#31867;&#22411;&#21644;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#20960;&#20010;&#20010;&#20307;&#34892;&#20026; - &#23588;&#20854;&#26159;&#25504;&#39135;&#32773;&#30340;&#34892;&#20026; - &#26377;&#21033;&#20110;&#20849;&#23384;&#65292;&#26368;&#32456;&#22312;&#19968;&#20010;&#24179;&#34913;&#28857;&#21608;&#22260;&#20135;&#29983;&#38663;&#33633;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#65292;&#22914;&#26524;&#29454;&#29289;&#21644;&#25504;&#39135;&#32773;&#37117;&#36275;&#22815;&#26234;&#33021;&#20197;&#25512;&#26029;&#24444;&#27492;&#30340;&#34892;&#20026;&#65292;&#20849;&#23384;&#23601;&#20250;&#20276;&#38543;&#30528;&#20004;&#20010;&#31181;&#32676;&#30340;&#26080;&#38480;&#22686;&#38271;&#12290;&#30001;&#20110;Lotka-Volterra&#27169;&#22411;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore a Leviathan analogy between neurons in a brain and human beings in society, asking ourselves whether individual intelligence is necessary for collective intelligence to emerge and, most importantly, what sort of individual intelligence is conducive of greater collective intelligence. We first review disparate insights from connectionist cognitive science, agent-based modeling, group psychology, economics and physics. Subsequently, we apply these insights to the sort and degrees of intelligence that in the Lotka-Volterra model lead to either co-existence or global extinction of predators and preys.  We find several individual behaviors -- particularly of predators -- that are conducive to co-existence, eventually with oscillations around an equilibrium. However, we also find that if both preys and predators are sufficiently intelligent to extrapolate one other's behavior, co-existence comes along with indefinite growth of both populations. Since the Lotka-Volterra model is al
&lt;/p&gt;</description></item><item><title>MDP Playground&#26159;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#32500;&#24230;&#30340;&#38590;&#24230;&#25511;&#21046;&#26041;&#24335;&#65292;&#25361;&#25112;&#20195;&#29702;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;&#23427;&#25552;&#20379;&#20102;&#21442;&#25968;&#21270;&#30340;&#29609;&#20855;&#29615;&#22659;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#29615;&#22659;&#23545;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/1909.07750</link><description>&lt;p&gt;
MDP Playground: &#19968;&#31181;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#26512;&#21644;&#35843;&#35797;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
MDP Playground: An Analysis and Debug Testbed for Reinforcement Learning. (arXiv:1909.07750v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1909.07750
&lt;/p&gt;
&lt;p&gt;
MDP Playground&#26159;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#32500;&#24230;&#30340;&#38590;&#24230;&#25511;&#21046;&#26041;&#24335;&#65292;&#25361;&#25112;&#20195;&#29702;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;&#23427;&#25552;&#20379;&#20102;&#21442;&#25968;&#21270;&#30340;&#29609;&#20855;&#29615;&#22659;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#29615;&#22659;&#23545;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MDP Playground&#65292;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#26681;&#25454;&#38590;&#24230;&#30340;&#19981;&#21516;&#32500;&#24230;&#36827;&#34892;&#25511;&#21046;&#65292;&#20197;&#25361;&#25112;&#20195;&#29702;&#24182;&#22312;&#29609;&#20855;&#21644;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#33719;&#24471;&#19981;&#21516;&#31243;&#24230;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#24182;&#20801;&#35768;&#23545;&#21508;&#31181;&#32500;&#24230;&#36827;&#34892;&#25511;&#21046;&#65292;&#21253;&#25324;&#24310;&#36831;&#22870;&#21169;&#12289;&#24207;&#21015;&#38271;&#24230;&#12289;&#22870;&#21169;&#23494;&#24230;&#12289;&#38543;&#26426;&#24615;&#12289;&#22270;&#20687;&#34920;&#31034;&#12289;&#26080;&#20851;&#29305;&#24449;&#12289;&#26102;&#38388;&#21333;&#20301;&#12289;&#21160;&#20316;&#33539;&#22260;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;OpenAI Gym&#20013;&#21464;&#21270;&#36825;&#20123;&#32500;&#24230;&#26469;&#23450;&#20041;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#24555;&#36895;&#36816;&#34892;&#30340;&#29609;&#20855;&#29615;&#22659;&#38598;&#21512;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#36825;&#20123;&#29615;&#22659;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#20195;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;MDP Playground&#35774;&#35745;&#23454;&#39564;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#29609;&#20855;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23558;&#35768;&#22810;&#36825;&#20123;&#32500;&#24230;&#27880;&#20837;&#21040;&#20219;&#20309;Gym&#29615;&#22659;&#20013;&#30340;&#21253;&#35013;&#22120;&#12290;&#25105;&#20204;&#22312;Atari&#21644;Mujoco&#19978;&#20351;&#29992;&#36825;&#20123;&#21253;&#35013;&#22120;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#36825;&#20123;&#32500;&#24230;&#23545;&#27604;&#29609;&#20855;&#29615;&#22659;&#26356;&#22797;&#26434;&#30340;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MDP Playground, a testbed for Reinforcement Learning (RL) agents with dimensions of hardness that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in toy and complex RL environments. We consider and allow control over a wide variety of dimensions, including delayed rewards, sequence lengths, reward density, stochasticity, image representations, irrelevant features, time unit, action range and more. We define a parameterised collection of fast-to-run toy environments in OpenAI Gym by varying these dimensions and propose to use these to understand agents better. We then show how to design experiments using MDP Playground to gain insights on the toy environments. We also provide wrappers that can inject many of these dimensions into any Gym environment. We experiment with these wrappers on Atari and Mujoco to allow for understanding the effects of these dimensions on environments that are more complex than the toy envi
&lt;/p&gt;</description></item></channel></rss>