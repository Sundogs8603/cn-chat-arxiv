<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.18075</link><description>&lt;p&gt;
DUMA&#65306;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18075
&lt;/p&gt;
&lt;p&gt;
DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DUMA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29992;&#20110;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#30340;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20307;&#29616;&#20102;&#21452;&#37325;&#24605;&#32500;&#26426;&#21046;&#12290;&#24555;&#36895;&#24605;&#32771;&#27169;&#22411;&#20316;&#20026;&#20027;&#35201;&#25509;&#21475;&#29992;&#20110;&#22806;&#37096;&#20132;&#20114;&#21644;&#21021;&#22987;&#21709;&#24212;&#29983;&#25104;&#65292;&#26681;&#25454;&#23436;&#25972;&#21709;&#24212;&#30340;&#22797;&#26434;&#24615;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#35843;&#29992;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#12290;&#19968;&#26086;&#34987;&#35843;&#29992;&#65292;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#25509;&#31649;&#23545;&#35805;&#65292;&#22312;&#32454;&#33268;&#35268;&#21010;&#12289;&#25512;&#29702;&#21644;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#36827;&#34892;&#24037;&#20316;&#65292;&#25552;&#20379;&#32463;&#36807;&#20805;&#20998;&#20998;&#26512;&#30340;&#21709;&#24212;&#12290;&#36825;&#31181;&#21452;&#37325;&#24605;&#32500;&#37197;&#32622;&#20801;&#35768;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#25151;&#22320;&#20135;&#34892;&#19994;&#22312;&#32447;&#21672;&#35810;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. When invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. This dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. We have constructed a conversational agent to handle online inquiries in the real estate industry. The experiment proves that our method balances effectiveness and efficiency, an
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;AutoDiff&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#21644;&#29305;&#24449;&#38388;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#32479;&#35745;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.15479</link><description>&lt;p&gt;
AutoDiff:&#32467;&#21512;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing. (arXiv:2310.15479v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15479
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;AutoDiff&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#21644;&#29305;&#24449;&#38388;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#32479;&#35745;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#35768;&#22810;&#23376;&#39046;&#22495;&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#20027;&#35201;&#33539;&#24335;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#35328;&#27169;&#22411;&#25110;&#35821;&#38899;&#21512;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#21147;&#37327;&#26469;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#19968;&#30452;&#26159;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#21512;&#25104;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#34920;&#26684;&#22312;&#32479;&#35745;&#19978;&#19982;&#30495;&#23454;&#25968;&#25454;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#22312;15&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#28789;&#27963;&#22320;&#25429;&#25417;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22914;&#33509;&#25509;&#32435;&#20102;&#35770;&#25991;&#65292;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#26681;&#25454;&#35201;&#27714;&#25552;&#20379;&#65292;&#24182;&#19988;&#23558;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model has become a main paradigm for synthetic data generation in many subfields of modern machine learning, including computer vision, language model, or speech synthesis. In this paper, we leverage the power of diffusion model for generating synthetic tabular data. The heterogeneous features in tabular data have been main obstacles in tabular data synthesis, and we tackle this problem by employing the auto-encoder architecture. When compared with the state-of-the-art tabular synthesizers, the resulting synthetic tables from our model show nice statistical fidelities to the real data, and perform well in downstream tasks for machine learning utilities. We conducted the experiments over 15 publicly available datasets. Notably, our model adeptly captures the correlations among features, which has been a long-standing challenge in tabular data synthesis. Our code is available upon request and will be publicly released if paper is accepted.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#39640;&#32500;&#24230;&#20302;&#26679;&#26412;(HDLSS)&#20998;&#31867;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30456;&#20284;&#24230;&#30340;&#23398;&#20064;&#39044;&#35745;&#31639;SVM&#26680;&#26041;&#27861;(RFSVM)&#65292;&#36890;&#36807;&#22312;40&#20010;&#20844;&#20849;HDLSS&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;HDLSS&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24182;&#19988;&#20445;&#25345;&#20102;&#38750;&#24120;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14710</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#20302;&#26679;&#26412;&#20998;&#31867;&#30340;&#38543;&#26426;&#26862;&#26519;&#24046;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Random Forest Dissimilarity for High-Dimension Low Sample Size Classification. (arXiv:2310.14710v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14710
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#32500;&#24230;&#20302;&#26679;&#26412;(HDLSS)&#20998;&#31867;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30456;&#20284;&#24230;&#30340;&#23398;&#20064;&#39044;&#35745;&#31639;SVM&#26680;&#26041;&#27861;(RFSVM)&#65292;&#36890;&#36807;&#22312;40&#20010;&#20844;&#20849;HDLSS&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;HDLSS&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24182;&#19988;&#20445;&#25345;&#20102;&#38750;&#24120;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#24230;&#20302;&#26679;&#26412;(HDLSS)&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#20174;&#21307;&#23398;&#24433;&#20687;&#21040;&#25991;&#26412;&#22788;&#29702;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#26080;&#27861;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#26368;&#20339;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#24322;&#24615;&#30340;&#22810;&#35270;&#35282;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;&#38543;&#26426;&#26862;&#26519;&#24046;&#24322;&#24615;(RFD)&#65292;&#35813;&#26041;&#27861;&#22312;&#36825;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#21407;&#21017;&#36716;&#21270;&#20026;&#35299;&#20915;HDLSS&#20998;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#30456;&#20284;&#24230;&#20316;&#20026;&#23398;&#20064;&#30340;&#39044;&#35745;&#31639;SVM&#26680;(RFSVM)&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#23398;&#20064;&#30456;&#20284;&#24230;&#24230;&#37327;&#22312;&#36825;&#31181;&#20998;&#31867;&#19978;&#29305;&#21035;&#36866;&#29992;&#21644;&#20934;&#30830;&#12290;&#36890;&#36807;&#23545;40&#20010;&#20844;&#20849;HDLSS&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#37197;&#21512;&#20005;&#26684;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;RFSVM&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;HDLSS&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#21516;&#26102;&#38750;&#24120;&#36830;&#36143;&#12290;
&lt;/p&gt;
&lt;p&gt;
High dimension, low sample size (HDLSS) problems are numerous among real-world applications of machine learning. From medical images to text processing, traditional machine learning algorithms are usually unsuccessful in learning the best possible concept from such data. In a previous work, we proposed a dissimilarity-based approach for multi-view classification, the Random Forest Dissimilarity (RFD), that perfoms state-of-the-art results for such problems. In this work, we transpose the core principle of this approach to solving HDLSS classification problems, by using the RF similarity measure as a learned precomputed SVM kernel (RFSVM). We show that such a learned similarity measure is particularly suited and accurate for this classification context. Experiments conducted on 40 public HDLSS classification datasets, supported by rigorous statistical analyses, show that the RFSVM method outperforms existing methods for the majority of HDLSS problems and remains at the same time very co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.16397</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#20027;&#21160;&#20132;&#20114;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#26694;&#26550;&#65292;&#22240;&#27492;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#26368;&#36817;Transformers&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#23558;&#31163;&#32447;RL&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#36825;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#36807;&#20110;&#20048;&#35266;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#30456;&#21516;&#30340;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#30456;&#21516;&#30340;&#21160;&#20316;&#19968;&#33268;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#36716;&#25442;&#27169;&#22411;&#25110;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UNREST&#36890;&#36807;&#36716;&#25442;&#19982;&#22238;&#25253;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#12290;&#36890;&#36807;&#21457;&#29616;&#39550;&#39542;&#29615;&#22659;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#32047;&#31215;&#8221;&#21644;&#8220;&#26102;&#38388;&#23616;&#37096;&#24615;&#8221;&#29305;&#24615;&#65292;UNREST&#23558;&#20915;&#31574;Transformer&#20013;&#30340;&#20840;&#23616;&#22238;&#25253;&#26367;&#25442;&#20026;&#36739;&#23569;&#30340;&#37096;&#20998;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) has emerged as a promising framework for learning policies without active interactions, making it especially appealing for autonomous driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which performs well in long-horizon tasks. However, they are overly optimistic in stochastic environments with incorrect assumptions that the same goal can be consistently achieved by identical actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates state uncertainties by the conditional mutual information between transitions and returns, and segments sequences accordingly. Discovering the `uncertainty accumulation' and `temporal locality' properties of driving environments, UNREST replaces the global returns in decision transformers with less 
&lt;/p&gt;</description></item><item><title>VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06595</link><description>&lt;p&gt;
VisIT-Bench: &#19968;&#20010;&#21463;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#21551;&#21457;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#31034;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06595
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VisIT-Bench&#65288;Visual InsTruction Benchmark&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20215;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#31034;&#36981;&#24490;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31574;&#21010;&#20102;70&#20010;&#8220;&#25351;&#31034;&#23478;&#26063;&#8221;&#65292;&#25105;&#20204;&#35748;&#20026;&#25351;&#31034;&#35843;&#20248;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#23478;&#26063;&#12290;&#20219;&#21153;&#19981;&#20165;&#38480;&#20110;VQAv2&#21644;COCO&#31561;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20174;&#22522;&#26412;&#35782;&#21035;&#21040;&#28216;&#25103;&#29609;&#27861;&#21644;&#21019;&#36896;&#24615;&#29983;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#31574;&#21010;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;592&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#27599;&#20010;&#26597;&#35810;&#37117;&#24102;&#26377;&#19968;&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#31034;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#12290;&#36825;&#20123;&#25551;&#36848;&#23637;&#29616;&#20102;&#29305;&#23450;&#25351;&#31034;&#22240;&#32032;&#65292;&#20363;&#22914;&#23545;&#20110;&#35810;&#38382;&#24215;&#38754;&#23545;&#20110;&#36718;&#26885;&#29992;&#25143;&#30340;&#26131;&#35775;&#38382;&#24615;&#30340;&#25351;&#31034;&#65292;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#25551;&#36848;&#20102;&#26012;&#22369;/&#28508;&#22312;&#38556;&#30861;&#29289;&#12290;&#36825;&#20123;&#25551;&#36848;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#65306;1&#65289;&#25910;&#38598;&#27599;&#20010;&#23454;&#20363;&#30340;&#20154;&#24037;&#39564;&#35777;&#30340;&#21442;&#32771;&#36755;&#20986;&#65307;2&#65289;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20505;&#36873;&#22810;&#27169;&#24577;&#29983;&#25104;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22235;&#31181;&#35299;&#37322;&#24615;&#26041;&#27861;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#28909;&#22270;&#20998;&#26512;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#38024;&#23545;&#29305;&#23450;&#30149;&#29702;&#31867;&#21035;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.08003</link><description>&lt;p&gt;
SHAMSUL: &#21033;&#29992;&#26412;&#22320;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#21516;&#26102;&#28909;&#22270;&#20998;&#26512;&#20197;&#30740;&#31350;&#21307;&#23398;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods. (arXiv:2307.08003v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22235;&#31181;&#35299;&#37322;&#24615;&#26041;&#27861;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#28909;&#22270;&#20998;&#26512;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#38024;&#23545;&#29305;&#23450;&#30149;&#29702;&#31867;&#21035;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#24050;&#25104;&#20026;&#21307;&#30103;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#19968;&#20010;&#28909;&#38376;&#35758;&#39064;&#12290;&#36825;&#31181;&#20851;&#27880;&#26469;&#28304;&#20110;&#23545;&#36879;&#26126;&#24230;&#12289;&#27861;&#24459;&#21644;&#20262;&#29702;&#32771;&#34385;&#20197;&#21450;&#36825;&#20123;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#29983;&#25104;&#30340;&#39044;&#27979;&#30340;&#21307;&#23398;&#24847;&#20041;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22235;&#31181;&#24050;&#24314;&#31435;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;: &#23616;&#37096;&#21487;&#35299;&#37322;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;(LIME)&#12289;Shapley&#21152;&#24615;&#35299;&#37322;(SHAP)&#12289;&#26799;&#24230;&#21152;&#26435;&#31867;&#21035;&#28608;&#27963;&#26144;&#23556;(Grad-CAM)&#21644;&#23618;&#20869;&#30456;&#20851;&#20256;&#25773;(LRP)&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#26631;&#31614;&#22810;&#31867;&#33016;&#37096;&#25918;&#23556;&#23398;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#37322;&#19982;&#29305;&#23450;&#30149;&#29702;&#31867;&#21035;&#30456;&#20851;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#39044;&#27979;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#25552;&#20379;&#20102;&#20840;&#38754;&#21644;&#20844;&#27491;&#30340;&#35780;&#20272;&#65292;
&lt;/p&gt;
&lt;p&gt;
The interpretability of deep neural networks has become a subject of great interest within the medical and healthcare domain. This attention stems from concerns regarding transparency, legal and ethical considerations, and the medical significance of predictions generated by these deep neural networks in clinical decision support systems. To address this matter, our study delves into the application of four well-established interpretability methods: Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations (SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise Relevance Propagation (LRP). Leveraging the approach of transfer learning with a multi-label-multi-class chest radiography dataset, we aim to interpret predictions pertaining to specific pathology classes. Our analysis encompasses both single-label and multi-label predictions, providing a comprehensive and unbiased assessment through quantitative and qualitative investigations, w
&lt;/p&gt;</description></item><item><title>DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16058</link><description>&lt;p&gt;
DUET: 2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16058
&lt;/p&gt;
&lt;p&gt;
DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;(MSSL)&#22522;&#20110;&#23398;&#20064;&#30456;&#23545;&#20110;&#19968;&#32452;&#36755;&#20837;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#21464;&#24615;&#20174;&#34920;&#31034;&#20013;&#37096;&#20998;&#25110;&#23436;&#20840;&#31227;&#38500;&#19982;&#21464;&#25442;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#38656;&#35201;&#36825;&#20123;&#20449;&#24687;&#30340;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#36896;&#25104;&#25439;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;2D&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#34920;&#31034;&#65292;&#31216;&#20026;DUET&#65292;&#23427;&#20204;&#26159;&#20197;&#30697;&#38453;&#32467;&#26500;&#32452;&#32455;&#30340;2D&#34920;&#31034;&#65292;&#24182;&#19988;&#23545;&#20316;&#29992;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#25442;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;DUET&#34920;&#31034;&#20445;&#30041;&#26377;&#20851;&#36755;&#20837;&#21464;&#25442;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#34920;&#36798;&#33021;&#21147;&#12290;&#19982;SimCLR&#65288;Chen&#31561;&#65292;2020&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#19981;&#21464;&#24615;&#65289;&#21644;ESSL&#65288;Dangovski&#31561;&#65292;2022&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#31561;&#21464;&#24615;&#65289;&#30456;&#27604;&#65292;DUET&#34920;&#31034;&#30340;&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#24615;&#20351;&#24471;&#29983;&#25104;&#20855;&#26377;&#26356;&#20302;&#30340;&#37325;&#24314;&#35823;&#24046;&#30340;&#21487;&#25511;&#24615;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;SimCLR&#25110;ESSL&#21017;&#26080;&#27861;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;DUET&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiview Self-Supervised Learning (MSSL) is based on learning invariances with respect to a set of input transformations. However, invariance partially or totally removes transformation-related information from the representations, which might harm performance for specific downstream tasks that require such information. We propose 2D strUctured and EquivarianT representations (coined DUET), which are 2d representations organized in a matrix structure, and equivariant with respect to transformations acting on the input data. DUET representations maintain information about an input transformation, while remaining semantically expressive. Compared to SimCLR (Chen et al., 2020) (unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured and equivariant), the structured and equivariant nature of DUET representations enables controlled generation with lower reconstruction error, while controllability is not possible with SimCLR or ESSL. DUET also achieves higher accuracy fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#21629;&#20196;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#36896;&#24615;&#22320;&#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.09802</link><description>&lt;p&gt;
Sasha: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Sasha: creative goal-oriented reasoning in smart homes with large language models. (arXiv:2305.09802v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#21629;&#20196;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#36896;&#24615;&#22320;&#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#30340;&#20351;&#29992;&#32773;&#19982;&#35774;&#22791;&#30340;&#20132;&#20114;&#37117;&#26377;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#30340;&#23478;&#24237;&#21161;&#25163;&#33021;&#22815;&#36731;&#26494;&#22320;&#23454;&#29616;&#26126;&#30830;&#30340;&#30446;&#26631;&#65292;&#20363;&#22914;"&#25171;&#24320;&#28783;"&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#33258;&#28982;&#30340;&#20132;&#27969;&#20013;&#65292;&#20154;&#20204;&#24448;&#24448;&#20250;&#25551;&#36848;&#38544;&#21547;&#30340;&#30446;&#26631;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#35753;&#21035;&#20154;"&#35753;&#25151;&#38388;&#21464;&#24471;&#33298;&#36866;"&#32780;&#19981;&#26159;&#25551;&#36848;&#20855;&#20307;&#30340;&#27493;&#39588;&#12290;&#24403;&#21069;&#30340;&#31995;&#32479;&#24456;&#38590;&#35299;&#20915;&#36825;&#31181;&#27495;&#20041;&#65292;&#22240;&#20026;&#38656;&#35201;&#23558;&#27169;&#31946;&#30340;&#24847;&#22270;&#19982;&#20855;&#20307;&#30340;&#35774;&#22791;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20174;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#26041;&#24335;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#25511;&#21046;&#35774;&#22791;&#24182;&#21019;&#24314;&#33258;&#21160;&#21270;&#31243;&#24207;&#20197;&#28385;&#36275;&#29992;&#25143;&#21629;&#20196;&#30340;&#38544;&#21547;&#30446;&#26631;&#12290;&#22312;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#21487;&#20197;&#21019;&#36896;&#24615;&#22320;&#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#38477;&#20302;&#20854;&#26377;&#29992;&#24615;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#20351;&#29992;Sasha&#35299;&#20915;&#20102;&#36825;&#20123;&#24046;&#36317;&#65306;&#36825;&#26159;&#19968;&#20010;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;&#28789;&#27963;&#35299;&#37322;&#29992;&#25143;&#21629;&#20196;&#21644;&#35774;&#22791;&#25511;&#21046;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every smart home user interaction has an explicit or implicit goal. Existing home assistants easily achieve explicit goals, e.g., "turn on the light". In more natural communication, however, humans tend to describe implicit goals. We can, for example, ask someone to "make it cozy" rather than describe the specific steps involved. Current systems struggle with this ambiguity since it requires them to relate vague intent to specific devices. We approach this problem of flexibly achieving user goals from the perspective of general-purpose large language models (LLMs) trained on gigantic corpora and adapted to downstream tasks with remarkable flexibility. We explore the use of LLMs for controlling devices and creating automation routines to meet the implicit goals of user commands. In a user-focused study, we find that LLMs can reason creatively to achieve challenging goals, while also revealing gaps that diminish their usefulness. We address these gaps with Sasha: a system for creative, g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17491</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35745;&#31639;&#26426;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#36890;&#29992;&#20219;&#21153;&#30340;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#21644;&#21327;&#21161;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#35299;&#20915;&#26032;&#30340;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#31034;&#33539;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#26032;&#20219;&#21153;&#26469;&#35828;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#65288;RCI&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#24182;&#22312;&#25209;&#35780;&#21644;&#25913;&#36827;&#36755;&#20986;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;RCI&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;RCI&#26041;&#27861;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#20165;&#26377;&#30340;&#23569;&#25968;&#31034;&#33539;&#65292;&#19982;&#26368;&#26032;&#30340;SL+RL&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.16047</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#25506;&#32034;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#25972;&#20010;&#20248;&#31168;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#39046;&#22495;&#19987;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#29983;&#25104;&#21333;&#20010;&#27169;&#22411;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#19981;&#21033;&#20110;&#27492;&#31867;&#20132;&#20114;&#12290;&#36817;&#20284;&#21644;&#25506;&#32034;Rashomon&#38598;&#65292;&#21363;&#25152;&#26377;&#36817;&#20046;&#26368;&#20248;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#25552;&#20379;&#29992;&#25143;&#21487;&#25628;&#32034;&#30340;&#31354;&#38388;&#21253;&#21547;&#22810;&#26679;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#23454;&#38469;&#25361;&#25112;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#20174;&#20013;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#36817;&#20284;&#20855;&#26377;&#22266;&#23450;&#25903;&#25345;&#38598;&#30340;GAMs&#30340;Rashomon&#38598;&#30340;&#26925;&#29699;&#24418;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26925;&#29699;&#24418;&#36817;&#20284;&#20102;&#35768;&#22810;&#19981;&#21516;&#25903;&#25345;&#38598;&#30340;Rashomon&#38598;&#12290;&#36817;&#20284;&#30340;Rashomon&#38598;&#20026;&#35299;&#20915;&#23454;&#38469;&#25361;&#25112;&#65292;&#20363;&#22914;&#65288;1&#65289;&#30740;&#31350;&#27169;&#22411;&#31867;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#65307;&#65288;2&#65289;&#22312;&#29992;&#25143;&#25351;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;&#26597;&#25214;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
&lt;/p&gt;</description></item></channel></rss>