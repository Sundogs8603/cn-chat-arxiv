<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ASNR-MICCAI&#33041;&#32959;&#30244;&#20998;&#21106;&#25361;&#25112;2023&#23558;&#25552;&#20379;&#19968;&#20010;&#36866;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#39045;&#20869;&#33041;&#33180;&#30244;&#30340;&#26368;&#20808;&#36827;&#33258;&#21160;&#21270;&#39045;&#20869;&#33041;&#33180;&#30244;&#20998;&#21106;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.07642</link><description>&lt;p&gt;
ASNR-MICCAI&#33041;&#32959;&#30244;&#20998;&#21106;&#25361;&#25112;2023&#65306;&#39045;&#20869;&#33041;&#33180;&#30244;
&lt;/p&gt;
&lt;p&gt;
The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma. (arXiv:2305.07642v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07642
&lt;/p&gt;
&lt;p&gt;
ASNR-MICCAI&#33041;&#32959;&#30244;&#20998;&#21106;&#25361;&#25112;2023&#23558;&#25552;&#20379;&#19968;&#20010;&#36866;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#39045;&#20869;&#33041;&#33180;&#30244;&#30340;&#26368;&#20808;&#36827;&#33258;&#21160;&#21270;&#39045;&#20869;&#33041;&#33180;&#30244;&#20998;&#21106;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#33180;&#30244;&#26159;&#25104;&#20154;&#39045;&#20869;&#26368;&#24120;&#35265;&#30340;&#21407;&#21457;&#24615;&#32959;&#30244;&#65292;&#21487;&#33021;&#19982;&#37325;&#22823;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#26377;&#20851;&#12290;&#25918;&#23556;&#31185;&#21307;&#29983;&#12289;&#31070;&#32463;&#22806;&#31185;&#21307;&#29983;&#12289;&#31070;&#32463;&#32959;&#30244;&#23398;&#23478;&#21644;&#25918;&#23556;&#32959;&#30244;&#31185;&#21307;&#29983;&#20381;&#38752;&#22810;&#21442;&#25968;MRI&#65288;mpMRI&#65289;&#36827;&#34892;&#35786;&#26029;&#12289;&#27835;&#30103;&#35268;&#21010;&#21644;&#38271;&#26399;&#27835;&#30103;&#30417;&#27979;&#65307;&#28982;&#32780;&#65292;&#32570;&#20047;&#33258;&#21160;&#21270;&#12289;&#23458;&#35266;&#21270;&#21644;&#23450;&#37327;&#21270;&#30340;&#24037;&#20855;&#26469;&#23545;mpMRI&#20013;&#30340;&#33041;&#33180;&#30244;&#36827;&#34892;&#38750;&#20405;&#20837;&#24615;&#35780;&#20272;&#12290;BraTS&#33041;&#33180;&#30244;2023&#25361;&#25112;&#23558;&#25552;&#20379;&#19968;&#20010;&#31038;&#21306;&#26631;&#20934;&#21644;&#22522;&#20110;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#33041;&#33180;&#30244;mpMRI&#25968;&#25454;&#38598;&#30340;&#26368;&#20808;&#36827;&#33258;&#21160;&#21270;&#39045;&#20869;&#33041;&#33180;&#30244;&#20998;&#21106;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25361;&#25112;&#21442;&#36187;&#32773;&#23558;&#24320;&#21457;&#33258;&#21160;&#21270;&#20998;&#21106;&#27169;&#22411;&#65292;&#39044;&#27979;MRI&#19978;&#30340;&#19977;&#20010;&#19981;&#21516;&#30340;&#33041;&#33180;&#30244;&#20122;&#21306;&#22495;&#65292;&#21253;&#25324;&#22686;&#24378;&#32959;&#30244;&#12289;&#38750;&#22686;&#24378;&#32959;&#30244;&#26680;&#24515;&#21644;&#21608;&#22260;&#26080;&#22686;&#24378;T2/FLAIR&#39640;&#20449;&#21495;&#21306;&#12290;&#27169;&#22411;&#23558;&#20351;&#29992;&#26631;&#20934;&#21270;&#25351;&#26631;&#22312;&#21333;&#29420;&#30340;&#39564;&#35777;&#21644;&#20445;&#30041;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meningiomas are the most common primary intracranial tumor in adults and can be associated with significant morbidity and mortality. Radiologists, neurosurgeons, neuro-oncologists, and radiation oncologists rely on multiparametric MRI (mpMRI) for diagnosis, treatment planning, and longitudinal treatment monitoring; yet automated, objective, and quantitative tools for non-invasive assessment of meningiomas on mpMRI are lacking. The BraTS meningioma 2023 challenge will provide a community standard and benchmark for state-of-the-art automated intracranial meningioma segmentation models based on the largest expert annotated multilabel meningioma mpMRI dataset to date. Challenge competitors will develop automated segmentation models to predict three distinct meningioma sub-regions on MRI including enhancing tumor, non-enhancing tumor core, and surrounding nonenhancing T2/FLAIR hyperintensity. Models will be evaluated on separate validation and held-out test datasets using standardized metri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#39033;&#30446;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#38646;&#26679;&#26412;&#39033;&#30446;&#25512;&#33616;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;&#22235;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#36866;&#24212;&#23618;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#27169;&#22411;&#24494;&#35843;&#21040;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.07633</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20219;&#21153;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#22522;&#20110;&#39033;&#30446;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training. (arXiv:2305.07633v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#39033;&#30446;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#38646;&#26679;&#26412;&#39033;&#30446;&#25512;&#33616;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;&#22235;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#36866;&#24212;&#23618;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#27169;&#22411;&#24494;&#35843;&#21040;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#38646;&#26679;&#26412;&#39033;&#30446;&#65288;&#21363;&#22312;&#35757;&#32451;&#38454;&#27573;&#27809;&#26377;&#19982;&#29992;&#25143;&#36827;&#34892;&#36807;&#21382;&#21490;&#20114;&#21160;&#30340;&#39033;&#30446;&#65289;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#25552;&#21462;&#36890;&#29992;&#39033;&#30446;&#34920;&#31034;&#65292;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#39033;&#30446;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#20174;PLMs&#20013;&#25552;&#28860;&#20986;&#39033;&#30446;&#29305;&#24449;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#39033;&#30446;&#25512;&#33616;&#65288;ZSIR&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#39044;&#35757;&#32451;PKG&#30340;&#19977;&#20010;&#25361;&#25112;&#65292;&#21363;PKG&#20013;&#30340;&#22810;&#31867;&#22411;&#20851;&#31995;&#65292;&#39033;&#30446;&#36890;&#29992;&#20449;&#24687;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#20197;&#21450;&#20174;PKG&#21040;&#19979;&#28216;ZSIR&#20219;&#21153;&#30340;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#22235;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#36866;&#24212;&#65288;ToA&#65289;&#23618;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#23545;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#24471;ToA&#23618;&#36866;&#24212;&#20110;ZSIR&#20219;&#21153;&#12290;&#22312;18&#20010;&#24066;&#22330;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage. Though recent works extract universal item representation via pre-trained language models (PLMs), they ignore the crucial item relationships. This paper presents a novel paradigm for the Zero-Shot Item-based Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph (PKG) to refine the item features from PLMs. We identify three challenges for pre-training PKG, which are multi-type relations in PKG, semantic divergence between item generic information and relations and domain discrepancy from PKG to downstream ZSIR task. We address the challenges by proposing four pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover, this paper discusses how to fine-tune the model on new recommendation task such that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18 markets dataset ar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#20010;&#24615;&#21270;&#31038;&#20132;&#26426;&#22120;&#20154;&#29992;&#20110;&#30417;&#27979;&#21644;&#36741;&#23548;&#20013;&#39118;&#21518;&#24674;&#22797;&#36816;&#21160;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#27169;&#22411;&#30456;&#32467;&#21512;&#30417;&#27979;&#21644;&#35780;&#20272;&#24739;&#32773;&#24247;&#22797;&#38203;&#28860;&#24182;&#25552;&#20379;&#23454;&#26102;&#20010;&#24615;&#21270;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2305.07632</link><description>&lt;p&gt;
&#35774;&#35745;&#12289;&#24320;&#21457;&#21644;&#35780;&#20272;&#20132;&#20114;&#24335;&#20010;&#24615;&#21270;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#20197;&#30417;&#27979;&#21644;&#36741;&#23548;&#20013;&#39118;&#21518;&#24674;&#22797;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Design, Development, and Evaluation of an Interactive Personalized Social Robot to Monitor and Coach Post-Stroke Rehabilitation Exercises. (arXiv:2305.07632v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#20010;&#24615;&#21270;&#31038;&#20132;&#26426;&#22120;&#20154;&#29992;&#20110;&#30417;&#27979;&#21644;&#36741;&#23548;&#20013;&#39118;&#21518;&#24674;&#22797;&#36816;&#21160;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#27169;&#22411;&#30456;&#32467;&#21512;&#30417;&#27979;&#21644;&#35780;&#20272;&#24739;&#32773;&#24247;&#22797;&#38203;&#28860;&#24182;&#25552;&#20379;&#23454;&#26102;&#20010;&#24615;&#21270;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#36741;&#21161;&#26426;&#22120;&#20154;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25506;&#32034;&#29992;&#20110;&#25913;&#21892;&#32769;&#24180;&#20154;&#21644;&#27531;&#30142;&#20154;&#21442;&#19982;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#38203;&#28860;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20154;&#20204;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;&#36523;&#20307;&#29366;&#20917;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#38203;&#28860;&#36741;&#23548;&#31995;&#32479;&#22312;&#22238;&#39304;&#26041;&#38754;&#37117;&#20351;&#29992;&#36890;&#29992;&#30340;&#39044;&#23450;&#20041;&#30340;&#22238;&#39304;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#37096;&#32626;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#27835;&#30103;&#24072;&#21644;&#20013;&#39118;&#24184;&#23384;&#32773;&#30340;&#35775;&#35848;&#26469;&#35774;&#35745;&#12289;&#24320;&#21457;&#21644;&#35780;&#20272;&#19968;&#20010;&#20010;&#24615;&#21270;&#24247;&#22797;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#19982;&#27835;&#30103;&#24072;&#36827;&#34892;&#35775;&#35848;&#65292;&#35774;&#35745;&#20102;&#31995;&#32479;&#29992;&#25143;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#31038;&#20132;&#26426;&#22120;&#20154;&#38203;&#28860;&#36741;&#23548;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#30417;&#27979;&#21644;&#35780;&#20272;&#24739;&#32773;&#30340;&#24247;&#22797;&#38203;&#28860;&#65292;&#24182;&#21487;&#20197;&#26681;&#25454;&#20010;&#20307;&#24739;&#32773;&#30340;&#25968;&#25454;&#35843;&#25972;&#65292;&#20026;&#27599;&#20010;&#24739;&#32773;&#29983;&#25104;&#23454;&#26102;&#30340;&#20010;&#24615;&#21270;&#30699;&#27491;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Socially assistive robots are increasingly being explored to improve the engagement of older adults and people with disability in health and well-being-related exercises. However, even if people have various physical conditions, most prior work on social robot exercise coaching systems has utilized generic, predefined feedback. The deployment of these systems still remains a challenge. In this paper, we present our work of iteratively engaging therapists and post-stroke survivors to design, develop, and evaluate a social robot exercise coaching system for personalized rehabilitation. Through interviews with therapists, we designed how this system interacts with the user and then developed an interactive social robot exercise coaching system. This system integrates a neural network model with a rule-based model to automatically monitor and assess patients' rehabilitation exercises and can be tuned with individual patient's data to generate real-time, personalized corrective feedback for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07622</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;LMMs&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PALR&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#29289;&#21697;&#20114;&#21160;&#20316;&#20026;&#20505;&#36873;&#26816;&#32034;&#30340;&#25351;&#23548;&#65292;&#28982;&#21518;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#25490;&#24207;&#27169;&#22411;&#29983;&#25104;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;NP-hard&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#31163;&#25955;&#22270;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#21516;&#26102;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20855;&#26377;&#23545;&#39044;&#27979;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.07617</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;NP-hard&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#31163;&#25955;&#22270;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#21516;&#26102;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20855;&#26377;&#23545;&#39044;&#27979;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#31163;&#25955;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#28151;&#21512;&#30340;&#19981;&#26029;&#25506;&#32034;&#20013;&#65292;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#23545;&#31070;&#32463;&#32467;&#26500;&#20855;&#22791;&#20174;&#33258;&#28982;&#36755;&#20837;&#20013;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#31163;&#25955;&#25512;&#29702;&#25110;&#20248;&#21270;&#38382;&#39064;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32467;&#26500;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#23398;&#20064;&#34987;&#34920;&#31034;&#20026;&#31163;&#25955;&#22270;&#27169;&#22411;&#30340; NP-hard &#25512;&#29702;&#38382;&#39064;&#30340;&#32422;&#26463;&#21644;&#26631;&#20934;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#35299;&#20915;&#20102; Besag &#30340;&#20266;&#23545;&#25968;&#20284;&#28982;&#30340;&#20027;&#35201;&#38480;&#21046;&#20043;&#19968;&#65292;&#33021;&#22815;&#23398;&#20064;&#39640;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#36755;&#20837;&#20013;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915; NP-hard &#25512;&#29702;&#38382;&#39064;&#65292;&#22914;&#31526;&#21495;&#12289;&#35270;&#35273;&#25110;&#22810;&#35299;&#25968;&#25968;&#29420;&#38382;&#39064;&#65292;&#20197;&#21450;&#34507;&#30333;&#36136;&#35774;&#35745;&#38382;&#39064;&#30340;&#33021;&#37327;&#20248;&#21270;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#23545;&#39044;&#27979;&#30340; \textit{a posteriori} &#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag's pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and \textit{a posteriori} control over predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spider GAN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23547;&#25214;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21451;&#22909;&#37051;&#23621;&#26469;&#25552;&#39640;GAN&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#21152;&#36895;&#25910;&#25947;&#65292;&#21363;&#20351;&#26159;&#19981;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#20063;&#21487;&#20197;&#21457;&#29616;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.07613</link><description>&lt;p&gt;
Spider GAN:&#21033;&#29992;&#21451;&#22909;&#37051;&#23621;&#21152;&#36895;GAN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training. (arXiv:2305.07613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spider GAN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23547;&#25214;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21451;&#22909;&#37051;&#23621;&#26469;&#25552;&#39640;GAN&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#21152;&#36895;&#25910;&#25947;&#65292;&#21363;&#20351;&#26159;&#19981;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#20063;&#21487;&#20197;&#21457;&#29616;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GAN&#30340;&#35757;&#32451;&#26159;&#20010;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Spider GAN&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#32467;&#26500;&#30340;&#29305;&#28857;&#20248;&#21270;&#29983;&#25104;&#22120;&#30340;&#36716;&#25442;&#65292;&#36890;&#36807;&#23450;&#20041;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#24335;&#65292;&#21363;&#26377;&#31526;&#21495;&#21551;&#21160;&#36317;&#31163;&#65288;SID&#65289;&#65292;&#20351;&#20854;&#26356;&#39640;&#25928;&#22320;&#23547;&#25214;&#21451;&#22909;&#37051;&#23621;&#65292;&#32467;&#26524;&#23548;&#33268;&#26356;&#24555;&#30340;&#25910;&#25947;&#65292;&#21363;&#20351;&#22312;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#20063;&#21487;&#20197;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training Generative adversarial networks (GANs) stably is a challenging task. The generator in GANs transform noise vectors, typically Gaussian distributed, into realistic data such as images. In this paper, we propose a novel approach for training GANs with images as inputs, but without enforcing any pairwise constraints. The intuition is that images are more structured than noise, which the generator can leverage to learn a more robust transformation. The process can be made efficient by identifying closely related datasets, or a ``friendly neighborhood'' of the target distribution, inspiring the moniker, Spider GAN. To define friendly neighborhoods leveraging proximity between datasets, we propose a new measure called the signed inception distance (SID), inspired by the polyharmonic kernel. We show that the Spider GAN formulation results in faster convergence, as the generator can discover correspondence even between seemingly unrelated datasets, for instance, between Tiny-ImageNet 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;C-LLM&#65289;&#65292;&#20197;&#21450;&#23427;&#22312;&#22797;&#26434;&#23398;&#29983;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#21644;&#23457;&#26597;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.07605</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#25945;&#32946;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative AI: Implications and Applications for Education. (arXiv:2305.07605v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;C-LLM&#65289;&#65292;&#20197;&#21450;&#23427;&#22312;&#22797;&#26434;&#23398;&#29983;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#21644;&#23457;&#26597;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2022&#24180;11&#26376;ChatGPT&#30340;&#25512;&#20986;&#24341;&#21457;&#20102;&#19968;&#20123;&#25945;&#32946;&#24037;&#20316;&#32773;&#30340;&#24656;&#24908;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#20854;&#20182;&#20154;&#30340;&#28909;&#24773;&#12290;&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#36825;&#20010;&#22823;&#20254;&#19979;&#65292;ChatGPT&#26159;&#19968;&#31995;&#21015;&#25216;&#26415;&#30340;&#20363;&#23376;&#65292;&#29992;&#20110;&#25552;&#20379;&#35745;&#31639;&#26426;&#29983;&#25104;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#20854;&#20182;&#25968;&#23383;&#21270;&#23186;&#20307;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20043;&#19968;&#8212;&#8212;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(C-LLM)&#65292;&#20197;&#21450;&#20854;&#22312;&#22797;&#26434;&#23398;&#29983;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#21644;&#23457;&#26597;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22312;&#35752;&#35770;&#20013;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20869;&#22312;&#38480;&#21046;&#65292;&#21363;&#32465;&#23450;&#20110;&#35821;&#26009;&#24211;&#21450;&#20854;&#36890;&#36807;&#20108;&#36827;&#21046;&#34920;&#31034;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#22312;&#36825;&#20123;&#38480;&#21046;&#20043;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#20013;&#26032;&#20852;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#20195;&#30721;&#37325;&#26500;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#32806;&#21512;&#21644;&#20869;&#32858;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#28436;&#31034;&#21644;&#23454;&#29616;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.07594</link><description>&lt;p&gt;
Opti Code Pro&#65306;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#20195;&#30721;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Opti Code Pro: A Heuristic Search-based Approach to Code Refactoring. (arXiv:2305.07594v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#20195;&#30721;&#37325;&#26500;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#32806;&#21512;&#21644;&#20869;&#32858;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#28436;&#31034;&#21644;&#23454;&#29616;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#26469;&#36827;&#34892;&#20195;&#30721;&#37325;&#26500;&#30340;&#26041;&#27861;&#12290;&#20195;&#30721;&#37325;&#26500;&#30340;&#21160;&#26426;&#21487;&#33021;&#26159;&#25913;&#21892;&#29616;&#26377;&#31243;&#24207;&#30340;&#35774;&#35745;&#12289;&#32467;&#26500;&#25110;&#23454;&#29616;&#65292;&#32780;&#19981;&#25913;&#21464;&#20854;&#21151;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#32806;&#21512;&#21644;&#20869;&#32858;&#24615;&#30340;&#38382;&#39064;&#65292;&#20197;&#24341;&#23548;&#37325;&#26500;&#36807;&#31243;&#26397;&#30528;&#20855;&#26377;&#39640;&#20869;&#32858;&#24615;&#21644;&#20302;&#32806;&#21512;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#38543;&#26426;&#29366;&#24577;&#38382;&#39064;&#30340;&#28436;&#31034;&#31034;&#20363;&#24182;&#21019;&#24314;&#19968;&#20010;&#24037;&#20855;&#26469;&#23454;&#29616;Java&#39033;&#30446;&#19978;&#30340;&#31639;&#27861;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach that evaluates best-first search methods to code refactoring. The motivation for code refactoring could be to improve the design, structure, or implementation of an existing program without changing its functionality. To solve a very specific problem of coupling and cohesion, we propose using heuristic search-based techniques on an approximation of the full code refactoring problem, to guide the refactoring process toward solutions that have high cohesion and low coupling. We evaluated our approach by providing demonstrative examples of the effectiveness of this approach on random state problems and created a tool to implement the algorithm on Java projects.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#25490;&#32451;&#21644;&#39044;&#26399;&#26469;&#35760;&#24518;&#26377;&#20851;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#24212;&#29992;&#33258;&#30417;&#30563;&#26426;&#21046;&#65292;&#36890;&#36807;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#23631;&#34109;&#24314;&#27169;&#20219;&#21153;&#35757;&#32451;&#65292;&#25104;&#21151;&#36890;&#36807;&#30701;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.07565</link><description>&lt;p&gt;
&#19968;&#31181;&#25903;&#25345;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#38382;&#31572;&#27969;&#24335;&#25968;&#25454;&#35760;&#24518;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information. (arXiv:2305.07565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#25490;&#32451;&#21644;&#39044;&#26399;&#26469;&#35760;&#24518;&#26377;&#20851;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#24212;&#29992;&#33258;&#30417;&#30563;&#26426;&#21046;&#65292;&#36890;&#36807;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#23631;&#34109;&#24314;&#27169;&#20219;&#21153;&#35757;&#32451;&#65292;&#25104;&#21151;&#36890;&#36807;&#30701;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38382;&#31572;&#26041;&#27861;&#24448;&#24448;&#20551;&#35774;&#36755;&#20837;&#20869;&#23481;&#65288;&#22914;&#25991;&#20214;&#25110;&#35270;&#39057;&#65289;&#24635;&#26159;&#21487;&#35775;&#38382;&#30340;&#65292;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#35760;&#24518;&#32593;&#32476;&#34987;&#24341;&#20837;&#26469;&#27169;&#20223;&#20154;&#31867;&#36880;&#27493;&#29702;&#35299;&#21644;&#21387;&#32553;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#23398;&#20064;&#22914;&#20309;&#36890;&#36807;&#25972;&#20010;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#38169;&#35823;&#26469;&#32500;&#25252;&#20869;&#23384;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#20855;&#26377;&#25552;&#39640;&#35760;&#24518;&#23481;&#37327;&#30340;&#26377;&#25928;&#26426;&#21046;&#65292;&#20363;&#22914;&#25490;&#32451;&#21644;&#39044;&#26399;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#32451;&#21644;&#39044;&#26399;&#26469;&#22788;&#29702;&#36755;&#20837;&#20197;&#35760;&#24518;&#26377;&#20851;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#35757;&#32451;&#26399;&#38388;&#36890;&#36807;&#38024;&#23545;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#23631;&#34109;&#24314;&#27169;&#20219;&#21153;&#36827;&#34892;&#33258;&#30417;&#30563;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;&#30701;&#24207;&#21015;&#65288;bAbI&#65289;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39135;&#21697;&#30424;&#20013;&#33756;&#32948;&#26816;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#39278;&#39135;&#35760;&#24405;&#21644;&#33829;&#20859;&#31649;&#29702;&#65292;&#24182;&#36890;&#36807;&#21360;&#24230;&#39184;&#30424;&#20013;&#33756;&#32948;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#20307;&#26816;&#27979;&#26550;&#26500;&#65292;&#36890;&#36807;&#22823;&#37327;&#22270;&#20687;&#21644;&#27880;&#37322;&#30340;&#27604;&#36739;&#65292;&#30830;&#23450;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65306;ResNet50-v2&#21644;EfficientDet D1&#12290;</title><link>http://arxiv.org/abs/2305.07552</link><description>&lt;p&gt;
&#39135;&#21697;&#30424;&#20013;&#30340;&#33756;&#32948;&#26816;&#27979;&#65306;&#33258;&#21160;&#39278;&#39135;&#35760;&#24405;&#21644;&#33829;&#20859;&#31649;&#29702;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dish detection in food platters: A framework for automated diet logging and nutrition management. (arXiv:2305.07552v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39135;&#21697;&#30424;&#20013;&#33756;&#32948;&#26816;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#39278;&#39135;&#35760;&#24405;&#21644;&#33829;&#20859;&#31649;&#29702;&#65292;&#24182;&#36890;&#36807;&#21360;&#24230;&#39184;&#30424;&#20013;&#33756;&#32948;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#20307;&#26816;&#27979;&#26550;&#26500;&#65292;&#36890;&#36807;&#22823;&#37327;&#22270;&#20687;&#21644;&#27880;&#37322;&#30340;&#27604;&#36739;&#65292;&#30830;&#23450;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65306;ResNet50-v2&#21644;EfficientDet D1&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39278;&#39135;&#23545;&#20110;&#29983;&#27963;&#26041;&#24335;&#30142;&#30149;&#30340;&#27969;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#31934;&#30830;&#32780;&#36731;&#26494;&#30340;&#39278;&#39135;&#35760;&#24405;&#26159;&#26377;&#25928;&#30340;&#39278;&#39135;&#31649;&#29702;&#21644;&#21345;&#36335;&#37324;&#38480;&#21046;&#30340;&#20027;&#35201;&#29942;&#39048;&#20043;&#19968;&#12290;&#37492;&#20110;&#39135;&#21697;&#24067;&#23616;&#30340;&#35270;&#35273;&#22797;&#26434;&#24615;&#65292;&#20174;&#39135;&#21697;&#30424;&#20013;&#26816;&#27979;&#20986;&#33756;&#32948;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#39278;&#39135;&#31649;&#29702;&#65292;&#20174;&#25968;&#25454;&#32534;&#35793;&#12289;&#26631;&#27880;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#35782;&#21035;&#21040;&#20854;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#21360;&#24230;&#39135;&#21697;&#30424;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#36825;&#20123;&#39135;&#21697;&#30424;&#20197;&#20854;&#22797;&#26434;&#30340;&#21576;&#29616;&#26041;&#24335;&#32780;&#33879;&#31216;&#65292;&#36825;&#32473;&#33756;&#32948;&#30340;&#33258;&#21160;&#26816;&#27979;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20174;&#26368;&#21463;&#27426;&#36814;&#30340;61&#36947;&#21360;&#24230;&#33756;&#32948;&#24320;&#22987;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#20307;&#26816;&#27979;&#26550;&#26500;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#30830;&#23450;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;68005&#20010;&#30424;&#23376;&#22270;&#20687;&#30340;&#31934;&#24515;&#32534;&#21046;&#65292;&#24182;&#36827;&#34892;&#20102;134814&#20010;&#25163;&#21160;&#33756;&#32948;&#27880;&#37322;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#39318;&#20808;&#27604;&#36739;&#20102;&#21313;&#31181;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#26550;&#26500;&#65292;&#20197;&#35782;&#21035;ResNet50-v2&#21644;EfficientDet D1&#20004;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diet is central to the epidemic of lifestyle disorders. Accurate and effortless diet logging is one of the significant bottlenecks for effective diet management and calorie restriction. Dish detection from food platters is a challenging problem due to a visually complex food layout. We present an end-to-end computational framework for diet management, from data compilation, annotation, and state-of-the-art model identification to its mobile app implementation. As a case study, we implement the framework in the context of Indian food platters known for their complex presentation that poses a challenge for the automated detection of dishes. Starting with the 61 most popular Indian dishes, we identify the state-of-the-art model through a comparative analysis of deep-learning-based object detection architectures. Rooted in a meticulous compilation of 68,005 platter images with 134,814 manual dish annotations, we first compare ten architectures for multi-label classification to identify Res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#24191;&#27867;&#20998;&#31867;&#33258;&#21160;&#24494;&#20998;&#30340;&#38382;&#39064;&#29992;&#27861;&#65292;&#24182;&#36890;&#36807;&#20030;&#20363;&#35828;&#26126;&#27599;&#20010;&#31867;&#21035;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#36991;&#20813;&#24847;&#22806;&#34892;&#20026;&#65292;&#22312;&#21457;&#29983;&#38382;&#39064;&#26102;&#26356;&#23481;&#26131;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20174;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#20013;&#33719;&#24471;&#26356;&#23454;&#38469;&#30340;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2305.07546</link><description>&lt;p&gt;
&#29702;&#35299;&#33258;&#21160;&#24494;&#20998;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Understanding Automatic Differentiation Pitfalls. (arXiv:2305.07546v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#24191;&#27867;&#20998;&#31867;&#33258;&#21160;&#24494;&#20998;&#30340;&#38382;&#39064;&#29992;&#27861;&#65292;&#24182;&#36890;&#36807;&#20030;&#20363;&#35828;&#26126;&#27599;&#20010;&#31867;&#21035;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#36991;&#20813;&#24847;&#22806;&#34892;&#20026;&#65292;&#22312;&#21457;&#29983;&#38382;&#39064;&#26102;&#26356;&#23481;&#26131;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20174;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#20013;&#33719;&#24471;&#26356;&#23454;&#38469;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#65288;&#20063;&#34987;&#31216;&#20026;&#21453;&#21521;&#20256;&#25773;&#12289;AD&#12289;autodiff&#25110;&#31639;&#27861;&#24494;&#20998;&#65289;&#26159;&#19968;&#31181;&#35745;&#31639;&#35745;&#31639;&#26426;&#31243;&#24207;&#23548;&#25968;&#30340;&#27969;&#34892;&#25216;&#26415;&#65292;&#26082;&#20934;&#30830;&#21448;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#20986;&#30340;&#23548;&#25968;&#26377;&#26102;&#21487;&#33021;&#34987;&#35299;&#37322;&#20026;&#19981;&#27491;&#30830;&#30340;&#12290;&#36825;&#20123;&#38519;&#38449;&#22312;&#24037;&#20855;&#21644;&#26041;&#27861;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#25991;&#24191;&#27867;&#20998;&#31867;&#33258;&#21160;&#24494;&#20998;&#30340;&#38382;&#39064;&#29992;&#27861;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#65288;&#22914;&#28151;&#27788;&#12289;&#26102;&#38388;&#24179;&#22343;&#25391;&#33633;&#12289;&#31163;&#25955;&#21270;&#12289;&#22266;&#23450;&#28857;&#24490;&#29615;&#12289;&#26597;&#25214;&#34920;&#21644;&#32447;&#24615;&#27714;&#35299;&#22120;&#65289;&#26469;&#35828;&#26126;&#27599;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#35843;&#35797;&#25216;&#26415;&#21450;&#20854;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#36991;&#20813;&#24847;&#22806;&#34892;&#20026;&#65292;&#22312;&#21457;&#29983;&#38382;&#39064;&#26102;&#26356;&#23481;&#26131;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20174;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#20013;&#33719;&#24471;&#26356;&#23454;&#38469;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation, also known as backpropagation, AD, autodiff, or algorithmic differentiation, is a popular technique for computing derivatives of computer programs accurately and efficiently. Sometimes, however, the derivatives computed by AD could be interpreted as incorrect. These pitfalls occur systematically across tools and approaches. In this paper we broadly categorize problematic usages of AD and illustrate each category with examples such as chaos, time-averaged oscillations, discretizations, fixed-point loops, lookup tables, and linear solvers. We also review debugging techniques and their effectiveness in these situations. With this article we hope to help readers avoid unexpected behavior, detect problems more easily when they occur, and have more realistic expectations from AD tools.
&lt;/p&gt;</description></item><item><title>WEDGE&#26159;&#19968;&#20010;&#36890;&#36807;&#29983;&#25104;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#27668;&#20505;&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#25506;&#27979;&#22120;&#22312;&#30495;&#23454;&#19990;&#30028;&#27668;&#35937;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07528</link><description>&lt;p&gt;
WEDGE&#65306;&#22522;&#20110;&#29983;&#25104;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#22810;&#27668;&#20505;&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models. (arXiv:2305.07528v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07528
&lt;/p&gt;
&lt;p&gt;
WEDGE&#26159;&#19968;&#20010;&#36890;&#36807;&#29983;&#25104;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#27668;&#20505;&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#25506;&#27979;&#22120;&#22312;&#30495;&#23454;&#19990;&#30028;&#27668;&#35937;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#36947;&#36335;&#23545;&#20110;&#33258;&#20027;&#24863;&#30693;&#25552;&#20986;&#20102;&#35768;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#33021;&#35265;&#24230;&#24046;&#12290;&#22312;&#22909;&#22825;&#27668;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#32463;&#24120;&#26080;&#27861;&#22312;&#36825;&#20123;&#20998;&#24067;&#21306;&#22495;&#20013;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#22686;&#24378;&#24863;&#30693;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;WEDGE&#65288;&#30001;DALL-E&#29983;&#25104;&#30340;&#27668;&#35937;&#22270;&#20687;&#65289;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;WEDGE&#21253;&#25324;3360&#24352;&#25163;&#21160;&#27880;&#37322;&#30340;16&#31181;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#22270;&#20687;&#65292;&#25903;&#25345;&#22825;&#27668;&#20998;&#31867;&#21644;2D&#29289;&#20307;&#26816;&#27979;&#31561;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20174;&#30740;&#31350;&#35282;&#24230;&#20998;&#26512;&#20102;WEDGE&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;&#26497;&#31471;&#22825;&#27668;&#19979;&#30340;&#33258;&#20027;&#24863;&#30693;&#25928;&#26524;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20998;&#31867;&#21644;&#26816;&#27979;&#30340;&#22522;&#20934;&#24615;&#33021;&#65292;&#20998;&#21035;&#20026;53.87%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#21644;45.41 mAP&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;WEDGE&#21487;&#29992;&#20110;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#25506;&#27979;&#22120;&#65292;&#23558;&#22312;&#30495;&#23454;&#19990;&#30028;&#27668;&#35937;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;DAWN&#65289;&#19978;&#30340;SOTA&#24615;&#33021;&#25552;&#39640;4.48 A&#12290;
&lt;/p&gt;
&lt;p&gt;
The open road poses many challenges to autonomous perception, including poor visibility from extreme weather conditions. Models trained on good-weather datasets frequently fail at detection in these out-of-distribution settings. To aid adversarial robustness in perception, we introduce WEDGE (WEather images by DALL-E GEneration): a synthetic dataset generated with a vision-language generative model via prompting. WEDGE consists of 3360 images in 16 extreme weather conditions manually annotated with 16513 bounding boxes, supporting research in the tasks of weather classification and 2D object detection. We have analyzed WEDGE from research standpoints, verifying its effectiveness for extreme-weather autonomous perception. We establish baseline performance for classification and detection with 53.87% test accuracy and 45.41 mAP. Most importantly, WEDGE can be used to fine-tune state-of-the-art detectors, improving SOTA performance on real-world weather benchmarks (such as DAWN) by 4.48 A
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#30913;&#20849;&#25391;&#24207;&#21015;&#20248;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#23454;&#29616;MR&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#21442;&#25968;&#23545;&#23454;&#38469;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20248;&#21270;&#23556;&#39057;&#33033;&#20914;&#21015;&#35774;&#35745;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07524</link><description>&lt;p&gt;
&#32852;&#21512;&#30913;&#20849;&#25391;&#24207;&#21015;&#20248;&#21270;&#36229;&#36234;&#20102;&#32431;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#33258;&#26059;&#22238;&#27874;&#30913;&#20849;&#25391;&#25104;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Joint MR sequence optimization beats pure neural network approaches for spin-echo MRI super-resolution. (arXiv:2305.07524v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#30913;&#20849;&#25391;&#24207;&#21015;&#20248;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#23454;&#29616;MR&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#21442;&#25968;&#23545;&#23454;&#38469;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20248;&#21270;&#23556;&#39057;&#33033;&#20914;&#21015;&#35774;&#35745;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;MRI&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#20165;&#20351;&#29992;&#20174;&#20856;&#22411;&#20020;&#24202;&#24207;&#21015;&#20013;&#33719;&#21462;&#30340;&#24050;&#26377;&#23545;&#27604;&#24230;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;(NN)&#30340;&#36755;&#20837;&#12290;&#22312;&#28065;&#36718;&#33258;&#26059;&#22238;&#27874;&#24207;&#21015;(TSE)&#20013;&#65292;&#24207;&#21015;&#21442;&#25968;&#23545;&#23454;&#38469;&#33719;&#24471;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#20174;&#32780;&#23545;NN&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24050;&#30693;&#36816;&#31639;&#31526;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#20248;&#21270;MR&#24207;&#21015;&#21644;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26469;&#36827;&#34892;SR-TSE&#12290;&#36825;&#31181;&#22522;&#20110;MR&#29289;&#29702;&#23398;&#30340;&#35757;&#32451;&#36807;&#31243;&#32852;&#21512;&#20248;&#21270;&#20102;&#36136;&#23376;&#23494;&#24230;(PD)&#21152;&#26435;TSE&#21644;T2&#21152;&#26435;TSE&#30340;&#23556;&#39057;&#33033;&#20914;&#21015;&#65292;&#20197;&#21450;&#38543;&#21518;&#24212;&#29992;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#30456;&#24212;&#30340;&#36229;&#20998;&#36776;&#29575;PDw&#21644;T2w TSE&#22270;&#20687;&#12290;&#25152;&#21457;&#29616;&#30340;&#23556;&#39057;&#33033;&#20914;&#21015;&#35774;&#35745;&#20026;NN&#29983;&#25104;&#20102;&#26368;&#20248;&#20449;&#21495;&#26469;&#25191;&#34892;SR&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#22522;&#20110;&#27169;&#25311;&#30340;&#20248;&#21270;&#25512;&#24191;&#21040;&#20307;&#20869;&#23454;&#27979;&#21644;&#33719;&#24471;&#30340;&#29289;&#29702;&#23398;&#30693;&#35782;&#30340;SR&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current MRI super-resolution (SR) methods only use existing contrasts acquired from typical clinical sequences as input for the neural network (NN). In turbo spin echo sequences (TSE) the sequence parameters can have a strong influence on the actual resolution of the acquired image and have consequently a considera-ble impact on the performance of the NN. We propose a known-operator learning approach to perform an end-to-end optimization of MR sequence and neural net-work parameters for SR-TSE. This MR-physics-informed training procedure jointly optimizes the radiofrequency pulse train of a proton density- (PD-) and T2-weighted TSE and a subsequently applied convolutional neural network to predict the corresponding PDw and T2w super-resolution TSE images. The found radiofrequency pulse train designs generate an optimal signal for the NN to perform the SR task. Our method generalizes from the simulation-based optimi-zation to in vivo measurements and the acquired physics-informed SR ima
&lt;/p&gt;</description></item><item><title>PillarAcc&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#22686;&#24378;&#22522;&#20110;Pillar&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#35745;&#31639;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.07522</link><description>&lt;p&gt;
PillarAcc: &#31232;&#30095;&#28857;&#20113;Pillars&#21152;&#36895;&#22120;&#8212;&#8212;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices. (arXiv:2305.07522v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07522
&lt;/p&gt;
&lt;p&gt;
PillarAcc&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#22686;&#24378;&#22522;&#20110;Pillar&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#35745;&#31639;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28857;&#20113;(PC)&#25968;&#25454;&#36827;&#34892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#31649;&#36947;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#39640;&#25928;&#30340;&#32534;&#30721;&#26159;&#28385;&#36275;&#20005;&#26684;&#36164;&#28304;&#21644;&#24310;&#36831;&#35201;&#27714;&#30340;&#20851;&#38190;&#12290;PointPillars&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#40479;&#30640;&#22270;(BEV)&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#32858;&#21512;&#21040;&#20108;&#32500;pillar&#20013;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#37319;&#29992;PointPillar&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;pillar&#32534;&#30721;&#30340;&#22266;&#26377;&#31232;&#30095;&#24615;&#65292;&#38169;&#22833;&#20102;&#22823;&#37327;&#35745;&#31639;&#20943;&#23569;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#21152;&#36895;&#31232;&#30095;&#21367;&#31215;&#22788;&#29702;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;pillar&#30340;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#22522;&#20110;pillar&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;pillar pruning&#26041;&#27861;&#23545;&#31232;&#30095;&#21270;&#26426;&#20250;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;PillarAcc&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#25903;&#25345;&#26426;&#21046;&#65292;&#36890;&#36807;&#36755;&#20837;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22686;&#24378;&#31232;&#30095;pillar&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object detection using point cloud (PC) data is vital for autonomous driving perception pipelines, where efficient encoding is key to meeting stringent resource and latency requirements. PointPillars, a widely adopted bird's-eye view (BEV) encoding, aggregates 3D point cloud data into 2D pillars for high-accuracy 3D object detection. However, most state-of-the-art methods employing PointPillar overlook the inherent sparsity of pillar encoding, missing opportunities for significant computational reduction. In this study, we propose a groundbreaking algorithm-hardware co-design that accelerates sparse convolution processing and maximizes sparsity utilization in pillar-based 3D object detection networks. We investigate sparsification opportunities using an advanced pillar-pruning method, achieving an optimal balance between accuracy and sparsity. We introduce PillarAcc, a state-of-the-art sparsity support mechanism that enhances sparse pillar convolution through linear complexity input
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;ML&#27169;&#22411;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07512</link><description>&lt;p&gt;
&#23398;&#20064;&#21435;&#23398;&#20064;&#65306;&#26426;&#22120;&#21435;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learn to Unlearn: A Survey on Machine Unlearning. (arXiv:2305.07512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;ML&#27169;&#22411;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#23454;&#29616;&#34987;&#36951;&#24536;&#26435;&#26159;&#35768;&#22810;&#25968;&#25454;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;&#26426;&#22120;&#21435;&#23398;&#20064;&#24050;&#25104;&#20026;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#30340;&#31616;&#35201;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#31934;&#30830;&#21644;&#36817;&#20284;&#26041;&#27861;&#12289;&#21487;&#33021;&#30340;&#25915;&#20987;&#20197;&#21450;&#39564;&#35777;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#27604;&#36739;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20351;&#29992;Deltagrad&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#36824;&#24378;&#35843;&#20102;&#25361;&#25112;&#65292;&#22914;&#38750;IID&#21024;&#38500;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#21644;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25104;&#20026;&#23547;&#27714;&#26426;&#22120;&#21435;&#23398;&#20064;&#36164;&#26009;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) models contain private information, and implementing the right to be forgotten is a challenging privacy issue in many data applications. Machine unlearning has emerged as an alternative to remove sensitive data from a trained model, but completely retraining ML models is often not feasible. This survey provides a concise appraisal of Machine Unlearning techniques, encompassing both exact and approximate methods, probable attacks, and verification approaches. The survey compares the merits and limitations each method and evaluates their performance using the Deltagrad exact machine unlearning method. The survey also highlights challenges like the pressing need for a robust model for non-IID deletion to mitigate fairness issues. Overall, the survey provides a thorough synopsis of machine unlearning techniques and applications, noting future research directions in this evolving field. The survey aims to be a valuable resource for researchers and practitioners seeking
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#20998;&#26512;&#20102;XAI&#39046;&#22495;&#20013;&#24212;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#30340;&#30740;&#31350;&#65292;&#20197;&#20415;&#35299;&#37322;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#38024;&#23545;&#30340;&#30142;&#30149;&#21253;&#25324;&#30284;&#30151;&#21644;COVID-19&#12290;</title><link>http://arxiv.org/abs/2305.07511</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence on Medical Images: A Survey. (arXiv:2305.07511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07511
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#20998;&#26512;&#20102;XAI&#39046;&#22495;&#20013;&#24212;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#30340;&#30740;&#31350;&#65292;&#20197;&#20415;&#35299;&#37322;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#38024;&#23545;&#30340;&#30142;&#30149;&#21253;&#25324;&#30284;&#30151;&#21644;COVID-19&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#20110;&#21307;&#30103;&#39046;&#22495;&#30340;&#30740;&#31350;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#12290;&#20026;&#20102;&#21521;&#21442;&#19982;&#21307;&#23398;&#26816;&#26597;&#30340;&#25152;&#26377;&#20154;&#35299;&#37322;&#36825;&#20123;&#32467;&#26524;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;&#26368;&#36817;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20063;&#31216;&#20026;XAI&#65292;&#26088;&#22312;&#35299;&#37322;&#36825;&#20123;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#20197;&#20415;&#36827;&#34892;&#25152;&#38656;&#35780;&#20272;&#12290;&#36825;&#39033;&#35843;&#26597;&#20998;&#26512;&#20102;XAI&#39046;&#22495;&#20013;&#38024;&#23545;&#21307;&#23398;&#35786;&#26029;&#30740;&#31350;&#30340;&#20960;&#39033;&#26368;&#26032;&#30740;&#31350;&#65292;&#20801;&#35768;&#26377;&#20851;&#22810;&#31181;&#19981;&#21516;&#30142;&#30149;&#65288;&#22914;&#30284;&#30151;&#21644;COVID-19&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, the number of works about deep learning applied to the medical field has increased enormously. The necessity of a rigorous assessment of these models is required to explain these results to all people involved in medical exams. A recent field in the machine learning area is explainable artificial intelligence, also known as XAI, which targets to explain the results of such black box models to permit the desired assessment. This survey analyses several recent studies in the XAI field applied to medical diagnosis research, allowing some explainability of the machine learning results in several different diseases, such as cancers and COVID-19.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;OT&#38382;&#39064;&#30340;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#30340;&#65292;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.07500</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21487;&#23545;&#40784;&#30340;&#28508;&#22312;&#31354;&#38388;&#29992;&#20110;&#39640;&#25928;&#38381;&#21512;&#24418;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation. (arXiv:2305.07500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;OT&#38382;&#39064;&#30340;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#30340;&#65292;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20960;&#20309;&#24037;&#20855;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#23545;&#40784;&#27010;&#29575;&#27979;&#24230;&#65292;&#36981;&#24490;&#26368;&#23567;&#21162;&#21147;&#21407;&#21017;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#65292;OT&#30340;&#35768;&#22810;&#25104;&#21151;&#24212;&#29992;&#20043;&#19968;&#26159;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#30446;&#26631;&#26159;&#23558;&#20998;&#31867;&#22120;&#20174;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#26410;&#26631;&#35760;&#25110;&#31232;&#30095;&#26631;&#35760;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;OT&#30340;DA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30001;&#20223;&#23556;&#26144;&#23556;&#32473;&#20986;&#30340;OT&#38382;&#39064;&#30340;&#38381;&#24335;&#35299;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;&#35813;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Among many successful applications of OT in machine learning (ML), domain adaptation (DA) -- a field of study where the goal is to transfer a classifier from one labelled domain to another similar, yet different unlabelled or scarcely labelled domain -- has been historically among the most investigated ones. This success is due to the ability of OT to provide both a meaningful discrepancy measure to assess the similarity of two domains' distributions and a mapping that can project source domain data onto the target one. In this paper, we propose a principally new OT-based approach applied to DA that uses the closed-form solution of the OT problem given by an affine mapping and learns an embedding space for which this solution is optimal and computationally less complex. We show that our approach works in both homogeneous and heterogeneous DA settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20445;&#23432;&#35268;&#21010;&#22120;&#26041;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#27599;&#31181;&#24773;&#20917;&#30340;&#8220;&#38271;&#23614;&#8221;&#29575;&#33258;&#21160;&#35843;&#25972;&#20445;&#23432;&#31243;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;SDVs&#22312;&#23454;&#38469;&#39550;&#39542;&#20013;&#36935;&#21040;&#30340;&#32597;&#35265;&#20851;&#38190;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.07497</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20445;&#23432;&#22411;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#19982;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Dynamically Conservative Self-Driving Planner for Long-Tail Cases. (arXiv:2305.07497v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20445;&#23432;&#35268;&#21010;&#22120;&#26041;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#27599;&#31181;&#24773;&#20917;&#30340;&#8220;&#38271;&#23614;&#8221;&#29575;&#33258;&#21160;&#35843;&#25972;&#20445;&#23432;&#31243;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;SDVs&#22312;&#23454;&#38469;&#39550;&#39542;&#20013;&#36935;&#21040;&#30340;&#32597;&#35265;&#20851;&#38190;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;SDVs&#65289;&#22312;&#23454;&#38469;&#39550;&#39542;&#20013;&#32463;&#24120;&#38754;&#20020;&#8220;&#38271;&#23614;&#8221;&#25361;&#25112;&#65292;&#21363;SDVs&#23558;&#19981;&#26029;&#36935;&#21040;&#22312;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#30340;&#32597;&#35265;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#12290;&#19968;&#20123;&#23433;&#20840;&#20445;&#35777;&#35268;&#21010;&#22120;&#36890;&#36807;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#20445;&#23432;&#22788;&#29702;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#39550;&#39542;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#20445;&#23432;&#35268;&#21010;&#22120;&#65288;DCP&#65289;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#27599;&#31181;&#24773;&#20917;&#30340;&#8220;&#38271;&#23614;&#8221;&#29575;&#33258;&#21160;&#35843;&#25972;&#20445;&#23432;&#31243;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#8220;&#38271;&#23614;&#8221;&#29575;&#23450;&#20041;&#20026;SDV&#36890;&#36807;&#39550;&#39542;&#26696;&#20363;&#30340;&#20449;&#24515;&#27700;&#24179;&#12290;&#36825;&#20010;&#29575;&#34920;&#31034;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#30340;&#27010;&#29575;&#65292;&#24182;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#30340;&#32479;&#35745;&#33258;&#21161;&#27861;&#36827;&#34892;&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#21547;&#19981;&#21516;&#20445;&#23432;&#31243;&#24230;&#20505;&#36873;&#31574;&#30053;&#30340;&#35268;&#21010;&#22120;&#12290;&#26368;&#32456;&#31574;&#30053;&#22522;&#20110;&#20272;&#35745;&#30340;&#8220;&#38271;&#23614;&#8221;&#29575;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;DCP&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#38271;&#23614;&#26696;&#20363;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#39550;&#39542;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-driving vehicles (SDVs) are becoming reality but still suffer from "long-tail" challenges during natural driving: the SDVs will continually encounter rare, safety-critical cases that may not be included in the dataset they were trained. Some safety-assurance planners solve this problem by being conservative in all possible cases, which may significantly affect driving mobility. To this end, this work proposes a method to automatically adjust the conservative level according to each case's "long-tail" rate, named dynamically conservative planner (DCP). We first define the "long-tail" rate as an SDV's confidence to pass a driving case. The rate indicates the probability of safe-critical events and is estimated using the statistics bootstrapped method with historical data. Then, a reinforcement learning-based planner is designed to contain candidate policies with different conservative levels. The final policy is optimized based on the estimated "long-tail" rate. In this way, the DCP
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#24555;&#36895;&#30340;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22270;&#24211;&#25968;&#25454;&#36827;&#34892;&#37319;&#26679;&#22788;&#29702;&#65292;&#22312;&#20943;&#23569;&#25628;&#32034;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#23545;&#24322;&#24120;&#22270;&#20687;&#22914;&#38169;&#35823;&#26631;&#35760;&#12289;&#20302;&#36136;&#37327;&#21644;&#20449;&#24687;&#36739;&#23569;&#30340;&#22270;&#20687;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;5.4M&#32593;&#32476;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FNIR&#26041;&#38754;&#36798;&#21040;&#20102;0.0975&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#20026;0.3891&#12290;</title><link>http://arxiv.org/abs/2305.07495</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24211;&#37319;&#26679;&#30340;&#20154;&#33080;&#35782;&#21035;&#24555;&#36895;&#20934;&#30830;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gallery Sampling for Robust and Fast Face Identification. (arXiv:2305.07495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#24555;&#36895;&#30340;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22270;&#24211;&#25968;&#25454;&#36827;&#34892;&#37319;&#26679;&#22788;&#29702;&#65292;&#22312;&#20943;&#23569;&#25628;&#32034;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#23545;&#24322;&#24120;&#22270;&#20687;&#22914;&#38169;&#35823;&#26631;&#35760;&#12289;&#20302;&#36136;&#37327;&#21644;&#20449;&#24687;&#36739;&#23569;&#30340;&#22270;&#20687;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;5.4M&#32593;&#32476;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FNIR&#26041;&#38754;&#36798;&#21040;&#20102;0.0975&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#20026;0.3891&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26497;&#22823;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#23613;&#21487;&#33021;&#22810;&#30340;&#22270;&#20687;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26631;&#35782;&#25968;&#25454;&#21644;&#26816;&#26597;&#22823;&#37327;&#22270;&#20687;&#25968;&#25454;&#30340;&#36136;&#37327;&#26159;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22823;&#25968;&#25454;&#26102;&#19981;&#33021;&#36991;&#20813;&#38169;&#35823;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#19968;&#30452;&#35797;&#22270;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#22914;&#26524;&#38169;&#35823;&#20986;&#29616;&#22312;&#20154;&#33080;&#35782;&#21035;&#30340;&#22270;&#24211;&#25968;&#25454;&#20013;&#65292;&#20250;&#24102;&#26469;&#26356;&#20026;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22270;&#24211;&#25968;&#25454;&#37319;&#26679;&#26041;&#27861;&#65292;&#21253;&#25324;&#38169;&#35823;&#26631;&#35760;&#12289;&#20302;&#36136;&#37327;&#21644;&#20449;&#24687;&#36739;&#23569;&#30340;&#22270;&#20687;&#65292;&#24182;&#20943;&#23569;&#20102;&#25628;&#32034;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#37319;&#26679;-&#20462;&#21098;&#21644;&#37319;&#26679;-&#29983;&#25104;&#26041;&#27861;&#22312;5.4M&#20010;&#21517;&#20154;&#32593;&#32476;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#33080;&#35782;&#21035;&#24615;&#33021;&#12290;&#22312;FPIR=0.01&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FNIR&#26041;&#38754;&#36798;&#21040;&#20102;0.0975&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#21017;&#26174;&#31034;&#20026;0.3891&#12290;&#24179;&#22343;&#29305;&#24449;&#21521;&#37327;&#25968;&#37327;&#20943;&#23569;&#20102;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have been achieved brilliant results in face recognition. One of the important tasks to improve the performance is to collect and label images as many as possible. However, labeling identities and checking qualities of large image data are difficult task and mistakes cannot be avoided in processing large data. Previous works have been trying to deal with the problem only in training domain, however it can cause much serious problem if the mistakes are in gallery data of face identification. We proposed gallery data sampling methods which are robust to outliers including wrong labeled, low quality, and less-informative images and reduce searching time. The proposed sampling-by-pruning and sampling-by-generating methods significantly improved face identification performance on our 5.4M web image dataset of celebrities. The proposed method achieved 0.0975 in terms of FNIR at FPIR=0.01, while conventional method showed 0.3891. The average number of feature vectors for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#38388;&#22270;&#21644;&#21338;&#24328;&#35770;&#32593;&#32476;&#24418;&#25104;&#65292;&#26088;&#22312;&#35299;&#20915;&#26102;&#38388;&#36335;&#24452;&#20445;&#35777;&#25152;&#26377;&#23545;&#20043;&#38388;&#30340;&#21487;&#36798;&#24615;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07494</link><description>&lt;p&gt;
&#26102;&#38388;&#32593;&#32476;&#21019;&#24314;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Temporal Network Creation Games. (arXiv:2305.07494v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#38388;&#22270;&#21644;&#21338;&#24328;&#35770;&#32593;&#32476;&#24418;&#25104;&#65292;&#26088;&#22312;&#35299;&#20915;&#26102;&#38388;&#36335;&#24452;&#20445;&#35777;&#25152;&#26377;&#23545;&#20043;&#38388;&#30340;&#21487;&#36798;&#24615;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#32593;&#32476;&#19981;&#26159;&#38745;&#24577;&#30340;&#23545;&#35937;&#65292;&#32780;&#26159;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#22312;&#26368;&#36817;&#20960;&#24180;&#20869;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#22270;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;&#22312;&#26102;&#38388;&#22270;&#20013;&#65292;&#25105;&#20204;&#26377;&#19968;&#32452;&#22266;&#23450;&#30340;&#33410;&#28857;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#36830;&#36890;&#24615;&#21482;&#22312;&#26576;&#20123;&#26102;&#38388;&#27493;&#39588;&#19978;&#21487;&#29992;&#12290;&#36825;&#24341;&#21457;&#20102;&#22823;&#37327;&#31639;&#27861;&#38382;&#39064;&#65292;&#20854;&#20013;&#26368;&#20026;&#31361;&#20986;&#30340;&#26159;&#23547;&#25214;&#26102;&#38388;&#24352;&#37327;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#26102;&#38388;&#36335;&#24452;&#20445;&#35777;&#25152;&#26377;&#23545;&#20043;&#38388;&#30340;&#21487;&#36798;&#24615;&#30340;&#35745;&#31639;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#20165;&#30693;&#36947;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#38598;&#20013;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32593;&#32476;&#24182;&#19981;&#26159;&#30001;&#20013;&#22830;&#35774;&#35745;&#24072;&#22609;&#36896;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#35768;&#22810;&#25112;&#30053;&#20195;&#29702;&#20154;&#30340;&#20114;&#21160;&#32780;&#20135;&#29983;&#21644;&#21457;&#23637;&#30340;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#26159;&#26368;&#36817;&#23545;&#21338;&#24328;&#35770;&#32593;&#32476;&#24418;&#25104;&#27169;&#22411;&#36827;&#34892;&#23494;&#38598;&#30740;&#31350;&#30340;&#25512;&#21160;&#21147;&#37327;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#36817;&#26399;&#30340;&#30740;&#31350;&#26041;&#21521;&#32467;&#21512;&#22312;&#19968;&#36215;&#65306;&#26102;&#38388;&#22270;&#21644;&#21338;&#24328;&#35770;&#32593;&#32476;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most networks are not static objects, but instead they change over time. This observation has sparked rigorous research on temporal graphs within the last years. In temporal graphs, we have a fixed set of nodes and the connections between them are only available at certain time steps. This gives rise to a plethora of algorithmic problems on such graphs, most prominently the problem of finding temporal spanners, i.e., the computation of subgraphs that guarantee all pairs reachability via temporal paths. To the best of our knowledge, only centralized approaches for the solution of this problem are known. However, many real-world networks are not shaped by a central designer but instead they emerge and evolve by the interaction of many strategic agents. This observation is the driving force of the recent intensive research on game-theoretic network formation models.  In this work we bring together these two recent research directions: temporal graphs and game-theoretic network formation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#33258;&#21160;&#39550;&#39542;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#19981;&#21487;&#38752;&#24615;&#65292;&#20197;&#20445;&#25252;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#21644;&#38480;&#21046;&#31574;&#30053;&#30340;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07487</link><description>&lt;p&gt;
&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#30340;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#36793;&#30028;&#30830;&#23450; (arXiv:2305.07487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Identify, Estimate and Bound the Uncertainty of Reinforcement Learning for Autonomous Driving. (arXiv:2305.07487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#33258;&#21160;&#39550;&#39542;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#19981;&#21487;&#38752;&#24615;&#65292;&#20197;&#20445;&#25252;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#21644;&#38480;&#21046;&#31574;&#30053;&#30340;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#24320;&#21457;&#26356;&#26234;&#33021;&#21270;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;(AVs)&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;AVs&#19978;&#30340;&#20856;&#22411;DRL&#24212;&#29992;&#26159;&#35757;&#32451;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#20915;&#31574;&#22833;&#35823;&#65292;&#20351;&#36825;&#20123;AVs&#19981;&#21487;&#38752;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#21644;&#20445;&#25252;DRL&#39550;&#39542;&#31574;&#30053;&#30340;&#19981;&#21487;&#38752;&#20915;&#31574;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#20272;&#35745;&#21644;&#38480;&#21046;&#31574;&#30053;&#30340;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#25110;&#32593;&#32476;&#25311;&#21512;&#35823;&#24046;&#23548;&#33268;&#30340;&#28508;&#22312;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#38480;&#21046;&#19981;&#30830;&#23450;&#24615;&#65292;DRL&#27169;&#22411;&#30340;&#24615;&#33021;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#31574;&#30053;&#12290;&#30001;&#19981;&#36275;&#30340;&#25968;&#25454;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#29992;&#33258;&#21161;&#27861;&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#38598;&#25104;&#32593;&#32476;&#20272;&#35745;&#30001;&#32593;&#32476;&#25311;&#21512;&#35823;&#24046;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#23558;&#22522;&#32447;&#31574;&#30053;&#28155;&#21152;&#20026;&#24615;&#33021;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has emerged as a promising approach for developing more intelligent autonomous vehicles (AVs). A typical DRL application on AVs is to train a neural network-based driving policy. However, the black-box nature of neural networks can result in unpredictable decision failures, making such AVs unreliable. To this end, this work proposes a method to identify and protect unreliable decisions of a DRL driving policy. The basic idea is to estimate and constrain the policy's performance uncertainty, which quantifies potential performance drop due to insufficient training data or network fitting errors. By constraining the uncertainty, the DRL model's performance is always greater than that of a baseline policy. The uncertainty caused by insufficient data is estimated by the bootstrapped method. Then, the uncertainty caused by the network fitting error is estimated using an ensemble network. Finally, a baseline policy is added as the performance lower bound to a
&lt;/p&gt;</description></item><item><title>BactInt&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;&#24182;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;BactInt&#35821;&#26009;&#24211;&#26631;&#27880;&#20102;1200&#31687;PubMed&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.07468</link><description>&lt;p&gt;
BactInt:&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#20010;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text. (arXiv:2305.07468v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07468
&lt;/p&gt;
&lt;p&gt;
BactInt&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;&#24182;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;BactInt&#35821;&#26009;&#24211;&#26631;&#27880;&#20102;1200&#31687;PubMed&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#39046;&#22495;&#20013;&#19981;&#21516;&#31867;&#22411;&#24494;&#29983;&#29289;&#22312;&#29983;&#29289;&#23398;&#31354;&#38388;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#20123;&#24494;&#29983;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#24494;&#29983;&#29289;&#32676;&#33853;&#32467;&#26500;&#30340;&#22522;&#26412;&#26500;&#24314;&#21333;&#20803;&#12290;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#21487;&#20316;&#20026;&#39044;&#27979;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#21487;&#38752;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#38405;&#35835;&#28023;&#37327;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26159;&#19968;&#39033;&#32791;&#26102;&#24182;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#24037;&#20316;&#12290;&#36825;&#23601;&#24517;&#28982;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#20934;&#30830;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25152;&#25253;&#36947;&#30340;&#32454;&#33740;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#24494;&#29983;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;&#29305;&#21035;&#26159;&#32454;&#33740;&#20043;&#38388;&#65289;&#30340;&#26041;&#27861;&#20197;&#21450;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#31649;&#36947;&#65292;&#29992;&#20110;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;Bacterial Interaction (BactInt)&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;1200&#31687;PubMed&#25688;&#35201;&#65292;&#27880;&#37322;&#26377;&#32454;&#33740;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The community of different types of microbes present in a biological niche plays a very important role in functioning of the system. The crosstalk or interactions among the different microbes contributes to the building blocks of such microbial community structures. Evidence reported in biomedical text serves as a reliable source for predicting such interactions. However, going through the vast and ever-increasing volume of biomedical literature is an intimidating and time consuming process. This necessitates development of automated methods capable of accurately extracting bacterial relations reported in biomedical literature. In this paper, we introduce a method for automated extraction of microbial interactions (specifically between bacteria) from biomedical literature along with ways of using transfer learning to improve its accuracy. We also describe a pipeline using which relations among specific bacteria groups can be mined. Additionally, we introduce the first publicly availabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#24378;&#21270;&#23398;&#20064;&#22312;&#37329;&#34701;&#31185;&#25216;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32452;&#21512;&#20248;&#21270;&#12289;&#38477;&#20302;&#20449;&#29992;&#39118;&#38505;&#12289;&#25237;&#36164;&#36164;&#26412;&#31649;&#29702;&#12289;&#21033;&#28070;&#26368;&#22823;&#21270;&#12289;&#26377;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26356;&#22909;&#30340;&#20215;&#26684;&#31574;&#30053;&#30830;&#23450;&#12290;&#26412;&#25991;&#36890;&#36807;PRISMA&#25216;&#26415;&#31579;&#36873;&#25991;&#29486;&#65292;&#31361;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;Fintech&#20013;&#30340;&#39044;&#27979;&#31934;&#24230;&#12289;&#22797;&#26434;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#39118;&#38505;&#12289;&#30408;&#21033;&#33021;&#21147;&#21644;&#19994;&#32489;&#65292;&#26088;&#22312;&#25506;&#35752;&#20854;&#22312;Fintech&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.07466</link><description>&lt;p&gt;
&#37329;&#34701;&#31185;&#25216;&#39046;&#22495;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Systematic Review on Reinforcement Learning in the Field of Fintech. (arXiv:2305.07466v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#24378;&#21270;&#23398;&#20064;&#22312;&#37329;&#34701;&#31185;&#25216;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32452;&#21512;&#20248;&#21270;&#12289;&#38477;&#20302;&#20449;&#29992;&#39118;&#38505;&#12289;&#25237;&#36164;&#36164;&#26412;&#31649;&#29702;&#12289;&#21033;&#28070;&#26368;&#22823;&#21270;&#12289;&#26377;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26356;&#22909;&#30340;&#20215;&#26684;&#31574;&#30053;&#30830;&#23450;&#12290;&#26412;&#25991;&#36890;&#36807;PRISMA&#25216;&#26415;&#31579;&#36873;&#25991;&#29486;&#65292;&#31361;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;Fintech&#20013;&#30340;&#39044;&#27979;&#31934;&#24230;&#12289;&#22797;&#26434;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#39118;&#38505;&#12289;&#30408;&#21033;&#33021;&#21147;&#21644;&#19994;&#32489;&#65292;&#26088;&#22312;&#25506;&#35752;&#20854;&#22312;Fintech&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#37329;&#34701;&#31185;&#25216;&#65288;Fintech&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;&#36890;&#36807;&#20854;&#20016;&#23500;&#30340;&#33021;&#21147;&#21644;&#39640;&#25928;&#24615;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;Fintech&#39046;&#22495;&#21462;&#24471;&#20102;&#21331;&#36234;&#25104;&#26524;&#12290;&#26412;&#31995;&#32479;&#24615;&#32508;&#36848;&#30340;&#30446;&#30340;&#26159;&#24320;&#23637;&#19968;&#39033;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#19982;Fintech&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#31361;&#20986;&#20854;&#39044;&#27979;&#31934;&#24230;&#12289;&#22797;&#26434;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#39118;&#38505;&#12289;&#30408;&#21033;&#33021;&#21147;&#21644;&#19994;&#32489;&#12290;&#24378;&#21270;&#23398;&#20064;&#22312;&#37329;&#34701;&#25110;Fintech&#39046;&#22495;&#30340;&#20027;&#35201;&#29992;&#36884;&#21253;&#25324;&#32452;&#21512;&#20248;&#21270;&#12289;&#38477;&#20302;&#20449;&#29992;&#39118;&#38505;&#12289;&#25237;&#36164;&#36164;&#26412;&#31649;&#29702;&#12289;&#21033;&#28070;&#26368;&#22823;&#21270;&#12289;&#26377;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26356;&#22909;&#30340;&#20215;&#26684;&#31574;&#30053;&#30830;&#23450;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#23545;&#37329;&#34701;&#26426;&#26500;&#19994;&#32489;&#30340;&#23454;&#38469;&#36129;&#29486;&#12290;&#26412;&#32508;&#36848;&#21253;&#21547;&#20102;2018&#24180;&#20197;&#26469;&#21457;&#34920;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20351;&#29992;PRISMA&#25216;&#26415;&#24320;&#23637;&#20102;&#26412;&#27425;&#32508;&#36848;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#21457;&#29616;&#21644;&#31579;&#36873;&#21463;&#32435;&#20837;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications of Reinforcement Learning in the Finance Technology (Fintech) have acquired a lot of admiration lately. Undoubtedly Reinforcement Learning, through its vast competence and proficiency, has aided remarkable results in the field of Fintech. The objective of this systematic survey is to perform an exploratory study on a correlation between reinforcement learning and Fintech to highlight the prediction accuracy, complexity, scalability, risks, profitability and performance. Major uses of reinforcement learning in finance or Fintech include portfolio optimization, credit risk reduction, investment capital management, profit maximization, effective recommendation systems, and better price setting strategies. Several studies have addressed the actual contribution of reinforcement learning to the performance of financial institutions. The latest studies included in this survey are publications from 2018 onward. The survey is conducted using PRISMA technique which focuses on the re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#31354;&#38388;&#65292;&#25506;&#32034;&#20102;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20043;&#38388;&#21508;&#31181;&#19981;&#21516;&#30340;&#21019;&#24847;&#24847;&#22270;&#27807;&#36890;&#26041;&#24335;&#65292;&#21457;&#29616;MI-CC&#31995;&#32479;&#20013;&#26356;&#22810;&#30001;&#20154;&#24037;&#26234;&#33021;&#21457;&#36215;&#30340;&#36129;&#29486;&#26159;&#29992;&#25143;&#20248;&#20808;&#36873;&#25321;&#30340;&#65292;&#36825;&#35828;&#26126;&#20102;&#36229;&#36234;&#25552;&#31034;&#65292;&#25506;&#32034;&#26356;&#22810;&#30340;MI-CC&#35774;&#35745;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.07465</link><description>&lt;p&gt;
&#36229;&#36234;&#25552;&#31034;&#65306;&#25506;&#32034;&#28151;&#21512;&#20027;&#21160;&#21512;&#20316;&#21019;&#36896;&#24615;&#31995;&#32479;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond Prompts: Exploring the Design Space of Mixed-Initiative Co-Creativity Systems. (arXiv:2305.07465v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#31354;&#38388;&#65292;&#25506;&#32034;&#20102;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20043;&#38388;&#21508;&#31181;&#19981;&#21516;&#30340;&#21019;&#24847;&#24847;&#22270;&#27807;&#36890;&#26041;&#24335;&#65292;&#21457;&#29616;MI-CC&#31995;&#32479;&#20013;&#26356;&#22810;&#30001;&#20154;&#24037;&#26234;&#33021;&#21457;&#36215;&#30340;&#36129;&#29486;&#26159;&#29992;&#25143;&#20248;&#20808;&#36873;&#25321;&#30340;&#65292;&#36825;&#35828;&#26126;&#20102;&#36229;&#36234;&#25552;&#31034;&#65292;&#25506;&#32034;&#26356;&#22810;&#30340;MI-CC&#35774;&#35745;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#31995;&#32479;&#24050;&#32463;&#38024;&#23545;&#22270;&#20687;&#12289;&#20195;&#30721;&#12289;&#25925;&#20107;&#21644;&#28216;&#25103;&#29983;&#25104;&#31561;&#26041;&#38754;&#24320;&#21457;&#20986;&#26469;&#65292;&#26088;&#22312;&#20419;&#36827;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#21457;&#25381;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#31070;&#32463;&#29983;&#25104;&#31995;&#32479;&#30340;&#30740;&#31350;&#22987;&#32456;&#24378;&#35843;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#30340;&#26041;&#24335;&#65306;&#29992;&#25143;&#25552;&#20379;&#35828;&#26126;&#65288;&#36890;&#24120;&#26159;&#25552;&#31034;&#65289;&#28982;&#21518;AI&#31995;&#32479;&#29983;&#25104;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#22312;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#27169;&#24335;&#20013;&#36824;&#26377;&#20854;&#20182;&#37197;&#32622;&#65292;&#20363;&#22914;&#20849;&#21019;&#65288;CC&#65289;&#65292;&#21363;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#37117;&#21487;&#20197;&#36129;&#29486;&#20869;&#23481;&#30340;&#21019;&#36896;&#65292;&#20197;&#21450;&#28151;&#21512;&#20027;&#21160;&#65288;MI&#65289;&#27169;&#24335;&#65292;&#21363;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#37117;&#21487;&#20197;&#21457;&#36215;&#20869;&#23481;&#30340;&#26356;&#25913;&#35831;&#27714;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#20551;&#24819;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#31354;&#38388;&#65292;&#30001;&#19981;&#21516;&#30340;&#26041;&#24335;&#32452;&#25104;&#65292;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#20114;&#30456;&#27807;&#36890;&#21019;&#36896;&#24615;&#24847;&#22270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#31867;&#21442;&#19982;&#30740;&#31350;&#65292;&#26377;185&#21517;&#21442;&#19982;&#32773;&#65292;&#20102;&#35299;&#29992;&#25143;&#24819;&#35201;&#22914;&#20309;&#19982;MI-CC&#31995;&#32479;&#36827;&#34892;&#19981;&#21516;&#37197;&#32622;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MI-CC&#31995;&#32479;&#20013;&#26356;&#22810;&#30001;&#20154;&#24037;&#26234;&#33021;&#21457;&#36215;&#30340;&#36129;&#29486;&#26159;&#29992;&#25143;&#20248;&#20808;&#36873;&#25321;&#30340;&#65292;&#32780;&#38750;&#20165;&#20381;&#38752;&#20154;&#31867;&#21457;&#36215;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25506;&#32034;&#36229;&#36234;&#25552;&#31034;&#30340;&#26356;&#24191;&#27867;&#30340;MI-CC&#35774;&#35745;&#26041;&#26696;&#21487;&#20197;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence systems have been developed for image, code, story, and game generation with the goal of facilitating human creativity. Recent work on neural generative systems has emphasized one particular means of interacting with AI systems: the user provides a specification, usually in the form of prompts, and the AI system generates the content. However, there are other configurations of human and AI coordination, such as co-creativity (CC) in which both human and AI systems can contribute to content creation, and mixed-initiative (MI) in which both human and AI systems can initiate content changes. In this paper, we define a hypothetical human-AI configuration design space consisting of different means for humans and AI systems to communicate creative intent to each other. We conduct a human participant study with 185 participants to understand how users want to interact with differently configured MI-CC systems. We find out that MI-CC systems with more extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07440</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#20869;&#23384;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Optimizing Memory Mapping Using Deep Reinforcement Learning. (arXiv:2305.07440v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#35843;&#24230;&#21644;&#20998;&#37197;&#26159;&#35768;&#22810;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#28085;&#30422;&#25317;&#22622;&#25511;&#21046;&#21040;&#20113;&#35745;&#31639;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35843;&#24230;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#21363;&#32534;&#35793;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#26399;&#38388;&#20986;&#29616;&#30340;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#65306;&#21363;&#23558;&#24352;&#37327;&#26144;&#23556;&#21040;&#19981;&#21516;&#30340;&#20869;&#23384;&#23618;&#20197;&#20248;&#21270;&#25191;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#21644;&#39640;&#32500;&#25968;&#25454;&#36755;&#20837;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource scheduling and allocation is a critical component of many high impact systems ranging from congestion control to cloud computing. Finding more optimal solutions to these problems often has significant impact on resource and time savings, reducing device wear-and-tear, and even potentially improving carbon emissions. In this paper, we focus on a specific instance of a scheduling problem, namely the memory mapping problem that occurs during compilation of machine learning programs: That is, mapping tensors to different memory layers to optimize execution time.  We introduce an approach for solving the memory mapping problem using Reinforcement Learning. RL is a solution paradigm well-suited for sequential decision making problems that are amenable to planning, and combinatorial search spaces with high-dimensional data inputs. We formulate the problem as a single-player game, which we call the mallocGame, such that high-reward trajectories of the game correspond to efficient memo
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;(GFTNN)&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#36827;&#34892;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#24378;&#22823;&#30340;&#25805;&#20316;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#21464;&#25442;(GFT)&#65292;&#21487;&#20197;&#27719;&#24635;&#22330;&#26223;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#22312;&#39640;&#36895;&#20844;&#36335;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#21253;&#21547;&#20219;&#20309;&#24490;&#29615;&#20803;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.07416</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#22312;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Multidimensional Graph Fourier Transformation Neural Network for Vehicle Trajectory Prediction. (arXiv:2305.07416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07416
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;(GFTNN)&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#36827;&#34892;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#24378;&#22823;&#30340;&#25805;&#20316;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#21464;&#25442;(GFT)&#65292;&#21487;&#20197;&#27719;&#24635;&#22330;&#26223;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#22312;&#39640;&#36895;&#20844;&#36335;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#21253;&#21547;&#20219;&#20309;&#24490;&#29615;&#20803;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;(GFTNN)&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#36827;&#34892;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#12290;&#31867;&#20284;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#65292;GFTNN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#22312;&#22270;&#32467;&#26500;&#19978;&#25805;&#20316;&#12290;&#36890;&#36807;&#24378;&#22823;&#30340;&#25805;&#20316;&#22810;&#32500;&#22270;&#20613;&#31435;&#21494;&#21464;&#25442;(GFT)&#65292;&#27169;&#22411;&#23558;&#22330;&#26223;&#23646;&#24615;&#27719;&#24635;&#21040;&#19968;&#36215;&#12290;&#36890;&#36807;&#20351;&#29992;GFT&#65292;&#36710;&#36742;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20132;&#20114;&#22270;&#34987;&#36716;&#25442;&#25104;&#20026;&#35889;&#22495;&#22330;&#26223;&#34920;&#31034;&#12290;&#35813;&#26377;&#30410;&#30340;&#34920;&#31034;&#34987;&#36755;&#20837;&#21040;&#30001;&#31070;&#32463;&#32593;&#32476;&#21644;&#25551;&#36848;&#32534;&#30721;&#22120;&#32452;&#25104;&#30340;&#39044;&#27979;&#26694;&#26550;&#20013;&#12290;&#23613;&#31649;&#25152;&#25552;&#20986;&#30340;GFTNN&#27809;&#26377;&#21253;&#21547;&#20219;&#20309;&#24490;&#29615;&#20803;&#20214;&#65292;&#20294;&#22312;&#39640;&#36895;&#20844;&#36335;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#20844;&#24320;&#25968;&#25454;&#38598;highD&#21644;NGSIM&#34987;&#29992;&#20110;&#23454;&#39564;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the multidimensional Graph Fourier Transformation Neural Network (GFTNN) for long-term trajectory predictions on highways. Similar to Graph Neural Networks (GNNs), the GFTNN is a novel network architecture that operates on graph structures. While several GNNs lack discriminative power due to suboptimal aggregation schemes, the proposed model aggregates scenario properties through a powerful operation: the multidimensional Graph Fourier Transformation (GFT). The spatio-temporal vehicle interaction graph of a scenario is converted into a spectral scenario representation using the GFT. This beneficial representation is input to the prediction framework composed of a neural network and a descriptive decoder. Even though the proposed GFTNN does not include any recurrent element, it outperforms state-of-the-art models in the task of highway trajectory prediction. For experiments and evaluation, the publicly available datasets highD and NGSIM are used
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#28216;&#25103;&#20013;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#20174;&#24773;&#24863;&#22238;&#36335;&#30340;&#35270;&#35282;&#35752;&#35770;&#20102;AI&#22312;&#28216;&#25103;&#24320;&#21457;&#20013;&#25152;&#38754;&#20020;&#30340;&#19977;&#20010;&#20262;&#29702;&#25361;&#25112;&#65306;&#20154;&#24037;&#35825;&#23548;&#24773;&#24863;&#30340;&#20262;&#29702;&#30028;&#38480;&#65292;&#38544;&#31169;&#21644;&#23433;&#20840;&#28216;&#25103;&#31354;&#38388;&#20043;&#38388;&#30340;&#26435;&#34913;&#20197;&#21450;&#22312;&#28216;&#25103;&#36866;&#24212;&#20013;&#24212;&#29992;&#30340;&#26816;&#27979;&#25152;&#24102;&#26469;&#30340;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#25511;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07392</link><description>&lt;p&gt;
&#28216;&#25103;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
The Ethics of AI in Games. (arXiv:2305.07392v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#28216;&#25103;&#20013;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#20174;&#24773;&#24863;&#22238;&#36335;&#30340;&#35270;&#35282;&#35752;&#35770;&#20102;AI&#22312;&#28216;&#25103;&#24320;&#21457;&#20013;&#25152;&#38754;&#20020;&#30340;&#19977;&#20010;&#20262;&#29702;&#25361;&#25112;&#65306;&#20154;&#24037;&#35825;&#23548;&#24773;&#24863;&#30340;&#20262;&#29702;&#30028;&#38480;&#65292;&#38544;&#31169;&#21644;&#23433;&#20840;&#28216;&#25103;&#31354;&#38388;&#20043;&#38388;&#30340;&#26435;&#34913;&#20197;&#21450;&#22312;&#28216;&#25103;&#36866;&#24212;&#20013;&#24212;&#29992;&#30340;&#26816;&#27979;&#25152;&#24102;&#26469;&#30340;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#28216;&#25103;&#26159;&#20154;&#26426;&#20132;&#20114;&#26368;&#20016;&#23500;&#21644;&#26368;&#21463;&#27426;&#36814;&#30340;&#24418;&#24335;&#20043;&#19968;&#65292;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#21644;&#24773;&#24863;&#26041;&#38754;&#30340;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28216;&#25103;&#19994;&#36880;&#28176;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#65292;&#19968;&#31995;&#21015;&#20262;&#29702;&#38382;&#39064;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35270;&#39057;&#28216;&#25103;&#39046;&#22495;&#20013;&#36804;&#20170;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#35752;&#35770;&#12290;&#30001;&#20110;&#32570;&#20047;&#20851;&#20110;&#28216;&#25103;&#20013;&#24212;&#29992;AI&#30340;&#20262;&#29702;&#32508;&#36848;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#24182;&#20174;&#24773;&#24863;&#22238;&#36335;&#30340;&#25972;&#20307;&#35270;&#35282;&#35752;&#35770;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#20262;&#29702;&#32771;&#34385;&#12290;&#36890;&#36807;&#36825;&#20010;&#22238;&#36335;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#22312;&#35270;&#39057;&#28216;&#25103;&#24320;&#21457;&#20013;&#25152;&#38754;&#20020;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#24773;&#24863;&#21796;&#36215;&#20984;&#26174;&#20102;&#20154;&#24037;&#35825;&#23548;&#24773;&#24863;&#30340;&#20262;&#29702;&#30028;&#38480;&#65307;&#24863;&#30693;&#23637;&#31034;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#28216;&#25103;&#31354;&#38388;&#20043;&#38388;&#30340;&#26435;&#34913;&#65307;&#32780;&#26816;&#27979;&#65292;&#22312;&#28216;&#25103;&#36866;&#24212;&#20013;&#30340;&#24212;&#29992;&#65292;&#23545;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#25511;&#21046;&#36896;&#25104;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video games are one of the richest and most popular forms of human-computer interaction and, hence, their role is critical for our understanding of human behaviour and affect at a large scale. As artificial intelligence (AI) tools are gradually adopted by the game industry a series of ethical concerns arise. Such concerns, however, have so far not been extensively discussed in a video game context. Motivated by the lack of a comprehensive review of the ethics of AI as applied to games, we survey the current state of the art in this area and discuss ethical considerations of these systems from the holistic perspective of the affective loop. Through the components of this loop, we study the ethical challenges that AI faces in video game development. Elicitation highlights the ethical boundaries of artificially induced emotions; sensing showcases the trade-off between privacy and safe gaming spaces; and detection, as utilised during in-game adaptation, poses challenges to transparency and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.07375</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#26029;&#22120;&#21527;&#65311;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#20247;&#22810;NLP&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20294;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#20294;&#26159;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#30340;&#24187;&#35273;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20851;&#31995;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#30340;&#25253;&#21578;&#20559;&#35265;&#65292;&#20197;&#21450;ChatGPT&#30340;&#21319;&#32423;&#36807;&#31243;&#65292;&#22914;RLHF&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#36825;&#31181;&#22240;&#26524;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#22312;&#25552;&#31034;&#20013;&#34920;&#36798;&#22240;&#26524;&#27010;&#24565;&#30340;&#35789;&#35821;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#23553;&#38381;&#25552;&#31034;&#27604;&#24320;&#25918;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#21477;&#23376;&#20013;&#30340;&#20107;&#20214;&#65292;ChatGPT&#25797;&#38271;&#25429;&#25417;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#20102;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#20998;&#31867;&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#30005;&#36335;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#38382;&#31572;&#31995;&#32479;&#25972;&#20307;&#36136;&#37327;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.07374</link><description>&lt;p&gt;
&#20511;&#21161;&#28145;&#24230;&#30005;&#36335;&#25552;&#39640;&#37327;&#23376;&#38382;&#31572;&#31995;&#32479;&#30340;&#36136;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Implications of Deep Circuits in Improving Quality of Quantum Question Answering. (arXiv:2305.07374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#20102;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#20998;&#31867;&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#30005;&#36335;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#38382;&#31572;&#31995;&#32479;&#25972;&#20307;&#36136;&#37327;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#19968;&#30452;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#29305;&#24615;&#65292;&#37319;&#29992;&#22522;&#20110;&#37327;&#23376;&#30340;&#20998;&#31867;&#22120;&#31639;&#27861;&#65288;QSVM&#21644;VQC&#65289;&#23545;SelQA&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#31867;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#30005;&#36335;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#38382;&#39064;&#20998;&#31867;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#38382;&#31572;&#31995;&#32479;&#25972;&#20307;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Answering (QA) has proved to be an arduous challenge in the area of natural language processing (NLP) and artificial intelligence (AI). Many attempts have been made to develop complete solutions for QA as well as improving significant sub-modules of the QA systems to improve the overall performance through the course of time. Questions are the most important piece of QA, because knowing the question is equivalent to knowing what counts as an answer (Harrah in Philos Sci, 1961 [1]). In this work, we have attempted to understand questions in a better way by using Quantum Machine Learning (QML). The properties of Quantum Computing (QC) have enabled classically intractable data processing. So, in this paper, we have performed question classification on questions from two classes of SelQA (Selection-based Question Answering) dataset using quantum-based classifier algorithms-quantum support vector machine (QSVM) and variational quantum classifier (VQC) from Qiskit (Quantum Informati
&lt;/p&gt;</description></item><item><title>S-REINFORCE&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#22120;&#29983;&#25104;&#25968;&#23383;&#21644;&#31526;&#21495;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#21487;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.07367</link><description>&lt;p&gt;
S-REINFORCE&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning. (arXiv:2305.07367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07367
&lt;/p&gt;
&lt;p&gt;
S-REINFORCE&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#22120;&#29983;&#25104;&#25968;&#23383;&#21644;&#31526;&#21495;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#21487;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;S-REINFORCE&#65292;&#26088;&#22312;&#20026;&#21160;&#24577;&#20915;&#31574;&#20219;&#21153;&#29983;&#25104;&#21487;&#35299;&#37322;&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#31526;&#21495;&#22238;&#24402;&#22120;&#65288;SR&#65289;&#65292;&#20998;&#21035;&#29983;&#25104;&#25968;&#23383;&#21644;&#31526;&#21495;&#31574;&#30053;&#12290;NN&#32452;&#20214;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#29983;&#25104;&#21487;&#33021;&#25805;&#20316;&#30340;&#25968;&#23383;&#27010;&#29575;&#20998;&#24067;&#65292;&#32780;SR&#32452;&#20214;&#21017;&#25429;&#33719;&#19982;&#25805;&#20316;&#27010;&#29575;&#30456;&#20851;&#30340;&#29366;&#24577;&#38388;&#20851;&#31995;&#30340;&#20989;&#25968;&#24418;&#24335;&#12290;&#28982;&#21518;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#21033;&#29992;SR&#29983;&#25104;&#30340;&#31574;&#30053;&#34920;&#36798;&#24335;&#25913;&#36827;&#23398;&#20064;&#36807;&#31243;&#20013;&#25509;&#25910;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#20302;&#21644;&#39640;&#32500;&#34892;&#21160;&#31354;&#38388;&#30340;&#21508;&#31181;&#21160;&#24577;&#20915;&#31574;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#25552;&#20986;&#30340;S-REINFORCE&#31639;&#27861;&#65292;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#23454;&#29616;&#21487;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;NN&#21644;SR&#30340;&#20248;&#21183;&#65292;S-REINFORCE&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel RL algorithm, S-REINFORCE, which is designed to generate interpretable policies for dynamic decision-making tasks. The proposed algorithm leverages two types of function approximators, namely Neural Network (NN) and Symbolic Regressor (SR), to produce numerical and symbolic policies, respectively. The NN component learns to generate a numerical probability distribution over the possible actions using a policy gradient, while the SR component captures the functional form that relates the associated states with the action probabilities. The SR-generated policy expressions are then utilized through importance sampling to improve the rewards received during the learning process. We have tested the proposed S-REINFORCE algorithm on various dynamic decision-making problems with low and high dimensional action spaces, and the results demonstrate its effectiveness and impact in achieving interpretable solutions. By leveraging the strengths of both NN and SR, S-REINF
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#26469;&#23454;&#29616;&#22810;&#20540;&#25512;&#24191;&#30340;&#35268;&#33539;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#31995;&#32479;&#20013;&#26234;&#33021;&#20307;&#30340;&#24322;&#36136;&#24615;&#21644;&#21516;&#26102;&#23545;&#40784;&#22810;&#20010;&#20215;&#20540;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.07366</link><description>&lt;p&gt;
&#35268;&#33539;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#22810;&#20540;&#23545;&#40784;&#65306;&#36827;&#21270;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Value Alignment in Normative Multi-Agent System: Evolutionary Optimisation Approach. (arXiv:2305.07366v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#26469;&#23454;&#29616;&#22810;&#20540;&#25512;&#24191;&#30340;&#35268;&#33539;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#31995;&#32479;&#20013;&#26234;&#33021;&#20307;&#30340;&#24322;&#36136;&#24615;&#21644;&#21516;&#26102;&#23545;&#40784;&#22810;&#20010;&#20215;&#20540;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#23545;&#40784;&#26159;&#29992;&#20110;&#25512;&#24191;&#26576;&#31181;&#20215;&#20540;&#24182;&#30830;&#20445;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#29486;&#38480;&#20110;&#23558;&#26377;&#25928;&#30340;&#35268;&#33539;&#32435;&#20837;&#21333;&#19968;&#20215;&#20540;&#23545;&#40784;&#20013;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#26234;&#33021;&#20307;&#30340;&#24322;&#36136;&#24615;&#21644;&#21516;&#26102;&#25512;&#36827;&#21644;&#23545;&#40784;&#22810;&#20010;&#20215;&#20540;&#30340;&#35201;&#27714;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20540;&#25512;&#24191;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#29983;&#25104;&#19982;&#24322;&#36136;&#26234;&#33021;&#20307;&#21644;&#31995;&#32479;&#30340;&#22810;&#20010;&#21516;&#26102;&#20540;&#23545;&#40784;&#30340;&#26368;&#20248;&#21442;&#25968;&#35268;&#33539;&#38598;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#20010;&#22797;&#26434;&#38382;&#39064;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20351;&#29992;&#20102;&#20960;&#31181;&#36827;&#21270;&#31639;&#27861;&#26469;&#25214;&#21040;&#19968;&#32452;&#20248;&#21270;&#30340;&#35268;&#33539;&#21442;&#25968;&#65292;&#32771;&#34385;&#20102;&#20004;&#20010;&#29609;&#20855;&#31246;&#25910;&#22330;&#26223;&#30340;&#20004;&#20010;&#21644;&#20116;&#20010;&#20540;&#12290;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#32467;&#26524;&#65292;&#20197;&#23637;&#31034;&#25152;&#36873;&#36827;&#21270;&#31639;&#27861;&#23545;&#35299;&#20915;&#26041;&#26696;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value-alignment in normative multi-agent systems is used to promote a certain value and to ensure the consistent behavior of agents in autonomous intelligent systems with human values. However, the current literature is limited to incorporation of effective norms for single value alignment with no consideration of agents' heterogeneity and the requirement of simultaneous promotion and alignment of multiple values. This research proposes a multi-value promotion model that uses multi-objective evolutionary algorithms to produce the optimum parametric set of norms that is aligned with multiple simultaneous values of heterogeneous agents and the system. To understand various aspects of this complex problem, several evolutionary algorithms were used to find a set of optimised norm parameters considering two toy tax scenarios with two and five values are considered. The results are analysed from different perspectives to show the impact of a selected evolutionary algorithm on the solution, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22825;&#22478;&#25991;&#36763;&#36842;&#36716;&#25442;&#20026;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#36763;&#36842;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#28151;&#21512;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#21644;&#27010;&#29575;&#27169;&#22411;&#65292;&#31995;&#32479;&#21462;&#24471;&#20102;99.64&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07365</link><description>&lt;p&gt;
&#23454;&#29616;&#20174;&#22825;&#22478;&#25991;&#36763;&#36842;&#21040;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#36763;&#36842;&#30340;&#38899;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Transliteration between Sindhi Scripts from Devanagari to Perso-Arabic. (arXiv:2305.07365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22825;&#22478;&#25991;&#36763;&#36842;&#36716;&#25442;&#20026;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#36763;&#36842;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#28151;&#21512;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#21644;&#27010;&#29575;&#27169;&#22411;&#65292;&#31995;&#32479;&#21462;&#24471;&#20102;99.64&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#22825;&#22478;&#25991;&#36763;&#36842;&#36716;&#25442;&#20026;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#36763;&#36842;&#30340;&#36716;&#25442;&#25216;&#26415;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#20854;&#20013;&#37096;&#20998;&#25991;&#26412;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#36827;&#34892;&#36716;&#25442;&#65292;&#22914;&#26524;&#26377;&#27495;&#20041;&#65292;&#21017;&#20351;&#29992;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#35299;&#20915;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#31995;&#32479;&#24635;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;99.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we have shown a script conversion (transliteration) technique that converts Sindhi text in the Devanagari script to the Perso-Arabic script. We showed this by incorporating a hybrid approach where some part of the text is converted using a rule base and in case an ambiguity arises then a probabilistic model is used to resolve the same. Using this approach, the system achieved an overall accuracy of 99.64%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#36827;&#34892;&#32763;&#35793;/&#38899;&#35793;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#32763;&#35793;&#22823;&#22810;&#25968;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.52&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.07360</link><description>&lt;p&gt;
&#36890;&#36807;&#36866;&#24403;&#32763;&#35793;&#21629;&#21517;&#23454;&#20307;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving the Quality of Neural Machine Translation Through Proper Translation of Name Entities. (arXiv:2305.07360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#36827;&#34892;&#32763;&#35793;/&#38899;&#35793;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#32763;&#35793;&#22823;&#22810;&#25968;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.52&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#36827;&#34892;&#32763;&#35793;/&#38899;&#35793;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#23545;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#21363;&#20154;&#21517;&#12289;&#22320;&#21517;&#21644;&#32452;&#32455;&#21517;&#12290;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#22320;&#32763;&#35793;&#22823;&#22810;&#25968;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#23545;&#20110;&#20154;&#21517;&#65292;&#20934;&#30830;&#29575;&#20026;99.86&#65285;&#65292;&#23545;&#20110;&#22320;&#21517;&#65292;&#20934;&#30830;&#29575;&#20026;99.63&#65285;&#65292;&#23545;&#20110;&#32452;&#32455;&#21517;&#65292;&#20934;&#30830;&#29575;&#20026;99.05&#65285;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#31995;&#32479;&#30340;&#20934;&#30830;&#29575;&#20026;99.52&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we have shown a method of improving the quality of neural machine translation by translating/transliterating name entities as a preprocessing step. Through experiments we have shown the performance gain of our system. For evaluation we considered three types of name entities viz person names, location names and organization names. The system was able to correctly translate mostly all the name entities. For person names the accuracy was 99.86%, for location names the accuracy was 99.63% and for organization names the accuracy was 99.05%. Overall, the accuracy of the system was 99.52%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#30446;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#20173;&#38656;&#20811;&#26381;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.07348</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#30446;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#32508;&#36848;&#65306;&#24403;&#21069;&#29366;&#24577;&#12289;&#38480;&#21046;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects. (arXiv:2305.07348v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#30446;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#20173;&#38656;&#20811;&#26381;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#31639;&#19981;&#37197;&#21512;&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#36712;&#33258;&#21160;&#35270;&#35273;&#31995;&#32479;&#30340;&#37096;&#32626;&#65292;&#20854;&#24212;&#29992;&#33539;&#22260;&#20174;&#36712;&#36947;&#32500;&#20462;&#21040;&#22826;&#31354;&#30862;&#29255;&#28165;&#38500;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36235;&#21183;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24037;&#20316;&#22312;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20294;&#26159;&#23613;&#31649;&#22312;&#30740;&#31350;&#38454;&#27573;&#33719;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20173;&#23384;&#22312;&#38459;&#27490;&#36825;&#31181;&#26041;&#27861;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#37096;&#32626;&#36825;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#31639;&#27861;&#20173;&#28982;&#21463;&#21040;&#23569;&#37327;&#30740;&#31350;&#65292;&#32780;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#24615;&#33021;&#19979;&#38477;&#20173;&#28982;&#38656;&#35201;&#20943;&#36731;&#12290;&#26412;&#25991;&#20027;&#35201;&#30446;&#30340;&#26159;&#20840;&#38754;&#25551;&#36848;&#24403;&#21069;&#22522;&#20110;DL&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36741;&#21161;&#30830;&#23450;&#26377;&#25928;&#37096;&#32626;DL&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the pose of an uncooperative spacecraft is an important computer vision problem for enabling the deployment of automatic vision-based systems in orbit, with applications ranging from on-orbit servicing to space debris removal. Following the general trend in computer vision, more and more works have been focusing on leveraging Deep Learning (DL) methods to address this problem. However and despite promising research-stage results, major challenges preventing the use of such methods in real-life missions still stand in the way. In particular, the deployment of such computation-intensive algorithms is still under-investigated, while the performance drop when training on synthetic and testing on real images remains to mitigate. The primary goal of this survey is to describe the current DL-based methods for spacecraft pose estimation in a comprehensive manner. The secondary goal is to help define the limitations towards the effective deployment of DL-based spacecraft pose estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35268;&#33539;&#21270;&#26041;&#27861;CrossConST&#65292;&#29992;&#20110;&#25913;&#36827;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CrossConST&#21487;&#20197;&#32553;&#23567;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07310</link><description>&lt;p&gt;
&#21033;&#29992;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35268;&#33539;&#21270;&#25913;&#36827;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization. (arXiv:2305.07310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35268;&#33539;&#21270;&#26041;&#27861;CrossConST&#65292;&#29992;&#20110;&#25913;&#36827;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CrossConST&#21487;&#20197;&#32553;&#23567;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#26377;&#24456;&#24378;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#23545;&#20043;&#38388;&#36827;&#34892;&#30452;&#25509;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#20174;&#26377;&#30417;&#30563;&#26041;&#21521;&#21040;&#38646;&#26679;&#26412;&#26041;&#21521;&#30340;&#33391;&#22909;&#36716;&#31227;&#24615;&#33021;&#65292;&#38656;&#35201;&#35753;&#22810;&#35821;&#35328;NMT&#27169;&#22411;&#23398;&#20064;&#21040;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35268;&#33539;&#21270;CrossConST&#65292;&#20197;&#24357;&#21512;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#65292;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#32763;&#35793;&#24615;&#33021;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;CrossConST&#38544;&#21547;&#22320;&#26368;&#22823;&#21270;&#20102;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#22312;&#20302;&#36164;&#28304;&#21644;&#39640;&#36164;&#28304;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#25552;&#21319;&#12290;&#23454;&#39564;&#20998;&#26512;&#36824;&#35777;&#26126;&#65292;CrossConST&#21487;&#20197;&#32553;&#23567;&#21477;&#23376;&#34920;&#31034;&#24046;&#36317;&#24182;&#26356;&#22909;&#22320;&#23545;&#40784;&#34920;&#31034;&#31354;&#38388;&#12290;&#37492;&#20110;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;CrossConST&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;&#20854;&#20182;&#22810;&#35821;&#35328;NMT&#27169;&#22411;&#20013;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#38646;&#26679;&#26412;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multilingual neural machine translation (NMT) model has a promising capability of zero-shot translation, where it could directly translate between language pairs unseen during training. For good transfer performance from supervised directions to zero-shot directions, the multilingual NMT model is expected to learn universal representations across different languages. This paper introduces a cross-lingual consistency regularization, CrossConST, to bridge the representation gap among different languages and boost zero-shot translation performance. The theoretical analysis shows that CrossConST implicitly maximizes the probability distribution for zero-shot translation, and the experimental results on both low-resource and high-resource benchmarks show that CrossConST consistently improves the translation performance. The experimental analysis also proves that CrossConST could close the sentence representation gap and better align the representation space. Given the universality and s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP-Count&#65292;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#25991;&#26412;&#24341;&#23548;&#30340;&#29289;&#20307;&#35745;&#25968;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#19969;-&#25991;&#26412;&#23545;&#27604;&#25439;&#22833;&#21644;&#20998;&#23618;&#30340;patch-text&#20132;&#20114;&#27169;&#22359;&#65292;&#33719;&#24471;&#20102;&#39640;&#25928;&#30340;&#23494;&#38598;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07304</link><description>&lt;p&gt;
CLIP-Count&#65306;&#38754;&#21521;&#25991;&#26412;&#24341;&#23548;&#19979;&#38646;&#26679;&#26412;&#29289;&#20307;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
CLIP-Count: Towards Text-Guided Zero-Shot Object Counting. (arXiv:2305.07304v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP-Count&#65292;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#25991;&#26412;&#24341;&#23548;&#30340;&#29289;&#20307;&#35745;&#25968;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#19969;-&#25991;&#26412;&#23545;&#27604;&#25439;&#22833;&#21644;&#20998;&#23618;&#30340;patch-text&#20132;&#20114;&#27169;&#22359;&#65292;&#33719;&#24471;&#20102;&#39640;&#25928;&#30340;&#23494;&#38598;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;-&#22270;&#20687;&#21305;&#37197;&#33021;&#21147;&#65292;&#21487;&#36716;&#31227;&#21040;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#20110;&#30446;&#26631;&#35745;&#25968;&#8212;&#8212;&#20272;&#35745;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;&#25968;&#37327;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#39318;&#27425;&#25506;&#32034;&#65292;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36716;&#31227;&#29992;&#20110;&#26080;&#31867;&#21035;&#20559;&#35265;&#30340;&#29289;&#20307;&#35745;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP-Count&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#31243;&#65292;&#23427;&#36890;&#36807;&#38646;&#26679;&#26412;&#30340;&#25991;&#26412;&#24341;&#23548;&#65292;&#20026;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#20272;&#35745;&#23494;&#24230;&#22270;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#29305;&#23450;&#23545;&#35937;&#31867;&#21035;&#19978;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#20026;&#20102;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#21644;&#23494;&#38598;&#22270;&#20687;&#29305;&#24449;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#19969;-&#25991;&#26412;&#23545;&#27604;&#25439;&#22833;&#65292;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#26377;&#29992;&#30340;&#34917;&#19969;&#32423;&#22270;&#20687;&#34920;&#31034;&#20197;&#36827;&#34892;&#23494;&#38598;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;&#30340;patch-text&#20132;&#20114;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#32423;&#21035;&#19978;&#20256;&#36882;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in visual-language models have shown remarkable zero-shot text-image matching ability that is transferable to down-stream tasks such as object detection and segmentation. However, adapting these models for object counting, which involves estimating the number of objects in an image, remains a formidable challenge. In this study, we conduct the first exploration of transferring visual-language models for class-agnostic object counting. Specifically, we propose CLIP-Count, a novel pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner, without requiring any finetuning on specific object classes. To align the text embedding with dense image features, we introduce a patch-text contrastive loss that guides the model to learn informative patch-level image representations for dense prediction. Moreover, we design a hierarchical patch-text interaction module that propagates semantic information across different resolution level
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;GPRL&#65292;&#20351;&#29992;&#39640;&#26031;&#20808;&#39564;&#35843;&#25972;&#23884;&#22871;&#36793;&#30028;&#26631;&#35760;&#30340;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#19977;&#20803;&#32452;&#65292;&#26080;&#38656;&#32771;&#34385;&#37329;&#26631;&#31614;&#20013;&#30340;&#23454;&#20307;&#39034;&#24207;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#20110;&#20197;&#21069;&#30340;&#23884;&#22871;NER&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07266</link><description>&lt;p&gt;
&#39640;&#26031;&#20808;&#39564;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition. (arXiv:2305.07266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07266
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;GPRL&#65292;&#20351;&#29992;&#39640;&#26031;&#20808;&#39564;&#35843;&#25972;&#23884;&#22871;&#36793;&#30028;&#26631;&#35760;&#30340;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#19977;&#20803;&#32452;&#65292;&#26080;&#38656;&#32771;&#34385;&#37329;&#26631;&#31614;&#20013;&#30340;&#23454;&#20307;&#39034;&#24207;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#20110;&#20197;&#21069;&#30340;&#23884;&#22871;NER&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#32463;&#36807;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65292;&#26368;&#36817;&#65292;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#30340;&#23454;&#29992;&#24615;&#21644;&#38590;&#24230;&#12290;&#29616;&#26377;&#30340;&#23884;&#22871;NER&#20316;&#21697;&#24573;&#30053;&#20102;&#23884;&#22871;&#23454;&#20307;&#30340;&#35782;&#21035;&#39034;&#24207;&#21644;&#36793;&#30028;&#20301;&#32622;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;seq2seq&#27169;&#22411;GPRL&#65292;&#23558;&#23884;&#22871;NER&#20219;&#21153;&#24418;&#25104;&#19968;&#20010;&#23454;&#20307;&#19977;&#20803;&#32452;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#12290;GPRL&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#19977;&#20803;&#32452;&#65292;&#23558;&#37329;&#26631;&#31614;&#20013;&#30340;&#23454;&#20307;&#39034;&#24207;&#35299;&#32806;&#65292;&#24182;&#26399;&#26395;&#36890;&#36807;&#35797;&#38169;&#23398;&#20064;&#21512;&#29702;&#30340;&#23454;&#20307;&#35782;&#21035;&#39034;&#24207;&#12290;&#22522;&#20110;&#23884;&#22871;&#23454;&#20307;&#36793;&#30028;&#36317;&#31163;&#30340;&#32479;&#35745;&#65292;GPRL&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#26031;&#20808;&#39564;&#65292;&#20195;&#34920;&#23884;&#22871;&#23454;&#20307;&#20043;&#38388;&#30340;&#36793;&#30028;&#36317;&#31163;&#20998;&#24067;&#65292;&#24182;&#35843;&#25972;&#23884;&#22871;&#36793;&#30028;&#26631;&#35760;&#30340;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#19977;&#20010;&#23884;&#22871;NER&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GPRL&#20248;&#20110;&#20197;&#21069;&#30340;&#23884;&#22871;NER&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a well and widely studied task in natural language processing. Recently, the nested NER has attracted more attention since its practicality and difficulty. Existing works for nested NER ignore the recognition order and boundary position relation of nested entities. To address these issues, we propose a novel seq2seq model named GPRL, which formulates the nested NER task as an entity triplet sequence generation process. GPRL adopts the reinforcement learning method to generate entity triplets decoupling the entity order in gold labels and expects to learn a reasonable recognition order of entities via trial and error. Based on statistics of boundary distance for nested entities, GPRL designs a Gaussian prior to represent the boundary distance distribution between nested entities and adjust the output probability distribution of nested boundary tokens. Experiments on three nested NER datasets demonstrate that GPRL outperforms previous nested NER models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20248;&#21270;&#32047;&#31215;&#22870;&#21169;&#20998;&#20301;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;QPO&#21644;&#20854;&#21464;&#20307;QPPO&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25511;&#21046;&#21160;&#20316;&#30340;&#31574;&#30053;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07248</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#20004;&#26102;&#38388;&#26631;&#24230;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantile-Based Deep Reinforcement Learning using Two-Timescale Policy Gradient Algorithms. (arXiv:2305.07248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20248;&#21270;&#32047;&#31215;&#22870;&#21169;&#20998;&#20301;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;QPO&#21644;&#20854;&#21464;&#20307;QPPO&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25511;&#21046;&#21160;&#20316;&#30340;&#31574;&#30053;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26088;&#22312;&#20248;&#21270;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20248;&#21270;&#32047;&#31215;&#22870;&#21169;&#20998;&#20301;&#25968;&#30340;RL&#35774;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25511;&#21046;&#21160;&#20316;&#30340;&#31574;&#30053;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Quantile-Based Policy Optimization&#65288;QPO&#65289;&#30340;&#26032;&#22411;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;Quantile-Based Proximal Policy Optimization&#65288;QPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37327;&#21270;&#30446;&#26631;&#30340;&#28145;&#24230;RL&#38382;&#39064;&#12290;QPO&#20351;&#29992;&#20004;&#20010;&#32806;&#21512;&#36845;&#20195;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#26631;&#24230;&#19978;&#26356;&#26032;&#20998;&#20301;&#25968;&#21644;&#31574;&#30053;&#21442;&#25968;&#65292;&#32780;QPPO&#26159;QPO&#30340;&#31163;&#32447;&#29256;&#26412;&#65292;&#20801;&#35768;&#22312;&#19968;&#20010;&#27169;&#25311;&#22238;&#21512;&#20013;&#22810;&#27425;&#26356;&#26032;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20998;&#20301;&#25968;&#26631;&#20934;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical reinforcement learning (RL) aims to optimize the expected cumulative reward. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative reward. We parameterize the policy controlling actions by neural networks, and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) for solving deep RL problems with quantile objectives. QPO uses two coupled iterations running at different timescales for simultaneously updating quantiles and policy parameters, whereas QPPO is an off-policy version of QPO that allows multiple updates of parameters during one simulation episode, leading to improved algorithm efficiency. Our numerical results indicate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#23376;&#65292;&#31216;&#20026;&#24369;&#36951;&#24536;&#65292;&#19982;&#26631;&#20934;&#36951;&#24536;&#30456;&#20114;&#23545;&#20598;&#65292;&#24182;&#20849;&#21516;&#23637;&#31034;&#20102;&#36951;&#24536;&#31639;&#23376;&#30340;&#26032;&#30340;&#26356;&#32479;&#19968;&#30340;&#35270;&#35282;&#12290;&#20108;&#32773;&#37117;&#26159;&#22522;&#20110;&#34164;&#21547;&#21644;&#25512;&#29702;&#32780;&#38750;&#27169;&#22411;&#35770;&#35821;&#20041;&#65292;&#23481;&#26131;&#37319;&#29992;Ackermman&#24341;&#29702;&#21644;&#20854;&#19981;&#21160;&#28857;&#27010;&#25324;&#30340;&#31639;&#27861;&#35270;&#35282;&#36827;&#34892;&#25551;&#36848;&#21644;&#36816;&#29992;&#12290;&#23450;&#37327;&#25551;&#36848;&#20102;&#26631;&#20934;&#36951;&#24536;&#21644;&#26368;&#24378;&#24517;&#35201;&#26465;&#20214;&#20043;&#38388;&#30340;&#24378;&#20851;&#31995;&#20197;&#21450;&#24369;&#36951;&#24536;&#21644;&#26368;&#24369;&#20805;&#20998;&#26465;&#20214;&#20043;&#38388;&#30340;&#24378;&#24418;&#24335;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.07233</link><description>&lt;p&gt;
&#22312;&#26368;&#24369;&#20805;&#20998;&#26465;&#20214;&#21644;&#26368;&#24378;&#24517;&#35201;&#26465;&#20214;&#30340;&#32972;&#26223;&#19979;&#30340;&#21452;&#36951;&#24536;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Dual Forgetting Operators in the Context of Weakest Sufficient and Strongest Necessary Conditions. (arXiv:2305.07233v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#23376;&#65292;&#31216;&#20026;&#24369;&#36951;&#24536;&#65292;&#19982;&#26631;&#20934;&#36951;&#24536;&#30456;&#20114;&#23545;&#20598;&#65292;&#24182;&#20849;&#21516;&#23637;&#31034;&#20102;&#36951;&#24536;&#31639;&#23376;&#30340;&#26032;&#30340;&#26356;&#32479;&#19968;&#30340;&#35270;&#35282;&#12290;&#20108;&#32773;&#37117;&#26159;&#22522;&#20110;&#34164;&#21547;&#21644;&#25512;&#29702;&#32780;&#38750;&#27169;&#22411;&#35770;&#35821;&#20041;&#65292;&#23481;&#26131;&#37319;&#29992;Ackermman&#24341;&#29702;&#21644;&#20854;&#19981;&#21160;&#28857;&#27010;&#25324;&#30340;&#31639;&#27861;&#35270;&#35282;&#36827;&#34892;&#25551;&#36848;&#21644;&#36816;&#29992;&#12290;&#23450;&#37327;&#25551;&#36848;&#20102;&#26631;&#20934;&#36951;&#24536;&#21644;&#26368;&#24378;&#24517;&#35201;&#26465;&#20214;&#20043;&#38388;&#30340;&#24378;&#20851;&#31995;&#20197;&#21450;&#24369;&#36951;&#24536;&#21644;&#26368;&#24369;&#20805;&#20998;&#26465;&#20214;&#20043;&#38388;&#30340;&#24378;&#24418;&#24335;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#26159;&#30693;&#35782;&#34920;&#31034;&#21644;&#33258;&#21160;&#25512;&#29702;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#12290; [Lin&#21644;Reiter'94]&#20013;&#25551;&#36848;&#30340;&#26631;&#20934;&#36951;&#24536;&#31639;&#23376;&#22522;&#20110;&#27169;&#22411;&#35770;&#35821;&#20041;&#65292;&#20027;&#35201;&#20851;&#27880;&#21629;&#39064;&#24773;&#20917;&#65292;&#24320;&#21019;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#23376;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#23376;&#65292;&#31216;&#20026;&#24369;&#36951;&#24536;&#65292;&#19982;&#26631;&#20934;&#36951;&#24536;&#26159;&#21452;&#37325;&#30340;&#65292;&#20108;&#32773;&#20849;&#21516;&#23637;&#31034;&#20102;&#36951;&#24536;&#31639;&#23376;&#30340;&#26032;&#30340;&#26356;&#32479;&#19968;&#30340;&#35270;&#35282;&#12290;&#24369;&#36951;&#24536;&#31639;&#23376;&#21644;&#26631;&#20934;&#36951;&#24536;&#31639;&#23376;&#37117;&#26159;&#22522;&#20110;&#34164;&#21547;&#21644;&#25512;&#29702;&#32780;&#38750;&#27169;&#22411;&#35770;&#35821;&#20041;&#26469;&#25551;&#36848;&#12290;&#36825;&#33258;&#28982;&#22320;&#24341;&#20986;&#20102;&#22522;&#20110;&#37327;&#35789;&#28040;&#38500;&#21644;&#20351;&#29992;Ackermman&#24341;&#29702;&#21450;&#20854;&#19981;&#21160;&#28857;&#27010;&#25324;&#30340;&#26377;&#29992;&#30340;&#31639;&#27861;&#35270;&#35282;&#12290;&#26631;&#20934;&#36951;&#24536;&#21644;&#26368;&#24378;&#24517;&#35201;&#26465;&#20214;&#20043;&#38388;&#30340;&#24378;&#20851;&#31995;&#20197;&#21450;&#24369;&#36951;&#24536;&#21644;&#26368;&#24369;&#20805;&#20998;&#26465;&#20214;&#20043;&#38388;&#30340;&#24378;&#24418;&#24335;&#20851;&#31995;&#20063;&#34987;&#26126;&#30830;&#22320;&#25551;&#36848;&#21644;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forgetting is an important concept in knowledge representation and automated reasoning with widespread applications across a number of disciplines. A standard forgetting operator, characterized in [Lin and Reiter'94] in terms of model-theoretic semantics and primarily focusing on the propositional case, opened up a new research subarea. In this paper, a new operator called weak forgetting, dual to standard forgetting, is introduced and both together are shown to offer a new more uniform perspective on forgetting operators in general. Both the weak and standard forgetting operators are characterized in terms of entailment and inference, rather than a model theoretic semantics. This naturally leads to a useful algorithmic perspective based on quantifier elimination and the use of Ackermman's Lemma and its fixpoint generalization. The strong formal relationship between standard forgetting and strongest necessary conditions and weak forgetting and weakest sufficient conditions is also char
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#25105;&#20013;&#24515;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#22810;&#27169;&#24577;&#27867;&#21270;&#38382;&#39064;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MMG-Ego4D&#65292;&#20854;&#20013;&#21253;&#21547;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#24815;&#24615;&#36816;&#21160;&#20256;&#24863;&#22120;(IMU)&#27169;&#24577;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30417;&#30563;&#21160;&#20316;&#35782;&#21035;&#21644;&#23398;&#20064;&#26032;&#30340;&#21160;&#20316;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#23545;MMG&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.07214</link><description>&lt;p&gt;
MMG-Ego4D: &#22522;&#20110;&#22810;&#27169;&#24577;&#27867;&#21270;&#30340;&#33258;&#25105;&#20013;&#24515;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition. (arXiv:2305.07214v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#25105;&#20013;&#24515;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#22810;&#27169;&#24577;&#27867;&#21270;&#38382;&#39064;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MMG-Ego4D&#65292;&#20854;&#20013;&#21253;&#21547;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#24815;&#24615;&#36816;&#21160;&#20256;&#24863;&#22120;(IMU)&#27169;&#24577;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30417;&#30563;&#21160;&#20316;&#35782;&#21035;&#21644;&#23398;&#20064;&#26032;&#30340;&#21160;&#20316;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#23545;MMG&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#25105;&#20013;&#24515;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21629;&#21517;&#20026;&#8220;&#22810;&#27169;&#24577;&#27867;&#21270;&#8221;(MMG)&#12290;MMG&#26088;&#22312;&#30740;&#31350;&#24403;&#26576;&#20123;&#27169;&#24577;&#30340;&#25968;&#25454;&#21463;&#21040;&#38480;&#21046;&#29978;&#33267;&#23436;&#20840;&#32570;&#22833;&#26102;&#65292;&#31995;&#32479;&#22914;&#20309;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30417;&#30563;&#21160;&#20316;&#35782;&#21035;&#21644;&#23398;&#20064;&#26032;&#30340;&#21160;&#20316;&#20998;&#31867;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#24443;&#24213;&#30740;&#31350;&#20102;MMG&#12290;MMG&#21253;&#25324;&#20004;&#20010;&#26032;&#30340;&#22330;&#26223;&#65292;&#26088;&#22312;&#25903;&#25345;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#32771;&#34385;&#65306;(1)&#32570;&#22833;&#27169;&#24577;&#27867;&#21270;&#65292;&#22312;&#25512;&#26029;&#26102;&#19968;&#20123;&#22312;&#35757;&#32451;&#26102;&#23384;&#22312;&#30340;&#27169;&#24577;&#32570;&#22833;&#20102;&#65292;(2)&#36328;&#27169;&#24577;&#38646;&#26679;&#26412;&#27867;&#21270;&#65292;&#22312;&#25512;&#26029;&#26102;&#21644;&#35757;&#32451;&#26102;&#30340;&#27169;&#24577;&#26159;&#19981;&#30456;&#20132;&#30340;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#35843;&#26597;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;MMG-Ego4D&#65292;&#20854;&#20013;&#21253;&#21547;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#24815;&#24615;&#36816;&#21160;&#20256;&#24863;&#22120;(IMU)&#27169;&#24577;&#30340;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28304;&#20110;Ego4D&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study a novel problem in egocentric action recognition, which we term as "Multimodal Generalization" (MMG). MMG aims to study how systems can generalize when data from certain modalities is limited or even completely missing. We thoroughly investigate MMG in the context of standard supervised action recognition and the more challenging few-shot setting for learning new action categories. MMG consists of two novel scenarios, designed to support security, and efficiency considerations in real-world applications: (1) missing modality generalization where some modalities that were present during the train time are missing during the inference time, and (2) cross-modal zero-shot generalization, where the modalities present during the inference time and the training time are disjoint. To enable this investigation, we construct a new dataset MMG-Ego4D containing data points with video, audio, and inertial motion sensor (IMU) modalities. Our dataset is derived from Ego4D data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28369;&#21160;&#31383;&#21475;&#21152;&#36895;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#31639;&#27861;&#30340;&#31181;&#32676;&#22823;&#23567;&#65292;&#20174;&#32780;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#23454;&#29616;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#23454;&#20363;&#21644;&#32422;&#26463;&#35774;&#32622;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07178</link><description>&lt;p&gt;
&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#36873;&#25321;&#30340;&#24555;&#36895;Pareto&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fast Pareto Optimization Using Sliding Window Selection. (arXiv:2305.07178v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28369;&#21160;&#31383;&#21475;&#21152;&#36895;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#31639;&#27861;&#30340;&#31181;&#32676;&#22823;&#23567;&#65292;&#20174;&#32780;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#23454;&#29616;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#23454;&#20363;&#21644;&#32422;&#26463;&#35774;&#32622;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pareto&#20248;&#21270;&#20351;&#29992;&#36827;&#21270;&#22810;&#30446;&#26631;&#31639;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#32422;&#26463;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#12290;&#20915;&#23450;&#25152;&#20351;&#29992;&#30340;&#36827;&#21270;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#31639;&#27861;&#30340;&#31181;&#32676;&#22823;&#23567;&#65292;&#20854;&#38543;&#31639;&#27861;&#36935;&#21040;&#30340;&#25240;&#34935;&#25968;&#37327;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#28369;&#21160;&#31383;&#21475;&#21152;&#36895;&#25216;&#26415;&#26469;&#21152;&#36895;&#26368;&#36817;&#24341;&#20837;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#28040;&#38500;&#20102;&#31181;&#32676;&#22823;&#23567;&#20316;&#20026;&#24433;&#21709;&#36816;&#34892;&#26102;&#38388;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#19988;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#26102;&#38388;&#20869;&#23454;&#29616;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#23545;&#32463;&#20856;&#30340;&#26368;&#22823;&#35206;&#30422;&#38382;&#39064;&#36827;&#34892;&#30340;&#23454;&#39564;&#35843;&#26597;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#28369;&#21160;&#31383;&#21475;&#25216;&#26415;&#26126;&#26174;&#23548;&#33268;&#24191;&#27867;&#30340;&#23454;&#20363;&#21644;&#32422;&#26463;&#35774;&#32622;&#30340;&#26356;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pareto optimization using evolutionary multi-objective algorithms has been widely applied to solve constrained submodular optimization problems. A crucial factor determining the runtime of the used evolutionary algorithms to obtain good approximations is the population size of the algorithms which grows with the number of trade-offs that the algorithms encounter. In this paper, we introduce a sliding window speed up technique for recently introduced algorithms. We prove that our technique eliminates the population size as a crucial factor negatively impacting the runtime and achieves the same theoretical performance guarantees as previous approaches within less computation time. Our experimental investigations for the classical maximum coverage problem confirms that our sliding window technique clearly leads to better results for a wide range of instances and constraint settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33539;&#30068;&#35770;&#35821;&#20041;&#30340;&#26041;&#27861;CatE&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#23558;&#26412;&#20307;&#20844;&#29702;&#25237;&#24433;&#21040;&#22270;&#24418;&#20013;&#65292;&#24182;&#29983;&#25104;&#26412;&#20307;&#35770;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#22312;&#30693;&#35782;&#22270;&#35889;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07163</link><description>&lt;p&gt;
CatE&#65306;&#20351;&#29992;&#33539;&#30068;&#35770;&#35821;&#20041;&#23884;&#20837;$\mathcal{ALC}$&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
CatE: Embedding $\mathcal{ALC}$ ontologies using category-theoretical semantics. (arXiv:2305.07163v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33539;&#30068;&#35770;&#35821;&#20041;&#30340;&#26041;&#27861;CatE&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#23558;&#26412;&#20307;&#20844;&#29702;&#25237;&#24433;&#21040;&#22270;&#24418;&#20013;&#65292;&#24182;&#29983;&#25104;&#26412;&#20307;&#35770;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#22312;&#30693;&#35782;&#22270;&#35889;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#20041;Web&#26412;&#20307;&#19968;&#36215;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#36981;&#24490;&#20960;&#20010;&#31574;&#30053;&#20043;&#19968;&#65292;&#20854;&#20013;&#21253;&#25324;&#23558;&#26412;&#20307;&#25237;&#24433;&#21040;&#22270;&#24418;&#32467;&#26500;&#20013;&#65292;&#24182;&#23558;&#22270;&#24418;&#23884;&#20837;&#25110;&#22522;&#20110;&#22270;&#24418;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#30340;&#22270;&#24418;&#12290;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20960;&#31181;&#23558;&#26412;&#20307;&#20844;&#29702;&#25237;&#24433;&#21040;&#22270;&#24418;&#20013;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23427;&#20204;&#21487;&#20197;&#25237;&#24433;&#30340;&#20844;&#29702;&#31867;&#22411;&#65288;&#20840;&#38754;&#24615;&#65289;&#12289;&#23427;&#20204;&#26159;&#21542;&#21487;&#36870;&#65288;&#21333;&#23556;&#24615;&#65289;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#21487;&#20197;&#24212;&#29992;&#30340;&#20219;&#21153;&#31867;&#22411;&#12290;&#36923;&#36753;&#35821;&#35328;&#30340;&#33539;&#30068;&#35770;&#35821;&#20041;&#20197;&#33539;&#30068;&#32780;&#19981;&#26159;&#38598;&#21512;&#30340;&#24418;&#24335;&#24418;&#24335;&#21270;&#35299;&#37322;&#65292;&#24182;&#19988;&#33539;&#30068;&#20855;&#26377;&#31867;&#20284;&#20110;&#22270;&#24418;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;CatE&#65292;&#23427;&#20351;&#29992;$\mathcal{ALC}$&#25551;&#36848;&#36923;&#36753;&#30340;&#33539;&#30068;&#35770;&#20844;&#24335;&#26469;&#29983;&#25104;&#26412;&#20307;&#20844;&#29702;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;CatE&#25237;&#24433;&#20855;&#26377;&#20840;&#38754;&#24615;&#21644;&#21333;&#23556;&#24615;&#65292;&#22240;&#27492;&#20811;&#26381;&#20102;&#20854;&#20182;&#22522;&#20110;&#22270;&#24418;&#26412;&#20307;&#35770;&#30340;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;CatE&#36824;&#36890;&#36807;&#23558;$\mathcal{ALC}$&#30340;&#35821;&#27861;&#32534;&#30721;&#20026;&#19968;&#31181;&#33539;&#30068;&#26469;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#26412;&#20307;&#35770;&#29983;&#25104;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#30693;&#35782;&#22270;&#35889;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;CatE&#65292;&#24182;&#34920;&#26126;&#23427;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning with Semantic Web ontologies follows several strategies, one of which involves projecting ontologies into graph structures and applying graph embeddings or graph-based machine learning methods to the resulting graphs. Several methods have been developed that project ontology axioms into graphs. However, these methods are limited in the type of axioms they can project (totality), whether they are invertible (injectivity), and how they exploit semantic information. These limitations restrict the kind of tasks to which they can be applied. Category-theoretical semantics of logic languages formalizes interpretations using categories instead of sets, and categories have a graph-like structure. We developed CatE, which uses the category-theoretical formulation of the semantics of the Description Logic $\mathcal{ALC}$ to generate a graph representation for ontology axioms. The CatE projection is total and injective, and therefore overcomes limitations of other graph-based ont
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22235;&#31181;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#39046;&#22495;&#36866;&#24212;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#20197;&#21450;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#24615;&#33021;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.07157</link><description>&lt;p&gt;
&#25506;&#32034;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25216;&#26415;&#29992;&#20110;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring Zero and Few-shot Techniques for Intent Classification. (arXiv:2305.07157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07157
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22235;&#31181;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#39046;&#22495;&#36866;&#24212;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#20197;&#21450;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#24615;&#33021;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#25552;&#20379;&#32773;&#36890;&#24120;&#38656;&#35201;&#25193;&#23637;&#21040;&#25968;&#21315;&#20010;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;&#65292;&#20854;&#20013;&#26032;&#23458;&#25143;&#32463;&#24120;&#38754;&#20020;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#22312;&#25317;&#26377;&#36825;&#20040;&#22810;&#23458;&#25143;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#65292;&#20250;&#23545;&#23384;&#20648;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#20302;&#36164;&#28304;&#38480;&#21046;&#30340;&#21046;&#32422;&#65306;1&#65289;&#39046;&#22495;&#36866;&#24212;&#65292;2&#65289;&#25968;&#25454;&#22686;&#24378;&#65292;3&#65289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38646;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#65292;&#20197;&#21450;4&#65289;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#31243;&#24230;&#19981;&#21516;&#12290;&#20351;&#29992;Flan-T5&#65288;Chang et al&#65292;2022&#65289;&#22312;T-few&#37197;&#26041;&#65288;Liu et al&#65292;2022&#65289;&#19978;&#36827;&#34892;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65292;&#21363;&#20351;&#27599;&#20010;&#24847;&#22270;&#21482;&#26377;&#19968;&#20010;&#26679;&#26412;&#65292;&#24615;&#33021;&#20063;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#24847;&#22270;&#25551;&#36848;&#25552;&#31034;LLM&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on Flan-T5 (Chang et al., 2022) yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptARC&#65292;&#38024;&#23545;ARC&#39046;&#22495;&#30340;&#25277;&#35937;&#21644;&#25512;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25277;&#35937;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.07141</link><description>&lt;p&gt;
ConceptARC&#22522;&#20934;&#65306;&#35780;&#20272;ARC&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain. (arXiv:2305.07141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptARC&#65292;&#38024;&#23545;ARC&#39046;&#22495;&#30340;&#25277;&#35937;&#21644;&#25512;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25277;&#35937;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#25104;&#21644;&#25277;&#35937;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20851;&#38190;&#65292;&#20294;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#27424;&#32570;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#20851;&#20110;&#27010;&#24565;&#25277;&#35937;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#29702;&#24819;&#21270;&#30340;&#39046;&#22495;&#65292;&#22914;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#21644;Bongard&#38382;&#39064;&#65292;&#20294;&#21363;&#20351;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25104;&#21151;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#29702;&#35299;&#24773;&#20917;&#20063;&#24456;&#23569;&#34987;&#35780;&#20272;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#38024;&#23545;&#25277;&#35937;&#21644;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;ARC&#65289;&#30340;&#28145;&#20837;&#35780;&#20272;&#22522;&#20934;&#65292;ARC&#26159;Chollet [2019]&#24320;&#21457;&#30340;&#19968;&#32452;&#23569;&#37327;&#25277;&#35937;&#21644;&#31867;&#27604;&#38382;&#39064;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;ConceptARC&#30340;&#26032;&#30340;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;ARC&#22522;&#20934;&#65292;&#23427;&#22312;&#35768;&#22810;&#22522;&#26412;&#31354;&#38388;&#21644;&#35821;&#20041;&#27010;&#24565;&#19978;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#25277;&#35937;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#19982;&#21407;&#22987;&#30340;ARC&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;ConceptARC&#29305;&#21035;&#22260;&#32469;&#8220;&#27010;&#24565;&#32452;&#8221;&#36827;&#34892;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture.  In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around "concept groups" -- se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;k-&#21311;&#21517;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#33021;&#28304;&#28040;&#32791;&#36739;&#23569;&#65292;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#33021;&#28304;&#28040;&#32791;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07116</link><description>&lt;p&gt;
k-&#21311;&#21517;&#21644;&#21512;&#25104;&#25968;&#25454;&#25216;&#26415;&#30340;&#33021;&#37327;&#25104;&#26412;&#21644;&#26426;&#22120;&#23398;&#20064;&#20934;&#30830;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques. (arXiv:2305.07116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;k-&#21311;&#21517;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#33021;&#28304;&#28040;&#32791;&#36739;&#23569;&#65292;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#33021;&#28304;&#28040;&#32791;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#19982;&#38544;&#31169;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#20851;&#30340;&#24840;&#21457;&#22686;&#38271;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#27431;&#30431;&#39041;&#24067;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#24182;&#25215;&#35834;&#20102;&#32511;&#33394;&#21327;&#35758;&#12290;&#22823;&#37327;&#30740;&#31350;&#25506;&#31350;&#20102;&#36816;&#29992;&#21311;&#21517;&#25968;&#25454;&#38598;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#25928;&#21644;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#25506;&#31350;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;k-&#21311;&#21517;&#12290;&#30001;&#20110;&#21512;&#25104;&#25968;&#25454;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#27492;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#20934;&#30830;&#24615;&#65306;a&#65289;&#23558;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#24212;&#29992;&#20110;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;b&#65289;&#22312;&#30456;&#20851;&#38544;&#31169;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65306;k-&#21311;&#21517;&#21270;&#65288;&#20351;&#29992;&#27867;&#21270;&#21644;&#25233;&#21046;&#65289;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#21450;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#27599;&#20010;&#27169;&#22411;&#37117;&#22312;&#27599;&#20010;&#38544;&#31169;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32463;&#36807;k-&#21311;&#21517;&#21270;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#28040;&#32791;&#30340;&#33021;&#37327;&#36739;&#23569;&#65292;&#19982;&#22312;&#38750;&#21311;&#21517;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;k-&#21311;&#21517;&#21270;&#36807;&#31243;&#20013;&#28040;&#32791;&#30340;&#33021;&#37327;&#38750;&#24120;&#21487;&#35266;&#65292;&#22312;&#35780;&#20272;&#20854;&#26377;&#29992;&#24615;&#26102;&#24517;&#39035;&#23558;&#20854;&#32771;&#34385;&#22312;&#20869;&#12290;&#21512;&#25104;&#25968;&#25454;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#22312;&#28040;&#32791;&#26356;&#23569;&#33021;&#28304;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#38750;&#21311;&#21517;&#21270;&#25968;&#25454;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on both the energy consumption and accuracy of the machine learning models, focusing on k-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on k-a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25513;&#27169;&#24341;&#23548;&#19979;&#30340;&#35270;&#35273;Transformer&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#32454;&#31890;&#24230;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#25429;&#25417;&#26368;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#24046;&#24322;&#21644;&#24573;&#30053;&#19981;&#30456;&#20851;&#21306;&#22495;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.07102</link><description>&lt;p&gt;
&#26174;&#33879;&#25513;&#27169;&#24341;&#23548;&#19979;&#30340;&#35270;&#35273;Transformer&#29992;&#20110;&#32454;&#31890;&#24230;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Salient Mask-Guided Vision Transformer for Fine-Grained Classification. (arXiv:2305.07102v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07102
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25513;&#27169;&#24341;&#23548;&#19979;&#30340;&#35270;&#35273;Transformer&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#32454;&#31890;&#24230;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#25429;&#25417;&#26368;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#24046;&#24322;&#21644;&#24573;&#30053;&#19981;&#30456;&#20851;&#21306;&#22495;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#20854;&#20219;&#21153;&#26159;&#22312;&#20122;&#31867;&#21035;&#20013;&#33258;&#21160;&#35782;&#21035;&#23545;&#35937;&#12290;&#20854;&#20013;&#20027;&#35201;&#30340;&#22256;&#38590;&#26159;&#25429;&#25417;&#37027;&#20123;&#20165;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#20294;&#22312;&#31867;&#21035;&#38388;&#26377;&#26368;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#24046;&#24322;&#12290;&#26368;&#36817;&#65292;&#37319;&#29992;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#30340;&#26041;&#27861;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#36890;&#24120;&#36890;&#36807;&#36816;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20854;&#20182;&#32791;&#36153;&#36164;&#28304;&#30340;&#25216;&#26415;&#26469;&#21306;&#20998;&#20855;&#26377;&#28508;&#22312;&#21306;&#21035;&#24615;&#30340;&#21306;&#22495;&#65292;&#32780;&#24573;&#30053;&#20854;&#20313;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21482;&#20381;&#36182;&#20869;&#22312;&#30340;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#65292;&#22312;&#26377;&#25928;&#32858;&#28966;&#30495;&#27491;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#21306;&#22495;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#22256;&#38590;&#65292;&#23548;&#33268;&#20998;&#31867;&#20196;&#29260;&#21487;&#33021;&#20250;&#20174;&#19981;&#37325;&#35201;&#30340;&#32972;&#26223;&#21306;&#22495;&#27719;&#38598;&#20840;&#23616;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#28857;&#65292;&#20998;&#31867;&#22120;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#26368;&#26377;&#24110;&#21161;&#30340;&#31867;&#38388;&#21306;&#21035;&#29305;&#24449;&#65292;&#22240;&#20026;&#20854;&#20182;&#26080;&#20851;&#20294;&#29420;&#29305;&#30340;&#29305;&#24449;&#36890;&#24120;&#20250;&#21463;&#21040;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained visual classification (FGVC) is a challenging computer vision problem, where the task is to automatically recognise objects from subordinate categories. One of its main difficulties is capturing the most discriminative inter-class variances among visually similar classes. Recently, methods with Vision Transformer (ViT) have demonstrated noticeable achievements in FGVC, generally by employing the self-attention mechanism with additional resource-consuming techniques to distinguish potentially discriminative regions while disregarding the rest. However, such approaches may struggle to effectively focus on truly discriminative regions due to only relying on the inherent self-attention mechanism, resulting in the classification token likely aggregating global information from less-important background patches. Moreover, due to the immense lack of the datapoints, classifiers may fail to find the most helpful inter-class distinguishing features, since other unrelated but distinc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#19968;&#31181;&#21516;&#26102;&#23558;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#21644;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#32467;&#21512;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#21033;&#29992;&#20960;&#20309;&#20449;&#24687;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07100</link><description>&lt;p&gt;
$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
$\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks. (arXiv:2305.07100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#19968;&#31181;&#21516;&#26102;&#23558;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#21644;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#32467;&#21512;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#21033;&#29992;&#20960;&#20309;&#20449;&#24687;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#22312;&#20960;&#20309;&#22270;&#24418;&#21644;&#28857;&#20113;&#19978;&#30340;&#26041;&#27861;&#65292;&#20854;&#31561;&#21464;&#20110;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#21453;&#23556;&#12290;EMPSNs&#21487;&#20197;&#23398;&#20064;&#22312;&#22270;&#24418;&#20013;&#30340;&#39640;&#32500;&#21333;&#32431;&#38754;&#65288;&#22914;&#19977;&#35282;&#24418;&#65289;&#65292;&#24182;&#20197;$\mathrm{E}(n)$&#31561;&#21464;&#26041;&#24335;&#21033;&#29992;&#26356;&#39640;&#32500;&#21333;&#32431;&#20307;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;EMPSNs&#21516;&#26102;&#23558;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#26356;&#21152;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#39046;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#20013;&#21253;&#21547;&#20960;&#20309;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EMPSNs&#21487;&#20197;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#21333;&#29420;&#20351;&#29992;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#65292;&#24615;&#33021;&#26377;&#20102;&#26222;&#36941;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#32500;&#25805;&#20316;&#20013;&#65292;&#21253;&#21547;&#20960;&#20309;&#20449;&#24687;&#26159;&#38450;&#27490;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#36807;&#24230;&#24179;&#28369;&#30340;&#26377;&#25928;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents $\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks (EMPSNs), a novel approach to learning on geometric graphs and point clouds that is equivariant to rotations, translations, and reflections. EMPSNs can learn high-dimensional simplex features in graphs (e.g. triangles), and use the increase of geometric information of higher-dimensional simplices in an $\mathrm{E}(n)$ equivariant fashion. EMPSNs simultaneously generalize $\mathrm{E}(n)$ Equivariant Graph Neural Networks to a topologically more elaborate counterpart and provide an approach for including geometric information in Message Passing Simplicial Networks. The results indicate that EMPSNs can leverage the benefits of both approaches, leading to a general increase in performance when compared to either method. Furthermore, the results suggest that incorporating geometric information serves as an effective measure against over-smoothing in message passing networks, especially when operating on high
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20135;&#29983;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#23545;&#20154;&#31867;&#26159;&#21542;&#26377;&#29992;&#65292;&#21457;&#29616;&#29616;&#26377;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#36828;&#20302;&#20110;&#29702;&#24819;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#20272;&#35745;&#29702;&#30001;&#22312;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.07095</link><description>&lt;p&gt;
&#26426;&#22120;&#29702;&#30001;&#23545;&#20154;&#31867;&#26159;&#21542;&#26377;&#29992;&#65311;&#35780;&#20272;&#21644;&#25552;&#39640;&#33258;&#28982;&#25991;&#26412;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales. (arXiv:2305.07095v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20135;&#29983;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#23545;&#20154;&#31867;&#26159;&#21542;&#26377;&#29992;&#65292;&#21457;&#29616;&#29616;&#26377;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#36828;&#20302;&#20110;&#29702;&#24819;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#20272;&#35745;&#29702;&#30001;&#22312;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#26174;&#30528;&#20986;&#29616;&#33021;&#21147;&#20013;&#65292;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#26159;&#20854;&#20013;&#20043;&#19968;&#65307;&#36229;&#36807;&#26576;&#20010;&#35268;&#27169;&#21518;&#65292;&#22823;&#22411;LMs&#33021;&#22815;&#29983;&#25104;&#30475;&#20284;&#26377;&#29992;&#30340;&#29702;&#30001;&#65292;&#36827;&#32780;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#23427;&#20204;&#22312;&#39046;&#23548;&#27036;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#29616;&#35937;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26426;&#22120;&#29983;&#25104;&#30340;&#29702;&#30001;&#26159;&#21542;&#20063;&#33021;&#23545;&#20154;&#31867;&#26377;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#26222;&#36890;&#20154;&#23581;&#35797;&#26681;&#25454;&#36825;&#20123;&#26426;&#22120;&#29702;&#30001;&#22238;&#31572;&#38382;&#39064;&#26102;&#65311;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#36828;&#26410;&#20196;&#20154;&#28385;&#24847;&#65292;&#24182;&#19988;&#26114;&#36149;&#30340;&#20154;&#31867;&#30740;&#31350;&#25165;&#33021;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#29983;&#25104;&#29702;&#30001;LM&#30340;&#20219;&#21153;&#34920;&#29616;&#25110;&#29983;&#25104;&#29702;&#30001;&#19982;&#40644;&#37329;&#29702;&#30001;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#26126;&#23427;&#20204;&#30340;&#20154;&#31867;&#25928;&#29992;&#12290;&#34429;&#28982;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29702;&#30001;&#30340;&#26576;&#20123;&#23646;&#24615;&#65292;&#22914;&#31616;&#27905;&#24615;&#21644;&#26032;&#39062;&#24615;&#65292;&#19982;&#23427;&#20204;&#30340;&#20154;&#31867;&#25928;&#29992;&#26377;&#20851;&#65292;&#20294;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#23427;&#20204;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20272;&#35745;&#29702;&#30001;&#22312;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond a certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory, and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales, or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale's helpfulness in ans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24178;&#25200;&#31649;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;UAV&#36890;&#20449;&#20013;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#20808;&#30693;&#24178;&#25200;&#20449;&#21495;&#30340;&#20449;&#36947;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.07069</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#19977;&#32500;&#32593;&#32476;&#20013;&#30340;&#24178;&#25200;&#31649;&#29702;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#28508;&#21147;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Interference Management in UAV-based 3D Networks: Potentials and Challenges. (arXiv:2305.07069v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24178;&#25200;&#31649;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;UAV&#36890;&#20449;&#20013;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#20808;&#30693;&#24178;&#25200;&#20449;&#21495;&#30340;&#20449;&#36947;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#34562;&#31389;&#32593;&#32476;&#26159;&#22810;&#23567;&#21306;&#30340;&#65292;&#37319;&#29992;&#36890;&#29992;&#39057;&#29575;&#37325;&#29992;&#26469;&#26368;&#22823;&#21270;&#39057;&#35889;&#25928;&#29575;&#12290;&#36825;&#23548;&#33268;&#39640;&#24178;&#25200;&#12290;&#38543;&#30528;&#26080;&#20154;&#26426;(UAV)&#30340;&#37319;&#29992;&#65292;&#36825;&#20010;&#38382;&#39064;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#20005;&#37325;&#65292;&#22240;&#20026;UAV&#36890;&#20449;&#20013;&#30340;&#30452;&#23556;&#39057;&#36947;&#20250;&#36805;&#36895;&#22686;&#21152;&#24178;&#25200;&#38142;&#36335;&#30340;&#24378;&#24230;&#21644;&#25968;&#37327;&#12290;&#29616;&#26377;&#30340;&#24178;&#25200;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#27599;&#20010;&#21457;&#23556;&#22120;&#30693;&#36947;&#24178;&#25200;&#20449;&#21495;&#30340;&#20449;&#36947;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#22810;&#30340;&#20449;&#20196;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24178;&#25200;&#31649;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#19981;&#30693;&#36947;&#24178;&#25200;&#20449;&#21495;&#30340;&#20449;&#36947;&#20449;&#24687;&#65292;&#20063;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;&#24178;&#25200;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#32447;&#24615;/&#20122;&#32447;&#24615;&#22797;&#26434;&#24230;&#25193;&#23637;&#31639;&#27861;&#21644;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23545;&#20854;&#36827;&#34892;&#20998;&#25955;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern cellular networks are multi-cell and use universal frequency reuse to maximize spectral efficiency. This results in high inter-cell interference. This problem is growing as cellular networks become three-dimensional with the adoption of unmanned aerial vehicles (UAVs). This is because the strength and number of interference links rapidly increase due to the line-of-sight channels in UAV communications. Existing interference management solutions need each transmitter to know the channel information of interfering signals, rendering them impractical due to excessive signaling overhead. In this paper, we propose leveraging deep reinforcement learning for interference management to tackle this shortcoming. In particular, we show that interference can still be effectively mitigated even without knowing its channel information. We then discuss novel approaches to scale the algorithms with linear/sublinear complexity and decentralize them using multi-agent reinforcement learning. By ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#26469;&#35299;&#20915;&#20540;&#36845;&#20195;&#32593;&#32476;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#21644;&#20943;&#36731;&#32047;&#31215;&#35823;&#24046;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#21644;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07039</link><description>&lt;p&gt;
&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Value Iteration Networks with Gated Summarization Module. (arXiv:2305.07039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#26469;&#35299;&#20915;&#20540;&#36845;&#20195;&#32593;&#32476;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#21644;&#20943;&#36731;&#32047;&#31215;&#35823;&#24046;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#21644;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;VIN&#65289;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#65292;&#20943;&#36731;&#30001;&#22686;&#21152;&#36845;&#20195;&#27425;&#25968;&#24341;&#36215;&#30340;&#32047;&#31215;&#35823;&#24046;&#30340;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#21367;&#31215;&#26680;&#20943;&#23569;&#36845;&#20195;&#27425;&#25968;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#65292;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#21010;&#36807;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#20351;&#24471;&#32593;&#32476;&#21487;&#20197;&#24378;&#35843;&#25972;&#20010;&#35268;&#21010;&#36807;&#31243;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#26368;&#32456;&#30340;&#20840;&#23616;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments 
&lt;/p&gt;</description></item><item><title>GFlowNets&#26694;&#26550;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#36712;&#36857;&#19978;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#20005;&#26684;&#19982;&#20154;&#31867;&#35780;&#32423;&#25104;&#27604;&#20363;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27604;RLHF&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.07036</link><description>&lt;p&gt;
&#24102;&#20154;&#31867;&#21453;&#39304;&#30340;GFlowNets
&lt;/p&gt;
&lt;p&gt;
GFlowNets with Human Feedback. (arXiv:2305.07036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07036
&lt;/p&gt;
&lt;p&gt;
GFlowNets&#26694;&#26550;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#36712;&#36857;&#19978;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#20005;&#26684;&#19982;&#20154;&#31867;&#35780;&#32423;&#25104;&#27604;&#20363;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27604;RLHF&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;GFlowNets (GFlowHF) &#26694;&#26550;&#26469;&#25913;&#36827;&#35757;&#32451; AI &#27169;&#22411;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23545;&#20110;&#22870;&#21169;&#26410;&#30693;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#36712;&#36857;&#19978;&#30340;&#20154;&#31867;&#35780;&#20272;&#26469;&#36866;&#24212;&#22870;&#21169;&#20989;&#25968;&#12290;GFlowHF &#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#20005;&#26684;&#19982;&#20154;&#31867;&#35780;&#32423;&#25104;&#27604;&#20363;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#20110;&#31867;&#20284; RLHF &#30340;&#20154;&#31867;&#21916;&#22909;&#35780;&#32423;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;GFlowHF &#27604; RLHF &#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the GFlowNets with Human Feedback (GFlowHF) framework to improve the exploration ability when training AI models. For tasks where the reward is unknown, we fit the reward function through human evaluations on different trajectories. The goal of GFlowHF is to learn a policy that is strictly proportional to human ratings, instead of only focusing on human favorite ratings like RLHF. Experiments show that GFlowHF can achieve better exploration ability than RLHF.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31192;&#23494;&#34892;&#21160;&#30340;&#24418;&#24335;&#35821;&#20041;&#21644;&#36923;&#36753;&#31995;&#32479;&#12290;&#20854;&#24378;&#35843;&#20998;&#24067;&#24335;&#30693;&#35782;&#21644;&#32852;&#30431;&#21147;&#37327;&#23545;&#31192;&#23494;&#34892;&#21160;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.07035</link><description>&lt;p&gt;
&#23433;&#38745;&#65281;&#23494;&#35851;&#34892;&#21160;&#30340;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shhh! The Logic of Clandestine Operations. (arXiv:2305.07035v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31192;&#23494;&#34892;&#21160;&#30340;&#24418;&#24335;&#35821;&#20041;&#21644;&#36923;&#36753;&#31995;&#32479;&#12290;&#20854;&#24378;&#35843;&#20998;&#24067;&#24335;&#30693;&#35782;&#21644;&#32852;&#30431;&#21147;&#37327;&#23545;&#31192;&#23494;&#34892;&#21160;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#25805;&#20316;&#38544;&#34255;&#20102;&#25805;&#20316;&#32773;&#30340;&#36523;&#20221;&#65292;&#21017;&#31216;&#20854;&#20026;&#38544;&#34109;&#25805;&#20316;; &#22914;&#26524;&#36974;&#25513;&#20102;&#34892;&#21160;&#20107;&#23454;&#65292;&#21017;&#31216;&#20854;&#20026;&#31192;&#23494;&#25805;&#20316;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31192;&#23494;&#34892;&#21160;&#30340;&#24418;&#24335;&#35821;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25551;&#36848;&#20998;&#24067;&#24335;&#30693;&#35782;&#27169;&#24577;&#21644;&#25429;&#33719;&#32852;&#30431;&#34892;&#20351;&#31192;&#23494;&#34892;&#21160;&#21147;&#37327;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22768;&#38899;&#21644;&#23436;&#25972;&#30340;&#36923;&#36753;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
An operation is called covert if it conceals the identity of the actor; it is called clandestine if the very fact that the operation is conducted is concealed. The paper proposes a formal semantics of clandestine operations and introduces a sound and complete logical system that describes the interplay between the distributed knowledge modality and a modality capturing coalition power to conduct clandestine operations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;CTC&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#26469;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#12290;&#37319;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.07034</link><description>&lt;p&gt;
&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#30340;&#21476;&#20848;&#32463;&#26391;&#35829;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Quran Recitation Recognition using End-to-End Deep Learning. (arXiv:2305.07034v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;CTC&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#26469;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#12290;&#37319;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21476;&#20848;&#32463;&#26159;&#20234;&#26031;&#20848;&#25945;&#30340;&#22307;&#20070;&#65292;&#20854;&#26391;&#35829;&#26159;&#35813;&#23447;&#25945;&#20449;&#20208;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#30001;&#20110;&#21476;&#20848;&#32463;&#30340;&#29420;&#29305;&#35268;&#21017;&#19981;&#36866;&#29992;&#20110;&#27491;&#24120;&#30340;&#28436;&#35762;&#65292;&#25152;&#20197;&#33258;&#21160;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#24050;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#65292;&#20294;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#26391;&#35829;&#38169;&#35823;&#26816;&#27979;&#35270;&#20026;&#20998;&#31867;&#20219;&#21153;&#25110;&#20351;&#29992;&#20256;&#32479;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#12290;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;CNN-Bidirectional GRU&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;CTC&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#21644;&#22522;&#20110;&#23383;&#31526;&#30340;&#35299;&#30721;&#22120;&#65292;&#21363;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#26159;&#22312;&#30001;&#30701;&#33410;&#21644;&#20960;&#31456;&#21476;&#20848;&#32463;&#32452;&#25104;&#30340;&#23567;&#22411;&#31169;&#20154;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#12290;&#30001;&#20110;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;Ar-DAD&#65289;&#20316;&#20026;&#23454;&#39564;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Quran is the holy scripture of Islam, and its recitation is an important aspect of the religion. Recognizing the recitation of the Holy Quran automatically is a challenging task due to its unique rules that are not applied in normal speaking speeches. A lot of research has been done in this domain, but previous works have detected recitation errors as a classification task or used traditional automatic speech recognition (ASR). In this paper, we proposed a novel end-to-end deep learning model for recognizing the recitation of the Holy Quran. The proposed model is a CNN-Bidirectional GRU encoder that uses CTC as an objective function, and a character-based decoder which is a beam search decoder. Moreover, all previous works were done on small private datasets consisting of short verses and a few chapters of the Holy Quran. As a result of using private datasets, no comparisons were done. To overcome this issue, we used a public dataset that has recently been published (Ar-DAD) and co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#31934;&#30830;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#65292;&#24182;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#31038;&#20250;&#25193;&#25955;&#21644;&#22320;&#38663;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.07031</link><description>&lt;p&gt;
&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Hawkes Process based on Controlled Differential Equations. (arXiv:2305.07031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#31934;&#30830;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#65292;&#24182;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#31038;&#20250;&#25193;&#25955;&#21644;&#22320;&#38663;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hawkes&#36807;&#31243;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22810;&#20010;&#39046;&#22495;&#30340;&#24207;&#36143;&#20107;&#20214;&#21457;&#29983;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#31038;&#20250;&#25193;&#25955;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#20107;&#20214;&#20043;&#38388;&#30340;&#38388;&#38548;&#26102;&#38388;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#19981;&#20165;&#38590;&#20197;&#25429;&#25417;&#36825;&#31181;&#22797;&#26434;&#30340;&#19981;&#35268;&#21017;&#21160;&#24577;&#65292;&#32780;&#19988;&#36824;&#20250;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#35745;&#31639;&#20107;&#20214;&#30340;&#23545;&#25968;&#20284;&#28982;&#65292;&#22240;&#20026;&#23427;&#20204;&#22823;&#22810;&#22522;&#20110;&#35774;&#35745;&#29992;&#20110;&#35268;&#21017;&#31163;&#25955;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;(CDE)&#30340;Hawkes&#36807;&#31243;&#27010;&#24565;&#65292;&#36890;&#36807;&#37319;&#29992;&#31867;&#20284;&#20110;&#36830;&#32493;RNN&#30340;&#31070;&#32463;CDE&#25216;&#26415;&#12290;&#30001;&#20110;HP-CDE&#19981;&#26029;&#22320;&#35835;&#21462;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#36866;&#24403;&#22320;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#23427;&#20204;&#30340;&#19981;&#22343;&#21248;&#26102;&#38388;&#31354;&#38388;&#65292;&#24182;&#19988;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;Hawkes&#36807;&#31243;&#21644;&#31070;&#32463;CDE&#37117;&#26159;&#22312;&#36830;&#32493;&#30340;&#26102;&#38388;&#22495;&#20013;&#39318;&#20808;&#24320;&#21457;&#30340;&#65292;&#23427;&#20204;&#20855;&#26377;&#30456;&#20284;&#30340;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;HP-CDE&#20855;&#26377;&#36879;&#26126;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#23454;&#38469;&#22330;&#26223;&#65292;&#20363;&#22914;&#31038;&#20250;&#25193;&#25955;&#65292;&#20854;&#20013;&#20107;&#20214;&#20043;&#38388;&#30340;&#38388;&#38548;&#26102;&#38388;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#31038;&#20132;&#25193;&#25955;&#21644;&#22320;&#38663;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hawkes processes are a popular framework to model the occurrence of sequential events, i.e., occurrence dynamics, in several fields such as social diffusion. In real-world scenarios, the inter-arrival time among events is irregular. However, existing neural network-based Hawkes process models not only i) fail to capture such complicated irregular dynamics, but also ii) resort to heuristics to calculate the log-likelihood of events since they are mostly based on neural networks designed for regular discrete inputs. To this end, we present the concept of Hawkes process based on controlled differential equations (HP-CDE), by adopting the neural controlled differential equation (neural CDE) technology which is an analogue to continuous RNNs. Since HP-CDE continuously reads data, i) irregular time-series datasets can be properly treated preserving their uneven temporal spaces, and ii) the log-likelihood can be exactly computed. Moreover, as both Hawkes processes and neural CDEs are first de
&lt;/p&gt;</description></item><item><title>Musketeer&#26159;&#19968;&#31181;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#65288;TEP&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#24322;&#26500;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#21248;</title><link>http://arxiv.org/abs/2305.07019</link><description>&lt;p&gt;
Musketeer&#65288;&#19968;&#20154;&#20043;&#21147;&#65292;&#19975;&#20154;&#20043;&#21147;&#65289;&#65306;&#20855;&#26377;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts. (arXiv:2305.07019v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07019
&lt;/p&gt;
&lt;p&gt;
Musketeer&#26159;&#19968;&#31181;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#65288;TEP&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#24322;&#26500;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#21248;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65288;&#19975;&#20154;&#20043;&#21147;&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#23436;&#20840;&#20849;&#20139;&#65288;&#19968;&#20154;&#20043;&#21147;&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;Musketeer&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a sequence-to-sequence vision-language model whose parameters are jointly trained on all tasks (all for one) and fully shared among multiple tasks (one for all), resulting in a single model which we named Musketeer. The integration of knowledge across heterogeneous tasks is enabled by a novel feature called Task Explanation Prompt (TEP). TEP reduces interference among tasks, allowing the model to focus on their shared structure. With a single model, Musketeer achieves results comparable to or better than strong baselines trained on single tasks, almost uniformly across multiple tasks.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#25193;&#23637;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;&#65292;&#20316;&#32773;&#36890;&#36807;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#39640;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.06972</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#26377;&#25928;&#25193;&#23637;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns. (arXiv:2305.06972v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06972
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#25193;&#23637;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;&#65292;&#20316;&#32773;&#36890;&#36807;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#39640;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24050;&#32463;&#20135;&#29983;&#20102;&#21151;&#33021;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;&#21452;&#37325;&#29992;&#36884;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;&#65292;&#36825;&#31181;&#27969;&#34892;&#30340;&#32593;&#32476;&#29359;&#32618;&#24418;&#24335;&#28041;&#21450;&#23558;&#30446;&#26631;&#20154;&#29289;&#35825;&#39575;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#20316;&#32773;&#39318;&#20808;&#30740;&#31350;&#20102;LLMs&#22312;&#25104;&#21151;&#30340;&#38035;&#40060;&#25915;&#20987;&#30340;&#20390;&#23519;&#21644;&#20449;&#24687;&#29983;&#25104;&#38454;&#27573;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22312;&#36825;&#20123;&#38454;&#27573;&#26174;&#30528;&#25552;&#39640;&#32593;&#32476;&#32618;&#29359;&#30340;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#20316;&#32773;&#20351;&#29992;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#20026;&#36229;&#36807;600&#21517;&#33521;&#22269;&#35758;&#21592;&#21019;&#24314;&#20102;&#29420;&#29305;&#30340;&#38035;&#40060;&#37038;&#20214;&#30340;&#23454;&#35777;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#37038;&#20214;&#19981;&#20165;&#36924;&#30495;&#32780;&#19988;&#25104;&#26412;&#25928;&#30410;&#26174;&#33879;&#65292;&#27599;&#23553;&#30005;&#23376;&#37038;&#20214;&#20165;&#33457;&#36153;&#20960;&#20998;&#20043;&#19968;&#30340;&#32654;&#20998;&#21363;&#21487;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in artificial intelligence (AI), particularly in the domain of large language models (LLMs), has resulted in powerful and versatile dual-use systems. Indeed, cognition can be put towards a wide variety of tasks, some of which can result in harm. This study investigates how LLMs can be used for spear phishing, a prevalent form of cybercrime that involves manipulating targets into divulging sensitive information. I first explore LLMs' ability to assist with the reconnaissance and message generation stages of a successful spear phishing attack, where I find that advanced LLMs are capable of meaningfully improving cybercriminals' efficiency during these stages. Next, I conduct an empirical test by creating unique spear phishing messages for over 600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. My findings reveal that these messages are not only realistic but also remarkably cost-effective, as each email cost only a fraction of a cent to generate. N
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#23454;&#29616;&#20102;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;&#65292;&#38024;&#23545;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#20808;&#39564;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.06924</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#22312;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#23454;&#29616;&#20808;&#39564;&#30693;&#35782;&#20256;&#36882;&#20197;&#36827;&#34892;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation. (arXiv:2305.06924v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#23454;&#29616;&#20102;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;&#65292;&#38024;&#23545;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#20808;&#39564;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21147;&#24066;&#22330;&#30340;&#25237;&#26631;&#28216;&#25103;&#20013;&#65292;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#20272;&#35745;&#26159;&#21457;&#30005;&#20844;&#21496;&#65288;GENCO&#65289;&#36827;&#34892;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#29420;&#31435;&#31995;&#32479;&#36816;&#33829;&#21830;&#65288;ISO&#65289;&#36827;&#34892;&#24066;&#22330;&#30417;&#35270;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NE&#20272;&#35745;&#26041;&#27861;&#22312;&#26032;&#20852;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#65288;FEM&#65289;&#20013;&#26159;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22312;&#20219;&#20309;&#29615;&#22659;&#21464;&#21270;&#20043;&#21069;&#65292;&#22914;&#36127;&#36733;&#38656;&#27714;&#21464;&#21270;&#12289;&#32593;&#32476;&#25317;&#22581;&#21644;&#24066;&#22330;&#35774;&#35745;&#30340;&#20462;&#25913;&#65292;&#25237;&#26631;&#31574;&#30053;&#30340;&#20808;&#39564;&#30693;&#35782;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#38024;&#23545;FEM&#24320;&#21457;&#20102;Bayes&#33258;&#36866;&#24212;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;BAMDP-FEM&#65289;&#65292;&#20197;&#32771;&#34385;&#20808;&#39564;&#30693;&#35782;&#26469;&#24314;&#27169;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#12290;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65288;MAGAIL-FEM&#65289;&#65292;&#20351;GENCO&#33021;&#22815;&#21516;&#26102;&#20174;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#24471;&#21040;&#30340;NE&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#65288;BNE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Nash Equilibrium (NE) estimation in bidding games of electricity markets is the key concern of both generation companies (GENCOs) for bidding strategy optimization and the Independent System Operator (ISO) for market surveillance. However, existing methods for NE estimation in emerging modern electricity markets (FEM) are inaccurate and inefficient because the priori knowledge of bidding strategies before any environment changes, such as load demand variations, network congestion, and modifications of market design, is not fully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM (BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategy optimization considering the priori knowledge. A novel Multi-Agent Generative Adversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed to enable GENCOs to learn simultaneously from priori knowledge and interactions with changing environments. The obtained NE is a Bayesian Nash Equilibrium (BNE) with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26469;&#32852;&#21512;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#21644;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#20135;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2305.06921</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#26410;&#26469;&#30340;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#65311;&#31532;&#20108;&#37096;&#20998;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications. (arXiv:2305.06921v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26469;&#32852;&#21512;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#21644;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#20135;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20026;&#20004;&#37096;&#20998;&#30340;&#35770;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#33539;&#24335;&#29702;&#35770;&#21644;&#35814;&#32454;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#25311;&#26469;&#32852;&#21512;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#36890;&#36807;&#38416;&#36848;&#35814;&#32454;&#30340;&#26041;&#27861;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#65288;ESM&#65289;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#65288;ASM&#65289;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#65288;RC&#65289;&#21644;&#37329;&#34701;&#24066;&#22330;&#65288;FM&#65289;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#65288;VB&#65289;&#20135;&#21697;&#26469;&#36827;&#19968;&#27493;&#28436;&#31034;&#36825;&#19968;&#29702;&#35770;&#12290;&#26681;&#25454;&#31532;&#19968;&#37096;&#20998;&#25552;&#20986;&#30340;&#29702;&#35770;&#65292;&#39318;&#20808;&#30830;&#23450;&#32852;&#21512;&#24066;&#22330;&#20013;&#30340;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;&#25509;&#30528;&#65292;&#24320;&#21457;&#20102;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#21644;&#19981;&#30830;&#23450;&#39118;&#38505;&#32435;&#20837;&#27169;&#22411;&#20844;&#24335;&#20013;&#12290;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#36817;&#31471;&#20248;&#21270;&#65288;MAPPO&#65289;&#31639;&#27861;&#65292;&#20316;&#20026;&#31532;&#19968;&#37096;&#20998;&#24320;&#21457;&#30340;&#24191;&#20041;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;&#30340;&#23454;&#38469;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20123;&#24066;&#22330;&#36816;&#34892;&#32489;&#25928;&#25351;&#26631;&#65292;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
This two-part paper develops a paradigmatic theory and detailed methods of the joint electricity market design using reinforcement-learning (RL)-based simulation. In Part 2, this theory is further demonstrated by elaborating detailed methods of designing an electricity spot market (ESM), together with a reserved capacity product (RC) in the ancillary service market (ASM) and a virtual bidding (VB) product in the financial market (FM). Following the theory proposed in Part 1, firstly, market design options in the joint market are specified. Then, the Markov game model is developed, in which we show how to incorporate market design options and uncertain risks in model formulation. A multi-agent policy proximal optimization (MAPPO) algorithm is elaborated, as a practical implementation of the generalized market simulation method developed in Part 1. Finally, the case study demonstrates how to pick the best market design options by using some of the market operation performance indicators 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.06472</link><description>&lt;p&gt;
ChatGPT&#24335;&#30340;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#25216;&#26415;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#35774;&#22791;&#32500;&#25252;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;PHM&#25216;&#26415;&#35782;&#21035;&#21644;&#39044;&#27979;&#35774;&#22791;&#25925;&#38556;&#21644;&#25439;&#22351;&#12290;&#29616;&#22312;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;AI&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#36825;&#31181;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24037;&#19994;&#39046;&#22495;&#65292;&#22914;&#38081;&#36335;&#12289;&#33021;&#28304;&#21644;&#33322;&#31354;&#31561;&#65292;&#20197;&#25552;&#39640;&#35774;&#22791;&#30340;&#26381;&#21153;&#23551;&#21629;&#21644;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#29983;&#20135;&#25104;&#26412;&#21644;&#20572;&#26426;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.06446</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;: &#24322;&#27493;&#36890;&#20449;&#21644;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35774;&#32622;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20197;&#21512;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#21516;&#26102;&#30830;&#20445;&#21512;&#20316;&#20248;&#21183;&#19988;&#36890;&#20449;&#24320;&#38144;&#20302;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377; $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ &#30340;&#36951;&#25022;&#20540;&#21644; $\tilde{\mathcal{O}}(dHM^2)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013; $d$ &#26159;&#29305;&#24449;&#32500;&#25968;&#65292;$H$ &#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$M$ &#26159;&#26234;&#33021;&#20307;&#24635;&#25968;&#65292;$K$ &#26159;&#24635;&#24773;&#33410;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#38480;&#35777;&#26126;&#65292;&#34920;&#26126;&#36890;&#36807;&#21327;&#20316;&#33267;&#23569;&#38656;&#35201; $\Omega(dM)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#25165;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#65292;&#21487;&#20197;&#20943;&#23569;&#25317;&#22581;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#25193;&#22823;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06436</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#21644;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-Robot Coordination and Layout Design for Automated Warehousing. (arXiv:2305.06436v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06436
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#65292;&#21487;&#20197;&#20943;&#23569;&#25317;&#22581;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#25193;&#22823;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#23558;MAPF&#31639;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#33258;&#21160;&#21270;&#20179;&#24211;&#20013;&#65292;&#20197;&#21327;&#35843;&#25968;&#30334;&#20010;&#26426;&#22120;&#20154;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#26356;&#22909;&#30340;MAPF&#31639;&#27861;&#26469;&#25552;&#39640;&#20179;&#24211;&#30340;&#21534;&#21520;&#37327;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#26469;&#25552;&#39640;&#20854;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;MAPF&#31639;&#27861;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#20154;&#24037;&#35774;&#35745;&#24067;&#23616;&#20063;&#21487;&#33021;&#23548;&#33268;&#20179;&#24211;&#30340;&#25317;&#22581;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#20197;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#20248;&#21270;&#21518;&#30340;&#20179;&#24211;&#24067;&#23616;(1)&#20943;&#23569;&#20102;&#20132;&#36890;&#25317;&#22581;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;(2)&#36890;&#36807;&#21152;&#20493;&#26426;&#22120;&#20154;&#25968;&#37327;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#21487;&#20280;&#32553;&#24615;&#65292;(3)&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#29992;&#25143;&#25351;&#23450;&#22810;&#26679;&#24615;&#25351;&#26631;&#30340;&#24067;&#23616;&#12290;&#28304;&#20195;&#30721;&#20301;&#20110;&#65306;\url{https://github.com/lun}
&lt;/p&gt;
&lt;p&gt;
With the rapid progress in Multi-Agent Path Finding (MAPF), researchers have studied how MAPF algorithms can be deployed to coordinate hundreds of robots in large automated warehouses. While most works try to improve the throughput of such warehouses by developing better MAPF algorithms, we focus on improving the throughput by optimizing the warehouse layout. We show that, even with state-of-the-art MAPF algorithms, commonly used human-designed layouts can lead to congestion for warehouses with large numbers of robots and thus have limited scalability. We extend existing automatic scenario generation methods to optimize warehouse layouts. Results show that our optimized warehouse layouts (1) reduce traffic congestion and thus improve throughput, (2) improve the scalability of the automated warehouses by doubling the number of robots in some cases, and (3) are capable of generating layouts with user-specified diversity measures. We include the source code at: \url{https://github.com/lun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.06435</link><description>&lt;p&gt;
&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#29616;&#22312;&#38750;&#24120;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#23569;&#23450;&#37327;&#35299;&#37322;&#26368;&#20339;&#23567;&#25209;&#37327;&#22823;&#23567;&#24212;&#35813;&#26159;&#22810;&#22823;&#30340;&#29702;&#35770;&#12290;&#26412;&#25991;&#23581;&#35797;&#31995;&#32479;&#22320;&#29702;&#35299;&#23567;&#25209;&#37327;&#22823;&#23567;&#22312;&#35757;&#32451;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#12290;&#22312;&#25945;&#24072;-&#23398;&#29983;&#24773;&#22659;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#25945;&#24072;&#65292;&#24182;&#32858;&#28966;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;m&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#30340;&#27867;&#21270;&#24615;&#33021;&#24378;&#28872;&#20381;&#36182;&#20110;m&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#20020;&#30028;&#20540;mc&#22788;&#32463;&#21382;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#36825;&#26679;&#24403;m&lt; mc&#26102;&#65292;&#35757;&#32451;&#36807;&#31243;&#22833;&#36133;&#65292;&#32780;&#24403;m&gt; mc&#26102;&#65292;&#23398;&#29983;&#21487;&#20197;&#23436;&#32654;&#22320;&#23398;&#20064;&#25110;&#24456;&#22909;&#22320;&#27867;&#21270;&#25945;&#24072;&#12290;&#30456;&#21464;&#26159;&#30001;&#32479;&#35745;&#21147;&#23398;&#39318;&#27425;&#21457;&#29616;&#30340;&#38598;&#20307;&#29616;&#35937;&#65292;&#24182;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#35266;&#23519;&#21040;&#12290;&#25214;&#21040;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;&#65292;&#21487;&#20197;&#38416;&#26126;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m&lt;m_c$ the training process fails, while for $m&gt;m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#36741;&#21161;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#31216;&#20026;HAISTA-NET&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#25351;&#23450;&#30340;&#37096;&#20998;&#36793;&#30028;&#22320;&#22270;&#65292;&#20197;&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2305.03105</link><description>&lt;p&gt;
HAISTA-NET: &#36890;&#36807;&#27880;&#24847;&#21147;&#36827;&#34892;&#20154;&#31867;&#36741;&#21161;&#30340;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
HAISTA-NET: Human Assisted Instance Segmentation Through Attention. (arXiv:2305.03105v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#36741;&#21161;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#31216;&#20026;HAISTA-NET&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#25351;&#23450;&#30340;&#37096;&#20998;&#36793;&#30028;&#22320;&#22270;&#65292;&#20197;&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20363;&#20998;&#21106;&#26159;&#22270;&#20687;&#26816;&#27979;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#22312;&#29289;&#20307;&#32454;&#21270;&#12289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#22270;&#20687;/&#35270;&#39057;&#32534;&#36753;&#31561;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#24212;&#29992;&#37117;&#38656;&#35201;&#39640;&#24230;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20415;&#26159;&#26368;&#20808;&#36827;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#23454;&#20363;&#20998;&#21106;&#31639;&#27861;&#65292;&#20854;&#31934;&#24230;&#24120;&#24120;&#26080;&#27861;&#36798;&#21040;&#12290;&#23545;&#20110;&#23567;&#32780;&#22797;&#26434;&#30340;&#23545;&#35937;&#26469;&#35828;&#65292;&#24615;&#33021;&#24046;&#36317;&#23588;&#20026;&#26126;&#26174;&#12290;&#36890;&#24120;&#65292;&#20174;&#19994;&#32773;&#21482;&#33021;&#37319;&#29992;&#23436;&#20840;&#25163;&#21160;&#30340;&#27880;&#37322;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#32321;&#29712;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#20026;&#39640;&#26354;&#29575;&#12289;&#22797;&#26434;&#21644;&#23567;&#35268;&#27169;&#23545;&#35937;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;&#25105;&#20204;&#30340;&#20154;&#31867;&#36741;&#21161;&#20998;&#21106;&#27169;&#22411;HAISTA-NET&#65292;&#25193;&#20805;&#20102;&#29616;&#26377;&#30340;Strong Mask R-CNN&#32593;&#32476;&#65292;&#20197;&#21253;&#25324;&#20154;&#31867;&#25351;&#23450;&#30340;&#37096;&#20998;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#25163;&#32472;&#37096;&#20998;&#29289;&#20307;&#36793;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;&#20154;&#31867;&#27880;&#24847;&#21147;&#22320;&#22270;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#32771;&#34385;&#21040;&#20102;&#37096;&#20998;&#27880;&#24847;&#21147;&#22320;&#22270;&#21644;&#21407;&#22987;&#25513;&#27169;&#25552;&#35758;&#65292;&#36825;&#20351;&#32593;&#32476;&#33021;&#22815;&#20851;&#27880;&#20154;&#20204;&#35748;&#20026;&#26368;&#37325;&#35201;&#30340;&#21306;&#22495;&#12290;&#22312;PASCAL VOC&#21644;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#32447;&#65292;&#24182;&#22312;&#23567;&#32780;&#22797;&#26434;&#23545;&#35937;&#23454;&#20363;&#20998;&#21106;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation is a form of image detection which has a range of applications, such as object refinement, medical image analysis, and image/video editing, all of which demand a high degree of accuracy. However, this precision is often beyond the reach of what even state-of-the-art, fully automated instance segmentation algorithms can deliver. The performance gap becomes particularly prohibitive for small and complex objects. Practitioners typically resort to fully manual annotation, which can be a laborious process. In order to overcome this problem, we propose a novel approach to enable more precise predictions and generate higher-quality segmentation masks for high-curvature, complex and small-scale objects. Our human-assisted segmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network to incorporate human-specified partial boundaries. We also present a dataset of hand-drawn partial object boundaries, which we refer to as human attention maps. In addition, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#20197;&#24212;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#65292;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#65292;&#24182;&#20026;&#29615;&#22659;&#20570;&#20986;&#36129;&#29486;&#12290;&#35813;&#33539;&#22411;&#29702;&#35770;&#30340;&#26694;&#26550;&#23558;&#22312;&#20004;&#37096;&#20998;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;</title><link>http://arxiv.org/abs/2305.02485</link><description>&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#26410;&#26469;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#65311;&#31532;&#19968;&#37096;&#20998;&#65306;&#33539;&#22411;&#29702;&#35770;&#12290;&#65288;arXiv:2305.02485v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory. (arXiv:2305.02485v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#20197;&#24212;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#65292;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#65292;&#24182;&#20026;&#29615;&#22659;&#20570;&#20986;&#36129;&#29486;&#12290;&#35813;&#33539;&#22411;&#29702;&#35770;&#30340;&#26694;&#26550;&#23558;&#22312;&#20004;&#37096;&#20998;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#37325;&#26032;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#26159;&#19968;&#31181;&#23439;&#35266;&#23618;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#39640;&#28183;&#36879;&#29575;&#65292;&#24182;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#25805;&#20316;&#23433;&#20840;&#12289;&#32463;&#27982;&#25928;&#29575;&#21644;&#29615;&#22659;&#21451;&#22909;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24066;&#22330;&#35774;&#35745;&#26041;&#27861;&#23398;&#23384;&#22312;&#20110;&#33021;&#28304;&#29616;&#36135;&#24066;&#22330;&#65288;ESM&#65289;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#65288;ASM&#65289;&#21644;&#37329;&#34701;&#24066;&#22330;&#65288;FM&#65289;&#20043;&#38388;&#21327;&#35843;&#19981;&#36275;&#65292;&#21363;&#8220;&#32852;&#21512;&#24066;&#22330;&#8221;&#65292;&#20197;&#21450;&#32570;&#20047;&#21487;&#38752;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#26412;&#25991;&#23558;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#25311;&#65292;&#24320;&#21457;&#32852;&#21512;&#24066;&#22330;&#35774;&#35745;&#30340;&#33539;&#22411;&#29702;&#35770;&#21644;&#35814;&#32454;&#26041;&#27861;&#12290;&#31532;&#19968;&#37096;&#20998;&#25552;&#20986;&#20102;&#36825;&#31181;&#26032;&#22411;&#24066;&#22330;&#35774;&#35745;&#21746;&#23398;&#30340;&#29702;&#35770;&#21644;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#24635;&#32467;&#20102;&#22312;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#26102;&#23384;&#22312;&#30340;&#26377;&#20105;&#35758;&#30340;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#20316;&#20026;&#30446;&#26631;&#30740;&#31350;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In face of the pressing need of decarbonization in the power sector, the re-design of electricity market is necessary as a Marco-level approach to accommodate the high penetration of renewable generations, and to achieve power system operation security, economic efficiency, and environmental friendliness. However, existing market design methodologies suffer from the lack of coordination among energy spot market (ESM), ancillary service market (ASM) and financial market (FM), i.e., the "joint market", and the lack of reliable simulation-based verification. To tackle these deficiencies, this two-part paper develops a paradigmatic theory and detailed methods of the joint market design using reinforcement-learning (RL)-based simulation. In Part 1, the theory and framework of this novel market design philosophy are proposed. First, the controversial market design options while designing the joint market are summarized as the targeted research questions. Second, the Markov game model is deve
&lt;/p&gt;</description></item><item><title>PaTeCon&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26102;&#38388;&#32422;&#26463;&#26469;&#32500;&#25252;KG&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#26102;&#38388;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2304.09015</link><description>&lt;p&gt;
PaTeCon&#65306;&#22522;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;&#29992;&#20110;&#20914;&#31361;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs. (arXiv:2304.09015v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09015
&lt;/p&gt;
&lt;p&gt;
PaTeCon&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26102;&#38388;&#32422;&#26463;&#26469;&#32500;&#25252;KG&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#26102;&#38388;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30740;&#31350;&#31038;&#21306;&#20013;&#65292;&#26102;&#38388;&#20107;&#23454;&#25351;&#29305;&#23450;&#26102;&#38388;&#27573;&#20869;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#25968;&#25454;&#12290;&#24341;&#20837;&#26102;&#38388;&#38480;&#21046;&#32473;KG&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#32500;&#25252;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#21160;&#21015;&#20030;&#26102;&#38388;&#32422;&#26463;&#26469;&#26816;&#27979;&#20914;&#31361;&#65292;&#36825;&#24456;&#36153;&#21147;&#19988;&#21487;&#33021;&#23384;&#22312;&#31890;&#24230;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;PaTeCon&#65292;&#23427;&#20351;&#29992;&#33258;&#21160;&#30830;&#23450;&#30340;&#22270;&#24418;&#27169;&#24335;&#21450;&#20854;&#30456;&#20851;&#32479;&#35745;&#20449;&#24687;&#20195;&#26367;&#20154;&#24037;&#19987;&#23478;&#26469;&#29983;&#25104;&#26102;&#38388;&#32422;&#26463;&#12290;&#20855;&#20307;&#22320;&#65292;PaTeCon&#26681;&#25454;&#20854;&#27979;&#37327;&#24471;&#20998;&#21160;&#24577;&#22320;&#23558;&#31867;&#38480;&#21046;&#38468;&#21152;&#21040;&#20505;&#36873;&#32422;&#26463;&#19978;&#12290;&#25105;&#20204;&#22522;&#20110;&#32500;&#22522;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;PaTeCon&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal facts, the facts for characterizing events that hold in specific time periods, are attracting rising attention in the knowledge graph (KG) research communities. In terms of quality management, the introduction of time restrictions brings new challenges to maintaining the temporal consistency of KGs and detecting potential temporal conflicts. Previous studies rely on manually enumerated temporal constraints to detect conflicts, which are labor-intensive and may have granularity issues. We start from the common pattern of temporal facts and constraints and propose a pattern-based temporal constraint mining method, PaTeCon. PaTeCon uses automatically determined graph patterns and their relevant statistical information over the given KG instead of human experts to generate time constraints. Specifically, PaTeCon dynamically attaches class restriction to candidate constraints according to their measuring scores.We evaluate PaTeCon on two large-scale datasets based on Wikidata and F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#20551;&#35774;&#65292;&#21457;&#29616;&#20102;Grassmannian Frame&#32467;&#26500;&#21644;&#23545;&#31216;&#27867;&#21270;&#29616;&#35937;&#65292;&#36825;&#23545;&#29305;&#24449;&#36873;&#25321;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#37117;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.08914</link><description>&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#30740;&#31350;&#65306;Grassmannian Frame&#12289;&#23545;&#31216;&#24615;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization. (arXiv:2304.08914v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#20551;&#35774;&#65292;&#21457;&#29616;&#20102;Grassmannian Frame&#32467;&#26500;&#21644;&#23545;&#31216;&#27867;&#21270;&#29616;&#35937;&#65292;&#36825;&#23545;&#29305;&#24449;&#36873;&#25321;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#37117;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#20551;&#35774;&#25512;&#24191;&#20102;&#21407;&#22987;&#30340;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#31867;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#24471;&#21040;&#20102;Grassmannian Frame&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#22312;&#29699;&#38754;&#19978;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#20102;&#27599;&#20004;&#20010;&#31867;&#21035;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#19968;&#20010;&#26356;&#22823;&#30340;&#29305;&#24449;&#32500;&#24230;&#12290;&#20986;&#20110;&#23545;Grassmannian Frame&#23545;&#31216;&#24615;&#30340;&#22909;&#22855;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25506;&#32034;&#19981;&#21516;Grassmannian Frame&#27169;&#22411;&#26159;&#21542;&#20250;&#20135;&#29983;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#31216;&#27867;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#32622;&#25442;&#23545;&#31216;&#27867;&#21270;&#30340;&#23450;&#29702;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#29305;&#24449;&#30340;&#19981;&#21516;&#26041;&#21521;&#20250;&#23548;&#33268;&#22914;&#27492;&#19981;&#21516;&#30340;&#27867;&#21270;&#29616;&#35937;&#30340;&#38382;&#39064;&#20173;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we extends original Neural Collapse Phenomenon by proving Generalized Neural Collapse hypothesis. We obtain Grassmannian Frame structure from the optimization and generalization of classification. This structure maximally separates features of every two classes on a sphere and does not require a larger feature dimension than the number of classes. Out of curiosity about the symmetry of Grassmannian Frame, we conduct experiments to explore if models with different Grassmannian Frames have different performance. As a result, we discover the Symmetric Generalization phenomenon. We provide a theorem to explain Symmetric Generalization of permutation. However, the question of why different directions of features can lead to such different generalization is still open for future investigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.06715</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#24403;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#22320;&#25551;&#36848;&#25152;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;&#65292;&#35299;&#37322;&#26041;&#27861;&#25165;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#39044;&#27979;&#22312;&#29305;&#23450;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#36825;&#21253;&#25324;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#26550;&#26500;&#12290;&#20219;&#20309;&#24544;&#23454;&#25551;&#36848;&#36825;&#31181;&#31867;&#22411;&#27169;&#22411;&#30340;&#35299;&#37322;&#37117;&#38656;&#35201;&#19982;&#35813;&#19981;&#21464;&#24615;&#23646;&#24615;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#36816;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#26469;&#24418;&#24335;&#21270;&#36825;&#31181;&#30452;&#35273;&#12290;&#36890;&#36807;&#36825;&#31181;&#20005;&#26684;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#65288;1&#65289;&#20004;&#20010;&#24230;&#37327;&#26469;&#34913;&#37327;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#27169;&#22411;&#23545;&#31216;&#32676;&#30340;&#20581;&#22766;&#24615;;&#65288;2&#65289;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#65307;&#65288;3&#65289;&#25552;&#39640;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#23545;&#31216;&#32676;&#30340;&#19981;&#21464;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#19982;&#19981;&#21516;&#23545;&#31216;&#32676;&#30456;&#20851;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#32463;&#39564;&#22320;&#27979;&#37327;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#23545;&#20110;&#24378;&#22823;&#30340;&#35299;&#37322;&#26041;&#27861;&#26159;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07260</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20808;&#39564;&#32593;&#32476;&#30340;&#39640;&#32500;&#36755;&#20986;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#28041;&#21450;&#21040;&#26410;&#30693;&#30340;&#39640;&#32500;&#24230;&#26144;&#23556;&#19968;&#32452;&#21487;&#25511;&#21464;&#37327;&#21040;&#26114;&#36149;&#23454;&#39564;&#32467;&#26524;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#20219;&#21153;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#26102;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24403;&#22788;&#29702;&#39640;&#32500;&#36755;&#20986;&#26102;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#32500;&#24230;&#20027;&#35201;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#20030;&#38598;&#25104;&#30340;BO&#21644;&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#20307;&#31995;&#32467;&#26500;&#36873;&#25321;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36817;&#20284;&#35774;&#35745;&#21464;&#37327;&#21644;&#24863;&#20852;&#36259;&#37327;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#21363;&#20351;&#22312;&#21518;&#32773;&#21462;&#20540;&#20110;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#25110;&#29978;&#33267;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22788;&#29702;&#39640;&#32500;&#24230;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#35752;&#35770;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#21487;&#20449;AI&#30417;&#31649;&#65292;&#21253;&#25324;&#30452;&#25509;&#30417;&#31649;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#20869;&#23481;&#30417;&#31649;&#21644;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#21306;&#20998;&#21442;&#19982;&#32773;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;LGAIMs&#30340;&#21487;&#20449;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.02337</link><description>&lt;p&gt;
&#31649;&#21046;ChatGPT&#21644;&#20854;&#20182;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Regulating ChatGPT and other Large Generative AI Models. (arXiv:2302.02337v5 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#35752;&#35770;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#21487;&#20449;AI&#30417;&#31649;&#65292;&#21253;&#25324;&#30452;&#25509;&#30417;&#31649;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#20869;&#23481;&#30417;&#31649;&#21644;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#21306;&#20998;&#21442;&#19982;&#32773;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;LGAIMs&#30340;&#21487;&#20449;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#65288;LGAIMs&#65289;&#65292;&#22914;ChatGPT&#25110;Stable Diffusion&#65292;&#27491;&#22312;&#24555;&#36895;&#25913;&#21464;&#25105;&#20204;&#30340;&#27807;&#36890;&#12289;&#35828;&#26126;&#21644;&#21019;&#36896;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#27431;&#30431;&#21450;&#20854;&#20182;&#22320;&#21306;&#30340;AI&#30417;&#31649;&#20027;&#35201;&#38598;&#20013;&#22312;&#20256;&#32479;AI&#27169;&#22411;&#19978;&#65292;&#32780;&#38750;LGAIMs&#12290;&#26412;&#25991;&#23558;&#25226;&#36825;&#20123;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#25918;&#32622;&#22312;&#24403;&#21069;&#30340;&#8220;&#21487;&#20449;AI&#30417;&#31649;&#8221;&#36777;&#35770;&#20013;&#65292;&#24182;&#25506;&#35752;&#22914;&#20309;&#35843;&#25972;&#27861;&#24459;&#20197;&#36866;&#24212;&#20854;&#33021;&#21147;&#12290;&#22312;&#22880;&#23450;&#25216;&#26415;&#22522;&#30784;&#20043;&#21518;&#65292;&#26412;&#25991;&#30340;&#27861;&#24459;&#37096;&#20998;&#20998;&#22235;&#27493;&#36827;&#34892;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#30417;&#31649;&#65292;&#65288;2&#65289;&#25968;&#25454;&#20445;&#25252;&#65292;&#65288;3&#65289;&#20869;&#23481;&#30417;&#31649;&#21644;&#65288;4&#65289;&#25919;&#31574;&#24314;&#35758;&#12290;&#23427;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#25429;&#25417;LGAIM&#35774;&#32622;&#20013;&#30340;AI&#20215;&#20540;&#38142;&#65292;&#21306;&#20998;LGAIM&#24320;&#21457;&#20154;&#21592;&#12289;&#37096;&#32626;&#32773;&#12289;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#29992;&#25143;&#65292;&#20197;&#21450;LGAIM&#36755;&#20986;&#30340;&#25509;&#25910;&#32773;&#12290;&#25105;&#20204;&#23558;&#30417;&#31649;&#32844;&#36131;&#38024;&#23545;&#36825;&#20123;&#19981;&#21516;&#30340;&#20215;&#20540;&#38142;&#21442;&#19982;&#32773;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#22235;&#20010;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;LGAIMs&#30340;&#20449;&#20219;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest four strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RHO ($\rho$)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#38142;&#25509;&#23454;&#20307;&#21644;&#20851;&#31995;&#35859;&#35789;&#30340;&#34920;&#31034;&#26469;&#20943;&#23569;&#23545;&#35805;&#31995;&#32479;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#28459;&#27493;&#30340;&#22238;&#22797;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2212.01588</link><description>&lt;p&gt;
&#12298;RHO ($\rho$)&#65306;&#21033;&#29992;&#30693;&#35782;&#38142;&#25509;&#20943;&#23569;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#30340;&#24187;&#35273;&#12299;
&lt;/p&gt;
&lt;p&gt;
RHO ($\rho$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding. (arXiv:2212.01588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RHO ($\rho$)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#38142;&#25509;&#23454;&#20307;&#21644;&#20851;&#31995;&#35859;&#35789;&#30340;&#34920;&#31034;&#26469;&#20943;&#23569;&#23545;&#35805;&#31995;&#32479;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#28459;&#27493;&#30340;&#22238;&#22797;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#24211;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#22238;&#22797;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#24335;&#30340;&#22238;&#22797;&#65292;&#24182;&#20005;&#37325;&#24433;&#21709;&#24212;&#29992;&#12290;&#22806;&#37096;&#30693;&#35782;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#24322;&#26500;&#24615;&#25361;&#25112;&#20102;&#34920;&#24449;&#23398;&#20064;&#21644;&#28304;&#38598;&#25104;&#65292;&#36827;&#19968;&#27493;&#23548;&#33268;&#19981;&#24544;&#23454;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24182;&#29983;&#25104;&#26356;&#24544;&#23454;&#30340;&#22238;&#22797;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#38142;&#25509;&#23454;&#20307;&#21644;&#20851;&#31995;&#35859;&#35789;&#30340;&#34920;&#31034;&#26469;&#20943;&#23569;&#24187;&#35273;&#30340;&#26041;&#27861;&#65292;&#21363;RHO ($\rho$)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;(1)&#26412;&#22320;&#30693;&#35782;&#22522;&#30784;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#19982;&#23545;&#24212;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30456;&#32467;&#21512;&#65307;&#20197;&#21450;(2)&#20840;&#23616;&#30693;&#35782;&#22522;&#30784;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#20351;RHO&#20855;&#26377;&#22810;&#27425;&#36339;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#28459;&#27493;&#30340;&#22238;&#22797;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#35805;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36890;&#36807;&#22870;&#21169;&#35774;&#35745;&#35753;&#26234;&#33021;&#20307;&#23398;&#20064;&#23631;&#34109;&#26080;&#20851;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;RL&#26041;&#27861;&#26356;&#22909;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.15589</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#8212;&#8212;&#19981;&#36866;&#29992;&#21160;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inapplicable Actions Learning for Knowledge Transfer in Reinforcement Learning. (arXiv:2211.15589v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36890;&#36807;&#22870;&#21169;&#35774;&#35745;&#35753;&#26234;&#33021;&#20307;&#23398;&#20064;&#23631;&#34109;&#26080;&#20851;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;RL&#26041;&#27861;&#26356;&#22909;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#35768;&#22810;&#21487;&#33021;&#21160;&#20316;&#30340;&#29615;&#22659;&#20013;&#24456;&#38590;&#25193;&#23637;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#26159;&#22312;&#27599;&#20010;&#21487;&#33021;&#30340;&#29366;&#24577;&#19979;&#32771;&#34385;&#30456;&#21516;&#30340;&#22266;&#23450;&#21160;&#20316;&#31354;&#38388;&#65292;&#36825;&#24847;&#21619;&#30528;&#26234;&#33021;&#20307;&#24517;&#39035;&#29702;&#35299;&#12289;&#21516;&#26102;&#23398;&#20064;&#22914;&#20309;&#26368;&#22823;&#21270;&#20854;&#22870;&#21169;&#65292;&#24573;&#30053;&#26080;&#20851;&#30340;&#21160;&#20316;&#65292;&#27604;&#22914;&#8220;&#19981;&#36866;&#29992;&#21160;&#20316;&#8221;&#65288;&#21363;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#25191;&#34892;&#26102;&#19981;&#20250;&#24433;&#21709;&#29615;&#22659;&#30340;&#21160;&#20316;&#65289;&#12290;&#20102;&#35299;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#23631;&#34109;&#19982;&#23547;&#25214;&#26368;&#20248;&#31574;&#30053;&#30456;&#20851;&#30340;&#21160;&#20316;&#65292;&#20174;&#31574;&#30053;&#20998;&#24067;&#20013;&#21435;&#25506;&#32034;&#26080;&#20851;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#24110;&#21161;&#38477;&#20302;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#38454;&#27573;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#35774;&#35745;&#26041;&#27861;&#23398;&#20064;&#23631;&#34109;&#19981;&#36866;&#29992;&#30340;&#21160;&#20316;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#19968;&#20010;&#21160;&#20316;&#26159;&#21542;&#36866;&#29992;&#65292;&#24182;&#26681;&#25454;&#20854;&#39044;&#27979;&#33719;&#24471;&#22870;&#21169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#26679;&#26412;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) algorithms are known to scale poorly to environments with many available actions, requiring numerous samples to learn an optimal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as $\textit{inapplicable actions}$ (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. While this technique has been formalized for quite some time within the Automated Planning community with the concept of precondition in the STRIPS language, RL algorithms have never formally taken advantage of this information to prune the search space to explore. This is typically done in an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.08073</link><description>&lt;p&gt;
GLUE-X: &#20174;ODD&#26222;&#36866;&#24615;&#35282;&#24230;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24050;&#30693;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#30340;ODD&#26222;&#36866;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#24378;&#35843;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#22914;&#20309;&#34913;&#37327;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#65288;&#21253;&#25324;GPT-3&#21644;GPT-3.5&#65289;&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#30830;&#35748;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#19979;&#65292;&#19982;ID&#20934;&#30830;&#24230;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#38656;&#35201;&#25913;&#21892;NLP&#20219;&#21153;&#20013;&#30340;OOD&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW&#65292;&#30456;&#36739;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05208</link><description>&lt;p&gt;
&#32593;&#32476;&#27969;&#30340;&#22270;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Modeling of Network Flows. (arXiv:2209.05208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW&#65292;&#30456;&#36739;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#38382;&#39064;&#28041;&#21450;&#23558;&#27969;&#37327;&#20998;&#24067;&#22312;&#32593;&#32476;&#20013;&#65292;&#20197;&#20351;&#22522;&#30784;&#35774;&#26045;&#24471;&#21040;&#26377;&#25928;&#21033;&#29992;&#65292;&#36825;&#22312;&#20132;&#36890;&#36816;&#36755;&#21644;&#29289;&#27969;&#20013;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#20854;&#20013;&#65292;&#22810;&#21830;&#21697;&#32593;&#32476;&#27969; (MCNF) &#38382;&#39064;&#26159;&#26222;&#36941;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#22312;&#22810;&#20010;&#28304;&#21644;&#27719;&#20043;&#38388;&#20998;&#37197;&#19981;&#21516;&#22823;&#23567;&#30340;&#22810;&#20010;&#27969;&#65292;&#21516;&#26102;&#23454;&#29616;&#38142;&#36335;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;&#30001;&#20110;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#30340;&#21560;&#24341;&#21147;&#65292;&#36825;&#20123;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#27969;&#38382;&#39064;&#22270;&#23398;&#20064;&#26550;&#26500; PEW (Per-Edge Weights)&#12290;&#27492;&#26041;&#27861;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#24182;&#27839;&#30528;&#27599;&#20010;&#38142;&#25509;&#20351;&#29992;&#19981;&#21516;&#21442;&#25968;&#21270;&#30340;&#28040;&#24687;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992; $17$ &#20010;&#26381;&#21153;&#25552;&#20379;&#21830;&#25299;&#25169;&#21644; $2$ &#20010;&#36335;&#30001;&#26041;&#26696;&#36827;&#34892;&#20114;&#32852;&#32593;&#27969;&#37327;&#36335;&#30001;&#26696;&#20363;&#30740;&#31350;&#65292;&#23545;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; PEW &#30456;&#23545;&#20110;&#19981;&#32771;&#34385;&#38142;&#25509;&#30340;&#27969;&#37327;&#29305;&#23450;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#22312;&#36335;&#30001;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network flow problems, which involve distributing traffic over a network such that the underlying infrastructure is used effectively, are ubiquitous in transportation and logistics. Among them, the Multi-Commodity Network Flow (MCNF) problem is of general interest, as it concerns the distribution of multiple flows of different sizes between several sources and sinks, while achieving effective utilization of the links. Due to the appeal of data-driven optimization, these problems have increasingly been approached using graph learning methods. In this paper, we propose a novel graph learning architecture for network flow problems called Per-Edge Weights (PEW). This method builds on a Graph Attention Network and uses distinctly parametrized message functions along each link. We extensively evaluate the proposed solution through an Internet flow routing case study using $17$ Service Provider topologies and $2$ routing schemes. We show that PEW yields substantial gains over architectures wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.01937</link><description>&lt;p&gt;
BoMD&#65306;&#36866;&#29992;&#20110;&#22024;&#26434;X&#20809;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#21253;
&lt;/p&gt;
&lt;p&gt;
BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#38382;&#39064;&#30340;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20855;&#26377;&#28165;&#27905;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#36825;&#31181;&#25163;&#21160;&#27880;&#37322;&#30340;&#39640;&#25104;&#26412;&#65292;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#20381;&#36182;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#20107;&#23454;&#19978;&#65292;&#35768;&#22810;&#33016;&#37096;X&#20809;&#20998;&#31867;&#22120;&#24050;&#32463;&#20174;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#19981;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#23548;&#33268;&#27425;&#20248;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;CXR&#25968;&#25454;&#38598;&#22823;&#22810;&#26159;&#22810;&#26631;&#35760;&#30340;&#65292;&#22240;&#27492;&#24403;&#21069;&#35774;&#35745;&#29992;&#20110;&#22810;&#31867;&#38382;&#39064;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#19981;&#33021;&#36731;&#26494;&#22320;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22024;&#26434;&#22810;&#26631;&#31614;CXR&#23398;&#20064;&#65292;&#20854;&#20013;&#26816;&#27979;&#24182;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#24120;&#35265;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#20351;&#29992;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#20854;&#20248;&#28857;&#21450;&#23616;&#38480;&#24615;&#65292;&#20174;&#32593;&#32476;&#32467;&#26500;&#21644;&#24212;&#29992;&#20004;&#20010;&#35282;&#24230;&#23457;&#35270;&#20102;&#20854;&#36866;&#24212;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2202.07125</link><description>&lt;p&gt;
Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformers in Time Series: A Survey. (arXiv:2202.07125v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#24635;&#32467;&#20102;&#20854;&#20248;&#28857;&#21450;&#23616;&#38480;&#24615;&#65292;&#20174;&#32593;&#32476;&#32467;&#26500;&#21644;&#24212;&#29992;&#20004;&#20010;&#35282;&#24230;&#23457;&#35270;&#20102;&#20854;&#36866;&#24212;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20063;&#24341;&#36215;&#20102;&#26102;&#38388;&#24207;&#21015;&#31038;&#21306;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;Transformer&#30340;&#22810;&#20010;&#20248;&#21183;&#20043;&#19968;&#26159;&#33021;&#22815;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#29305;&#21035;&#36866;&#21512;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#28857;&#21450;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#32593;&#32476;&#32467;&#26500;&#21644;&#24212;&#29992;&#20004;&#20010;&#23618;&#38754;&#23457;&#35270;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#36866;&#24212;&#21644;&#25913;&#36827;&#12290;&#20174;&#32593;&#32476;&#32467;&#26500;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20026;&#20102;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#32780;&#20570;&#20986;&#30340;&#25913;&#21464;&#21644;&#35843;&#25972;&#12290;&#20174;&#24212;&#29992;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#26681;&#25454;&#24120;&#35265;&#20219;&#21153;&#65288;&#21253;&#25324;&#39044;&#27979;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#31867;&#65289;&#23545;&#26102;&#38388;&#24207;&#21015;Transformer&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#32771;&#34385;&#25152;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#25152;&#20851;&#32852;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#38543;&#26426;&#21644;&#35748;&#35782;&#37096;&#20998;&#12290;&#30456;&#27604;&#20110;&#19981;&#20351;&#29992;&#35813;&#27169;&#22411;&#65292;&#20351;&#29992;&#38169;&#35823;&#21464;&#37327;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#23545;&#24050;&#30693;&#22238;&#24402;&#20989;&#25968;&#30340;&#35206;&#30422;&#29575;&#65292;&#19988;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.09095</link><description>&lt;p&gt;
&#28145;&#24230;&#22238;&#24402;&#20013;&#30340;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Aleatoric uncertainty for Errors-in-Variables models in deep regression. (arXiv:2105.09095v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.09095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#32771;&#34385;&#25152;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#25152;&#20851;&#32852;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#38543;&#26426;&#21644;&#35748;&#35782;&#37096;&#20998;&#12290;&#30456;&#27604;&#20110;&#19981;&#20351;&#29992;&#35813;&#27169;&#22411;&#65292;&#20351;&#29992;&#38169;&#35823;&#21464;&#37327;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#23545;&#24050;&#30693;&#22238;&#24402;&#20989;&#25968;&#30340;&#35206;&#30422;&#29575;&#65292;&#19988;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#22788;&#29702;&#21487;&#20197;&#35745;&#31639;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#22238;&#24402;&#20013;&#20351;&#29992;&#21464;&#37327;&#35823;&#24046;&#30340;&#27010;&#24565;&#65292;&#20197;&#32771;&#34385;&#25152;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#25152;&#20851;&#32852;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#30456;&#20851;&#20294;&#36890;&#24120;&#34987;&#24573;&#35270;&#30340;&#19981;&#30830;&#23450;&#24615;&#28304;&#65292;&#24182;&#23558;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#38543;&#26426;&#21644;&#35748;&#35782;&#37096;&#20998;&#65292;&#36825;&#22312;&#32479;&#35745;&#23398;&#35282;&#24230;&#26356;&#23436;&#25972;&#65292;&#32780;&#19988;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#26356;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#30340;&#20363;&#23376;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#20351;&#29992;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#20250;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#20351;&#29992;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#30340;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#23545;&#20110;&#24050;&#30693;&#22238;&#24402;&#20989;&#25968;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21464;&#37327;&#35823;&#24046;&#27169;&#22411;&#22823;&#22823;&#25552;&#39640;&#20102;&#23545;&#22522;&#30784;&#20107;&#23454;&#30340;&#35206;&#30422;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Bayesian treatment of deep learning allows for the computation of uncertainties associated with the predictions of deep neural networks. We show how the concept of Errors-in-Variables can be used in Bayesian deep regression to also account for the uncertainty associated with the input of the employed neural network. The presented approach thereby exploits a relevant, but generally overlooked, source of uncertainty and yields a decomposition of the predictive uncertainty into an aleatoric and epistemic part that is more complete and, in many cases, more consistent from a statistical perspective. We discuss the approach along various simulated and real examples and observe that using an Errors-in-Variables model leads to an increase in the uncertainty while preserving the prediction performance of models without Errors-in-Variables. For examples with known regression function we observe that this ground truth is substantially better covered by the Errors-in-Variables model, indicating 
&lt;/p&gt;</description></item></channel></rss>