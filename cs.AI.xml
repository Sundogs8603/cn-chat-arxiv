<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#26410;&#35265;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#24456;&#22810;&#27169;&#22411;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#21387;&#32553;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.00861</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Generalization and Robustness via Data Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00861
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#26410;&#35265;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#24456;&#22810;&#27169;&#22411;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#21387;&#32553;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#38754;&#20020;&#25968;&#25454;&#27745;&#26579;&#12289;&#23545;&#25552;&#31034;&#25935;&#24863;&#20197;&#21450;&#22522;&#20934;&#27979;&#35797;&#21019;&#24314;&#25104;&#26412;&#39640;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#27979;&#35797;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#20854;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#27867;&#21270;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20174;2017&#24180;&#21040;2023&#24180;&#20849;83&#20010;&#26376;&#30340;&#20840;&#38754;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#25130;&#27490;&#26085;&#26399;&#23558;&#25968;&#25454;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65306;1&#65289;&#27979;&#35797;&#26399;&#30340;&#21387;&#32553;&#24615;&#33021;&#20316;&#20026;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#34913;&#37327;&#65307;2&#65289;&#35757;&#32451;&#26399;&#21644;&#27979;&#35797;&#26399;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#20316;&#20026;&#40065;&#26834;&#24615;&#30340;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#30340;&#21387;&#32553;&#29575;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#26174;&#33879;&#38477;&#20302;&#65292;&#20294;&#20687;... (&#20869;&#23481;&#36807;&#38271;&#65292;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a
&lt;/p&gt;</description></item><item><title>SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00854</link><description>&lt;p&gt;
SymbolicAI: &#19968;&#20010;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SymbolicAI: A framework for logic-based approaches combining generative models and solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00854
&lt;/p&gt;
&lt;p&gt;
SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SymbolicAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#19988;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#27969;&#31243;&#31649;&#29702;&#12290;SymbolicAI&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#26469;&#25191;&#34892;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;&#29983;&#25104;&#27169;&#22411;&#19982;&#21508;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#32534;&#31243;&#21407;&#29702;&#26469;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#21487;&#24494;&#20998;&#21644;&#32463;&#20856;&#32534;&#31243;&#33539; paradigms &#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#22810;&#24577;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#33258;&#25351;&#30340;&#25968;&#25454;&#27969;&#25805;&#20316;&#65292;&#23558;LLM&#30340;&#36755;&#20986;&#19982;&#29992;&#25143;&#30340;&#30446;&#26631;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20855;&#26377;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#33021;&#21147;&#30340;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36807;&#28193;&#65292;&#24182;&#19982;&#25797;&#38271;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#19987;&#19994;&#21270;&#35843;&#20248;&#27169;&#22411;&#25110;&#27714;&#35299;&#22120;&#37197;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;X-CBA&#30340;&#26032;&#39062;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#20197;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00839</link><description>&lt;p&gt;
X-CBA: &#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;CatBoosted Anomal-E&#29992;&#20110;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;X-CBA&#30340;&#26032;&#39062;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#20197;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23041;&#32961;&#26085;&#30410;&#22797;&#26434;&#30340;&#26102;&#20195;&#65292;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#30340;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20026;&#35782;&#21035;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#30340;&#25915;&#20987;&#21644;&#24322;&#24120;&#25552;&#20379;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;IDS&#20013;&#20351;&#29992;ML&#21644;DL&#27169;&#22411;&#23548;&#33268;&#20102;&#20449;&#20219;&#36196;&#23383;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#12290;&#36825;&#31181;IDS&#30740;&#31350;&#20013;&#30340;&#36879;&#26126;&#24230;&#24046;&#36317;&#26174;&#33879;&#65292;&#24433;&#21709;&#20102;&#20449;&#24515;&#21644;&#38382;&#36131;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#31216;&#20026;X-CBA&#65292;&#23427;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32467;&#26500;&#20248;&#21183;&#26469;&#26377;&#25928;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#19982;&#22823;&#22810;&#25968;&#20197;GNN&#20026;&#22522;&#30784;&#30340;IDS&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#32593;&#32476;&#27969;&#37327;&#21644;&#33410;&#28857;&#29305;&#24449;&#65292;&#36824;&#36890;&#36807;&#32593;&#32476;&#27969;&#37327;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#26469;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to
&lt;/p&gt;</description></item><item><title>ALISON&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#39118;&#26684;&#23398;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;AA&#27169;&#22411;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#28151;&#28102;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#28151;&#28102;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00835</link><description>&lt;p&gt;
ALISON: &#24555;&#36895;&#26377;&#25928;&#30340;&#39118;&#26684;&#23398;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
ALISON: Fast and Effective Stylometric Authorship Obfuscation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00835
&lt;/p&gt;
&lt;p&gt;
ALISON&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#39118;&#26684;&#23398;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;AA&#27169;&#22411;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#28151;&#28102;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#28151;&#28102;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#24230;&#65288;AA&#65289;&#21644;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#65288;AO&#65289;&#26159;&#38544;&#31169;&#30740;&#31350;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#20004;&#39033;&#31454;&#20105;&#20219;&#21153;&#12290;&#29616;&#20195;AA&#21033;&#29992;&#20316;&#32773;&#30340;&#19968;&#36143;&#20889;&#20316;&#39118;&#26684;&#65292;&#20351;&#29992;AA&#20998;&#31867;&#22120;&#23558;&#25991;&#26412;&#19982;&#20854;&#20316;&#32773;&#21305;&#37197;&#12290;AO&#26159;&#30456;&#24212;&#30340;&#23545;&#25239;&#24615;&#20219;&#21153;&#65292;&#26088;&#22312;&#20197;&#19968;&#31181;&#26041;&#24335;&#20462;&#25913;&#25991;&#26412;&#65292;&#20351;&#20854;&#35821;&#20041;&#24471;&#21040;&#20445;&#30041;&#65292;&#20294;AA&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#25512;&#26029;&#20854;&#20316;&#32773;&#12290;&#20026;&#20102;&#35299;&#20915;&#26368;&#20808;&#36827;&#30340;AA&#26041;&#27861;&#24341;&#21457;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;AO&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#35757;&#32451;&#21644;&#28151;&#28102;&#36895;&#24230;&#36807;&#24930;&#65288;&#36890;&#24120;&#38656;&#35201;&#25968;&#23567;&#26102;&#65289;&#65292;&#20351;&#29992;&#36215;&#26469;&#20173;&#28982;&#19981;&#22826;&#23454;&#38469;&#12290;&#38754;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;AO&#26041;&#27861;ALISON&#65292;&#23427;&#65288;1&#65289;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;/&#28151;&#28102;&#26102;&#38388;&#65292;&#28436;&#31034;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;AO&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#30340;&#28151;&#28102;&#36895;&#24230;&#65292;&#65288;2&#65289;&#36890;&#36807;&#25915;&#20987;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#19977;&#31181;&#22522;&#20110;Transformer&#30340;AA&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#28151;&#28102;&#25104;&#21151;&#29575;&#65292;&#36890;&#24120;&#27604;&#31454;&#20105;&#26041;&#27861;&#34920;&#29616;&#22909;15%&#12290;
&lt;/p&gt;
&lt;p&gt;
Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing method
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;YANG&#25968;&#25454;&#27169;&#22411;&#19982;&#40657;&#27934;&#25935;&#24863;&#24230;&#37327;&#30697;&#38453;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39592;&#24178;&#32593;&#32476;&#20013;&#36827;&#34892;&#40657;&#27934;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22635;&#34917;&#20102;&#39592;&#24178;&#32593;&#32476;&#40657;&#27934;&#26816;&#27979;&#26041;&#27861;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26816;&#27979;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.00831</link><description>&lt;p&gt;
YANG&#36741;&#21161;&#19979;&#30340;&#39592;&#24178;&#32593;&#32476;&#40657;&#27934;&#26816;&#27979;&#32479;&#19968;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;YANG&#25968;&#25454;&#27169;&#22411;&#19982;&#40657;&#27934;&#25935;&#24863;&#24230;&#37327;&#30697;&#38453;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39592;&#24178;&#32593;&#32476;&#20013;&#36827;&#34892;&#40657;&#27934;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#22635;&#34917;&#20102;&#39592;&#24178;&#32593;&#32476;&#40657;&#27934;&#26816;&#27979;&#26041;&#27861;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20114;&#32852;&#32593;&#39592;&#24178;&#32593;&#32476;&#20013;&#35299;&#20915;&#40657;&#27934;&#25925;&#38556;&#30340;&#37325;&#35201;&#24615;&#19981;&#21487;&#24573;&#35270;&#65292;&#20294;&#39592;&#24178;&#32593;&#32476;&#20013;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#20173;&#28982;&#32570;&#20047;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31227;&#21160;&#33258;&#32452;&#32593;(MANETs)&#19978;&#65292;&#32780;MANETs&#22312;&#21160;&#24577;&#12289;&#21327;&#35758;&#21644;&#25299;&#25169;&#19978;&#26377;&#30528;&#23436;&#20840;&#19981;&#21516;&#30340;&#25805;&#20316;&#65292;&#22240;&#27492;&#20854;&#30740;&#31350;&#32467;&#26524;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#39592;&#24178;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#39592;&#24178;&#32593;&#32476;&#20013;&#30340;&#40657;&#27934;&#25925;&#38556;&#26816;&#27979;&#26159;&#19968;&#39033;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#38656;&#35201;&#32771;&#34385;&#22810;&#26679;&#30340;&#26465;&#20214;&#65292;&#36825;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#30340;&#32593;&#32476;&#25968;&#25454;&#65292;&#20351;&#24471;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#21464;&#24471;&#24182;&#19981;&#30452;&#35266;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;Yet Another Next Generation (YANG)&#25968;&#25454;&#27169;&#22411;&#19982;&#40657;&#27934;&#25935;&#24863;&#24230;&#37327;&#30697;&#38453;(BHMM)&#20998;&#26512;&#26469;&#36827;&#34892;&#39592;&#24178;&#32593;&#32476;&#20013;&#30340;&#40657;&#27934;&#26816;&#27979;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#36873;&#25321;&#21644;&#20998;&#26512;&#20102;&#19982;&#40657;&#27934;&#26816;&#27979;&#30456;&#20851;&#30340;&#22235;&#20010;YANG&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking. This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks. Furthermore, detecting Black Hole failures in backbone networks is particularly challenging. It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward. Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our method of selecting and analyzing four YANG models relevant to Black Hole de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36719;&#36866;&#37197;&#22120;&#28151;&#21512;&#23454;&#29616;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#38899;&#39057;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00828</link><description>&lt;p&gt;
&#36890;&#36807;&#36719;&#36866;&#37197;&#22120;&#28151;&#21512;&#23454;&#29616;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36719;&#36866;&#37197;&#22120;&#28151;&#21512;&#23454;&#29616;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#38899;&#39057;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#36817;&#24180;&#26469;&#24320;&#22987;&#20852;&#36215;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#21487;&#25215;&#21463;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21487;&#20197;&#24212;&#29992;&#20110;&#21464;&#25442;&#22120;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36825;&#20123;&#26159;&#24403;&#21069;&#20247;&#22810;&#39046;&#22495;&#20013;&#39047;&#26377;&#25104;&#23601;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;MoE&#20027;&#35201;&#29992;&#20110;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#20854;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#35797;&#22270;&#25581;&#31034;&#20351;&#29992;MoE&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#24494;&#35843;&#21040;&#38899;&#39057;&#21644;&#35821;&#38899;&#19979;&#28216;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#36866;&#37197;&#22120;&#28151;&#21512;&#65288;Soft-MoA&#65289;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36866;&#37197;&#22120;&#20316;&#20026;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#36719;MoE&#26041;&#27861;&#65292;&#22312;&#36755;&#20837;&#35760;&#21495;&#21644;&#19987;&#23478;&#20043;&#38388;&#36827;&#34892;&#36719;&#20998;&#37197;&#65292;&#20197;&#20445;&#25345;&#35745;&#31639;&#26102;&#38388;&#26377;&#38480;&#12290;&#23545;4&#20010;&#22522;&#20934;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Soft-MoA&#20248;&#20110;&#21333;&#19968;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model's capacity while maintaining the computational cost affordable. Furthermore, they can be applied to both Transformers and State Space Models, the current state-of-the-art models in numerous fields. While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning settings is under-explored. To narrow this gap, this paper attempts to demystify the use of MoE for parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and speech downstream tasks. Specifically, we propose Soft Mixture of Adapters (Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited. Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on pa
&lt;/p&gt;</description></item><item><title>SLIM&#26159;&#19968;&#31181;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#32452;&#21512;&#22810;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#20811;&#26381;&#20102;&#22870;&#21169;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2402.00823</link><description>&lt;p&gt;
SLIM: &#22810;&#21028;&#21035;&#22120;&#22312;&#25216;&#33021;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SLIM: Skill Learning with Multiple Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00823
&lt;/p&gt;
&lt;p&gt;
SLIM&#26159;&#19968;&#31181;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#32452;&#21512;&#22810;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#20811;&#26381;&#20102;&#22870;&#21169;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#25216;&#33021;&#23398;&#20064;&#26088;&#22312;&#33719;&#21462;&#21033;&#29992;&#29615;&#22659;&#30340;&#24213;&#23618;&#21160;&#24577;&#30340;&#26377;&#29992;&#34892;&#20026;&#12290;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#21487;&#33021;&#28041;&#21450;&#21040;&#29615;&#22659;&#20013;&#24456;&#22810;&#33258;&#30001;&#24230;&#65292;&#21333;&#32431;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26080;&#27861;&#20135;&#29983;&#26377;&#29992;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLIM&#65292;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#28857;&#26159;&#65292;&#22312;&#28436;&#21592;-&#35780;&#35770;&#32773;&#26694;&#26550;&#20013;&#21033;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#26469;&#20248;&#38597;&#22320;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#21516;&#26102;&#20811;&#26381;&#22870;&#21169;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#30340;&#24178;&#25200;&#65292;&#38459;&#30861;&#23545;&#26377;&#29992;&#25216;&#33021;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop man
&lt;/p&gt;</description></item><item><title>WiOpen&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#22522;&#20110;Wi-Fi&#30340;&#24320;&#25918;&#24335;&#25163;&#21183;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19981;&#30830;&#23450;&#24615;&#21644;&#23450;&#20041;&#31934;&#30830;&#30340;&#20915;&#31574;&#36793;&#30028;&#26469;&#35299;&#20915;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26410;&#35265;&#25163;&#21183;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00822</link><description>&lt;p&gt;
WiOpen: &#19968;&#31181;&#31283;&#20581;&#30340;&#22522;&#20110;Wi-Fi&#30340;&#24320;&#25918;&#24335;&#25163;&#21183;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00822
&lt;/p&gt;
&lt;p&gt;
WiOpen&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#22522;&#20110;Wi-Fi&#30340;&#24320;&#25918;&#24335;&#25163;&#21183;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19981;&#30830;&#23450;&#24615;&#21644;&#23450;&#20041;&#31934;&#30830;&#30340;&#20915;&#31574;&#36793;&#30028;&#26469;&#35299;&#20915;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26410;&#35265;&#25163;&#21183;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Wi-Fi&#30340;&#25163;&#21183;&#35782;&#21035;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23553;&#38381;&#38598;&#33539;&#24335;&#19978;&#65292;&#20854;&#20013;&#25152;&#26377;&#27979;&#35797;&#25163;&#21183;&#22312;&#35757;&#32451;&#26399;&#38388;&#37117;&#26159;&#39044;&#23450;&#20041;&#30340;&#12290;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#27979;&#35797;&#20013;&#65292;&#26410;&#35265;&#36807;&#30340;&#25163;&#21183;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#20026;&#24050;&#30693;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WiOpen&#65292;&#19968;&#31181;&#31283;&#20581;&#30340;&#22522;&#20110;Wi-Fi&#30340;&#24320;&#25918;&#24335;&#25163;&#21183;&#35782;&#21035;&#26694;&#26550;&#12290;&#23454;&#29616;&#24320;&#25918;&#24335;&#25163;&#21183;&#35782;&#21035;&#38656;&#35201;&#35299;&#20915;Wi-Fi&#24863;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#30001;&#22122;&#22768;&#21644;&#39046;&#22495;&#24341;&#36215;&#65292;&#22312;&#25910;&#38598;&#30340;Wi-Fi&#24863;&#27979;&#25968;&#25454;&#20013;&#23548;&#33268;&#20102;&#24191;&#27867;&#25955;&#24067;&#21644;&#19981;&#35268;&#21017;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#31867;&#21035;&#20043;&#38388;&#30340;&#25968;&#25454;&#27169;&#31946;&#20197;&#21450;&#23450;&#20041;&#36866;&#24403;&#30340;&#20915;&#31574;&#36793;&#30028;&#20197;&#35782;&#21035;&#26410;&#30693;&#25163;&#21183;&#30340;&#25361;&#25112;&#20063;&#38543;&#20043;&#20986;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;WiOpen&#37319;&#29992;&#20102;&#19968;&#20010;&#21452;&#37325;&#26041;&#27861;&#26469;&#28040;&#38500;&#19981;&#30830;&#23450;&#24615;&#24182;&#23450;&#20041;&#31934;&#30830;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition. However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training. This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing. To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework. Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data. Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise. To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries. Initially, it addresses uncerta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#21033;&#29992;&#36817;&#20284;&#22522;&#20110;&#27169;&#22411;&#30340;&#23631;&#34109;&#25216;&#26415;&#23454;&#29616;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00816</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#21033;&#29992;&#36817;&#20284;&#22522;&#20110;&#27169;&#22411;&#30340;&#23631;&#34109;&#25216;&#26415;&#23454;&#29616;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#36830;&#32493;&#29615;&#22659;&#20013;&#21033;&#29992;&#36817;&#20284;&#22522;&#20110;&#27169;&#22411;&#30340;&#23631;&#34109;&#25216;&#26415;&#23454;&#29616;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23631;&#34109;&#25216;&#26415;&#26159;&#23454;&#29616;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23631;&#34109;&#26041;&#27861;&#23384;&#22312;&#30456;&#24403;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#20351;&#20854;&#38590;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#37096;&#32626;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#25110;&#34892;&#21160;&#31354;&#38388;&#30340;&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#23558;&#26356;&#36890;&#29992;&#30340;&#36817;&#20284;&#22522;&#20110;&#27169;&#22411;&#30340;&#23631;&#34109;&#65288;AMBS&#65289;&#26694;&#26550;&#25193;&#23637;&#21040;&#36830;&#32493;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;Safety Gym&#20316;&#20026;&#25105;&#20204;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#23558;AMBS&#19982;&#27969;&#34892;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#20026;&#36830;&#32493;&#29615;&#22659;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#24809;&#32602;&#25216;&#26415;&#65292;&#30452;&#25509;&#20462;&#25913;&#31574;&#30053;&#26799;&#24230;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shielding is a popular technique for achieving safe reinforcement learning (RL). However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces. In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting. In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms. We also provide strong probabilistic safety guarantees for the continuous setting. In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#21327;&#20316;&#35013;&#37197;&#22330;&#26223;&#20013;&#65292;Cobot&#30340;&#29983;&#20135;&#33410;&#22863;&#23545;&#21442;&#19982;&#32773;&#30340;&#32463;&#39564;&#24615;&#23450;&#20301;&#25511;&#21046;&#21644;&#24773;&#32490;&#29366;&#24577;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#22312;&#24773;&#32490;&#29366;&#24577;&#21644;&#23450;&#20301;&#25511;&#21046;&#26041;&#38754;&#26410;&#21457;&#29616;&#24046;&#24322;&#65292;&#20294;&#32771;&#34385;&#21040;&#20854;&#20182;&#24515;&#29702;&#21464;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#32771;&#34385;&#20010;&#20307;&#30340;&#24515;&#29702;&#29305;&#24449;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.00808</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#35013;&#37197;&#22330;&#26223;&#20013;&#65292;&#25506;&#32034;Cobot&#30340;&#29983;&#20135;&#33410;&#22863;&#12289;&#25511;&#21046;&#23450;&#20301;&#21644;&#24773;&#32490;&#29366;&#24577;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#21327;&#20316;&#35013;&#37197;&#22330;&#26223;&#20013;&#65292;Cobot&#30340;&#29983;&#20135;&#33410;&#22863;&#23545;&#21442;&#19982;&#32773;&#30340;&#32463;&#39564;&#24615;&#23450;&#20301;&#25511;&#21046;&#21644;&#24773;&#32490;&#29366;&#24577;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#22312;&#24773;&#32490;&#29366;&#24577;&#21644;&#23450;&#20301;&#25511;&#21046;&#26041;&#38754;&#26410;&#21457;&#29616;&#24046;&#24322;&#65292;&#20294;&#32771;&#34385;&#21040;&#20854;&#20182;&#24515;&#29702;&#21464;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#32771;&#34385;&#20010;&#20307;&#30340;&#24515;&#29702;&#29305;&#24449;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#65292;&#21327;&#20316;&#26426;&#22120;&#20154;&#65288;cobots&#65289;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#19988;&#23545;&#35780;&#20272;&#21644;&#27979;&#37327;cobots&#30340;&#26576;&#20123;&#29305;&#24449;&#23545;&#20154;&#30340;&#24433;&#21709;&#20135;&#29983;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#22312;&#26412;&#27425;&#35797;&#28857;&#30740;&#31350;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#20010;cobots&#30340;&#29983;&#20135;&#33410;&#22863;&#65288;C1-&#24930;&#12289;C2-&#24555;&#12289;C3-&#26681;&#25454;&#21442;&#19982;&#32773;&#30340;&#27493;&#35843;&#35843;&#25972;&#65289;&#23545;31&#21517;&#21442;&#19982;&#32773;&#30340;&#32463;&#39564;&#24615;&#23450;&#20301;&#25511;&#21046;&#65288;ELoC&#65289;&#21644;&#24773;&#32490;&#29366;&#24577;&#30340;&#24433;&#21709;&#12290;&#36824;&#32771;&#34385;&#20102;&#25805;&#20316;&#20154;&#21592;&#30340;&#32489;&#25928;&#12289;&#22522;&#26412;&#20869;&#37096;&#23450;&#20301;&#25511;&#21046;&#31243;&#24230;&#21644;&#23545;&#26426;&#22120;&#20154;&#30340;&#24577;&#24230;&#12290;&#20851;&#20110;&#24773;&#32490;&#29366;&#24577;&#21644;ELoC&#65292;&#22312;&#19977;&#31181;&#26465;&#20214;&#19979;&#27809;&#26377;&#21457;&#29616;&#24046;&#24322;&#65292;&#20294;&#32771;&#34385;&#21040;&#20854;&#20182;&#24515;&#29702;&#21464;&#37327;&#65292;&#20986;&#29616;&#20102;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#32467;&#26524;&#20284;&#20046;&#34920;&#26126;&#38656;&#35201;&#32771;&#34385;&#20010;&#20307;&#30340;&#24515;&#29702;&#29305;&#24449;&#65292;&#20197;&#25552;&#20379;&#19981;&#21516;&#21644;&#26368;&#20339;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industrial scenarios, there is widespread use of collaborative robots (cobots), and growing interest is directed at evaluating and measuring the impact of some characteristics of the cobot on the human factor. In the present pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 - Adapted to the participant's pace) of a cobot has on the Experiential Locus of Control (ELoC) and the emotional state of 31 participants has been examined. The operators' performance, the degree of basic internal Locus of Control, and the attitude towards the robots were also considered. No difference was found regarding the emotional state and the ELoC in the three conditions, but considering the other psychological variables, a more complex situation emerges. Overall, results seem to indicate a need to consider the person's psychological characteristics to offer a differentiated and optimal interaction experience.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.00795</link><description>&lt;p&gt;
LLMs&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#25511;&#21046;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;-shot&#20219;&#21153;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#29702;&#35299;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#21463;&#29289;&#29702;&#21407;&#29702;&#25511;&#21046;&#30340;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;LLaMA 2&#22312;&#27809;&#26377;&#24494;&#35843;&#25110;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;LLMs&#20013;&#25552;&#21462;&#22810;&#20301;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.00794</link><description>&lt;p&gt;
ReAGent: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAs&#65289;&#65292;&#22914;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30830;&#23450;&#25152;&#26377;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24320;&#21457;&#21644;&#27979;&#35797;FAs&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#25991;&#26412;&#29983;&#25104;&#19978;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;FAs&#26469;&#22788;&#29702;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#26550;&#26500;&#21644;&#20219;&#21153;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;FA&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#21644;&#20219;&#21153;&#12290;&#36825;&#20351;&#24471;&#38024;&#23545;&#22823;&#22411;LMs&#36873;&#25321;FA&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#36755;&#20837;&#37325;&#35201;&#24615;&#30340;&#25512;&#23548;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#36882;&#65292;&#21253;&#25324;&#21487;&#33021;&#26159;&#38480;&#21046;&#24615;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;LMs&#30340;&#27169;&#22411;&#26080;&#20851;FA&#65292;&#31216;&#20026;&#36882;&#24402;&#24402;&#22240;&#29983;&#25104;&#22120;&#65288;ReAGent&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.00793</link><description>&lt;p&gt;
&#26080;&#27861;&#21306;&#20998;&#30340;&#21306;&#20998;&#65306;&#31639;&#27861;&#39044;&#27979;&#20013;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#26469;&#21306;&#20998;&#37027;&#20123;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#28548;&#28165;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#36890;&#24120;&#20855;&#26377;&#20449;&#24687;&#30340;&#35775;&#38382;&#26435;&#38480;&#8212;&#8212;&#29305;&#21035;&#26159;&#20027;&#35266;&#20449;&#24687;&#8212;&#8212;&#32780;&#36825;&#20123;&#20449;&#24687;&#26159;&#31639;&#27861;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#32534;&#30721;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35748;&#35782;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#26377;&#21407;&#21017;&#30340;&#31639;&#27861;&#65292;&#20165;&#22312;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#26377;&#25152;&#25913;&#21892;&#26102;&#25165;&#36873;&#25321;&#24615;&#22320;&#32435;&#20837;&#20154;&#31867;&#21453;&#39304;&#12290;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#31639;&#27861;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24448;&#24448;&#20248;&#20110;&#20154;&#31867;&#23545;&#24212;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#31867;&#21028;&#26029;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65288;&#21487;&#20197;&#39044;&#20808;&#30830;&#23450;&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;X&#23556;&#32447;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#23376;&#38598;&#22312;&#24739;&#32773;&#32676;&#20307;&#20013;&#21344;&#25454;&#20102;&#36817;30%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way
&lt;/p&gt;</description></item><item><title>Graph-Mamba&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#36890;&#36807;&#23558;Mamba&#27169;&#22359;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#33410;&#28857;&#36873;&#25321;&#26426;&#21046;&#38598;&#25104;&#26469;&#22686;&#24378;&#22270;&#32593;&#32476;&#20013;&#38271;&#31243;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00789</link><description>&lt;p&gt;
Graph-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#38271;&#31243;&#22270;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00789
&lt;/p&gt;
&lt;p&gt;
Graph-Mamba&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#36890;&#36807;&#23558;Mamba&#27169;&#22359;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#33410;&#28857;&#36873;&#25321;&#26426;&#21046;&#38598;&#25104;&#26469;&#22686;&#24378;&#22270;&#32593;&#32476;&#20013;&#38271;&#31243;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22270;&#21464;&#25442;&#22120;&#20013;&#24191;&#27867;&#29992;&#20110;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#30001;&#20110;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22823;&#22411;&#22270;&#20013;&#26080;&#27861;&#25193;&#23637;&#12290;&#26368;&#36817;&#30340;&#35745;&#31639;&#25928;&#29575;&#25913;&#36827;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25110;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#22270;&#23376;&#37319;&#26679;&#36827;&#34892;&#27880;&#24847;&#21147;&#31232;&#30095;&#21270;&#23454;&#29616;&#65292;&#20294;&#22312;&#25968;&#25454;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#25512;&#29702;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65288;&#22914;Mamba&#65289;&#22240;&#20854;&#22312;&#24207;&#21015;&#25968;&#25454;&#20013;&#24314;&#27169;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23558;SSM&#36866;&#24212;&#38750;&#24207;&#21015;&#22270;&#25968;&#25454;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22270;-Mamba&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#23558;Mamba&#27169;&#22359;&#19982;&#22522;&#20110;&#36755;&#20837;&#30340;&#33410;&#28857;&#36873;&#25321;&#26426;&#21046;&#38598;&#25104;&#65292;&#20197;&#22686;&#24378;&#22270;&#32593;&#32476;&#20013;&#30340;&#38271;&#31243;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#23581;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20197;&#22270;&#20026;&#20013;&#24515;&#30340;&#33410;&#28857;&#20248;&#20808;&#32423;&#21644;&#25490;&#21015;&#31574;&#30053;&#26469;&#22686;&#24378;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#25928;&#26524;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00759</link><description>&lt;p&gt;
&#26500;&#24314;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Building Expressive and Tractable Probabilistic Generative Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#22266;&#26377;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#31361;&#20986;&#20102;&#20351;PCs&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#36890;&#36807;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#27010;&#24565;&#26469;&#26500;&#24314;&#28145;&#24230;&#21644;&#28151;&#21512;PCs&#30340;&#21162;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00751</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#31639;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Algorithms for In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#26410;&#30693;&#26469;&#28304;&#30340;&#25968;&#25454;&#19978;&#65292;&#26426;&#22120;&#21435;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#31934;&#30830;&#30340;&#21435;&#23398;&#20064;&#8212;&#8212;&#22312;&#27809;&#26377;&#20351;&#29992;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#19982;&#27169;&#22411;&#20998;&#24067;&#21305;&#37197;&#30340;&#27169;&#22411;&#8212;&#8212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#20302;&#25928;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20219;&#21153;&#36866;&#24212;&#38454;&#27573;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#36827;&#34892;&#20219;&#21153;&#36866;&#24212;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#25928;&#31934;&#30830;&#21435;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#21152;&#21040;LLM&#30340;&#25552;&#31034;&#21069;&#38754;&#65288;&#29992;&#20110;&#20219;&#21153;&#36866;&#24212;&#65289;&#65292;&#21517;&#20026;ERASE&#65292;&#23427;&#30340;&#21435;&#23398;&#20064;&#25805;&#20316;&#25104;&#26412;&#19982;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#24847;&#21619;&#30528;&#23427;&#36866;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#20351;&#25105;&#20204;&#24471;&#21040;&#20102;&#20197;&#19979;&#32467;&#35770;&#65306;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#20102;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#22870;&#21169;&#27169;&#22411;&#30340;&#36873;&#25321;&#20197;&#21450;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#65292;&#35813;&#21464;&#25442;&#24378;&#35843;&#25913;&#21892;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#21644;&#22870;&#21169;&#27450;&#39575;&#12290;</title><link>https://arxiv.org/abs/2402.00742</link><description>&lt;p&gt;
&#25913;&#21464;&#21644;&#32452;&#21512;&#22870;&#21169;&#20197;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transforming and Combining Rewards for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#20102;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#22870;&#21169;&#27169;&#22411;&#30340;&#36873;&#25321;&#20197;&#21450;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#65292;&#35813;&#21464;&#25442;&#24378;&#35843;&#25913;&#21892;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#21644;&#22870;&#21169;&#27450;&#39575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#39318;&#20808;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#22870;&#21169;&#27169;&#22411;&#26469;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22870;&#21169;&#27169;&#22411;&#30340;&#20219;&#20309;&#21333;&#35843;&#21464;&#25442;&#37117;&#20445;&#25345;&#20559;&#22909;&#25490;&#21517;&#65307;&#26159;&#21542;&#26377;&#19968;&#31181;&#27604;&#20854;&#20182;&#36873;&#25321;&#8220;&#26356;&#22909;&#8221;&#30340;&#36873;&#25321;&#65311;&#20854;&#27425;&#65292;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#20010;&#29305;&#24615;&#23545;&#40784;&#65306;&#25105;&#20204;&#22914;&#20309;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#65311;&#36890;&#36807;&#23545;&#40784;&#36807;&#31243;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#23398;&#20064;&#30340;&#22870;&#21169;&#65288;&#24120;&#35265;&#24773;&#20917;&#65289;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#12290;&#36825;&#20010;&#27966;&#29983;&#30340;&#21464;&#25442;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#24378;&#35843;&#25913;&#36827;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#24050;&#32463;&#24471;&#20998;&#33391;&#22909;&#30340;&#36755;&#20986;&#12290;&#36825;&#26082;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#65288;&#20854;&#20013;&#19968;&#20123;&#25552;&#31034;&#27809;&#26377;&#24471;&#21040;&#25913;&#36827;&#65289;&#65292;&#21448;&#20943;&#23569;&#20102;&#22870;&#21169;&#27450;&#39575;&#65288;&#27169;&#22411;&#23398;&#20064;&#21033;&#29992;&#38169;&#35823;&#25351;&#23450;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FM3Q&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20010;&#20307;&#20840;&#23616;&#26368;&#23567;&#26368;&#22823;&#21407;&#21017;&#65292;&#36890;&#36807;&#20998;&#35299;&#32852;&#21512;&#26368;&#23567;&#26368;&#22823;Q&#20989;&#25968;&#20026;&#20010;&#20307;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#22242;&#38431;&#20869;&#20449;&#29992;&#20998;&#37197;&#12289;&#25968;&#25454;&#21033;&#29992;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00738</link><description>&lt;p&gt;
FM3Q&#65306;&#20998;&#35299;&#30340;&#22810;&#26234;&#33021;&#20307;&#26368;&#23567;&#26368;&#22823;Q&#23398;&#20064;&#29992;&#20110;&#20004;&#20010;&#22242;&#38431;&#30340;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FM3Q&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20010;&#20307;&#20840;&#23616;&#26368;&#23567;&#26368;&#22823;&#21407;&#21017;&#65292;&#36890;&#36807;&#20998;&#35299;&#32852;&#21512;&#26368;&#23567;&#26368;&#22823;Q&#20989;&#25968;&#20026;&#20010;&#20307;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#22242;&#38431;&#20869;&#20449;&#29992;&#20998;&#37197;&#12289;&#25968;&#25454;&#21033;&#29992;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#28041;&#21450;&#21040;&#19968;&#20123;&#26234;&#33021;&#20307;&#20998;&#20026;&#20004;&#20010;&#22242;&#38431;&#65292;&#21516;&#19968;&#22242;&#38431;&#20869;&#30340;&#22238;&#25253;&#30456;&#31561;&#65292;&#32780;&#23545;&#25163;&#22242;&#38431;&#20043;&#38388;&#30340;&#22238;&#25253;&#21017;&#30456;&#21453;&#12290;&#36825;&#31181;&#25152;&#35859;&#30340;&#20004;&#20010;&#22242;&#38431;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#65288;2t0sMGs&#65289;&#21487;&#20197;&#22312;&#26368;&#36817;&#20960;&#24180;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#32771;&#34385;&#22242;&#38431;&#20869;&#20449;&#29992;&#20998;&#37197;&#12289;&#25968;&#25454;&#21033;&#29992;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#19981;&#36275;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#20307;&#20840;&#23616;&#26368;&#23567;&#26368;&#22823;&#65288;IGMM&#65289;&#21407;&#21017;&#65292;&#36890;&#36807;Q&#20989;&#25968;&#22312;2t0sMGs&#20013;&#30830;&#20445;&#20004;&#20010;&#22242;&#38431;&#26368;&#23567;&#26368;&#22823;&#34892;&#20026;&#21644;&#20010;&#20307;&#36138;&#23146;&#34892;&#20026;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20998;&#35299;&#30340;&#22810;&#26234;&#33021;&#20307;&#26368;&#23567;&#26368;&#22823;Q&#23398;&#20064;&#65288;FM3Q&#65289;&#65292;&#23427;&#21487;&#20197;&#23558;&#32852;&#21512;&#26368;&#23567;&#26368;&#22823;Q&#20989;&#25968;&#20998;&#35299;&#20026;&#20010;&#20307;&#20989;&#25968;&#65292;&#24182;&#36845;&#20195;&#35299;&#20986;&#28385;&#36275;IGMM&#30340;2t0sMGs&#26368;&#23567;&#26368;&#22823;Q&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is prop
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#21644;TD3&#32593;&#32476;&#30340;&#20849;&#20139;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22810;&#31181;&#39118;&#26684;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#36816;&#21160;&#20013;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#32593;&#32476;&#26469;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#39118;&#26684;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.00722</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#24310;&#36831;DDPG&#30340;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#20849;&#20139;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#21644;TD3&#32593;&#32476;&#30340;&#20849;&#20139;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22810;&#31181;&#39118;&#26684;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#36816;&#21160;&#20013;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#32593;&#32476;&#26469;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#39118;&#26684;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#65288;NST&#65289;&#26159;&#19968;&#31867;&#33021;&#22815;&#20351;&#20803;&#32032;&#65288;&#36890;&#24120;&#20026;&#22270;&#20687;&#65289;&#37319;&#29992;&#21478;&#19968;&#20010;&#20803;&#32032;&#30340;&#22806;&#35266;&#25110;&#39118;&#26684;&#30340;&#31639;&#27861;&#12290;&#27599;&#20010;&#20803;&#32032;&#30001;&#20869;&#23481;&#21644;&#39118;&#26684;&#32452;&#25104;&#65306;&#20869;&#23481;&#21487;&#20197;&#27010;&#24565;&#19978;&#23450;&#20041;&#20026;&#20803;&#32032;&#30340;&#8220;&#26159;&#20160;&#20040;&#8221;&#65292;&#32780;&#39118;&#26684;&#21017;&#26159;&#20803;&#32032;&#30340;&#8220;&#22914;&#20309;&#8221;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;NST&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#19968;&#32452;&#39118;&#26684;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#36816;&#21160;&#20013;&#65292;&#20363;&#22914;&#65292;&#30456;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#21487;&#20197;&#37319;&#29992;&#24868;&#24594;&#12289;&#24555;&#20048;&#12289;&#24179;&#38745;&#25110;&#24754;&#20260;&#30340;&#26041;&#24335;&#25191;&#34892;&#12290;&#19968;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#25552;&#21462;&#21644;&#23450;&#20041;&#30446;&#26631;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#12290;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;TD3&#65289;&#32593;&#32476;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23450;&#20041;&#30340;&#25439;&#22833;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#31227;TD3&#65288;NPST3&#65289;&#36890;&#36807;&#24341;&#20837;&#35757;&#32451;&#39118;&#26684;&#26469;&#25913;&#21464;&#26426;&#22120;&#20154;&#36816;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20197;&#22312;&#32447;&#25110;&#31163;&#32447;&#30340;&#26041;&#24335;&#23454;&#29616;&#33258;&#20027;&#26426;&#22120;&#20154;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one. Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element. In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way. An autoencoder architecture extracts and defines the Content and the Style of the target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style. Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20197;&#24847;&#22270;&#28418;&#31227;&#20026;&#24341;&#23548;&#30340;LLMs&#36827;&#34892;&#24847;&#22270;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31574;&#30053;&#35782;&#21035;&#21644;&#22788;&#29702;&#24847;&#22270;&#28418;&#31227;&#65292;&#20197;&#23454;&#29616;&#24847;&#22270;&#19982;&#19994;&#21153;&#30446;&#26631;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.00715</link><description>&lt;p&gt;
&#20351;&#29992;&#20197;&#24847;&#22270;&#28418;&#31227;&#20026;&#24341;&#23548;&#30340;LLMs&#36827;&#34892;&#24847;&#22270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Intent Assurance using LLMs guided by Intent Drift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20197;&#24847;&#22270;&#28418;&#31227;&#20026;&#24341;&#23548;&#30340;LLMs&#36827;&#34892;&#24847;&#22270;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31574;&#30053;&#35782;&#21035;&#21644;&#22788;&#29702;&#24847;&#22270;&#28418;&#31227;&#65292;&#20197;&#23454;&#29616;&#24847;&#22270;&#19982;&#19994;&#21153;&#30446;&#26631;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#39537;&#21160;&#30340;&#32593;&#32476;&#65288;IBN&#65289;&#20026;&#32593;&#32476;&#31649;&#29702;&#25552;&#20379;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#25215;&#35834;&#20197;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#23558;&#24847;&#22270;&#21644;&#19994;&#21153;&#30446;&#26631;&#19982;&#32593;&#32476;&#25805;&#20316;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#20854;&#23454;&#38469;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;1&#65289;&#22788;&#29702;&#24847;&#22270;&#65292;&#21363;&#32763;&#35793;&#12289;&#20998;&#35299;&#21644;&#35782;&#21035;&#23454;&#29616;&#24847;&#22270;&#30340;&#36923;&#36753;&#65307;2&#65289;&#24847;&#22270;&#19968;&#33268;&#24615;&#65292;&#21363;&#32771;&#34385;&#21040;&#21160;&#24577;&#32593;&#32476;&#65292;&#36923;&#36753;&#24212;&#36866;&#24403;&#35843;&#25972;&#20197;&#20445;&#35777;&#24847;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#21518;&#32773;&#65292;&#24847;&#22270;&#20445;&#35777;&#36127;&#36131;&#25345;&#32493;&#39564;&#35777;&#21644;&#39564;&#35777;&#65292;&#21253;&#25324;&#37319;&#21462;&#24517;&#35201;&#34892;&#21160;&#20351;&#25805;&#20316;&#29366;&#24577;&#21644;&#30446;&#26631;&#29366;&#24577;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20445;&#35777;&#26694;&#26550;&#65292;&#20801;&#35768;&#25105;&#20204;&#22312;&#21457;&#29983;&#24847;&#22270;&#28418;&#31227;&#26102;&#36827;&#34892;&#26816;&#27979;&#21644;&#37319;&#21462;&#34892;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#22522;&#20110;AI&#39537;&#21160;&#30340;&#31574;&#30053;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#24517;&#35201;&#30340;&#19978;&#19979;&#25991;&#35201;&#27714;&#65292;&#24182;&#21327;&#21161;&#23454;&#29616;&#21644;&#20445;&#35777;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner. However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents. To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states. In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs. To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#25193;&#23637;&#30340;&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.00707</link><description>&lt;p&gt;
&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#35821;&#35328;&#29983;&#25104;&#19982;&#26368;&#36817;&#37051;
&lt;/p&gt;
&lt;p&gt;
Non-Exchangeable Conformal Language Generation with Nearest Neighbors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#25193;&#23637;&#30340;&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#35753;&#20154;&#20204;&#26816;&#26597;&#28508;&#22312;&#30340;&#38169;&#35273;&#21644;&#20351;&#31995;&#32479;&#26356;&#21487;&#38752;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20849;&#24418;&#39044;&#27979;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#65292;&#28982;&#32780;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20219;&#20309;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#37117;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#20851;&#20110;&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#39044;&#27979;&#30340;&#32467;&#26524;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#30830;&#20445;&#35206;&#30422;&#33539;&#22260;&#12290;&#32467;&#26524;--&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#26680;&#37319;&#26679;&#65292;&#26159;&#23545;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#29983;&#25104;&#30340;&#20849;&#24418;&#39044;&#27979;&#26694;&#26550;&#30340;&#19968;&#31181;&#26032;&#39062;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20219;&#24847;&#27169;&#22411;&#30340;&#20107;&#21518;&#22788;&#29702;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#26631;&#35760;&#32423;&#21035;&#12289;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#29983;&#25104;&#36136;&#37327;&#32467;&#26524;&#12290;&#36890;&#36807;&#21516;&#26102;&#20135;&#29983;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#26356;&#32039;&#23494;&#30340;&#39044;&#27979;&#38598;&#65292;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good cove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PeaTMOSS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35760;&#24405;&#21644;&#20998;&#26512;&#24320;&#28304;&#36719;&#20214;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20803;&#25968;&#25454;&#21644;&#24212;&#29992;&#24773;&#20917;&#12290;&#36825;&#23545;&#20110;&#20102;&#35299;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37319;&#29992;&#21644;&#37325;&#22797;&#20351;&#29992;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.00699</link><description>&lt;p&gt;
PeaTMOSS: &#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PeaTMOSS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35760;&#24405;&#21644;&#20998;&#26512;&#24320;&#28304;&#36719;&#20214;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20803;&#25968;&#25454;&#21644;&#24212;&#29992;&#24773;&#20917;&#12290;&#36825;&#23545;&#20110;&#20102;&#35299;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37319;&#29992;&#21644;&#37325;&#22797;&#20351;&#29992;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#35757;&#32451;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#21644;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#27491;&#22312;&#37319;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;(PTMs)&#26469;&#36827;&#34892;&#21518;&#32493;&#24212;&#29992;&#12290;PTM&#20379;&#24212;&#38142;&#30340;&#21160;&#24577;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#32467;&#26500;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#19981;&#20165;&#35760;&#24405;&#20803;&#25968;&#25454;&#65292;&#36824;&#35760;&#24405;&#36825;&#20123;&#27169;&#22411;&#30340;&#21518;&#32493;&#24212;&#29992;&#12290;&#27809;&#26377;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;MSR&#31038;&#21306;&#26080;&#27861;&#20840;&#38754;&#29702;&#35299;PTM&#30340;&#37319;&#29992;&#21644;&#37325;&#22797;&#20351;&#29992;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PeaTMOSS&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;281,638&#20010;PTM&#30340;&#20803;&#25968;&#25454;&#21644;&#36229;&#36807;50&#20010;&#26376;&#19979;&#36733;&#37327;&#30340;&#25152;&#26377;PTM&#30340;&#35814;&#32454;&#24555;&#29031;(14,296&#20010;PTMs)&#65292;&#20197;&#21450;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;28,575&#20010;&#26469;&#33258;GitHub&#30340;&#24320;&#28304;&#36719;&#20214;&#20195;&#30721;&#24211;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;15,129&#20010;GitHub&#20195;&#30721;&#24211;&#21040;&#23427;&#20204;&#20351;&#29992;&#30340;2,530&#20010;PTMs&#30340;44,337&#20010;&#26144;&#23556;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#24615;&#65292;&#25105;&#20204;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#25552;&#31034;&#65292;&#20197;&#33258;&#21160;&#22320;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22235;&#31181;&#20808;&#36827;&#30340;LLMs&#22312;9&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#21644;&#29702;&#35299;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#19988;&#23433;&#20840;&#22320;&#37096;&#32626;LLMs&#29983;&#25104;&#20248;&#36136;&#20195;&#30721;&#30340;&#26465;&#20214;&#21644;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.00689</link><description>&lt;p&gt;
&#20598;&#23572;&#23433;&#20840;&#65306;&#20195;&#30721;&#29983;&#25104;&#36741;&#21161;&#24037;&#20855;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ocassionally Secure: A Comparative Analysis of Code Generation Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22235;&#31181;&#20808;&#36827;&#30340;LLMs&#22312;9&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#21644;&#29702;&#35299;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#19988;&#23433;&#20840;&#22320;&#37096;&#32626;LLMs&#29983;&#25104;&#20248;&#36136;&#20195;&#30721;&#30340;&#26465;&#20214;&#21644;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20195;&#30721;&#29983;&#25104;&#23601;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#34920;&#26126;LLMs&#26377;&#33021;&#21147;&#29983;&#25104;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#20294;&#25991;&#29486;&#27809;&#26377;&#32771;&#34385;&#21040;&#20160;&#20040;&#22240;&#32032;&#26377;&#21161;&#20110;&#29983;&#25104;&#23433;&#20840;&#21644;&#26377;&#25928;&#30340;&#20195;&#30721;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37325;&#28857;&#26159;&#30830;&#23450;&#21644;&#29702;&#35299;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;LLMs&#33021;&#22815;&#26377;&#25928;&#21644;&#23433;&#20840;&#22320;&#37096;&#32626;&#26469;&#29983;&#25104;&#20248;&#36136;&#20195;&#30721;&#30340;&#26465;&#20214;&#21644;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#8212;&#8212;&#20351;&#29992;ChatGPT&#21644;Bard&#30340;GPT-3.5&#21644;GPT-4&#65292;&#20197;&#21450;&#26469;&#33258;Google&#30340;Gemini&#8212;&#8212;&#20351;&#29992;9&#20010;&#29420;&#31435;&#20219;&#21153;&#26469;&#35780;&#20272;&#27599;&#20010;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#32622;&#20110;&#19968;&#20010;&#20856;&#22411;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#65292;&#20195;&#34920;&#20102;&#24320;&#21457;&#20154;&#21592;&#22312;&#24037;&#20316;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#23433;&#20840;&#24847;&#35782;&#65292;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#24320;&#21457;&#30340;&#20004;&#20010;&#19981;&#21516;&#29256;&#26412;&#30340;&#24037;&#20855;&#26469;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
$ $Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example. While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code. Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities. We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work. Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our develop
&lt;/p&gt;</description></item><item><title>&#20197;&#24448;&#30340;&#26426;&#22120;&#20154;&#32534;&#31243;&#26041;&#27861;&#23545;&#38750;&#19987;&#23478;&#29992;&#25143;&#19981;&#21451;&#22909;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36830;&#32493;&#30446;&#26631;&#23548;&#21521;&#21160;&#20316;&#26469;&#23454;&#29616;&#30495;&#23454;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#21160;&#20316;&#30340;&#29305;&#24449;&#21464;&#21270;&#26469;&#36866;&#24212;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#26469;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#20851;&#33410;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.00678</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#30340;&#36830;&#32493;&#30446;&#26631;&#23548;&#21521;&#21160;&#20316;&#26469;&#23454;&#29616;&#30495;&#23454;&#35780;&#20272;&#30340;&#21487;&#36861;&#36394;&#24615;
&lt;/p&gt;
&lt;p&gt;
Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00678
&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#26426;&#22120;&#20154;&#32534;&#31243;&#26041;&#27861;&#23545;&#38750;&#19987;&#23478;&#29992;&#25143;&#19981;&#21451;&#22909;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#36830;&#32493;&#30446;&#26631;&#23548;&#21521;&#21160;&#20316;&#26469;&#23454;&#29616;&#30495;&#23454;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#21160;&#20316;&#30340;&#29305;&#24449;&#21464;&#21270;&#26469;&#36866;&#24212;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#26469;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#20851;&#33410;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#20351;&#31995;&#32479;&#36866;&#24212;&#19982;&#38750;&#19987;&#23478;&#29992;&#25143;&#30340;&#20132;&#20114;&#12290;&#26426;&#22120;&#20154;&#27169;&#20223;&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#31034;&#33539;&#30452;&#25509;&#32534;&#31243;&#26469;&#31616;&#21270;&#21644;&#20943;&#23569;&#26426;&#22120;&#20154;&#32534;&#31243;&#30340;&#26102;&#38388;&#12290;&#22312;&#20256;&#32479;&#30340;&#26694;&#26550;&#20013;&#65292;&#21160;&#20316;&#34987;&#24314;&#27169;&#20026;&#20851;&#33410;&#25110;&#31515;&#21345;&#23572;&#31354;&#38388;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#32431;&#20960;&#20309;&#26041;&#27861;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#20854;&#20182;&#29305;&#24449;&#65292;&#27604;&#22914;&#35270;&#35273;&#29305;&#24449;&#12290;&#36830;&#32493;&#30446;&#26631;&#23548;&#21521;&#21160;&#20316;(CGDA)&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#23558;&#21160;&#20316;&#32534;&#30721;&#20026;&#21487;&#20197;&#20174;&#29615;&#22659;&#20013;&#25552;&#21462;&#30340;&#20219;&#20309;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#31181;&#26080;&#20851;&#29305;&#24449;&#30340;&#32534;&#30721;&#65292;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#20851;&#33410;&#36712;&#36857;&#24517;&#39035;&#23436;&#20840;&#35745;&#31639;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;(EA)&#36827;&#34892;&#35745;&#31639;&#65292;&#20294;&#36825;&#36890;&#24120;&#38656;&#35201;&#36807;&#22810;&#30340;&#35780;&#20272;&#25165;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#36825;&#20010;&#36827;&#21270;&#27493;&#39588;&#12290;&#30446;&#21069;&#30340;&#31574;&#30053;&#26159;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#28982;&#21518;&#23558;&#35780;&#20272;&#32467;&#26524;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users. Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations. In classical frameworks, actions are modeled using joint or Cartesian space trajectories. Other features, such as visual ones, are not always well represented with these pure geometrical approaches. Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment. As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding. This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot. Current strategies involve performing evaluations in a simulation, transferring 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#25442;&#31639;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#25511;&#21046;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#25442;&#12290;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#32593;&#32476;&#26469;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#65292;&#21516;&#26102;&#32534;&#30721;&#20102;&#34892;&#20026;&#30340;&#30446;&#26631;&#21644;&#39118;&#26684;&#65292;&#20174;&#32780;&#23558;&#19968;&#20010;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31574;&#30053;&#32780;&#20445;&#25345;&#20854;&#20869;&#23481;&#19981;&#21464;&#12290;&#36890;&#36807;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#29992;&#25143;&#28436;&#31034;&#23454;&#29616;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39118;&#26684;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.00677</link><description>&lt;p&gt;
&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Neural Policy Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#25442;&#31639;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#25511;&#21046;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#25442;&#12290;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#32593;&#32476;&#26469;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#65292;&#21516;&#26102;&#32534;&#30721;&#20102;&#34892;&#20026;&#30340;&#30446;&#26631;&#21644;&#39118;&#26684;&#65292;&#20174;&#32780;&#23558;&#19968;&#20010;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31574;&#30053;&#32780;&#20445;&#25345;&#20854;&#20869;&#23481;&#19981;&#21464;&#12290;&#36890;&#36807;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#29992;&#25143;&#28436;&#31034;&#23454;&#29616;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39118;&#26684;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#36716;&#25442;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#25552;&#20986;&#65306;&#32654;&#26415;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22266;&#23450;&#36712;&#36857;&#12290;&#26412;&#25991;&#23558;&#36825;&#20010;&#27010;&#24565;&#25193;&#23637;&#21040;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#27599;&#20010;&#32593;&#32476;&#37117;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#22870;&#21169;&#65292;&#36890;&#24120;&#32534;&#30721;&#20102;&#19968;&#31181;&#34892;&#20026;&#30340;&#30446;&#26631;&#65292;&#21487;&#20197;&#25551;&#36848;&#20026;&#20869;&#23481;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#20351;&#24471;&#21487;&#20197;&#32534;&#30721;&#31532;&#20108;&#20010;&#20219;&#21153;&#65292;&#21487;&#20197;&#25551;&#36848;&#20026;&#39118;&#26684;&#12290;&#25552;&#20986;&#20102;&#31070;&#32463;&#31574;&#30053;&#39118;&#26684;&#36716;&#25442;&#65288;NPST&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#19968;&#20010;&#31574;&#30053;&#30340;&#39118;&#26684;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31574;&#30053;&#65292;&#21516;&#26102;&#20445;&#25345;&#21518;&#32773;&#30340;&#20869;&#23481;&#12290;&#36890;&#36807;&#28145;&#24230; Q-Network &#26550;&#26500;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#28436;&#31034;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#36827;&#34892;&#20102;&#20004;&#32452;&#19981;&#21516;&#30340;&#29992;&#25143;&#28436;&#31034;&#65292;&#19968;&#32452;&#20026;&#20869;&#23481;&#65292;&#21478;&#19968;&#32452;&#20026;&#39118;&#26684;&#12290;&#19981;&#21516;&#30340;&#39118;&#26684;&#36890;&#36807;&#29992;&#25143;&#28436;&#31034;&#36827;&#34892;&#32534;&#30721;&#12290;&#29983;&#25104;&#30340;&#31574;&#30053;&#26159;&#36890;&#36807;&#23558;&#20869;&#23481;&#31574;&#30053;&#36755;&#36865;&#21040;&#19968;&#20010;&#29983;&#25104;&#22120;&#20013;&#26469;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content poli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#24341;&#20837;&#28145;&#24230;Q&#23398;&#20064;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#36827;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.00676</link><description>&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#20154;&#32032;&#25551;&#65306;&#28145;&#24230;Q&#23398;&#20064;&#32593;&#32476;&#22312;&#20154;&#31867;&#32032;&#25551;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#24341;&#20837;&#28145;&#24230;Q&#23398;&#20064;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#36827;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36825;&#28608;&#21457;&#20102;&#35768;&#22810;&#26368;&#26032;&#30340;&#35748;&#30693;&#31185;&#23398;&#29702;&#35770;&#26041;&#27861;&#12290;&#33402;&#26415;&#29615;&#22659;&#34987;&#35748;&#30693;&#31185;&#23398;&#30028;&#35270;&#20026;&#20016;&#23500;&#12289;&#33258;&#28982;&#12289;&#22810;&#24863;&#23448;&#12289;&#22810;&#25991;&#21270;&#30340;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25511;&#21046;&#12290;&#28145;&#24230;Q&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65288;DQN&#65289;&#26159;&#22312;&#26426;&#22120;&#20154;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#26368;&#25104;&#21151;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;DQN&#26041;&#27861;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#29983;&#25104;&#22797;&#26434;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#25191;&#34892;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;&#24403;&#21069;&#30340;&#33402;&#26415;&#32472;&#30011;&#26426;&#22120;&#20154;&#24212;&#29992;&#20351;&#29992;&#31616;&#21333;&#30340;&#25511;&#21046;&#27861;&#21017;&#65292;&#38480;&#21046;&#20102;&#26694;&#26550;&#30340;&#36866;&#24212;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33402;&#26415;&#32472;&#30011;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#24341;&#20837;DQN&#12290;&#30446;&#26631;&#26159;&#30740;&#31350;&#22914;&#20309;&#24341;&#20837;&#22797;&#26434;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#25913;&#36827;&#33402;&#26415;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control pol
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.00672</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#26631;&#31614;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#26080;&#38656;&#27880;&#37322;&#20174;&#19981;&#21516;&#27169;&#24577;&#20013;&#26816;&#32034;&#30456;&#21516;&#36523;&#20221;&#30340;&#34892;&#20154;&#22270;&#20687;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#24314;&#31435;&#36328;&#27169;&#24577;&#30340;&#20266;&#26631;&#31614;&#20851;&#32852;&#20197;&#24357;&#21512;&#27169;&#24577;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#24573;&#30053;&#20102;&#22312;&#20266;&#26631;&#31614;&#31354;&#38388;&#20013;&#20445;&#25345;&#23454;&#20363;&#32423;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#20851;&#32852;&#31895;&#31961;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#65288;MULT&#65289;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#32454;&#31890;&#24230;&#23454;&#20363;&#32423;&#32467;&#26500;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#12290;&#23427;&#24314;&#27169;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#30340;&#20851;&#32852;&#24615;&#65292;&#21033;&#29992;&#23427;&#20204;&#23450;&#20041;&#20266;&#26631;&#31614;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#26368;&#23567;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32500;&#25345;&#20102;&#36328;&#27169;&#24577;&#30340;&#23545;&#40784;&#24182;&#20445;&#25345;&#20102;&#20869;&#37096;&#27169;&#24577;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#22312;&#32447;&#20132;&#21449;&#35760;&#24518;&#26631;&#31614;&#24341;&#29992;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Ref
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00658</link><description>&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36712;&#36857;&#21644;&#21512;&#25104;&#22870;&#21169;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#36880;&#27493;&#21512;&#29702;&#21270;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#21512;&#29702;&#21270;&#30340;&#21487;&#38752;&#24615;&#21644;&#24544;&#23454;&#24615;&#65292;&#27491;&#22312;&#36827;&#34892;&#22823;&#37327;&#24037;&#20316;&#12290;&#26377;&#20123;&#26041;&#27861;&#23558;&#25512;&#29702;&#24314;&#27169;&#20026;&#35268;&#21010;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21017;&#19987;&#27880;&#20110;&#27880;&#37322;&#30340;&#36807;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35268;&#21010;&#30340;&#25628;&#32034;&#36807;&#31243;&#24448;&#24448;&#30001;&#20110;&#39057;&#32321;&#35780;&#20272;&#20013;&#38388;&#25512;&#29702;&#29366;&#24577;&#21644;&#24191;&#27867;&#30340;&#25506;&#32034;&#31354;&#38388;&#32780;&#23548;&#33268;&#39640;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30417;&#30563;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;LLM&#35757;&#32451;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#36712;&#36857;&#30452;&#25509;&#26681;&#25454;&#21512;&#25104;&#30340;&#36807;&#31243;&#22870;&#21169;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. 
&lt;/p&gt;</description></item><item><title>CapHuman&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#32534;&#30721;&#28982;&#21518;&#23398;&#20064;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#30340;&#22836;&#37096;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#29305;&#23450;&#20010;&#20307;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.00627</link><description>&lt;p&gt;
CapHuman: &#22312;&#24179;&#34892;&#23431;&#23449;&#20013;&#25429;&#25417;&#20320;&#30340;&#30636;&#38388;
&lt;/p&gt;
&lt;p&gt;
CapHuman: Capture Your Moments in Parallel Universes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00627
&lt;/p&gt;
&lt;p&gt;
CapHuman&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#32534;&#30721;&#28982;&#21518;&#23398;&#20064;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#30340;&#22836;&#37096;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#29305;&#23450;&#20010;&#20307;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#65292;&#21363;&#20165;&#32473;&#23450;&#19968;&#20010;&#21442;&#32771;&#38754;&#37096;&#29031;&#29255;&#65292;&#26399;&#26395;&#33021;&#22815;&#22312;&#19981;&#21516;&#24773;&#22659;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#30340;&#22836;&#37096;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#29305;&#23450;&#20010;&#20307;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#20855;&#22791;&#20197;&#19979;&#26377;&#21033;&#29305;&#24449;&#65306;&#65288;1&#65289;&#23545;&#19990;&#30028;&#21644;&#20154;&#31867;&#31038;&#20250;&#26377;&#24378;&#22823;&#30340;&#35270;&#35273;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#29992;&#20110;&#22522;&#26412;&#29289;&#20307;&#21644;&#20154;&#31867;&#22270;&#20687;&#30340;&#29983;&#25104;&#65307;&#65288;2&#65289;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#65307;&#65288;3&#65289;&#28789;&#27963;&#32454;&#31890;&#24230;&#30340;&#22836;&#37096;&#25511;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22522;&#30784;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19978;&#36848;&#20004;&#31181;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CapHuman&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#8220;&#32534;&#30721;&#28982;&#21518;&#23398;&#20064;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#65292;&#20026;&#26032;&#20010;&#20307;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align" paradigm, which enables generalizable identity preservation for new individuals without cum
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InfoBoost&#65292;&#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#22810;&#20010;&#24178;&#25200;&#28304;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00607</link><description>&lt;p&gt;
&#20154;&#24037;&#21512;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30495;&#30340;&#19981;&#22914;&#30495;&#23454;&#25968;&#25454;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Synthetic Time-series Data Really not as Good as Real Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InfoBoost&#65292;&#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#22810;&#20010;&#24178;&#25200;&#28304;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23384;&#22312;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#12289;&#20559;&#35265;&#21644;&#33030;&#24369;&#24615;&#20197;&#21450;&#27867;&#21270;&#38382;&#39064;&#12290;&#25972;&#21512;&#36890;&#29992;&#30340;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#21253;&#21547;&#25152;&#26377;&#26410;&#35265;&#36807;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InfoBoost -- &#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#27169;&#22411;&#35757;&#32451;&#32780;&#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#65292;&#36229;&#36234;&#20102;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#22810;&#20010;&#28304;&#30340;&#33410;&#22863;&#20449;&#21495;&#12289;&#22122;&#22768;&#24178;&#25200;&#21644;&#36229;&#36807;&#37319;&#26679;&#31383;&#21475;&#33021;&#21147;&#30340;&#38271;&#21608;&#26399;&#29305;&#24449;&#30340;&#24178;&#25200;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#38750;&#28145;&#24230;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem. Integrating universal data synthesis methods holds promise in improving generalization. However, current methods cannot guarantee that the generator's output covers all unseen real data. In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability. We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data. Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data. Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities. Through experiments, our non-deep-learning synthetic data enables 
&lt;/p&gt;</description></item><item><title>Sandra&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#23558;&#30690;&#37327;&#34920;&#31034;&#19982;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26412;&#20307;&#35770;&#24314;&#31435;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;&#30340;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20174;&#19968;&#32452;&#20107;&#23454;&#20013;&#25512;&#26029;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#32447;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21521;&#37327;&#31354;&#38388;&#30340;&#21487;&#25511;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00591</link><description>&lt;p&gt;
Sandra -- &#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00591
&lt;/p&gt;
&lt;p&gt;
Sandra&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#23558;&#30690;&#37327;&#34920;&#31034;&#19982;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26412;&#20307;&#35770;&#24314;&#31435;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;&#30340;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20174;&#19968;&#32452;&#20107;&#23454;&#20013;&#25512;&#26029;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#32447;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21521;&#37327;&#31354;&#38388;&#30340;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Sandra&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#30690;&#37327;&#34920;&#31034;&#19982;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#12290;Sandra&#20351;&#29992;&#26412;&#20307;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#24182;&#22312;&#20854;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#25512;&#29702;&#22120;&#30340;&#20960;&#20309;&#29305;&#24615;&#20351;&#24471;&#23427;&#33021;&#22815;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#24357;&#21512;&#20102;&#31526;&#21495;&#30693;&#35782;&#34920;&#36798;&#19982;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;Sandra&#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;(DnS)&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#23427;&#26159;&#19968;&#31181;&#26694;&#26550;&#35821;&#20041;&#30340;&#24418;&#24335;&#21270;&#12290;&#32473;&#23450;&#19968;&#32452;&#20107;&#23454;(&#24773;&#22659;)&#65292;&#23427;&#33021;&#22815;&#25512;&#26029;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#36879;&#35270;&#22270;(&#25551;&#36848;)&#65292;&#20026;&#20854;&#25552;&#20379;&#19968;&#20010;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#21363;&#20351;&#22312;&#20449;&#24687;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DnS&#27169;&#22411;&#19978;&#26159;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#21450;&#20854;&#26631;&#20934;&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;Sandra&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#65306;(i)&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#65307;(ii) &#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#65307;(iii)&#23545;&#21521;&#37327;&#31354;&#38388;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents sandra, a neuro-symbolic reasoner combining vectorial representations with deductive reasoning. Sandra builds a vector space constrained by an ontology and performs reasoning over it. The geometric nature of the reasoner allows its combination with neural networks, bridging the gap with symbolic knowledge representations. Sandra is based on the Description and Situation (DnS) ontology design pattern, a formalization of frame semantics. Given a set of facts (a situation) it allows to infer all possible perspectives (descriptions) that can provide a plausible interpretation for it, even in presence of incomplete information. We prove that our method is correct with respect to the DnS model. We experiment with two different tasks and their standard benchmarks, demonstrating that, without increasing complexity, sandra (i) outperforms all the baselines (ii) provides interpretability in the classification process, and (iii) allows control over the vector space, which is d
&lt;/p&gt;</description></item><item><title>BrainSLAM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#40736;&#19981;&#21516;&#33041;&#21306;&#30340;&#20154;&#32676;&#27963;&#21160;&#25968;&#25454;&#36827;&#34892;SLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35299;&#30721;&#36895;&#24230;&#21644;&#29087;&#24713;&#24230;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#21560;&#24341;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#24452;&#31215;&#20998;&#21644;&#38381;&#29615;&#26816;&#27979;&#65292;&#20174;&#32780;&#26500;&#24314;&#22320;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.00588</link><description>&lt;p&gt;
BrainSLAM: &#31070;&#32463;&#20154;&#32676;&#27963;&#21160;&#25968;&#25454;&#19978;&#30340;SLAM
&lt;/p&gt;
&lt;p&gt;
BrainSLAM: SLAM on Neural Population Activity Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00588
&lt;/p&gt;
&lt;p&gt;
BrainSLAM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#40736;&#19981;&#21516;&#33041;&#21306;&#30340;&#20154;&#32676;&#27963;&#21160;&#25968;&#25454;&#36827;&#34892;SLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35299;&#30721;&#36895;&#24230;&#21644;&#29087;&#24713;&#24230;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#21560;&#24341;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#24452;&#31215;&#20998;&#21644;&#38381;&#29615;&#26816;&#27979;&#65292;&#20174;&#32780;&#26500;&#24314;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#23450;&#20301;&#19982;&#26144;&#23556; (SLAM) &#31639;&#27861;&#24120;&#29992;&#20110;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#26032;&#29615;&#22659;&#30340;&#22320;&#22270;&#12290;&#22823;&#33041;&#20284;&#20046;&#20063;&#23398;&#20064;&#22320;&#22270;&#65292;&#20294;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#65292;&#24182;&#19988;&#22914;&#20309;&#20174;&#31070;&#32463;&#27963;&#21160;&#25968;&#25454;&#20013;&#25512;&#26029;&#36825;&#20123;&#22320;&#22270;&#20063;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BrainSLAM&#65307;&#19968;&#31181;&#20165;&#20351;&#29992;&#20174;&#22823;&#40736;&#30340;&#19977;&#20010;&#33041;&#21306;&#65288;&#28023;&#39532;&#20307;&#65292;&#21069;&#39069;&#30382;&#36136;&#21644;&#39030;&#21494;&#30382;&#36136;&#65289;&#21516;&#26102;&#35760;&#24405;&#30340;&#20154;&#32676;&#27963;&#21160;&#65288;&#23616;&#37096;&#22330;&#30005;&#20301;&#65292;LFP&#65289;&#25968;&#25454;&#36827;&#34892;SLAM&#30340;&#26041;&#27861;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#20174;&#22823;&#40736;&#22312;2D&#36855;&#23467;&#20013;&#23548;&#33322;&#26102;&#35760;&#24405;&#30340;&#31070;&#32463;&#23616;&#37096;&#22330;&#30005;&#20301;&#25968;&#25454;&#30340;&#23567;&#27874;&#22270;&#35299;&#26469;&#35299;&#30721;&#36895;&#24230;&#21644;&#29087;&#24713;&#24230;&#20449;&#24687;&#12290;CNN&#30340;&#36755;&#20986;&#39537;&#21160;RatSLAM&#21551;&#21457;&#24335;&#26550;&#26500;&#65292;&#39537;&#21160;&#25191;&#34892;&#36335;&#24452;&#31215;&#20998;&#30340;&#21560;&#24341;&#23376;&#32593;&#32476;&#20197;&#21450;&#25191;&#34892;&#8220;&#38381;&#29615;&#26816;&#27979;&#8221;&#65288;&#26816;&#27979;&#20808;&#21069;&#35775;&#38382;&#36807;&#30340;&#20301;&#32622;&#24182;&#32416;&#27491;&#22320;&#22270;&#21035;&#21517;&#38169;&#35823;&#65289;&#30340;&#29420;&#31435;&#31995;&#32479;&#12290;&#36825;&#19977;&#20010;&#32452;&#20214;&#20849;&#21516;&#21487;&#20197;&#26500;&#24314;&#20986;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;EE-Tuning&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#38024;&#23545;&#26089;&#26399;&#32456;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#65292;&#36890;&#36807;&#24615;&#33021;&#20248;&#21270;&#21644;3D&#24182;&#34892;&#24615;&#23454;&#29616;&#21331;&#36234;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#32456;&#27490;LLM&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.00518</link><description>&lt;p&gt;
EE-Tuning:&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#25193;&#23637;&#30340;&#35843;&#25972;&#26089;&#26399;&#32456;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;EE-Tuning&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#38024;&#23545;&#26089;&#26399;&#32456;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#65292;&#36890;&#36807;&#24615;&#33021;&#20248;&#21270;&#21644;3D&#24182;&#34892;&#24615;&#23454;&#29616;&#21331;&#36234;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#32456;&#27490;LLM&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EE-Tuning&#65292;&#19968;&#31181;&#36731;&#37327;&#19988;&#32463;&#27982;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35757;&#32451;/&#35843;&#25972;&#26089;&#26399;&#32456;&#27490;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#19982;&#23436;&#25972;&#21442;&#25968;&#30340;&#39044;&#35757;&#32451;&#24120;&#35265;&#26041;&#27861;&#19981;&#21516;&#65292;EE-Tuning&#36890;&#36807;&#22312;&#21442;&#25968;&#39640;&#25928;&#26041;&#24335;&#19979;&#22686;&#21152;&#39069;&#22806;&#30340;&#26089;&#26399;&#32456;&#27490;&#23618;&#65292;&#19982;&#20219;&#20309;&#39044;&#35757;&#32451;&#65288;&#21487;&#33021;&#26159;&#24494;&#35843;&#65289;&#30340;&#26631;&#20934;LLM&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#24615;&#33021;&#20248;&#21270;&#21644;&#23436;&#20840;&#20860;&#23481;3D&#24182;&#34892;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#23454;&#29616;&#20102;EE-Tuning&#30340;&#21331;&#36234;&#35757;&#32451;&#25928;&#29575;&#12290;&#31995;&#32479;&#23454;&#39564;&#35777;&#23454;&#20102;EE-Tuning&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#32456;&#27490;LLM&#25512;&#29702;&#12290;&#20026;&#20102;&#23558;&#26089;&#26399;&#32456;&#27490;LLMs&#25512;&#24191;&#21040;&#31038;&#21306;&#65292;&#25105;&#20204;&#22312;https://github.com/pan-x-c/EE-LLM&#19978;&#21457;&#24067;&#20102;EE-Tuning&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20840;&#23616;&#27169;&#22411;&#23548;&#21521;&#35299;&#37322;&#22312;&#25968;&#25454;&#37197;&#32622;&#36807;&#31243;&#20013;&#30340;&#25351;&#23548;&#20316;&#29992;&#19981;&#36275;&#65292;&#25968;&#25454;&#23548;&#21521;&#35299;&#37322;&#25552;&#39640;&#20102;&#23545;&#37197;&#32622;&#21518;&#31995;&#32479;&#21464;&#21270;&#30340;&#29702;&#35299;&#65292;&#20004;&#31181;&#35299;&#37322;&#31867;&#22411;&#30340;&#28151;&#21512;&#34701;&#21512;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00491</link><description>&lt;p&gt;
EXMOS: &#36890;&#36807;&#22810;&#26041;&#38754;&#35299;&#37322;&#21644;&#25968;&#25454;&#37197;&#32622;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#23548;&#21521;
&lt;/p&gt;
&lt;p&gt;
EXMOS: Explanatory Model Steering Through Multifaceted Explanations and Data Configurations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20840;&#23616;&#27169;&#22411;&#23548;&#21521;&#35299;&#37322;&#22312;&#25968;&#25454;&#37197;&#32622;&#36807;&#31243;&#20013;&#30340;&#25351;&#23548;&#20316;&#29992;&#19981;&#36275;&#65292;&#25968;&#25454;&#23548;&#21521;&#35299;&#37322;&#25552;&#39640;&#20102;&#23545;&#37197;&#32622;&#21518;&#31995;&#32479;&#21464;&#21270;&#30340;&#29702;&#35299;&#65292;&#20004;&#31181;&#35299;&#37322;&#31867;&#22411;&#30340;&#28151;&#21512;&#34701;&#21512;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#35843;&#35797;&#21644;&#25913;&#36827;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21508;&#31181;&#20840;&#23616;&#30340;&#27169;&#22411;&#23548;&#21521;&#21644;&#25968;&#25454;&#23548;&#21521;&#30340;&#35299;&#37322;&#65292;&#22312;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#26816;&#27979;&#21644;&#35299;&#20915;&#28508;&#22312;&#25968;&#25454;&#38382;&#39064;&#20197;&#25913;&#36827;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25968;&#25454;&#23548;&#21521;&#21644;&#27169;&#22411;&#23548;&#21521;&#20840;&#23616;&#35299;&#37322;&#22312;&#25903;&#25345;&#21307;&#30103;&#19987;&#23478;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#25163;&#21160;&#25968;&#25454;&#37197;&#32622;&#20248;&#21270;&#27169;&#22411;&#30340;&#31995;&#32479;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#65288;n = 70&#65289;&#21644;&#23450;&#24615;&#65288;n = 30&#65289;&#30740;&#31350;&#19982;&#21307;&#30103;&#19987;&#23478;&#19968;&#36215;&#25506;&#32034;&#19981;&#21516;&#35299;&#37322;&#23545;&#20449;&#20219;&#12289;&#21487;&#29702;&#35299;&#24615;&#21644;&#27169;&#22411;&#25913;&#36827;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25968;&#25454;&#37197;&#32622;&#36807;&#31243;&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#23548;&#21521;&#30340;&#35299;&#37322;&#19981;&#36275;&#20197;&#25351;&#23548;&#29992;&#25143;&#12290;&#34429;&#28982;&#25968;&#25454;&#23548;&#21521;&#30340;&#35299;&#37322;&#22686;&#21152;&#20102;&#23545;&#37197;&#32622;&#21518;&#31995;&#32479;&#21464;&#21270;&#30340;&#29702;&#35299;&#65292;&#20294;&#20004;&#31181;&#35299;&#37322;&#31867;&#22411;&#30340;&#28151;&#21512;&#34701;&#21512;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations in interactive machine-learning systems facilitate debugging and improving prediction models. However, the effectiveness of various global model-centric and data-centric explanations in aiding domain experts to detect and resolve potential data issues for model improvement remains unexplored. This research investigates the influence of data-centric and model-centric global explanations in systems that support healthcare experts in optimising models through automated and manual data configurations. We conducted quantitative (n=70) and qualitative (n=30) studies with healthcare experts to explore the impact of different explanations on trust, understandability and model improvement. Our results reveal the insufficiency of global model-centric explanations for guiding users during data configuration. Although data-centric explanations enhanced understanding of post-configuration system changes, a hybrid fusion of both explanation types demonstrated the highest effectiveness. 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#28040;&#36153;&#32773;&#21644;&#29983;&#20135;&#32773;&#20004;&#26041;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#32676;&#20307;&#20998;&#21106;&#12289;&#25512;&#33616;&#27169;&#22411;&#36873;&#25321;&#21644;&#39046;&#22495;&#35774;&#32622;&#23454;&#29616;&#20844;&#24179;&#24615;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.00485</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#28040;&#36153;&#32773;&#21644;&#29983;&#20135;&#32773;&#32676;&#20307;&#20844;&#24179;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00485
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#28040;&#36153;&#32773;&#21644;&#29983;&#20135;&#32773;&#20004;&#26041;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#32676;&#20307;&#20998;&#21106;&#12289;&#25512;&#33616;&#27169;&#22411;&#36873;&#25321;&#21644;&#39046;&#22495;&#35774;&#32622;&#23454;&#29616;&#20844;&#24179;&#24615;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#65292;&#24403;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#21270;&#20915;&#31574;&#26102;&#65292;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#25110;&#32676;&#20307;&#36827;&#34892;&#19981;&#20844;&#24179;&#24453;&#36935;&#65292;&#20174;&#32780;&#20135;&#29983;&#27861;&#24459;&#12289;&#20262;&#29702;&#25110;&#32463;&#27982;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#25512;&#33616;&#31995;&#32479;&#26159;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#37325;&#35201;&#20363;&#23376;&#65292;&#23427;&#20204;&#24110;&#21161;&#29992;&#25143;&#20570;&#20986;&#20915;&#31574;&#12290;&#36807;&#21435;&#22823;&#37096;&#20998;&#20851;&#20110;&#25512;&#33616;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#25991;&#29486;&#30740;&#31350;&#37117;&#26159;&#23558;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#29420;&#31435;&#23545;&#24453;&#65292;&#24573;&#35270;&#20102;&#25512;&#33616;&#31995;&#32479;&#22312;&#21452;&#36793;&#24066;&#22330;&#20013;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CP-FairRank&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#37325;&#26032;&#25490;&#21517;&#31639;&#27861;&#65292;&#22312;&#32852;&#21512;&#30446;&#26631;&#26694;&#26550;&#20013;&#26080;&#32541;&#38598;&#25104;&#20102;&#28040;&#36153;&#32773;&#21644;&#29983;&#20135;&#32773;&#20004;&#26041;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#65292;&#24182;&#21487;&#20197;&#26681;&#25454;&#32676;&#20307;&#20998;&#21106;&#12289;&#25512;&#33616;&#27169;&#22411;&#36873;&#25321;&#21644;&#39046;&#22495;&#32771;&#34385;&#22810;&#26679;&#30340;&#20844;&#24179;&#24615;&#35774;&#32622;&#65292;&#36825;&#26159;&#20854;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#28040;&#36153;&#32773;&#21644;&#29983;&#20135;&#32773;&#30340;&#28385;&#24847;&#24230;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an increasing recognition that when machine learning (ML) algorithms are used to automate decisions, they may mistreat individuals or groups, with legal, ethical, or economic implications. Recommender systems are prominent examples of these machine learning (ML) systems that aid users in making decisions. The majority of past literature research on RS fairness treats user and item fairness concerns independently, ignoring the fact that recommender systems function in a two-sided marketplace. In this paper, we propose CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates fairness constraints from both the consumer and producer side in a joint objective framework. The framework is generalizable and may take into account varied fairness settings based on group segmentation, recommendation model selection, and domain, which is one of its key characteristics. For instance, we demonstrate that the system may jointly increase consum
&lt;/p&gt;</description></item><item><title>SA-MDKIF&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#24182;&#35757;&#32451;&#21307;&#23398;&#25216;&#33021;&#65292;&#24182;&#22312;&#25512;&#29702;&#20013;&#23558;&#20854;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00474</link><description>&lt;p&gt;
SA-MDKIF&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00474
&lt;/p&gt;
&lt;p&gt;
SA-MDKIF&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#24182;&#35757;&#32451;&#21307;&#23398;&#25216;&#33021;&#65292;&#24182;&#22312;&#25512;&#29702;&#20013;&#23558;&#20854;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#26377;&#25928;&#24212;&#29992;&#21463;&#21040;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#32570;&#20047;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#26694;&#26550;SA-MDKIF&#65292;&#26088;&#22312;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23558;&#21307;&#23398;&#30693;&#35782;&#27880;&#20837;&#36890;&#29992;&#22411;LLMs&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;SA-MDKIF&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25216;&#33021;&#35757;&#32451;&#21644;&#25216;&#33021;&#36866;&#24212;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;12&#31181;&#22522;&#26412;&#30340;&#21307;&#23398;&#25216;&#33021;&#65292;&#24182;&#20351;&#29992;AdaLoRA&#26681;&#25454;&#25105;&#20204;&#26500;&#24314;&#30340;&#32479;&#19968;&#26684;&#24335;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#36825;&#20123;&#25216;&#33021;&#12290;&#22312;&#19979;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#29305;&#23450;&#20219;&#21153;&#30340;&#19979;&#28216;&#25968;&#25454;&#26469;&#35757;&#32451;&#25216;&#33021;&#36335;&#30001;&#22120;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#35813;&#36335;&#30001;&#22120;&#23558;&#33719;&#21462;&#30340;&#25216;&#33021;&#19982;LLMs&#38598;&#25104;&#12290;&#23545;9&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#27604;&#65292;SA-MDKIF&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10-20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the origina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230; Q &#23398;&#20064;&#30340;&#26550;&#26500; RadDQN&#65292;&#22312;&#36752;&#23556;&#21306;&#22495;&#20013;&#25552;&#20379;&#26102;&#38388;&#39640;&#25928;&#30340;&#26368;&#23567;&#36752;&#23556;&#26292;&#38706;&#36335;&#24452;&#12290;&#36890;&#36807;&#20351;&#29992;&#36752;&#23556;&#24863;&#30693;&#22870;&#21169;&#20989;&#25968;&#21644;&#29420;&#29305;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#20248;&#21270;&#20154;&#21592;&#36752;&#23556;&#26292;&#38706;&#65292;&#20026;&#33258;&#20027;&#26080;&#20154;&#26426;&#39046;&#22495;&#24102;&#26469;&#36752;&#23556;&#38450;&#25252;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.00468</link><description>&lt;p&gt;
RadDQN: &#19968;&#31181;&#22522;&#20110;&#28145;&#24230; Q &#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#23547;&#25214;&#26102;&#38388;&#39640;&#25928;&#30340;&#26368;&#23567;&#36752;&#23556;&#26292;&#38706;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient Minimum Radiation Exposure Pathway
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230; Q &#23398;&#20064;&#30340;&#26550;&#26500; RadDQN&#65292;&#22312;&#36752;&#23556;&#21306;&#22495;&#20013;&#25552;&#20379;&#26102;&#38388;&#39640;&#25928;&#30340;&#26368;&#23567;&#36752;&#23556;&#26292;&#38706;&#36335;&#24452;&#12290;&#36890;&#36807;&#20351;&#29992;&#36752;&#23556;&#24863;&#30693;&#22870;&#21169;&#20989;&#25968;&#21644;&#29420;&#29305;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#20248;&#21270;&#20154;&#21592;&#36752;&#23556;&#26292;&#38706;&#65292;&#20026;&#33258;&#20027;&#26080;&#20154;&#26426;&#39046;&#22495;&#24102;&#26469;&#36752;&#23556;&#38450;&#25252;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (DRL) &#25216;&#26415;&#30340;&#36827;&#27493;&#24341;&#21457;&#20102;&#20854;&#22312;&#33258;&#21160;&#21270;&#39046;&#22495;&#30340;&#22810;&#26041;&#38754;&#24212;&#29992;&#12290;&#22312;&#26680;&#24037;&#19994;&#20013;&#20351;&#29992; DRL &#22788;&#29702;&#22797;&#26434;&#30340;&#20915;&#31574;&#38382;&#39064;&#40723;&#21169;&#20854;&#29992;&#20110;&#22312;&#27491;&#24120;&#36816;&#34892;&#21644;&#28508;&#22312;&#20107;&#25925;&#24773;&#20917;&#19979;&#20248;&#21270;&#20154;&#21592;&#36752;&#23556;&#26292;&#38706;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#39640;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#38459;&#30861;&#20102;&#20854;&#22312;&#24320;&#21457;&#36752;&#23556;&#24863;&#30693;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#39046;&#22495;&#30340;&#23454;&#29616;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#38480;&#24230;&#30340;&#36752;&#23556;&#38450;&#25252;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#26377;&#36259;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230; Q &#23398;&#20064;&#30340;&#26550;&#26500; (RadDQN)&#65292;&#35813;&#26550;&#26500;&#22522;&#20110;&#36752;&#23556;&#24863;&#30693;&#22870;&#21169;&#20989;&#25968;&#65292;&#22312;&#36752;&#23556;&#21306;&#22495;&#20013;&#25552;&#20379;&#26102;&#38388;&#39640;&#25928;&#30340;&#26368;&#23567;&#36752;&#23556;&#26292;&#38706;&#36335;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#29420;&#29305;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#26681;&#25454;&#21508;&#20010;&#29366;&#24577;&#30340;&#21464;&#21270;&#35843;&#25972;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep reinforcement learning (DRL) techniques have sparked its multifaceted applications in the automation sector. Managing complex decision-making problems with DRL encourages its use in the nuclear industry for tasks such as optimizing radiation exposure to the personnel during normal operating conditions and potential accidental scenarios. However, the lack of efficient reward function and effective exploration strategy thwarted its implementation in the development of radiation-aware autonomous unmanned aerial vehicle (UAV) for achieving maximum radiation protection. Here, in this article, we address these intriguing issues and introduce a deep Q-learning based architecture (RadDQN) that operates on a radiation-aware reward function to provide time-efficient minimum radiation-exposure pathway in a radiation zone. We propose a set of unique exploration strategies that fine-tune the extent of exploration and exploitation based on the state-wise variation in radi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#32422;&#26463;&#32534;&#31243;&#26041;&#27861;&#26469;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36827;&#21270;&#32534;&#31243;&#21457;&#29616;&#39640;&#25928;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#34920;&#31034;&#26041;&#27861;&#12289;&#36866;&#24212;&#24230;&#35780;&#20272;&#26041;&#26696;&#21644;&#39044;&#36873;&#25321;&#26426;&#21046;&#23454;&#29616;&#20102;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.00459</link><description>&lt;p&gt;
&#22522;&#20110;&#36951;&#20256;&#30340;&#32422;&#26463;&#32534;&#31243;&#22312;&#36164;&#28304;&#21463;&#38480;&#20316;&#19994;&#35843;&#24230;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Genetic-based Constraint Programming for Resource Constrained Job Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#32422;&#26463;&#32534;&#31243;&#26041;&#27861;&#26469;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36827;&#21270;&#32534;&#31243;&#21457;&#29616;&#39640;&#25928;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#34920;&#31034;&#26041;&#27861;&#12289;&#36866;&#24212;&#24230;&#35780;&#20272;&#26041;&#26696;&#21644;&#39044;&#36873;&#25321;&#26426;&#21046;&#23454;&#29616;&#20102;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#21463;&#38480;&#30340;&#20316;&#19994;&#35843;&#24230;&#26159;&#19968;&#31181;&#22256;&#38590;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#28304;&#33258;&#20110;&#37319;&#30719;&#34892;&#19994;&#12290;&#36890;&#29992;&#27714;&#35299;&#22120;&#26080;&#27861;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#20196;&#20154;&#28385;&#24847;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#20854;&#20182;&#35299;&#27861;&#26041;&#27861;&#22914;&#35768;&#22810;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#21644;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#20302;&#32423;&#23450;&#21046;&#21644;&#19987;&#38376;&#30340;&#21551;&#21457;&#24335;&#25216;&#24039;&#25165;&#33021;&#26377;&#25928;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#30340;&#31639;&#27861;&#26469;&#21457;&#29616;&#36164;&#28304;&#21463;&#38480;&#20316;&#19994;&#35843;&#24230;&#32422;&#26463;&#32534;&#31243;&#30340;&#39640;&#25928;&#25628;&#32034;&#31574;&#30053;&#26469;&#22635;&#34917;&#36825;&#20010;&#32570;&#21475;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20013;&#65292;&#36827;&#21270;&#30340;&#31243;&#24207;&#20195;&#34920;&#20102;&#22312;&#32422;&#26463;&#32534;&#31243;&#30340;&#25628;&#32034;&#36807;&#31243;&#20013;&#35201;&#20351;&#29992;&#30340;&#21464;&#37327;&#36873;&#25321;&#22120;&#65292;&#23427;&#20204;&#30340;&#36866;&#24212;&#24230;&#30001;&#38024;&#23545;&#35757;&#32451;&#23454;&#20363;&#33719;&#24471;&#30340;&#35299;&#36136;&#37327;&#26469;&#30830;&#23450;&#12290;&#35813;&#31639;&#27861;&#30340;&#21019;&#26032;&#28857;&#26377;&#65306;&#65288;1&#65289;&#26032;&#30340;&#21464;&#37327;&#36873;&#25321;&#22120;&#34920;&#31034;&#26041;&#27861;&#65292;&#65288;2&#65289;&#26032;&#30340;&#36866;&#24212;&#24230;&#35780;&#20272;&#26041;&#26696;&#65292;&#21644;&#65288;3&#65289;&#39044;&#36873;&#25321;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource constrained job scheduling is a hard combinatorial optimisation problem that originates in the mining industry. Off-the-shelf solvers cannot solve this problem satisfactorily in reasonable timeframes, while other solution methods such as many evolutionary computation methods and matheuristics cannot guarantee optimality and require low-level customisation and specialised heuristics to be effective. This paper addresses this gap by proposing a genetic programming algorithm to discover efficient search strategies of constraint programming for resource-constrained job scheduling. In the proposed algorithm, evolved programs represent variable selectors to be used in the search process of constraint programming, and their fitness is determined by the quality of solutions obtained for training instances. The novelties of this algorithm are (1) a new representation of variable selectors, (2) a new fitness evaluation scheme, and (3) a pre-selection mechanism. Tests with a large set of
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#21452;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#20855;&#26377;&#30456;&#21516;&#35268;&#27169;&#20294;&#32467;&#26500;&#30456;&#21453;&#30340;&#23398;&#29983;&#32593;&#32476;&#21644;&#19968;&#20010;&#21333;&#19968;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#35782;&#21035;&#19968;&#33268;&#24615;&#65292;&#24182;&#21516;&#26102;&#24341;&#20837;&#20102;&#24322;&#24120;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00448</link><description>&lt;p&gt;
&#21452;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#32593;&#32476;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#21452;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#20855;&#26377;&#30456;&#21516;&#35268;&#27169;&#20294;&#32467;&#26500;&#30456;&#21453;&#30340;&#23398;&#29983;&#32593;&#32476;&#21644;&#19968;&#20010;&#21333;&#19968;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#35782;&#21035;&#19968;&#33268;&#24615;&#65292;&#24182;&#21516;&#26102;&#24341;&#20837;&#20102;&#24322;&#24120;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#32570;&#38519;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#23398;&#29983;-&#25945;&#24072;&#32593;&#32476;&#65288;S-T&#65289;&#22312;&#25506;&#32034;&#30001;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#24471;&#20986;&#30340;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#24046;&#24322;&#20197;&#35782;&#21035;&#24322;&#24120;&#26041;&#38754;&#24471;&#21040;&#20102;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;S-T&#32593;&#32476;&#19981;&#22815;&#31283;&#23450;&#12290;&#37319;&#29992;&#30456;&#21516;&#30340;&#32467;&#26500;&#26500;&#24314;S-T&#32593;&#32476;&#21487;&#33021;&#20250;&#21066;&#24369;&#23545;&#24322;&#24120;&#30340;&#20195;&#34920;&#24615;&#24046;&#24322;&#12290;&#20294;&#20351;&#29992;&#19981;&#21516;&#30340;&#32467;&#26500;&#21487;&#20197;&#22686;&#21152;&#22312;&#27491;&#24120;&#25968;&#25454;&#19978;&#20135;&#29983;&#24046;&#24322;&#24615;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#65288;DSKD&#65289;&#26550;&#26500;&#12290;&#19982;&#20854;&#20182;S-T&#32593;&#32476;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#20855;&#26377;&#30456;&#21516;&#35268;&#27169;&#20294;&#32467;&#26500;&#30456;&#21453;&#30340;&#23398;&#29983;&#32593;&#32476;&#21644;&#19968;&#20010;&#21333;&#19968;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22686;&#24378;&#33976;&#39311;&#25928;&#26524;&#65292;&#25552;&#39640;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#35782;&#21035;&#19968;&#33268;&#24615;&#65292;&#24182;&#21516;&#26102;&#24341;&#20837;&#24322;&#24120;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semanti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00447</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Data-Efficient Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#31038;&#20132;&#32593;&#32476;&#21040;&#29983;&#29289;&#21270;&#23398;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#26159;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#36825;&#31181;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#26631;&#27880;&#36164;&#28304;&#26377;&#38480;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;(DEGL)&#30340;&#30740;&#31350;&#21069;&#27839;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;DEGL&#24403;&#21069;&#36827;&#23637;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#25105;&#20204;&#23545;DEGL&#30340;&#25506;&#32034;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#19968;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38750;&#32447;&#24615;&#38382;&#39064;&#26041;&#38754;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00435</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
A practical existence theorem for reduced order models based on convolutional autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38750;&#32447;&#24615;&#38382;&#39064;&#26041;&#38754;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#38477;&#38454;&#24314;&#27169;&#39046;&#22495;&#36234;&#21457;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#12289;&#31070;&#32463;&#31639;&#23376;&#12289;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#38477;&#38454;&#27169;&#22411;&#31561;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#25928;&#26524;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#26102;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38477;&#38454;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22522;&#20110;CNN&#30340;&#33258;&#32534;&#30721;&#22120;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#36825;&#20123;&#26550;&#26500;&#65292;&#36890;&#24120;&#20197;&#19975;&#33021;&#36924;&#36817;&#23450;&#29702;&#30340;&#24418;&#24335;&#38472;&#36848;&#12290;&#23588;&#20854;&#26159;&#65292;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20026;&#35774;&#35745;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#38024;&#65292;&#20294;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#21518;&#32493;&#25361;&#25112;&#20960;&#20046;&#27809;&#26377;&#34987;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has gained increasing popularity in the fields of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM), providing domain practitioners with new powerful data-driven techniques such as Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context, deep autoencoders based on Convolutional Neural Networks (CNNs) have proven extremely effective, outperforming established techniques, such as the reduced basis method, when dealing with complex nonlinear problems. However, despite the empirical success of CNN-based autoencoders, there are only a few theoretical results supporting these architectures, usually stated in the form of universal approximation theorems. In particular, although the existing literature provides users with guidelines for designing convolutional autoencoders, the subsequent challenge of learning the latent features has been barely inv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#22522;&#20110;&#25552;&#31034;&#39537;&#21160;&#30340;&#30693;&#35782;&#25429;&#33719;&#65292;&#29305;&#21035;&#20851;&#27880;&#25552;&#31034;&#21040;&#19977;&#20803;&#32452;&#30340;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#19987;&#38376;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19977;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00414</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22522;&#20110;&#25552;&#31034;&#26102;&#38388;&#30340;&#31526;&#21495;&#30693;&#35782;&#25429;&#33719;
&lt;/p&gt;
&lt;p&gt;
Prompt-Time Symbolic Knowledge Capture with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#22522;&#20110;&#25552;&#31034;&#39537;&#21160;&#30340;&#30693;&#35782;&#25429;&#33719;&#65292;&#29305;&#21035;&#20851;&#27880;&#25552;&#31034;&#21040;&#19977;&#20803;&#32452;&#30340;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#19987;&#38376;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19977;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#20010;&#20154;AI&#21161;&#25163;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29992;&#25143;&#29305;&#23450;&#30340;&#30693;&#35782;&#30456;&#32467;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLMs&#26412;&#36523;&#32570;&#20047;&#22522;&#20110;&#25552;&#31034;&#39537;&#21160;&#30340;&#30693;&#35782;&#25429;&#33719;&#26426;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;LLMs&#33021;&#21147;&#23454;&#29616;&#22522;&#20110;&#25552;&#31034;&#39537;&#21160;&#30340;&#30693;&#35782;&#25429;&#33719;&#65292;&#29305;&#21035;&#20851;&#27880;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36890;&#36807;&#20851;&#27880;&#25552;&#31034;&#21040;&#19977;&#20803;&#32452;&#65288;P2T&#65289;&#29983;&#25104;&#26469;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#38646;&#23556;&#36317;&#12289;&#23569;&#23556;&#36317;&#21644;&#24494;&#35843;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#19987;&#38376;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/HaltiaAI/paper-PTSKC&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with user-specific knowledge is crucial for real-world applications, such as personal AI assistants. However, LLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper investigates utilizing the existing LLM capabilities to enable prompt-driven knowledge capture, with a particular emphasis on knowledge graphs. We address this challenge by focusing on prompt-to-triple (P2T) generation. We explore three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and then assess their performance via a specialized synthetic dataset. Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23545;&#25239;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#23398;&#29983;&#35770;&#25991;&#26816;&#27979;&#65292;&#36890;&#36807;&#26500;&#24314;AIG-ASAP&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#26816;&#27979;&#22120;&#26131;&#34987;&#33258;&#21160;&#23545;&#25239;&#25915;&#20987;&#25152;&#32469;&#36807;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00412</link><description>&lt;p&gt;
&#38544;&#34255;&#20195;&#31508;&#32773;&#65306;&#23545;AI&#29983;&#25104;&#30340;&#23398;&#29983;&#35770;&#25991;&#26816;&#27979;&#36827;&#34892;&#23545;&#25239;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#25239;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#23398;&#29983;&#35770;&#25991;&#26816;&#27979;&#65292;&#36890;&#36807;&#26500;&#24314;AIG-ASAP&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#26816;&#27979;&#22120;&#26131;&#34987;&#33258;&#21160;&#23545;&#25239;&#25915;&#20987;&#25152;&#32469;&#36807;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#25220;&#34989;&#12289;&#20256;&#25773;&#20551;&#26032;&#38395;&#20197;&#21450;&#25945;&#32946;&#20064;&#39064;&#20013;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#26377;&#20960;&#31181;&#26816;&#27979;&#22120;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#25239;&#25200;&#21160;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#29983;&#35770;&#25991;&#20889;&#20316;&#30340;&#32972;&#26223;&#19979;&#65292;&#20173;&#28982;&#34987;&#36739;&#23569;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#26500;&#24314;AIG-ASAP&#65292;&#19968;&#20010;&#22522;&#20110;AI&#29983;&#25104;&#30340;&#23398;&#29983;&#35770;&#25991;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#19968;&#31995;&#21015;&#39044;&#35745;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#35770;&#25991;&#30340;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#21516;&#26102;&#36530;&#36991;&#26816;&#27979;&#12290;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;AIGC&#26816;&#27979;&#22120;&#22312;AIG-ASAP&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#34987;&#30452;&#25509;&#30340;&#33258;&#21160;&#23545;&#25239;&#25915;&#20987;&#25152;&#35268;&#36991;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35789;&#26367;&#25442;&#21644;&#21477;&#23376;&#26367;&#25442;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LM-HT&#27169;&#22411;&#65292;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.00411</link><description>&lt;p&gt;
LM-HT SNN: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#22686;&#24378;SNN&#19982;ANN&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LM-HT&#27169;&#22411;&#65292;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#27604;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22240;&#20854;&#26356;&#20855;&#29983;&#29289;&#21551;&#21457;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#20449;&#24687;&#20256;&#36882;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#23398;&#26415;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#23545;SNN&#30340;&#23398;&#20064;&#26799;&#24230;&#21644;&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20294;&#22312;&#24615;&#33021;&#26041;&#38754;SNN&#20173;&#28982;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33853;&#21518;&#20110;ANN&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22810;&#38408;&#20540;&#27169;&#22411;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;SNN&#30340;&#23398;&#20064;&#33021;&#21147;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#23398;&#30340;&#35282;&#24230;&#20005;&#26684;&#20998;&#26512;&#20102;&#22810;&#38408;&#20540;&#27169;&#22411;&#12289;&#21407;&#22987;&#33033;&#20914;&#27169;&#22411;&#21644;&#37327;&#21270;ANN&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LM-HT&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#31561;&#36317;&#22810;&#23618;&#27425;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#21160;&#24577;&#35843;&#33410;&#20840;&#23616;&#36755;&#20837;&#30005;&#27969;&#21644;&#33180;&#30005;&#20301;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#22522;&#20110;LM-HT&#27169;&#22411;&#30340;&#30452;&#25509;&#35757;&#32451;&#31639;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#36830;&#25509;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner. However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. In addition, we note that the direct training algorithm based on the LM-HT model can seamlessl
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#30740;&#31350;&#20102;Llama 2 Chat&#20013;&#30340;&#20559;&#35265;&#34920;&#31034;&#38382;&#39064;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35266;&#23519;&#21040;&#20559;&#35265;&#19982;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#30340;&#20542;&#21521;&#20043;&#38388;&#23384;&#22312;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.00402</link><description>&lt;p&gt;
&#36890;&#36807;&#28608;&#27963;&#23548;&#21521;&#30740;&#31350; Llama 2 Chat &#20013;&#30340;&#20559;&#35265;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Investigating Bias Representations in Llama 2 Chat via Activation Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00402
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#30740;&#31350;&#20102;Llama 2 Chat&#20013;&#30340;&#20559;&#35265;&#34920;&#31034;&#38382;&#39064;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35266;&#23519;&#21040;&#20559;&#35265;&#19982;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#30340;&#20542;&#21521;&#20043;&#38388;&#23384;&#22312;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880; Llama 2 7B Chat &#27169;&#22411;&#12290;&#38543;&#30528; LLMs &#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#20855;&#26377;&#37325;&#22823;&#31038;&#20250;&#24433;&#21709;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;&#24378;&#21270;&#29616;&#26377;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#26469;&#25506;&#27979;&#21644;&#20943;&#36731;&#19982;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#26377;&#20851;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25805;&#32437;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#25351;&#23548;&#22238;&#24212;&#26397;&#21521;&#25110;&#36828;&#31163;&#26377;&#20559;&#35265;&#30340;&#36755;&#20986;&#65292;&#21033;&#29992;&#20174;StereoSet&#25968;&#25454;&#38598;&#21644;&#33258;&#23450;&#20041;GPT4&#29983;&#25104;&#30340;&#24615;&#21035;&#20559;&#35265;&#25552;&#31034;&#24471;&#21040;&#30340;&#23548;&#21521;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;Llama 2 7B Chat&#20013;&#22266;&#26377;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21363;&#20351;&#22312;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20043;&#21518;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#20559;&#35265;&#19982;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#30340;&#20542;&#21521;&#20043;&#38388;&#23384;&#22312;&#21487;&#39044;&#27979;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013; tend &#36235;&#21521;&#22686;&#21152;&#27169;&#22411;&#23545;&#19981;&#21516;&#24418;&#24335;&#31038;&#20250;&#20559;&#35265;&#30340;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal bia
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#20174;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#23398;&#20064;&#24182;&#39044;&#27979;&#20854;&#20182;&#22478;&#24066;&#30340;&#20132;&#36890;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.00397</link><description>&lt;p&gt;
&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00397
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#20174;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#23398;&#20064;&#24182;&#39044;&#27979;&#20854;&#20182;&#22478;&#24066;&#30340;&#20132;&#36890;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#23545;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#20197;&#24110;&#21161;&#39640;&#25928;&#20998;&#37197;&#36164;&#28304;&#21644;&#26377;&#25928;&#25511;&#21046;&#20132;&#36890;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#25928;&#24615;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#20016;&#23500;&#30340;&#20132;&#36890;&#25968;&#25454;&#65292;&#32780;&#35768;&#22810;&#22478;&#24066;&#30001;&#20110;&#35774;&#22791;&#25903;&#25345;&#26377;&#38480;&#32780;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;&#36825;&#23545;&#20132;&#36890;&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#35266;&#23519;&#65306;&#20132;&#36890;&#27169;&#24335;&#22312;&#19981;&#21516;&#22478;&#24066;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#20851;&#38190;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#65288;MTPB&#65289;&#12290;&#20027;&#35201;&#19978;&#65292;MTPB&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#21551;&#21160;&#20854;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#33719;&#21462;&#20840;&#38754;&#30340;&#20132;&#36890;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20808;&#36827;&#30340;&#32858;&#31867;&#25216;&#26415;&#20174;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20013;&#31995;&#32479;&#29983;&#25104;&#19968;&#20010;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#12290;&#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20934;&#30830;&#30340;&#20132;&#36890;&#27169;&#24335;&#26816;&#32034;&#26426;&#21046;&#36827;&#34892;&#36328;&#22478;&#24066;&#30340;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, th
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.00396</link><description>&lt;p&gt;
LLMs&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00396
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#39640;&#25928;&#25506;&#32034;&#22312;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#31243;&#24207;&#22312;&#25910;&#21040;&#21453;&#39304;&#26102;&#23558;&#22870;&#21169;&#27169;&#22411;&#25311;&#21512;&#21040;&#26597;&#35810;&#19978;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#21452;Thompson&#37319;&#26679;&#29983;&#25104;&#26597;&#35810;&#65292;&#19981;&#30830;&#23450;&#24615;&#30001;&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#25928;&#25506;&#32034;&#20351;&#24471;&#24615;&#33021;&#27700;&#24179;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#26597;&#35810;&#19979;&#36798;&#21040;&#36739;&#39640;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#26426;&#26800;&#25163;&#33218;&#27515;&#21306;&#20869;&#30340;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#21487;&#29992;&#30340;&#36816;&#21160;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00393</link><description>&lt;p&gt;
&#32771;&#34385;&#27515;&#21306;&#30340;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Loss Function Considering Dead Zone for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#26426;&#26800;&#25163;&#33218;&#27515;&#21306;&#20869;&#30340;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#21487;&#29992;&#30340;&#36816;&#21160;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#26426;&#26800;&#25163;&#33218;&#30340;&#36870;&#21160;&#21147;&#23398;&#26159;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#25511;&#21046;&#30340;&#25511;&#21046;&#24615;&#33021;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#31070;&#32463;&#32593;&#32476;&#26159;&#34920;&#31034;&#22797;&#26434;&#36870;&#21160;&#21147;&#23398;&#30340;&#26377;&#26395;&#25216;&#26415;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#36816;&#21160;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#25191;&#34892;&#26426;&#26500;&#30340;&#27515;&#21306;&#20013;&#30340;&#36816;&#21160;&#25968;&#25454;&#19981;&#36866;&#21512;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#65292;&#36825;&#38477;&#20302;&#20102;&#21487;&#29992;&#20110;&#35757;&#32451;&#30340;&#26377;&#29992;&#25968;&#25454;&#37327;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#26426;&#26800;&#25163;&#33218;&#20851;&#33410;&#22312;&#27515;&#21306;&#20869;&#19981;&#24037;&#20316;&#30340;&#20107;&#23454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21482;&#32771;&#34385;&#19981;&#22312;&#27515;&#21306;&#30340;&#20851;&#33410;&#30340;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#29992;&#20110;&#35757;&#32451;&#30340;&#36816;&#21160;&#25968;&#25454;&#37327;&#22686;&#21152;&#65292;&#24182;&#25552;&#39640;&#20102;&#36870;&#21160;&#21147;&#23398;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#33258;&#30001;&#24230;&#26426;&#26800;&#25163;&#33218;&#30340;&#23454;&#38469;&#35774;&#22791;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#24182;&#35752;&#35770;&#20102;&#22312;&#27515;&#21306;&#20013;&#20351;&#29992;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is important to reveal the inverse dynamics of manipulators to improve control performance of model-based control. Neural networks (NNs) are promising techniques to represent complicated inverse dynamics while they require a large amount of motion data. However, motion data in dead zones of actuators is not suitable for training models decreasing the number of useful training data. In this study, based on the fact that the manipulator joint does not work irrespective of input torque in dead zones, we propose a new loss function that considers only errors of joints not in dead zones. The proposed method enables to increase in the amount of motion data available for training and the accuracy of the inverse dynamics computation. Experiments on actual equipment using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than conventional methods. We also confirmed and discussed the behavior of the model of the proposed method in dead zones.
&lt;/p&gt;</description></item><item><title>EASRec&#26159;&#19968;&#20010;&#38024;&#23545;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#24377;&#24615;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21098;&#26525;&#25216;&#26415;&#21644;&#20808;&#36827;&#27169;&#22411;&#26550;&#26500;&#32467;&#21512;&#65292;&#20197;&#21450;&#36164;&#28304;&#21463;&#38480;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00390</link><description>&lt;p&gt;
EASRec&#65306;&#29992;&#20110;&#39640;&#25928;&#38271;&#26399;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#24377;&#24615;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
EASRec: Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00390
&lt;/p&gt;
&lt;p&gt;
EASRec&#26159;&#19968;&#20010;&#38024;&#23545;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#24377;&#24615;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21098;&#26525;&#25216;&#26415;&#21644;&#20808;&#36827;&#27169;&#22411;&#26550;&#26500;&#32467;&#21512;&#65292;&#20197;&#21450;&#36164;&#28304;&#21463;&#38480;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#26102;&#20195;&#65292;&#20174;&#28023;&#37327;&#20449;&#24687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#24403;&#21069;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65288;SRSs&#65289;&#22312;&#35745;&#31639;&#21644;&#36164;&#28304;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#37319;&#29992;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#65288;&#22914;SASRec&#65289;&#30340;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#26088;&#22312;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#25512;&#33616;&#65292;&#20174;&#30005;&#23376;&#21830;&#21153;&#21040;&#31038;&#20132;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#25512;&#29702;&#38454;&#27573;&#20250;&#20135;&#29983;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#33258;&#21160;&#21098;&#26525;&#25216;&#26415;&#21644;&#20808;&#36827;&#27169;&#22411;&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#27969;&#34892;&#30340;&#36164;&#28304;&#21463;&#38480;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#20197;&#35843;&#25972;&#27169;&#22411;&#20197;&#20943;&#23569;FLOPs&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
In this age where data is abundant, the ability to distill meaningful insights from the sea of information is essential. Our research addresses the computational and resource inefficiencies that current Sequential Recommender Systems (SRSs) suffer from. especially those employing attention-based models like SASRec, These systems are designed for next-item recommendations in various applications, from e-commerce to social networks. However, such systems suffer from substantial computational costs and resource consumption during the inference stage. To tackle these issues, our research proposes a novel method that combines automatic pruning techniques with advanced model architectures. We also explore the potential of resource-constrained Neural Architecture Search (NAS), a technique prevalent in the realm of recommendation systems, to fine-tune models for reduced FLOPs, latency, and energy usage while retaining or even enhancing accuracy. The main contribution of our work is developing 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00389</link><description>&lt;p&gt;
&#20851;&#20110;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;$O(\frac{\sqrt{d}}{T^{1/4}})$&#25910;&#25947;&#36895;&#24230;&#21644;&#23545;&#32500;&#24230;&#30340;&#25913;&#36827;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the $O(\frac{\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\ell_1$ Norm: Better Dependence on the Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00389
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20854;&#25910;&#25947;&#36895;&#24230;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20854;&#23545;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;$\ell_1$&#33539;&#25968;&#24314;&#31435;&#20102;&#25910;&#25947;&#29575;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#26080;&#38656;&#20551;&#35774;&#26799;&#24230;&#26377;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#20248;&#21270;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;$T$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#30001;&#20110;&#23545;&#20110;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#65292;$\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#21487;&#20197;&#31867;&#27604;&#20026;SGD&#30340;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$&#65292;&#27979;&#24230;&#20026;$\ell_1$&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$ measured by $\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number. Since $\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$ one of SGD measured by $\ell_1$ norm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CuFun&#27169;&#22411;&#65292;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36890;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#24378;&#24230;&#20989;&#25968;&#24314;&#27169;&#12289;&#31215;&#20998;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#38271;&#26399;&#26102;&#24207;&#20381;&#36182;&#24615;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00388</link><description>&lt;p&gt;
&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36890;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Cumulative Distribution Function based General Temporal Point Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;CuFun&#27169;&#22411;&#65292;&#22522;&#20110;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36890;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#24378;&#24230;&#20989;&#25968;&#24314;&#27169;&#12289;&#31215;&#20998;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#38271;&#26399;&#26102;&#24207;&#20381;&#36182;&#24615;&#25429;&#25417;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#28857;&#36807;&#31243;&#22312;&#24314;&#27169;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20107;&#20214;&#24207;&#21015;&#65288;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#65289;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#25512;&#33616;&#31995;&#32479;&#21644;&#20449;&#24687;&#26816;&#32034;&#31574;&#30053;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#12290;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#20132;&#20114;&#21644;&#20132;&#26131;&#31561;&#20107;&#20214;&#65292;&#26102;&#38388;&#28857;&#36807;&#31243;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#34892;&#20026;&#27169;&#24335;&#27934;&#23519;&#65292;&#26377;&#21161;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#26102;&#38388;&#28857;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#26102;&#38388;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24314;&#27169;&#24378;&#24230;&#20989;&#25968;&#12289;&#22797;&#26434;&#31215;&#20998;&#35745;&#31639;&#21644;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#26102;&#24207;&#20381;&#36182;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CuFun&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Point Processes (TPPs) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies. Through the analysis of events such as user interactions and transactions, TPPs offer valuable insights into behavioral patterns, facilitating the prediction of future trends. However, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns. The integration of Neural Networks with TPPs has ushered in the development of advanced deep TPP models. While these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively. In this study, we introduce the CuFun model, representing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31070;&#32463;&#27979;&#37327;&#32593;&#32476;&#19982;&#19981;&#21464;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#29366;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20223;&#30495;&#25968;&#25454;&#65292;&#36890;&#36807;&#35843;&#25972;&#23398;&#20064;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#26469;&#32553;&#23567;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.00366</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21464;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#27979;&#37327;&#32593;&#32476;&#36827;&#34892;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Legged Robot State Estimation With Invariant Extended Kalman Filter Using Neural Measurement Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31070;&#32463;&#27979;&#37327;&#32593;&#32476;&#19982;&#19981;&#21464;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#29366;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20223;&#30495;&#25968;&#25454;&#65292;&#36890;&#36807;&#35843;&#25972;&#23398;&#20064;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#26469;&#32553;&#23567;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#26412;&#20307;&#24863;&#30693;&#29366;&#24577;&#20272;&#35745;&#22120;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#22810;&#23618;&#24863;&#30693;&#22120;&#25110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#65292;&#21253;&#25324;&#25509;&#35302;&#27010;&#29575;&#21644;&#32447;&#36895;&#24230;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#23558;&#31070;&#32463;&#27979;&#37327;&#32593;&#32476;(NMN)&#19982;&#19981;&#21464;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#22320;&#24418;&#19979;&#25913;&#21892;&#20102;&#20272;&#35745;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28388;&#27874;&#22120;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#20223;&#30495;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36731;&#26494;&#33719;&#24471;&#22823;&#37327;&#25968;&#25454;&#12290;&#36825;&#31181;&#24046;&#24322;&#23548;&#33268;&#20102;&#23398;&#20064;&#21644;&#25512;&#29702;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#27169;&#25311;&#21040;&#30495;&#23454;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#25972;&#29616;&#26377;&#30340;&#23398;&#20064;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel proprioceptive state estimator for legged robots that combines model-based filters and deep neural networks. Recent studies have shown that neural networks such as multi-layer perceptron or recurrent neural networks can estimate the robot states, including contact probability and linear velocity. Inspired by this, we develop a state estimation framework that integrates a neural measurement network (NMN) with an invariant extended Kalman filter. We show that our framework improves estimation performance in various terrains. Existing studies that combine model-based filters and learning-based approaches typically use real-world data. However, our approach relies solely on simulation data, as it allows us to easily obtain extensive data. This difference leads to a gap between the learning and the inference domain, commonly referred to as a sim-to-real gap. We address this challenge by adapting existing learning techniques and regularization. To validate our p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37325;&#24314;&#20102;&#36807;&#21435;&#30340;&#28909;&#24102;&#27668;&#26059;&#35266;&#27979;&#65292;&#24182;&#33719;&#24471;&#20102;&#20840;&#29699;&#28909;&#24102;&#27668;&#26059;&#39118;&#22330;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#21644;&#33021;&#37327;&#30340;&#27668;&#20505;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.00362</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25581;&#31034;&#20102;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#21644;&#33021;&#37327;&#26497;&#20540;&#30340;&#27668;&#20505;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Climate Trends of Tropical Cyclone Intensity and Energy Extremes Revealed by Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37325;&#24314;&#20102;&#36807;&#21435;&#30340;&#28909;&#24102;&#27668;&#26059;&#35266;&#27979;&#65292;&#24182;&#33719;&#24471;&#20102;&#20840;&#29699;&#28909;&#24102;&#27668;&#26059;&#39118;&#22330;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#21644;&#33021;&#37327;&#30340;&#27668;&#20505;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#24050;&#19982;&#28909;&#24102;&#27668;&#26059;&#21521;&#26497;&#22320;&#36801;&#31227;&#12289;&#27668;&#26059;&#26497;&#31471;&#38477;&#27700;&#20197;&#21450;&#37325;&#22823;&#39123;&#39118;&#27604;&#20363;&#22686;&#21152;&#30456;&#20851;&#32852;&#12290;&#20102;&#35299;&#36807;&#21435;&#28909;&#24102;&#27668;&#26059;&#30340;&#36235;&#21183;&#21644;&#21464;&#21270;&#23545;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#23545;&#20154;&#31867;&#31038;&#20250;&#30340;&#26410;&#26469;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#65292;&#36807;&#21435;&#30340;&#28909;&#24102;&#27668;&#26059;&#32467;&#26500;&#21644;&#33021;&#37327;&#36235;&#21183;&#20173;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20027;&#35266;&#20998;&#26512;&#21644;&#26102;&#31354;&#24322;&#36136;&#30340;&#8220;&#26368;&#20339;&#36335;&#24452;&#8221;&#25968;&#25454;&#38598;&#23548;&#33268;&#23545;&#27668;&#20505;&#21464;&#21270;&#23545;&#27668;&#26059;&#21709;&#24212;&#30340;&#35780;&#20272;&#32570;&#20047;&#20449;&#24515;&#12290;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#20102;&#36807;&#21435;&#30340;&#8220;&#35266;&#27979;&#8221;&#65292;&#24182;&#22312;1981&#24180;&#33267;2020&#24180;&#26399;&#38388;&#33719;&#24471;&#20102;&#23458;&#35266;&#30340;&#20840;&#29699;&#28909;&#24102;&#27668;&#26059;&#39118;&#22330;&#36164;&#26009;&#65292;&#26377;&#21161;&#20110;&#20840;&#38754;&#30740;&#31350;&#28909;&#24102;&#27668;&#26059;&#30340;&#32467;&#26500;&#21644;&#33021;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#26368;&#20339;&#36335;&#24452;&#21644;&#25968;&#20540;&#27169;&#22411;&#20998;&#26512;2004&#24180;&#33267;2018&#24180;&#28909;&#24102;&#27668;&#26059;&#30340;&#29420;&#29305;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22810;&#36890;&#36947;&#21355;&#26143;&#22270;&#20687;&#36716;&#25442;&#20026;0-750&#20844;&#37324;&#30340;&#36724;&#23545;&#31216;&#22320;&#34920;&#39118;&#36895;&#39118;&#22330;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#22797;&#26434;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#21306;&#20998;&#19981;&#21516;&#30340;&#22825;&#27668;&#31995;&#32479;&#65292;&#21253;&#25324;&#21488;&#39118;&#24378;&#24230;&#12289;&#21453;&#23556;&#29575;&#12289;&#39118;&#36895;&#21644;&#27668;&#28201;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anthropogenic influences have been linked to tropical cyclone (TC) poleward migration, TC extreme precipitation, and an increased proportion of major hurricanes [1, 2, 3, 4]. Understanding past TC trends and variability is critical for projecting future TC impacts on human society considering the changing climate [5]. However, past trends of TC structure/energy remain uncertain due to limited observations; subjective-analyzed and spatiotemporal-heterogeneous "best-track" datasets lead to reduced confidence in the assessed TC repose to climate change [6, 7]. Here, we use deep learning to reconstruct past "observations" and yield an objective global TC wind profile dataset during 1981 to 2020, facilitating a comprehensive examination of TC structure/energy. By training with uniquely labeled data integrating best tracks and numerical model analysis of 2004 to 2018 TCs, our model converts multichannel satellite imagery to a 0-750-km wind profile of axisymmetric surface winds. The model per
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#33258;&#36866;&#24212;&#23398;&#20064;&#36895;&#29575;&#20197;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12289;&#26368;&#20248;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00355</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Primal-Dual Method for Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#33258;&#36866;&#24212;&#23398;&#20064;&#36895;&#29575;&#20197;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12289;&#26368;&#20248;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#33258;&#28982;&#24212;&#29992;&#65292;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23558;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#27599;&#27425;&#35299;&#20915;&#23884;&#20837;&#30340;&#26080;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#23398;&#20064;&#36895;&#29575;&#65288;LR&#65289;&#21644;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65288;&#23545;&#20598;&#21464;&#37327;&#65289;&#20043;&#38388;&#23384;&#22312;&#30456;&#20114;&#20381;&#36182;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#12289;&#20998;&#26512;&#21644;&#35780;&#20272;&#20102;&#36866;&#24212;&#24615;&#21407;&#22987;-&#23545;&#20598;&#65288;APD&#65289;&#26041;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#35843;&#25972;&#20004;&#20010;&#33258;&#36866;&#24212;LR&#20197;&#20351;&#20043;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;APD&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12289;&#26368;&#20248;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Bullet-Safey-Gym&#20013;&#30340;&#22235;&#20010;&#30693;&#21517;&#29615;&#22659;&#65292;&#21033;&#29992;&#20004;&#20010;&#20808;&#36827;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO-Lagrangian&#21644;DDPG-Lagrangian&#65289;&#23545;&#23454;&#38469;APD&#31639;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;&#25152;&#26377;&#23454;&#39564;&#34920;&#26126;&#65292;&#23454;&#38469;APD&#31639;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#65288;&#25110;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65289;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#31283;&#23450;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Primal-dual methods have a natural application in Safe Reinforcement Learning (SRL), posed as a constrained policy optimization problem. In practice however, applying primal-dual methods to SRL is challenging, due to the inter-dependency of the learning rate (LR) and Lagrangian multipliers (dual variables) each time an embedded unconstrained RL problem is solved. In this paper, we propose, analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the policy in each iteration. We theoretically establish the convergence, optimality and feasibility of the APD algorithm. Finally, we conduct numerical evaluation of the practical APD algorithm with four well-known environments in Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian and DDPG-Lagrangian. All experiments show that the practical APD algorithm outperforms (or achieves comparable performance) and attains more stable
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#23545;LLMs&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#22522;&#20110;LLMs&#30340;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.00350</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Based Fuzzing Techniques: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00350
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#23545;LLMs&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#22522;&#20110;LLMs&#30340;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36719;&#20214;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#26102;&#20195;&#65292;&#36719;&#20214;&#23433;&#20840;&#21644;&#28431;&#27934;&#20998;&#26512;&#23545;&#20110;&#36719;&#20214;&#24320;&#21457;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#65292;&#27169;&#31946;&#27979;&#35797;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#21487;&#20197;&#22312;&#36719;&#20214;&#27979;&#35797;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#24182;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#29616;&#26377;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#24182;&#19981;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#36719;&#20214;&#28431;&#27934;&#19981;&#26029;&#28436;&#21270;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#36235;&#21183;&#26159;&#37319;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#27169;&#31946;&#27979;&#35797;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#34701;&#21512;LLMs&#21644;&#27169;&#31946;&#27979;&#35797;&#30340;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#36890;&#36807;&#24635;&#32467;2024&#24180;&#20043;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#23545;LLMs&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#22522;&#20110;LLMs&#30340;&#27169;&#31946;&#27979;&#35797;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#21644;&#35752;&#35770;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#36824;&#30740;&#31350;&#20102;LLMs&#21644;&#27169;&#31946;&#27979;&#35797;&#22312;&#36719;&#20214;&#27979;&#35797;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern era where software plays a pivotal role, software security and vulnerability analysis have become essential for software development. Fuzzing test, as an efficient software testing method, are widely used in various domains. Moreover, the rapid development of Large Language Models (LLMs) has facilitated their application in the field of software testing, demonstrating remarkable performance. Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models. This survey provides a systematic overview of the approaches that fuse LLMs and fuzzing tests for software testing. In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024. Our survey also invest
&lt;/p&gt;</description></item><item><title>ODICE&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#37325;&#35201;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#20351;&#29992;&#30495;&#26799;&#24230;&#26356;&#26032;&#23398;&#20064;&#20540;&#20989;&#25968;&#26102;&#23384;&#22312;&#21069;&#21521;&#26799;&#24230;&#21644;&#21518;&#21521;&#26799;&#24230;&#20004;&#20010;&#26799;&#24230;&#39033;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20462;&#27491;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00348</link><description>&lt;p&gt;
ODICE: &#36890;&#36807;&#27491;&#20132;&#26799;&#24230;&#26356;&#26032;&#25581;&#31034;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00348
&lt;/p&gt;
&lt;p&gt;
ODICE&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#37325;&#35201;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#20351;&#29992;&#30495;&#26799;&#24230;&#26356;&#26032;&#23398;&#20064;&#20540;&#20989;&#25968;&#26102;&#23384;&#22312;&#21069;&#21521;&#26799;&#24230;&#21644;&#21518;&#21521;&#26799;&#24230;&#20004;&#20010;&#26799;&#24230;&#39033;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20462;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#20013;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#22522;&#20110;DICE&#30340;&#26041;&#27861;&#23545;&#29366;&#24577;&#34892;&#20026;&#32423;&#21035;&#30340;&#34892;&#20026;&#32422;&#26463;&#26045;&#21152;&#20102;&#65292;&#36825;&#23545;&#20110;&#31163;&#32447;&#23398;&#20064;&#26159;&#19968;&#20010;&#29702;&#24819;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#27604;&#20165;&#20351;&#29992;&#21160;&#20316;&#32423;&#21035;&#34892;&#20026;&#32422;&#26463;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;&#22312;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;DICE&#30340;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20351;&#29992;&#30495;&#26799;&#24230;&#26356;&#26032;&#23398;&#20064;&#20540;&#20989;&#25968;&#26102;&#23384;&#22312;&#20004;&#20010;&#26799;&#24230;&#39033;&#65306;&#21069;&#21521;&#26799;&#24230;&#65288;&#22312;&#24403;&#21069;&#29366;&#24577;&#19978;&#65289;&#21644;&#21518;&#21521;&#26799;&#24230;&#65288;&#22312;&#19979;&#19968;&#20010;&#29366;&#24577;&#19978;&#65289;&#12290;&#20351;&#29992;&#21069;&#21521;&#26799;&#24230;&#19982;&#35768;&#22810;&#31163;&#32447;RL&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#22240;&#27492;&#21487;&#20197;&#34987;&#35270;&#20026;&#24212;&#29992;&#21160;&#20316;&#32423;&#21035;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20004;&#20010;&#26799;&#24230;&#26377;&#30456;&#20114;&#20914;&#31361;&#30340;&#26041;&#21521;&#65292;&#30452;&#25509;&#21152;&#19978;&#21518;&#21521;&#26799;&#24230;&#21487;&#33021;&#20250;&#36864;&#21270;&#25110;&#25269;&#28040;&#20854;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20462;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#21644;&#21327;&#21516;&#33258;&#21160;&#39550;&#39542;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#20132;&#21449;&#21475;&#39542;&#20837;&#39034;&#24207;&#30340;&#23436;&#25972;&#31639;&#27861;OBS-KATS&#65292;&#35813;&#31639;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.00334</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#29992;&#20110;&#21327;&#21516;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Path Finding for Cooperative Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#21644;&#21327;&#21516;&#33258;&#21160;&#39550;&#39542;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#20132;&#21449;&#21475;&#39542;&#20837;&#39034;&#24207;&#30340;&#23436;&#25972;&#31639;&#27861;OBS-KATS&#65292;&#35813;&#31639;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#32593;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;CAV&#65289;&#21487;&#33021;&#22312;&#26410;&#26469;&#30340;&#37096;&#32626;&#65292;&#25511;&#21046;&#29702;&#35770;&#21644;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#30340;&#35768;&#22810;&#30740;&#31350;&#23545;&#20132;&#21449;&#21475;&#30340;&#21327;&#21516;&#33258;&#21160;&#39550;&#39542;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26368;&#36817;&#24182;&#34892;&#24037;&#20316;&#20013;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#23613;&#31649;&#29615;&#22659;&#20855;&#26377;&#31616;&#21270;&#30340;&#36816;&#21160;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;MAPF&#30340;&#27934;&#23519;&#21644;&#31639;&#27861;&#19982;&#20248;&#21270;CAV&#22312;&#26080;&#20449;&#21495;&#20132;&#21449;&#21475;&#30340;&#20132;&#21449;&#39034;&#24207;&#30340;&#32467;&#26500;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#28151;&#21512;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20248;&#21270;&#21644;&#23436;&#25972;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#39034;&#24207;&#30340;&#25628;&#32034;&#19982;&#36816;&#21160;&#23398;&#21040;&#36798;&#26102;&#38388;&#35843;&#24230;&#65288;OBS-KATS&#65289;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#31639;&#27861;&#12289;&#22266;&#23450;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#20248;&#20808;&#35745;&#21010;&#19982;KATS&#12290;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#19981;&#21516;&#30340;&#36710;&#36742;&#21040;&#36798;&#29575;&#12289;&#36710;&#36947;&#38271;&#24230;&#12289;&#20132;&#21449;&#36895;&#24230;&#21644;&#25511;&#21046;&#33539;&#22260;&#19979;&#37117;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;&#36890;&#36807;&#28040;&#34701;&#21644;&#35299;&#21078;&#20998;&#26512;&#65292;&#25105;&#20204;&#23545;OBS-KATS&#30340;&#36129;&#29486;&#22240;&#32032;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating possible future deployment of connected and automated vehicles (CAVs), cooperative autonomous driving at intersections has been studied by many works in control theory and intelligent transportation across decades. Simultaneously, recent parallel works in robotics have devised efficient algorithms for multi-agent path finding (MAPF), though often in environments with simplified kinematics. In this work, we hybridize insights and algorithms from MAPF with the structure and heuristics of optimizing the crossing order of CAVs at signal-free intersections. We devise an optimal and complete algorithm, Order-based Search with Kinematics Arrival Time Scheduling (OBS-KATS), which significantly outperforms existing algorithms, fixed heuristics, and prioritized planning with KATS. The performance is maintained under different vehicle arrival rates, lane lengths, crossing speeds, and control horizon. Through ablations and dissections, we offer insight on the contributing factors to O
&lt;/p&gt;</description></item><item><title>SCO-VIST&#26159;&#19968;&#20010;&#22522;&#20110;&#31038;&#20132;&#20114;&#21160;&#24120;&#35782;&#30340;&#35270;&#35273;&#25925;&#20107;&#21019;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#34920;&#31034;&#22270;&#24418;&#21644;&#20351;&#29992;&#21152;&#26435;&#25925;&#20107;&#22270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#12289;&#24341;&#20154;&#20837;&#32988;&#30340;&#35270;&#35273;&#25925;&#20107;&#12290;</title><link>https://arxiv.org/abs/2402.00319</link><description>&lt;p&gt;
SCO-VIST: &#22522;&#20110;&#31038;&#20132;&#20114;&#21160;&#24120;&#35782;&#30340;&#35270;&#35273;&#25925;&#20107;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00319
&lt;/p&gt;
&lt;p&gt;
SCO-VIST&#26159;&#19968;&#20010;&#22522;&#20110;&#31038;&#20132;&#20114;&#21160;&#24120;&#35782;&#30340;&#35270;&#35273;&#25925;&#20107;&#21019;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#34920;&#31034;&#22270;&#24418;&#21644;&#20351;&#29992;&#21152;&#26435;&#25925;&#20107;&#22270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#12289;&#24341;&#20154;&#20837;&#32988;&#30340;&#35270;&#35273;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25925;&#20107;&#21019;&#20316;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#22270;&#20687;&#24207;&#21015;&#33258;&#21160;&#29983;&#25104;&#19968;&#20010;&#36830;&#36143;&#30340;&#25925;&#20107;&#12290;&#19982;&#22270;&#20687;&#25551;&#36848;&#20219;&#21153;&#19981;&#21516;&#65292;&#35270;&#35273;&#25925;&#20107;&#24212;&#21253;&#21547;&#20107;&#23454;&#25551;&#36848;&#12289;&#19990;&#30028;&#35266;&#21644;&#20154;&#31867;&#31038;&#20132;&#24120;&#35782;&#65292;&#23558;&#38646;&#25955;&#30340;&#20803;&#32032;&#32452;&#21512;&#25104;&#19968;&#20010;&#36830;&#36143;&#32780;&#24341;&#20154;&#20837;&#32988;&#30340;&#20154;&#24037;&#32534;&#20889;&#25925;&#20107;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20027;&#35201;&#20391;&#37325;&#20110;&#24212;&#29992;&#20107;&#23454;&#20449;&#24687;&#21644;&#20351;&#29992;&#20998;&#31867;/&#35789;&#27719;&#22806;&#37096;&#30693;&#35782;&#26469;&#21019;&#24314;&#25925;&#20107;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SCO-VIST&#65292;&#19968;&#31181;&#23558;&#22270;&#20687;&#24207;&#21015;&#34920;&#31034;&#20026;&#23545;&#35937;&#21644;&#20851;&#31995;&#30340;&#22270;&#24418;&#26694;&#26550;&#65292;&#21253;&#25324;&#20154;&#31867;&#34892;&#20026;&#21160;&#26426;&#21450;&#20854;&#31038;&#20132;&#20114;&#21160;&#24120;&#35782;&#30693;&#35782;&#12290;SCO-VIST&#28982;&#21518;&#23558;&#36825;&#20010;&#34920;&#31034;&#24773;&#33410;&#28857;&#30340;&#22270;&#24418;&#19982;&#35821;&#20041;&#21644;&#22522;&#20110;&#20107;&#20214;&#21457;&#29983;&#30340;&#36793;&#32536;&#26435;&#37325;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;&#36825;&#20010;&#21152;&#26435;&#25925;&#20107;&#22270;&#20351;&#29992;Floyd-Warshall&#31639;&#27861;&#20197;&#20107;&#20214;&#24207;&#21015;&#24418;&#24335;&#20135;&#29983;&#25925;&#20107;&#24773;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20135;&#29983;&#20986;&#33394;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual storytelling aims to automatically generate a coherent story based on a given image sequence. Unlike tasks like image captioning, visual stories should contain factual descriptions, worldviews, and human social commonsense to put disjointed elements together to form a coherent and engaging human-writeable story. However, most models mainly focus on applying factual information and using taxonomic/lexical external knowledge when attempting to create stories. This paper introduces SCO-VIST, a framework representing the image sequence as a graph with objects and relations that includes human action motivation and its social interaction commonsense knowledge. SCO-VIST then takes this graph representing plot points and creates bridges between plot points with semantic and occurrence-based edge weights. This weighted story graph produces the storyline in a sequence of events using Floyd-Warshall's algorithm. Our proposed framework produces stories superior across multiple metrics in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;AI&#36171;&#33021;&#30340;&#21512;&#25104;&#29983;&#29289;&#23398;&#38754;&#20020;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26032;&#20852;&#26694;&#26550;&#65292;&#32467;&#21512;&#21019;&#26032;&#21644;&#29983;&#29289;&#23433;&#20840;&#30340;&#20849;&#21516;&#30446;&#26631;&#65292;&#25506;&#35752;&#20102;&#21629;&#20196;&#19982;&#25511;&#21046;&#12289;&#31649;&#25252;&#12289;&#33258;&#24213;&#21521;&#19978;&#21644;&#33258;&#30001;&#25918;&#20219;&#31561;&#27835;&#29702;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.00312</link><description>&lt;p&gt;
AI&#36171;&#33021;&#21512;&#25104;&#29983;&#29289;&#23398;&#20013;&#30340;&#25171;&#22320;&#40736;&#27835;&#29702;&#25361;&#25112;&#65306;&#25991;&#29486;&#32508;&#36848;&#19982;&#26032;&#20852;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The whack-a-mole governance challenge for AI-enabled synthetic biology: literature review and emerging frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;AI&#36171;&#33021;&#30340;&#21512;&#25104;&#29983;&#29289;&#23398;&#38754;&#20020;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26032;&#20852;&#26694;&#26550;&#65292;&#32467;&#21512;&#21019;&#26032;&#21644;&#29983;&#29289;&#23433;&#20840;&#30340;&#20849;&#21516;&#30446;&#26631;&#65292;&#25506;&#35752;&#20102;&#21629;&#20196;&#19982;&#25511;&#21046;&#12289;&#31649;&#25252;&#12289;&#33258;&#24213;&#21521;&#19978;&#21644;&#33258;&#30001;&#25918;&#20219;&#31561;&#27835;&#29702;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#36171;&#33021;&#30340;&#21512;&#25104;&#29983;&#29289;&#23398;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#26174;&#33879;&#22686;&#21152;&#20102;&#29983;&#29289;&#39118;&#38505;&#65292;&#24182;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#12290;&#37492;&#20110;&#23558;&#26032;&#20852;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;AI&#36171;&#33021;&#30340;&#21512;&#25104;&#29983;&#29289;&#23398;&#28508;&#22312;&#22320;&#23558;&#29983;&#29289;&#24037;&#31243;&#25193;&#22823;&#21040;&#24037;&#19994;&#29983;&#29289;&#21046;&#36896;&#20043;&#20013;&#65292;&#36825;&#19968;&#23616;&#38754;&#21464;&#24471;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32508;&#36848;&#34920;&#26126;&#65292;&#22914;&#32500;&#25345;&#21512;&#29702;&#30340;&#21019;&#26032;&#33539;&#22260;&#65292;&#25110;&#32773;&#26356;&#26377;&#37326;&#24515;&#22320;&#22521;&#32946;&#24040;&#22823;&#30340;&#29983;&#29289;&#32463;&#27982;&#24182;&#19981;&#19968;&#23450;&#19982;&#29983;&#29289;&#23433;&#20840;&#30456;&#23545;&#31435;&#65292;&#32780;&#26159;&#38656;&#35201;&#30456;&#36741;&#30456;&#25104;&#12290;&#26412;&#25991;&#23545;&#30456;&#20851;&#38382;&#39064;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#25551;&#36848;&#20102;&#27178;&#36328;&#21629;&#20196;&#19982;&#25511;&#21046;&#12289;&#31649;&#25252;&#12289;&#33258;&#24213;&#21521;&#19978;&#21644;&#33258;&#30001;&#25918;&#20219;&#27835;&#29702;&#36873;&#25321;&#30340;&#25919;&#31574;&#21644;&#23454;&#36341;&#26032;&#20852;&#26694;&#26550;&#12290;&#22914;&#20309;&#23454;&#29616;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#20197;&#20415;&#39044;&#38450;&#21644;&#20943;&#36731;AI&#36171;&#33021;&#30340;&#29983;&#29289;&#21361;&#23475;&#65292;&#21253;&#25324;&#26469;&#33258;&#23454;&#39564;&#23460;&#12289;&#24694;&#24847;&#28389;&#29992;&#25110;&#20844;&#20849;&#39046;&#22495;&#65292;&#23558;&#19981;&#26029;&#38656;&#35201;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-enabled synthetic biology has tremendous potential but also significantly increases biorisks and brings about a new set of dual use concerns. The picture is complicated given the vast innovations envisioned to emerge by combining emerging technologies, as AI-enabled synthetic biology potentially scales up bioengineering into industrial biomanufacturing. However, the literature review indicates that goals such as maintaining a reasonable scope for innovation, or more ambitiously to foster a huge bioeconomy don't necessarily contrast with biosafety, but need to go hand in hand. This paper presents a literature review of the issues and describes emerging frameworks for policy and practice that transverse the options of command-and control, stewardship, bottom-up, and laissez-faire governance. How to achieve early warning systems that enable prevention and mitigation of future AI-enabled biohazards from the lab, from deliberate misuse, or from the public realm, will constantly need to e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#33021;&#12289;&#23567;&#22411;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25104;&#21151;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#65292;&#27169;&#22411;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.00306</link><description>&lt;p&gt;
&#19968;&#20010;&#20934;&#30830;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29992;&#20110;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#33021;&#12289;&#23567;&#22411;&#19988;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25104;&#21151;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#65292;&#27169;&#22411;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#39044;&#27979;&#26159;&#19968;&#38376;&#28041;&#21450;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#20301;&#32622;&#30340;&#23398;&#31185;&#12290;&#20854;&#24212;&#29992;&#21253;&#25324;&#36164;&#28304;&#20998;&#37197;&#12289;&#26381;&#21153;&#36136;&#37327;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#20132;&#36890;&#31649;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#33021;&#12289;&#23567;&#22411;&#21644;&#20302;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#20301;&#32622;&#39044;&#27979;&#65292;&#21487;&#37096;&#32626;&#22312;&#26222;&#36890;&#22522;&#31449;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#25972;&#20010;&#22478;&#24066;&#30340;&#23436;&#25972;&#20154;&#21592;&#27969;&#21160;&#27169;&#24335;&#36827;&#34892;&#20102;&#19968;&#30334;&#20010;&#36229;&#21442;&#25968;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#31934;&#30830;&#30340;ML&#26550;&#26500;&#65292;&#20854;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;&#26368;&#23569;&#25968;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24179;&#21488;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#24050;&#21457;&#34920;&#30340;ML&#26550;&#26500;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#20174;2.02&#20159;&#20943;&#23569;&#21040;200&#19975;&#12290;&#36825;&#23558;&#27169;&#22411;&#21442;&#25968;&#30340;&#24635;&#22823;&#23567;&#20174;791 MB&#20943;&#23569;&#21040;8 MB&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#22235;&#20493;&#65292;&#35757;&#32451;&#25152;&#38656;&#30340;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#20869;&#23384;&#37327;&#20063;&#20943;&#23569;&#20102;&#19968;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PAP-REC&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#25552;&#31034;&#26631;&#35760;&#26469;&#20943;&#36731;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#25152;&#24102;&#26469;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00284</link><description>&lt;p&gt;
PAP-REC: &#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PAP-REC: Personalized Automatic Prompt for Recommendation Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PAP-REC&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#25552;&#31034;&#26631;&#35760;&#26469;&#20943;&#36731;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#25152;&#24102;&#26469;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#21487;&#20197;&#32479;&#19968;&#35299;&#20915;&#22810;&#20010;&#25512;&#33616;&#20219;&#21153;&#12290;&#36825;&#20123;RLM&#20805;&#20998;&#21033;&#29992;&#20102;&#20174;&#20016;&#23500;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#36951;&#20256;&#30693;&#35782;&#65292;&#36890;&#36807;&#25552;&#31034;&#26469;&#35299;&#20915;&#19979;&#28216;&#25512;&#33616;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#24341;&#20837;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#32593;&#32476;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25163;&#24037;&#35774;&#35745;&#30340;&#25552;&#31034;&#38656;&#35201;&#26174;&#33879;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#20154;&#21147;&#25237;&#20837;&#65292;&#31245;&#24494;&#25913;&#20889;&#25552;&#31034;&#23601;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#30340;&#24040;&#22823;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PAP-REC&#65292;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#30340;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20302;&#25928;&#29575;&#21644;&#20302;&#25928;&#26524;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#20801;&#35768;&#19981;&#21516;&#30340;&#29992;&#25143;&#22312;&#30456;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#36825;&#20123;&#26631;&#35760;&#26159;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;&#20010;&#24615;&#21270;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged prompt-based Recommendation Language Models (RLM) can solve multiple recommendation tasks uniformly. The RLMs make full use of the inherited knowledge learned from the abundant pre-training data to solve the downstream recommendation tasks by prompts, without introducing additional parameters or network training. However, handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes. In this paper, we propose PAP-REC, a framework to generate the Personalized Automatic Prompt for RECommendation language models to mitigate the inefficiency and ineffectiveness problems derived from manually designed prompts. Specifically, personalized automatic prompts allow different users to have different prompt tokens for the same task, automatically generated using a gradient-based method. One challenge for personalized automatic prompt generation for recommendation language models is the extremely large sear
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#21644;&#23637;&#26395;&#20171;&#32461;&#20102;&#23558;&#35745;&#31639;&#23454;&#39564;&#19982;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#35299;&#20915;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#20855;&#26377;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00262</link><description>&lt;p&gt;
&#35745;&#31639;&#23454;&#39564;&#35777;&#26126;&#36935;&#21040;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#39033;&#35843;&#26597;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#21644;&#23637;&#26395;&#20171;&#32461;&#20102;&#23558;&#35745;&#31639;&#23454;&#39564;&#19982;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#35299;&#20915;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#20855;&#26377;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#23454;&#39564;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#22797;&#26434;&#31995;&#32479;&#30340;&#26377;&#20215;&#20540;&#26041;&#27861;&#65292;&#28041;&#21450;&#23545;&#21453;&#20107;&#23454;&#24773;&#20917;&#30340;&#31639;&#27861;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#29305;&#24615;&#65288;&#21253;&#25324;&#26377;&#38480;&#29702;&#24615;&#21644;&#24322;&#36136;&#24615;&#65289;&#65292;&#20934;&#30830;&#22320;&#34920;&#31034;&#30495;&#23454;&#31038;&#20250;&#31995;&#32479;&#22312;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#24314;&#27169;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20855;&#22791;&#20154;&#31867;&#29305;&#24449;&#65292;&#27604;&#22914;&#22797;&#26434;&#25512;&#29702;&#21644;&#33258;&#20027;&#23398;&#20064;&#33021;&#21147;&#12290;&#36825;&#20123;&#34987;&#31216;&#20026;LLM-based Agent&#30340;&#26234;&#33021;&#20307;&#26377;&#28508;&#21147;&#22686;&#24378;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20154;&#31867;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;LLM&#20013;&#32570;&#20047;&#26126;&#30830;&#35299;&#37322;&#24615;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#35745;&#31639;&#23454;&#39564;&#22312;&#25552;&#20379;&#20010;&#20307;&#34892;&#20026;&#21644;&#22797;&#26434;&#29616;&#35937;&#30340;&#22240;&#26524;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#12290;&#22240;&#27492;&#65292;&#23558;&#35745;&#31639;&#23454;&#39564;&#19982;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#30456;&#32467;&#21512;&#20855;&#26377;&#37325;&#22823;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational experiments have emerged as a valuable method for studying complex systems, involving the algorithmization of counterfactuals. However, accurately representing real social systems in Agent-based Modeling (ABM) is challenging due to the diverse and intricate characteristics of humans, including bounded rationality and heterogeneity. To address this limitation, the integration of Large Language Models (LLMs) has been proposed, enabling agents to possess anthropomorphic abilities such as complex reasoning and autonomous learning. These agents, known as LLM-based Agent, offer the potential to enhance the anthropomorphism lacking in ABM. Nonetheless, the absence of explicit explainability in LLMs significantly hinders their application in the social sciences. Conversely, computational experiments excel in providing causal analysis of individual behaviors and complex phenomena. Thus, combining computational experiments with LLM-based Agent holds substantial research potential. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00260</link><description>&lt;p&gt;
&#20197;LLM&#20026;&#22522;&#30784;&#23454;&#29616;&#38754;&#21521;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#30340;&#21487;&#25193;&#23637;&#26426;&#22120;&#20154;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#20132;&#27969;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Large Language Model (LLM)&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#31038;&#20132;&#26426;&#22120;&#20154;NAO&#25198;&#28436;&#20102;&#19968;&#20010;&#21050;&#28608;&#22120;(&#21475;&#22836;&#25551;&#36848;&#31038;&#20132;&#24773;&#26223;&#24182;&#25552;&#38382;)&#12289;&#25552;&#31034;&#22120;(&#25552;&#20379;&#19977;&#20010;&#36873;&#25321;&#39033;&#20379;&#36873;&#25321;)&#21644;&#22870;&#21169;&#22120;(&#24403;&#31572;&#26696;&#27491;&#30830;&#26102;&#32473;&#20104;&#31216;&#36190;)&#30340;&#35282;&#33394;&#12290;&#23545;&#20110;&#21050;&#28608;&#22120;&#30340;&#35282;&#33394;&#65292;&#31038;&#20132;&#24773;&#22659;&#12289;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26159;&#20351;&#29992;&#25105;&#20204;&#30340;LLM&#31649;&#36947;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;GPT-2 + BART&#21644;GPT-2 + GPT-2&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;GPT-2&#22312;&#31649;&#36947;&#20013;&#26159;&#29992;&#20110;&#26080;&#30417;&#30563;&#31038;&#20132;&#24773;&#22659;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;SOCIALIQA&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;LLM&#31649;&#36947;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-2 + BART&#31649;&#36947;&#22312;&#36890;&#36807;&#32467;&#21512;&#21508;&#33258;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;BERTscore&#12290;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#20063;&#19982;&#20799;&#31461;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27700;&#24179;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a social robot capable of verbally interacting with children with Autism Spectrum Disorder (ASD). This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a social situation and asks a question), prompter (presents three options to choose from), and reinforcer (praises when the answer is correct). For the role of the stimulator, the social situation, questions, and options are generated using our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 + GPT-2, where the first GPT-2 common between the pipelines is used for unsupervised social situation generation. We use the SOCIALIQA dataset to fine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had a better BERTscore for generating the questions and the options by combining their individual loss functions. This observation was also consistent with the h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#30340;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;&#65288;VSR-DPG&#65289;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#21457;&#29616;&#28041;&#21450;&#22810;&#20010;&#29420;&#31435;&#21464;&#37327;&#30340;&#31526;&#21495;&#26041;&#31243;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#21644;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.00254</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#36827;&#34892;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Vertical Symbolic Regression via Deep Policy Gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00254
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#30340;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;&#65288;VSR-DPG&#65289;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#21457;&#29616;&#28041;&#21450;&#22810;&#20010;&#29420;&#31435;&#21464;&#37327;&#30340;&#31526;&#21495;&#26041;&#31243;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#21644;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;&#65288;VSR&#65289;&#65292;&#29992;&#20110;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#24555;&#36895;&#21457;&#29616;&#20855;&#26377;&#22810;&#20010;&#29420;&#31435;&#21464;&#37327;&#30340;&#31526;&#21495;&#26041;&#31243;&#12290;VSR&#36890;&#36807;&#26500;&#24314;&#30001;&#28041;&#21450;&#19968;&#37096;&#20998;&#29420;&#31435;&#21464;&#37327;&#30340;&#31616;&#21270;&#24418;&#24335;&#26041;&#31243;&#21040;&#23436;&#25972;&#26041;&#31243;&#30340;&#22402;&#30452;&#21457;&#29616;&#36335;&#24452;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35768;&#22810;&#31526;&#21495;&#22238;&#24402;&#22120;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#65292;&#39044;&#35745;&#33021;&#36827;&#19968;&#27493;&#25193;&#22823;VSR&#30340;&#35268;&#27169;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;VSR&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#23558;&#23548;&#33268;&#26799;&#24230;&#20256;&#36882;&#22256;&#38590;&#21644;&#20854;&#20182;&#24037;&#31243;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#31574;&#30053;&#26799;&#24230;&#30340;&#22402;&#30452;&#31526;&#21495;&#22238;&#24402;&#65288;VSR-DPG&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;VSR-DPG&#21487;&#20197;&#24674;&#22797;&#28041;&#21450;&#22810;&#20010;&#36755;&#20837;&#21464;&#37327;&#30340;&#30495;&#23454;&#26041;&#31243;&#65292;&#26174;&#33879;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#20808;&#21069;&#30340;VSR&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;VSR-DPG&#23558;&#31526;&#21495;&#22238;&#24402;&#24314;&#27169;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#26041;&#31243;&#26159;&#36890;&#36807;&#22810;&#27425;&#24212;&#29992;&#26469;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Symbolic Regression (VSR) recently has been proposed to expedite the discovery of symbolic equations with many independent variables from experimental data. VSR reduces the search spaces following the vertical discovery path by building from reduced-form equations involving a subset of independent variables to full-fledged ones. Proved successful by many symbolic regressors, deep neural networks are expected to further scale up VSR. Nevertheless, directly combining VSR with deep neural networks will result in difficulty in passing gradients and other engineering issues. We propose Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and demonstrate that VSR-DPG can recover ground-truth equations involving multiple input variables, significantly beyond both deep reinforcement learning-based approaches and previous VSR variants. Our VSR-DPG models symbolic regression as a sequential decision-making process, in which equations are built from repeated applications of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20915;&#31574;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#36755;&#20837;-&#20915;&#31574;&#20043;&#38388;&#30340;&#36880;&#28857;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#19978;&#23545;&#20915;&#31574;&#21487;&#20449;&#24230;&#30340;&#35299;&#37322;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20915;&#31574;&#20195;&#29702;&#35774;&#35745;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#21160;&#20316;&#65292;&#24182;&#22312;&#23384;&#22312;&#22810;&#20010;&#39640;&#20272;&#35745;&#36880;&#28857;&#20381;&#36182;&#24615;&#30340;&#21160;&#20316;&#26102;&#35201;&#27714;&#29992;&#25143;&#25552;&#20379;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.00251</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#38750;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20915;&#31574;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20915;&#31574;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#36755;&#20837;-&#20915;&#31574;&#20043;&#38388;&#30340;&#36880;&#28857;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#19978;&#23545;&#20915;&#31574;&#21487;&#20449;&#24230;&#30340;&#35299;&#37322;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20915;&#31574;&#20195;&#29702;&#35774;&#35745;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#21160;&#20316;&#65292;&#24182;&#22312;&#23384;&#22312;&#22810;&#20010;&#39640;&#20272;&#35745;&#36880;&#28857;&#20381;&#36182;&#24615;&#30340;&#21160;&#20316;&#26102;&#35201;&#27714;&#29992;&#25143;&#25552;&#20379;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36880;&#27493;&#20915;&#31574;&#35268;&#21010;&#22312;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#21457;&#23637;&#20013;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#20915;&#31574;&#35268;&#21010;&#65292;&#20197;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#26159;&#30333;&#30418;&#26041;&#27861;&#65292;&#35201;&#20040;&#26159;&#35745;&#31639;&#22797;&#26434;&#65292;&#38480;&#21046;&#20102;&#40657;&#30418;&#19987;&#26377;LLMs&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;LLMs&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#19968;&#25512;&#29702;&#26377;&#25928;&#22320;&#20272;&#35745;&#36755;&#20837;-&#20915;&#31574;&#20043;&#38388;&#30340;&#36880;&#28857;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20196;&#29260;logits&#12290;&#35813;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#23545;&#20915;&#31574;&#21487;&#20449;&#24230;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20915;&#31574;&#20195;&#29702;&#35774;&#35745;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#31034;&#22914;&#8220;&#25171;&#24320;&#28020;&#23460;&#28783;&#8221;&#65292;&#29983;&#25104;&#21160;&#20316;&#12290;&#24403;&#26377;&#22810;&#20010;&#21160;&#20316;&#30340;&#20272;&#35745;&#36880;&#28857;&#20381;&#36182;&#24615;&#37117;&#24456;&#39640;&#26102;&#65292;&#29992;&#25143;&#23558;&#34987;&#35201;&#27714;&#25552;&#20379;&#20559;&#22909;&#12290;&#24635;&#32467;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26410;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20915;&#31574;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our un
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00234</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#33021;&#21542;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Generative AI systems Capable of Supporting Information Needs of Patients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00234
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#26377;&#22797;&#26434;&#30142;&#30149;&#22914;&#30284;&#30151;&#30340;&#24739;&#32773;&#38754;&#20020;&#22797;&#26434;&#30340;&#20449;&#24687;&#25361;&#25112;&#65292;&#20182;&#20204;&#19981;&#20165;&#38656;&#35201;&#20102;&#35299;&#20182;&#20204;&#30340;&#30142;&#30149;&#65292;&#36824;&#38656;&#35201;&#23398;&#20250;&#22914;&#20309;&#31649;&#29702;&#23427;&#12290;&#19982;&#21307;&#30103;&#19987;&#23478;&#65288;&#25918;&#23556;&#31185;&#21307;&#24072;&#12289;&#32959;&#30244;&#31185;&#21307;&#24072;&#65289;&#23494;&#20999;&#20114;&#21160;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#30142;&#30149;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#19988;&#21344;&#29992;&#20102;&#19987;&#23478;&#30340;&#26102;&#38388;&#65292;&#20351;&#20182;&#20204;&#26080;&#27861;&#23436;&#25104;&#20854;&#20182;&#20851;&#38190;&#20219;&#21153;&#12290;&#37492;&#20110;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#25913;&#36827;&#21307;&#30103;&#31995;&#32479;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#22312;&#25918;&#23556;&#23398;&#25104;&#20687;&#25968;&#25454;&#32972;&#26223;&#19979;&#22914;&#20309;&#36127;&#36131;&#20219;&#22320;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#38656;&#27714;&#21457;&#29616;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#35752;&#35770;&#20102;&#19968;&#20010;&#34394;&#26500;&#36817;&#20146;&#30340;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#30340;&#20027;&#39064;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#34701;&#20837;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#23454;&#20363;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#23545;&#27604;&#65292;&#21019;&#24314;&#19968;&#20010;&#26356;&#20026;&#33391;&#22909;&#32467;&#26500;&#21270;&#21644;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.00232</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Label Hierarchy with Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#34701;&#20837;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#23454;&#20363;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#23545;&#27604;&#65292;&#21019;&#24314;&#19968;&#20010;&#26356;&#20026;&#33391;&#22909;&#32467;&#26500;&#21270;&#21644;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SCL&#65289;&#26694;&#26550;&#23558;&#27599;&#20010;&#31867;&#21035;&#35270;&#20026;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#35748;&#20026;&#25152;&#26377;&#31867;&#21035;&#21516;&#31561;&#37325;&#35201;&#12290;&#36825;&#24573;&#30053;&#20102;&#26631;&#31614;&#23618;&#27425;&#23384;&#22312;&#30340;&#19968;&#33324;&#24773;&#20917;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#19979;&#30340;&#32454;&#31890;&#24230;&#31867;&#21035;&#20043;&#38388;&#27604;&#38750;&#24120;&#19981;&#21516;&#30340;&#31867;&#21035;&#26356;&#30456;&#20284;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26631;&#31614;&#24863;&#30693;&#30340;SCL&#26041;&#27861;&#65288;LASCL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#23558;&#23618;&#27425;&#20449;&#24687;&#34701;&#20837;SCL&#20013;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#26356;&#20026;&#33391;&#22909;&#32467;&#26500;&#21270;&#21644;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#36825;&#26159;&#36890;&#36807;&#20808;&#26681;&#25454;&#31867;&#21035;&#20043;&#38388;&#30340;&#25509;&#36817;&#31243;&#24230;&#35843;&#25972;&#23454;&#20363;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#30340;&#65292;&#36890;&#36807;&#32553;&#25918;&#30340;&#23454;&#20363;-&#23454;&#20363;&#23545;&#27604;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#23454;&#20363;-&#20013;&#24515;&#23545;&#27604;&#65292;&#23558;&#21516;&#31867;&#21035;&#30340;&#31034;&#20363;&#31227;&#21160;&#21040;&#23427;&#20204;&#30340;&#20013;&#24515;&#38468;&#36817;&#65292;&#20013;&#24515;&#30001;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#26631;&#31614;&#21442;&#25968;&#34920;&#31034;&#12290;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#21442;&#25968;&#21487;&#20197;&#30452;&#25509;&#29992;&#20316;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised contrastive learning (SCL) frameworks treat each class as independent and thus consider all classes to be equally important. This neglects the common scenario in which label hierarchy exists, where fine-grained classes under the same category show more similarity than very different ones. This paper introduces a family of Label-Aware SCL methods (LASCL) that incorporates hierarchical information to SCL by leveraging similarities between classes, resulting in creating a more well-structured and discriminative feature space. This is achieved by first adjusting the distance between instances based on measures of the proximity of their classes with the scaled instance-instance-wise contrastive. An additional instance-center-wise contrastive is introduced to move within-class examples closer to their centers, which are represented by a set of learnable label parameters. The learned label parameters can be directly used as a nearest neighbor classifier without further finetuning. 
&lt;/p&gt;</description></item><item><title>FedCore&#26159;&#19968;&#31181;&#36890;&#36807;&#20998;&#24067;&#24335;&#36873;&#25321;&#26680;&#24515;&#38598;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24930;&#36895;&#23458;&#25143;&#31471;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2402.00219</link><description>&lt;p&gt;
FedCore: &#20351;&#29992;&#20998;&#24067;&#24335;&#26680;&#24515;&#38598;&#35299;&#20915;&#26080;&#25302;&#36710;&#29616;&#35937;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedCore: Straggler-Free Federated Learning with Distributed Coresets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00219
&lt;/p&gt;
&lt;p&gt;
FedCore&#26159;&#19968;&#31181;&#36890;&#36807;&#20998;&#24067;&#24335;&#36873;&#25321;&#26680;&#24515;&#38598;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24930;&#36895;&#23458;&#25143;&#31471;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#20445;&#30041;&#33258;&#24049;&#30340;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24930;&#36895;&#23458;&#25143;&#31471;&#65292;&#25302;&#36710;&#29616;&#35937;&#32463;&#24120;&#24433;&#21709;FL&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedCore&#65292;&#19968;&#31181;&#36890;&#36807;&#20998;&#24067;&#24335;&#36873;&#25321;&#26680;&#24515;&#38598;&#65288;&#25968;&#25454;&#38598;&#30340;&#20195;&#34920;&#23376;&#38598;&#65289;&#21019;&#26032;&#22320;&#35299;&#20915;&#25302;&#36710;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#26680;&#24515;&#38598;&#26041;&#27861;&#19981;&#21516;&#65292;FedCore&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#30452;&#25509;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#21019;&#24314;&#26680;&#24515;&#38598;&#65292;&#30830;&#20445;&#22312;FL&#20013;&#20445;&#25252;&#38544;&#31169;&#12290;FedCore&#23558;&#26680;&#24515;&#38598;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;k-medoids&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#25805;&#20316;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#23454;&#20102;FedCore&#30340;&#25910;&#25947;&#24615;&#65292;&#23454;&#38469;&#35780;&#20272;&#26174;&#31034;FL&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;8&#20493;&#65292;&#32780;&#27169;&#22411;&#20934;&#30830;&#24615;&#27809;&#26377;&#38477;&#20302;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#36824;&#34920;&#26126;&#65292;FedCore&#23545;&#29616;&#26377;&#30340;FL&#26694;&#26550;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model while keeping their data on-premise. However, the straggler issue, due to slow clients, often hinders the efficiency and scalability of FL. This paper presents FedCore, an algorithm that innovatively tackles the straggler problem via the decentralized selection of coresets, representative subsets of a dataset. Contrary to existing centralized coreset methods, FedCore creates coresets directly on each client in a distributed manner, ensuring privacy preservation in FL. FedCore translates the coreset optimization problem into a more tractable k-medoids clustering problem and operates distributedly on each client. Theoretical analysis confirms FedCore's convergence, and practical evaluations demonstrate an 8x reduction in FL training time, without compromising model accuracy. Our extensive evaluations also show that FedCore generalizes well to existing FL frameworks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#25552;&#39640;&#30340;&#30446;&#26631;&#26816;&#27979;&#20934;&#30830;&#24615;&#22312;&#36275;&#29699;&#27604;&#36187;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.00163</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#25552;&#39640;&#36275;&#29699;&#30446;&#26631;&#26816;&#27979;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving Object Detection Quality in Football Through Super-Resolution Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00163
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#25552;&#39640;&#30340;&#30446;&#26631;&#26816;&#27979;&#20934;&#30830;&#24615;&#22312;&#36275;&#29699;&#27604;&#36187;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36275;&#29699;&#27604;&#36187;&#20013;&#21033;&#29992;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#12290;&#32771;&#34385;&#21040;&#36275;&#29699;&#27604;&#36187;&#30340;&#24555;&#33410;&#22863;&#20197;&#21450;&#31934;&#30830;&#29289;&#20307;&#65288;&#22914;&#29699;&#12289;&#29699;&#21592;&#65289;&#36319;&#36394;&#23545;&#20110;&#20998;&#26512;&#21644;&#24191;&#25773;&#30340;&#37325;&#35201;&#24615;&#65292;&#36229;&#20998;&#36776;&#29575;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#36229;&#20998;&#36776;&#29575;&#30340;&#20808;&#36827;&#22270;&#20687;&#22788;&#29702;&#23545;&#22788;&#29702;&#36275;&#29699;&#27604;&#36187;&#24405;&#20687;&#20013;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23558;&#26368;&#20808;&#36827;&#30340;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#24212;&#29992;&#20110;&#26469;&#33258;SoccerNet&#30340;&#22810;&#26679;&#21270;&#30340;&#36275;&#29699;&#27604;&#36187;&#24405;&#20687;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;Faster R-CNN&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#24212;&#29992;&#36229;&#20998;&#36776;&#29575;&#39044;&#22788;&#29702;&#26102;&#65292;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the potential of super-resolution techniques in enhancing object detection accuracy in football. Given the sport's fast-paced nature and the critical importance of precise object (e.g. ball, player) tracking for both analysis and broadcasting, super-resolution could offer significant improvements. We investigate how advanced image processing through super-resolution impacts the accuracy and reliability of object detection algorithms in processing football match footage.   Our methodology involved applying state-of-the-art super-resolution techniques to a diverse set of football match videos from SoccerNet, followed by object detection using Faster R-CNN. The performance of these algorithms, both with and without super-resolution enhancement, was rigorously evaluated in terms of detection accuracy.   The results indicate a marked improvement in object detection accuracy when super-resolution preprocessing is applied. The improvement of object detection through the in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20248;&#21270;&#35774;&#32622;&#29992;&#20110;&#22312;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#31169;&#26377;&#30340;&#32452;&#20214;&#20989;&#25968;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#21644;&#38598;&#20013;&#32858;&#21512;&#30340;&#26041;&#24335;&#26469;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00138</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Decomposable Submodular Maximization in Federated Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20248;&#21270;&#35774;&#32622;&#29992;&#20110;&#22312;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#31169;&#26377;&#30340;&#32452;&#20214;&#20989;&#25968;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#21644;&#38598;&#20013;&#32858;&#21512;&#30340;&#26041;&#24335;&#26469;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#31119;&#21033;&#26368;&#22823;&#21270;&#31561;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;&#23376;&#27169;&#20989;&#25968;&#20197;&#21450;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#21450;&#20854;&#20248;&#21270;&#38382;&#39064;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#32452;&#20998;&#20989;&#25968;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#32452;&#20998;&#20989;&#25968;&#21487;&#33021;&#26159;&#31169;&#26377;&#30340;&#65288;&#20363;&#22914;&#21487;&#33021;&#34920;&#31034;&#29992;&#25143;&#20559;&#22909;&#20989;&#25968;&#65289;&#65292;&#19981;&#33021;&#24191;&#27867;&#20849;&#20139;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#30340;&#8220;&#32852;&#37030;&#20248;&#21270;&#8221;&#35774;&#32622;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#33258;&#24049;&#30340;&#20559;&#22909;&#20989;&#25968;&#65292;&#38656;&#35201;&#26368;&#22823;&#21270;&#36825;&#20123;&#20559;&#22909;&#30340;&#21152;&#26435;&#21644;&#12290;&#25105;&#20204;&#22312;&#35813;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#27969;&#34892;&#30340;&#8220;&#36830;&#32493;&#36138;&#23146;&#8221;&#31639;&#27861;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20197;&#24182;&#34892;&#30340;&#26041;&#24335;&#26397;&#30528;&#23616;&#37096;&#35299;&#21521;&#21069;&#36808;&#20986;&#23567;&#30340;&#23616;&#37096;&#27493;&#39588;&#65292;&#28982;&#21518;&#23558;&#23616;&#37096;&#21464;&#21270;&#32858;&#21512;&#21040;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#25296;&#26454;&#19978;&#22320;&#38754;&#21453;&#20316;&#29992;&#21147;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#65292;&#20197;&#20943;&#36731;&#22806;&#39592;&#39612;&#29992;&#25143;&#19978;&#21322;&#36523;&#21162;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.00135</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#20197;&#26368;&#23567;&#21270;&#19979;&#32930;&#22806;&#39592;&#39612;&#25296;&#26454;&#19978;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Based Controller to Minimize Forces on the Crutches of a Lower-Limb Exoskeleton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#25296;&#26454;&#19978;&#22320;&#38754;&#21453;&#20316;&#29992;&#21147;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#65292;&#20197;&#20943;&#36731;&#22806;&#39592;&#39612;&#29992;&#25143;&#19978;&#21322;&#36523;&#21162;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#21508;&#37096;&#20998;&#30340;&#21147;&#37327;&#20027;&#35201;&#26469;&#33258;&#19978;&#21322;&#36523;&#30340;&#21162;&#21147;&#65292;&#32780;&#19979;&#21322;&#36523;&#34987;&#35748;&#20026;&#26159;&#34987;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35774;&#35745;&#36816;&#21160;&#25511;&#21046;&#22120;&#26102;&#65292;&#25991;&#29486;&#20013;&#24448;&#24448;&#24573;&#35270;&#20102;&#20351;&#29992;&#32773;&#19978;&#21322;&#36523;&#30340;&#21162;&#21147;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#19968;&#31181;&#36816;&#21160;&#25511;&#21046;&#22120;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#25296;&#26454;&#19978;&#30340;&#22320;&#38754;&#21453;&#20316;&#29992;&#21147;&#65292;&#20197;&#20943;&#36731;&#20351;&#29992;&#32773;&#30340;&#19978;&#21322;&#36523;&#21162;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20154;&#20307;-&#22806;&#39592;&#39612;&#31995;&#32479;&#30340;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#31181;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#40723;&#21169;&#20154;&#20307;-&#22806;&#39592;&#39612;&#31995;&#32479;&#30340;&#21069;&#21521;&#20301;&#31227;&#24182;&#28385;&#36275;&#29289;&#29702;&#26426;&#22120;&#20154;&#30340;&#39044;&#23450;&#32422;&#26463;&#12290;&#25105;&#20204;&#20351;&#29992;MuJoCo&#29289;&#29702;&#27169;&#25311;&#22120;&#21644;&#22810;&#20010;&#23454;&#39564;&#36827;&#34892;&#20102;Proximal Policy Optimization&#65292;&#21363;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metabolic energy consumption of a powered lower-limb exoskeleton user mainly comes from the upper body effort since the lower body is considered to be passive. However, the upper body effort of the users is largely ignored in the literature when designing motion controllers. In this work, we use deep reinforcement learning to develop a locomotion controller that minimizes ground reaction forces (GRF) on crutches. The rationale for minimizing GRF is to reduce the upper body effort of the user. Accordingly, we design a model and a learning framework for a human-exoskeleton system with crutches. We formulate a reward function to encourage the forward displacement of a human-exoskeleton system while satisfying the predetermined constraints of a physical robot. We evaluate our new framework using Proximal Policy Optimization, a state-of-the-art deep reinforcement learning (RL) method, on the MuJoCo physics simulator with different hyperparameters and network architectures over multiple tria
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#31867;&#21035;&#65292;&#20854;&#37319;&#29992;&#22810;&#23618;&#26641;&#29366;&#32467;&#26500;&#30340;&#26550;&#26500;&#24182;&#20351;&#29992;&#38750;&#38463;&#22522;&#31859;&#24503;&#23616;&#37096;&#22495;&#30340;&#25972;&#25968;&#29615;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#20123;DNNs&#26159;&#31283;&#20581;&#30340;&#23545;&#23454;&#20540;&#20989;&#25968;&#21644;&#23454;&#20540;&#24179;&#26041;&#21487;&#31215;&#20989;&#25968;&#30340;&#26222;&#36941;&#36924;&#36817;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.00094</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;: &#38750;&#38463;&#22522;&#31859;&#24503;&#20998;&#26512;&#30340;&#34920;&#36848;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks: A Formulation Via Non-Archimedean Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#31867;&#21035;&#65292;&#20854;&#37319;&#29992;&#22810;&#23618;&#26641;&#29366;&#32467;&#26500;&#30340;&#26550;&#26500;&#24182;&#20351;&#29992;&#38750;&#38463;&#22522;&#31859;&#24503;&#23616;&#37096;&#22495;&#30340;&#25972;&#25968;&#29615;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#20123;DNNs&#26159;&#31283;&#20581;&#30340;&#23545;&#23454;&#20540;&#20989;&#25968;&#21644;&#23454;&#20540;&#24179;&#26041;&#21487;&#31215;&#20989;&#25968;&#30340;&#26222;&#36941;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#37319;&#29992;&#22810;&#23618;&#26641;&#29366;&#32467;&#26500;&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#26550;&#26500;&#20351;&#29992;&#38750;&#38463;&#22522;&#31859;&#24503;&#23616;&#37096;&#22495;&#30340;&#25972;&#25968;&#29615;&#20013;&#30340;&#25968;&#23383;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#20123;&#29615;&#20855;&#26377;&#33258;&#28982;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#31867;&#20284;&#26080;&#38480;&#26681;&#26641;&#12290;&#36825;&#20123;&#29615;&#19978;&#30340;&#33258;&#28982;&#24577;&#23556;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#26377;&#38480;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#26032;&#30340;DNNs&#26159;&#23545;&#22312;&#25152;&#25552;&#21040;&#30340;&#29615;&#19978;&#23450;&#20041;&#30340;&#23454;&#20540;&#20989;&#25968;&#30340;&#31283;&#20581;&#30340;&#26222;&#36941;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;DNNs&#20063;&#26159;&#23545;&#22312;&#21333;&#20301;&#21306;&#38388;&#19978;&#23450;&#20041;&#30340;&#23454;&#20540;&#24179;&#26041;&#21487;&#31215;&#20989;&#25968;&#30340;&#31283;&#20581;&#30340;&#26222;&#36941;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of deep neural networks (DNNs) with multilayered tree-like architectures. The architectures are codified using numbers from the ring of integers of non-Archimdean local fields. These rings have a natural hierarchical organization as infinite rooted trees. Natural morphisms on these rings allow us to construct finite multilayered architectures. The new DNNs are robust universal approximators of real-valued functions defined on the mentioned rings. We also show that the DNNs are robust universal approximators of real-valued square-integrable functions defined in the unit interval.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20998;&#38598;&#35757;&#32451;&#30340;&#20803;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#20123;&#26080;&#20998;&#38598;&#20219;&#21153;&#23545;&#20803;&#23398;&#20064;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20146;&#21644;&#24615;&#26631;&#20934;&#26469;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00092</link><description>&lt;p&gt;
&#26080;&#20998;&#38598;&#20219;&#21153;&#36873;&#25321;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Episodic-free Task Selection for Few-shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20998;&#38598;&#35757;&#32451;&#30340;&#20803;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#20123;&#26080;&#20998;&#38598;&#20219;&#21153;&#23545;&#20803;&#23398;&#20064;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20146;&#21644;&#24615;&#26631;&#20934;&#26469;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#38598;&#35757;&#32451;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#20027;&#27969;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#30340;&#22330;&#26223;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#24448;&#24448;&#21155;&#20110;&#19968;&#20123;&#38750;&#20998;&#38598;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#37051;&#22495;&#20998;&#37327;&#20998;&#26512;&#65288;NCA&#65289;&#65292;&#36825;&#25361;&#25112;&#20102;&#35757;&#32451;&#26465;&#20214;&#24517;&#39035;&#19982;&#27979;&#35797;&#26465;&#20214;&#21305;&#37197;&#30340;&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#20309;&#25628;&#32034;&#26080;&#20998;&#38598;&#20219;&#21153;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25928;&#26524;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20998;&#38598;&#35757;&#32451;&#30340;&#26032;&#22411;&#20803;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20998;&#38598;&#20219;&#21153;&#19981;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#65292;&#32780;&#26159;&#29992;&#20110;&#35780;&#20272;&#20174;&#20219;&#21153;&#38598;&#20013;&#36873;&#25321;&#30340;&#19968;&#20123;&#26080;&#20998;&#38598;&#20219;&#21153;&#23545;&#20803;&#23398;&#20064;&#22120;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;&#36873;&#25321;&#26631;&#20934;&#26159;&#36890;&#36807;&#20146;&#21644;&#24615;&#35774;&#35745;&#30340;&#65292;&#20146;&#21644;&#24615;&#34913;&#37327;&#20102;&#22312;&#20351;&#29992;&#36873;&#23450;&#20219;&#21153;&#35757;&#32451;&#21518;&#65292;&#25191;&#34892;&#30446;&#26631;&#20219;&#21153;&#26102;&#25439;&#22833;&#38477;&#20302;&#30340;&#31243;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35757;&#32451;&#20219;&#21153;&#38598;&#21253;&#21547;&#20102;&#19968;&#20123;&#26377;&#21069;&#26223;&#30340;&#31867;&#22411;&#65292;&#22914;&#23545;&#27604;&#23398;&#20064;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Episodic training is a mainstream training strategy for few-shot learning. In few-shot scenarios, however, this strategy is often inferior to some non-episodic training strategy, e. g., Neighbourhood Component Analysis (NCA), which challenges the principle that training conditions must match testing conditions. Thus, a question is naturally asked: How to search for episodic-free tasks for better few-shot learning? In this work, we propose a novel meta-training framework beyond episodic training. In this framework, episodic tasks are not used directly for training, but for evaluating the effectiveness of some selected episodic-free tasks from a task set that are performed for training the meta-learners. The selection criterion is designed with the affinity, which measures the degree to which loss decreases when executing the target tasks after training with the selected tasks. In experiments, the training task set contains some promising types, e. g., contrastive learning and classifica
&lt;/p&gt;</description></item><item><title>SCAPE&#26159;&#19968;&#20010;&#23558;&#36827;&#21270;&#25628;&#32034;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#25805;&#20316;&#25506;&#32034;&#30001;&#21021;&#22987;&#36755;&#20837;&#21551;&#21457;&#30340;&#21019;&#36896;&#24615;&#21644;&#39640;&#36136;&#37327;&#35774;&#35745;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#26032;&#39062;&#24615;&#12289;&#36136;&#37327;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00089</link><description>&lt;p&gt;
SCAPE: &#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#27010;&#24565;&#26550;&#26500;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
SCAPE: Searching Conceptual Architecture Prompts using Evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00089
&lt;/p&gt;
&lt;p&gt;
SCAPE&#26159;&#19968;&#20010;&#23558;&#36827;&#21270;&#25628;&#32034;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#25805;&#20316;&#25506;&#32034;&#30001;&#21021;&#22987;&#36755;&#20837;&#21551;&#21457;&#30340;&#21019;&#36896;&#24615;&#21644;&#39640;&#36136;&#37327;&#35774;&#35745;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#26032;&#39062;&#24615;&#12289;&#36136;&#37327;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26550;&#26500;&#28041;&#21450;&#23545;&#26469;&#33258;&#20854;&#20182;&#23398;&#31185;&#30340;&#26032;&#39062;&#29702;&#24565;&#36827;&#34892;&#39640;&#24230;&#21019;&#36896;&#24615;&#30340;&#25506;&#32034;&#65292;&#24314;&#31569;&#24072;&#32771;&#34385;&#23558;&#24314;&#31569;&#30340;&#24418;&#24335;&#12289;&#26448;&#26009;&#12289;&#36136;&#22320;&#21644;&#39068;&#33394;&#36827;&#34892;&#28608;&#36827;&#30340;&#21019;&#26032;&#12290;&#34429;&#28982;&#22914;&#20170;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#20960;&#21313;&#24180;&#26469;&#36827;&#21270;&#31639;&#27861;&#25152;&#23637;&#31034;&#30340;&#21019;&#36896;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24037;&#20855;SCAPE&#23558;&#36827;&#21270;&#25628;&#32034;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#31616;&#21333;&#30340;&#28857;&#25353;&#30028;&#38754;&#25506;&#32034;&#30001;&#21021;&#22987;&#36755;&#20837;&#21551;&#21457;&#30340;&#21019;&#36896;&#24615;&#21644;&#39640;&#36136;&#37327;&#35774;&#35745;&#12290;SCAPE&#23558;&#38543;&#26426;&#24615;&#27880;&#20837;&#21040;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#24182;&#21033;&#29992;GPT-4&#30340;&#20869;&#32622;&#35821;&#35328;&#33021;&#21147;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#31361;&#21464;&#21644;&#20132;&#21449;&#25805;&#20316;&#26469;&#21464;&#21270;&#25552;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;DALL-E 3&#30456;&#27604;&#65292;SCAPE&#22312;&#22270;&#20687;&#30340;&#26032;&#39062;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;67%&#65292;&#21516;&#26102;&#22312;&#36136;&#37327;&#21644;&#20351;&#29992;&#25928;&#26524;&#26041;&#38754;&#20063;&#26377;&#25152;&#25913;&#36827;&#65307;&#25105;&#20204;&#23637;&#31034;&#65292;&#20165;&#32463;&#36807;3&#27425;&#36845;&#20195;&#65292;SCAPE&#30340;&#22270;&#20687;&#26032;&#39062;&#24615;&#22686;&#21152;&#20102;24%&#65292;&#20351;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conceptual architecture involves a highly creative exploration of novel ideas, often taken from other disciplines as architects consider radical new forms, materials, textures and colors for buildings. While today's generative AI systems can produce remarkable results, they lack the creativity demonstrated for decades by evolutionary algorithms. SCAPE, our proposed tool, combines evolutionary search with generative AI, enabling users to explore creative and good quality designs inspired by their initial input through a simple point and click interface. SCAPE injects randomness into generative AI, and enables memory, making use of the built-in language skills of GPT-4 to vary prompts via text-based mutation and crossover. We demonstrate that compared to DALL-E 3, SCAPE enables a 67% improvement in image novelty, plus improvements in quality and effectiveness of use; we show that in just 3 iterations SCAPE has a 24% image novelty increase enabling effective exploration, plus optimization
&lt;/p&gt;</description></item><item><title>RetroWISE &#26159;&#19968;&#20010;&#21033;&#29992;&#35745;&#31639;&#27169;&#25311;&#30340;&#21453;&#24212;&#25968;&#25454;&#22686;&#24378;&#30340; retrosynthesis &#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#38750;&#37197;&#23545;&#25968;&#25454;&#29983;&#25104;&#37197;&#23545;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00086</link><description>&lt;p&gt;
&#21033;&#29992;&#35745;&#31639;&#27169;&#25311;&#21453;&#24212;&#25968;&#25454;&#22686;&#24378;&#30340; retrosynthesis &#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis prediction enhanced by in-silico reaction data augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00086
&lt;/p&gt;
&lt;p&gt;
RetroWISE &#26159;&#19968;&#20010;&#21033;&#29992;&#35745;&#31639;&#27169;&#25311;&#30340;&#21453;&#24212;&#25968;&#25454;&#22686;&#24378;&#30340; retrosynthesis &#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#38750;&#37197;&#23545;&#25968;&#25454;&#29983;&#25104;&#37197;&#23545;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312; retrosynthesis &#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#24110;&#21161;&#21270;&#23398;&#23478;&#26356;&#39640;&#25928;&#22320;&#35774;&#35745;&#23454;&#39564;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#22823;&#37327;&#25104;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#21363;&#21270;&#23398;&#21453;&#24212;&#65306;&#20135;&#29289;-&#21453;&#24212;&#29289;&#23545;&#65289;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#20844;&#21496;&#23558;&#21453;&#24212;&#25968;&#25454;&#35270;&#20026;&#26377;&#20215;&#20540;&#30340;&#36164;&#20135;&#65292;&#24182;&#38480;&#21046;&#23545;&#30740;&#31350;&#20154;&#21592;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#26356;&#24378;&#22823;&#30340; retrosynthesis &#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#38750;&#37197;&#23545;&#25968;&#25454;&#65288;&#21363;&#20135;&#29289;-&#21453;&#24212;&#29289;&#23545;&#20013;&#30340;&#19968;&#20010;&#32452;&#20998;&#65289;&#29983;&#25104;&#35745;&#31639;&#27169;&#25311;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#24110;&#21161;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; RetroWISE&#65292;&#19968;&#20010;&#33258;&#25105;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#20174;&#30495;&#23454;&#37197;&#23545;&#25968;&#25454;&#25512;&#26029;&#20986;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#38750;&#37197;&#23545;&#25968;&#25454;&#25191;&#34892;&#35745;&#31639;&#27169;&#25311;&#30340;&#21453;&#24212;&#29983;&#25104;&#21644;&#22686;&#24378;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19968;&#20010;&#21331;&#36234;&#30340;&#27169;&#22411;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RetroWISE &#23454;&#29616;&#26368;&#20339;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning (ML) have expedited retrosynthesis research by assisting chemists to design experiments more efficiently. However, all ML-based methods consume substantial amounts of paired training data (i.e., chemical reaction: product-reactant(s) pair), which is costly to obtain. Moreover, companies view reaction data as a valuable asset and restrict the accessibility to researchers. These issues prevent the creation of more powerful retrosynthesis models due to their data-driven nature. As a response, we exploit easy-to-access unpaired data (i.e., one component of product-reactant(s) pair) for generating in-silico paired data to facilitate model training. Specifically, we present RetroWISE, a self-boosting framework that employs a base model inferred from real paired data to perform in-silico reaction generation and augmentation using unpaired data, ultimately leading to a superior model. On three benchmark datasets, RetroWISE achieves the best overall performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00085</link><description>&lt;p&gt;
&#39044;&#23450;&#22909;&#22855;&#24515;-&#28145;&#24230;&#21160;&#24577;-Q&#65306;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#22521;&#35757;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#38656;&#35201;&#19982;&#30495;&#23454;&#29992;&#25143;&#36827;&#34892;&#22823;&#37327;&#30340;&#20132;&#20114;&#12290;&#22914;&#20309;&#22312;&#26377;&#38480;&#30340;&#23545;&#35805;&#32463;&#39564;&#20013;&#25484;&#25569;&#23545;&#35805;&#31574;&#30053;&#20173;&#28982;&#26159;&#20351;&#20195;&#29702;&#22521;&#35757;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#30340;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26694;&#26550;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#22521;&#35757;&#26679;&#26412;&#24320;&#22987;&#22521;&#35757;&#65292;&#36825;&#19982;&#20154;&#31867;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#25439;&#23475;&#20102;&#22521;&#35757;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#35805;&#27169;&#22411;Deep Dyna-Q(DDQ)&#30340;&#39044;&#23450;&#22909;&#22855;&#24515;-&#28145;&#24230;&#21160;&#24577;-Q (SC-DDQ)&#30340;&#22909;&#22855;&#24515;&#39537;&#21160;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;SC-DDQ&#21644;DDQ&#35774;&#35745;&#20102;&#23398;&#20064;&#35745;&#21010;&#65292;&#36981;&#24490;&#20004;&#31181;&#30456;&#21453;&#30340;&#22521;&#35757;&#31574;&#30053;&#65306;&#32463;&#20856;&#35838;&#31243;&#23398;&#20064;&#21450;&#20854;&#36870;&#21521;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26032;&#26694;&#26550;&#22312;DDQ&#21644;Dee&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training task-oriented dialog agents based on reinforcement learning is time-consuming and requires a large number of interactions with real users. How to grasp dialog policy within limited dialog experiences remains an obstacle that makes the agent training process less efficient. In addition, most previous frameworks start training by randomly choosing training samples, which differs from the human learning method and hurts the efficiency and stability of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a curiosity-driven curriculum learning framework based on a state-of-the-art model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ). Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively, following two opposite training strategies: classic curriculum learning and its reverse version. Our results show that by introducing scheduled learning and curiosity, the new framework leads to a significant improvement over the DDQ and Dee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#26399;&#20462;&#21098;&#19982;&#33258;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2402.00084</link><description>&lt;p&gt;
EPSD: &#26089;&#26399;&#20462;&#21098;&#19982;&#33258;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
EPSD: Early Pruning with Self-Distillation for Efficient Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00084
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#26399;&#20462;&#21098;&#19982;&#33258;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#32593;&#32476;&#20462;&#21098;&#65292;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#33976;&#39311;&#8221;&#34920;&#26126;&#65292;&#20462;&#21098;&#21518;&#30340;&#36866;&#21512;&#23398;&#29983;&#30340;&#25945;&#24072;&#32593;&#32476;&#21487;&#20197;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25945;&#24072;-&#23398;&#29983;&#27969;&#31243;&#65292;&#28041;&#21450;&#32321;&#29712;&#30340;&#25945;&#24072;&#39044;&#35757;&#32451;&#21644;&#22797;&#26434;&#30340;&#21387;&#32553;&#27493;&#39588;&#65292;&#20351;&#24471;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#20462;&#21098;&#21464;&#24471;&#19981;&#22815;&#39640;&#25928;&#12290;&#38500;&#20102;&#21387;&#32553;&#27169;&#22411;&#65292;&#26368;&#36817;&#30340;&#21387;&#32553;&#25216;&#26415;&#20063;&#24378;&#35843;&#25928;&#29575;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;&#26089;&#26399;&#20462;&#21098;&#19982;&#20256;&#32479;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35201;&#27714;&#30340;&#35745;&#31639;&#25104;&#26412;&#35201;&#23567;&#24471;&#22810;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#21516;&#26679;&#65292;&#33258;&#33976;&#39311;&#20316;&#20026;&#30693;&#35782;&#33976;&#39311;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#26356;&#21152;&#39640;&#25928;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#25110;&#23398;&#29983;-&#25945;&#24072;&#23545;&#30340;&#36873;&#25321;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23558;&#26089;&#26399;&#20462;&#21098;&#19982;&#33258;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21517;&#20026;&#8220;&#26089;&#26399;&#20462;&#21098;&#8221;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network compression techniques, such as knowledge distillation (KD) and network pruning, have received increasing attention. Recent work `Prune, then Distill' reveals that a pruned student-friendly teacher network can benefit the performance of KD. However, the conventional teacher-student pipeline, which entails cumbersome pre-training of the teacher and complicated compression steps, makes pruning with KD less efficient. In addition to compressing models, recent compression techniques also emphasize the aspect of efficiency. Early pruning demands significantly less computational cost in comparison to the conventional pruning methods as it does not require a large pre-trained model. Likewise, a special case of KD, known as self-distillation (SD), is more efficient since it requires no pre-training or student-teacher pair selection. This inspires us to collaborate early pruning with SD for efficient model compression. In this work, we propose the framework named Early Pruning wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20934;&#20837;&#24046;&#24322;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#36164;&#28304;&#20998;&#37197;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#20934;&#20837;&#27169;&#22411;&#21644;&#20934;&#20837;&#24863;&#30693;&#30340;&#20998;&#37197;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#30693;&#36947;&#20934;&#20837;&#24046;&#36317;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#20943;&#23569;&#36164;&#28304;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.00083</link><description>&lt;p&gt;
&#20943;&#23569;&#36164;&#28304;&#20998;&#37197;&#24046;&#24322;&#30340;&#20934;&#20837;&#24046;&#24322;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Access Differences to Reduce Disparity in Resource Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20934;&#20837;&#24046;&#24322;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#36164;&#28304;&#20998;&#37197;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#20934;&#20837;&#27169;&#22411;&#21644;&#20934;&#20837;&#24863;&#30693;&#30340;&#20998;&#37197;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#30693;&#36947;&#20934;&#20837;&#24046;&#36317;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#20943;&#23569;&#36164;&#28304;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;COVID-19&#30123;&#33495;&#20998;&#37197;&#20013;&#65292;&#26131;&#21463;&#24433;&#21709;&#30340;&#20122;&#20154;&#32676;&#22312;&#20581;&#24247;&#21644;&#25509;&#31181;&#30123;&#33495;&#20934;&#20837;&#26041;&#38754;&#26356;&#20855;&#21155;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#21644;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#19982;&#20248;&#21183;&#21644;&#21155;&#21183;&#30456;&#20851;&#30340;&#22266;&#26377;&#20934;&#20837;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#20943;&#23569;&#36164;&#28304;&#24046;&#24322;&#26159;&#36825;&#19968;&#32972;&#26223;&#19979;&#30340;&#20851;&#38190;&#30446;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;&#26356;&#32454;&#33268;&#30340;&#19979;&#28216;&#24433;&#21709;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#20934;&#20837;&#27169;&#22411;&#65292;&#24110;&#21161;&#37327;&#21270;&#32473;&#23450;&#20998;&#37197;&#23545;&#20110;&#26377;&#21033;&#22320;&#21306;&#21644;&#19981;&#21033;&#22320;&#21306;&#30340;&#36164;&#28304;&#27969;&#36716;&#30340;&#24433;&#21709;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#23427;&#20204;&#20043;&#38388;&#30340;&#20934;&#20837;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20934;&#20837;&#24863;&#30693;&#30340;&#20998;&#37197;&#26041;&#27861;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#20998;&#37197;&#21033;&#29992;&#26356;&#22810;&#30123;&#33495;&#20998;&#37197;&#32473;&#26131;&#21463;&#25439;&#20154;&#32676;&#26356;&#22810;&#30340;&#22320;&#21306;&#65292;&#20197;&#20943;&#23569;&#20934;&#20837;&#24046;&#36317;&#21644;&#25972;&#20307;&#24046;&#24322;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36890;&#24120;&#19981;&#38656;&#35201;&#20102;&#35299;&#20934;&#20837;&#24046;&#36317;&#23601;&#21487;&#20197;&#25191;&#34892;&#20934;&#20837;&#24863;&#30693;&#30340;&#20998;&#37197;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#20855;&#20307;&#24314;&#27169;&#21644;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by COVID-19 vaccine allocation, where vulnerable subpopulations are simultaneously more impacted in terms of health and more disadvantaged in terms of access to the vaccine, we formalize and study the problem of resource allocation when there are inherent access differences that correlate with advantage and disadvantage. We identify reducing resource disparity as a key goal in this context and show its role as a proxy to more nuanced downstream impacts. We develop a concrete access model that helps quantify how a given allocation translates to resource flow for the advantaged vs. the disadvantaged, based on the access gap between them. We then provide a methodology for access-aware allocation. Intuitively, the resulting allocation leverages more vaccines in locations with higher vulnerable populations to mitigate the access gap and reduce overall disparity. Surprisingly, knowledge of the access gap is often not needed to perform access-aware allocation. To support this formal
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25506;&#32034;&#20102;&#25193;&#23637;&#26377;&#26465;&#20214;&#39532;&#23572;&#21487;&#22827;&#38142;&#25628;&#32034;&#65288;CMCS&#65289;&#26694;&#26550;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#24320;&#21457;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#38454;&#27573;CMCS&#20248;&#20110;&#21333;&#19968;&#38454;&#27573;CMCS&#12290;</title><link>https://arxiv.org/abs/2402.00076</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#39532;&#23572;&#21487;&#22827;&#38142;&#25628;&#32034;&#20013;&#30340;&#24320;&#21457;&#31574;&#30053;&#65306;&#22522;&#20110;&#19977;&#25351;&#25968;&#20998;&#37197;&#38382;&#39064;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploitation Strategies in Conditional Markov Chain Search: A case study on the three-index assignment problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00076
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25506;&#32034;&#20102;&#25193;&#23637;&#26377;&#26465;&#20214;&#39532;&#23572;&#21487;&#22827;&#38142;&#25628;&#32034;&#65288;CMCS&#65289;&#26694;&#26550;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#24320;&#21457;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#38454;&#27573;CMCS&#20248;&#20110;&#21333;&#19968;&#38454;&#27573;CMCS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#39532;&#23572;&#21487;&#22827;&#38142;&#25628;&#32034;&#65288;CMCS&#65289;&#26159;&#29992;&#20110;&#31163;&#25955;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#20803;&#21551;&#21457;&#24335;&#33258;&#21160;&#35774;&#35745;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#32452;&#31639;&#27861;&#32452;&#20214;&#65292;&#22914;&#23665;&#29228;&#21644;&#31361;&#21464;&#65292;CMCS&#20915;&#23450;&#20197;&#20309;&#31181;&#39034;&#24207;&#24212;&#29992;&#36825;&#20123;&#32452;&#20214;&#12290;&#20915;&#31574;&#30001;&#31163;&#32447;&#23398;&#20064;&#30340;CMCS&#37197;&#32622;&#25351;&#23548;&#12290;CMCS&#27809;&#26377;&#25509;&#21463;&#26631;&#20934;&#65292;&#26694;&#26550;&#25509;&#21463;&#20219;&#20309;&#31227;&#21160;&#12290;&#22240;&#27492;&#65292;&#23427;&#22312;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24320;&#21457;&#26041;&#38754;&#19981;&#22914;&#20854;&#20182;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#25193;&#23637;CMCS&#30340;&#26041;&#27861;&#20197;&#25552;&#39640;&#20854;&#24320;&#21457;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#35745;&#31639;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#19977;&#25351;&#25968;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#38454;&#27573;CMCS&#30340;&#30830;&#20248;&#20110;&#21333;&#19968;&#38454;&#27573;CMCS&#12290;
&lt;/p&gt;
&lt;p&gt;
The Conditional Markov Chain Search (CMCS) is a framework for automated design of metaheuristics for discrete combinatorial optimisation problems. Given a set of algorithmic components such as hill climbers and mutations, CMCS decides in which order to apply those components. The decisions are dictated by the CMCS configuration that can be learnt offline. CMCS does not have an acceptance criterion; any moves are accepted by the framework. As a result, it is particularly good in exploration but is not as good at exploitation. In this study, we explore several extensions of the framework to improve its exploitation abilities. To perform a computational study, we applied the framework to the three-index assignment problem. The results of our experiments showed that a two-stage CMCS is indeed superior to a single-stage CMCS.
&lt;/p&gt;</description></item><item><title>EvoMerge&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#21512;&#24182;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20132;&#21449;&#21644;&#24494;&#35843;&#36827;&#34892;&#26435;&#37325;&#21464;&#24322;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#21521;&#36229;&#36234;&#20256;&#32479;&#24494;&#35843;&#38480;&#21046;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.00070</link><description>&lt;p&gt;
EvoMerge:&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
EvoMerge: Neuroevolution for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00070
&lt;/p&gt;
&lt;p&gt;
EvoMerge&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#21512;&#24182;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20132;&#21449;&#21644;&#24494;&#35843;&#36827;&#34892;&#26435;&#37325;&#21464;&#24322;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#21521;&#36229;&#36234;&#20256;&#32479;&#24494;&#35843;&#38480;&#21046;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24494;&#35843;&#20013;&#65292;&#24182;&#19981;&#24635;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#24448;&#24448;&#27169;&#22411;&#26356;&#25797;&#38271;&#27169;&#20223;&#19968;&#31181;&#25968;&#25454;&#24418;&#24335;&#32780;&#19981;&#20855;&#22791;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#33021;&#22833;&#21435;&#19968;&#20123;&#26234;&#33021;&#12290;&#36825;&#37324;&#25105;&#20171;&#32461;&#20102;EvoMerge&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#21512;&#24182;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20132;&#21449;&#21644;&#24494;&#35843;&#36827;&#34892;&#26435;&#37325;&#21464;&#24322;&#65292;EvoMerge&#24314;&#31435;&#20102;&#19968;&#20010;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#21521;&#36229;&#36234;&#20256;&#32479;&#24494;&#35843;&#38480;&#21046;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive fine-tuning on Large Language Models does not always yield better results. Oftentimes, models tend to get better at imitating one form of data without gaining greater reasoning ability and may even end up losing some intelligence. Here I introduce EvoMerge, a systematic approach to large language model training and merging. Leveraging model merging for weight crossover and fine-tuning for weight mutation, EvoMerge establishes an evolutionary process aimed at pushing models beyond the limits of conventional fine-tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#25277;&#35937;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#25551;&#36848;&#35821;&#35328;&#65288;ACAD&#65289;&#23545;AI&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#24314;&#27169;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20351;&#29992;ACAD&#65292;&#24037;&#31243;&#24072;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#19981;&#21516;&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#26696;&#30340;&#24615;&#33021;&#29305;&#24615;&#65292;&#20174;&#32780;&#22312;&#36873;&#25321;&#21644;&#37197;&#32622;&#21152;&#36895;&#22120;&#26102;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.00069</link><description>&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#25551;&#36848;&#35821;&#35328;&#23545;AI&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#25277;&#35937;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#25551;&#36848;&#35821;&#35328;&#65288;ACAD&#65289;&#23545;AI&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#24314;&#27169;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20351;&#29992;ACAD&#65292;&#24037;&#31243;&#24072;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#19981;&#21516;&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#26696;&#30340;&#24615;&#33021;&#29305;&#24615;&#65292;&#20174;&#32780;&#22312;&#36873;&#25321;&#21644;&#37197;&#32622;&#21152;&#36895;&#22120;&#26102;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26222;&#21450;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#25512;&#21160;&#20102;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#25216;&#26415;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#65292;&#19987;&#38376;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#36825;&#31181;&#38656;&#27714;&#24341;&#21457;&#20102;&#19981;&#21516;&#20379;&#24212;&#21830;&#25552;&#20379;&#30340;&#21487;&#21442;&#25968;&#21270;AI&#30828;&#20214;&#21152;&#36895;&#22120;&#24066;&#22330;&#12290;&#38598;&#25104;AI&#20135;&#21697;&#21046;&#36896;&#21830;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#36873;&#25321;&#19982;&#20854;&#20135;&#21697;&#24615;&#33021;&#35201;&#27714;&#30456;&#21305;&#37197;&#30340;&#21152;&#36895;&#22120;&#12290;&#36825;&#20010;&#20915;&#31574;&#21253;&#25324;&#36873;&#25321;&#21512;&#36866;&#30340;&#30828;&#20214;&#21644;&#37197;&#32622;&#19968;&#32452;&#21512;&#36866;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#27604;&#36739;&#19981;&#21516;&#30340;&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#26696;&#20173;&#28982;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36890;&#24120;&#65292;&#24037;&#31243;&#24072;&#20381;&#36182;&#20110;&#25968;&#25454;&#34920;&#12289;&#30005;&#23376;&#34920;&#26684;&#35745;&#31639;&#25110;&#32531;&#24930;&#30340;&#40657;&#30418;&#27169;&#25311;&#22120;&#65292;&#36825;&#20123;&#21482;&#33021;&#25552;&#20379;&#23545;&#24615;&#33021;&#29305;&#24615;&#30340;&#31895;&#30053;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has witnessed remarkable growth, particularly through the proliferation of Deep Neural Networks (DNNs). These powerful models drive technological advancements across various domains. However, to harness their potential in real-world applications, specialized hardware accelerators are essential. This demand has sparked a market for parameterizable AI hardware accelerators offered by different vendors.   Manufacturers of AI-integrated products face a critical challenge: selecting an accelerator that aligns with their product's performance requirements. The decision involves choosing the right hardware and configuring a suitable set of parameters. However, comparing different accelerator design alternatives remains a complex task. Often, engineers rely on data sheets, spreadsheet calculations, or slow black-box simulators, which only offer a coarse understanding of the performance characteristics.   The Abstract Computer Architecture Description Language (ACAD
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00068</link><description>&lt;p&gt;
GPT4Battery: &#19968;&#31181;&#22522;&#20110;LLM&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#29366;&#24577;&#65288;SOH&#65289;&#26159;&#35780;&#20272;&#30005;&#27744;&#36864;&#21270;&#27700;&#24179;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#20294;&#38656;&#35201;&#20272;&#35745;&#12290;&#20934;&#30830;&#30340;SOH&#20272;&#35745;&#25552;&#21319;&#20102;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#26816;&#27979;&#12289;&#25511;&#21046;&#21644;&#21453;&#39304;&#33021;&#21147;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#65292;&#24182;&#25351;&#23548;&#26032;&#19968;&#20195;&#30005;&#27744;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;SOH&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20026;&#29983;&#25104;&#23551;&#21629;&#38271;&#26399;&#35757;&#32451;&#25968;&#25454;&#32780;&#36827;&#34892;&#30340;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#36864;&#21270;&#23454;&#39564;&#22312;&#24314;&#31435;&#19968;&#20010;&#33021;&#22788;&#29702;&#22810;&#26679;&#21270;&#38146;&#31163;&#23376;&#30005;&#27744;&#65288;&#20363;&#22914;&#65292;&#36328;&#21270;&#23398;&#12289;&#36328;&#21046;&#36896;&#21830;&#21644;&#36328;&#23481;&#37327;&#65289;&#30340;&#22823;&#22411;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#30005;&#27744;&#30340;&#21487;&#35843;&#25972;SOH&#20272;&#35745;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#20026;&#20102;&#36866;&#24212;&#23454;&#38469;&#24773;&#26223;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25353;&#39034;&#24207;&#20197;&#21450;&#20998;&#24067;&#21464;&#21270;&#30340;&#26041;&#24335;&#21040;&#36798;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of health (SOH) is a crucial indicator for assessing the degradation level of batteries that cannot be measured directly but requires estimation. Accurate SOH estimation enhances detection, control, and feedback for Li-ion batteries, allowing for safe and efficient energy management and guiding the development of new-generation batteries. Despite the significant progress in data-driven SOH estimation, the time and resource-consuming degradation experiments for generating lifelong training data pose a challenge in establishing one large model capable of handling diverse types of Li-ion batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity. Hence, this paper utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries. To match the real scenario where unlabeled data sequentially arrives in use with distribution shifts, the proposed model is modified by a test-time t
&lt;/p&gt;</description></item><item><title>TrackGPT&#26159;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#23454;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#21253;&#25324;&#38271;&#26399;&#39044;&#27979;&#21644;&#30701;&#26399;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00066</link><description>&lt;p&gt;
TrackGPT -- &#29992;&#20110;&#36328;&#39046;&#22495;&#23454;&#20307;&#36712;&#36857;&#39044;&#27979;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
TrackGPT -- A generative pre-trained transformer for cross-domain entity trajectory forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00066
&lt;/p&gt;
&lt;p&gt;
TrackGPT&#26159;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#23454;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#21253;&#25324;&#38271;&#26399;&#39044;&#27979;&#21644;&#30701;&#26399;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21830;&#19994;&#21644;&#22269;&#38450;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#65292;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#23454;&#20307;&#36712;&#36857;&#36827;&#34892;&#39044;&#27979;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#33021;&#21147;&#32570;&#21475;&#12290;&#36817;&#24180;&#26469;&#65292;&#21464;&#25442;&#22120;&#21644;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#32593;&#32476;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#20960;&#20010;&#39046;&#22495;&#38761;&#21629;&#21270;&#65292;&#20854;&#20013;&#26368;&#20026;&#33879;&#21517;&#30340;&#26159;&#20351;&#29992;&#24320;&#25918;AI&#30340;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TrackGPT&#65292;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#23454;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#28023;&#19978;&#21644;&#33322;&#31354;&#39046;&#22495;&#37117;&#26174;&#31034;&#20986;&#20102;&#23454;&#29992;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#39044;&#35745;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#33021;&#34920;&#29616;&#20986;&#33394;&#12290;TrackGPT&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;GPT&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#23454;&#20307;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#38271;&#26399;&#39044;&#27979;&#20855;&#26377;&#25345;&#32493;&#20934;&#30830;&#24615;&#21644;&#30701;&#26399;&#39044;&#27979;&#20855;&#26377;&#39640;&#31934;&#24230;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;TrackGPT&#30340;&#39044;&#27979;&#33021;&#21147;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The forecasting of entity trajectories at future points in time is a critical capability gap in applications across both Commercial and Defense sectors. Transformers, and specifically Generative Pre-trained Transformer (GPT) networks have recently revolutionized several fields of Artificial Intelligence, most notably Natural Language Processing (NLP) with the advent of Large Language Models (LLM) like OpenAI's ChatGPT. In this research paper, we introduce TrackGPT, a GPT-based model for entity trajectory forecasting that has shown utility across both maritime and air domains, and we expect to perform well in others. TrackGPT stands as a pioneering GPT model capable of producing accurate predictions across diverse entity time series datasets, demonstrating proficiency in generating both long-term forecasts with sustained accuracy and short-term forecasts with high precision. We present benchmarks against state-of-the-art deep learning techniques, showing that TrackGPT's forecasting capa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;3-SAT&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QAOA&#30340;&#38388;&#25509;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#30340;&#25490;&#21517;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#35299;&#26512;&#65292;&#21487;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00065</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#38388;&#25509;QAOA&#26041;&#27861;&#30340;91&#20010;&#23376;&#21477;SAT&#35299;&#26512;&#30340;&#25216;&#26415;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A technical note for the 91-clauses SAT resolution with Indirect QAOA based approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;3-SAT&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QAOA&#30340;&#38388;&#25509;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#30340;&#25490;&#21517;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#35299;&#26512;&#65292;&#21487;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#31867;&#20284;QAOA&#30340;&#26041;&#27861;&#35299;&#20915;3-SAT&#38382;&#39064;&#12290;&#25152;&#36873;&#25321;&#30340;&#21407;&#21017;&#28041;&#21450;&#23545;3-SAT&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#22312;&#36825;&#31181;&#29305;&#23450;&#24773;&#20917;&#19979;&#30452;&#25509;&#20195;&#34920;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#38750;&#24120;&#32039;&#20945;&#30340;&#30005;&#36335;&#65292;&#38376;&#25968;&#37327;&#36739;&#23569;&#65292;&#21487;&#20197;&#23545;&#22823;&#35268;&#27169;&#30340;3-SAT&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;Qiskit&#23454;&#29616;&#35299;&#20915;&#30001;91&#20010;&#23376;&#21477;&#21644;20&#20010;&#21464;&#37327;&#32452;&#25104;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the resolution of the 3-SAT problem using a QAOA-like approach. The chosen principle involves modeling the solution ranks of the 3-SAT problem, which, in this particular case, directly represent a solution. This results in a highly compact circuit with few gates, enabling the modeling of large-sized 3-SAT problems. Numerical experimentation demonstrates that the approach can solve instances composed of 91 clauses and 20 variables with an implementation based on Qiskit.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#20195;&#29702;&#30340;&#22768;&#35465;&#31995;&#32479;&#21512;&#24182;&#20855;&#26377;&#20851;&#20110;&#21160;&#20316;&#21644;&#30446;&#26631;&#30340;&#19981;&#23436;&#25972;&#30693;&#35782;&#30340;&#35745;&#21010;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#29983;&#25104;&#36807;&#28193;&#35745;&#21010;&#30340;&#24037;&#20855;&#65292;&#23545;&#35748;&#30693;&#38556;&#30861;&#20154;&#22763;&#20855;&#26377;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.00064</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20195;&#29702;&#30340;&#22768;&#35465;&#31995;&#32479;&#21512;&#24182;&#20855;&#26377;&#20851;&#20110;&#21160;&#20316;&#21644;&#30446;&#26631;&#30340;&#19981;&#23436;&#25972;&#30693;&#35782;&#30340;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Merging plans with incomplete knowledge about actions and goals through an agent-based reputation system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#20195;&#29702;&#30340;&#22768;&#35465;&#31995;&#32479;&#21512;&#24182;&#20855;&#26377;&#20851;&#20110;&#21160;&#20316;&#21644;&#30446;&#26631;&#30340;&#19981;&#23436;&#25972;&#30693;&#35782;&#30340;&#35745;&#21010;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#29983;&#25104;&#36807;&#28193;&#35745;&#21010;&#30340;&#24037;&#20855;&#65292;&#23545;&#35748;&#30693;&#38556;&#30861;&#20154;&#22763;&#20855;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#29702;&#36807;&#28193;&#35745;&#21010;&#26159;&#35748;&#30693;&#38556;&#30861;&#20154;&#22763;&#38754;&#20020;&#30340;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#25214;&#21040;&#19968;&#31181;&#33258;&#21160;&#21270;&#29983;&#25104;&#36825;&#31181;&#35745;&#21010;&#30340;&#26041;&#27861;&#23558;&#26159;&#23545;&#36825;&#20010;&#32676;&#20307;&#26377;&#30410;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26367;&#20195;&#26041;&#24335;&#65292;&#36890;&#36807;&#21512;&#24182;&#30001;&#22810;&#20010;&#25805;&#20316;&#20195;&#29702;&#25191;&#34892;&#30340;&#30446;&#26631;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#26410;&#30693;&#30456;&#20284;&#24615;&#24207;&#21015;&#24418;&#25104;&#30340;&#35745;&#21010;&#65292;&#36825;&#20123;&#20195;&#29702;&#20043;&#38388;&#36827;&#34892;&#21512;&#20316;&#65292;&#23545;&#19968;&#20123; passsive &#20803;&#32032;&#65288;&#33410;&#28857;&#20195;&#29702;&#65289;&#24212;&#29992;&#36825;&#20123;&#34892;&#21160;&#65292;&#22312;&#20351;&#29992;&#19968;&#27573;&#26102;&#38388;&#21518;&#38656;&#35201;&#25191;&#34892;&#21478;&#19968;&#20010;&#35745;&#21010;&#12290;&#36825;&#31181;&#23545;&#35745;&#21010;&#21160;&#20316;&#21644;&#30446;&#26631;&#30456;&#20284;&#24615;&#30340;&#26080;&#30693;&#21487;&#20197;&#35299;&#37322;&#20026;&#20351;&#29992;&#20998;&#24067;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20250;&#26681;&#25454;&#20854;&#20182;&#25805;&#20316;&#20195;&#29702;&#20197;&#21069;&#25191;&#34892;&#19981;&#21516;&#35745;&#21010;&#30340;&#24050;&#30693;&#32467;&#26524;&#65292;&#20026;&#29305;&#23450;&#30446;&#26631;&#30340;&#32473;&#23450;&#25805;&#20316;&#20195;&#29702;&#25552;&#20379;&#19968;&#20010;&#26377;&#29992;&#30340;&#35745;&#21010;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25191;&#34892;&#30340;&#24635;&#20307;&#26694;&#26550;&#65288;&#20195;&#29702;&#31995;&#32479;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#21512;&#24182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Managing transition plans is one of the major problems of people with cognitive disabilities. Therefore, finding an automated way to generate such plans would be a helpful tool for this community. In this paper we have specifically proposed and compared different alternative ways to merge plans formed by sequences of actions of unknown similarities between goals and actions executed by several operator agents which cooperate between them applying such actions over some passive elements (node agents) that require additional executions of another plan after some time of use. Such ignorance of the similarities between plan actions and goals would justify the use of a distributed recommendation system that would provide an useful plan to be applied for a certain goal to a given operator agent, generated from the known results of previous executions of different plans by other operator agents. Here we provide the general framework of execution (agent system), and the different merging algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20849;&#21516;&#25968;&#25454;&#20449;&#24687;&#20013;&#24314;&#27169;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#26681;&#25454;&#32622;&#20449;&#24230;&#23545;&#20849;&#21516;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#26500;&#24314;&#27010;&#29575;&#31665;&#21644;DSt&#32467;&#26500;&#65292;&#21487;&#20197;&#35745;&#31639;&#29305;&#23450;&#30896;&#25758;&#27010;&#29575;&#30340;&#32622;&#20449;&#24230;&#21644;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00060</link><description>&lt;p&gt;
&#22312;Dempster-Shafer&#29702;&#35770;&#20013;&#22788;&#29702;&#20849;&#21516;&#20998;&#26512;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Treatment of Epistemic Uncertainty in Conjunction Analysis with Dempster-Shafer Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20849;&#21516;&#25968;&#25454;&#20449;&#24687;&#20013;&#24314;&#27169;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#26681;&#25454;&#32622;&#20449;&#24230;&#23545;&#20849;&#21516;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#26500;&#24314;&#27010;&#29575;&#31665;&#21644;DSt&#32467;&#26500;&#65292;&#21487;&#20197;&#35745;&#31639;&#29305;&#23450;&#30896;&#25758;&#27010;&#29575;&#30340;&#32622;&#20449;&#24230;&#21644;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#21516;&#25968;&#25454;&#20449;&#24687;&#65288;CDM&#65289;&#20013;&#24314;&#27169;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#26681;&#25454;&#30896;&#25758;&#27010;&#29575;&#30340;&#32622;&#20449;&#24230;&#23545;&#20849;&#21516;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;Dempster-Shafer&#35770;&#35777;&#30340;&#29702;&#35770;&#65292;&#20551;&#35774;&#35266;&#23519;&#21040;&#30340;CDMs&#26469;&#33258;&#19968;&#32452;&#26410;&#30693;&#20998;&#24067;&#30340;&#23478;&#26063;&#12290;&#20351;&#29992;Dvoretzky-Kiefer-Wolfowitz&#19981;&#31561;&#24335;&#20174;CDMs&#30340;&#26102;&#38388;&#24207;&#21015;&#26500;&#24314;&#40065;&#26834;&#30028;&#38480;&#12290;&#28982;&#21518;&#20351;&#29992;DKW&#19981;&#31561;&#24335;&#26500;&#24314;&#30340;&#27010;&#29575;&#31665;&#27966;&#29983;DSt&#32467;&#26500;&#12290;&#35813;DSt&#32467;&#26500;&#23553;&#35013;&#20102;&#26102;&#38388;&#24207;&#21015;&#19978;&#27599;&#20010;&#28857;&#30340;CDMs&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20801;&#35768;&#35745;&#31639;&#29305;&#23450;&#30896;&#25758;&#27010;&#29575;&#23454;&#29616;&#30340;&#32622;&#20449;&#24230;&#21644;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#20123;&#23454;&#38469;&#20107;&#20214;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#27431;&#27954;&#29616;&#26377;&#30340;&#23454;&#36341;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents an approach to the modelling of epistemic uncertainty in Conjunction Data Messages (CDM) and the classification of conjunction events according to the confidence in the probability of collision. The approach proposed in this paper is based on the Dempster-Shafer Theory (DSt) of evidence and starts from the assumption that the observed CDMs are drawn from a family of unknown distributions. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality is used to construct robust bounds on such a family of unknown distributions starting from a time series of CDMs. A DSt structure is then derived from the probability boxes constructed with DKW inequality. The DSt structure encapsulates the uncertainty in the CDMs at every point along the time series and allows the computation of the belief and plausibility in the realisation of a given probability of collision. The methodology proposed in this paper is tested on a number of real events and compared against existing practices in the Eu
&lt;/p&gt;</description></item><item><title>FengWu-GHR&#26159;&#20840;&#29699;&#39318;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#36816;&#34892;&#30340;&#20844;&#37324;&#32423;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#21644;&#21487;&#27604;&#29978;&#33267;&#26356;&#39640;&#30340;&#39044;&#25253;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.00059</link><description>&lt;p&gt;
FengWu-GHR: &#23398;&#20064;&#20844;&#37324;&#32423;&#20013;&#31243;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00059
&lt;/p&gt;
&lt;p&gt;
FengWu-GHR&#26159;&#20840;&#29699;&#39318;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#36816;&#34892;&#30340;&#20844;&#37324;&#32423;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#21644;&#21487;&#27604;&#29978;&#33267;&#26356;&#39640;&#30340;&#39044;&#25253;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#37324;&#32423;&#21035;&#30340;&#20840;&#29699;&#22823;&#27668;&#21160;&#21147;&#23398;&#24314;&#27169;&#21487;&#20197;&#23454;&#29616;&#31934;&#32454;&#21270;&#30340;&#22825;&#27668;&#39044;&#25253;&#65292;&#38477;&#20302;&#28798;&#23475;&#24615;&#22825;&#27668;&#21644;&#27668;&#20505;&#27963;&#21160;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#20844;&#37324;&#32423;&#20840;&#29699;&#39044;&#25253;&#27169;&#22411;&#26159;&#27668;&#35937;&#39046;&#22495;&#19968;&#30452;&#20197;&#26469;&#30340;&#36861;&#27714;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#65292;&#22269;&#38469;&#31038;&#20250;&#31215;&#26497;&#21162;&#21147;&#25913;&#21892;&#25968;&#20540;&#22825;&#27668;&#27169;&#22411;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#36164;&#28304;&#30340;&#28040;&#32791;&#24040;&#22823;&#65292;&#21457;&#23637;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#20540;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#21033;&#29992;&#20877;&#20998;&#26512;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20986;&#19982;&#25968;&#20540;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#30340;&#39044;&#25253;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#21463;&#38480;&#20110;&#20877;&#20998;&#26512;&#25968;&#25454;&#30340;&#20998;&#36776;&#29575;&#65292;&#26080;&#27861;&#29983;&#25104;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#39044;&#25253;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FengWu-GHR&#65292;&#36825;&#26159;&#39318;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#36816;&#34892;&#12289;0.09$^{\circ}$&#27700;&#24179;&#20998;&#36776;&#29575;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kilometer-scale modeling of global atmosphere dynamics enables fine-grained weather forecasting and decreases the risk of disastrous weather and climate activity. Therefore, building a kilometer-scale global forecast model is a persistent pursuit in the meteorology domain. Active international efforts have been made in past decades to improve the spatial resolution of numerical weather models. Nonetheless, developing the higher resolution numerical model remains a long-standing challenge due to the substantial consumption of computational resources. Recent advances in data-driven global weather forecasting models utilize reanalysis data for model training and have demonstrated comparable or even higher forecasting skills than numerical models. However, they are all limited by the resolution of reanalysis data and incapable of generating higher-resolution forecasts. This work presents FengWu-GHR, the first data-driven global weather forecasting model running at the 0.09$^{\circ}$ horizo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38543;&#26426;&#25277;&#26679;&#24102;&#26469;&#30340;&#25490;&#21517;&#25351;&#26631;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00053</link><description>&lt;p&gt;
&#25105;&#20204;&#22312;&#28010;&#36153;&#26102;&#38388;&#21527;&#65311;&#19968;&#31181;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38543;&#26426;&#25277;&#26679;&#24102;&#26469;&#30340;&#25490;&#21517;&#25351;&#26631;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#34913;&#37327;&#30693;&#35782;&#22270;&#35889;&#23436;&#21892;&#26041;&#27861;&#36136;&#37327;&#30340;&#26631;&#20934;&#35780;&#20272;&#21327;&#35758;&#36890;&#24120;&#21253;&#25324;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#27599;&#20010;&#23454;&#20307;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#20316;&#20026;&#20505;&#36873;&#38142;&#25509;&#22836;&#37096;&#25110;&#23614;&#37096;&#30340;&#36866;&#21512;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#35268;&#27169;&#36739;&#22823;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#65292;&#36825;&#20010;&#20219;&#21153;&#24456;&#24555;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#23454;&#20307;&#36827;&#34892;&#38543;&#26426;&#25277;&#26679;&#26469;&#35780;&#20272;&#39044;&#27979;&#25110;&#24314;&#35758;&#26041;&#27861;&#30340;&#38142;&#25509;&#36136;&#37327;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#20135;&#29983;&#30340;&#25490;&#21517;&#25351;&#26631;&#19981;&#27491;&#30830;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#32467;&#26524;&#12290;&#26412;&#25991;&#23545;&#36825;&#20123;&#25928;&#24212;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#35770;&#35777;&#25214;&#20986;&#20102;&#20026;&#20160;&#20040;&#38543;&#26426;&#22343;&#21248;&#25277;&#26679;&#26497;&#22823;&#22320;&#39640;&#20272;&#20102;&#26041;&#27861;&#30340;&#25490;&#21517;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;&#26131;/&#38590;&#24230;&#36127;&#20505;&#36873;&#32773;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard evaluation protocol for measuring the quality of Knowledge Graph Completion methods - the task of inferring new links to be added to a graph - typically involves a step which ranks every entity of a Knowledge Graph to assess their fit as a head or tail of a candidate link to be added. In Knowledge Graphs on a larger scale, this task rapidly becomes prohibitively heavy. Previous approaches mitigate this problem by using random sampling of entities to assess the quality of links predicted or suggested by a method. However, we show that this approach has serious limitations since the ranking metrics produced do not properly reflect true outcomes. In this paper, we present a thorough analysis of these effects along with the following findings. First, we empirically find and theoretically motivate why sampling uniformly at random vastly overestimates the ranking performance of a method. We show that this can be attributed to the effect of easy versus hard negative candidates. S
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#25512;&#29702;&#21644;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#38646;&#23556;&#20987;&#39034;&#24207;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26550;&#26500;&#21407;&#29702;&#22270;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#19987;&#23478;&#35265;&#35299;&#20381;&#36182;&#21644;&#25216;&#26415;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00052</link><description>&lt;p&gt;
&#38646;&#23556;&#20987;&#39034;&#24207;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26550;&#26500;&#21407;&#29702;&#22270;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Sequential Neuro-symbolic Reasoning for Automatically Generating Architecture Schematic Designs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00052
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#25512;&#29702;&#21644;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#38646;&#23556;&#20987;&#39034;&#24207;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26550;&#26500;&#21407;&#29702;&#22270;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#19987;&#23478;&#35265;&#35299;&#20381;&#36182;&#21644;&#25216;&#26415;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#25104;&#26088;&#22312;&#31616;&#21270;&#22810;&#25143;&#22411;&#25151;&#22320;&#20135;&#39033;&#30446;&#30340;&#22797;&#26434;&#20915;&#31574;&#30340;&#26550;&#26500;&#21407;&#29702;&#22270;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;AI&#65288;&#31070;&#32463;&#25512;&#29702;&#65289;&#21644;&#25968;&#23398;&#35268;&#21010;&#27714;&#35299;&#22120;&#65288;&#31526;&#21495;&#25512;&#29702;&#65289;&#30340;&#32508;&#21512;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#26550;&#26500;&#21407;&#29702;&#22270;&#35774;&#35745;&#20013;&#19987;&#23478;&#35265;&#35299;&#20381;&#36182;&#21644;&#25216;&#26415;&#25361;&#25112;&#20004;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#22788;&#29702;&#35774;&#35745;&#25972;&#20010;&#24314;&#31569;&#25152;&#38656;&#35201;&#30340;&#22823;&#35268;&#27169;&#21644;&#30456;&#20114;&#20851;&#32852;&#24615;&#20915;&#31574;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39034;&#24207;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#20174;&#21021;&#22987;&#27010;&#24565;&#21040;&#35814;&#32454;&#24067;&#23616;&#30340;&#20256;&#32479;&#26550;&#26500;&#35774;&#35745;&#36807;&#31243;&#12290;&#20026;&#20102;&#28040;&#38500;&#25163;&#24037;&#21046;&#20316;&#36924;&#36817;&#25152;&#38656;&#30446;&#26631;&#30340;&#25104;&#26412;&#20989;&#25968;&#30340;&#38656;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#31070;&#32463;&#25512;&#29702;&#29983;&#25104;&#31526;&#21495;&#27714;&#35299;&#22120;&#21487;&#20197;&#20351;&#29992;&#30340;&#32422;&#26463;&#21644;&#25104;&#26412;&#20989;&#25968;&#36827;&#34892;&#27714;&#35299;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#35774;&#35745;&#38454;&#27573;&#24341;&#20837;&#20102;&#21453;&#39304;&#24490;&#29615;&#65292;&#20197;&#30830;&#20445;&#35774;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#36845;&#20195;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel automated system for generating architecture schematic designs aimed at streamlining complex decision-making at the multifamily real estate development project's outset. Leveraging the combined strengths of generative AI (neuro reasoning) and mathematical program solvers (symbolic reasoning), the method addresses both the reliance on expert insights and technical challenges in architectural schematic design. To address the large-scale and interconnected nature of design decisions needed for designing a whole building, we proposed a novel sequential neuro-symbolic reasoning approach, emulating traditional architecture design processes from initial concept to detailed layout. To remove the need to hand-craft a cost function to approximate the desired objectives, we propose a solution that uses neuro reasoning to generate constraints and cost functions that the symbolic solvers can use to solve. We also incorporate feedback loops for each design stage to ensu
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;IICONGRAPH&#22635;&#34917;&#20102;&#24403;&#21069;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#20110;&#22270;&#26631;&#23398;&#21644;&#22270;&#35937;&#23398;&#38472;&#36848;&#30340;&#31354;&#32570;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#21644;&#25193;&#23637;ArCo&#21644;Wikidata&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00048</link><description>&lt;p&gt;
IICONGRAPH: &#25913;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#22270;&#26631;&#23398;&#21644;&#22270;&#35937;&#23398;&#38472;&#36848;
&lt;/p&gt;
&lt;p&gt;
IICONGRAPH: improved Iconographic and Iconological Statements in Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00048
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;IICONGRAPH&#22635;&#34917;&#20102;&#24403;&#21069;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#20110;&#22270;&#26631;&#23398;&#21644;&#22270;&#35937;&#23398;&#38472;&#36848;&#30340;&#31354;&#32570;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#21644;&#25193;&#23637;ArCo&#21644;Wikidata&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26631;&#23398;&#21644;&#22270;&#35937;&#23398;&#26159;&#29702;&#35299;&#25991;&#21270;&#36951;&#20135;&#20013;&#30340;&#33402;&#26415;&#21697;&#30340;&#22522;&#26412;&#39046;&#22495;&#12290;&#22270;&#26631;&#23398;&#28041;&#21450;&#23545;&#33402;&#26415;&#21697;&#20013;&#25551;&#32472;&#30340;&#35270;&#35273;&#20803;&#32032;&#21450;&#20854;&#35937;&#24449;&#24847;&#20041;&#30340;&#30740;&#31350;&#21644;&#35299;&#37322;&#65292;&#32780;&#22270;&#35937;&#23398;&#26356;&#28145;&#20837;&#25506;&#32034;&#20854;&#20013;&#30340;&#25991;&#21270;&#21644;&#21382;&#21490;&#24847;&#20041;&#12290;&#23613;&#31649;&#21033;&#29992;&#38142;&#25509;&#24320;&#25918;&#25968;&#25454;&#65288;LOD&#65289;&#26469;&#34920;&#31034;&#25991;&#21270;&#36951;&#20135;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#30340;&#22270;&#26631;&#23398;&#21644;&#22270;&#35937;&#23398;&#38472;&#36848;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;IICONGRAPH&#65292;&#23427;&#26159;&#36890;&#36807;&#25913;&#36827;&#21644;&#25193;&#23637;ArCo&#65288;&#24847;&#22823;&#21033;&#25991;&#21270;&#36951;&#20135;&#30693;&#35782;&#22270;&#35889;&#65289;&#21644;Wikidata&#30340;&#22270;&#26631;&#23398;&#21644;&#22270;&#35937;&#23398;&#38472;&#36848;&#32780;&#21019;&#24314;&#30340;KG&#12290;IICONGRAPH&#30340;&#24320;&#21457;&#20063;&#21463;&#21040;&#19968;&#31995;&#21015;&#30740;&#31350;&#26696;&#20363;&#30340;&#35201;&#27714;&#30340;&#39537;&#21160;&#65292;&#36825;&#20123;&#35201;&#27714;&#22312;&#38750;&#37325;&#26032;&#35774;&#35745;&#29256;&#26412;&#30340;KG&#20013;&#26080;&#27861;&#23454;&#29616;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;IICONGRAPH&#19981;&#20165;&#36229;&#36234;&#20102;ArCo&#21644;W
&lt;/p&gt;
&lt;p&gt;
Iconography and iconology are fundamental domains when it comes to understanding artifacts of cultural heritage. Iconography deals with the study and interpretation of visual elements depicted in artifacts and their symbolism, while iconology delves deeper, exploring the underlying cultural and historical meanings. Despite the advances in representing cultural heritage with Linked Open Data (LOD), recent studies show persistent gaps in the representation of iconographic and iconological statements in current knowledge graphs (KGs). To address them, this paper presents IICONGRAPH, a KG that was created by refining and extending the iconographic and iconological statements of ArCo (the Italian KG of cultural heritage) and Wikidata. The development of IICONGRAPH was also driven by a series of requirements emerging from research case studies that were unattainable in the non-reengineered versions of the KGs. The evaluation results demonstrate that IICONGRAPH not only outperforms ArCo and W
&lt;/p&gt;</description></item><item><title>PetriRL&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;Petri&#32593;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00046</link><description>&lt;p&gt;
&#24341;&#20837;PetriRL&#65306;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#30340;JSSP&#35299;&#20915;&#26041;&#26696;&#30340;&#21019;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00046
&lt;/p&gt;
&lt;p&gt;
PetriRL&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;Petri&#32593;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#36710;&#38388;&#20013;&#65292;&#20248;&#36136;&#35843;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#20854;&#22312;&#24037;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Petri&#32593;&#23545;&#20316;&#19994;&#36710;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#33021;&#30452;&#25509;&#23558;&#21407;&#22987;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#65292;&#26080;&#38656;&#23545;JSSP&#23454;&#20363;&#36827;&#34892;&#39044;&#22788;&#29702;&#25104;&#38750;&#20132;&#26367;&#22270;&#12290;Petri&#32593;&#30340;&#25511;&#21046;&#33021;&#21147;&#36824;&#21487;&#20197;&#31649;&#29702;&#36807;&#31243;&#20013;&#30340;&#33258;&#21160;&#21270;&#32452;&#20214;&#65292;&#20351;&#20195;&#29702;&#20154;&#33021;&#22815;&#19987;&#27880;&#20110;&#20851;&#38190;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#22312;&#20844;&#20849;&#27979;&#35797;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#36328;&#19968;&#31995;&#21015;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65288;&#21253;&#25324;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#23398;&#20064;&#31639;&#27861;&#65289;&#36827;&#34892;&#30340;&#27604;&#36739;&#20998;&#26512;&#31361;&#20986;&#20102;&#20854;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality scheduling in industrial job shops is crucial. Although neural networks excel in solving these problems, their limited explainability hinders their widespread industrial adoption. In this research, we introduce an innovative framework for solving job shop scheduling problems (JSSP). Our methodology leverages Petri nets to model the job shop, not only improving explainability but also enabling direct incorporation of raw data without the need to preprocess JSSP instances into disjunctive graphs. The Petri net, with its controlling capacities, also governs the automated components of the process, allowing the agent to focus on critical decision-making, particularly resource allocation. The integration of event-based control and action masking in our approach yields competitive performance on public test benchmarks. Comparative analyses across a wide spectrum of optimization solutions, including heuristics, metaheuristics, and learning-based algorithms, highlight the competitivene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00045</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting Multimedia Generated by Large AI Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#26032;&#30340;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#30410;&#65292;&#20294;&#36825;&#20123;&#20869;&#23481;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#21253;&#25324;&#28508;&#22312;&#30340;&#28389;&#29992;&#12289;&#31038;&#20250;&#30772;&#22351;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#30001;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#30456;&#20851;&#30740;&#31350;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#21363;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#19987;&#38376;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#20840;&#38754;&#28085;&#30422;&#29616;&#26377;&#30740;&#31350;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20869;&#23481;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#25353;&#23186;&#20307;&#24418;&#24335;&#20998;&#31867;&#65292;&#24182;&#19982;&#32431;&#26816;&#27979;&#65288;&#26088;&#22312;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65289;&#21644;&#24212;&#29992;&#22330;&#26223;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26234;&#33021;&#30340;&#21046;&#36896;&#19994;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#32467;&#21512;&#30005;&#21160;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#30340;&#19987;&#23478;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#24182;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.00043</link><description>&lt;p&gt;
&#21033;&#29992;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#21046;&#36896;&#19994;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20114;&#21160;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interactive and Intelligent Root Cause Analysis in Manufacturing with Causal Bayesian Networks and Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26234;&#33021;&#30340;&#21046;&#36896;&#19994;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#32467;&#21512;&#30005;&#21160;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#30340;&#19987;&#23478;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#24182;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#27773;&#36710;&#21046;&#36896;&#20043;&#20013;&#65292;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;(RCA)&#26159;&#35782;&#21035;&#25925;&#38556;&#21407;&#22240;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#19978;&#65292;RCA&#26159;&#20381;&#38752;&#36807;&#31243;&#19987;&#23478;&#30693;&#35782;&#25163;&#24037;&#36827;&#34892;&#30340;&#12290;&#21516;&#26102;&#65292;&#20256;&#24863;&#22120;&#32593;&#32476;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;RCA&#33021;&#22815;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;(&#22914;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;)&#22312;&#22823;&#35268;&#27169;&#30340;&#23454;&#38469;&#21046;&#36896;&#36807;&#31243;&#20013;&#20855;&#26377;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#22823;&#37327;&#30340;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;(CERs)&#12290;&#27492;&#22806;&#65292;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#24573;&#30053;&#24050;&#30693;&#30340;CERs&#25110;&#23398;&#20064;&#21040;&#38169;&#35823;&#30340;CERs&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26234;&#33021;&#30340;RCA&#24037;&#20855;&#65292;&#23558;&#30005;&#21160;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#30340;&#19987;&#23478;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#21046;&#36896;&#36807;&#31243;&#30340;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Root Cause Analysis (RCA) in the manufacturing of electric vehicles is the process of identifying fault causes. Traditionally, the RCA is conducted manually, relying on process expert knowledge. Meanwhile, sensor networks collect significant amounts of data in the manufacturing process. Using this data for RCA makes it more efficient. However, purely data-driven methods like Causal Bayesian Networks have problems scaling to large-scale, real-world manufacturing processes due to the vast amount of potential cause-effect relationships (CERs). Furthermore, purely data-driven methods have the potential to leave out already known CERs or to learn spurious CERs. The paper contributes by proposing an interactive and intelligent RCA tool that combines expert knowledge of an electric vehicle manufacturing process and a data-driven machine learning method. It uses reasoning over a large-scale Knowledge Graph of the manufacturing process while learning a Causal Bayesian Network. In addition, an I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20998;&#24067;&#24335;&#20915;&#31574;&#26041;&#27861;&#65292;&#29992;&#20110;&#24037;&#19994;&#35774;&#22791;&#20219;&#21153;&#20998;&#37197;&#21644;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#20581;&#24247;&#31649;&#29702;&#20915;&#31574;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#24182;&#23558;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#25968;&#23398;&#27169;&#22411;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00042</link><description>&lt;p&gt;
&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20248;&#21270;&#24037;&#19994;&#35774;&#22791;&#30340;&#20219;&#21153;&#20998;&#37197;&#19982;&#39044;&#27979;&#24615;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
Optimized Task Assignment and Predictive Maintenance for Industrial Machines using Markov Decision Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20998;&#24067;&#24335;&#20915;&#31574;&#26041;&#27861;&#65292;&#29992;&#20110;&#24037;&#19994;&#35774;&#22791;&#20219;&#21153;&#20998;&#37197;&#21644;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#20581;&#24247;&#31649;&#29702;&#20915;&#31574;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#24182;&#23558;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#25968;&#23398;&#27169;&#22411;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20915;&#31574;&#26041;&#27861;&#65292;&#29992;&#20110;&#21046;&#36896;&#20219;&#21153;&#20998;&#37197;&#21644;&#22522;&#20110;&#35774;&#22791;&#29366;&#20917;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#20581;&#24247;&#31649;&#29702;&#20915;&#31574;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#35774;&#35745;&#20915;&#31574;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#32771;&#34385;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#38469;&#25191;&#34892;&#31574;&#30053;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#38115;&#24202;&#24037;&#20855;&#36864;&#21270;&#25968;&#25454;&#30340;&#35814;&#32454;&#25968;&#20540;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25104;&#26412;&#21442;&#25968;&#36873;&#25321;&#26041;&#38754;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#20801;&#35768;&#23545;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#31163;&#32447;&#35745;&#31639;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a distributed decision-making approach for manufacturing task assignment and condition-based machine health maintenance. Our approach considers information sharing between the task assignment and health management decision-making agents. We propose the design of the decision-making agents based on Markov decision processes. The key advantage of using a Markov decision process-based approach is the incorporation of uncertainty involved in the decision-making process. The paper provides detailed mathematical models along with the associated practical execution strategy. In order to demonstrate the effectiveness and practical applicability of our proposed approach, we have included a detailed numerical case study that is based on open source milling machine tool degradation data. Our case study indicates that the proposed approach offers flexibility in terms of the selection of cost parameters and it allows for offline computation and analysis of the decision-making p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#35299;&#20915;&#22823;&#35268;&#27169;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#12289;&#26102;&#38388;&#12289;&#38656;&#27714;&#25968;&#25454;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#36335;&#24452;&#38382;&#39064;&#65292;&#28982;&#21518;&#20998;&#21035;&#35299;&#20915;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#26469;&#25913;&#36827;&#25972;&#20307;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.00041</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;-&#38656;&#27714;&#32858;&#31867;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#22823;&#35268;&#27169;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal-demand clustering for solving large-scale vehicle routing problems with time windows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00041
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#35299;&#20915;&#22823;&#35268;&#27169;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#12289;&#26102;&#38388;&#12289;&#38656;&#27714;&#25968;&#25454;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#36335;&#24452;&#38382;&#39064;&#65292;&#28982;&#21518;&#20998;&#21035;&#35299;&#20915;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#26469;&#25913;&#36827;&#25972;&#20307;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#20351;&#29992;&#20998;&#35299;&#21644;&#20462;&#21098;&#31574;&#30053;&#26469;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#30340;&#22823;&#35268;&#27169;&#23454;&#20363;&#12290;&#36825;&#20123;&#22797;&#26434;&#24615;&#20943;&#23569;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#12289;&#38382;&#39064;&#29305;&#23450;&#30340;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#21487;&#29992;&#25968;&#25454;&#30340;&#22686;&#38271;&#21644;&#35745;&#31639;&#26426;&#30828;&#20214;&#30340;&#36827;&#27493;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;-&#36335;&#24452;&#25913;&#36827;&#65288;DRI&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#32858;&#31867;&#23558;&#23458;&#25143;&#20998;&#32452;&#12290;&#20854;&#30456;&#20284;&#24230;&#24230;&#37327;&#25351;&#26631;&#21253;&#25324;&#23458;&#25143;&#30340;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#38656;&#27714;&#25968;&#25454;&#65292;&#24182;&#19988;&#34987;&#21046;&#23450;&#25104;&#21453;&#26144;&#38382;&#39064;&#30446;&#26631;&#20989;&#25968;&#21644;&#32422;&#26463;&#30340;&#24418;&#24335;&#12290;&#23548;&#33268;&#30340;&#23376;&#36335;&#24452;&#38382;&#39064;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#21512;&#36866;&#30340;&#31639;&#27861;&#29420;&#31435;&#22320;&#35299;&#20915;&#12290;&#25105;&#20204;&#22312;&#35299;&#20915;&#30340;&#23376;&#38382;&#39064;&#20043;&#38388;&#24212;&#29992;&#20462;&#21098;&#30340;&#23616;&#37096;&#25628;&#32034;&#65288;LS&#65289;&#26469;&#25913;&#36827;&#25972;&#20307;&#35299;&#12290;&#20462;&#21098;&#22522;&#20110;&#22312;&#20998;&#35299;&#38454;&#27573;&#33719;&#24471;&#30340;&#23458;&#25143;&#30456;&#20284;&#24230;&#20449;&#24687;&#12290;&#22312;&#35745;&#31639;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21442;&#25968;&#21270;&#24182;&#27604;&#36739;&#29616;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several metaheuristics use decomposition and pruning strategies to solve large-scale instances of the vehicle routing problem (VRP). Those complexity reduction techniques often rely on simple, problem-specific rules. However, the growth in available data and advances in computer hardware enable data-based approaches that use machine learning (ML) to improve scalability of solution algorithms. We propose a decompose-route-improve (DRI) framework that groups customers using clustering. Its similarity metric incorporates customers' spatial, temporal, and demand data and is formulated to reflect the problem's objective function and constraints. The resulting sub-routing problems can independently be solved using any suitable algorithm. We apply pruned local search (LS) between solved subproblems to improve the overall solution. Pruning is based on customers' similarity information obtained in the decomposition phase. In a computational study, we parameterize and compare existing clustering
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;STEM&#22242;&#38431;&#20013;&#20419;&#36827;&#22810;&#26679;&#24615;&#19982;&#21253;&#23481;&#24615;&#30340;&#28508;&#21147;&#12290;&#20027;&#35201;&#36890;&#36807;&#21253;&#23481;&#24615;&#20998;&#26512;&#21644;&#20010;&#24615;&#21270;&#30340;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23454;&#29616;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#22235;&#20010;&#25919;&#31574;&#24314;&#35758;&#65292;&#21253;&#25324;&#35268;&#33539;&#21270;&#21512;&#20316;&#35780;&#20272;&#12289;&#21253;&#23481;&#24615;&#20998;&#26512;&#12289;&#20026;&#31038;&#20250;&#35748;&#30693;&#30740;&#31350;&#31609;&#38598;&#36164;&#37329;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#36827;&#34892;&#21253;&#23481;&#24615;&#22521;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.00037</link><description>&lt;p&gt;
&#22312;STEM&#22242;&#38431;&#20013;&#20419;&#36827;&#20844;&#24179;&#65306;&#21033;&#29992;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#21253;&#23481;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Catalyzing Equity in STEM Teams: Harnessing Generative AI for Inclusion and Diversity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;STEM&#22242;&#38431;&#20013;&#20419;&#36827;&#22810;&#26679;&#24615;&#19982;&#21253;&#23481;&#24615;&#30340;&#28508;&#21147;&#12290;&#20027;&#35201;&#36890;&#36807;&#21253;&#23481;&#24615;&#20998;&#26512;&#21644;&#20010;&#24615;&#21270;&#30340;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23454;&#29616;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#22235;&#20010;&#25919;&#31574;&#24314;&#35758;&#65292;&#21253;&#25324;&#35268;&#33539;&#21270;&#21512;&#20316;&#35780;&#20272;&#12289;&#21253;&#23481;&#24615;&#20998;&#26512;&#12289;&#20026;&#31038;&#20250;&#35748;&#30693;&#30740;&#31350;&#31609;&#38598;&#36164;&#37329;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#36827;&#34892;&#21253;&#23481;&#24615;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#26159;STEM&#30340;&#20851;&#38190;&#65292;&#22810;&#23398;&#31185;&#22242;&#38431;&#30740;&#31350;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;STEM&#39046;&#22495;&#30340;&#19981;&#24179;&#31561;&#38459;&#30861;&#20102;&#20854;&#20805;&#20998;&#21457;&#23637;&#65292;&#22240;&#20026;&#22312;&#20844;&#24179;&#19982;&#21253;&#23481;&#22242;&#38431;&#20013;&#23384;&#22312;&#30528;&#25345;&#32493;&#30340;&#24515;&#29702;&#38556;&#30861;&#12290;&#26412;&#25991;&#35760;&#24405;&#20102;STEM&#22242;&#38431;&#21512;&#20316;&#65292;&#24182;&#25506;&#35752;&#20102;&#35745;&#31639;&#24314;&#27169;&#21644;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#20419;&#36827;STEM&#22242;&#38431;&#22810;&#26679;&#24615;&#21644;&#21253;&#23481;&#24615;&#26041;&#38754;&#30340;&#36716;&#21464;&#28508;&#21147;&#12290;&#21033;&#29992;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65292;&#26412;&#25991;&#27010;&#36848;&#20102;&#20004;&#20010;&#25512;&#36827;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#19982;&#21253;&#23481;&#30340;&#20027;&#35201;&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21253;&#23481;&#24615;&#20998;&#26512;&#65292;&#35268;&#33539;&#21270;&#21512;&#20316;&#35780;&#20272;&#21487;&#20197;&#25429;&#25417;&#32454;&#31890;&#24230;&#23398;&#20064;&#32773;&#34892;&#20026;&#12290;&#20854;&#27425;&#65292;&#20010;&#24615;&#21270;&#30340;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#25903;&#25345;STEM&#22242;&#38431;&#30340;&#22810;&#26679;&#24615;&#19982;&#21253;&#23481;&#24615;&#12290;&#22235;&#20010;&#25919;&#31574;&#24314;&#35758;&#31361;&#26174;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#65306;&#35268;&#33539;&#21270;&#21512;&#20316;&#25216;&#33021;&#35780;&#20272;&#12289;&#21253;&#23481;&#24615;&#20998;&#26512;&#12289;&#20026;&#31038;&#20250;&#35748;&#30693;&#30740;&#31350;&#31609;&#38598;&#36164;&#37329;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#36827;&#34892;&#21253;&#23481;&#24615;&#22521;&#35757;&#12290;&#30740;&#31350;&#20154;&#21592;&#12289;&#25945;&#32946;&#32773;&#12289;&#20915;&#31574;&#32773;&#21487;&#20197;&#20849;&#21516;&#26500;&#24314;&#19968;&#20010;&#20844;&#24179;&#30340;STEM&#29983;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaboration is key to STEM, where multidisciplinary team research can solve complex problems. However, inequality in STEM fields hinders their full potential, due to persistent psychological barriers in underrepresented students' experience. This paper documents teamwork in STEM and explores the transformative potential of computational modeling and generative AI in promoting STEM-team diversity and inclusion. Leveraging generative AI, this paper outlines two primary areas for advancing diversity, equity, and inclusion. First, formalizing collaboration assessment with inclusive analytics can capture fine-grained learner behavior. Second, adaptive, personalized AI systems can support diversity and inclusion in STEM teams. Four policy recommendations highlight AI's capacity: formalized collaborative skill assessment, inclusive analytics, funding for socio-cognitive research, human-AI teaming for inclusion training. Researchers, educators, policymakers can build an equitable STEM ecosys
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#20113;&#25925;&#38556;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#27491;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#27491;&#26679;&#26412;&#23398;&#20064;&#39118;&#38505;&#20272;&#31639;&#22120;&#65288;Uptake&#65289;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#21518;&#39044;&#27979;&#20934;&#30830;&#24615;&#21487;&#33021;&#19979;&#38477;&#32422;9%&#12290;</title><link>https://arxiv.org/abs/2402.00034</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#39044;&#27979;&#20934;&#30830;&#24615;&#20250;&#38543;&#26102;&#38388;&#38477;&#20302;&#65311;&#20113;&#25925;&#38556;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#27491;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Why does Prediction Accuracy Decrease over Time? Uncertain Positive Learning for Cloud Failure Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#20113;&#25925;&#38556;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#27491;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#27491;&#26679;&#26412;&#23398;&#20064;&#39118;&#38505;&#20272;&#31639;&#22120;&#65288;Uptake&#65289;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#21518;&#39044;&#27979;&#20934;&#30830;&#24615;&#21487;&#33021;&#19979;&#38477;&#32422;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20113;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21508;&#31181;&#36719;&#20214;&#26381;&#21153;&#34987;&#37096;&#32626;&#22312;&#20113;&#20013;&#12290;&#20026;&#20102;&#20445;&#35777;&#20113;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25925;&#38556;&#23454;&#20363;&#65288;&#30913;&#30424;&#12289;&#33410;&#28857;&#21644;&#20132;&#25442;&#26426;&#31561;&#65289;&#30340;&#39044;&#27979;&#12290;&#19968;&#26086;&#39044;&#27979;&#30340;&#36755;&#20986;&#32467;&#26524;&#20026;&#27491;&#65292;&#23601;&#20250;&#37319;&#21462;&#24212;&#23545;&#25514;&#26045;&#24555;&#36895;&#35299;&#20915;&#28508;&#22312;&#25925;&#38556;&#12290;&#22312;&#25105;&#20204;&#22312;Microsoft Azure&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20043;&#21518;&#65292;&#39044;&#27979;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#19979;&#38477;&#32422;9%&#12290;&#32771;&#34385;&#21040;&#24212;&#23545;&#25514;&#26045;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#30830;&#23450;&#30340;&#27491;&#23454;&#20363;&#65292;&#22240;&#20026;&#22312;&#24212;&#23545;&#20043;&#21518;&#26080;&#27861;&#39564;&#35777;&#23427;&#20204;&#65292;&#36825;&#21487;&#33021;&#22312;&#26356;&#26032;&#39044;&#27979;&#27169;&#22411;&#26102;&#24341;&#20837;&#26356;&#22810;&#22122;&#38899;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22312;&#23454;&#38469;&#20113;&#25925;&#38556;&#39044;&#27979;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#21457;&#29616;&#36825;&#20010;&#19981;&#30830;&#23450;&#27491;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#27491;&#26679;&#26412;&#23398;&#20064;&#39118;&#38505;&#20272;&#31639;&#22120;&#65288;Uptake&#65289;&#26041;&#27861;&#12290;&#20351;&#29992;&#20004;&#20010;&#23454;&#38469;&#30340;&#30913;&#30424;&#25925;&#38556;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of cloud computing, a variety of software services have been deployed in the cloud. To ensure the reliability of cloud services, prior studies focus on failure instance (disk, node, and switch, etc.) prediction. Once the output of prediction is positive, mitigation actions are taken to rapidly resolve the underlying failure. According to our real-world practice in Microsoft Azure, we find that the prediction accuracy may decrease by about 9% after retraining the models. Considering that the mitigation actions may result in uncertain positive instances since they cannot be verified after mitigation, which may introduce more noise while updating the prediction model. To the best of our knowledge, we are the first to identify this Uncertain Positive Learning (UPLearning) issue in the real-world cloud failure prediction scenario. To tackle this problem, we design an Uncertain Positive Learning Risk Estimator (Uptake) approach. Using two real-world datasets of disk fai
&lt;/p&gt;</description></item><item><title>LF-ViT&#27169;&#22411;&#36890;&#36807;&#23616;&#37096;&#21270;&#21644;&#32858;&#28966;&#30340;&#26041;&#24335;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#22270;&#20687;&#35782;&#21035;&#25928;&#29575;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;Deit-S&#27169;&#22411;&#30340;FLOPs&#12290;</title><link>https://arxiv.org/abs/2402.00033</link><description>&lt;p&gt;
LF-ViT: &#20026;&#25552;&#39640;&#22270;&#20687;&#35782;&#21035;&#25928;&#29575;&#20943;&#23569;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#30340;&#31354;&#38388;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00033
&lt;/p&gt;
&lt;p&gt;
LF-ViT&#27169;&#22411;&#36890;&#36807;&#23616;&#37096;&#21270;&#21644;&#32858;&#28966;&#30340;&#26041;&#24335;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#22270;&#20687;&#35782;&#21035;&#25928;&#29575;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;Deit-S&#27169;&#22411;&#30340;FLOPs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26102;&#20855;&#26377;&#20986;&#33394;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#38754;&#20020;&#26174;&#33879;&#30340;&#31354;&#38388;&#20887;&#20313;&#25361;&#25112;&#65292;&#23548;&#33268;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#22686;&#21152;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#37096;&#21270;&#21644;&#32858;&#28966;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;LF-ViT&#65289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#21066;&#20943;&#35745;&#31639;&#38656;&#27714;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#26041;&#24335;&#36827;&#34892;&#25805;&#20316;&#12290;&#22312;&#23616;&#37096;&#21270;&#38454;&#27573;&#65292;&#25105;&#20204;&#22788;&#29702;&#38477;&#20302;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65307;&#22914;&#26524;&#20173;&#26080;&#27861;&#24471;&#20986;&#26126;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#37319;&#29992;&#21019;&#26032;&#30340;&#37051;&#36817;&#20840;&#23616;&#31867;&#21035;&#27880;&#24847;&#21147;&#65288;NGCA&#65289;&#26426;&#21046;&#65292;&#26681;&#25454;&#21021;&#27493;&#21457;&#29616;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#31361;&#20986;&#29305;&#23450;&#31867;&#21035;&#21306;&#22495;&#12290;&#38543;&#21518;&#65292;&#22312;&#32858;&#28966;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#21407;&#22987;&#22270;&#20687;&#20013;&#30340;&#25351;&#23450;&#21306;&#22495;&#26469;&#22686;&#24378;&#35782;&#21035;&#32467;&#26524;&#12290;&#29420;&#29305;&#30340;&#26159;&#65292;LF-ViT&#22312;&#20004;&#20010;&#38454;&#27573;&#37117;&#20351;&#29992;&#19968;&#33268;&#30340;&#21442;&#25968;&#65292;&#30830;&#20445;&#26080;&#32541;&#30340;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#35777;&#23454;&#20102;LF-ViT&#30340;&#23041;&#21147;&#65306;&#23427;&#23558;Deit-S&#30340;FLOPs&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22242;&#38431;&#24418;&#25104;&#20043;&#21069;&#30340;&#38431;&#21592;&#25216;&#33021;&#25968;&#25454;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#39044;&#27979;&#22242;&#38431;&#34920;&#29616;&#65292;&#22635;&#34917;&#20102;FIRST Robotics Competition&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.00031</link><description>&lt;p&gt;
FIRST Robotics Competition&#20013;&#22242;&#38431;&#32452;&#24314;&#21644;&#20896;&#20891;&#39044;&#27979;&#30340;&#32508;&#21512;&#26694;&#26550;&#65306;&#27169;&#22411;&#65292;&#31639;&#27861;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Integrated Framework for Team Formation and Winner Prediction in the FIRST Robotics Competition: Model, Algorithm, and Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22242;&#38431;&#24418;&#25104;&#20043;&#21069;&#30340;&#38431;&#21592;&#25216;&#33021;&#25968;&#25454;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#39044;&#27979;&#22242;&#38431;&#34920;&#29616;&#65292;&#22635;&#34917;&#20102;FIRST Robotics Competition&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#22522;&#20110;&#22242;&#38431;&#25104;&#21592;&#25216;&#33021;&#30340;&#25968;&#25454;&#65292;&#20248;&#21270;&#22242;&#38431;&#32452;&#24314;&#21644;&#39044;&#27979;&#22242;&#38431;&#34920;&#29616;&#65292;&#20197;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#26368;&#22823;&#21270;&#22242;&#38431;&#25928;&#33021;&#12290;&#30446;&#21069;&#65292;&#31185;&#23398;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#20248;&#21270;&#22242;&#38431;&#34920;&#29616;&#21644;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#20010;&#20307;&#25104;&#21592;&#30340;&#32454;&#31890;&#24230;&#25216;&#33021;&#32479;&#35745;&#25110;&#22242;&#38431;&#25104;&#21592;&#32452;&#21512;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#28041;&#21450;&#21040;&#39640;&#24230;&#32422;&#26463;&#30340;FIRST Robotics Competition&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#20801;&#35768;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#21033;&#29992;&#20808;&#21069;&#22242;&#38431;&#34920;&#29616;&#30340;&#25351;&#26631;&#26469;&#20248;&#21270;&#21644;&#39044;&#27979;&#22242;&#38431;&#34920;&#29616;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;FIRST Robotics&#31454;&#36187;&#30340;&#36873;&#31168;&#36807;&#31243;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#25216;&#33021;&#27599;&#24180;&#25913;&#21464;&#65292;&#22242;&#38431;&#25104;&#21592;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research work aims to develop an analytical approach for optimizing team formation and predicting team performance in a competitive environment based on data on the competitors' skills prior to the team formation. There are several approaches in scientific literature to optimize and predict a team's performance. However, most studies employ fine-grained skill statistics of the individual members or constraints such as teams with a set group of members. Currently, no research tackles the highly constrained domain of the FIRST Robotics Competition. This research effort aims to fill this gap by providing an analytical method for optimizing and predicting team performance in a competitive environment while allowing these constraints and only using metrics on previous team performance, not on each individual member's performance. We apply our method to the drafting process of the FIRST Robotics competition, a domain in which the skills change year-over-year, team members change through
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#21019;&#36896;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#22312;&#19968;&#20010;&#30001;&#33258;&#28982;&#36873;&#25321;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#27604;&#20154;&#31867;&#26356;&#26089;&#36827;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#36827;&#21270;&#24341;&#23548;&#33719;&#24471;&#36229;&#32423;&#26234;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00030</link><description>&lt;p&gt;
&#36827;&#21270;&#24341;&#23548;&#27169;&#25311;: &#20154;&#24037;&#26234;&#33021;&#36824;&#26159;&#20154;&#31867;&#26234;&#33021;&#65306;&#21738;&#20010;&#20808;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
Evolution-Bootstrapped Simulation: Artificial or Human Intelligence: Which Came First?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00030
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21019;&#36896;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#22312;&#19968;&#20010;&#30001;&#33258;&#28982;&#36873;&#25321;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#27604;&#20154;&#31867;&#26356;&#26089;&#36827;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#36827;&#21270;&#24341;&#23548;&#33719;&#24471;&#36229;&#32423;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21019;&#36896;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#32780;&#19981;&#26159;&#30456;&#21453;&#12290; &#36825;&#20010;&#38472;&#36848;&#30475;&#20284;&#26174;&#32780;&#26131;&#35265;&#12290; &#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20915;&#23450;&#25361;&#25112;&#36825;&#20010;&#38472;&#36848;&#65292;&#20197;&#19968;&#31181;&#23567;&#32780;&#36731;&#26494;&#30340;&#8220;Gedankenexperiment&#8221;&#12290; &#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#22312;&#19968;&#20010;&#30001;&#33258;&#28982;&#36873;&#25321;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#25110;&#20154;&#31867;&#26356;&#26377;&#21487;&#33021;&#39318;&#20808;&#36827;&#21270;&#65311; &#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#32773;&#30340;Solomonoff-Kolmogorov-Chaitin&#22797;&#26434;&#24615;&#65292;&#24182;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#65288;&#29978;&#33267;&#26159;LLMs&#65289;&#27604;&#20154;&#31867;&#35201;&#31616;&#21333;&#24471;&#22810;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#22768;&#31216;&#65292;&#24182;&#19981;&#38656;&#35201;&#20219;&#20309;&#22797;&#26434;&#30340;&#20154;&#36896;&#35774;&#22791;&#23384;&#22312;&#25165;&#33021;&#26377;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#22312;&#12290; &#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#26089;&#20110;&#20154;&#31867;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#21270;&#23398;&#21453;&#24212;&#25110;&#37238;&#21453;&#24212;&#35745;&#31639;&#30340;&#33258;&#28982;&#23384;&#22312;&#30340;&#23545;&#35937;&#32780;&#36827;&#21270;&#12290; &#29616;&#22312;&#25105;&#20204;&#30693;&#36947;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#65292;&#36824;&#24576;&#30097;&#23427;&#20204;&#21487;&#33021;&#20855;&#22791;&#36229;&#32423;&#26234;&#33021;&#65292;&#25105;&#20204;&#38382;&#33021;&#21542;&#20174;&#32431;&#31929;&#30340;&#33258;&#28982;&#36873;&#25321;&#36827;&#21270;&#21040;&#36890;&#36807;&#36827;&#21270;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have created artificial intelligence (AI), not the other way around. This statement is deceptively obvious. In this note, we decided to challenge this statement as a small, lighthearted Gedankenexperiment. We ask a simple question: in a world driven by evolution by natural selection, would neural networks or humans be likely to evolve first? We compare the Solomonoff--Kolmogorov--Chaitin complexity of the two and find neural networks (even LLMs) to be significantly simpler than humans. Further, we claim that it is unnecessary for any complex human-made equipment to exist for there to be neural networks. Neural networks may have evolved as naturally occurring objects before humans did as a form of chemical reaction-based or enzyme-based computation. Now that we know that neural networks can pass the Turing test and suspect that they may be capable of superintelligence, we ask whether the natural evolution of neural networks could lead from pure evolution by natural selection to w
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65292;&#30740;&#31350;&#21457;&#29616;&#32654;&#22269;&#20844;&#20247;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#20849;&#20139;&#21644;&#23545;&#27604;&#30340;&#35266;&#28857;&#65292;&#36825;&#20026;&#24320;&#21457;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#32771;&#34385;&#20010;&#20307;&#24046;&#24322;&#21644;&#32676;&#20307;&#25991;&#21270;&#35266;&#28857;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2402.00029</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#25506;&#32034;&#20844;&#20247;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#30475;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Public Opinion on Responsible AI Through The Lens of Cultural Consensus Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00029
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65292;&#30740;&#31350;&#21457;&#29616;&#32654;&#22269;&#20844;&#20247;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#20849;&#20139;&#21644;&#23545;&#27604;&#30340;&#35266;&#28857;&#65292;&#36825;&#20026;&#24320;&#21457;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#32771;&#34385;&#20010;&#20307;&#24046;&#24322;&#21644;&#32676;&#20307;&#25991;&#21270;&#35266;&#28857;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#31038;&#20250;&#24433;&#21709;&#19981;&#26029;&#22686;&#38271;&#65292;&#36861;&#27714;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#38656;&#35201;&#20844;&#20247;&#21442;&#19982;&#20854;&#21457;&#23637;&#21644;&#27835;&#29702;&#36807;&#31243;&#12290;&#36825;&#31181;&#21442;&#19982;&#23545;&#20110;&#25429;&#25417;&#21508;&#31181;&#35266;&#28857;&#21644;&#20419;&#36827;&#20844;&#24179;&#23454;&#36341;&#21644;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24212;&#29992;&#25991;&#21270;&#20849;&#35782;&#29702;&#35770;&#65288;CCT&#65289;&#23545;&#19968;&#20221;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21508;&#20010;&#26041;&#38754;&#30340;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#32654;&#22269;&#20851;&#20110;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24565;&#21644;&#24577;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36890;&#36807;&#35782;&#21035;&#20849;&#20139;&#21644;&#23545;&#27604;&#30340;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#35266;&#28857;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;&#27492;&#22806;&#65292;&#22312;&#21046;&#23450;&#37325;&#35201;&#20915;&#31574;&#21644;&#35299;&#20915;&#20844;&#20247;&#20851;&#20999;&#26102;&#65292;&#36825;&#20123;&#21457;&#29616;&#21487;&#20316;&#20026;&#24320;&#21457;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#30340;&#20851;&#38190;&#21442;&#32771;&#28857;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#32771;&#34385;&#20010;&#20307;&#24046;&#24322;&#21644;&#32676;&#20307;&#25991;&#21270;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the societal implications of Artificial Intelligence (AI) continue to grow, the pursuit of responsible AI necessitates public engagement in its development and governance processes. This involvement is crucial for capturing diverse perspectives and promoting equitable practices and outcomes. We applied Cultural Consensus Theory (CCT) to a nationally representative survey dataset on various aspects of AI to discern beliefs and attitudes about responsible AI in the United States. Our results offer valuable insights by identifying shared and contrasting views on responsible AI. Furthermore, these findings serve as critical reference points for developers and policymakers, enabling them to more effectively consider individual variances and group-level cultural perspectives when making significant decisions and addressing the public's concerns.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;W4A16&#37327;&#21270;&#25512;&#26029;&#30340;Triton&#34701;&#21512;&#20869;&#26680;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;SplitK&#24037;&#20316;&#20998;&#35299;&#23454;&#29616;&#35299;&#37327;&#21270;&#21644;GEMM&#25805;&#20316;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#30246;&#30697;&#38453;&#20056;&#27861;&#30340;&#25191;&#34892;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.00025</link><description>&lt;p&gt;
&#20351;&#29992;SplitK&#24037;&#20316;&#20998;&#35299;&#21152;&#36895;W4A16&#37327;&#21270;&#25512;&#26029;&#30340;Triton&#34701;&#21512;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK work decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;W4A16&#37327;&#21270;&#25512;&#26029;&#30340;Triton&#34701;&#21512;&#20869;&#26680;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;SplitK&#24037;&#20316;&#20998;&#35299;&#23454;&#29616;&#35299;&#37327;&#21270;&#21644;GEMM&#25805;&#20316;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#30246;&#30697;&#38453;&#20056;&#27861;&#30340;&#25191;&#34892;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#34701;&#21512;&#30697;&#38453;&#20056;&#27861;&#20869;&#26680;&#30340;&#23454;&#29616;&#65292;&#29992;&#20110;W4A16&#37327;&#21270;&#25512;&#26029;&#65292;&#22312;&#34701;&#21512;&#20869;&#26680;&#20013;&#25191;&#34892;&#35299;&#37327;&#21270;&#21644;GEMM&#25805;&#20316;&#65292;&#24182;&#20351;&#29992;SplitK&#24037;&#20316;&#20998;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#23545;&#20110;&#22522;&#20110;foundation&#27169;&#22411;&#25512;&#26029;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#30246;&#30697;&#38453;&#20056;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#30246;&#28608;&#27963;&#30697;&#38453;&#21644;&#26041;&#24418;&#26435;&#37325;&#30697;&#38453;&#20043;&#38388;&#30340;&#30697;&#38453;&#20056;&#27861;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#31995;&#21015;&#30697;&#38453;&#32500;&#24230;&#65288;&#21253;&#25324;llama-style&#27169;&#22411;&#20013;&#30340;&#32500;&#24230;&#65292;&#20854;&#20013;m &lt; n = k&#65289;&#19978;&#65292;A100&#30340;&#24179;&#22343;&#36895;&#24230;&#25552;&#21319;&#20102;65&#65285;&#65292;H100&#30340;&#24179;&#22343;&#36895;&#24230;&#25552;&#21319;&#20102;124&#65285;&#65288;&#23792;&#20540;&#20026;295&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an implementation of an efficient fused matrix multiplication kernel for W4A16 quantized inference, where we perform dequantization and GEMM in a fused kernel using a SplitK work decomposition. Our implementation shows improvement for the type of skinny matrix-matrix multiplications found in foundation model inference workloads. In particular, this paper surveys the type of matrix multiplication between a skinny activation matrix and a square weight matrix. Our results show an average of 65% speed improvement on A100, and an average of 124% speed improvement on H100 (with a peak of 295%) for a range of matrix dimensions including those found in a llama-style model, where m &lt; n = k.
&lt;/p&gt;</description></item><item><title>LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.00024</link><description>&lt;p&gt;
LLaMA&#21644;ChatGPT&#23884;&#20837;&#22312;&#20998;&#23376;&#23884;&#20837;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00024
&lt;/p&gt;
&lt;p&gt;
LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#37322;Simplified Molecular Input Line Entry System (SMILES)&#26041;&#38754;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;SMILES&#23383;&#31526;&#20018;&#35299;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20026;&#29702;&#35299;&#21270;&#23398;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#22312;&#23884;&#20837;SMILES&#23383;&#31526;&#20018;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#65306;&#20998;&#23376;&#24615;&#36136;&#65288;MP&#65289;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#65292;&#36825;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;LLaMA&#29983;&#25104;&#30340;SMILES&#23884;&#20837;&#22312;MP&#21644;DDI&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;LLaMA&#30340;SMILES&#23884;&#20837;&#22312;&#36825;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#24212;&#29992;LLMs&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;SMILES&#36827;&#34892;&#23884;&#20837;&#26041;&#38754;&#65292;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#19982;&#23612;&#26085;&#21033;&#20122;&#25919;&#24220;&#21512;&#20316;&#37096;&#32626;&#20102;ADVISE&#65306;AI-&#39537;&#21160;&#30340;&#30123;&#33495;&#25509;&#31181;&#24178;&#39044;&#20248;&#21270;&#22120;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#25104;&#21151;&#30123;&#33495;&#25509;&#31181;&#30340;&#32047;&#35745;&#27010;&#29575;&#65292;&#22312;&#23612;&#26085;&#21033;&#20122;&#26159;&#39318;&#27425;&#25104;&#21151;&#37096;&#32626;&#30340;AI&#24037;&#20855;&#38142;&#12290;</title><link>https://arxiv.org/abs/2402.00017</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#23612;&#26085;&#21033;&#20122;&#20799;&#31461;&#30123;&#33495;&#25509;&#31181;&#29575;&#30340;ADVISE&#24037;&#20855;&#30340;&#37096;&#32626;&#65306;&#24433;&#21709;&#19982;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Deploying ADVISER: Impact and Lessons from Using Artificial Intelligence for Child Vaccination Uptake in Nigeria
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00017
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#19982;&#23612;&#26085;&#21033;&#20122;&#25919;&#24220;&#21512;&#20316;&#37096;&#32626;&#20102;ADVISE&#65306;AI-&#39537;&#21160;&#30340;&#30123;&#33495;&#25509;&#31181;&#24178;&#39044;&#20248;&#21270;&#22120;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#25104;&#21151;&#30123;&#33495;&#25509;&#31181;&#30340;&#32047;&#35745;&#27010;&#29575;&#65292;&#22312;&#23612;&#26085;&#21033;&#20122;&#26159;&#39318;&#27425;&#25104;&#21151;&#37096;&#32626;&#30340;AI&#24037;&#20855;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#26377;500&#22810;&#19975;&#20116;&#23681;&#20197;&#19979;&#30340;&#20799;&#31461;&#27515;&#20110;&#21487;&#39044;&#38450;&#25110;&#21487;&#27835;&#30103;&#30340;&#21307;&#30103;&#29366;&#20917;&#65292;&#20854;&#20013;&#32477;&#22823;&#37096;&#20998;&#27515;&#20129;&#21457;&#29983;&#22312;&#25509;&#31181;&#30123;&#33495;&#29575;&#20302;&#30340;&#27424;&#21457;&#36798;&#22269;&#23478;&#12290;&#32852;&#21512;&#22269;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#20043;&#19968;&#65288;SDG 3&#65289;&#26088;&#22312;&#32467;&#26463;&#26032;&#29983;&#20799;&#21644;&#20116;&#23681;&#20197;&#19979;&#20799;&#31461;&#30340;&#21487;&#39044;&#38450;&#27515;&#20129;&#12290;&#25105;&#20204;&#20851;&#27880;&#23612;&#26085;&#21033;&#20122;&#65292;&#35813;&#22269;&#23156;&#20799;&#27515;&#20129;&#29575;&#20196;&#20154;&#38663;&#24778;&#12290;&#29305;&#21035;&#26159;&#22312;&#23612;&#26085;&#21033;&#20122;&#65292;&#20302;&#30123;&#33495;&#25509;&#31181;&#29575;&#26159;&#27599;&#22825;&#23548;&#33268;2000&#22810;&#21517;&#20116;&#23681;&#20197;&#19979;&#20799;&#31461;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#19982;&#23612;&#26085;&#21033;&#20122;&#25919;&#24220;&#21512;&#20316;&#37096;&#32626;ADVISE&#65306;AI-&#39537;&#21160;&#30340;&#30123;&#33495;&#25509;&#31181;&#24178;&#39044;&#20248;&#21270;&#22120;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#25104;&#21151;&#30123;&#33495;&#25509;&#31181;&#30340;&#32047;&#35745;&#27010;&#29575;&#65292;&#22312;&#23612;&#26085;&#21033;&#20122;&#20248;&#21270;&#21355;&#29983;&#24178;&#39044;&#37197;&#32622;&#26041;&#38754;&#26159;&#39318;&#27425;&#25104;&#21151;&#37096;&#32626;AI&#24037;&#20855;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
More than 5 million children under five years die from largely preventable or treatable medical conditions every year, with an overwhelmingly large proportion of deaths occurring in underdeveloped countries with low vaccination uptake. One of the United Nations' sustainable development goals (SDG 3) aims to end preventable deaths of newborns and children under five years of age. We focus on Nigeria, where the rate of infant mortality is appalling. In particular, low vaccination uptake in Nigeria is a major driver of more than 2,000 daily deaths of children under the age of five years. In this paper, we describe our collaboration with government partners in Nigeria to deploy ADVISER: AI-Driven Vaccination Intervention Optimiser. The framework, based on an integer linear program that seeks to maximize the cumulative probability of successful vaccination, is the first successful deployment of an AI-enabled toolchain for optimizing the allocation of health interventions in Nigeria. In this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#38454;&#27573;&#37117;&#20351;&#29992;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#32500;&#25252;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#23545;&#25512;&#29702;&#32467;&#26524;&#30340;&#26377;&#20449;&#24515;&#30340;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#21360;&#24230;&#30340;&#25968;&#21315;&#21517;&#26825;&#20892;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#20854;&#20182;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#12290;</title><link>https://arxiv.org/abs/2402.00015</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#38454;&#27573;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25512;&#26029;&#26469;&#32500;&#25252;&#29992;&#25143;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Maintaining User Trust Through Multistage Uncertainty Aware Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#38454;&#27573;&#37117;&#20351;&#29992;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#32500;&#25252;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#23545;&#25512;&#29702;&#32467;&#26524;&#30340;&#26377;&#20449;&#24515;&#30340;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#21360;&#24230;&#30340;&#25968;&#21315;&#21517;&#26825;&#20892;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#20854;&#20182;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#26041;&#27861;&#12290;&#27599;&#20010;&#38454;&#27573;&#37117;&#28041;&#21450;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20294;&#27599;&#20010;&#38454;&#27573;&#30340;&#24320;&#38144;&#20063;&#36880;&#28176;&#22686;&#21152;&#12290;&#22312;&#27010;&#36848;&#26550;&#26500;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#26377;&#20449;&#24515;&#30340;&#24310;&#26399;&#20915;&#31574;&#12290;&#35813;&#26550;&#26500;&#30446;&#21069;&#27491;&#22312;&#21360;&#24230;&#30340;&#25968;&#21315;&#21517;&#26825;&#20892;&#20013;&#36827;&#34892;&#31215;&#26497;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26356;&#24191;&#27867;&#30340;&#24819;&#27861;&#20063;&#36866;&#29992;&#20110;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes and evaluates a multistage approach to AI deployment. Each stage involves a more accurate method of inference, yet engaging each comes with an increasing cost. In outlining the architecture, we present a method for quantifying model uncertainty that facilitates confident deferral decisions. The architecture is currently under active deployment to thousands of cotton farmers across India. The broader idea however is applicable to a growing sector of AI deployments in challenging low resources settings.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#23458;&#39532;&#25289;&#26494;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#22312;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.17434</link><description>&lt;p&gt;
&#22312;&#40657;&#23458;&#39532;&#25289;&#26494;&#20013;&#38598;&#25104;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;: &#26426;&#36935;&#65292;&#25361;&#25112;&#21644;&#25945;&#32946;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17434
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#23458;&#39532;&#25289;&#26494;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#22312;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#23458;&#39532;&#25289;&#26494;&#21644;&#36719;&#20214;&#31454;&#36187;&#22312;&#36719;&#20214;&#34892;&#19994;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#23427;&#20204;&#23545;&#32452;&#32455;&#21644;&#23398;&#29983;&#30340;&#21019;&#26032;&#21644;&#25216;&#33021;&#21457;&#23637;&#36215;&#21040;&#37325;&#35201;&#25512;&#21160;&#20316;&#29992;&#12290;&#36825;&#20123;&#24179;&#21488;&#20351;&#20844;&#21496;&#33021;&#22815;&#36805;&#36895;&#21407;&#22411;&#21270;&#24819;&#27861;&#65292;&#32780;&#23398;&#29983;&#21017;&#33719;&#24471;&#20016;&#23500;&#30340;&#23398;&#20064;&#32463;&#39564;&#65292;&#22686;&#24378;&#20182;&#20204;&#30340;&#23454;&#36341;&#25216;&#33021;&#12290;&#22810;&#24180;&#26469;&#65292;&#40657;&#23458;&#39532;&#25289;&#26494;&#24050;&#32463;&#20174;&#31616;&#21333;&#30340;&#31454;&#20105;&#27963;&#21160;&#36716;&#21464;&#20026;&#37325;&#35201;&#30340;&#25945;&#32946;&#24037;&#20855;&#65292;&#23558;&#29702;&#35770;&#30693;&#35782;&#19982;&#23454;&#38469;&#38382;&#39064;&#35299;&#20915;&#30456;&#32467;&#21512;&#12290;&#23558;&#40657;&#23458;&#39532;&#25289;&#26494;&#32435;&#20837;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#36719;&#20214;&#24037;&#31243;&#35838;&#31243;&#30340;&#25972;&#21512;&#26088;&#22312;&#22312;&#21512;&#20316;&#30340;&#29615;&#22659;&#20013;&#23545;&#40784;&#25945;&#32946;&#33021;&#21147;&#65292;&#36890;&#36807;&#20135;&#23398;&#21512;&#20316;&#20419;&#36827;&#21516;&#34892;&#20043;&#38388;&#30340;&#36830;&#25509;&#21644;&#20016;&#23500;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#39640;&#32423;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#34701;&#21512;&#36827;&#40657;&#23458;&#39532;&#25289;&#26494;&#27491;&#22312;&#25913;&#21464;&#23427;&#20204;&#30340;&#32467;&#26500;&#21644;&#32467;&#26524;&#12290;&#36825;&#31181;&#28436;&#21464;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#22914;&#22686;&#24378;&#30340;&#23398;&#20064;&#20307;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Hackathons and software competitions, increasingly pivotal in the software industry, serve as vital catalysts for innovation and skill development for both organizations and students. These platforms enable companies to prototype ideas swiftly, while students gain enriched learning experiences, enhancing their practical skills. Over the years, hackathons have transitioned from mere competitive events to significant educational tools, fusing theoretical knowledge with real-world problem-solving. The integration of hackathons into computer science and software engineering curricula aims to align educational proficiencies within a collaborative context, promoting peer connectivity and enriched learning via industry-academia collaborations. However, the infusion of advanced technologies, notably artificial intelligence (AI), and machine learning, into hackathons is revolutionizing their structure and outcomes. This evolution brings forth both opportunities, like enhanced learning experienc
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SERL&#36719;&#20214;&#22871;&#20214;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#24211;&#12290;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#20010;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12289;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;&#36825;&#20010;&#36719;&#20214;&#22871;&#20214;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#38590;&#20197;&#20351;&#29992;&#21644;&#33719;&#21462;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.16013</link><description>&lt;p&gt;
SERL: &#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SERL&#36719;&#20214;&#22871;&#20214;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#24211;&#12290;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#20010;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12289;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;&#36825;&#20010;&#36719;&#20214;&#22871;&#20214;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#38590;&#20197;&#20351;&#29992;&#21644;&#33719;&#21462;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#22270;&#20687;&#35266;&#23519;&#65292;&#23454;&#38469;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#36741;&#21161;&#25968;&#25454;&#65288;&#22914;&#31034;&#33539;&#21644;&#20808;&#21069;&#32463;&#39564;&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#38590;&#20197;&#20351;&#29992;&#12290;&#20174;&#23454;&#36341;&#32773;&#20013;&#35748;&#35782;&#21040;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24120;&#24120;&#19982;&#31639;&#27861;&#36873;&#25321;&#21516;&#26679;&#37325;&#35201;&#65288;&#22914;&#26524;&#19981;&#26159;&#26356;&#37325;&#35201;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#21450;&#36827;&#19968;&#27493;&#21457;&#23637;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#30456;&#23545;&#38590;&#20197;&#33719;&#21462;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31934;&#24515;&#23454;&#29616;&#30340;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31181;&#39640;&#25928;&#26679;&#26412;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#20154;&#30340;&#39640;&#36136;&#37327;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to widespread adoption of robotic RL, as well as further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging example task
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24378;&#35843;&#20102;&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#27169;&#22411;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#20551;&#35774;&#35748;&#35782;&#35770;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#65292;&#21516;&#26102;&#32467;&#21512;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07359</link><description>&lt;p&gt;
&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reliability and Interpretability in Science and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07359
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24378;&#35843;&#20102;&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#27169;&#22411;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#20551;&#35774;&#35748;&#35782;&#35770;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#65292;&#21516;&#26102;&#32467;&#21512;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#26085;&#30410;&#37325;&#35201;&#65292;&#24182;&#19988;&#19982;&#27492;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#24050;&#32463;&#28608;&#21457;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#20165;&#23558;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#26631;&#20934;&#31185;&#23398;&#24314;&#27169;&#26377;&#25152;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#23545;DNN&#27169;&#22411;&#19982;&#26631;&#20934;&#31185;&#23398;&#24314;&#27169;&#30340;&#21487;&#33021;&#24046;&#24322;&#20197;&#21450;&#36825;&#20123;&#24046;&#24322;&#22312;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#30340;&#26356;&#28145;&#23618;&#27425;&#30340;&#35748;&#35782;&#35770;&#20998;&#26512;&#30456;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20960;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#24378;&#35843;&#20102;&#27169;&#22411;&#20551;&#35774;&#65288;&#22312;ML&#21644;&#20256;&#32479;&#31185;&#23398;&#20013;&#22343;&#23384;&#22312;&#65289;&#22312;&#26080;&#29702;&#35770;&#31185;&#23398;&#30340;&#38169;&#35273;&#19979;&#30340;&#26222;&#36941;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#20174;&#65288;&#35748;&#35782;&#35770;&#30340;&#65289;&#22797;&#26434;&#24615;&#35282;&#24230;&#20998;&#26512;&#20102;&#27169;&#22411;&#20551;&#35774;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#20551;&#35774;&#22312;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models, and in particular Deep Neural Network (DNN) models, which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional Science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#27169;&#22359;&#65292;&#20197;&#20811;&#26381;&#23567;&#22411;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#21644;&#24037;&#20855;&#26356;&#26032;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.07324</link><description>&lt;p&gt;
&#23567;&#22411;LLMs&#26159;&#24369;&#24037;&#20855;&#23398;&#20064;&#32773;&#65306;&#22810;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Small LLMs Are Weak Tool Learners: A Multi-LLM Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#27169;&#22359;&#65292;&#20197;&#20811;&#26381;&#23567;&#22411;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#21644;&#24037;&#20855;&#26356;&#26032;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#22823;&#22823;&#25193;&#23637;&#20102;&#29420;&#31435;LLMs&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;API&#65292;&#20989;&#25968;&#65289;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#33258;&#20027;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#24037;&#20855;&#20351;&#29992;&#30340;&#25361;&#25112;&#35201;&#27714;LLMs&#19981;&#20165;&#33021;&#29702;&#35299;&#29992;&#25143;&#26597;&#35810;&#24182;&#29983;&#25104;&#31572;&#26696;&#65292;&#36824;&#35201;&#22312;&#20219;&#21153;&#35268;&#21010;&#12289;&#35760;&#24518;&#31649;&#29702;&#12289;&#24037;&#20855;&#35843;&#29992;&#21644;&#32467;&#26524;&#24635;&#32467;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20256;&#32479;&#26041;&#27861;&#38598;&#20013;&#20110;&#35757;&#32451;&#21333;&#20010;&#20855;&#22791;&#25152;&#26377;&#36825;&#20123;&#21151;&#33021;&#30340;LLM&#65292;&#20294;&#22312;&#23567;&#22411;&#27169;&#22411;&#19978;&#20250;&#20986;&#29616;&#24615;&#33021;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#27492;&#22806;&#65292;&#24403;&#24037;&#20855;&#26356;&#26032;&#26102;&#65292;&#25972;&#20010;LLM&#21487;&#33021;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#19978;&#36848;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#12290;&#27599;&#20010;&#32452;&#20214;&#30001;&#19968;&#20010;&#21333;&#29420;&#30340;LLM&#23454;&#29616;&#65292;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#24182;&#19982;&#20854;&#20182;&#32452;&#20214;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#26694;&#26550;&#20415;&#20110;&#36827;&#34892;&#20010;&#20307;&#26356;&#26032;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;-&#35270;&#39057;&#23450;&#20301;&#20013;&#24120;&#35782;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;CORONET&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24120;&#35782;&#36827;&#34892;&#35270;&#39057;&#21644;&#29983;&#25104;&#30340;&#20266;&#26597;&#35810;&#20043;&#38388;&#30340;&#26725;&#25509;&#12290;&#23454;&#39564;&#35777;&#26126;CORONET&#22312;&#38646;&#26679;&#26412;&#21644;&#20256;&#32479;NLVL&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.17429</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Commonsense for Zero-Shot Natural Language Video Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;-&#35270;&#39057;&#23450;&#20301;&#20013;&#24120;&#35782;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;CORONET&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24120;&#35782;&#36827;&#34892;&#35270;&#39057;&#21644;&#29983;&#25104;&#30340;&#20266;&#26597;&#35810;&#20043;&#38388;&#30340;&#26725;&#25509;&#12290;&#23454;&#39564;&#35777;&#26126;CORONET&#22312;&#38646;&#26679;&#26412;&#21644;&#20256;&#32479;NLVL&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;-&#35270;&#39057;&#23450;&#20301;&#65288;NLVL&#65289;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35270;&#39057;&#29255;&#27573;&#21644;&#20266;&#26597;&#35810;&#27880;&#37322;&#65292;&#22312;&#20165;&#29992;&#21407;&#22987;&#35270;&#39057;&#25968;&#25454;&#35757;&#32451;NLVL&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20266;&#26597;&#35810;&#32463;&#24120;&#32570;&#20047;&#23545;&#28304;&#35270;&#39057;&#30340;&#25166;&#23454;&#22522;&#30784;&#65292;&#23548;&#33268;&#20869;&#23481;&#19981;&#32467;&#26500;&#21270;&#21644;&#19981;&#36830;&#36143;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;NLVL&#20013;&#24120;&#35782;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CORONET&#65292;&#19968;&#20010;&#38646;&#26679;&#26412;NLVL&#26694;&#26550;&#65292;&#36890;&#36807;&#24120;&#35782;&#22686;&#24378;&#27169;&#22359;&#26725;&#25509;&#35270;&#39057;&#21644;&#29983;&#25104;&#30340;&#20266;&#26597;&#35810;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;CORONET&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#32534;&#30721;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30340;&#24120;&#35782;&#20449;&#24687;&#65292;&#26465;&#20214;&#26159;&#35270;&#39057;&#65292;&#20197;&#21450;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#26469;&#22686;&#24378;&#32534;&#30721;&#35270;&#39057;&#21644;&#20266;&#26597;&#35810;&#34920;&#31034;&#20197;&#36827;&#34892;&#23450;&#20301;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;CORONET&#22312;&#38646;&#26679;&#26412;&#21644;&#20256;&#32479;NLVL&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Natural Language-Video Localization (NLVL) methods have exhibited promising results in training NLVL models exclusively with raw video data by dynamically generating video segments and pseudo-query annotations. However, existing pseudo-queries often lack grounding in the source video, resulting in unstructured and disjointed content. In this paper, we investigate the effectiveness of commonsense reasoning in zero-shot NLVL. Specifically, we present CORONET, a zero-shot NLVL framework that leverages commonsense to bridge the gap between videos and generated pseudo-queries via a commonsense enhancement module. CORONET employs Graph Convolution Networks (GCN) to encode commonsense information extracted from a knowledge graph, conditioned on the video, and cross-attention mechanisms to enhance the encoded video and pseudo-query representations prior to localization. Through empirical evaluations on two benchmark datasets, we demonstrate that CORONET surpasses both zero-shot and w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#20110;&#22240;&#26524;&#20986;&#29616;&#21644;&#30456;&#20851;&#23450;&#37327;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#37327;&#21270;&#22240;&#26524;&#20986;&#29616;&#21644;&#22312;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#22240;&#26524;&#20986;&#29616;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.16815</link><description>&lt;p&gt;
&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#20986;&#29616;&#21644;&#22240;&#26524;&#20851;&#31995;&#65306;&#20851;&#20110;&#22240;&#26524;&#20986;&#29616;&#21644;&#30456;&#20851;&#23450;&#37327;&#30740;&#31350;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Emergence and Causality in Complex Systems: A Survey on Causal Emergence and Related Quantitative Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#20110;&#22240;&#26524;&#20986;&#29616;&#21644;&#30456;&#20851;&#23450;&#37327;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#37327;&#21270;&#22240;&#26524;&#20986;&#29616;&#21644;&#22312;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#22240;&#26524;&#20986;&#29616;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#29616;&#21644;&#22240;&#26524;&#20851;&#31995;&#26159;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#23427;&#20204;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#12290;&#20986;&#29616;&#19968;&#26041;&#38754;&#25351;&#30340;&#26159;&#23439;&#35266;&#23646;&#24615;&#19981;&#33021;&#20165;&#24402;&#22240;&#20110;&#20010;&#20307;&#23646;&#24615;&#30340;&#21407;&#22240;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#34920;&#29616;&#20986;&#20986;&#29616;&#65292;&#24847;&#21619;&#30528;&#38543;&#30528;&#25277;&#35937;&#23618;&#27425;&#30340;&#22686;&#21152;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#26032;&#30340;&#22240;&#26524;&#23450;&#24459;&#12290;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#26088;&#22312;&#36830;&#25509;&#36825;&#20004;&#20010;&#27010;&#24565;&#65292;&#29978;&#33267;&#37319;&#29992;&#22240;&#26524;&#24230;&#37327;&#26469;&#37327;&#21270;&#20986;&#29616;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#20986;&#29616;&#23450;&#37327;&#29702;&#35770;&#21644;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#37325;&#28857;&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#37327;&#21270;&#22240;&#26524;&#20986;&#29616;&#21644;&#22312;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#20986;&#29616;&#12290;&#35299;&#20915;&#21518;&#32773;&#38656;&#35201;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#22240;&#26524;&#20986;&#29616;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#29992;&#20110;&#35782;&#21035;&#22240;&#26524;&#20986;&#29616;&#30340;&#26550;&#26500;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergence and causality are two fundamental concepts for understanding complex systems. They are interconnected. On one hand, emergence refers to the phenomenon where macroscopic properties cannot be solely attributed to the cause of individual properties. On the other hand, causality can exhibit emergence, meaning that new causal laws may arise as we increase the level of abstraction. Causal emergence theory aims to bridge these two concepts and even employs measures of causality to quantify emergence. This paper provides a comprehensive review of recent advancements in quantitative theories and applications of causal emergence. Two key problems are addressed: quantifying causal emergence and identifying it in data. Addressing the latter requires the use of machine learning techniques, thus establishing a connection between causal emergence and artificial intelligence. We highlighted that the architectures used for identifying causal emergence are shared by causal representation learn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LAION-2B&#20013;&#65292;&#26631;&#39064;&#23494;&#38598;&#22320;&#8220;&#27169;&#20223;&#8221;&#22270;&#20687;&#20013;&#23884;&#20837;&#30340;&#35270;&#35273;&#25991;&#26412;&#65292;&#23548;&#33268;CLIP&#27169;&#22411;&#21463;&#21040;&#25991;&#26412;&#23450;&#20301;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#20197;&#8220;&#27169;&#20223;&#26631;&#39064;&#8221;&#20026;&#26631;&#20934;&#30340;&#35757;&#32451;&#38598;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.14232</link><description>&lt;p&gt;
Parrot Captions Teach CLIP to Spot Text
&lt;/p&gt;
&lt;p&gt;
Parrot Captions Teach CLIP to Spot Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LAION-2B&#20013;&#65292;&#26631;&#39064;&#23494;&#38598;&#22320;&#8220;&#27169;&#20223;&#8221;&#22270;&#20687;&#20013;&#23884;&#20837;&#30340;&#35270;&#35273;&#25991;&#26412;&#65292;&#23548;&#33268;CLIP&#27169;&#22411;&#21463;&#21040;&#25991;&#26412;&#23450;&#20301;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#20197;&#8220;&#27169;&#20223;&#26631;&#39064;&#8221;&#20026;&#26631;&#20934;&#30340;&#35757;&#32451;&#38598;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;CLIP&#22312;&#24456;&#22810;&#35270;&#35273;&#35821;&#35328;&#24212;&#29992;&#20013;&#26159;&#22522;&#30784;&#27169;&#22411;&#65292;&#20294;&#23427;&#23384;&#22312;&#20005;&#37325;&#30340;&#25991;&#26412;&#23450;&#20301;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#23548;&#33268;CLIP&#27169;&#22411;&#8220;&#27169;&#20223;&#8221;&#22270;&#20687;&#20013;&#23884;&#20837;&#30340;&#35270;&#35273;&#25991;&#26412;&#65292;&#32780;&#24573;&#35270;&#20102;&#30495;&#23454;&#30340;&#35270;&#35273;&#35821;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26368;&#27969;&#34892;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LAION-2B&#20013;&#65292;&#26631;&#39064;&#20063;&#23494;&#38598;&#22320;&#8220;&#27169;&#20223;&#8221;&#22270;&#20687;&#20013;&#23884;&#20837;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#32422;50%&#30340;&#22270;&#20687;&#23884;&#20837;&#20102;&#35270;&#35273;&#25991;&#26412;&#20869;&#23481;&#65292;&#32780;&#32422;30%&#30340;&#26631;&#39064;&#21333;&#35789;&#23646;&#20110;&#36825;&#20123;&#23884;&#20837;&#30340;&#35270;&#35273;&#20869;&#23481;&#12290;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#24443;&#24213;&#26816;&#26597;&#20102;&#19981;&#21516;&#21457;&#24067;&#29256;&#26412;&#30340;CLIP&#27169;&#22411;&#65292;&#24182;&#39564;&#35777;&#20102;&#35270;&#35273;&#25991;&#26412;&#26159;&#34913;&#37327;&#36825;&#20123;&#27169;&#22411;&#30340;LAION&#39118;&#26684;&#22270;&#20687;-&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#20026;&#20102;&#26816;&#26597;&#36825;&#20123;&#8220;&#27169;&#20223;&#8221;&#30340;&#26631;&#39064;&#26159;&#21542;&#22609;&#36896;&#20102;&#25991;&#26412;&#23450;&#20301;&#20559;&#24046;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#8220;&#27169;&#20223;&#26631;&#39064;&#8221;&#20026;&#26631;&#20934;&#65292;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#20197;LAION&#23376;&#38598;&#20026;&#22522;&#30784;&#30340;CLIP&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#8220;&#27169;&#20223;&#8221;&#26631;&#39064;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#35299;&#20915;&#25991;&#26412;&#23450;&#20301;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite CLIP being the foundation model in numerous vision-language applications, the CLIP suffers from a severe text spotting bias. Such bias causes CLIP models to `Parrot' the visual text embedded within images while disregarding the authentic visual semantics. We uncover that in the most popular image-text dataset LAION-2B, the captions also densely parrot (spell) the text embedded in images. Our analysis shows that around 50% of images are embedded with visual text content, and around 30% of captions words are in these embedded visual content. Based on such observation, we thoroughly inspect the different released versions of CLIP models and verify that the visual text is the dominant factor in measuring the LAION-style image-text similarity for these models. To examine whether these parrot captions shape the text spotting bias, we train a series of CLIP models with LAION subsets curated by different parrot-caption-oriented criteria. We show that training with parrot captions easil
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21442;&#19982;&#32773;&#30340;&#27880;&#35270;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#21521;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#27880;&#35270;&#21487;&#20197;&#20316;&#20026;&#21551;&#21160;&#32852;&#21512;&#27963;&#21160;&#30340;&#35302;&#21457;&#22120;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#25913;&#36827;&#24037;&#19994;&#20154;&#26426;&#21327;&#20316;&#21644;&#25805;&#20316;&#20307;&#39564;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2312.06643</link><description>&lt;p&gt;
&#26816;&#27979;&#19982;&#20998;&#26512;&#27880;&#35270;&#20197;&#21551;&#21160;&#24037;&#19994;&#20154;&#26426;&#21327;&#20316;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gaze Detection and Analysis for Initiating Joint Activity in Industrial Human-Robot Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06643
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21442;&#19982;&#32773;&#30340;&#27880;&#35270;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#21521;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#27880;&#35270;&#21487;&#20197;&#20316;&#20026;&#21551;&#21160;&#32852;&#21512;&#27963;&#21160;&#30340;&#35302;&#21457;&#22120;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#25913;&#36827;&#24037;&#19994;&#20154;&#26426;&#21327;&#20316;&#21644;&#25805;&#20316;&#20307;&#39564;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#26426;&#22120;&#20154;&#65288;cobots&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#65292;&#20294;&#20173;&#38656;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#20197;&#25552;&#21319;&#20154;&#26426;&#21327;&#20316;&#21644;&#25805;&#20316;&#20307;&#39564;&#12290;&#19968;&#31181;&#25552;&#39640;&#21327;&#20316;&#20307;&#39564;&#30340;&#28508;&#22312;&#26041;&#27861;&#26159;&#22522;&#20110;&#25805;&#20316;&#32773;&#30340;&#33258;&#28982;&#32447;&#32034;&#35843;&#25972;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#12290;&#21463;&#21040;&#20154;&#38469;&#20114;&#21160;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24043;&#24072;&#35797;&#39564;&#65292;&#20197;&#25506;&#31350;&#21521;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#27880;&#35270;&#26159;&#21542;&#33021;&#22815;&#25104;&#20026;&#21551;&#21160;&#32852;&#21512;&#27963;&#21160;&#30340;&#35302;&#21457;&#22120;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;37&#21517;&#21442;&#19982;&#32773;&#22312;&#36827;&#34892;&#32452;&#35013;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20182;&#20204;&#30340;&#27880;&#35270;&#34892;&#20026;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27880;&#35270;&#30340;&#27880;&#24847;&#21147;&#35782;&#21035;&#27169;&#22411;&#26469;&#30830;&#23450;&#21442;&#19982;&#32773;&#20309;&#26102;&#30475;&#21521;&#21327;&#20316;&#26426;&#22120;&#20154;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65288;84.88\%&#65289;&#65292;&#32852;&#21512;&#27963;&#21160;&#21069;&#20250;&#20986;&#29616;&#23545;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#27880;&#35270;&#12290;&#27492;&#22806;&#65292;&#22312;&#25972;&#20010;&#32452;&#35013;&#36807;&#31243;&#20013;&#65292;&#21442;&#19982;&#32773;&#24448;&#24448;&#22312;&#32852;&#21512;&#27963;&#21160;&#26399;&#38388;&#27880;&#35270;&#21327;&#20316;&#26426;&#22120;&#20154;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;
&lt;/p&gt;
&lt;p&gt;
Collaborative robots (cobots) are widely used in industrial applications, yet extensive research is still needed to enhance human-robot collaborations and operator experience. A potential approach to improve the collaboration experience involves adapting cobot behavior based on natural cues from the operator. Inspired by the literature on human-human interactions, we conducted a wizard-of-oz study to examine whether a gaze towards the cobot can serve as a trigger for initiating joint activities in collaborative sessions. In this study, 37 participants engaged in an assembly task while their gaze behavior was analyzed. We employ a gaze-based attention recognition model to identify when the participants look at the cobot. Our results indicate that in most cases (84.88\%), the joint activity is preceded by a gaze towards the cobot. Furthermore, during the entire assembly cycle, the participants tend to look at the cobot around the time of the joint activity. To the best of our knowledge, 
&lt;/p&gt;</description></item><item><title>EE-LLM&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#21644;&#22810;&#39033;&#31639;&#27861;&#21019;&#26032;&#12290;&#30740;&#31350;&#21457;&#29616;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35745;&#31639;&#24320;&#38144;&#26497;&#23567;&#12290;</title><link>https://arxiv.org/abs/2312.04916</link><description>&lt;p&gt;
EE-LLM: &#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#30340;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04916
&lt;/p&gt;
&lt;p&gt;
EE-LLM&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#21644;&#22810;&#39033;&#31639;&#27861;&#21019;&#26032;&#12290;&#30740;&#31350;&#21457;&#29616;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35745;&#31639;&#24320;&#38144;&#26497;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EE-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#21021;&#27493;&#35777;&#26126;&#20102;&#26089;&#36864;&#20986;&#22312;&#21152;&#36895;LLM&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;EE-LLM&#36890;&#36807;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#24182;&#34892;&#24615;&#26469;&#25512;&#21160;&#26089;&#36864;&#20986;LLM&#30340;&#35268;&#27169;&#21270;&#12290;&#22522;&#20110;Megatron-LM&#26500;&#24314;&#30340;EE-LLM&#23454;&#29616;&#20102;&#21508;&#31181;&#31639;&#27861;&#21019;&#26032;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#20197;&#36866;&#24212;&#26089;&#36864;&#20986;&#65292;&#21253;&#25324;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#27700;&#32447;&#24182;&#34892;&#24615;&#20419;&#36827;&#26089;&#36864;&#20986;&#35757;&#32451;&#30446;&#26631;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#21033;&#29992;&#21407;&#22987;&#27969;&#27700;&#32447;&#35843;&#24230;&#20013;&#30340;&#31354;&#38386;&#36164;&#28304;&#36827;&#34892;&#19982;&#26089;&#36864;&#20986;&#23618;&#30456;&#20851;&#30340;&#35745;&#31639;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#20004;&#31181;&#19982;KV&#32531;&#23384;&#20860;&#23481;&#30340;&#26089;&#36864;&#20986;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#24573;&#30053;&#30340;&#35745;&#31639;&#24320;&#38144;&#30456;&#27604;&#65292;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared
&lt;/p&gt;</description></item><item><title>RLHF&#31639;&#27861;&#20013;&#30340;IIA&#20551;&#35774;&#23548;&#33268;&#20102;&#20498;&#32622;&#28608;&#21169;&#65292;&#38480;&#21046;&#20102;&#26597;&#35810;&#26684;&#24335;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2312.01057</link><description>&lt;p&gt;
RLHF&#21644;IIA&#65306;&#20498;&#32622;&#28608;&#21169;
&lt;/p&gt;
&lt;p&gt;
RLHF and IIA: Perverse Incentives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01057
&lt;/p&gt;
&lt;p&gt;
RLHF&#31639;&#27861;&#20013;&#30340;IIA&#20551;&#35774;&#23548;&#33268;&#20102;&#20498;&#32622;&#28608;&#21169;&#65292;&#38480;&#21046;&#20102;&#26597;&#35810;&#26684;&#24335;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RLHF&#65289;&#21487;&#20197;&#28608;&#21169;&#19982;&#20559;&#22909;&#19981;&#31526;&#30340;&#22238;&#24212;&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#20551;&#35774;&#26080;&#20851;&#27010;&#25324;&#30340;&#27169;&#22411;&#65288;IIA&#65289;&#12290;IIA&#24341;&#21457;&#30340;&#20498;&#32622;&#28608;&#21169;&#38459;&#30861;&#20102;&#26597;&#35810;&#26684;&#24335;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ERASER&#65292;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#26381;&#21153;&#22120;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#22312;MLaaS&#20013;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#30340;&#21435;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.16136</link><description>&lt;p&gt;
ERASER: &#36890;&#36807;&#25512;&#29702;&#26381;&#21153;&#22120;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#22312;MLaaS&#20013;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#30340;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ERASER&#65292;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#26381;&#21153;&#22120;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#22312;MLaaS&#20013;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#30340;&#21435;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26381;&#21153;&#65288;MLaaS&#65289;&#22312;&#25903;&#25345;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26381;&#21153;&#26041;&#38754;&#26377;&#30528;&#22823;&#37327;&#38656;&#27714;&#65292;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#25552;&#20379;&#38761;&#21629;&#24615;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;MLaaS&#22522;&#20110;&#20174;&#20247;&#22810;&#20010;&#20307;&#25968;&#25454;&#25152;&#26377;&#32773;&#25910;&#38598;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#25512;&#29702;&#26381;&#21153;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#38544;&#31169;&#24182;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#27861;&#30340;&#8220;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#8221;&#65292;&#35768;&#22810;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#20197;&#22312;&#35831;&#27714;&#21435;&#23398;&#20064;&#26102;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#29575;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#37117;&#29420;&#31435;&#22788;&#29702;&#21435;&#23398;&#20064;&#35831;&#27714;&#21644;&#25512;&#29702;&#35831;&#27714;&#65292;&#36825;&#19981;&#24184;&#22320;&#24341;&#20837;&#20102;&#25512;&#29702;&#26381;&#21153;&#36807;&#26102;&#24615;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#26426;&#22120;&#21435;&#23398;&#20064;&#20013;&#26497;&#26131;&#36973;&#21463;&#19981;&#24517;&#35201;&#30340;&#26333;&#20809;&#30340;&#38544;&#31169;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the "right to be forgotten (RTBF)" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#21487;&#20449;&#30340;&#22823;&#27169;&#22411;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20154;&#20026;&#35823;&#29992;&#12289;&#28431;&#27934;&#24615;&#12289;&#22266;&#26377;&#38382;&#39064;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#24212;&#30340;&#25361;&#25112;&#21644;&#23545;&#31574;&#65292;&#26088;&#22312;&#25512;&#21160;&#21487;&#20449;&#30340;LMs&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#23545;&#40784;</title><link>https://arxiv.org/abs/2311.09680</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;&#22823;&#27169;&#22411;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Large Models in Vision: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09680
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#21487;&#20449;&#30340;&#22823;&#27169;&#22411;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20154;&#20026;&#35823;&#29992;&#12289;&#28431;&#27934;&#24615;&#12289;&#22266;&#26377;&#38382;&#39064;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#24212;&#30340;&#25361;&#25112;&#21644;&#23545;&#31574;&#65292;&#26088;&#22312;&#25512;&#21160;&#21487;&#20449;&#30340;LMs&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#37117;&#26377;&#26174;&#33879;&#30340;&#25104;&#32489;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#24615;&#33021;&#20294;&#19981;&#21487;&#20449;&#30340;&#34892;&#20026;&#65292;LMs&#36234;&#26469;&#36234;&#21463;&#21040;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#25361;&#25112;&#21644;&#25209;&#35780;&#65292;&#36843;&#20999;&#38656;&#35201;&#21487;&#38752;&#30340;&#26041;&#27861;&#21152;&#20197;&#32531;&#35299;&#12290;&#23613;&#31649;&#20851;&#20110;&#21487;&#20449;&#30340;NLP&#39046;&#22495;&#20013;&#30340;LMs&#24050;&#26377;&#22823;&#37327;&#25991;&#29486;&#65292;&#20294;&#31995;&#32479;&#30740;&#31350;&#20851;&#20110;CV&#39046;&#22495;&#20013;LMs&#30340;&#21487;&#20449;&#24615;&#30340;&#25991;&#29486;&#20173;&#28982;&#32570;&#20047;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#26412;&#32508;&#36848;&#20013;&#24635;&#32467;&#20102;&#22235;&#20010;&#24433;&#21709;&#22312;&#35270;&#35273;&#39046;&#22495;&#20013;&#20351;&#29992;&#21487;&#20449;LMs&#30340;&#30456;&#20851;&#38382;&#39064;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20154;&#20026;&#35823;&#29992;&#65292;2&#65289;&#28431;&#27934;&#24615;&#65292;3&#65289;&#22266;&#26377;&#38382;&#39064;&#21644;4&#65289;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#31361;&#20986;&#27599;&#20010;&#20027;&#39064;&#20013;&#30340;&#30456;&#24212;&#25361;&#25112;&#12289;&#23545;&#31574;&#21644;&#35752;&#35770;&#65292;&#25105;&#20204;&#24076;&#26395;&#26412;&#32508;&#36848;&#33021;&#22815;&#20419;&#36827;&#35835;&#32773;&#23545;&#36825;&#19968;&#39046;&#22495;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;LMs&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of Large Models (LMs) has recently revolutionized various fields of deep learning with remarkable grades, ranging from Natural Language Processing (NLP) to Computer Vision (CV). However, LMs are increasingly challenged and criticized by academia and industry due to their powerful performance but untrustworthy behavior, which urgently needs to be alleviated by reliable methods. Despite the abundance of literature on trustworthy LMs in NLP, a systematic survey specifically delving into the trustworthiness of LMs in CV remains absent. In order to mitigate this gap, we summarize four relevant concerns that obstruct the trustworthy usage in vision of LMs in this survey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4) interpretability. By highlighting corresponding challenge, countermeasures, and discussion in each topic, we hope this survey will facilitate readers' understanding of this field, promote alignment of LMs with human expectations and enab
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#29702;&#24615;&#21270;&#33021;&#21147;&#26377;&#24453;&#25506;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#32534;&#20889;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#21463;&#27426;&#36814;&#30340;&#22522;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#29702;&#24615;&#21270;&#26041;&#24335;&#12290;&#36825;&#20123;&#29702;&#24615;&#21270;&#26041;&#24335;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#22312;&#38169;&#35823;&#39044;&#27979;&#30340;&#29702;&#24615;&#21270;&#26041;&#38754;&#20250;&#25439;&#23475;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2311.05085</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#24615;&#20026;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#29702;&#24615;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05085
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#29702;&#24615;&#21270;&#33021;&#21147;&#26377;&#24453;&#25506;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#32534;&#20889;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#21463;&#27426;&#36814;&#30340;&#22522;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#29702;&#24615;&#21270;&#26041;&#24335;&#12290;&#36825;&#20123;&#29702;&#24615;&#21270;&#26041;&#24335;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#22312;&#38169;&#35823;&#39044;&#27979;&#30340;&#29702;&#24615;&#21270;&#26041;&#38754;&#20250;&#25439;&#23475;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20960;&#20046;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#22522;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#20805;&#20998;&#29702;&#24615;&#25903;&#25345;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#31867;&#20219;&#21153;&#65292;&#27604;&#22914;&#24120;&#35782;&#24615;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#38656;&#35201;&#22522;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#29702;&#24615;&#26469;&#25903;&#25345;&#39044;&#27979;&#24182;&#25512;&#32763;&#22791;&#36873;&#36873;&#39033;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#32534;&#20889;&#30340;&#26679;&#20363;&#20197;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#29983;&#25104;&#30693;&#35782;&#24341;&#23548;&#30340;&#29702;&#24615;&#21270;&#20219;&#21153;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24037;&#20154;&#32676;&#20307;&#26356;&#21916;&#27426;&#22522;&#20110;&#30693;&#35782;&#30340;&#29702;&#24615;&#21270;&#26041;&#24335;&#65292;&#35748;&#20026;&#20854;&#20855;&#26377;&#20107;&#23454;&#24615;&#12289;&#20805;&#20998;&#24615;&#21644;&#20840;&#38754;&#24615;&#30340;&#21453;&#39539;&#12290;&#34429;&#28982;LLMs&#29983;&#25104;&#30340;&#29702;&#24615;&#21270;&#26041;&#24335;&#26356;&#21463;&#27426;&#36814;&#65292;&#20294;&#36824;&#38656;&#35201;&#22312;&#31616;&#27905;&#24615;&#21644;&#26032;&#39062;&#24615;&#26041;&#38754;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#22312;&#21478;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38169;&#35823;&#27169;&#22411;&#39044;&#27979;&#30340;&#29702;&#24615;&#21270;&#22914;&#20309;&#20405;&#34432;&#20154;&#31867;&#23545;LLMs&#29983;&#25104;&#30340;&#29702;&#24615;&#21270;&#30340;&#20449;&#20219;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline t
&lt;/p&gt;</description></item><item><title>SugarViT&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#21644;&#28145;&#24230;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#22823;&#35268;&#27169;&#30000;&#38388;&#22270;&#20687;&#30340;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#35780;&#20998;&#12290;&#23558;&#36965;&#24863;&#25968;&#25454;&#19982;&#29615;&#22659;&#21442;&#25968;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#31958;&#33756;&#21494;&#26001;&#30149;&#20005;&#37325;&#31243;&#24230;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2311.03076</link><description>&lt;p&gt;
SugarViT - &#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#21644;&#28145;&#24230;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#30340;&#22810;&#30446;&#26631;&#22238;&#24402;&#26080;&#20154;&#26426;&#22270;&#20687;&#65292;&#20197;&#29980;&#33756;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
SugarViT -- Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03076
&lt;/p&gt;
&lt;p&gt;
SugarViT&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#21644;&#28145;&#24230;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#22823;&#35268;&#27169;&#30000;&#38388;&#22270;&#20687;&#30340;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#35780;&#20998;&#12290;&#23558;&#36965;&#24863;&#25968;&#25454;&#19982;&#29615;&#22659;&#21442;&#25968;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#31958;&#33756;&#21494;&#26001;&#30149;&#20005;&#37325;&#31243;&#24230;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#21644;&#20154;&#24037;&#26234;&#33021;&#26159;&#29616;&#20195;&#31934;&#20934;&#20892;&#19994;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#22823;&#35268;&#27169;&#30000;&#38388;&#22270;&#20687;&#30340;&#39640;&#25928;&#26816;&#32034;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#29289;&#20505;&#23398;&#12289;&#38500;&#33609;&#12289;&#20316;&#29289;&#21644;&#30149;&#23475;&#25511;&#21046;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#23558;&#20171;&#32461;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21270;&#22823;&#35268;&#27169;&#26893;&#29289;&#29305;&#23450;&#29305;&#24449;&#27880;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#31958;&#33756;&#21494;&#26001;&#30149;(CLS)&#30340;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#35780;&#20998;&#20026;&#20363;&#12290;&#36890;&#36807;&#28145;&#24230;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;(DLDL)&#30340;&#27010;&#24565;&#12289;&#29305;&#27530;&#25439;&#22833;&#20989;&#25968;&#21644;&#37327;&#36523;&#23450;&#21046;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;SugarViT&#30340;&#39640;&#25928;&#35270;&#35273;&#36716;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#35780;&#20998;&#12290;&#26412;&#24037;&#20316;&#30340;&#19968;&#20010;&#21019;&#26032;&#20043;&#22788;&#26159;&#23558;&#36965;&#24863;&#25968;&#25454;&#19982;&#23454;&#39564;&#22330;&#22320;&#30340;&#29615;&#22659;&#21442;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#12290;&#23613;&#31649;&#35813;&#27169;&#22411;&#22312;&#29305;&#23450;&#30340;&#29992;&#20363;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#23613;&#21487;&#33021;&#20445;&#25345;&#36890;&#29992;&#24615;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote sensing and artificial intelligence are pivotal technologies of precision agriculture nowadays. The efficient retrieval of large-scale field imagery combined with machine learning techniques shows success in various tasks like phenotyping, weeding, cropping, and disease control. This work will introduce a machine learning framework for automatized large-scale plant-specific trait annotation for the use case disease severity scoring for Cercospora Leaf Spot (CLS) in sugar beet. With concepts of Deep Label Distribution Learning (DLDL), special loss functions, and a tailored model architecture, we develop an efficient Vision Transformer based model for disease severity scoring called SugarViT. One novelty in this work is the combination of remote sensing data with environmental parameters of the experimental sites for disease severity prediction. Although the model is evaluated on this special use case, it is held as generic as possible to also be applicable to various image-based 
&lt;/p&gt;</description></item><item><title>InstructRetro&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#27169;&#22411;Retro 48B&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2310.07713</link><description>&lt;p&gt;
InstructRetro: &#26816;&#32034;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07713
&lt;/p&gt;
&lt;p&gt;
InstructRetro&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#27169;&#22411;Retro 48B&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#23545;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#22256;&#24785;&#24230;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26816;&#32034;&#22686;&#24378;LLM&#30340;&#35268;&#27169;&#20173;&#28982;&#26377;&#38480;&#65288;&#22914;Retro&#20855;&#26377;75&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#25351;&#20196;&#35843;&#20248;&#21644;&#38646;&#26679;&#20363;&#27867;&#21270;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Retro 48B&#65292;&#36825;&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#25216;&#26415;&#20174;1.2&#19975;&#20159;&#20010;&#26631;&#35760;&#20013;&#32487;&#32493;&#39044;&#35757;&#32451;&#19968;&#20010;43B&#30340;GPT&#27169;&#22411;&#65292;&#24182;&#20511;&#21161;Retro&#26041;&#27861;&#23558;&#20854;&#25193;&#23637;&#21040;4800&#20159;&#20010;&#21442;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#24471;&#21040;&#30340;&#22522;&#30784;&#27169;&#22411;Retro 48B&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20165;&#20351;&#29992;1.2&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#30340;43B GPT&#27169;&#22411;&#65292;&#19988;&#21482;&#22686;&#21152;&#20102;2.58%&#30340;GPU&#20351;&#29992;&#26102;&#38388;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#25193;&#23637;&#28508;&#21147;&#12290;&#22312;&#23545;Retro&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#21518;&#65292;InstructRetro&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Spe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2310.05866</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative quantum machine learning via denoising diffusion probabilistic models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05866
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#32467;&#26500;&#28789;&#27963;&#12289;&#35757;&#32451;&#31616;&#21333;&#30340;&#29305;&#28857;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#21033;&#29992;&#32416;&#32544;&#21644;&#21472;&#21152;&#30340;&#33021;&#21147;&#20026;&#23398;&#20064;&#32463;&#20856;&#21644;&#37327;&#23376;&#25968;&#25454;&#24102;&#26469;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#21463;&#32463;&#20856;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#8221;&#65288;QuDDPM&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#12290;QuDDPM&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#26469;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#19982;&#22122;&#22768;&#20043;&#38388;&#30340;&#25554;&#20540;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#23398;&#20064;&#35823;&#24046;&#30340;&#19978;&#30028;&#21644;...&#65288;&#26410;&#23436;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#21152;&#36895;&#25193;&#25955;&#26426;&#21046;&#65292;&#25552;&#20986;&#26032;&#30340;&#21152;&#36895;&#26041;&#26696;&#65292;&#21487;&#20197;&#20197;50&#20493;&#21040;100&#20493;&#30340;&#36895;&#24230;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#26500;&#35937;&#12290;</title><link>https://arxiv.org/abs/2310.04915</link><description>&lt;p&gt;
&#20851;&#20110;&#21152;&#36895;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#22522;&#20110;&#25193;&#25955;&#30340;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#21152;&#36895;&#25193;&#25955;&#26426;&#21046;&#65292;&#25552;&#20986;&#26032;&#30340;&#21152;&#36895;&#26041;&#26696;&#65292;&#21487;&#20197;&#20197;50&#20493;&#21040;100&#20493;&#30340;&#36895;&#24230;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#26500;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#20855;&#26377;&#25968;&#21315;&#20010;&#26356;&#26032;&#27493;&#39588;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDEs)&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22914;&#20309;&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#26126;&#30830;&#26377;&#25928;&#22320;&#21152;&#36895;&#36825;&#20010;&#36807;&#31243;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#36825;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#24341;&#36215;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#31995;&#32479;&#30740;&#31350;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#25237;&#24433;&#24494;&#20998;&#26041;&#31243;&#30340;&#19978;&#19979;&#25991;&#20013;&#21457;&#23637;&#20102;&#26356;&#31934;&#30830;&#30340;SE(3)&#20869;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#23558;&#36229;&#21442;&#25968;&#19982;&#36825;&#20123;&#35823;&#24046;&#32852;&#31995;&#36215;&#26469;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;SE(3)&#19981;&#21464;&#31354;&#38388;&#20013;&#29983;&#25104;&#20998;&#23376;&#26500;&#35937;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#21487;&#20197;&#20197;50&#20493;&#21040;100&#20493;&#30340;&#36895;&#24230;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26500;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models in SE(3)-invariant space have demonstrated promising performance in molecular conformation generation, but typically require solving stochastic differential equations (SDEs) with thousands of update steps. Till now, it remains unclear how to effectively accelerate this procedure explicitly in SE(3)-invariant space, which greatly hinders its wide application in the real world. In this paper, we systematically study the diffusion mechanism in SE(3)-invariant space via the lens of approximate errors induced by existing methods. Thereby, we develop more precise approximate in SE(3) in the context of projected differential equations. Theoretical analysis is further provided as well as empirical proof relating hyper-parameters with such errors. Altogether, we propose a novel acceleration scheme for generating molecular conformations in SE(3)-invariant space. Experimentally, our scheme can generate high-quality conformations with 50x--100x speedup compared to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26657;&#20934;&#20809;&#24230;&#31435;&#20307;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20809;&#24230;&#31435;&#20307;&#39046;&#22495;&#30340;&#39640;&#32423;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2212.08414</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26657;&#20934;&#20809;&#24230;&#31435;&#20307;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Methods for Calibrated Photometric Stereo and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.08414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26657;&#20934;&#20809;&#24230;&#31435;&#20307;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20809;&#24230;&#31435;&#20307;&#39046;&#22495;&#30340;&#39640;&#32423;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#24230;&#31435;&#20307;&#26159;&#20174;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#38452;&#24433;&#32447;&#32034;&#30340;&#22270;&#20687;&#20013;&#24674;&#22797;&#29289;&#20307;&#30340;&#34920;&#38754;&#27861;&#32447;&#65292;&#21363;&#24314;&#31435;&#27599;&#20010;&#20687;&#32032;&#28857;&#30340;&#34920;&#38754;&#26041;&#21521;&#21644;&#20142;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#27169;&#22411;&#12290;&#20809;&#24230;&#31435;&#20307;&#22312;&#27599;&#20010;&#20687;&#32032;&#28857;&#30340;&#20998;&#36776;&#29575;&#21644;&#32454;&#33410;&#37325;&#24314;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#20848;&#20271;&#29305;&#34920;&#38754;&#21453;&#23556;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#20809;&#24230;&#31435;&#20307;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#38750;&#20848;&#20271;&#29305;&#34920;&#38754;&#30340;&#20809;&#24230;&#31435;&#20307;&#29615;&#22659;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26657;&#20934;&#20809;&#24230;&#31435;&#20307;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#36755;&#20837;&#22788;&#29702;&#12289;&#30417;&#30563;&#21644;&#32593;&#32476;&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#28145;&#24230;&#23398;&#20064;&#20809;&#24230;&#31435;&#20307;&#27169;&#22411;&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#35777;&#26126;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20809;&#24230;&#31435;&#20307;&#26041;&#27861;&#30340;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photometric stereo recovers the surface normals of an object from multiple images with varying shading cues, i.e., modeling the relationship between surface orientation and intensity at each pixel. Photometric stereo prevails in superior per-pixel resolution and fine reconstruction details. However, it is a complicated problem because of the non-linear relationship caused by non-Lambertian surface reflectance. Recently, various deep learning methods have shown a powerful ability in the context of photometric stereo against non-Lambertian surfaces. This paper provides a comprehensive review of existing deep learning-based calibrated photometric stereo methods. We first analyze these methods from different perspectives, including input processing, supervision, and network architecture. We summarize the performance of deep learning photometric stereo models on the most widely-used benchmark data set. This demonstrates the advanced performance of deep learning-based photometric stereo meth
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BLEST-ML&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22359;&#22823;&#23567;&#20272;&#35745;&#65292;&#20197;&#21152;&#36895;&#24182;&#34892;&#25968;&#25454;&#23494;&#38598;&#22411;&#24212;&#29992;&#21644;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#24211;dislib&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2211.10819</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#39640;&#24615;&#33021;&#35745;&#31639;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#20998;&#21306;&#30340;&#22359;&#22823;&#23567;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Block size estimation for data partitioning in HPC applications using machine learning techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.10819
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BLEST-ML&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22359;&#22823;&#23567;&#20272;&#35745;&#65292;&#20197;&#21152;&#36895;&#24182;&#34892;&#25968;&#25454;&#23494;&#38598;&#22411;&#24212;&#29992;&#21644;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#24211;dislib&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#21644;&#26694;&#26550;&#30340;&#24191;&#27867;&#20351;&#29992;&#20419;&#20351;&#23545;&#25968;&#25454;&#20998;&#21306;&#25216;&#26415;&#21644;&#31574;&#30053;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#12290;&#23454;&#38469;&#19978;&#65292;&#24212;&#29992;&#24615;&#33021;&#21487;&#33021;&#21463;&#25968;&#25454;&#20998;&#21306;&#26041;&#24335;&#30340;&#24433;&#21709;&#65292;&#32780;&#36825;&#21448;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#25968;&#25454;&#22359;&#22823;&#23567;&#65292;&#21363;&#22359;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#25214;&#21040;&#19968;&#31181;&#26377;&#25928;&#30340;&#20998;&#21306;&#21363;&#21512;&#36866;&#30340;&#22359;&#22823;&#23567;&#26159;&#21152;&#36895;&#24182;&#34892;&#25968;&#25454;&#23494;&#38598;&#22411;&#24212;&#29992;&#21644;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;BLEST-ML&#65288;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#22359;&#22823;&#23567;&#20272;&#35745;&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22359;&#22823;&#23567;&#20272;&#35745;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#38024;&#23545;&#22522;&#20110;PyCOMPSs&#26694;&#26550;&#30340;&#39640;&#24230;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#24211;dislib&#35774;&#35745;&#30340;&#23454;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#26469;&#35780;&#20272;&#25552;&#20379;&#30340;&#23454;&#29616;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extensive use of HPC infrastructures and frameworks for running dataintensive applications has led to a growing interest in data partitioning techniques and strategies. In fact, application performance can be heavily affected by how data are partitioned, which in turn depends on the selected size for data blocks, i.e. the block size. Therefore, finding an effective partitioning, i.e. a suitable block size, is a key strategy to speed-up parallel data-intensive applications and increase scalability. This paper describes a methodology, namely BLEST-ML (BLock size ESTimation through Machine Learning), for block size estimation that relies on supervised machine learning techniques. The proposed methodology was evaluated by designing an implementation tailored to dislib, a distributed computing library highly focused on machine learning algorithms built on top of the PyCOMPSs framework. We assessed the effectiveness of the provided implementation through an extensive experimental evaluat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24369;&#30456;&#20851;&#22238;&#24402;&#65288;WCR&#65289;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#26377;&#25928;&#22320;&#25581;&#31034;&#39640;&#32500;&#32858;&#21512;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#38543;&#26426;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2209.02628</link><description>&lt;p&gt;
&#24369;&#30456;&#20851;&#22238;&#24402;&#26041;&#27861;&#65306;&#24555;&#36895;&#25581;&#31034;&#39640;&#32500;&#32858;&#21512;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#38543;&#26426;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Weak Collocation Regression method: fast reveal hidden stochastic dynamics from high-dimensional aggregate data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.02628
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24369;&#30456;&#20851;&#22238;&#24402;&#65288;WCR&#65289;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#24555;&#36895;&#26377;&#25928;&#22320;&#25581;&#31034;&#39640;&#32500;&#32858;&#21512;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#38543;&#26426;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#25968;&#25454;&#20013;&#25581;&#31034;&#38544;&#34255;&#30340;&#21160;&#24577;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38543;&#26426;&#24615;&#21442;&#19982;&#20102;&#25968;&#25454;&#30340;&#28436;&#21270;&#36807;&#31243;&#12290;&#24403;&#38543;&#26426;&#25968;&#25454;&#30340;&#36712;&#36857;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#32570;&#22833;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#24322;&#24120;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#24418;&#24335;Fokker-Planck&#65288;FP&#65289;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#24314;&#27169;&#38543;&#26426;&#25968;&#25454;&#30340;&#21160;&#24577;&#12290;&#35813;&#26041;&#31243;&#25511;&#21046;&#30528;&#24067;&#26391;&#36816;&#21160;&#20013;&#23494;&#24230;&#20989;&#25968;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#23558;&#39640;&#26031;&#20989;&#25968;&#30340;&#30456;&#20851;&#24615;&#20316;&#20026;FP&#26041;&#31243;&#30340;&#24369;&#24418;&#24335;&#30340;&#27979;&#35797;&#20989;&#25968;&#65292;&#23558;&#23548;&#25968;&#36716;&#21270;&#20026;&#39640;&#26031;&#20989;&#25968;&#65292;&#20174;&#32780;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#26399;&#26395;&#21644;&#26469;&#36817;&#20284;&#24369;&#24418;&#24335;&#12290;&#36890;&#36807;&#26410;&#30693;&#39033;&#30340;&#23383;&#20856;&#34920;&#31034;&#65292;&#26500;&#24314;&#19968;&#20010;&#32447;&#24615;&#31995;&#32479;&#65292;&#28982;&#21518;&#36890;&#36807;&#22238;&#24402;&#35299;&#20915;&#65292;&#25581;&#31034;&#25968;&#25454;&#30340;&#26410;&#30693;&#21160;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#21629;&#21517;&#20026;&#24369;&#30456;&#20851;&#22238;&#24402;&#65288;WCR&#65289;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#26377;&#19977;&#20010;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#24369;&#24418;&#24335;&#65292;&#30456;&#20851;&#24615;&#65292;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revealing hidden dynamics from the stochastic data is a challenging problem as randomness takes part in the evolution of the data. The problem becomes exceedingly complex when the trajectories of the stochastic data are absent in many scenarios. Here we present an approach to effectively modeling the dynamics of the stochastic data without trajectories based on the weak form of the Fokker-Planck (FP) equation, which governs the evolution of the density function in the Brownian process. Taking the collocations of Gaussian functions as the test functions in the weak form of the FP equation, we transfer the derivatives to the Gaussian functions and thus approximate the weak form by the expectational sum of the data. With a dictionary representation of the unknown terms, a linear system is built and then solved by the regression, revealing the unknown dynamics of the data. Hence, we name the method with the Weak Collocation Regression (WCR) method for its three key components: weak form, c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#20998;&#37197;&#20844;&#27491;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#23558;&#20844;&#24179;&#38382;&#39064;&#20998;&#20026;&#36164;&#28304;&#24179;&#22343;&#20998;&#37197;&#21644;&#24615;&#33021;&#24179;&#31561;&#20004;&#20010;&#31867;&#21035;&#65292;&#24182;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#30456;&#20851;&#30340;&#20844;&#27491;&#24230;&#37327;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#20559;&#35265;&#21644;&#32531;&#35299;&#31574;&#30053;&#65292;&#25506;&#35752;&#20102;&#20559;&#35265;&#19982;&#20854;&#23545;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2206.14397</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Fair Machine Learning in Healthcare: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.14397
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#20998;&#37197;&#20844;&#27491;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#23558;&#20844;&#24179;&#38382;&#39064;&#20998;&#20026;&#36164;&#28304;&#24179;&#22343;&#20998;&#37197;&#21644;&#24615;&#33021;&#24179;&#31561;&#20004;&#20010;&#31867;&#21035;&#65292;&#24182;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#30456;&#20851;&#30340;&#20844;&#27491;&#24230;&#37327;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#20559;&#35265;&#21644;&#32531;&#35299;&#31574;&#30053;&#65292;&#25506;&#35752;&#20102;&#20559;&#35265;&#19982;&#20854;&#23545;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#25968;&#25454;&#30340;&#25968;&#23383;&#21270;&#19982;&#35745;&#31639;&#33021;&#21147;&#30340;&#36827;&#27493;&#25512;&#21160;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#21152;&#21095;&#25110;&#29978;&#33267;&#21152;&#37325;&#29616;&#26377;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#36164;&#28304;&#19981;&#22343;&#21644;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#19981;&#19968;&#33268;&#31561;&#20844;&#24179;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#20844;&#24179;&#38382;&#39064;&#23545;&#20110;&#38450;&#27490;&#31038;&#20250;&#19981;&#20844;&#27491;&#30340;&#36827;&#19968;&#27493;&#24041;&#22266;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#19982;&#21307;&#30103;&#20445;&#20581;&#19981;&#20844;&#24179;&#30340;&#20132;&#21449;&#28857;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#37197;&#20844;&#27491;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#23558;&#20844;&#24179;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#36164;&#28304;&#24179;&#22343;&#20998;&#37197;&#21644;&#24615;&#33021;&#24179;&#31561;&#12290;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#30456;&#20851;&#30340;&#20844;&#27491;&#24230;&#37327;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#65292;&#24182;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#20559;&#35265;&#21644;&#32531;&#35299;&#31574;&#30053;&#65292;&#25506;&#35752;&#20102;&#20559;&#35265;&#19982;&#20854;&#23545;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digitization of healthcare data coupled with advances in computational capabilities has propelled the adoption of machine learning (ML) in healthcare. However, these methods can perpetuate or even exacerbate existing disparities, leading to fairness concerns such as the unequal distribution of resources and diagnostic inaccuracies among different demographic groups. Addressing these fairness problem is paramount to prevent further entrenchment of social injustices. In this survey, we analyze the intersection of fairness in machine learning and healthcare disparities. We adopt a framework based on the principles of distributive justice to categorize fairness concerns into two distinct classes: equal allocation and equal performance. We provide a critical review of the associated fairness metrics from a machine learning standpoint and examine biases and mitigation strategies across the stages of the ML lifecycle, discussing the relationship between biases and their countermeasures. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15122</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#32423;&#23545;&#31216;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#20272;&#35745;&#36816;&#36755;&#24615;&#33021;&#21644;&#25506;&#32034;&#21475;&#34955;&#20301;&#28857;&#12290;&#36890;&#36807;&#25913;&#36827;&#25968;&#20540;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22686;&#24378;MD&#27169;&#25311;&#30340;&#25928;&#29575;&#24050;&#32463;&#26377;&#20102;&#24456;&#38271;&#30340;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#20934;&#30830;&#24314;&#27169;&#25193;&#23637;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralMD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;ML&#36741;&#21161;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#26694;&#26550;&#28385;&#36275;&#32676;&#23545;&#31216;&#24615;&#24182;&#25429;&#33719;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;BindingNet&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#22686;&#24378;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#23398;&#20064;&#36712;&#36857;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
&lt;/p&gt;</description></item><item><title>Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.15098</link><description>&lt;p&gt;
Hi-Core: &#38754;&#21521;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15098
&lt;/p&gt;
&lt;p&gt;
Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;Continual Reinforcement Learning, CRL&#65289;&#36171;&#20104;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992;&#23427;&#26469;&#20419;&#36827;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#22312;&#31867;&#20284;&#20219;&#21153;&#20043;&#38388;&#20256;&#36755;&#20302;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#30693;&#25511;&#21046;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#23548;&#33268;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#19981;&#36275;&#12290;&#20026;&#20102;&#22686;&#24378;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#36801;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#30001;&#20004;&#23618;&#32467;&#26500;&#32452;&#25104;&#65306;1) &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Model, LLM&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;2) &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#65288;&#31574;&#30053;&#24211;&#65289;&#26469;&#23384;&#20648;&#21487;&#20197;&#29992;&#20110;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;&#30340;&#31574;&#30053;&#12290;&#22312;MiniGr&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
&lt;/p&gt;</description></item><item><title>UMBRELLA&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29289;&#32852;&#32593;&#35797;&#39564;&#24179;&#21488;&#65292;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#26696;&#20363;&#65292;&#21253;&#25324;&#33258;&#21160;&#34903;&#28783;&#30417;&#25511;&#12289;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#12289;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#23481;&#22120;&#21270;&#24212;&#29992;&#20837;&#20405;&#26816;&#27979;&#12290;&#26410;&#26469;&#65292;UMBRELLA&#36824;&#26377;&#28508;&#21147;&#29992;&#20110;&#26234;&#33021;&#22478;&#24066;&#21644;&#22810;&#26426;&#22120;&#20154;&#32676;&#24863;&#30693;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13346</link><description>&lt;p&gt;
&#36807;&#21435;&#12289;&#29616;&#22312;&#12289;&#26410;&#26469;&#65306;&#23545;UMBRELLA&#29289;&#32852;&#32593;&#35797;&#39564;&#24179;&#21488;&#20013;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26696;&#20363;&#30340;&#20840;&#38754;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Past, Present, Future: A Comprehensive Exploration of AI Use Cases in the UMBRELLA IoT Testbed. (arXiv:2401.13346v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13346
&lt;/p&gt;
&lt;p&gt;
UMBRELLA&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29289;&#32852;&#32593;&#35797;&#39564;&#24179;&#21488;&#65292;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#26696;&#20363;&#65292;&#21253;&#25324;&#33258;&#21160;&#34903;&#28783;&#30417;&#25511;&#12289;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#12289;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#21644;&#23481;&#22120;&#21270;&#24212;&#29992;&#20837;&#20405;&#26816;&#27979;&#12290;&#26410;&#26469;&#65292;UMBRELLA&#36824;&#26377;&#28508;&#21147;&#29992;&#20110;&#26234;&#33021;&#22478;&#24066;&#21644;&#22810;&#26426;&#22120;&#20154;&#32676;&#24863;&#30693;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
UMBRELLA&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#24335;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#65292;&#21253;&#25324;200&#22810;&#20010;&#22810;&#20256;&#24863;&#22120;&#22810;&#26080;&#32447;&#33410;&#28857;&#12289;20&#20010;&#21327;&#20316;&#26426;&#22120;&#20154;&#21644;&#25903;&#25345;&#36793;&#32536;&#26234;&#33021;&#30340;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;UMBRELLA&#22312;&#23454;&#38469;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#24050;&#23454;&#29616;&#21644;&#28508;&#22312;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#25351;&#21335;&#12290;&#35814;&#32454;&#20171;&#32461;&#20102;&#22235;&#20010;&#29616;&#26377;&#30340;UMBRELLA&#24212;&#29992;&#31243;&#24207;&#65306;1&#65289;&#29992;&#20110;&#26816;&#27979;&#38382;&#39064;&#24182;&#35302;&#21457;&#32500;&#25252;&#35686;&#25253;&#30340;&#33258;&#21160;&#34903;&#28783;&#30417;&#25511;&#65307;2&#65289;&#25552;&#20379;&#22686;&#24378;&#30340;&#31354;&#27668;&#36136;&#37327;&#24863;&#30693;&#21644;&#38477;&#20302;&#25104;&#26412;&#30340;&#24314;&#31569;&#29615;&#22659;&#25968;&#23383;&#23402;&#29983;&#65307;3&#65289;&#29992;&#20110;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65307;4&#65289;&#29992;&#20110;&#35782;&#21035;&#24694;&#24847;&#27963;&#21160;&#30340;&#23481;&#22120;&#21270;&#24212;&#29992;&#20837;&#20405;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#36824;&#27010;&#36848;&#20102;UMBRELLA&#22312;&#26410;&#26469;&#26234;&#33021;&#22478;&#24066;&#21644;&#22810;&#26426;&#22120;&#20154;&#32676;&#24863;&#30693;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#22686;&#24378;&#20102;&#35821;&#20041;&#36890;&#20449;&#21644;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23454;&#29616;&#19978;&#36848;&#29992;&#20363;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UMBRELLA&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem incorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative robots, and edge-intelligence-enabled devices. This paper provides a guide to the implemented and prospective artificial intelligence (AI) capabilities of UMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are presented in detail: 1) An automated streetlight monitoring for detecting issues and triggering maintenance alerts; 2) A Digital twin of building environments providing enhanced air quality sensing with reduced cost; 3) A large-scale Federated Learning framework for reducing communication overhead; and 4) An intrusion detection for containerised applications identifying malicious activities. Additionally, the potential of UMBRELLA is outlined for future smart city and multi-robot crowdsensing applications enhanced by semantic communications and multi-agent planning. Finally, to realise the above use-cases we disc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21518;&#24724;&#21305;&#37197;+&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#24179;&#34913;&#33258;&#21338;&#24328;&#35757;&#32451;&#20013;&#19981;&#21516;&#35282;&#33394;&#30340;AI&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.12557</link><description>&lt;p&gt;
&#24179;&#34913;&#33258;&#21338;&#24328;&#35757;&#32451;&#20013;&#35282;&#33394;&#30340;AI&#33021;&#21147;&#19982;&#21518;&#24724;&#21305;&#37197;+&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Balancing the AI Strength of Roles in Self-Play Training with Regret Matching+. (arXiv:2401.12557v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12557
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21518;&#24724;&#21305;&#37197;+&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#24179;&#34913;&#33258;&#21338;&#24328;&#35757;&#32451;&#20013;&#19981;&#21516;&#35282;&#33394;&#30340;AI&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#28085;&#30422;&#22810;&#20010;&#35282;&#33394;&#30340;&#28216;&#25103;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#26102;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#25511;&#21046;&#28216;&#25103;&#20869;&#20219;&#24847;&#35282;&#33394;&#30340;&#36890;&#29992;&#27169;&#22411;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;&#36825;&#31181;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#22312;&#35757;&#32451;&#38454;&#27573;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#65292;&#32780;&#19988;&#21487;&#20197;&#20943;&#23569;&#37096;&#32626;&#26102;&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#25511;&#21046;&#19981;&#21516;&#35282;&#33394;&#26102;&#33021;&#21147;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#21305;&#37197;+&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#20197;&#20419;&#36827;&#27169;&#22411;&#22312;&#25511;&#21046;&#19981;&#21516;&#35282;&#33394;&#26102;&#26356;&#24179;&#34913;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training artificial intelligence for games encompassing multiple roles, the development of a generalized model capable of controlling any character within the game presents a viable option. This strategy not only conserves computational resources and time during the training phase but also reduces resource requirements during deployment. training such a generalized model often encounters challenges related to uneven capabilities when controlling different roles. A simple method is introduced based on Regret Matching+, which facilitates a more balanced performance of strength by the model when controlling various roles.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.12258</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#20852;&#25903;&#37197;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20154;&#31867;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35774;&#32622;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#28151;&#21512;&#21160;&#26426;&#20195;&#29702;&#21327;&#20316;&#21462;&#20915;&#20110;&#20010;&#20307;&#21644;&#32676;&#20307;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#31038;&#20250;&#20064;&#24815;&#21644;&#35268;&#33539;&#65292;&#24448;&#24448;&#21463;&#21040;&#20154;&#31867;&#26426;&#26500;&#30340;&#21551;&#21457;&#65292;&#34987;&#29992;&#20316;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#19988;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#31038;&#20250;&#20064;&#24815;&#65292;&#21363;&#25903;&#37197;&#31561;&#32423;&#65292;&#23427;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#23558;&#25903;&#37197;&#31561;&#32423;&#30340;&#34892;&#20026;&#29702;&#35770;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20462;&#25913;&#29616;&#26377;&#30340;&#26415;&#35821;&#21644;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#25110;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#32676;&#20307;&#33021;&#22815;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;&#25152;&#20135;&#29983;&#30340;&#25903;&#37197;&#31561;&#32423;&#26377;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11864</link><description>&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#20855;&#26377;&#23567;&#20110;&#21313;&#20159;&#21442;&#25968;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#23553;&#35013;&#20026;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;EoTD&#25968;&#25454;&#38598;&#26469;&#23545;SLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;SLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36825;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#31243;&#24207;&#21644;&#24605;&#32500;&#26041;&#31243;&#65289;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;EoTD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;ETD&#20351;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.09769</link><description>&lt;p&gt;
&#36208;&#21521;&#24322;&#36136;&#22270;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#24322;&#36136;&#22270;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#25110;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#22312;&#19981;&#26029;&#21162;&#21147;&#25512;&#36827;&#20174;&#24322;&#36136;&#22270;&#20013;&#23398;&#20064;&#12290;&#34429;&#28982;&#26377;&#20851;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#23384;&#22312;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#24322;&#36136;&#22270;&#23398;&#20064;&#30340;&#20854;&#20182;&#23376;&#20027;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#22810;&#31687;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23618;&#27425;&#20998;&#31867;&#27861;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#21253;&#25324;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.11482</link><description>&lt;p&gt;
AGI&#31995;&#32479;&#30340;&#20803;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#25552;&#31034;(meta prompting)&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#37325;&#26032;&#22609;&#36896;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#21033;&#29992;&#12290;&#22522;&#20110;&#31867;&#22411;&#29702;&#35770;&#21644;&#33539;&#30068;&#35770;&#65292;&#20803;&#25552;&#31034;&#27880;&#37325;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#20197;&#20869;&#23481;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20803;&#25552;&#31034;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;(few-shot prompting)&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#20851;&#27880;&#23558;&#20803;&#25552;&#31034;&#25193;&#23637;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#23637;&#31034;&#22914;&#20309;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#20998;&#25104;&#36739;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20196;&#29260;&#25928;&#29575;&#65292;&#24182;&#20351;&#38382;&#39064;&#27714;&#35299;&#30340;&#27604;&#36739;&#26356;&#21152;&#20844;&#24179;&#65292;&#23588;&#20854;&#26159;&#19982;&#23569;&#26679;&#26412;&#31034;&#20363;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#25552;&#31034;&#20219;&#21153;&#65292;&#20801;&#35768;LLMs&#20197;&#36845;&#20195;&#30340;&#20803;&#32534;&#31243;&#24418;&#24335;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21464;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#21270;&#23398;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#65288;RCR&#65289;&#20219;&#21153;&#65292;&#36890;&#36807;&#27169;&#25311;&#19987;&#23478;&#21270;&#23398;&#23478;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#26032;&#21453;&#24212;&#25351;&#32441;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#12290;&#27492;&#31995;&#32479;&#21487;&#20197;&#20943;&#36731;&#21270;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#26356;&#22522;&#30784;&#21644;&#21019;&#36896;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.10776</link><description>&lt;p&gt;
&#22312;&#21270;&#23398;&#21512;&#25104;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#20013;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generative Agent for Reaction Condition Recommendation in Chemical Synthesis. (arXiv:2311.10776v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21464;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#21270;&#23398;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#65288;RCR&#65289;&#20219;&#21153;&#65292;&#36890;&#36807;&#27169;&#25311;&#19987;&#23478;&#21270;&#23398;&#23478;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#26032;&#21453;&#24212;&#25351;&#32441;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#12290;&#27492;&#31995;&#32479;&#21487;&#20197;&#20943;&#36731;&#21270;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#26356;&#22522;&#30784;&#21644;&#21019;&#36896;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20026;&#21270;&#23398;&#31038;&#20250;&#20013;&#30340;&#33258;&#21160;&#21270;&#21270;&#23398;&#21453;&#24212;&#38138;&#24179;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21464;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#21270;&#23398;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#65288;RCR&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#27169;&#25311;&#19987;&#23478;&#21270;&#23398;&#23478;&#30340;&#25628;&#32034;&#21644;&#20998;&#26512;&#31574;&#30053;&#65292;&#35813;&#20195;&#29702;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#26597;&#35810;&#20998;&#23376;&#25968;&#25454;&#24211;&#65292;&#24182;&#20174;&#22312;&#32447;&#25991;&#29486;&#20013;&#25552;&#21462;&#20851;&#38190;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36824;&#37197;&#22791;&#20102;&#25105;&#20204;&#20026;RCR&#20219;&#21153;&#24320;&#21457;&#30340;&#26032;&#21453;&#24212;&#25351;&#32441;&#12290;&#30001;&#20110;RAG&#25216;&#26415;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#20351;&#29992;&#26356;&#26032;&#30340;&#22312;&#32447;&#25968;&#25454;&#24211;&#20316;&#20026;&#30693;&#35782;&#28304;&#65292;&#26174;&#33879;&#20248;&#20110;&#20165;&#21463;&#20854;&#35757;&#32451;&#25968;&#25454;&#22266;&#23450;&#30693;&#35782;&#38480;&#21046;&#30340;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;&#21270;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#26356;&#22522;&#30784;&#21644;&#21019;&#36896;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;&#36825;&#19968;&#37325;&#22823;&#36827;&#23637;&#23558;&#35745;&#31639;&#25216;&#26415;&#19982;&#21270;&#23398;&#31038;&#20250;&#26356;&#32039;&#23494;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent artificial intelligence (AI) research plots a promising future of automatic chemical reactions within the chemistry society. This study presents a transformative AI agent that automates the reaction condition recommendation (RCR) task in chemistry using retrieval-augmented generation (RAG) technology. By emulating expert chemists search and analysis strategies, the agent employs large language models (LLMs) to interrogate molecular databases and distill critical data from online literature. Further, the AI agent is equipped with our novel reaction fingerprint developed for the RCR task. Thanks to the RAG technology, our agent uses updated online databases as knowledge sources, significantly outperforming conventional AIs confined to the fixed knowledge within its training data. The resulting system can significantly reduce chemists workload, allowing them to focus on more fundamental and creative scientific problems. This significant advancement brings closer computational techn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36827;&#21270;&#28216;&#25103;&#35774;&#35745;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#29983;&#25104;&#12298;&#39118;&#38505;&#12299;&#28216;&#25103;&#30340;&#26032;&#21464;&#20307;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#26032;&#21464;&#20307;&#25317;&#26377;&#26356;&#23567;&#30340;&#22320;&#22270;&#21644;&#26356;&#30701;&#30340;&#27604;&#36187;&#26102;&#38388;&#65292;&#24182;&#20135;&#29983;&#26356;&#21152;&#24179;&#34913;&#30340;&#28216;&#25103;&#23545;&#23616;&#12290;</title><link>http://arxiv.org/abs/2310.20008</link><description>&lt;p&gt;
&#36827;&#21270;&#26700;&#38754;&#28216;&#25103;&#35774;&#35745;&#65306;&#20197;&#8220;&#39118;&#38505;&#28216;&#25103;&#8221;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Tabletop Game Design: A Case Study in the Risk Game. (arXiv:2310.20008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36827;&#21270;&#28216;&#25103;&#35774;&#35745;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#29983;&#25104;&#12298;&#39118;&#38505;&#12299;&#28216;&#25103;&#30340;&#26032;&#21464;&#20307;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#26032;&#21464;&#20307;&#25317;&#26377;&#26356;&#23567;&#30340;&#22320;&#22270;&#21644;&#26356;&#30701;&#30340;&#27604;&#36187;&#26102;&#38388;&#65292;&#24182;&#20135;&#29983;&#26356;&#21152;&#24179;&#34913;&#30340;&#28216;&#25103;&#23545;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#21019;&#36896;&#21644;&#35780;&#20272;&#28216;&#25103;&#26159;&#19968;&#39033;&#33392;&#24040;&#32780;&#36153;&#26102;&#30340;&#20219;&#21153;&#12290;&#31243;&#24207;&#29983;&#25104;&#20869;&#23481;&#21487;&#20197;&#36890;&#36807;&#21019;&#24314;&#28216;&#25103;&#20803;&#32032;&#26469;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#36890;&#24120;&#19981;&#33021;&#29983;&#25104;&#23436;&#25972;&#30340;&#28216;&#25103;&#12290;&#36827;&#21270;&#28216;&#25103;&#35774;&#35745;&#23558;&#36827;&#21270;&#31639;&#27861;&#19982;&#33258;&#21160;&#21270;&#27979;&#35797;&#30456;&#32467;&#21512;&#65292;&#24050;&#29992;&#20110;&#21019;&#24314;&#20855;&#22791;&#31616;&#21333;&#35774;&#22791;&#30340;&#26032;&#39062;&#26700;&#38754;&#28216;&#25103;&#65307;&#28982;&#32780;&#65292;&#21407;&#26377;&#26041;&#27861;&#24182;&#19981;&#21253;&#25324;&#24102;&#26377;&#39600;&#23376;&#12289;&#21345;&#29260;&#21644;&#22320;&#22270;&#31561;&#22797;&#26434;&#26700;&#38754;&#28216;&#25103;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#26700;&#38754;&#28216;&#25103;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#12298;&#39118;&#38505;&#12299;&#36825;&#27454;&#20891;&#20107;&#31574;&#30053;&#28216;&#25103;&#30340;&#21464;&#20307;&#26469;&#35780;&#20272;&#35813;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#36827;&#21270;&#25152;&#36873;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#35268;&#21017;&#20013;&#24515;&#20195;&#29702;&#27979;&#35797;&#28216;&#25103;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#36136;&#37327;&#26631;&#20934;&#35780;&#20272;&#29983;&#25104;&#30340;&#26032;&#21464;&#20307;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#21019;&#36896;&#20102;&#21407;&#22987;&#28216;&#25103;&#30340;&#26032;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20855;&#26377;&#36739;&#23567;&#30340;&#22320;&#22270;&#65292;&#23548;&#33268;&#27604;&#36187;&#26102;&#38388;&#26356;&#30701;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#21464;&#20307;&#20135;&#29983;&#20102;&#26356;&#21152;&#24179;&#34913;&#30340;&#27604;&#36187;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating and evaluating games manually is an arduous and laborious task. Procedural content generation can aid by creating game artifacts, but usually not an entire game. Evolutionary game design, which combines evolutionary algorithms with automated playtesting, has been used to create novel board games with simple equipment; however, the original approach does not include complex tabletop games with dice, cards, and maps. This work proposes an extension of the approach for tabletop games, evaluating the process by generating variants of Risk, a military strategy game where players must conquer map territories to win. We achieved this using a genetic algorithm to evolve the chosen parameters, as well as a rules-based agent to test the games and a variety of quality criteria to evaluate the new variations generated. Our results show the creation of new variations of the original game with smaller maps, resulting in shorter matches. Also, the variants produce more balanced matches, main
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#27599;&#20010;&#30446;&#26631;&#30340;&#23454;&#29616;&#20998;&#37197;&#32473;&#21333;&#29420;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#29420;&#31435;&#21019;&#24314;&#12289;&#20462;&#25913;&#21644;&#26367;&#25442;&#36825;&#20123;&#31574;&#30053;&#12290;&#20351;&#29992;&#25293;&#21334;&#26426;&#21046;&#26469;&#35299;&#20915;&#20914;&#31361;&#21644;&#32452;&#21512;&#31574;&#30053;&#65292;&#30830;&#20445;&#38271;&#26399;&#30340;&#35843;&#24230;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11798</link><description>&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Auction-Based Scheduling. (arXiv:2310.11798v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#27599;&#20010;&#30446;&#26631;&#30340;&#23454;&#29616;&#20998;&#37197;&#32473;&#21333;&#29420;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#29420;&#31435;&#21019;&#24314;&#12289;&#20462;&#25913;&#21644;&#26367;&#25442;&#36825;&#20123;&#31574;&#30053;&#12290;&#20351;&#29992;&#25293;&#21334;&#26426;&#21046;&#26469;&#35299;&#20915;&#20914;&#31361;&#21644;&#32452;&#21512;&#31574;&#30053;&#65292;&#30830;&#20445;&#38271;&#26399;&#30340;&#35843;&#24230;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#38656;&#35201;&#28385;&#36275;&#22810;&#20010;&#37096;&#20998;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#26041;&#27861;&#26159;&#25972;&#20307;&#21270;&#30340;&#65292;&#21363;&#36890;&#36807;&#19968;&#20010;&#20989;&#25968;&#26469;&#36873;&#25321;&#19968;&#31995;&#21015;&#21160;&#20316;&#26469;&#28385;&#36275;&#25152;&#26377;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#22810;&#30446;&#26631;&#20915;&#31574;&#26694;&#26550;&#12290;&#27599;&#20010;&#30446;&#26631;&#37117;&#20351;&#29992;&#21333;&#29420;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29420;&#31435;&#21019;&#24314;&#12289;&#20462;&#25913;&#21644;&#26367;&#25442;&#12290;&#21487;&#20197;&#29702;&#35299;&#30340;&#26159;&#65292;&#20855;&#26377;&#20914;&#31361;&#30446;&#26631;&#30340;&#19981;&#21516;&#31574;&#30053;&#21487;&#33021;&#22312;&#32473;&#23450;&#26102;&#38388;&#36873;&#25321;&#20914;&#31361;&#30340;&#21160;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#20914;&#31361;&#21644;&#32452;&#21512;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25293;&#21334;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#32473;&#27599;&#20010;&#31574;&#30053;&#20998;&#37197;&#19968;&#20010;&#26377;&#38480;&#30340;&#39044;&#31639;&#65292;&#22312;&#27599;&#19968;&#27493;&#65292;&#31574;&#30053;&#21516;&#26102;&#20174;&#21487;&#29992;&#30340;&#39044;&#31639;&#20013;&#20986;&#20215;&#26469;&#33719;&#21462;&#35843;&#24230;&#21644;&#36873;&#25321;&#21160;&#20316;&#30340;&#29305;&#26435;&#12290;&#31574;&#30053;&#20351;&#29992;&#20854;&#20986;&#20215;&#26469;&#34920;&#36798;&#35843;&#24230;&#30340;&#32039;&#36843;&#24615;&#65292;&#26377;&#38480;&#30340;&#39044;&#31639;&#30830;&#20445;&#20102;&#38271;&#26399;&#30340;&#35843;&#24230;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sequential decision-making tasks require satisfaction of multiple, partially contradictory objectives. Existing approaches are monolithic, namely all objectives are fulfilled using a single policy, which is a function that selects a sequence of actions. We present auction-based scheduling, a modular framework for multi-objective decision-making problems. Each objective is fulfilled using a separate policy, and the policies can be independently created, modified, and replaced. Understandably, different policies with conflicting goals may choose conflicting actions at a given time. In order to resolve conflicts, and compose policies, we employ a novel auction-based mechanism. We allocate a bounded budget to each policy, and at each step, the policies simultaneously bid from their available budgets for the privilege of being scheduled and choosing an action. Policies express their scheduling urgency using their bids and the bounded budgets ensure long-run scheduling fairness. We lay 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#29702;&#35299;&#21644;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21457;&#29616;&#21151;&#33021;&#23376;&#32593;&#32476;&#24182;&#21033;&#29992;&#23427;&#20204;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.10899</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Instilling Inductive Biases with Subnetworks. (arXiv:2310.10899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10899
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#29702;&#35299;&#21644;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21457;&#29616;&#21151;&#33021;&#23376;&#32593;&#32476;&#24182;&#21033;&#29992;&#23427;&#20204;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#20960;&#20046;&#27809;&#26377;&#30693;&#35782;&#25110;&#25511;&#21046;&#33021;&#21147;&#12290;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;--&#23545;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#20559;&#22909;--&#26159;&#29702;&#35299;&#21644;&#25511;&#21046;&#36825;&#20123;&#27169;&#22411;&#34892;&#20026;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#26469;&#30740;&#31350;&#27169;&#22411;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#35774;&#35745;&#30340;&#32467;&#26500;&#25110;&#31934;&#24515;&#31574;&#21010;&#30340;&#35757;&#32451;&#26041;&#24335;&#27880;&#20837;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26356;&#26426;&#26800;&#30340;&#26041;&#27861;&#65306;&#23376;&#20219;&#21153;&#24402;&#32435;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#27169;&#22411;&#20013;&#23454;&#29616;&#29305;&#23450;&#23376;&#20219;&#21153;&#30340;&#21151;&#33021;&#23376;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#27880;&#20837;&#23545;&#21033;&#29992;&#35813;&#23376;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#23376;&#20219;&#21153;&#24402;&#32435;&#28789;&#27963;&#39640;&#25928;&#65292;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00113</link><description>&lt;p&gt;
HyperMask: &#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00113
&lt;/p&gt;
&lt;p&gt;
HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#39034;&#24207;&#35757;&#32451;&#26102;&#65292;&#24448;&#24448;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#26368;&#26377;&#25928;&#30340;&#20043;&#19968;&#26159;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36229;&#32593;&#32476;&#26681;&#25454;&#20219;&#21153;&#30340;&#29305;&#24449;&#29983;&#25104;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#38480;&#21046;&#26159;&#36229;&#32593;&#32476;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#21487;&#20197;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#21333;&#29420;&#35299;&#20915;&#30340;&#12290;&#27169;&#22411;&#22312;&#23398;&#20064;&#21518;&#32493;&#20219;&#21153;&#26102;&#19981;&#20351;&#29992;&#20043;&#21069;&#20219;&#21153;&#25152;&#20851;&#32852;&#30340;&#32593;&#32476;&#20449;&#24687;&#65292;&#24182;&#23454;&#38469;&#19978;&#20135;&#29983;&#20102;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24425;&#31080;&#31080;&#35777;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#23384;&#22312;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#20013;&#22870;&#31080;&#65289;&#65292;&#21487;&#20197;&#20445;&#25345;&#23436;&#25972;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperMask&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#12290;&#36229;&#32593;&#32476;&#20135;&#29983;&#21322;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20197;&#33719;&#21462;&#30446;&#26631;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13063</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#24212;&#29992;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#25968;&#25454;&#21487;&#20197;&#25581;&#31034;&#29992;&#25143;&#19982;&#32593;&#32476;&#25628;&#32034;&#26381;&#21153;&#30340;&#20132;&#20114;&#26041;&#24335;&#12289;&#29992;&#25143;&#30340;&#38656;&#27714;&#20197;&#21450;&#28385;&#24847;&#31243;&#24230;&#31561;&#23453;&#36149;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26032;&#30340;&#32593;&#32476;&#25628;&#32034;&#24418;&#24335;&#65292;&#22914;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32842;&#22825;&#12290;&#20026;&#20102;&#29702;&#35299;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#29992;&#26377;&#24847;&#20041;&#30340;&#20998;&#31867;&#26041;&#24335;&#26631;&#35760;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#20854;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#19988;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#32780;&#35328;&#65292;&#35201;&#20040;&#20195;&#20215;&#39640;&#26114;&#35201;&#20040;&#19981;&#22815;&#28789;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20016;&#23500;&#19988;&#30456;&#20851;&#30340;&#27010;&#24565;&#12289;&#25551;&#36848;&#21644;&#31034;&#20363;&#26469;&#34920;&#31034;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLM&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26085;&#24535;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#36825;&#26679;&#30340;&#20998;&#31867;&#24471;&#19981;&#21040;&#22806;&#37096;&#39564;&#35777;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#19981;&#33391;&#30340;&#21453;&#39304;&#22238;&#36335;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#35780;&#20272;&#32773;&#26469;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21160;&#24577;&#35268;&#21010;&#24212;&#29992;&#20110;&#20915;&#31574;Transformer&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#20010;&#27493;&#39588;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#35780;&#20272;&#21160;&#20316;&#36136;&#37327;&#65292;&#24182;&#35757;&#32451;ACT&#29983;&#25104;&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#21160;&#20316;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05915</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21183;&#35843;&#33410;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#22686;&#24378;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning. (arXiv:2309.05915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21160;&#24577;&#35268;&#21010;&#24212;&#29992;&#20110;&#20915;&#31574;Transformer&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#20010;&#27493;&#39588;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#35780;&#20272;&#21160;&#20316;&#36136;&#37327;&#65292;&#24182;&#35757;&#32451;ACT&#29983;&#25104;&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#21160;&#20316;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;Transformer (DT) &#21033;&#29992;&#34920;&#36798;&#20016;&#23500;&#30340;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#26469;&#25191;&#34892;&#21160;&#20316;&#29983;&#25104;&#65292;&#24050;&#25104;&#20026;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;DT &#29983;&#25104;&#30340;&#21160;&#20316;&#26159;&#22522;&#20110;&#26399;&#26395;&#26410;&#26469;&#22238;&#25253;&#30340;&#26465;&#20214;&#65292;&#24050;&#30693;&#20855;&#26377;&#26576;&#20123;&#24369;&#28857;&#65292;&#27604;&#22914;&#26131;&#21463;&#29615;&#22659;&#38543;&#26426;&#24615;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;DT&#30340;&#24369;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;DT&#20013;&#22686;&#21152;&#21160;&#24577;&#35268;&#21010;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#26469;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#36825;&#28041;&#21450;&#21040;MDP&#32467;&#26500;&#19978;&#30340;&#21160;&#24577;&#35268;&#21010;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#26469;&#35780;&#20272;&#21160;&#20316;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#20248;&#21183;&#20272;&#35745;&#22120;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20197;&#20272;&#35745;&#30340;&#20248;&#21183;&#20026;&#26465;&#20214;&#29983;&#25104;&#21160;&#20316;&#30340;&#20248;&#21183;&#26465;&#20214;Transformer (ACT)&#12290;&#26368;&#21518;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;ACT&#26681;&#25454;&#25152;&#38656;&#30340;&#20248;&#21183;&#29983;&#25104;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#23398;&#29983;&#30340;&#31572;&#39064;&#35760;&#24405;&#20013;&#30452;&#25509;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#31572;&#39064;&#35760;&#24405;&#26469;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00300</link><description>&lt;p&gt;
&#29992;&#32534;&#30721;-&#35299;&#30721;&#22120;&#36827;&#34892;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#26469;&#24314;&#27169;&#23398;&#29983;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance. (arXiv:2309.00300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#23398;&#29983;&#30340;&#31572;&#39064;&#35760;&#24405;&#20013;&#30452;&#25509;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#31572;&#39064;&#35760;&#24405;&#26469;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#26088;&#22312;&#26681;&#25454;&#23398;&#29983;&#22312;&#32771;&#35797;&#39064;&#30446;&#19978;&#30340;&#31572;&#39064;&#25104;&#32489;&#26469;&#35786;&#26029;&#20182;&#20204;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#36825;&#26159;&#35768;&#22810;&#39046;&#22495;&#22914;&#35745;&#31639;&#33258;&#36866;&#24212;&#27979;&#35797;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65288;CDMs&#65289;&#36981;&#24490;&#20102;&#19968;&#20010;&#33021;&#21147;-&#21709;&#24212;&#33539;&#24335;&#65292;&#21363;&#23558;&#35786;&#26029;&#32467;&#26524;&#35270;&#20026;&#23398;&#29983;&#21709;&#24212;&#30340;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26469;&#23398;&#20064;&#35786;&#26029;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#24456;&#23481;&#26131;&#23548;&#33268;&#19981;&#21487;&#35782;&#21035;&#30340;&#35786;&#26029;&#32467;&#26524;&#21644;&#35299;&#37322;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#23398;&#29983;&#23398;&#20064;&#34920;&#29616;&#30340;&#37327;&#21270;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#35786;&#26029;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#30452;&#25509;&#20174;&#21709;&#24212;&#26085;&#24535;&#20013;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#39044;&#27979;&#27169;&#22359;&#20174;&#35786;&#26029;&#32467;&#26524;&#20013;&#37325;&#24314;&#21709;&#24212;&#26085;&#24535;&#65292;&#20197;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive diagnosis aims to diagnose students' knowledge proficiencies based on their response scores on exam questions, which is the basis of many domains such as computerized adaptive testing. Existing cognitive diagnosis models (CDMs) follow a proficiency-response paradigm, which views diagnostic results as learnable embeddings that are the cause of students' responses and learns the diagnostic results through optimization. However, such a paradigm can easily lead to unidentifiable diagnostic results and the explainability overfitting problem, which is harmful to the quantification of students' learning performance. To address these problems, we propose a novel identifiable cognitive diagnosis framework. Specifically, we first propose a flexible diagnostic module which directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure
&lt;/p&gt;</description></item><item><title>FECoM&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#22240;&#32032;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.12264</link><description>&lt;p&gt;
FECoM: &#26397;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning. (arXiv:2308.12264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12264
&lt;/p&gt;
&lt;p&gt;
FECoM&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#22240;&#32032;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20351;&#29992;&#12289;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20854;&#33021;&#28304;&#28040;&#32791;&#36805;&#36895;&#22686;&#38271;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20419;&#36827;&#32511;&#33394;&#21457;&#23637;&#21644;&#19981;&#21516;&#31890;&#24230;&#30340;&#33021;&#28304;&#24847;&#35782;&#65292;&#20197;&#38480;&#21046;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#30899;&#25490;&#25918;&#26159;&#24403;&#21153;&#20043;&#24613;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20934;&#30830;&#27979;&#37327;&#21644;&#20248;&#21270;&#32454;&#31890;&#24230;&#65288;&#20363;&#22914;&#26041;&#27861;&#32423;&#21035;&#65289;&#33021;&#32791;&#30340;&#26631;&#20934;&#21644;&#21487;&#37325;&#22797;&#24037;&#20855;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FECoM&#65288;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#20202;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FECoM&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;FECoM&#36890;&#36807;&#20351;&#29992;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#21508;&#31181;&#22240;&#32032;&#26469;&#35299;&#20915;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;FECoM&#22312;&#26368;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#19978;&#27979;&#37327;&#32454;&#31890;&#24230;&#33021;&#32791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most p
&lt;/p&gt;</description></item><item><title>"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"</title><link>http://arxiv.org/abs/2308.10974</link><description>&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#65306;&#19968;&#31181;&#30740;&#31350;&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#30340;&#21019;&#26032;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;"
&lt;/p&gt;
&lt;p&gt;
"Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion. (arXiv:2308.10974v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10974
&lt;/p&gt;
&lt;p&gt;
"&#24341;&#29992;GPT&#30340;&#8220;&#35930;&#40736;&#35797;&#39564;&#8221;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#36827;&#34892;&#31454;&#20105;&#21644;&#21246;&#32467;&#30740;&#31350;&#12290;&#23427;&#27604;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#31454;&#20105;&#21644;&#21246;&#32467;&#28041;&#21450;&#22797;&#26434;&#30340;&#21160;&#24577;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20225;&#19994;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22797;&#26434;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#28041;&#21450;&#20154;&#31867;&#20027;&#20307;&#25110;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#25506;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26234;&#33021;&#20195;&#29702;&#24314;&#27169;&#65288;SABM&#65289;&#65292;&#20854;&#20013;&#30001;GPT-4&#25216;&#26415;&#25903;&#25345;&#30340;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#20225;&#19994;&#24182;&#30456;&#20114;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25511;&#21046;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#20225;&#19994;&#20215;&#26684;&#31454;&#20105;&#21644;&#21246;&#32467;&#34892;&#20026;&#12290;&#19982;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#36827;&#34892;&#23454;&#39564;&#30456;&#27604;&#65292;SABM&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#28789;&#27963;&#24615;&#12290;&#26234;&#33021;&#20195;&#29702;&#25317;&#26377;&#20915;&#31574;&#30340;&#24191;&#27867;&#30693;&#35782;&#24211;&#65292;&#23637;&#29616;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#25112;&#30053;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#24182;&#20010;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#30740;&#31350;&#28041;&#21450;&#27807;&#36890;&#30340;&#22797;&#26434;&#24773;&#20917;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02594</link><description>&lt;p&gt;
SMARLA&#65306;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#30830;&#20445;DRL&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#27979;&#35797;&#26159;&#19981;&#36275;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#33021;&#25552;&#20379;&#20445;&#35777;&#12290;&#26500;&#24314;&#23433;&#20840;&#30417;&#27979;&#22120;&#26159;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SMARLA&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;&#65292;&#19987;&#20026;DRL&#26234;&#33021;&#20307;&#35774;&#35745;&#12290;&#20986;&#20110;&#23454;&#38469;&#21407;&#22240;&#65292;SMARLA&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;(&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#26234;&#33021;&#20307;&#30340;&#20869;&#37096;)&#65292;&#24182;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#26469;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#20174;&#32780;&#20419;&#36827;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#23398;&#20064;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;RL&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;SMARLA&#12290;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#35823;&#25253;&#29575;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#19968;&#21322;&#24038;&#21491;&#30340;&#26089;&#26399;&#38454;&#27573;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#25351;&#20986;&#20102;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#33258;&#20027;&#33021;&#21147;&#12289;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#20197;&#21450;&#20307;&#39564;&#21040;&#27169;&#31946;&#38382;&#39064;&#30340;&#22823;&#19990;&#30028;&#31561;&#29305;&#28857;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#21017;&#19982;&#27492;&#30456;&#21453;&#12290;</title><link>http://arxiv.org/abs/2307.07515</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#31639;&#27861;&#27169;&#20223;&#65306;&#20026;&#20160;&#20040;&#20154;&#24037;&#8220;&#20195;&#29702;&#8221;&#19981;&#26159;&#65288;&#20063;&#19981;&#20250;&#25104;&#20026;&#65289;&#30495;&#27491;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is algorithmic mimicry: why artificial "agents" are not (and won't be) proper agents. (arXiv:2307.07515v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#25351;&#20986;&#20102;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#33258;&#20027;&#33021;&#21147;&#12289;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#20197;&#21450;&#20307;&#39564;&#21040;&#27169;&#31946;&#38382;&#39064;&#30340;&#22823;&#19990;&#30028;&#31561;&#29305;&#28857;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#21017;&#19982;&#27492;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;&#27604;&#29983;&#29289;&#31995;&#32479;&#21644;&#31639;&#27861;&#31995;&#32479;&#65292;&#37325;&#28857;&#25506;&#35752;&#8220;&#20195;&#29702;&#8221;&#27010;&#24565;&#65292;&#26469;&#25506;&#35752;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;&#20316;&#32773;&#25351;&#20986;&#20102;&#19977;&#20010;&#22522;&#26412;&#30340;&#24046;&#24322;&#65306;&#65288;1&#65289;&#29983;&#29289;&#31995;&#32479;&#20855;&#26377;&#33258;&#25105;&#21046;&#36896;&#30340;&#33258;&#20027;&#33021;&#21147;&#65292;&#33021;&#22815;&#35774;&#23450;&#33258;&#36523;&#30340;&#20869;&#22312;&#30446;&#26631;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#23384;&#22312;&#20110;&#19968;&#20010;&#30001;&#22806;&#37096;&#20195;&#29702;&#25552;&#20379;&#30446;&#26631;&#20989;&#25968;&#30340;&#35745;&#31639;&#29615;&#22659;&#20013;&#12290;&#65288;2&#65289;&#29983;&#29289;&#31995;&#32479;&#26159;&#20855;&#20307;&#20307;&#29616;&#30340;&#65292;&#21363;&#20854;&#31526;&#21495;&#21644;&#29289;&#29702;&#26041;&#38754;&#27809;&#26377;&#21306;&#20998;&#65292;&#32780;&#31639;&#27861;&#36816;&#34892;&#22312;&#35745;&#31639;&#32467;&#26500;&#19978;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#23558;&#36719;&#20214;&#19982;&#30828;&#20214;&#38548;&#31163;&#12290;&#65288;3&#65289;&#29983;&#29289;&#31995;&#32479;&#20307;&#39564;&#21040;&#19968;&#20010;&#24222;&#22823;&#30340;&#19990;&#30028;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38382;&#39064;&#26159;&#27169;&#31946;&#30340;&#65288;&#24182;&#38750;&#20840;&#37096;&#21487;&#23450;&#20041;&#65289;&#65292;&#32780;&#31639;&#27861;&#31995;&#32479;&#23384;&#22312;&#20110;&#19968;&#20010;&#23567;&#19990;&#30028;&#20013;&#65292;&#20854;&#20013;&#25152;&#26377;&#38382;&#39064;&#37117;&#26159;&#26126;&#30830;&#30340;&#12290;&#36825;&#19977;&#20010;&#24046;&#24322;&#35828;&#26126;&#20102;&#29983;&#29289;&#21644;&#31639;&#27861;&#31995;&#32479;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the prospect of developing artificial general intelligence (AGI)? I investigate this question by systematically comparing living and algorithmic systems, with a special focus on the notion of "agency." There are three fundamental differences to consider: (1) Living systems are autopoietic, that is, self-manufacturing, and therefore able to set their own intrinsic goals, while algorithms exist in a computational environment with target functions that are both provided by an external agent. (2) Living systems are embodied in the sense that there is no separation between their symbolic and physical aspects, while algorithms run on computational architectures that maximally isolate software from hardware. (3) Living systems experience a large world, in which most problems are ill-defined (and not all definable), while algorithms exist in a small world, in which all problems are well-defined. These three differences imply that living and algorithmic systems have very different capab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#23436;&#25972;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#22914;&#20309;&#20174;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#21487;&#36776;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16958</link><description>&lt;p&gt;
&#30452;&#25509;&#25928;&#24212;&#22312;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of direct effects from summary causal graphs. (arXiv:2306.16958v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#23436;&#25972;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#22914;&#20309;&#20174;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#21487;&#36776;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#30452;&#25509;&#25928;&#24212;&#65292;&#21363;&#34913;&#37327;&#19968;&#20010;&#21464;&#37327;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#21464;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21464;&#37327;&#19981;&#21464;&#12290;&#21160;&#24577;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#29992;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#26469;&#36827;&#34892;&#23450;&#24615;&#34920;&#31034;&#12290;&#20551;&#35774;&#32447;&#24615;&#21644;&#22240;&#26524;&#20805;&#20998;&#24615;&#65292;&#24182;&#32473;&#23450;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#24635;&#26159;&#21487;&#36776;&#35782;&#30340;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#30001;&#25152;&#35859;&#30340;&#21333;&#38376;&#20934;&#21017;&#32473;&#20986;&#30340;&#20219;&#20309;&#21464;&#37327;&#38598;&#21512;&#26469;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#27809;&#26377;&#27492;&#31867;&#22270;&#24418;&#21487;&#29992;&#65292;&#20294;&#19987;&#23478;&#20173;&#28982;&#21487;&#20197;&#35775;&#38382;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#19968;&#20010;&#25277;&#35937;&#65292;&#35813;&#25277;&#35937;&#34920;&#31034;&#20102;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#21516;&#26102;&#30465;&#30053;&#20102;&#26102;&#38388;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#65292;&#20854;&#20013;&#35814;&#32454;&#25551;&#36848;&#20102;&#25152;&#26377;&#30452;&#25509;&#25928;&#24212;&#22312;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#21487;&#36776;&#35782;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic structural causal models (SCMs) are a powerful framework for reasoning in dynamic systems about direct effects which measure how a change in one variable affects another variable while holding all other variables constant. The causal relations in a dynamic structural causal model can be qualitatively represented with a full-time causal graph. Assuming linearity and causal sufficiency and given the full-time causal graph, the direct causal effect is always identifiable and can be estimated from data by adjusting on any set of variables given by the so-called single-door criterion. However, in many application such a graph is not available for various reasons but nevertheless experts have access to an abstraction of the full-time causal graph which represents causal relations between time series while omitting temporal information. This paper presents a complete identifiability result which characterizes all cases for which the direct effect is graphically identifiable from summa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#30340;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#65292;&#22312;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#27604;&#36739;&#20013;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15880</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards Open Vocabulary Learning: A Survey. (arXiv:2306.15880v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#30340;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#65292;&#22312;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#27604;&#36739;&#20013;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#26816;&#27979;&#31561;&#21508;&#31181;&#26680;&#24515;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22522;&#20110;&#23553;&#38381;&#38598;&#30340;&#20551;&#35774;&#65292;&#21363;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#24050;&#23450;&#20041;&#30340;&#31867;&#21035;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#12290;&#36825;&#20123;&#26032;&#26041;&#27861;&#26088;&#22312;&#23450;&#20301;&#21644;&#35782;&#21035;&#36229;&#20986;&#27880;&#37322;&#26631;&#31614;&#31354;&#38388;&#30340;&#31867;&#21035;&#12290;&#19982;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#30456;&#27604;&#65292;&#24320;&#25918;&#35789;&#27719;&#26041;&#27861;&#26356;&#21152;&#36890;&#29992;&#12289;&#23454;&#29992;&#21644;&#26377;&#25928;&#12290;&#26412;&#25991;&#23545;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#36817;&#26399;&#22312;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#24320;&#25918;&#38598;&#35782;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#31561;&#30456;&#20851;&#27010;&#24565;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#22312;&#20998;&#21106;&#20219;&#21153;&#30340;&#20960;&#20010;&#32039;&#23494;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of visual scene understanding, deep neural networks have made impressive advancements in various core tasks like segmentation, tracking, and detection. However, most approaches operate on the close-set assumption, meaning that the model can only identify pre-defined categories that are present in the training set. Recently, open vocabulary settings were proposed due to the rapid progress of vision language pre-training. These new approaches seek to locate and recognize categories beyond the annotated label space. The open vocabulary approach is more general, practical, and effective compared to weakly supervised and zero-shot settings. This paper provides a thorough review of open vocabulary learning, summarizing and analyzing recent developments in the field. In particular, we begin by comparing it to related concepts such as zero-shot learning, open-set recognition, and out-of-distribution detection. Then, we review several closely related tasks in the case of segmentati
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01879</link><description>&lt;p&gt;
VisualGPTScore: &#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#20998;&#25968;&#30340;&#35270;&#35273;&#35821;&#20041;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01879
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; VisualGPTScore &#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#26469;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#19978;&#36816;&#31639;&#12290;&#19982;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#30340;VLM&#21482;&#26159;&#26080;&#24847;&#20041;&#30340;&#21333;&#35789;&#34955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340; VisualGPTScore &#22312; ARO &#21644; Crepe &#31561;&#26368;&#36817;&#25552;&#20986;&#30340;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#39030;&#23574;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#25237;&#24433;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12289;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#21644;&#36719;&#26368;&#22823;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23481;&#37327;&#24046;&#24322;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11098</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A closer look at the training dynamics of knowledge distillation. (arXiv:2303.11098v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#25237;&#24433;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12289;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#21644;&#36719;&#26368;&#22823;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23481;&#37327;&#24046;&#24322;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#23558;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#20989;&#25968;&#21305;&#37197;&#21644;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#39564;&#35777;&#19977;&#20010;&#37325;&#35201;&#35774;&#35745;&#20915;&#31574;&#65292;&#21363;&#26631;&#20934;&#21270;&#12289;&#36719;&#26368;&#22823;&#20989;&#25968;&#21644;&#25237;&#24433;&#23618;&#20316;&#20026;&#20851;&#38190;&#35201;&#32032;&#65292;&#25105;&#20204;&#26377;&#29702;&#35770;&#22320;&#26174;&#31034;&#20986;&#25237;&#24433;&#22120;&#38544;&#21547;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#36807;&#21435;&#26679;&#26412;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#23398;&#29983;&#25552;&#20379;&#20102;&#20851;&#32852;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#19982;&#25237;&#24433;&#22120;&#30340;&#35757;&#32451;&#21160;&#24577;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#21487;&#33021;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36719;&#26368;&#22823;&#20989;&#25968;&#21487;&#20197;&#29992;&#26469;&#35299;&#20915;&#20219;&#20309;&#26174;&#33879;&#23481;&#37327;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#25110;&#20248;&#20110;&#20854;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;LQR&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#36890;&#36807;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13144</link><description>&lt;p&gt;
&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;LQR&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient. (arXiv:2302.13144v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;LQR&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#36890;&#36807;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#12290;&#32467;&#21512;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#22312;&#949;-&#33539;&#25968;&#24847;&#20041;&#19979;&#25509;&#36817;LQR&#26368;&#20248;&#35299;&#30340;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#26368;&#36817;&#23558;RHPG&#24212;&#29992;&#20110;&#23398;&#20064;&#21345;&#23572;&#26364;&#28388;&#27874;&#20013;&#36827;&#34892;&#25299;&#23637;&#20998;&#26512;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit in this paper the discrete-time linear quadratic regulator (LQR) problem from the perspective of receding-horizon policy gradient (RHPG), a newly developed model-free learning framework for control applications. We provide a fine-grained sample complexity analysis for RHPG to learn a control policy that is both stabilizing and $\epsilon$-close to the optimal LQR solution, and our algorithm does not require knowing a stabilizing control policy for initialization. Combined with the recent application of RHPG in learning the Kalman filter, we demonstrate the general applicability of RHPG in linear control and estimation with streamlined analyses.
&lt;/p&gt;</description></item></channel></rss>