<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20110;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01349</link><description>&lt;p&gt;
&#36229;&#36234;&#31572;&#26696;&#65306;&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01349
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20110;&#22810;&#36873;&#39064;&#22238;&#31572;&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#19968;&#22330;&#33539;&#24335;&#36716;&#21464;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#23545;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#20173;&#28982;&#26159;&#31038;&#21306;&#38754;&#20020;&#30340;&#24517;&#28982;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23558;&#22810;&#36873;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#20316;&#20026;LLMs&#30340;&#22522;&#20934;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;MCQA&#20316;&#20026;LLMs&#35780;&#20272;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;&#22914;&#26524;LLMs&#30495;&#27491;&#29702;&#35299;&#38382;&#39064;&#30340;&#35821;&#20041;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24212;&#35813;&#22312;&#20174;&#30456;&#21516;&#38382;&#39064;&#27966;&#29983;&#30340;&#21508;&#31181;&#37197;&#32622;&#19978;&#34920;&#29616;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;LLMs&#30340;&#21709;&#24212;&#19968;&#33268;&#24615;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25105;&#20204;&#23558;&#20043;&#23450;&#20041;&#20026;LLMs&#30340;&#21709;&#24212;&#21487;&#21464;&#24615;&#32508;&#21512;&#24449;&#65288;REVAS&#65289;&#65292;&#36825;&#34920;&#26126;&#30446;&#21069;&#22522;&#20110;MCQA&#30340;&#22522;&#20934;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;LLMs&#30340;&#30495;&#23454;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#21512;&#36866;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions. Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01107</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#27169;&#25311;&#22270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulation of Graph Algorithms with Looped Transformers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#22270;&#31639;&#27861;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#30001;&#20110;&#26377;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#35777;&#36827;&#23637;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#33021;&#22815;&#20351;&#29992;&#20851;&#31995;&#25968;&#25454;&#22797;&#21046;&#25512;&#29702;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#24102;&#39069;&#22806;&#27880;&#24847;&#21147;&#22836;&#21644;&#19982;&#22270;&#24418;&#20132;&#20114;&#30340;&#24490;&#29615;&#21464;&#21387;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#20102;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#27169;&#25311;&#35832;&#22914;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#31561;&#31639;&#27861;&#12290;&#32593;&#32476;&#30340;&#23485;&#24230;&#19981;&#38543;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#22686;&#21152;&#65292;&#36825;&#24847;&#21619;&#30528;&#32593;&#32476;&#21487;&#20197;&#27169;&#25311;&#20219;&#20309;&#22270;&#19978;&#30340;&#19978;&#36848;&#31639;&#27861;&#12290;&#23613;&#31649;&#26377;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#26377;&#19968;&#20010;&#30001;&#20110;&#26377;&#38480;&#31934;&#24230;&#32780;&#21463;&#21040;&#38480;&#21046;&#30340;&#27169;&#25311;&#26497;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The execution of graph algorithms using neural networks has recently attracted significant interest due to promising empirical progress. This motivates further understanding of how neural networks can replicate reasoning steps with relational data. In this work, we study the ability of transformer networks to simulate algorithms on graphs from a theoretical perspective. The architecture that we utilize is a looped transformer with extra attention heads that interact with the graph. We prove by construction that this architecture can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth- and Depth-First Search, and Kosaraju's strongly connected components algorithm. The width of the network does not increase with the size of the input graph, which implies that the network can simulate the above algorithms for any graph. Despite this property, we show that there is a limit to simulation in our solution due to finite precision. Finally, we show a Turing Completeness resu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25554;&#20837;&#26469;&#28304;&#27169;&#22411;&#35789;&#27719;&#30340;&#26041;&#24335;&#65292;&#25104;&#21151;&#23454;&#26045;&#20102;&#23545;&#20004;&#20010;&#28909;&#38376;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30446;&#26631;&#21163;&#25345;&#65292;&#21019;&#36896;&#20986;&#38590;&#20197;&#26816;&#27979;&#30340;&#19981;&#24341;&#20154;&#27880;&#30446;&#25351;&#20196;&#12290;</title><link>https://arxiv.org/abs/2404.02637</link><description>&lt;p&gt;
&#35789;&#27719;&#25915;&#20987;&#20197;&#21163;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vocabulary Attack to Hijack Large Language Model Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02637
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25554;&#20837;&#26469;&#28304;&#27169;&#22411;&#35789;&#27719;&#30340;&#26041;&#24335;&#65292;&#25104;&#21151;&#23454;&#26045;&#20102;&#23545;&#20004;&#20010;&#28909;&#38376;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30446;&#26631;&#21163;&#25345;&#65292;&#21019;&#36896;&#20986;&#38590;&#20197;&#26816;&#27979;&#30340;&#19981;&#24341;&#20154;&#27880;&#30446;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#25512;&#21160;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#20063;&#30475;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#25915;&#20987;&#32773;&#35797;&#22270;&#26234;&#32988;&#36825;&#20123;&#31995;&#32479;&#12290;&#20182;&#20204;&#24076;&#26395;&#27169;&#22411;&#36879;&#38706;&#26426;&#23494;&#20449;&#24687;&#12289;&#29305;&#23450;&#38169;&#35823;&#20449;&#24687;&#25110;&#20882;&#29359;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#20182;&#20204;&#36890;&#36807;&#25554;&#20837;&#20998;&#38548;&#31526;&#25110;&#31995;&#32479;&#24615;&#22320;&#25913;&#20889;&#25351;&#20196;&#65292;&#30452;&#21040;&#36798;&#21040;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20247;&#19981;&#21516;&#12290;&#23427;&#25554;&#20837;&#26469;&#33258;&#27169;&#22411;&#35789;&#27719;&#30340;&#35789;&#27719;&#12290;&#25105;&#20204;&#20351;&#29992;&#20248;&#21270;&#36807;&#31243;&#21644;&#26469;&#33258;&#21478;&#19968;&#20010;LLM&#65288;&#25915;&#20987;&#32773;LLM&#65289;&#30340;&#23884;&#20837;&#26469;&#25214;&#21040;&#36825;&#20123;&#35789;&#27719;&#12290;&#25105;&#20204;&#36890;&#36807;&#21163;&#25345;&#20004;&#20010;&#28909;&#38376;&#24320;&#28304;LLM&#65288;&#20998;&#21035;&#26469;&#33258;Llama2&#21644;Flan-T5&#31995;&#21015;&#65289;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#19981;&#24341;&#20154;&#27880;&#30446;&#30340;&#25351;&#20196;&#65292;&#22240;&#27492;&#24456;&#38590;&#26816;&#27979;&#12290;&#23545;&#20110;&#35768;&#22810;&#25915;&#20987;&#26696;&#20363;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02637v1 Announce Type: cross  Abstract: The fast advancements in Large Language Models (LLMs) are driving an increasing number of applications. Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems. They want the model to reveal confidential information, specific false information, or offensive behavior. To this end, they manipulate their instructions for the LLM by inserting separators or rephrasing them systematically until they reach their goal. Our approach is different. It inserts words from the model vocabulary. We find these words using an optimization procedure and embeddings from another LLM (attacker LLM). We prove our approach by goal hijacking two popular open-source LLMs from the Llama2 and the Flan-T5 families, respectively. We present two main findings. First, our approach creates inconspicuous instructions and therefore it is hard to detect. For many attack cases, we find that even a single 
&lt;/p&gt;</description></item><item><title>DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00242</link><description>&lt;p&gt;
DeFT&#65306;&#24102;IO&#24847;&#35782;&#30340;Flash Tree-attention&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00242
&lt;/p&gt;
&lt;p&gt;
DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26641;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#26029;&#36136;&#37327;&#12290;&#26681;&#25454;&#24341;&#23548;&#20449;&#21495;&#65292;&#23427;&#36890;&#36807;&#24418;&#25104;LLM&#36755;&#20986;&#20174;&#26681;&#21040;&#21494;&#23376;&#30340;&#26368;&#20339;&#36335;&#24452;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#23545;&#40784;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#20887;&#20313;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#20869;&#23384;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#26641;&#35299;&#30721;&#31574;&#30053;&#21450;&#20854;&#25512;&#26029;&#31995;&#32479;&#20114;&#30456;&#19981;&#36866;&#37197;&#65292;&#23548;&#33268;&#25512;&#26029;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeFT&#65292;&#19968;&#31181;IO&#24863;&#30693;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#23427;&#22312;&#20004;&#20010;&#38454;&#27573;&#20013;&#20445;&#25345;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65306;&#65288;1&#65289;QKV&#20934;&#22791;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;KV&#24341;&#23548;&#26641;&#20998;&#35010;&#31574;&#30053;&#65292;&#20026;GPU&#30340;&#39640;&#21033;&#29992;&#29575;&#21644;&#23613;&#21487;&#33021;&#20943;&#23569;GPU&#20840;&#23616;&#20869;&#23384;&#21644;&#33455;&#29255;&#19978;&#20849;&#20139;&#20869;&#23384;&#20043;&#38388;&#30340;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#35835;/&#20889;; &#65288;2&#65289;&#27880;&#24847;&#21147;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
&lt;/p&gt;</description></item><item><title>Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19546</link><description>&lt;p&gt;
Croissant&#65306;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Croissant: A Metadata Format for ML-Ready Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19546
&lt;/p&gt;
&lt;p&gt;
Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20851;&#38190;&#36164;&#28304;&#65292;&#20294;&#22788;&#29702;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25705;&#25830;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Croissant&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#34987;ML&#24037;&#20855;&#21644;&#26694;&#26550;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;Croissant&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;Croissant&#24050;&#24471;&#21040;&#20960;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#24211;&#30340;&#25903;&#25345;&#65292;&#28085;&#30422;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21152;&#36733;&#21040;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;&#36830;&#32493;&#36755;&#20986;&#30340;&#25910;&#25947;&#36895;&#29575;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#27604;&#38598;&#25104;&#26041;&#27861;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20808;&#36827;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.16732</link><description>&lt;p&gt;
&#22312;&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Enabling Uncertainty Estimation in Iterative Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16732
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;&#36830;&#32493;&#36755;&#20986;&#30340;&#25910;&#25947;&#36895;&#29575;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#27604;&#38598;&#25104;&#26041;&#27861;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20808;&#36827;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20256;&#36882;&#32593;&#32476;&#26550;&#26500;&#36716;&#21464;&#20026;&#36845;&#20195;&#32593;&#32476;&#26550;&#26500;&#65292;&#36845;&#20195;&#32593;&#32476;&#20351;&#29992;&#33258;&#36523;&#30340;&#36755;&#20986;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#21319;&#24615;&#33021;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#26550;&#26500;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#65306;&#36830;&#32493;&#36755;&#20986;&#30340;&#25910;&#25947;&#36895;&#29575;&#19982;&#20854;&#25910;&#25947;&#20540;&#30340;&#20934;&#30830;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#25910;&#25947;&#36895;&#29575;&#29992;&#20316;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#29992;&#20195;&#29702;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#27604;&#35832;&#22914;&#38598;&#25104;&#26041;&#27861;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#20272;&#35745;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#23545;&#21407;&#22987;&#36845;&#20195;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#23884;&#20837;&#21040;&#20004;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#26469;&#23637;&#31034;&#20854;&#23454;&#29992;&#20215;&#20540;&#65306;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#36947;&#36335;&#26816;&#27979;&#21644;&#20108;&#32500;&#21644;&#19977;&#32500;&#24418;&#29366;&#30340;&#31354;&#27668;&#21160;&#21147;&#29305;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16732v1 Announce Type: new  Abstract: Turning pass-through network architectures into iterative ones, which use their own output as input, is a well-known approach for boosting performance. In this paper, we argue that such architectures offer an additional benefit: The convergence rate of their successive outputs is highly correlated with the accuracy of the value to which they converge. Thus, we can use the convergence rate as a useful proxy for uncertainty. This results in an approach to uncertainty estimation that provides state-of-the-art estimates at a much lower computational cost than techniques like Ensembles, and without requiring any modifications to the original iterative model. We demonstrate its practical value by embedding it in two application domains: road detection in aerial images and the estimation of aerodynamic properties of 2D and 3D shapes.
&lt;/p&gt;</description></item><item><title>SegICL&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26032;&#20219;&#21153;&#20013;&#36866;&#24212;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#25110;&#36827;&#34892;&#22797;&#26434;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.16578</link><description>&lt;p&gt;
SegICL&#65306;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#21307;&#23398;&#25104;&#20687;&#20998;&#21106;&#30340;&#36890;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16578
&lt;/p&gt;
&lt;p&gt;
SegICL&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26032;&#20219;&#21153;&#20013;&#36866;&#24212;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#25110;&#36827;&#34892;&#22797;&#26434;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20197;&#26032;&#20219;&#21153;&#20013;&#36866;&#24212;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#12290;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;&#26088;&#22312;&#27178;&#36328;&#21307;&#23398;&#22270;&#20687;&#30340;&#19981;&#21516;&#27169;&#24577;&#36827;&#34892;&#27010;&#25324;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#22312;&#24212;&#29992;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#65288;OOD&#65289;&#25968;&#25454;&#27169;&#24577;&#21644;&#20219;&#21153;&#26102;&#36890;&#24120;&#20250;&#20943;&#24369;&#65292;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#24494;&#35843;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SegICL&#65292;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;SegICL&#33021;&#22815;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#20998;&#21106;&#24182;&#20351;&#29992;&#19968;&#23567;&#32452;&#22270;&#20687;-&#25513;&#30721;&#23545;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#25110;&#20026;OOD&#20219;&#21153;&#65288;&#21253;&#25324;OOD&#27169;&#24577;&#21644;&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;SegICL&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#65292;&#25552;&#31034;&#31034;&#20363;&#25968;&#37327;&#19982;&#20998;&#21106;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16578v1 Announce Type: cross  Abstract: Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2403.15401</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#31995;&#32479;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Mental Health: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15401
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#22312;&#25345;&#32493;&#35752;&#35770;&#20013;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#35780;&#20215;&#26088;&#22312;&#24635;&#32467;&#21644;&#34920;&#24449;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35843;&#26597;LLMs&#26368;&#26032;&#30740;&#31350;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35752;&#35770;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#20197;&#21450;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26681;&#25454;PRISMA&#25351;&#21335;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;PubMed&#12289;DBLP&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#25968;&#25454;&#24211;&#21644;IEEE Xplore&#19978;&#21457;&#34920;&#30340;&#33521;&#25991;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2017&#24180;1&#26376;1&#26085;&#33267;2023&#24180;9&#26376;1&#26085;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#29702;&#20581;&#24247;&#21644;LLMs&#12290;&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;32&#31687;&#25991;&#31456;&#65292;&#21253;&#25324;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#65288;n=13&#65289;&#12289;&#24515;&#29702;&#20581;&#24247;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;n=10&#65289;&#20197;&#21450;&#20854;&#20182;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#65288;n=9&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15401v1 Announce Type: cross  Abstract: Large language models (LLMs) have received much attention and shown their potential in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to summarize and characterize the use of LLMs in mental health by investigating the strengths and limitations of the latest work in LLMs and discusses the challenges and opportunities for early screening, digital interventions, and other clinical applications in mental health. Following PRISMA guidelines, we examined English articles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore, published between 1 January 2017, and 1 September 2023, focusing on mental health and LLMs. The review analyzed 32 articles, including mental health analysis using social media datasets (n=13), mental health chatbots (n=10), and other mental health applications (n=9). Findings reveal LLMs' effectiveness in mental health issue detection and the
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07262</link><description>&lt;p&gt;
&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advantage-Aware Policy Optimization for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33268;&#21147;&#20110;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#21046;&#23450;&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#22312;&#32447;&#20132;&#20114;&#65292;&#36890;&#36807;&#22312;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#19979;&#26045;&#21152;&#36866;&#24403;&#30340;&#20445;&#23432;&#32422;&#26463;&#26469;&#35299;&#20915;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#26500;&#24314;&#38024;&#23545;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#23398;&#20064;&#20248;&#21183;&#24863;&#30693;&#31574;&#30053;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05300</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#32479;&#19968;&#22810;&#26679;&#24615;&#65306;&#25913;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unity by Diversity: Improved Representation Learning in Multimodal VAEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#34920;&#31034;&#23398;&#20064;&#12289;&#26377;&#26465;&#20214;&#29983;&#25104;&#21644;&#22635;&#34917;&#12290;&#30446;&#21069;&#30340;&#26550;&#26500;&#35201;&#20040;&#36328;&#27169;&#24577;&#20849;&#20139;&#32534;&#30721;&#22120;&#36755;&#20986;&#12289;&#35299;&#30721;&#22120;&#36755;&#20837;&#65292;&#35201;&#20040;&#20004;&#32773;&#37117;&#35201;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#20005;&#26684;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#36719;&#32422;&#26463;&#21462;&#20195;&#36825;&#20123;&#30828;&#32422;&#26463;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#36719;&#24615;&#22320;&#24341;&#23548;&#27599;&#20010;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26397;&#30528;&#20849;&#20139;&#30340;&#21518;&#39564;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20248;&#31168;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#32534;&#30721;&#20445;&#30041;&#26469;&#33258;&#20854;&#26410;&#21387;&#32553;&#21407;&#22987;&#29305;&#24449;&#26356;&#22909;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#21644;&#22635;&#34917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#30005;&#35805;&#23545;&#35805;&#39046;&#22495;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#30005;&#35805;&#36890;&#35759;&#26465;&#20214;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2403.04280</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#21628;&#21483;&#39046;&#22495;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#30005;&#35805;&#23545;&#35805;&#39046;&#22495;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#30005;&#35805;&#36890;&#35759;&#26465;&#20214;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#24341;&#20837;&#19968;&#20010;&#20840;&#38754;&#30340;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#19987;&#38376;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#38463;&#25289;&#20271;&#35821;&#20197;&#20854;&#20016;&#23500;&#30340;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#35821;&#38899;&#22797;&#26434;&#24615;&#32780;&#38395;&#21517;&#65292;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#30005;&#35805;&#36890;&#35805;&#39046;&#22495;&#36827;&#19968;&#27493;&#25918;&#22823;&#65292;&#37027;&#37324;&#30340;&#38899;&#39057;&#36136;&#37327;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#20250;&#35805;&#24335;&#35821;&#38899;&#39118;&#26684;&#20250;&#23545;&#35782;&#21035;&#20934;&#30830;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#65292;&#19981;&#20165;&#21253;&#21547;&#24191;&#27867;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#33539;&#22260;&#65292;&#36824;&#27169;&#25311;&#21628;&#21483;&#36890;&#35759;&#30340;&#30495;&#23454;&#26465;&#20214;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#26679;&#30340;&#26041;&#35328;&#34920;&#36798;&#65292;&#24182;&#32771;&#34385;&#21628;&#21483;&#24405;&#38899;&#30340;&#21487;&#21464;&#36136;&#37327;&#65292;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#24212;&#23545;&#23454;&#38469;&#25361;&#25112;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04280v1 Announce Type: new  Abstract: This work is an attempt to introduce a comprehensive benchmark for Arabic speech recognition, specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic speech recognition (ASR) systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational speech styles negatively affect recognition accuracy. Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25299;&#23637;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#20102;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#32763;&#35793;&#25968;&#25454;&#35780;&#20272;&#21644;&#22686;&#24378;&#32531;&#35299;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#37327;&#23545;&#32531;&#35299;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03893</link><description>&lt;p&gt;
&#20174;&#21333;&#19968;&#21040;&#22810;&#26679;&#65306;&#25299;&#23637;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03893
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25299;&#23637;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27602;&#24615;&#32531;&#35299;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#20102;&#22810;&#35821;&#35328;&#29615;&#22659;&#65292;&#36890;&#36807;&#32763;&#35793;&#25968;&#25454;&#35780;&#20272;&#21644;&#22686;&#24378;&#32531;&#35299;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#37327;&#23545;&#32531;&#35299;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27602;&#24615;&#32531;&#35299;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#20013;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#25317;&#25265;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#23433;&#20840;&#25514;&#26045;&#36319;&#19978;&#27493;&#20240;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#27602;&#24615;&#32531;&#35299;&#33539;&#22260;&#25193;&#23637;&#21040;&#24212;&#23545;&#22810;&#35821;&#35328;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#32570;&#20047;&#36328;&#35821;&#35328;&#30340;&#36275;&#22815;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#26469;&#35780;&#20272;&#21644;&#22686;&#24378;&#25105;&#20204;&#30340;&#32531;&#35299;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#22312;&#38745;&#24577;&#21644;&#25345;&#32493;&#27602;&#24615;&#32531;&#35299;&#22330;&#26223;&#19979;&#27604;&#36739;&#20102;&#24494;&#35843;&#32531;&#35299;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26816;&#39564;&#32763;&#35793;&#36136;&#37327;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#23545;&#27602;&#24615;&#32531;&#35299;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#25968;&#37327;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#32531;&#35299;&#24037;&#20316;&#30340;&#25104;&#21151;&#12290;&#28085;&#30422;&#20102;&#20061;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#24191;&#27867;&#30340;&#35821;&#35328;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03893v1 Announce Type: cross  Abstract: To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01643</link><description>&lt;p&gt;
&#24744;&#38656;&#35201;&#26356;&#22909;&#22320;&#20851;&#27880;&#20184;&#36153;
&lt;/p&gt;
&lt;p&gt;
You Need to Pay Better Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#20248;&#21270;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#20284;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#22235;&#20998;&#20043;&#19977;&#65292;&#27599;&#20010;&#22836;&#37096;&#23569;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#27599;&#20010;&#22836;&#37096;&#20943;&#23569;&#20102;&#20004;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#27880;&#24847;&#21147;&#24555;&#20004;&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#32423;&#27880;&#24847;&#21147;&#65292;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36234;&#20102;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#30697;&#38453;&#20056;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#20005;&#26684;&#30340;&#25968;&#23398;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MN&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
&lt;/p&gt;</description></item><item><title>ReMatch&#26041;&#27861;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#27169;&#24335;&#21305;&#37197;&#65292;&#36991;&#20813;&#20102;&#39044;&#23450;&#20041;&#26144;&#23556;&#12289;&#27169;&#22411;&#35757;&#32451;&#25110;&#23545;&#28304;&#25968;&#25454;&#24211;&#25968;&#25454;&#30340;&#35775;&#38382;&#12290;</title><link>https://arxiv.org/abs/2403.01567</link><description>&lt;p&gt;
ReMatch: &#22522;&#20110;LLMs&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#24335;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ReMatch: Retrieval Enhanced Schema Matching with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01567
&lt;/p&gt;
&lt;p&gt;
ReMatch&#26041;&#27861;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#27169;&#24335;&#21305;&#37197;&#65292;&#36991;&#20813;&#20102;&#39044;&#23450;&#20041;&#26144;&#23556;&#12289;&#27169;&#22411;&#35757;&#32451;&#25110;&#23545;&#28304;&#25968;&#25454;&#24211;&#25968;&#25454;&#30340;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#24335;&#21305;&#37197;&#22312;&#25968;&#25454;&#38598;&#25104;&#20013;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#28041;&#21450;&#23558;&#28304;&#25968;&#25454;&#24211;&#27169;&#24335;&#19982;&#30446;&#26631;&#27169;&#24335;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#24314;&#31435;&#23427;&#20204;&#20803;&#32032;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReMatch&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#21305;&#37197;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#39044;&#23450;&#20041;&#26144;&#23556;&#12289;&#27169;&#22411;&#35757;&#32451;&#25110;&#23545;&#28304;&#25968;&#25454;&#24211;&#20013;&#25968;&#25454;&#30340;&#35775;&#38382;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01567v1 Announce Type: cross  Abstract: Schema matching is a crucial task in data integration, involving the alignment of a source database schema with a target schema to establish correspondence between their elements. This task is challenging due to textual and semantic heterogeneity, as well as differences in schema sizes. Although machine-learning-based solutions have been explored in numerous studies, they often suffer from low accuracy, require manual mapping of the schemas for model training, or need access to source schema data which might be unavailable due to privacy concerns. In this paper we present a novel method, named ReMatch, for matching schemas using retrieval-enhanced Large Language Models (LLMs). Our method avoids the need for predefined mapping, any model training, or access to data in the source database. In the ReMatch method the tables of the target schema and the attributes of the source schema are first represented as structured passage-based docume
&lt;/p&gt;</description></item><item><title>UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.00131</link><description>&lt;p&gt;
UniTS: &#26500;&#24314;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniTS: Building a Unified Time Series Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00131
&lt;/p&gt;
&lt;p&gt;
UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LLMs&#65292;&#27491;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#25110;&#24494;&#35843;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#35768;&#22810;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#36866;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22266;&#26377;&#22810;&#26679;&#24615;&#21644;&#22810;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#20854;&#20182;&#31867;&#22411;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#35268;&#33539;&#20998;&#27495;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#19987;&#29992;&#27169;&#22411;&#30340;&#26126;&#26174;&#38656;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;UNITS&#65292;&#19968;&#31181;&#25903;&#25345;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#30340;&#32479;&#19968;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#23481;&#32435;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#30340;&#65292;&#35813;&#39592;&#24178;&#32467;&#21512;&#20102;&#24207;&#21015;&#21644;&#21464;&#37327;&#27880;&#24847;&#21147;&#20197;&#21450;&#21160;&#24577;&#32447;&#24615;&#31639;&#23376;&#65292;&#24182;&#20316;&#20026;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;38&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#65292;UNITS&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18496</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Language Models Represent Beliefs of Self and Others
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18496
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#24402;&#22240;&#24515;&#29702;&#29366;&#24577;&#65292;&#21363;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#65292;&#34987;&#35270;&#20026;&#20154;&#31867;&#31038;&#20250;&#25512;&#29702;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20284;&#20046;&#20855;&#26377;&#26576;&#20123;ToM&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#30721;&#21508;&#20010;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#26159;&#21487;&#33021;&#30340;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#33258;&#25105;&#30340;&#20869;&#37096;&#34920;&#36848;&#21644;&#20182;&#20154;&#20449;&#24565;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#34920;&#24449;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;ToM&#24615;&#33021;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#24310;&#20280;&#21040;&#28041;&#21450;&#19981;&#21516;&#22240;&#26524;&#25512;&#29702;&#27169;&#24335;&#30340;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;&#36825;&#20123;&#34920;&#24449;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18496v1 Announce Type: new  Abstract: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
&lt;/p&gt;</description></item><item><title>RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17257</link><description>&lt;p&gt;
RIME: &#20855;&#26377;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17257
&lt;/p&gt;
&lt;p&gt;
RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#23545;&#22870;&#21169;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;PbRL&#31639;&#27861;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#23548;&#33268;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RIME&#65292;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#22024;&#26434;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#37492;&#21035;&#22120;&#65292;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#20197;&#36827;&#34892;&#20581;&#22766;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#19981;&#27491;&#30830;&#36896;&#25104;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#27492;&#22806;&#36824;&#33021;&#22635;&#34917;PbRL&#20013;&#20174;&#39044;&#35757;&#32451;&#21040;&#22312;&#32447;&#35757;&#32451;&#36807;&#28193;&#26102;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#36816;&#21160;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RIME&#26174;&#33879;&#25552;&#21319;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#28909;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.15159</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Pre-trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32972;&#26223;&#19979;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20197;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#26126;&#26174;&#32570;&#20047;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;LLMs&#20013;&#21246;&#21202;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19971;&#31181;&#19981;&#21516;&#36951;&#24536;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;arXiv&#12289;&#20070;&#31821;&#21644;GitHub&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#26426;&#22120;&#36951;&#24536;&#24615;&#33021;&#22522;&#20934;&#65292;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#27604;&#37325;&#26032;&#35757;&#32451;&#39640;&#20986; $10^5$ &#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#23558;&#26799;&#24230;&#19978;&#21319;&#19982;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#35814;&#32454;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#26377;&#20851;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14800</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#19987;&#23478;&#37117;&#30456;&#31561;: &#28151;&#21512;&#19987;&#23478;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;
&lt;/p&gt;
&lt;p&gt;
Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14800
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#23637;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#26159;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;LLMs&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;LLMs&#30456;&#27604;&#65292;MoE LLMs&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#65292;&#20173;&#28982;&#24456;&#38590;&#37096;&#32626;&#23427;&#20204;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#19987;&#38376;&#35774;&#35745;&#30340;&#30828;&#20214;&#30340;&#26435;&#37325;&#21098;&#26525;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#20027;&#35201;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#21363;&#25554;&#21363;&#29992;&#30340;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#26469;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;MoE LLMs&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#22686;&#21152;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#39281;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#24322;&#36136;&#23458;&#25143;&#36164;&#28304;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#21644;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.11505</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#35821;&#35328;&#20219;&#21153;&#21644;&#23458;&#25143;&#36164;&#28304;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#24322;&#36136;&#23458;&#25143;&#36164;&#28304;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#21644;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#24212;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#30340;&#36164;&#28304;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#65292;&#36825;&#24341;&#21457;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;LLM&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#32531;&#35299;&#20256;&#32479;FL&#20013;&#30340;&#8220;&#26742;&#25928;&#24212;&#8221;&#65292;&#35813;&#25928;&#24212;&#38480;&#21046;&#20102;&#25317;&#26377;&#20016;&#23500;&#36164;&#28304;&#30340;&#23458;&#25143;&#23454;&#29616;&#28508;&#21147;&#65292;&#23558;&#20182;&#20204;&#19982;&#26368;&#32570;&#20047;&#36164;&#28304;&#30340;&#21442;&#19982;&#32773;&#30340;&#33021;&#21147;&#25414;&#32465;&#22312;&#19968;&#36215;&#12290;FlexLoRA&#20801;&#35768;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#65292;&#20419;&#36827;&#20840;&#23616;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#36171;&#20104;&#26356;&#24191;&#27867;&#12289;&#19981;&#22826;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20174;&#20010;&#20307;&#23458;&#25143;&#36129;&#29486;&#20013;&#21512;&#25104;&#23436;&#25972;&#22823;&#23567;&#30340;LoRA&#26435;&#37325;&#65292;&#24182;&#21033;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;FlexLoRA&#20805;&#20998;&#21033;&#29992;&#20102;&#23458;&#25143;&#38388;&#30340;&#36164;&#28304;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#28041;&#21450;&#36229;&#36807;1600&#20010;&#25191;&#34892;&#22810;&#26679;NLP&#20219;&#21153;&#30340;&#23458;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11505v1 Announce Type: cross  Abstract: Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the "buckets effect" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, ou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.10466</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#22120;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#26085;&#30410;&#26222;&#36941;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#19968;&#33324;&#24773;&#22659;&#20013;&#20855;&#26377;&#20808;&#36827;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#19981;&#20165;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#36824;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#20869;&#36827;&#34892;&#26377;&#25928;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#65288;DST&#65289;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#35299;&#20915;LLMs&#20013;&#30340;DST&#30340;&#26032;&#26041;&#27861;FnCTOD&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#20102;&#38646;-shot DST&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#24320;&#28304;&#25110;&#19987;&#26377;LLMs&#26102;&#37117;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20351;&#24471;&#21508;&#31181;7B&#25110;13B&#21442;&#25968;&#27169;&#22411;&#36229;&#36234;&#20102;&#20043;&#21069;&#30001;ChatGPT&#23454;&#29616;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#65288;SOTA&#65289;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#20987;&#36133;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#36125;&#22612;&#20998;&#24067;&#21051;&#30011;&#20559;&#22909;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26657;&#20934;&#27169;&#22411;&#19982;&#20559;&#22909;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#26368;&#32456;&#21033;&#29992;&#26399;&#26395;&#22870;&#21169;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.09764</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#24314;&#27169;&#23545;&#40784;&#20247;&#21253;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Aligning Crowd Feedback via Distributional Preference Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#36125;&#22612;&#20998;&#24067;&#21051;&#30011;&#20559;&#22909;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26657;&#20934;&#27169;&#22411;&#19982;&#20559;&#22909;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#26368;&#32456;&#21033;&#29992;&#26399;&#26395;&#22870;&#21169;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24191;&#27867;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22870;&#21169;&#24314;&#27169;&#20027;&#35201;&#20381;&#36182;&#20110;&#19968;&#32452;&#20010;&#20307;&#25552;&#20379;&#30340;&#20154;&#31867;&#26631;&#27880;&#12290;&#36825;&#31181;&#20381;&#36182;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#20542;&#21521;&#20110;&#21453;&#26144;&#36825;&#20123;&#26631;&#27880;&#32773;&#30340;&#20542;&#21521;&#65292;&#20174;&#32780;&#26410;&#33021;&#20805;&#20998;&#20195;&#34920;&#26356;&#24191;&#27867;&#20154;&#32676;&#30340;&#26399;&#26395;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#8212;&#8212;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;(DPRM)&#65292;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#22612;&#20998;&#24067;&#26469;&#21051;&#30011;&#20559;&#22909;&#65292;&#35813;&#20998;&#24067;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20559;&#22909;&#36235;&#21183;&#30340;&#27874;&#21160;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26657;&#20934;DPRM&#19982;&#20559;&#22909;&#20998;&#24067;&#30340;&#23545;&#40784;&#24230;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#26399;&#26395;&#22870;&#21169;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09764v1 Announce Type: new  Abstract: Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM polic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07865</link><description>&lt;p&gt;
&#36879;&#35270;VLMs&#65306;&#25506;&#32034;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#23545;&#35805;&#12289;&#22330;&#26223;&#29702;&#35299;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#24212;&#29992;&#20419;&#20351;&#20102;&#20687;LLaVa&#12289;InstructBLIP&#21644;PaLI-3&#31561;&#35768;&#22810;&#26032;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#26377;&#36825;&#20040;&#22810;&#26032;&#30340;&#21457;&#24067;&#65292;&#20294;&#20851;&#20110;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#26550;&#26500;&#21644;&#20248;&#21270;&#30340;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#36825;&#19968;&#25361;&#25112;&#21448;&#22240;&#32570;&#20047;&#23458;&#35266;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#35270;&#35273;&#38382;&#31572;&#12289;&#20174;&#35821;&#35328;&#20013;&#23450;&#20301;&#29289;&#20307;&#20197;&#21450;&#25506;&#32034;&#24187;&#35273;&#31561;&#23646;&#24615;&#30340;&#30446;&#26631;&#25361;&#25112;&#38598;&#65292;&#36825;&#20123;&#35780;&#20272;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;VLM&#33021;&#21147;&#30340;&#31934;&#32454;&#12289;&#20934;&#30830;&#30340;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20851;&#38190;&#30340;&#35774;&#35745;&#36724;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20351;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Midi-Tuning&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06967</link><description>&lt;p&gt;
&#19968;&#27425;&#25351;&#23548;&#65292;&#22810;&#36718;&#31283;&#23450;&#23545;&#35805;&#65306;&#23545;&#35805;&#30340;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Midi-Tuning&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#35805;&#29983;&#25104;&#24050;&#25104;&#20026;&#26500;&#24314;&#33021;&#21147;&#24378;&#22823;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35843;&#25972;&#26041;&#24335;&#29421;&#38552;&#22320;&#23558;&#23545;&#35805;&#29983;&#25104;&#35270;&#20026;&#31867;&#20284;&#20854;&#20182;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#24573;&#35270;&#20102;&#23545;&#35805;&#32773;&#20043;&#38388;&#30340;&#35282;&#33394;&#24046;&#24322;&#21644;&#23545;&#35805;&#24212;&#20855;&#22791;&#30340;&#22810;&#36718;&#20132;&#20114;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#24335;&#23548;&#33268;&#20102;&#25152;&#26500;&#24314;&#20195;&#29702;&#20154;&#30340;&#23545;&#35805;&#19968;&#33268;&#24615;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#23545;&#35805;&#30340;&#20132;&#20114;&#24615;&#21644;&#27807;&#36890;&#24615;&#65292;&#24182;&#35748;&#20026;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#30340;&#35762;&#35805;&#32773;&#35282;&#33394;&#36827;&#34892;&#24314;&#27169;&#26356;&#20026;&#21487;&#34892;&#65292;&#20351;&#24471;&#20195;&#29702;&#20154;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#35843;&#25972;&#65288;Midi-Tuning&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#20010;&#36866;&#37197;&#22120;&#20998;&#21035;&#23545;&#20195;&#29702;&#20154;&#21644;&#29992;&#25143;&#36827;&#34892;&#24314;&#27169;&#65292;&#23427;&#20204;&#25353;&#36718;&#27425;&#20132;&#26367;&#20351;&#29992;&#35805;&#35821;&#65292;&#24182;&#36890;&#36807;&#36718;&#27425;&#32423;&#20869;&#23384;&#32531;&#23384;&#26426;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.06782</link><description>&lt;p&gt;
&#19982;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;LLMs&#36777;&#35770;&#20250;&#23548;&#33268;&#26356;&#30495;&#23454;&#30340;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Debating with More Persuasive LLMs Leads to More Truthful Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25152;&#38656;&#34892;&#20026;&#19968;&#33268;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#23558;&#36229;&#36807;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#20154;&#31867;&#35780;&#20272;&#30340;&#35282;&#33394;&#23558;&#28436;&#21464;&#20026;&#38750;&#19987;&#23478;&#30417;&#30563;&#19987;&#23478;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#25105;&#20204;&#38382;&#65306;&#26356;&#24369;&#30340;&#27169;&#22411;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#22312;&#31867;&#20284;&#30340;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26356;&#24378;&#30340;&#27169;&#22411;&#65288;&#19987;&#23478;&#65289;&#25317;&#26377;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#32780;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#38750;&#19987;&#23478;&#65289;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#26041;&#27861;&#26159;\textit{&#36777;&#35770;}&#65292;&#20854;&#20013;&#20004;&#20010;LLM&#19987;&#23478;&#20998;&#21035;&#25903;&#25345;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#19968;&#20010;&#38750;&#19987;&#23478;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#36777;&#35770; consistently&#24110;&#21161;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#65292;&#20998;&#21035;&#36798;&#21040;76%&#21644;88%&#30340;&#20934;&#30830;&#24615;&#65288;&#26420;&#32032;&#22522;&#20934;&#20998;&#21035;&#20026;48%&#21644;60%&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#20248;&#21270;&#19987;&#23478;&#36777;&#35770;&#32773;&#30340;&#35828;&#26381;&#21147;&#20250;&#25552;&#39640;&#38750;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable classifiers for tabular data via discretization and feature selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#20998;&#31867;&#22120;&#26159;&#31616;&#30701;&#30340;DNF&#20844;&#24335;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#31163;&#25955;&#21270;&#20026;&#24067;&#23572;&#24418;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#32467;&#21512;&#38750;&#24120;&#24555;&#36895;&#30340;&#31639;&#27861;&#26469;&#20135;&#29983;&#26368;&#20339;&#30340;&#24067;&#23572;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;14&#20010;&#23454;&#39564;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;&#32467;&#26524;&#30340;&#20934;&#30830;&#24230;&#20027;&#35201;&#19982;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#20197;&#21450;&#25991;&#29486;&#20013;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#29616;&#26377;&#32467;&#26524;&#30456;&#20284;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#38469;&#19978;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#21442;&#32771;&#32467;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#30340;&#21363;&#26102;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20851;&#20110;&#20174;&#29616;&#23454;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#20998;&#31867;&#22120;&#19982;&#26469;&#33258;&#25968;&#25454;&#32972;&#26223;&#20998;&#24067;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#30456;&#23545;&#24212;&#30340;&#27010;&#29575;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data. The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting. We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature. In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers. We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05140</link><description>&lt;p&gt;
Tag-LLM: &#23558;&#36890;&#29992;&#30340;LLM&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#20877;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19987;&#38376;&#39046;&#22495;&#20013;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26410;&#20805;&#20998;&#28085;&#30422;&#30340;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36890;&#29992;LLMs&#37325;&#26032;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26377;&#25928;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#21521;&#37327;&#24182;&#38468;&#21152;&#21040;LLMs&#30340;&#23884;&#20837;&#23618;&#65292;&#20197;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26631;&#31614;&#65306;&#39046;&#22495;&#26631;&#31614;&#29992;&#20110;&#38480;&#23450;&#19987;&#19994;&#34920;&#31034;&#65288;&#20363;&#22914;&#21270;&#23398;&#24335;&#65289;&#24182;&#25552;&#20379;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65307;&#21151;&#33021;&#26631;&#31614;&#29992;&#20110;&#34920;&#31034;&#29305;&#23450;&#30340;&#21151;&#33021;&#65288;&#20363;&#22914;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#24182;&#21387;&#32553;&#21151;&#33021;&#35299;&#20915;&#25351;&#20196;&#12290;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36825;&#20123;&#26631;&#31614;&#30340;&#21327;&#35758;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03286</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#19968;&#33268;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Training-Free Consistent Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#36896;&#24615;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#19979;&#19968;&#33268;&#22320;&#25551;&#32472;&#30456;&#21516;&#30340;&#20027;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26469;&#25945;&#25480;&#23427;&#25551;&#36848;&#29305;&#23450;&#29992;&#25143;&#25552;&#20379;&#20027;&#39064;&#30340;&#26032;&#35789;&#27719;&#25110;&#32773;&#20026;&#27169;&#22411;&#28155;&#21152;&#22270;&#20687;&#26465;&#20214;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#38024;&#23545;&#27599;&#20010;&#20027;&#39064;&#36827;&#34892;&#28459;&#38271;&#30340;&#20248;&#21270;&#25110;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#21644;&#25551;&#32472;&#22810;&#20010;&#20027;&#39064;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#35757;&#32451;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#26469;&#23454;&#29616;&#19968;&#33268;&#30340;&#20027;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20027;&#39064;&#39537;&#21160;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20197;&#20419;&#36827;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31574;&#30053;&#20197;&#40723;&#21169;&#24067;&#23616;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining su
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.03271</link><description>&lt;p&gt;
&#24819;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#25628;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27604;&#22914;&#21307;&#23398;&#35786;&#26029;&#21644;&#25925;&#38556;&#25490;&#38500;&#65292;&#35299;&#20915;&#20219;&#21153;&#25152;&#38656;&#30340;&#20449;&#24687;&#19981;&#26159;&#21021;&#22987;&#32473;&#23450;&#30340;&#65292;&#32780;&#38656;&#35201;&#36890;&#36807;&#35810;&#38382;&#21518;&#32493;&#38382;&#39064;&#26469;&#20027;&#21160;&#23547;&#27714;&#65288;&#20363;&#22914;&#65292;&#21307;&#29983;&#21521;&#24739;&#32773;&#35810;&#38382;&#30151;&#29366;&#30340;&#26356;&#22810;&#32454;&#33410;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24605;&#24819;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;UoT&#65289;&#65292;&#19968;&#31181;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#20027;&#21160;&#25552;&#38382;&#20449;&#24687;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;UoT&#32467;&#21512;&#20102;1&#65289;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20223;&#30495;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#21487;&#33021;&#30340;&#26410;&#26469;&#22330;&#26223;&#65292;&#24182;&#20272;&#35745;&#20854;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;2&#65289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#65292;&#28608;&#21169;&#27169;&#22411;&#23547;&#27714;&#20449;&#24687;&#65307;3&#65289;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#30340;&#26041;&#24335;&#36873;&#25321;&#26368;&#20339;&#30340;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#25925;&#38556;&#25490;&#38500;&#21644;'20&#30340;&#23454;&#39564;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#25552;&#39640;&#22270;&#20687;&#29702;&#35299;&#21644;&#20943;&#23569;&#22238;&#31572;&#38169;&#35823;&#25554;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2401.17981</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#26816;&#27979;&#27169;&#22411;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#25552;&#39640;&#22270;&#20687;&#29702;&#35299;&#21644;&#20943;&#23569;&#22238;&#31572;&#38169;&#35823;&#25554;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38598;&#25104;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#32454;&#33410;&#35270;&#35273;&#20803;&#32032;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;MLLMs&#32467;&#21512;&#65292;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#29702;&#35299;&#65292;&#24182;&#20943;&#23569;&#22238;&#31572;&#20013;&#30340;&#38169;&#35823;&#25554;&#20837;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#26816;&#27979;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#36825;&#31181;&#34701;&#21512;&#23545;MLLMs&#30340;&#21407;&#22987;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#20114;&#25442;&#24615;&#12290;&#25105;&#20204;&#23545;LLaVA-1.5&#12289;DINO&#21644;PaddleOCRv2&#31561;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;MLLMs&#22312;&#29305;&#23450;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20445;&#25345;&#20102;&#23427;&#20204;&#30340;&#21407;&#22987;&#20248;&#21183;&#12290;&#36890;&#36807;&#22312;10&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22686;&#24378;&#30340;MLLMs&#22312;9&#20010;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#26631;&#20934;&#21270;&#24179;&#22343;&#24471;&#20998;&#25552;&#21319;&#39640;&#36798;12.99%&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a not
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17267</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35270;&#35282;&#35299;&#32806;&#23398;&#20064;&#25913;&#36827;&#20302;&#36164;&#28304;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#20219;&#21153;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#23545;&#20851;&#31995;&#30340;&#34920;&#23618;&#29702;&#35299;&#65292;&#20808;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#20851;&#31995;&#34920;&#31034;&#23545;&#20110;RE&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;MVRE&#65288;&#22810;&#35270;&#35282;&#20851;&#31995;&#25277;&#21462;&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;PLMs&#30340;&#33021;&#21147;&#26469;&#25913;&#21892;&#20302;&#36164;&#28304;&#25552;&#31034;&#35843;&#25972;&#33539;&#24335;&#19979;&#30340;RE&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MVRE&#23558;&#27599;&#20010;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20197;&#21253;&#21547;&#22810;&#35270;&#35282;&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#31995;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20284;&#28982;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;&#24615;&#30340;&#20302;&#39046;&#22495;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20851;&#31995;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning with pre-trained language models (PLMs) has demonstrated the significantly enhancing ability of relation extraction (RE) tasks. However, in low-resource scenarios, where the available training data is scarce, previous prompt-based methods may still perform poorly for prompt-based representation learning due to a superficial understanding of the relation. To this end, we highlight the importance of learning high-quality relation representation in low-resource scenarios for RE, and propose a novel prompt-based relation representation method, named MVRE (\underline{M}ulti-\underline{V}iew \underline{R}elation \underline{E}xtraction), to better leverage the capacity of PLMs to improve the performance of RE within the low-resource prompt-tuning paradigm. Specifically, MVRE decouples each relation into different perspectives to encompass multi-view relation representations for maximizing the likelihood during relation inference. Furthermore, we also design a Global-Lo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.12728</link><description>&lt;p&gt;
Lookahead:&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#26080;&#25439;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#38382;&#31572;&#12289;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20687;&#25903;&#20184;&#23453;&#36825;&#26679;&#20026;&#25968;&#21313;&#20159;&#29992;&#25143;&#25552;&#20379;&#37325;&#35201;&#37329;&#34701;&#20135;&#21697;&#30340;&#38656;&#35201;&#20934;&#30830;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25903;&#20184;&#23453;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#23558;LLMs&#19982;&#26368;&#20934;&#30830;&#21644;&#26368;&#26032;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20026;&#25968;&#30334;&#19975;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#30340;&#30495;&#23454;&#20135;&#21697;&#26469;&#35828;&#65292;LLMs&#30340;&#25512;&#29702;&#36895;&#24230;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#23454;&#39564;&#24615;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;RAG&#31995;&#32479;&#30340;&#36895;&#24230;&#22823;&#24133;&#25552;&#21319;&#21644;&#25104;&#26412;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#30528;&#26080;&#25439;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#12290;&#22312;&#20256;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#20196;&#29260;&#37117;&#30001;LLMs&#25353;&#39034;&#24207;&#29983;&#25104;&#65292;&#23548;&#33268;&#30340;&#26102;&#38388;&#28040;&#32791;&#19982;&#29983;&#25104;&#30340;&#20196;&#29260;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation (RAG) system that grounds LLMs on the most accurate and up-to-date information. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.  Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our RAG system, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#36830;&#32493;&#23398;&#20064;&#37325;&#22609;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#20803;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#38656;&#35201;&#23545;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#36830;&#32493;&#23398;&#20064;episode&#19978;&#30340;&#20803;&#32423;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36890;&#29992;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.11952</link><description>&lt;p&gt;
&#23558;&#36830;&#32493;&#23398;&#20064;&#37325;&#22609;&#20026;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Recasting Continual Learning as Sequence Modeling. (arXiv:2310.11952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#36830;&#32493;&#23398;&#20064;&#37325;&#22609;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#20803;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#38656;&#35201;&#23545;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#36830;&#32493;&#23398;&#20064;episode&#19978;&#30340;&#20803;&#32423;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36890;&#29992;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#20004;&#20010;&#37325;&#35201;&#39046;&#22495;&#20043;&#38388;&#30340;&#24378;&#36830;&#25509;&#65306;&#36830;&#32493;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#36827;&#34892;&#34920;&#36848;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#20808;&#36827;&#30340;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#25104;&#20026;&#24207;&#21015;&#27169;&#22411;&#30340;&#21069;&#21521;&#20256;&#25773;&#12290;&#36890;&#36807;&#37319;&#29992;&#20803;&#36830;&#32493;&#23398;&#20064;(MCL)&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22810;&#20010;&#36830;&#32493;&#23398;&#20064;episode&#19978;&#23545;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#20803;&#32423;&#35757;&#32451;&#12290;&#20316;&#20026;&#25105;&#20204;&#26032;&#26694;&#26550;&#30340;&#19968;&#20010;&#20855;&#20307;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;Transformer&#21450;&#20854;&#39640;&#25928;&#21464;&#20307;&#24212;&#29992;&#20110;MCL&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#19971;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36890;&#29992;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#36880;&#28857;&#12289;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#36880;&#28857;&#26041;&#27861;&#30340;&#25928;&#29575;&#39640;&#20294;&#25928;&#26524;&#24046;&#65292;&#36880;&#23545;&#26041;&#27861;&#25928;&#26524;&#22909;&#20294;&#35745;&#31639;&#22797;&#26434;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09497</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#30340;&#39640;&#25928;&#38598;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models. (arXiv:2310.09497v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#36880;&#28857;&#12289;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#36880;&#28857;&#26041;&#27861;&#30340;&#25928;&#29575;&#39640;&#20294;&#25928;&#26524;&#24046;&#65292;&#36880;&#23545;&#26041;&#27861;&#25928;&#26524;&#22909;&#20294;&#35745;&#31639;&#22797;&#26434;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;&#26679;&#26412;&#25991;&#26723;&#25490;&#21517;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#38024;&#23545;&#22522;&#20110;LLM&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#36880;&#28857;&#65292;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#23454;&#39564;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#26631;&#35760;&#28040;&#32791;&#65292;&#24310;&#36831;&#31561;&#22240;&#32032;&#12290;&#36825;&#31181;&#39318;&#27425;&#30340;&#27604;&#36739;&#35780;&#20272;&#35753;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#27599;&#31181;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#22266;&#26377;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36880;&#28857;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#24471;&#20998;&#24456;&#39640;&#65292;&#20294;&#22312;&#26377;&#25928;&#24615;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#36880;&#23545;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;LLM&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;LLM&#25512;&#29702;&#30340;&#27425;&#25968;&#21644;&#25490;&#21517;&#36807;&#31243;&#20013;&#30340;&#25552;&#31034;&#26631;&#35760;&#28040;&#32791;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate impressive effectiveness in zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting approaches have been proposed for LLM-based zero-shot ranking. Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others. This first-of-its-kind comparative evaluation of these approaches allows us to identify the trade-offs between effectiveness and efficiency inherent in each approach. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. To further enhance the efficiency of LLM-based zero-shot ranking, we propose a novel Setwise prompting approach. Our approach reduces the number of LLM inferences and the amount of prompt token consumption during the rankin
&lt;/p&gt;</description></item><item><title>ChatKBQA&#26159;&#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08975</link><description>&lt;p&gt;
ChatKBQA: &#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#29992;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. (arXiv:2310.08975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08975
&lt;/p&gt;
&lt;p&gt;
ChatKBQA&#26159;&#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#33719;&#21462;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#30740;&#31350;&#32452;&#25104;&#37096;&#20998;&#65306;&#30693;&#35782;&#26816;&#32034;&#21644;&#35821;&#20041;&#35299;&#26512;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#21253;&#25324;&#20302;&#25928;&#30340;&#30693;&#35782;&#26816;&#32034;&#12289;&#26816;&#32034;&#38169;&#35823;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#19981;&#21033;&#24433;&#21709;&#20197;&#21450;&#20043;&#21069;&#30340;KBQA&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatKBQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#31934;&#35843;&#24320;&#28304;LLMs&#65288;&#22914;Llama-2&#12289;ChatGLM2&#21644;Baichuan2&#65289;&#26500;&#24314;&#30340;&#29983;&#25104;-&#26816;&#32034;KBQA&#26694;&#26550;&#12290;ChatKBQA&#25552;&#35758;&#39318;&#20808;&#20351;&#29992;&#31934;&#35843;&#30340;LLMs&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#28982;&#21518;&#36890;&#36807;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#26816;&#32034;&#21644;&#26367;&#25442;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#30452;&#35266;&#22320;&#25913;&#36827;&#20102;&#29983;&#25104;&#21644;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatKBQA&#22312;&#26631;&#20934;KBQA&#25968;&#25454;&#38598;WebQSP&#21644;ComplexWebQuestions (CWQ)&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Question Answering (KBQA) aims to derive answers to natural language questions over large-scale knowledge bases (KBs), which are generally divided into two research components: knowledge retrieval and semantic parsing. However, three core challenges remain, including inefficient knowledge retrieval, retrieval errors adversely affecting semantic parsing, and the complexity of previous KBQA methods. In the era of large language models (LLMs), we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework built on fine-tuning open-source LLMs such as Llama-2, ChatGLM2 and Baichuan2. ChatKBQA proposes generating the logical form with fine-tuned LLMs first, then retrieving and replacing entities and relations through an unsupervised retrieval method, which improves both generation and retrieval more straightforwardly. Experimental results reveal that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and ComplexWebQuestions (CWQ). This
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07867</link><description>&lt;p&gt;
&#24265;&#20215;&#23545;&#35805;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cheap Talking Algorithms. (arXiv:2310.07867v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27169;&#25311;&#29420;&#31435;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20811;&#21171;&#31119;&#24503;&#21644;&#32034;&#36125;&#23572;&#65288;1982&#65289;&#30340;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19968;&#20010;&#21457;&#36865;&#32773;&#21644;&#19968;&#20010;&#25509;&#25910;&#32773;&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#65292;&#25910;&#25947;&#21040;&#25509;&#36817;&#28216;&#25103;&#20808;&#39564;&#26368;&#20248;&#22343;&#34913;&#30340;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#36890;&#20449;&#22312;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#31243;&#24230;&#32473;&#20986;&#30340;&#32435;&#20160;&#22343;&#34913;&#19979;&#65292;&#25353;&#29031;&#26368;&#22823;&#31243;&#24230;&#36827;&#34892;&#12290;&#36825;&#19968;&#32467;&#35770;&#23545;&#36229;&#21442;&#25968;&#21644;&#28216;&#25103;&#30340;&#22791;&#36873;&#35268;&#33539;&#31283;&#20581;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#31639;&#27861;&#38388;&#26032;&#20852;&#36890;&#20449;&#24037;&#20316;&#20197;&#21450;&#30001;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32452;&#25104;&#30340;&#24066;&#22330;&#20013;&#30340;&#23467;&#26007;&#32463;&#27982;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We simulate behaviour of independent reinforcement learning algorithms playing the Crawford and Sobel (1982) game of strategic information transmission. We show that a sender and a receiver training together converge to strategies close to the exante optimal equilibrium of the game. Hence, communication takes place to the largest extent predicted by Nash equilibrium given the degree of conflict of interest between agents. The conclusion is shown to be robust to alternative specifications of the hyperparameters and of the game. We discuss implications for theories of equilibrium selection in information transmission games, for work on emerging communication among algorithms in computer science and for the economics of collusions in markets populated by artificially intelligent agents.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#23545;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.16733</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#38887;&#24615;&#65306;&#20998;&#26512;&#21644;&#21152;&#22266;&#25216;&#26415;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques. (arXiv:2309.16733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16733
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#23545;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30446;&#21069;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#26159;&#26368;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#20043;&#19968;&#65292;&#22914;&#35270;&#35273;&#12289;&#33258;&#20027;&#31995;&#32479;&#31561;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#20154;&#20204;&#23545;ML&#24212;&#29992;&#22312;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#24433;&#21709;&#19979;&#30340;&#20998;&#26512;&#21644;&#35774;&#35745;&#20570;&#20986;&#20102;&#22823;&#37327;&#36129;&#29486;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#27425;&#28145;&#20837;&#30340;&#22238;&#39038;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;ML&#25216;&#26415;&#20043;&#19968;&#65289;&#23545;&#25239;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#30340;&#24050;&#26377;&#30693;&#35782;&#65292;&#28165;&#26224;&#22320;&#21576;&#29616;&#20102;&#36825;&#19968;&#25991;&#29486;&#27969;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25991;&#31456;&#22522;&#20110;2019&#24180;1&#26376;&#33267;2023&#24180;3&#26376;&#38388;&#21457;&#34920;&#30340;163&#31687;&#31185;&#23398;&#35770;&#25991;&#65292;&#37319;&#29992;&#20998;&#31867;&#26694;&#26550;&#26469;&#35299;&#35835;&#21644;&#31361;&#20986;&#30740;&#31350;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#29305;&#28857;&#65292;&#20174;&#24037;&#20316;&#30340;&#20027;&#35201;&#33539;&#22260;&#12289;&#37319;&#29992;&#30340;&#25925;&#38556;&#21644;&#38169;&#35823;&#27169;&#22411;&#31561;&#22810;&#20010;&#21442;&#25968;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is currently being exploited in numerous applications being one of the most effective Artificial Intelligence (AI) technologies, used in diverse fields, such as vision, autonomous systems, and alike. The trend motivated a significant amount of contributions to the analysis and design of ML applications against faults affecting the underlying hardware. The authors investigate the existing body of knowledge on Deep Learning (among ML techniques) resilience against hardware faults systematically through a thoughtful review in which the strengths and weaknesses of this literature stream are presented clearly and then future avenues of research are set out. The review is based on 163 scientific articles published between January 2019 and March 2023. The authors adopt a classifying framework to interpret and highlight research similarities and peculiarities, based on several parameters, starting from the main scope of the work, the adopted fault and error models, to the
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24182;&#35299;&#20915;&#22240;&#20026;&#32570;&#22833;&#20540;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13047</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#35266;&#27979;&#25968;&#25454;&#30340;&#32852;&#37030;&#22240;&#26524;&#25928;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Causal Effects from Incomplete Observational Data. (arXiv:2308.13047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13047
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24182;&#35299;&#20915;&#22240;&#20026;&#32570;&#22833;&#20540;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#26080;&#27861;&#21512;&#24182;&#20026;&#19968;&#20010;&#23454;&#20307;&#65292;&#32780;&#20854;&#20013;&#30340;&#32570;&#22833;&#20540;&#21487;&#33021;&#20250;&#24341;&#20837;&#20559;&#24046;&#21040;&#22240;&#26524;&#20272;&#35745;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#32852;&#37030;&#22240;&#26524;&#25512;&#26029;&#65292;&#20174;&#32780;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25439;&#22833;&#20989;&#25968;&#25286;&#20998;&#20026;&#22810;&#20010;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#29305;&#23450;&#25968;&#25454;&#28304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32570;&#22833;&#38543;&#26426;&#20551;&#35774;&#19979;&#32771;&#34385;&#20102;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#20272;&#35745;&#20102;&#22240;&#26524;&#20272;&#35745;&#30340;&#39640;&#38454;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#20998;&#25955;&#30340;&#25968;&#25454;&#28304;&#20013;&#24674;&#22797;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20272;&#35745;&#20102;&#24322;&#36136;&#30340;&#26465;&#20214;&#20998;&#24067;&#20197;&#24212;&#23545;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized and incomplete data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated into a single entity owing to privacy constraints, and the presence of missing values within them can potentially introduce bias to the causal estimands. We introduce a new approach for federated causal inference from incomplete data, enabling the estimation of causal effects from multiple decentralized and incomplete data sources. Our approach disentangles the loss function into multiple components, each corresponding to a specific data source with missing values. Our approach accounts for the missing data under the missing at random assumption, while also estimating higher-order statistics of the causal estimands. Our method recovers the conditional distribution of missing confounders given the observed confounders from the decentralized data sources to identify causal effects. Our framework estimates heterogeneou
&lt;/p&gt;</description></item><item><title>SecureFalcon&#26159;&#22522;&#20110;FalconLLM&#30340;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24494;&#35843;FalconLLM&#26469;&#23454;&#29616;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#65292;&#33021;&#22815;&#35782;&#21035;C&#20195;&#30721;&#26679;&#26412;&#20013;&#30340;&#28431;&#27934;&#21644;&#38750;&#28431;&#27934;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.06616</link><description>&lt;p&gt;
SecureFalcon:&#19979;&#19968;&#20195;&#38754;&#21521;&#32593;&#32476;&#23433;&#20840;&#30340;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SecureFalcon: The Next Cyber Reasoning System for Cyber Security. (arXiv:2307.06616v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06616
&lt;/p&gt;
&lt;p&gt;
SecureFalcon&#26159;&#22522;&#20110;FalconLLM&#30340;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24494;&#35843;FalconLLM&#26469;&#23454;&#29616;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#65292;&#33021;&#22815;&#35782;&#21035;C&#20195;&#30721;&#26679;&#26412;&#20013;&#30340;&#28431;&#27934;&#21644;&#38750;&#28431;&#27934;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#28431;&#27934;&#23548;&#33268;&#21508;&#31181;&#19981;&#21033;&#24433;&#21709;&#65292;&#22914;&#23849;&#28291;&#12289;&#25968;&#25454;&#20002;&#22833;&#21644;&#23433;&#20840;&#28431;&#27934;&#65292;&#20005;&#37325;&#24433;&#21709;&#36719;&#20214;&#24212;&#29992;&#21644;&#31995;&#32479;&#30340;&#24066;&#22330;&#37319;&#29992;&#29575;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#12289;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#26368;&#24120;&#29992;&#19988;&#26377;&#22266;&#26377;&#30340;&#35823;&#25253;&#29575;&#65292;&#32473;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#36825;&#20123;&#25345;&#20037;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;FalconLLM&#22312;&#35782;&#21035;&#22797;&#26434;&#27169;&#24335;&#21644;&#28431;&#27934;&#26041;&#38754;&#26174;&#31034;&#20986;&#37325;&#35201;&#28508;&#21147;&#65292;&#22240;&#27492;&#22312;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;FalconLLM&#36827;&#34892;&#20102;&#38024;&#23545;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#25512;&#20986;&#20102;SecureFalcon&#65292;&#36825;&#26159;&#22522;&#20110;FalconLLM&#30340;&#21019;&#26032;&#27169;&#22411;&#26550;&#26500;&#12290;SecureFalcon&#34987;&#35757;&#32451;&#29992;&#20110;&#21306;&#20998;&#26377;&#28431;&#27934;&#21644;&#26080;&#28431;&#27934;&#30340;C&#20195;&#30721;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software vulnerabilities leading to various detriments such as crashes, data loss, and security breaches, significantly hinder the quality, affecting the market adoption of software applications and systems. Although traditional methods such as automated software testing, fault localization, and repair have been intensively studied, static analysis tools are most commonly used and have an inherent false positives rate, posing a solid challenge to developer productivity. Large Language Models (LLMs) offer a promising solution to these persistent issues. Among these, FalconLLM has shown substantial potential in identifying intricate patterns and complex vulnerabilities, hence crucial in software vulnerability detection. In this paper, for the first time, FalconLLM is being fine-tuned for cybersecurity applications, thus introducing SecureFalcon, an innovative model architecture built upon FalconLLM. SecureFalcon is trained to differentiate between vulnerable and non-vulnerable C code sam
&lt;/p&gt;</description></item><item><title>FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.11650</link><description>&lt;p&gt;
FedNoisy: &#20998;&#24067;&#24335;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11650
&lt;/p&gt;
&lt;p&gt;
FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#22240;&#20026;&#26080;&#38656;&#23545;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25935;&#24863;&#25968;&#25454;&#36827;&#34892;&#32858;&#21512;&#32780;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#25968;&#25454;&#38548;&#31163;&#30340;&#20998;&#24067;&#24335;&#21644;&#23396;&#31435;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#36136;&#37327;&#30340;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#26631;&#31614;&#30340;&#24178;&#25200;&#12290;&#35768;&#22810;&#21162;&#21147;&#37117;&#33268;&#21147;&#20110;&#22312;&#38598;&#20013;&#24335;&#25110;&#32852;&#21512;&#24335;&#29615;&#22659;&#20013;&#38450;&#24481;&#22122;&#22768;&#26631;&#31614;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#32771;&#34385;&#21508;&#31181;&#20856;&#22411;&#32852;&#21512;&#23398;&#20064;&#22330;&#26223;&#20013;&#22122;&#22768;&#26631;&#31614;&#24433;&#21709;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#32852;&#21512;&#22122;&#22768;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25506;&#32034;&#36825;&#20123;&#25968;&#25454;&#35774;&#32622;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#65292;&#36825;&#21487;&#33021;&#25351;&#23548;&#26410;&#26469;&#30340;&#26041;&#27861;&#24320;&#21457;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20986;&#30340;20&#20010;&#22522;&#26412;&#35774;&#32622;&#65292;&#36866;&#29992;&#20110;5&#20010;&#20197;&#19978;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35299;&#20915;&#20102;&#20855;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#19981;&#19968;&#33268;&#25968;&#25454;&#24211;&#30340;&#20462;&#22797;&#21644;&#26597;&#35810;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#31216;&#24046;&#20998;&#20462;&#22797;&#24182;&#25351;&#23450;&#39318;&#36873;&#20462;&#22797;&#34892;&#21160;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#26368;&#20248;&#20462;&#22797;&#27010;&#24565;&#65292;&#24182;&#19988;&#30740;&#31350;&#20102;&#20462;&#22797;&#27010;&#24565;&#30340;&#35745;&#31639;&#23646;&#24615;&#65292;&#21516;&#26102;&#28548;&#28165;&#20102;&#19982;&#20027;&#21160;&#23436;&#25972;&#24615;&#32422;&#26463;&#26694;&#26550;&#20013;&#24341;&#20837;&#30340;&#20462;&#22797;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.03523</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#20248;&#20808;&#25968;&#25454;&#24211;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#22788;&#29702;&#65306;&#22797;&#26434;&#24230;&#20998;&#26512;&#21644;&#19982;&#20027;&#21160;&#23436;&#25972;&#24615;&#32422;&#26463;&#30340;&#32852;&#31995;(arXiv:2306.03523v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
Inconsistency Handling in Prioritized Databases with Universal Constraints: Complexity Analysis and Links with Active Integrity Constraints. (arXiv:2306.03523v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35299;&#20915;&#20102;&#20855;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#19981;&#19968;&#33268;&#25968;&#25454;&#24211;&#30340;&#20462;&#22797;&#21644;&#26597;&#35810;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#31216;&#24046;&#20998;&#20462;&#22797;&#24182;&#25351;&#23450;&#39318;&#36873;&#20462;&#22797;&#34892;&#21160;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#26368;&#20248;&#20462;&#22797;&#27010;&#24565;&#65292;&#24182;&#19988;&#30740;&#31350;&#20102;&#20462;&#22797;&#27010;&#24565;&#30340;&#35745;&#31639;&#23646;&#24615;&#65292;&#21516;&#26102;&#28548;&#28165;&#20102;&#19982;&#20027;&#21160;&#23436;&#25972;&#24615;&#32422;&#26463;&#26694;&#26550;&#20013;&#24341;&#20837;&#30340;&#20462;&#22797;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#24102;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#19981;&#19968;&#33268;&#25968;&#25454;&#24211;&#30340;&#20462;&#22797;&#21644;&#26597;&#35810;&#38382;&#39064;&#12290;&#37319;&#29992;&#23545;&#31216;&#24046;&#20998;&#20462;&#22797;&#65292;&#21363;&#36890;&#36807;&#21024;&#38500;&#21644;&#28155;&#21152;&#20107;&#23454;&#26469;&#24674;&#22797;&#19968;&#33268;&#24615;&#65292;&#24182;&#20551;&#35774;&#36890;&#36807;&#23545;&#65288;&#21542;&#23450;&#65289;&#20107;&#23454;&#30340;&#20108;&#20803;&#20248;&#20808;&#20851;&#31995;&#26469;&#25351;&#23450;&#39318;&#36873;&#20462;&#22797;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23637;&#31034;&#22914;&#20309;&#36866;&#24403;&#22320;&#23558;&#29616;&#26377;&#30340;&#26368;&#20248;&#20462;&#22797;&#27010;&#24565;&#65288;&#20165;&#23545;&#22522;&#20110;&#20107;&#23454;&#21024;&#38500;&#30340;&#31616;&#21333;&#25298;&#32477;&#32422;&#26463;&#21644;&#20462;&#22797;&#23450;&#20041;&#65289;&#25193;&#23637;&#21040;&#25105;&#20204;&#26356;&#20016;&#23500;&#30340;&#35774;&#32622;&#20013;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#24471;&#21040;&#30340;&#20462;&#22797;&#27010;&#24565;&#30340;&#35745;&#31639;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20462;&#22797;&#26816;&#26597;&#21644;&#23481;&#24525;&#19981;&#19968;&#33268;&#26597;&#35810;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28548;&#28165;&#20102;&#20248;&#20808;&#25968;&#25454;&#24211;&#30340;&#26368;&#20248;&#20462;&#22797;&#19982;&#22312;&#20027;&#21160;&#23436;&#25972;&#24615;&#32422;&#26463;&#26694;&#26550;&#20013;&#24341;&#20837;&#30340;&#20462;&#22797;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#20462;&#22797;&#23545;&#24212;&#20110; founded&#12289;grounded &#21644; just&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the problem of repairing and querying inconsistent databases equipped with universal constraints. We adopt symmetric difference repairs, in which both deletions and additions of facts can be used to restore consistency, and suppose that preferred repair actions are specified via a binary priority relation over (negated) facts. Our first contribution is to show how existing notions of optimal repairs, defined for simpler denial constraints and repairs solely based on fact deletion, can be suitably extended to our richer setting. We next study the computational properties of the resulting repair notions, in particular, the data complexity of repair checking and inconsistency-tolerant query answering. Finally, we clarify the relationship between optimal repairs of prioritized databases and repair notions introduced in the framework of active integrity constraints. In particular, we show that Pareto-optimal repairs in our setting correspond to founded, grounded and just
&lt;/p&gt;</description></item></channel></rss>